{
  "research_topic": "Improving efficiency of hyperparameter optimization",
  "queries": [
    "efficient hyperparameter optimization",
    "hyperparameter search acceleration",
    "multi-fidelity hyperparameter optimization",
    "gradient-based hyperparameter tuning",
    "parallel hyperparameter optimization"
  ],
  "research_study_list": [
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Bayesian Optimization for Iterative Learning (BOIL) to efficiently tune hyperparameters for iterative learning algorithms like Deep Reinforcement Learning (DRL) and Convolutional Neural Networks (CNNs). Key contributions include compressing the entire learning curve into a single numeric score that considers both training success and stability, learning this compression function directly from data, and employing a selective data augmentation technique to improve sample-efficiency and prevent Gaussian Process (GP) covariance matrix conditioning issues. BOIL significantly outperforms existing baselines in identifying optimal hyperparameters in less wall-clock time.",
        "methodology": "BOIL models the cost-sensitive objective function f(x,t) using a Gaussian Process (GP) with a product kernel over hyperparameters x and training iterations t. The training time cost c(x,t) is approximated by a linear regressor. The algorithm selects the next evaluation point by maximizing a cost-aware acquisition function (a modified Expected Improvement criterion) that balances utility, exploration, and computational cost. Learning curves are compressed into a numeric score using a Sigmoid preference function, whose growth and middle point parameters (g0, m0) are learned by maximizing the GP's log marginal likelihood. To augment data efficiently and avoid GP covariance matrix ill-conditioning, a subset of points from the observed learning curve is actively selected at regions of maximum GP predictive uncertainty, with the number of augmented points adaptively controlled by a condition number threshold.",
        "experimental_setup": "The algorithm's efficiency was demonstrated by tuning hyperparameters for Dueling DQN on the CartPole-v0 environment, and Advantage Actor Critic (A2C) agents on the InvertedPendulum-v2 and Reacher-v2 environments. Additionally, convolutional neural networks were tuned on the SVHN and CIFAR10 datasets. All experiments were conducted on an NVIDIA 1080 GTX GPU using the tensorflow-gpu Python package, with results averaged over 20 independent runs with different random seeds. Square-exponential kernels were used for the GP. Baselines for comparison included Hyperband and Continuous Multi-task/Multi-fidelity BO (CM-T/F-BO). Hyperparameter search ranges for DRL agents and CNNs, along with best-found parameters, were detailed in the appendix.",
        "limitations": "A primary limitation is the potential for Gaussian Process covariance matrix ill-conditioning if a naive, full-curve data augmentation approach is used, which BOIL addresses with its selective augmentation strategy. Existing early stopping criteria for Bayesian Optimization, such as the exponential decay assumption in Freeze-Thaw BO, were found unsuitable for the unpredictable and noisy fluctuations typical of DRL reward curves. The cost function is approximated by a linear regressor for simplicity, which may not capture more complex dependencies if they exist. A broader impact consideration is that increased automation in ML model training could distance humans from the modeling process, potentially making critical failure detection more challenging.",
        "future_research_directions": "The framework is highlighted for its broad applicability, extending beyond machine learning to any process exhibiting an iterative structure that can be exploited, such as the optimization of manufacturing pipelines. The work is also viewed as a step towards fully automated machine learning model training and deployment pipelines, suggesting further research in integrating BOIL into such systems. Future directions also include rigorous analysis of final training outcomes using interpretability methods to address the growing opacity of machine learning models and mitigate potential biases.",
        "experimental_code": "class ProductGaussianProcess(object):\n    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')\n    \n    \n    #def __init__ (self,param):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']\n#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        \"\"\"\n        Fit Gaussian Process model\n\n        Input Parameters\n        ----------\n        x: the observed points \n        t: time or number of episode\n        y: the outcome y=f(x)\n        \n        \"\"\" \n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n        #self.Y_curves=Y_curves[myidx]\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta\n    \n        Euc_dist_t=euclidean_distances(T,T)\n        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       \n    \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)\n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        \"\"\"\n        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter\n        \"\"\"\n\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            # compute K\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            # transform Y_curve to Y_original, then to Y\n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\\\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                # if the matrix is too large, we randomly select a part of the data for fast computation\n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                #Wi, LW, LWi, W_logdet = pdinv(KK)\n                #sign,W_logdet2=np.linalg.slogdet(KK)\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A\n    \n                #second_term=-0.5*W_logdet2\n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            #print(lengthscale, lengthscale_t,midpoint,growth,\"logmarginal:\",logmarginal)\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n#    def optimize_lengthscale_SE_maximizing(self,previous_theta,noise_delta):\n#        \"\"\"\n#        Optimize to select the optimal lengthscale parameter\n#        \"\"\"\n#                \n#        # define a bound on the lengthscale\n#        SearchSpace_lengthscale_min=0.01\n#        SearchSpace_lengthscale_max=0.5\n#        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n#        \n#        mySearchSpace=np.asarray([[SearchSpace_lengthscale_min,SearchSpace_lengthscale_max],\\\n#                             [10*SearchSpace_lengthscale_min,2*SearchSpace_lengthscale_max]])\n#        \n#        # Concatenate new random points to possible existing\n#        # points from self.explore method.           \n#        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, mySearchSpace.shape[0]))\n\n#        #print lengthscale_tries\n\n#        # evaluate\n#        self.flagOptimizeHyperFirst=0 # for efficiency\n\n#        logmarginal_tries=self.log_marginal_lengthscale(lengthscale_tries,noise_delta)\n#        #print logmarginal_tries\n\n#        #find x optimal for init\n#        idx_max=np.argmax(logmarginal_tries)\n#        lengthscale_init_max=lengthscale_tries[idx_max]\n#        #print lengthscale_init_max\n#        \n#        myopts ={'maxiter':20*self.dim,'maxfun':20*self.dim}\n\n#        x_max=[]\n#        max_log_marginal=None\n#        \n#        res = minimize(lambda x: -self.log_marginal_lengthscale(x,noise_delta),lengthscale_init_max,\n#                       SearchSpace=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n#        if 'x' not in res:\n#            val=self.log_marginal_lengthscale(res,noise_delta)    \n#        else:\n#            val=self.log_marginal_lengthscale(res.x,noise_delta)  \n#        \n#        # Store it if better than previous minimum(maximum).\n#        if max_log_marginal is None or val >= max_log_marginal:\n#            if 'x' not in res:\n#                x_max = res\n#            else:\n#                x_max = res.x\n#            max_log_marginal = val\n#            #print res.x\n\n#        return x_max\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        \"\"\"\n        Optimize to select the optimal lengthscale parameter\n        \"\"\"\n        \n        # define a bound on the lengthscale\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        # evaluate\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        #find x optimal for init\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        #print lengthscale_init_max\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        # Store it if better than previous minimum(maximum).\n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n            #print res.x\n\n        return x_max\n\n\n#    def optimize_lengthscale(self,previous_theta_x, previous_theta_t,noise_delta):\n#\n#        prev_theta=[previous_theta_x,previous_theta_t]\n#        newlengthscale,newlengthscale_t=self.optimize_lengthscale_SE_maximizing(prev_theta,noise_delta)\n#        self.hyper['lengthscale_x']=newlengthscale\n#        self.hyper['lengthscale_t']=newlengthscale_t\n#        \n#        # refit the model\n#        temp=np.hstack((self.X,self.T))\n#        ur = unique_rows(temp)\n#        \n#        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n#        \n#        return newlengthscale,newlengthscale_t\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        # optimize both GP lengthscale and logistic hyperparameter\n\n            \n        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        # refit the model\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        # update Y here\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        #\n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        \"\"\"\n        compute variance given X and xTest\n        \n        Input Parameters\n        ----------\n        X: the observed points\n        xTest: the testing points \n        \n        Returns\n        -------\n        diag(var)\n        \"\"\" \n        \n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            #Euc_dist=euclidean_distances(xTest,xTest)\n            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta\n            #ur = unique_rows(X)\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      \n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    \n        \n    def predict(self,xTest, eval_MSE=True):\n        \"\"\"\n        compute predictive mean and variance\n        Input Parameters\n        ----------\n        xTest: the testing points \n        \n        Returns\n        -------\n        mean, var\n        \"\"\"    \n\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        # prevent singular matrix\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        #Exp_dist_test_train_x*Exp_dist_test_train_t\n  \n        # using Cholesky update\n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n    def posterior(self,x):\n        # compute mean function and covariance function\n        return self.predict(self,x)\n        \n    \n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \n        gp_params:                  GP parameters\n        gp_params.theta:            to compute the kernel\n        gp_params.delta:            to compute the kernel\n        \n        func_params:                function to optimize\n        func_params.init bound:     initial SearchSpace for parameters\n        func_params.SearchSpace:        SearchSpace on parameters        \n        func_params.func:           a function to be optimized\n        \n        \n        acq_params:            acquisition function, \n        acq_params.acq_func['name']=['ei','ucb','poi']\n        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))\n\n\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    # this is the Logistic transformation, used in the paper\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    #print(MaxEpisode)\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\ndef return_logistic_curve(midpoint, growth, MaxEpoch=1000):\n    # given the growth, midpoint and npoint, return the Logistic curve for visualization\n    \n    def logistic_func(x):\n        #alpha=32\n        if len(x)==1:\n            return 1.0/(1+np.exp(-growth*(x-midpoint)))\n        else:\n            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]\n        \n    my_xrange_scaled=np.linspace(-6,6, MaxEpoch)\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n    \n    return my_logistic_value_scaled\n\n\ndef transform_logistic_marginal(curves,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    def transform_one_logistic_marginal(curves,MaxEpisode):\n        # curve is a vector [1 x MaxIter]\n    \n        midpoint_list=[-3,-2,-1,0,1]\n        growth_list=[0.1,1,2,3]\n        \n        temp_Y_value=[0]*(len(midpoint_list)*len(growth_list))\n        for idx, (val1, val2) in enumerate(itertools.product(midpoint_list,growth_list)):\n            temp_Y_value[idx]=apply_one_transform_logistic(curves,val1, val2,MaxEpisode)\n                \n        temp_Y_value=np.asarray(temp_Y_value)\n        \n        Y=np.mean(temp_Y_value,axis=0)\n        return Y\n    if len(curves)==1:\n        output=transform_one_logistic_marginal(curves[0],MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=transform_one_logistic_marginal(curve,MaxEpisode)\n    return output    \n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n    \n\n\nclass AcquisitionFunction(object):\n    \"\"\"\n    An object to compute the acquisition functions.\n    \"\"\"\n\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max'] # this is for ei_mu acquisition function\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max'                          ]\n        \n        # check valid acquisition function\n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        #if  not in acq_name:\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        \n        #if type(meta) is dict and 'y_max' in meta.keys():\n        #   y_max=meta['y_max']\n        y_max=np.max(gp.Y)\n        #print self.kind\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max': # using max mu(x) as incumbent\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    def utility_plot(self, x, gp, y_max):\n        if np.any(np.isnan(x)):\n            return 0\n        if self.acq_name == 'ei':\n            return self._ei_plot(x, gp, y_max)\n  \n   \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _lcb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n\n        return mean - np.sqrt(beta_t) * np.sqrt(var) \n        \n    \n    @staticmethod\n    def _ucb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T                \n        \n        # Linear in D, log in t https://github.com/kirthevasank/add-gp-bandits/blob/master/BOLibkky/getUCBUtility.m\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n  \n        #beta=300*0.1*np.log(5*len(gp.Y))# delta=0.2, gamma_t=0.1\n        return mean + np.sqrt(beta_t) * np.sqrt(var) \n    \n    \n    @staticmethod\n    def _ucb_pe(x, gp, kappa, maxlcb):\n        mean, var = gp.predict_bucb(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n\n        value=mean + kappa * np.sqrt(var)        \n        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]\n        var[myidx]=0        \n        return var\n    \n   \n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n        \n   \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)        \n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \n        out[var2<1e-10]=0\n        return out\n \n \n    @staticmethod      \n    def _poi(x, gp,y_max): # run Predictive Entropy Search using Spearmint\n        mean, var = gp.predict(x, eval_MSE=True)    \n        # Avoid points with zero variance\n        var = np.maximum(var, 1e-9 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var)        \n        return norm.cdf(z)\n\n   \ndef unique_rows(a):\n    \"\"\"\n    A functions to trim repeated rows that may appear when optimizing.\n    This is necessary to avoid the sklearn GP object from breaking\n\n    :param a: array to trim repeated rows from\n\n    :return: mask of unique rows\n    \"\"\"\n\n    # Sort array and kep track of where things should go back to\n    order = np.lexsort(a.T)\n    reorder = np.argsort(order)\n\n    a = a[order]\n    diff = np.diff(a, axis=0)\n    ui = np.ones(len(a), 'bool')\n    ui[1:] = (diff != 0).any(axis=1)\n\n    return ui[reorder]\n\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    dim=SearchSpace.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n    #sobol_sequence=generate_sobol_seq(dim=dim,nSobol=500*dim)\n\n    # multi start\n    for i in range(3*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        #x_tries=sobol_sequence\n    \n        # evaluate\n        y_tries=myfunc(x_tries,**kwargs)\n        \n        #find x optimal for init\n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        # Store it if better than previous minimum(maximum).\n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n            #print max_acq\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n    \ndef acq_max_scipy(ac, gp, y_max, bounds):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n\n    # multi start\n    for i in range(1*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        # evaluate\n        y_tries=ac(x_tries,gp=gp)\n        #print \"elapse evaluate={:.5f}\".format(end_eval-start_eval)\n        \n        #find x optimal for init\n        idx_max=np.argmax(y_tries)\n        #print \"max y_tries {:.5f} y_max={:.3f}\".format(np.max(y_tries),y_max)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n\n        \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n            #print max_acq\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])\n        #print max_acq\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n    \n    # COBYLA -> x_max[0]\n    # L-BFGS-B -> x_max",
        "experimental_info": "BOIL (Bayesian Optimization with Informed Learning) is implemented using a `ProductGaussianProcess` for the objective function and a `linear_model.LinearRegression` for the training time cost. The default acquisition function is 'ei_mu_max' (Expected Improvement using the maximum of the GP mean as incumbent).\n\n**GP Settings (`ProductGaussianProcess`):**\n*   Initial noise variance (`noise_delta`): 5e-4\n*   Initial lengthscale for hyperparameters `x` (`lengthscale_x`): 0.02\n*   Initial lengthscale for training iterations `t` (`lengthscale_t`): 0.2\n*   Hyperparameter optimization: Performed every `2 * dim` iterations.\n    *   Optimization bounds for lengthscales: `x` from 0.03 to 0.3; `t` from `10 * 0.03` to `2 * 0.3` (i.e., 0.3 to 0.6).\n    *   Number of random initial points for multi-start optimization: 20.\n    *   Optimizer options: `maxiter`: `30 * dim`, `maxfun`: `30 * dim`.\n\n**Learning Curve Compression (Sigmoid Preference Function):**\n*   Initial sigmoid midpoint (`midpoint`): 0.0\n*   Initial sigmoid growth (`growth`): 1.0\n*   Optimization bounds for sigmoid parameters: `midpoint` from -2 to 3; `growth` from 0.5 to 2.\n*   Learning method: Maximizing the GP's log marginal likelihood.\n*   Marginalization for final evaluation: The final score is evaluated by marginalizing across predefined lists of sigmoid `midpoint_list` ([-3, -2, -1, 0, 1]) and `growth_list` ([0.1, 1, 2, 3]).\n\n**Acquisition Function Maximization:**\n*   Optimization method: SciPy's L-BFGS-B (`minimize` function).\n*   Multi-start optimization: `1 * dim` restarts for maximization (`acq_max_scipy`), `3 * dim` restarts for minimization (`acq_min_scipy_kwargs`).\n*   Random initial points for optimization: `50 * dim` points for `acq_max_scipy`, `100 * dim` points for `acq_min_scipy_kwargs`.\n*   Optimizer options: `maxiter`: `10 * dim`, `maxfun`: `20 * dim`.\n\n**Data Augmentation (Virtual Observations):**\n*   Mechanism: Subsets of points from observed learning curves are actively selected at regions of maximum GP predictive uncertainty.\n*   Maximum number of augmentations per real observation (`max_n_augmentation`): 10.\n*   Adaptive control: Augmentation stops if the log of the GP covariance matrix's condition number (`log_cond`) exceeds a threshold of 15 (`threshold_cond`) or if the predictive variance is very small (`< 1e-3` after adding noise `1e-3` which is effectively `noise_delta`).\n\n**Initialization:**\n*   Number of initial points (`n_init_points`): 3 (default).\n*   Random seed for initialization (`seed`): 1 (default)."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Partitioned Neural Networks, a novel and efficient method for hyperparameter optimization that operates within a single training run and requires no validation data. Inspired by the marginal likelihood, this approach partitions both the training data into K shards and the neural network model's parameters into K partitions. Each parameter partition is optimized on specific data shards, and the 'out-of-training-sample' loss on data shards unseen by a subnetwork is used as the objective for hyperparameter optimization. This method is shown to be significantly computationally cheaper than alternative marginal likelihood-based approaches for neural networks and is particularly beneficial for complex tasks like learning input masks, data augmentations, feature extractors, and in resource-constrained environments such as Federated Learning, where retraining and cross-validation are challenging.",
        "methodology": "The core methodology involves optimizing an approximation to the lower-bound of the marginal likelihood, LML. The neural network's parameters 'w' are divided into 'C' partitions (w1, ..., wC), and 'C' subnetworks are constructed where the k-th subnetwork w(k)s uses parameters (w1, ..., wk) and sets remaining parameters to default values (ˆwk+1, ..., ˆwC). A partitioned training scheme interleaves stochastic gradient updates for both parameter partitions 'wk' and hyperparameters 'ψ'. Parameter updates for wk are computed using loss on data shards D1:k, while hyperparameter updates for ψ are computed using the sum of out-of-sample losses on shards Dk for subnetworks w(k-1)s. This ensures that a subnetwork only 'sees' data from its designated partitions. The most common partitioning scheme used in experiments is 'random weight partitioning', where a fixed proportion of weights in each layer is randomly assigned to a partition. For Federated Learning, clients are assigned to data chunks, and local updates involve optimizing relevant parameter partitions while only communicating changes to the server, thereby reducing upload costs.",
        "experimental_setup": "The method was evaluated across various tasks and datasets. For input selection, a synthetic dataset with informative and spurious features was used with fully connected neural networks. For learning invariances through data augmentations, MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet) were used with CNN and FixupResNet architectures (FixupResNet-8, FixupResNet-14, ResNet-50 with GroupNorm). Experiments compared partitioned networks against baselines like standard training, Augerino, Differentiable Laplace, and Last-layer ML. Validation methods included test accuracy and log-likelihood. A differentiable input selection task used a learnable mask with a concrete Bernoulli distribution. Comparisons to traditional train/validation splits for learning data augmentations on CIFAR10 subsets and learning feature extractors on CIFAR10 (Wide ResNet-20) were performed. Federated Learning experiments on non-i.i.d. partitioned MNIST and CIFAR10 (with label-skew and rotation-skew) evaluated performance against FedAvg and FedAvg + Augerino, optimizing data augmentation and dropout rates using CNN and ResNet-9 models. Training details specified Adam or SGD optimizers, varying learning rates, batch sizes, weight decay (scaled for earlier partitions), and specific iteration counts.",
        "limitations": "The proposed method has several limitations. It inherently requires an additional forward-backward pass for updating hyperparameters, which, despite being significantly less costly than existing methods, introduces extra computational overhead. Empirically, partitioned networks also tend to require more training iterations to converge. Furthermore, partitioning the network introduces a capacity constraint, which can lead to a slight loss in performance compared to a fully optimized, non-partitioned network with ideal hyperparameters. Lastly, the method introduces new hyperparameters related to the partitioning strategy itself (e.g., number of chunks, relative proportions of data and parameters per chunk), which may require additional tuning for optimal performance, although experiments suggest a reasonable level of robustness to these choices.",
        "future_research_directions": "Future research could explore dynamic partitioning of network parameters during training, allowing for more adaptive and potentially optimal allocation of resources. Investigating alternative partitioning schemes beyond the current random weight or node partitioning, such as those that automatically learn partitioning or are more amenable to single-batch multi-partition updates, is another promising direction. Efforts to alleviate the inherent performance loss due to network partitioning could involve adjusting training rounds or strategically increasing network capacity. Additionally, exploring different sequential updating schemes for partitions in Federated Learning and investigating methods to reduce computational overhead, such as accumulating gradients from different chunks or less frequent hyperparameter updates, are areas for further study."
      }
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf",
        "github_url": "https://github.com/QB3/sparse-ho"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces an efficient implicit differentiation algorithm for hyperparameter optimization of Lasso-type models, addressing the challenges of setting regularization parameters for non-smooth objectives. It proposes a novel approach that avoids matrix inversion and scales to high-dimensional data by leveraging solution sparsity. Key contributions include demonstrating that forward iterative differentiation of block coordinate descent (BCD) converges to the true gradient, and presenting an algorithm that decouples regression coefficient and Jacobian computation, thus not requiring the solution of potentially ill-conditioned linear systems. Experiments show that the proposed method outperforms numerous standard hyperparameter optimization techniques for optimizing held-out error or the Stein Unbiased Risk Estimator (SURE).",
        "methodology": "The methodology casts hyperparameter optimization as a bi-level optimization problem, where the inner problem is a Lasso-type estimator (Lasso or weighted Lasso) and the outer problem optimizes a criterion like held-out loss or SURE. The core is an efficient algorithm for estimating the gradient with respect to hyperparameters (hypergradient) using implicit differentiation. Unlike traditional implicit differentiation, which relies on smooth loss functions, this method is tailored for non-smooth Lasso-type problems. It leverages the fixed-point iteration of proximal BCD algorithms, differentiating the soft-thresholding step to derive a closed-form solution for the Jacobian. The proposed 'Implicit Forward Iterative Differentiation' algorithm decouples the computation of regression coefficients (computed using any state-of-the-art convex solver) from the Jacobian. The Jacobian is then computed by applying forward differentiation recursion steps restricted to the identified support, leading to faster and more stable computation compared to solving a linear system. Hyperparameters are parametrized as e^λ to avoid positivity constraints.",
        "experimental_setup": "The Python implementation, utilizing Numba for critical BCD loops, is released as an open-source package. For fair comparison, all methods used the same vanilla BCD algorithm (Algorithm 5) with a stopping tolerance of 10^-5. Hypergradient-based methods employed a line-search strategy. Lasso models were initialized with λ = λmax - log(10). Competitors included Implicit, Forward Iterative Differentiation, Grid-search, Random-search, Lattice Hypercube sampling, and Bayesian optimization. Evaluation used held-out loss on real, high-dimensional datasets (rcv1, 20news, finance, split into train/validation/test) and the Stein Unbiased Risk Estimator (SURE) with its differentiable approximation (dof_FDMC) on synthetic data. Synthetic data (n=100, p varied from 200 to 10,000, 5 non-zero coefficients, SNR=3) was used to measure normalized Mean Squared Error (MSE) over 50 repetitions. The method was also applied to a non-convex MCP estimator on rcv1 and 20news.",
        "limitations": "The theoretical guarantees of the proposed method, particularly regarding Jacobian convergence, rely on the assumption of a unique solution for the inner optimization problem (Lasso or weighted Lasso). While non-unique solution cases are typically rare, the theory does not cover such pathological settings or cases where the solution path is non-continuous. Furthermore, the theory explicitly does not cover non-convex penalty functions like MCP, although the method numerically behaves properly in such scenarios. The overall hyperparameter optimization problem is non-convex, meaning convergence to a global minimum is not guaranteed and relies on proper initialization. Backward iterative differentiation was found to be computationally expensive and memory-intensive for the target problems, leading to its exclusion from most benchmarks. The SURE criterion requires prior knowledge of the noise variance.",
        "future_research_directions": "Future work could focus on extending the theoretical guarantees to handle non-convex penalty formulations, such as the Minimax Concave Penalty (MCP), which were shown to behave properly numerically but are not covered by the current theory. Another direction is to further explore the integration of the proposed two-step approach with more advanced state-of-the-art Lasso solvers that utilize techniques like active sets or screening rules. This could involve addressing the discontinuities that such algorithms might introduce. Additionally, investigating the behavior and potential adaptations of the algorithm for rare cases where the Lasso solution is non-unique and the solution path is non-continuous could be a fruitful area, possibly leading to methods beyond gradient-based optimization.",
        "experimental_code": "import numpy as np\nfrom scipy.sparse import issparse\nfrom numba import njit\nfrom numpy.linalg import norm\nimport scipy.sparse.linalg as slinalg\nfrom sparse_ho.utils import init_dbeta0_new, ST\n\n# --- Core helper from sparse_ho/algo/forward.py ---\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    alpha = np.exp(log_alpha)\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    for i in range(max_iter):\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n        # Simplified stopping criterion check\n        if use_stop_crit and i > 0 and (model._get_pobj(dual_var, X, beta, alphas, y) - model._get_pobj(dual_var, X, beta, alphas, y)) <= model._get_pobj0(dual_var, beta, alphas, y) * tol:\n            break\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask) if compute_jac else None\n    \n    # Store dual variables for warm-start if model has dual attribute\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n\n    return mask, dense, jac\n\n\n# --- Implicit Forward Iterative Differentiation from sparse_ho/algo/implicit_forward.py ---\n\nclass ImplicitForward():\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    # Step 1: Compute regression coefficients beta using inner solver\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, model, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n\n    # Step 2: Initialize Jacobian (dbeta) on the identified support\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    # Initialize dual variables for Jacobian computation\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n\n    # Step 3: Compute Jacobian using forward recursion steps restricted to the identified support\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    # Initialize dual Jacobian (ddual_var)\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    residual_norm = [] # Used for stopping criterion\n\n    for i in range(niter_jac):\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        \n        # Stopping criterion for Jacobian computation\n        if use_stop_crit:\n            current_res_norm = model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var, ddual_var, alpha)\n            residual_norm.append(current_res_norm)\n            if i > 1:\n                rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n                if (rel_tol < np.abs(residual_norm[-1]) * tol_jac or residual_norm[-1] < 1e-10):\n                    break\n\n    return dbeta\n\n# --- Example of differentiating the soft-thresholding step (from sparse_ho/models/lasso.py) ---\n\nclass Lasso(): # Simplified Lasso model class for illustration\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    # Placeholder for other required BaseModel methods (e.g., get_L, _init_beta_dual_var, etc.)\n    def get_L(self, X):\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros(n_features)\n        if jac0 is None or not compute_jac:\n            ddual_var = np.zeros(n_samples)\n        else:\n            dbeta[mask0] = jac0.copy()\n            ddual_var = - X[:, mask0] @ jac0.copy()\n        return dbeta, ddual_var\n\n    def _get_pobj0(self, dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    def _get_pobj(self, dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) +\n            np.abs(alphas * beta).sum())\n\n    def _get_jac(self, dbeta, mask):\n        return dbeta[mask]\n\n    def _init_dbeta0(self, mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is not None:\n            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)\n        else:\n            dbeta0_new = np.zeros(size_mat)\n        return dbeta0_new\n\n    def _init_dbeta(self, n_features):\n        dbeta = np.zeros(n_features)\n        return dbeta\n\n    def _init_ddual_var(self, dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    def sign(self, x, log_alpha):\n        return np.sign(x)\n\n    def reduce_X(self, X, mask):\n        return X[:, mask]\n\n    def reduce_y(self, y, mask):\n        return y\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta, dbeta, dual_var, ddual_var, alpha):\n        return norm(ddual_var.T @ ddual_var + n_samples * alpha * sign_beta @ dbeta)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            # Primal update: zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j]) # Soft-thresholding step\n            if compute_jac:\n                # Jacobian update for the soft-thresholding step\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj # Differentiating soft-thresholding\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j] # Gradient w.r.t hyperparameter\n                # update residuals\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,\n                         L, alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features,\n            dbeta, dual_var, ddual_var, L, alpha, sign_beta):\n        for j in range(n_features):\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n",
        "experimental_info": "The methodology casts hyperparameter optimization as a bi-level optimization problem. The inner problem involves Lasso-type estimators (Lasso, Elastic Net, Sparse Logistic Regression, Weighted Lasso, SVM, SVR, Simplex SVR). The outer problem optimizes a criterion like held-out loss (MSE, Logistic Loss, Smoothed Hinge), Cross-Validation (MSE, Logistic Loss), or the Finite-Difference Monte-Carlo Stein Unbiased Risk Estimator (SURE).\n\n**1. Inner Problem Models and Solvers:**\n-   **Lasso**: `sparse_ho.models.Lasso` wrapping `celer.Lasso(fit_intercept=False, warm_start=True, max_iter=50-10000)`. Hyperparameter: `alpha` (L1 penalty).\n-   **ElasticNet**: `sparse_ho.models.ElasticNet` wrapping `celer.ElasticNet(fit_intercept=False, warm_start=True, max_iter=50-10000)`. Hyperparameters: `alpha_l1`, `alpha_l2`.\n-   **Sparse Logistic Regression**: `sparse_ho.models.SparseLogreg` wrapping `sklearn.linear_model.LogisticRegression(penalty='l1', fit_intercept=False, solver='saga', max_iter=100-10000)`. Hyperparameter: `alpha` (L1 penalty).\n-   **Weighted Lasso**: `sparse_ho.models.WeightedLasso` wrapping `celer.Lasso(fit_intercept=False, warm_start=True, tol=1e-3)`. Hyperparameters: an array of `n_features` `alpha_i`.\n\n**2. Hypergradient Estimation (Implicit Forward Iterative Differentiation):**\n-   **Algorithm**: `sparse_ho.ImplicitForward`.\n-   **Parameters**: `tol_jac` (tolerance for Jacobian computation, e.g., `1e-3`, `1e-8`), `n_iter_jac` (maximum iterations for Jacobian, e.g., `100`, `1000`). Inner solver `max_iter` (`100` to `100_000`) and `tol` (`1e-5` to `1e-8`) are passed to `compute_beta_grad`.\n\n**3. Outer Optimization Criteria:**\n-   **Held-out Mean Squared Error**: `sparse_ho.criterion.HeldOutMSE`.\n-   **Held-out Logistic Loss**: `sparse_ho.criterion.HeldOutLogistic`.\n-   **Held-out Smoothed Hinge Loss**: `sparse_ho.criterion.HeldOutSmoothedHinge`.\n-   **Cross-Validation**: `sparse_ho.criterion.CrossVal` (wraps Held-out MSE or Logistic Loss) with `cv=KFold(n_splits=5, shuffle=True, random_state=42)`.\n-   **Finite-Difference Monte-Carlo SURE**: `sparse_ho.criterion.FiniteDiffMonteCarloSure` with `sigma` (noise level, e.g., `1/np.sqrt(nave)` for MEG), `finite_difference_step` (often heuristic based on `sigma`), and `random_state`.\n-   **Multiclass Logistic Regression**: `sparse_ho.criterion.LogisticMulticlass`.\n\n**4. Outer Optimizers:**\n-   **LineSearch**: `sparse_ho.optimizers.LineSearch(n_outer=10-100, tol=1e-8, t_max=10000)`. Uses adaptive step size.\n-   **GradientDescent**: `sparse_ho.optimizers.GradientDescent(n_outer=10-100, step_size=None or explicit value, p_grad_norm=1-1.9, verbose=True, tol=1e-8, t_max=10000)`. Uses a heuristic adaptive step size.\n-   **Adam**: `sparse_ho.optimizers.Adam(n_outer=10, lr=0.01-0.11, beta_1=0.9, beta_2=0.999, epsilon=1e-3, verbose=False, tol=1e-5, t_max=10000)`.\n\n**5. Datasets:**\n-   **Real-world**: `rcv1.binary`, `rcv1`, `news20`, `real-sim`, `leukemia`, `finance`, `MEG data` (sample audvis). Used via `libsvmdata.datasets.fetch_libsvm`.\n-   **Synthetic (`simu`)**: `sklearn.datasets.make_classification(n_samples=100, n_features=1_000, random_state=42, flip_y=0.02)` or `celer.datasets.make_correlated_data(n_samples=200, n_features=400, snr=5, random_state=0)`.\n\n**6. Hyperparameter Ranges and Initialization:**\n-   **Alpha max (`alpha_max`)**: Empirically determined from training data, e.g., `np.max(np.abs(X[idx_train, :].T.dot(y[idx_train]))) / len(idx_train)`. Specific models might have scaling factors (e.g., Logistic Regression divided by 2 or 4).\n-   **Initial alpha (`alpha0`)**: Typically `0.1 * alpha_max`, `alpha_max / 10`, or `0.9 * alpha_max`. For multi-hyperparameter models (ElasticNet, WeightedLasso), it's an array of initial values.\n-   **Parametrization**: Hyperparameters are optimized in log-space (`log_alpha = np.log(alpha)`) to ensure positivity and facilitate unconstrained optimization. Internal `proj_hyperparam` function clips `log_alpha` to a valid range, e.g., `[log_alpha_max - 12, log_alpha_max + np.log(0.9)]` for Lasso."
      }
    },
    {
      "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
      "abstract": "An open problem in differentially private deep learning is hyperparameter\noptimization (HPO). DP-SGD introduces new hyperparameters and complicates\nexisting ones, forcing researchers to painstakingly tune hyperparameters with\nhundreds of trials, which in turn makes it impossible to account for the\nprivacy cost of HPO without destroying the utility. We propose an adaptive HPO\nmethod that uses cheap trials (in terms of privacy cost and runtime) to\nestimate optimal hyperparameters and scales them up. We obtain state-of-the-art\nperformance on 22 benchmark tasks, across computer vision and natural language\nprocessing, across pretraining and finetuning, across architectures and a wide\nrange of $\\varepsilon \\in [0.01,8.0]$, all while accounting for the privacy\ncost of HPO.",
      "full_text": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Ashwinee Panda * 1 Xinyu Tang* 1 Saeed Mahloujifar 1 Vikash Sehwag 1 Prateek Mittal 1 Figure 1.Visualization of our method. We use low-cost trials (small ε) to estimate hyperparameters (HPs) and scale these up to the privacy budget for the final run. We combine multiple HPs together, and have a prior that the scaling is linear. Abstract An open problem in differentially private deep learning is hyperparameter optimization (HPO). DP-SGD introduces new hyperparameters and complicates existing ones, forcing researchers to painstakingly tune hyperparameters with hun- dreds of trials, which in turn makes it impossible to account for the privacy cost of HPO without de- stroying the utility. We propose an adaptive HPO method that uses cheap trials (in terms of privacy cost and runtime) to estimate optimal hyperparam- eters and scales them up. We obtain state-of-the- art performance on 22 benchmark tasks, across computer vision and natural language processing, across pretraining and finetuning, across architec- tures and a wide range ofε ∈ [0.01, 8.0], all while accounting for the privacy cost of HPO. *Equal contribution 1Princeton University. Correspondence to: Ashwinee Panda <ashwinee@princeton.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 0.5 1 2 4 8 Privacy/uni00A0Budget/uni00A0() 85 86 87 88 89 90ImageNet/uni00A0Acc/uni00A0(%) Ours/uni00A0(Merged/uni00AD38M) Mehta/uni00A02023/uni00A0(JFT) Berrada/uni00A02023/uni00A0(JFT) Figure 2.Evaluation on ImageNet-1k finetuning. Our HPO only requires paying the privacy cost once, and can then be used to find good HPs for all values of ε > 0.5. We outperform prior work (Mehta et al., 2023b; Berrada et al., 2023) because our HPO finds better HPs, even though prior work has better non-private performance and does not report the privacy cost of their HPO. 1. Introduction A crucial component of interfacing machine learning models closely with user data is ensuring that the process remains private (Team, 2017), and Differential Privacy (DP) is the gold standard for quantifying privacy risks and providing provable guarantees against attacks (Dwork et al., 2006). DP implies that the output of an algorithm e.g., the final weights trained by stochastic gradient descent (SGD) do not change much if a single datapoint in the dataset changes. Definition 1.1 (Differential Privacy). A randomized mech- anism M with domain D and range R preserves (ε, δ)- differential privacy iff for any two neighboring datasets D, D′ ∈ Dand for any subsetS ⊆ Rwe have Pr[M(D) ∈ S] ≤ eε Pr[M(D′) ∈ S] + δ where D and D′ are neighboring datasets if they differ in a single entry, ε is the privacy budget and δ is the failure probability. Differentially Private Stochastic Gradient Descent (DP- SGD) (Song et al., 2013; Abadi et al., 2016) is the stan- dard privacy-preserving training algorithm for training neu- ral networks on private data. For a batch size B and 1 arXiv:2212.04486v3  [cs.LG]  5 May 2024A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization learning rate η, DP-SGD has an update rule given by w(t+1) = w(t)− ηt |Bt| \u0000P i∈Bt 1 C clipC(∇ℓ(xi, w(t))) + σξ \u0001 where the changes to SGD are the per-sample gradient clipping clipC(∇ℓ(xi, w(t))) = C×∇ℓ(xi,w(t)) max(C,||∇ℓ(xi,w(t))||2) , and addition of noise sampled from a d-dimensional Gaussian distribution ξ ∼ N(0, 1) with standard deviation σ. These steps alter the bias-variance tradeoff of SGD and degrade utility, creating a challenging privacy-utility tradeoff. Because private training introduces additional hyperparame- ters, biases optimization by clipping the gradient, and im- poses privacy-utility tradeoffs for existing hyperparameters, hyperparameter optimization (HPO) in DP is challenging. Many prior works report doing hundreds of hyperparameter trials and do not report the privacy cost of HPO in their final privacy guarantee (De et al., 2022; Bu et al., 2022a;b; Mehta et al., 2023a;b; Berrada et al., 2023). These works either assume that HPO does not leak privacy, that the best HPs are known beforehand, or that they can be transferred from a public dataset that is similar to the private dataset. More recently, researchers have proposed methods that do private HPO (Papernot & Steinke, 2021; Koskela & Kulka- rni, 2023; Wang et al., 2023) with R ´enyi DP. These pri- vate HPO methods have been evaluated on MNIST and CIFAR10, but have not been validated on more challenging tasks in CV , or on LLMs. We propose a new private adaptive HPO method ( Figure 1), which we call the new linear scaling rule. We first estimate the optimal HPs for small privacy budgets. We then scale the searched HPs linearly up to larger privacy budgets. Our full method is described in Algorithm 2. We summarize our contributions: • We demonstrate that our new linear scaling rule reduces the computation and privacy cost of HPO by an order of magnitude without sacrificing performance • We compare our private HPO method to random search, grid search, and 3 prior methods for private HPO • We evaluate our private HPO on 22 tasks spanning com- puter vision and natural language processing, fine-tuning and training from scratch, training models spanning from ResNets to multi-billion-parameter Transformers • We find that models trained with our method can pro- vide good performance even when there is a large shift between public and private data 2. Design We provide a set of design goals for our adaptive private HPO method and explain their importance. We use simple axioms for optimization and privacy as building blocks to motivate the high-level design of our method. We conduct preliminary experiments to quantitatively determine the re- lationships between key hyperparameters. Ultimately we compose the many hyperparameters of interest in DP into a single scalar variable r, and present a simple adaptive approach for privately optimizing this parameter. 2.1. Design Goals We draw our goals from the two simple baselines for HPO, random search and grid search. We define random search as drawing hyperparameters from the search space randomly and doing a single run with the entire privacy budget. We discuss variations on random search, such as doing multiple runs with smaller privacy budgets, in Section 5. Random search has low runtime, is parallelizable, and has low privacy cost, but typically does not provide good performance when the hyperparameter search space is large and the set of viable solutions is sparse. Grid search typically has high runtime and privacy cost, and is also parallelizable. Given sufficient trials, grid search should approach the performance of the oracle, the run with perfectly chosen hyperparameters. Our method should provide: • Better performance than random search and almost as good as the oracle; if we define the error rate of any HPO method as the difference in performance between that method and the oracle, our method should reduce the error rate relative to random search significantly. • Better privacy cost than grid search and almost the same privacy cost as random search; the difference in privacy-utility tradeoff between our method and the or- acle should not be the difference between a run with ε = 0.1 and ε = 1.0, it should be the relatively smaller gap between ex. ε = 0.9 and ε = 1.0. 2.2. Building Blocks of Linear Scaling The design of our adaptive private HPO method is based on simple building blocks derived from known theorems in optimization and privacy. First we inspect the definition of DP-SGD and the nature of adaptive composition. Suppose we are taking T steps with noise σ to produce some ε guar- antee. If we relax our privacy guarantee, so now we want to achieve some ε∗ > ε, we can either a) fix T and reduceσ, b) increase T and fix σ, or c) some combination of (a) and (b). Second we turn to a rule of thumb that is popularly known as the original linear scaling rule; the optimal learning rate is inversely proportional to the noise scale in GD (Malladi et al., 2022). In the case of DP-GD, that is in the full batch setting where there is no noise due to SGD, the learning rate should be inversely proportional to σ. If we combine these two axioms, we get the following heuristic: 2A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Proposition 2.1. If we are taking T steps with noise σ and learning rate η to achieve a target ε∗, we can achieve a target ˆε > ε∗ by either: a) Fix T, reduce σ, increase η) b) Increase T, fix σ, fix η c) Increase T slightly, reduce σ slightly, increase η slightly. We now formalize this intuition. 3. Analysis of Private Gradient Descent We analyze the excess empirical risk of private GD as the sum of two terms. The first term is the risk of non-private GD with the same hyperparameters. The second term is the divergence of private GD from non-private GD due to the added noise term. We consider optimizing a function using Differentially Pri- vate Gradient Descent (DP-GD). The presence of noise in GD introduces a deviation between the iterates of GD with noise, denoted as wT , and without noise, denoted as wTb , at iteration T. We first upper bound this deviation in expec- tation, which we refer to as the radius r. We then use this to upper bound the excess empirical risk of noisy GD. We finally use this bound to motivate the design of our private adaptive HPO method. 3.1. Assumptions We present four assumptions that simplify the conver- gence analysis. We acknowledge that these assumptions do not hold true in all settings, but nevertheless provide an important foundation for illustrating the intuition of our method. We empirically validate the success of our algo- rithm in complex neural network settings, such as training a 13B-parameter OPT Transformer model on the benchmark SQuAD task, in Section 3. • A function is α-strongly convex if for any two points x, yand any subgradient g at x, it holds that f(y) ≥ f(x) + g⊤(y − x) + α 2 ∥y − x∥2. • A function is β-smooth if its gradient is β- Lipschitz continuous, meaning for any two points x, y, ∥∇f(x) − ∇f(y)∥ ≤β∥x − y∥. • A function is L-Lipschitz if there exists a positive L such that |f(x) − f(y)| ≤L∥x − y∥ • A function satisfies the bounded gradient assumption if there exists a constant C such that E[∥∇f(w)∥ ≤ C ∀w ∈ Rd The bounded gradient assumption is implied by convexity and Lipschitzness. This allows us to ignore the impact of clipping in DP-SGD, which reduces the analysis to that of noisy GD. The noise added at each iteration for privacy has an expected norm ρ = √ d · σ, where d is the dimension of the model, and σ is the scale of the noise. The learning rate η satisfies 0 < η <2 β , ensuring convergence. Let c = max(|1 − ηα|, |1 − ηβ|), which characterizes the contraction factor in the optimization process. Given that η is chosen appropriately, we have 0 < c <1. 3.2. Definitions The empirical loss L(wT ) for a model parameterized by wT (ex. iterate T of GD) over a dataset D = {(xi, yi)}N i=1 is defined as the average loss over all training samples: L(wT ) = 1 N NX i=1 ℓ(f(xi; wT ), yi) The goal of our private Hyperparameter Optimization (HPO) is to find the hyperparameter set Λ∗ that minimizes the loss: Λ∗ = argmin Λ Lval(Λ) where Lval(Λ) denotes the loss on a validation dataset for a given hyperparameter configuration Λ. Because Noisy GD typically does not overfit due to the heavy regularization effect of the noise, and to make the convergence analysis straightforward, we use the empirical loss as a proxy for the validation loss throughout. We will analyze the excess empirical risk to motivate the design of our private HPO method. Let wT be the Tth iterate of noisy GD that optimizes a function satisfying the assumptions, and wTb be the Tth iterate of non-noisy GD that optimizes that same function. We define the excess empirical risk of noisy GD as: Rnoisy = E[L(wT )] − L(w∗), ≤ E[L(wT ) − L(wTb )] + L(wTb ) − L(w∗) ≤ E[L · ∥wT − wTb ∥] + Rnon-noisy Where L(w∗) denotes the empirical loss at the optimal pa- rameter set (without noise). Rnon-noisy = L(wTb ) − L(w∗ is the excess empirical risk of non-noisy GD. In the last line, we upper bounded the excess risk induced by noise L(wTb ) − L(w∗) by applying Lipschitzness of the loss. We now bound ∥wT − wTb ∥. Theorem 3.1. Let wT be the Tth iterate of noisy GD that optimizes an α-strongly convex and β-smooth function, and let wTb be the Tth iterate of non-noisy GD that optimizes 3A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization that same function. The ”noisy radius” distance, the ℓ2- norm between wT and wTb at iteration T, can be bounded in expectation as follows: E[∥wT − wTb ∥] ≤ ρη ×  T−1X i=0 ci ! = r Proof sketch. The full proof is in Appendix B.5. At each iteration the distance between the noisy iterate and the non- noisy iterate contracts by a factor ofc = max(|1−ηα|, |1− ηβ|) and then increases additively by ρη. The overall dis- tance then can be represented by scaling the additive noise term ρη by a geometric series that converges. Future work might incorporate additional factors such as momentum ac- celeration, bias introduced by clipping, or extend our analy- sis to the setting of more general neural networks. However, our objective here is to provide some theoretical intuition for our algorithm. Substituting Theorem 3.1 into the excess empirical risk we get Rnoisy ≤ Lr + Rnon-noisy where L is the Lipschitz constant, we can see that our pri- vate HPO needs to find HPs that are good for non-noisy optimization but do not create a large divergence between the noisy and non-noisy iterates. 4. Our Private HPO We have established a relationship between the excess em- pirical risk and the noisy radius. We can now connect this back to our goal of doing private HPO, which is to find the HPs that minimize the excess empirical risk. We want to find r∗ = r(ε), the optimal value of r for a given value of ε. We will first reduce the dimensionality of the search problem and then introduce a principled approximation. 4.1. Reducing the Dimensionality of HPO We want to reduce the dimensionality of HPO so that we can reduce the cost of HPO. For fixed ε, if we increase or decrease T then we will cor- respondingly increase or decrease σ by the Composition Theorem of DP. The actual statements of DP composition are somewhat complicated, but we can simplify them as say- ing σ grows slower than αT for some constant α. Because E[ρ] = √ dσ, we have that ρ grows slower than T. The geometric series converges to 1 1 − c as T increases, giving us E[∥wT − wTb ∥] ≤ (T η) · ( √ d 1 1 − c). Because we are interested in writing the radius in terms of hyperpa- rameters that we can optimize, we drop the second term for simplicity. Now we can write our hyperparameter of interest as r = η × T, reducing the 2D HPO to 1D. If we wanted to search for additional terms such as the batch size or clipping threshold, we could incorporate them into our theory, but we empirically find that it’s best to fix all other HPs to the values we provide and just search for η, T. 4.2. Our Private HPO In order to find the optimal r∗ = r(ε) without exhaustively searching, we need to approximate r(ε). A natural choice is Taylor approximation. We can sample points from r(ε) at different values of ε via random search, use this to ap- proximate a Taylor polynomial, and then use that Taylor polynomial to estimate r for any desired target ε. After we have our estimated r, we can decompose it into η, T by randomly sampling η, Tuntil their product is close to r. This is the procedure we use in Figure 2, paying for the privacy cost of building the approximation and then using it to estimate the optimal HPs for many values of ε ∈ [0.5, 8]. We now elaborate on the implementation of the method. The first-order Taylor approximation of a function r(ε) around a point ε0 is given by r(ε) ≈ r(ε0) + dr dε \f\f ε=ε0 · (ε − ε0), which linearly approximates r near ε0. Because we cannot analytically determine dr dε , we will have to ap- proximate this. To approximate the first-order Taylor polynomial we fit a line. We first use random search to find two empirical points (ε1, r(ε1)) and (ε2, r(ε2)). We then fit a line to these points to obtain the parameters of the linem, b(slope and intercept). We finally estimate the optimal r(εf ) = mεf + b such that the composition of privacy guarantees for the entire private HPO satisfies a target privacy budget according to Theorem 2.3. In practice we choose smaller values of ε for these points such as ε1 = 0.1, ε2 = 0.2, that we find provide a good privacy-utility tradeoff. More generally, we can approximate the Taylor polyno- mial by fitting a degree N polynomial with N + 1 points (ε1, r(ε1)) ··· (εN+1, r(εN+1)). We provide results com- paring the linear approximation to quadratic approximation in the 2nd common response PDF, but use the linear ap- proximation throughout our work because we find that it provides a good privacy-utility tradeoff. The full method is detailed in Algorithm 2. The final pri- vacy guarantee including the cost of HPO is given by Theo- rem 4.1. Theorem 4.1. The privacy guarantee of Algorithm 2 in terms of µ in f-DP is µt = q nµ2 1 + nµ2 2 + µ2 f . The proof and conversion to (ε, δ)-DP follow directly from Dong et al. (2022), so we defer it to Appendix B.5. 4A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 1.Our method fixes six design choices: the architecture and initialization (for CV tasks only), the batch size (full batch), the optimizer (SGD with momentum=0.9), the accounting method (PLV where all prior HPO methods use RDP), and the clipping norm (unit clipping). We report the improvement derived from following each of these techniques with respect to a competitive baseline from prior work on CIFAR100 at ε = 0.1. Method Baseline Baseline Accuracy Improvement Classifier (no bias) (Mehta et al., 2023b) 71.3 0 .36 Zero Initialization Random Initialization (De et al., 2022) 64.85 6 .81 Gradient Descent SGD(Batch=4096) (De et al., 2022) 70.2 1 .46 Momentum (ρ = 0.9) ρ = 0 (Bu et al., 2022a) 69.02 2 .09 PLV Accounting RDP (De et al., 2022) 68.43 3 .23 Unit Clipping (C = 1) C ≪ 1 (Mehta et al., 2023a) 71.2 0 .46 Implementing our method requires decomposing a target ε, δ-DP guarantee into a set of µs; we provide code for this. 4.3. Limitations Although this theory does not hold in general for training neural networks, we quantitatively evaluate the heuristic we develop in Section 3.4 and find that our method holds even for the complex setting of training Transformers on NLP benchmarks. Our HPO also requires more runtime than random search because it is adaptive. Algorithm 1 Model Training Subroutine Initialize model weights w at 0 Decompose r into η, Twithout exceeding Tmax or ηmax Use the PLD accountant to calibrate σ given µ, T for i = 1, 2, . . . , Tdo Compute gradient with unit clipping and add noise ∇(i) = 1 |D| \u0000P i∈D clip1(∇ℓ(xi, w(i))) + σξ \u0001 Take a step with momentum: v(i) ← ρ · v(i−1) + ∇(i), w(i) ← w(i−1) − ηv(i) end for return trained model w Algorithm 2 Adaptive HPO Routine Inputs: Privacy parameters for hyperparameter sweeps and final run µ1, µ2, µf , number of runs per sweep n, maximum learning rate ηmax, maximum number of itera- tions Tmax, dataset D, model M Perform n runs with µ1 using Hyperparameter Sweep Subroutine (Algorithm 3); obtain the best-performing r1 Perform n runs with µ2 using Hyperparameter Sweep Subroutine (Algorithm 3), obtain the best-performing r2 Perform linear interpolation to estimate the slope α and bias b of the line r = αε + b given (µ1, r1), (µ2, r2) Set r∗ = αµf + b given the estimated linear interpolation Launch the Model Training Subroutine (Algorithm 1) with r∗, µf , obtaining the final performance Af Output: Final performance Af , trained model M Algorithm 3 Hyperparameter Sweep Subroutine Inputs: Privacy parameter µ, number of runs per sweep n, search space for r for i = 1, 2, . . . , ndo Uniformly sample r from the search space Launch Model Training Subroutine (Algorithm 1) with configuration r, µ, returning performance Pi if Pi is the best performance so far on the training set then set best-performing ri = r end for return best-performing ri 5. Evaluation We provide results on a range of image classification, distri- bution shift, and natural language processing tasks, for both finetuning of models pretrained on public data and training from scratch without any additional data. Due to the large scope of our evaluation, we defer all experimental details and full results for all datasets and models to Appendix A. We provide ablations on all steps of our method (B.2). We provide hyperparameter grid search results (B.4). We also provide the code to reproduce our results at this link. Datasets. Image classification: ImageNet (Deng et al., 2009), CIFAR10 (training from scratch and finetuning), CI- FAR100 (Krizhevsky et al., 2009), FashionMNIST (Xiao et al., 2017), STL10 (Coates et al., 2011), EMNIST (Co- hen et al., 2017). Because these image classification datasets are generally considered in-distribution of the pretraining data, we also provide results on a number of distribution shift datasets (Koh et al., 2020) . CI- FAR10 → STL, CIFAR10p1, CIFAR10C, CIFAR100 → CIFAR100C (Hendrycks & Dietterich, 2019), Water- birds (Sagawa et al., 2019), FMoW (Christie et al., 2017), and Camelyon17 (B ´andi et al., 2019). For NLP tasks we consider SQuAD (Rajpurkar et al., 2016) for Ques- tions Answering, text classification tasks from the GLUE benchmark (Wang et al., 2019a): SST-2, QNLI, QQP, MNLI(m/mm) and for next word generation we use Per- 5A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization sonaChat (Zhang et al., 2018a) and WikiText-2 (Merity et al., 2017), and Enron Emails (Klimt & Yang, 2004). 5.1. Effectiveness of the Linear Scaling Rule ImageNet (with Public Data) In Figure 2 we compare the performance of our method on ImageNet against the com- petitive prior works of Mehta et al. (2023b); Berrada et al. (2023). Note that these works do not report the privacy cost of HPO and pretrain their models with JFT, Google’s proprietary internal dataset; as a result the non-private per- formance of their models exceeds ours (rightmost points). Despite this, given sufficient budget (ε >0.5) we match or exceed their performance while accounting for the pri- vacy cost of HPO. The downside of our method is that for sufficiently small ε on sufficiently difficult datasets such as ImageNet, there is no way to keep the privacy cost of HPO small enough to retain enough budget to do a final run, because HP trials with too small a budget do not provide any information. We provide a deep dive into these points of comparison in Appendix A. CIFAR-10 (without Public Data) In Table 2 we compare our method to random search and the grid search baseline, which does not consider the privacy cost of HPO. We sig- nificantly outperform random search, and approach the per- formance of grid search. To the best of our knowledge, we are the first to provide competitive performance when train- ing on CIFAR10 without public data under a strict privacy budget while accounting for the privacy cost of HPO. Table 2.Performance comparison of different methods on CI- FAR10. Our method outperforms prior work in linear probing settings when using a feature extractor pretrained on CIFAR100. In the setting where we do not have public data, we compare to random search and grid search and our method greatly outperforms random search. CIFAR10 Acc with Public Data (ε = 1) Koskela & Kulkarni (2023) 67% Papernot & Steinke (2021) 66% Ours 70.5% CIFAR10 Acc without Public Data (ε = 1) Random Search 44% Grid Search (cost of HPO not incl.) 68% Ours 62.63 % 5.2. Comparison to other Private HPO We provide a detailed comparison to 5 prior works in pri- vate HPO as well as the baselines of random search and grid search, and explain the design choices that enable our method to dominate all prior work. 5.2.1. C OMPARISON TO RENYI HPO We compare to three prior works that use Renyi DP to ana- lyze HPO (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023). In Table 2 we report that our linear scaling is 3.5% better on CIFAR10 at ε = 1 in the experimental setting of Koskela & Kulkarni (2023): linear probing on a ResNet20 check- point pretrained on CIFAR100. Koskela & Kulkarni (2023) achieve 67% on CIFAR10 at ε = 1. In the same setting, the method of Papernot & Steinke (2021) obtains 66%. We apply the linear scaling rule in the same setting, so that only the hyperparameters our method selects are different, and obtain 70.5% at ε = 1 . All methods use the same hyperparameter search space. The reason our method out- performs Koskela & Kulkarni (2023); Papernot & Steinke (2021) is because our prior is better than their random search, which is required by their method, enabling us to simulta- neously allocate a smaller portion of the privacy budget to HPO while still finding better hyperparameters. We also use PLD accounting which is tighter than the RDP accounting their method requires. Neither of these can be fixed; that is, we cannot modify their method to integrate the linear scaling prior or to use PLD accounting. Even with PLD accounting, we would not be able to make up for the gap in accuracy that comes from our adaptive method. An interesting question for future work is whether we can do RDP analysis of our adaptive method. More details in Appendix A.1. 5.2.2. C OMPARISON TO PARAMETER -FREE METHODS . A related area is parameter-free HPO, that builds optimizers that do not require specifying the learning rate as a hyper- parameter. In general it can be challenging to apply these parameter-free methods to DP, because the update rule for the scale of the gradient may not maintain its guarantees in the presence of noise (Li et al., 2023). Comparison to DPAdamWosm. One parameter-free opti- mizer specifically designed for DP is DPAdamWOSM (Mo- hapatra et al., 2021). On ImageNet DPAdamWOSM achieves 79% at ε = 1), which is 8% lower than our method (87% at ε = 1). We do not find that the data-independent learning rate selection works well for ImageNet, and still requires tuning the number of iterations (see Appendix A.1). Comparison to Mehta et al. (2023b) Mehta et al. (2023b) propose an approach where they fix the batch size to full batch, the number of steps to 1, and take a single step of DP-Adam with a very small learning rate. Their approach obtains just 81% at ε ≥ 1 on ImageNet for a model whose non-private accuracy is 88.7%, because they take only a single step. Our method smoothly interpolates between the low-r setting for small ε and the large-r setting for large ε, and outperforms their method across all privacy budgets. 6A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 3.Comparing various HPO methods on CIFAR10 (without public data), CIFAR100, ImageNet, and SQuAD (with public data). Reported numbers are mean over 5 trials. Dataset Random Search Oracle Ours RERR CIFAR10 44 68 62.63 77.63 CIFAR100 84.44 89.62 89.10 84.85 ImageNet 81.2 88.6 86.7 73.97 SQuAD 49.33 82.43 78.08 86.85 5.2.3. C OMPARISON TO BASELINES : RANDOM SEARCH AND GRID SEARCH . Linear scaling significantly outperforms random search. In Table 3 we report the performance for random search, our method, the oracle, and the relative error rate reduction (RERR). Across all datasets, our method significantly out- performs random search. We use the same logarithmic grid for both our method and random search that can be found in Appendix A. We vary this grid and find that the larger the search space, the more our method outperforms random search. 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80T est Accuracy  Accuracy via Linear Scaling Accuracy via Grid Search (a) 0.0 0.2 0.4 0.6 0.8 1.0 10 20 30 40Total Step Size (  × T) T otal step size by Linear Scaling T otal step size by Grid Search (b) Figure 3.Training the beit architecture on CIFAR100, the linear scaling rule produces values for r = η × T close to that of grid search, and the performance drop is only apparent at ε > 0.2 because of the cost of HPO, and vanishingly small for larger ε. Linear Scaling approaches grid search. We validate the effectiveness of linear scaling against the grid search base- line. In Fig. 3 (right) we compare Alg. 2 to the best run across 100 trials from the search space. The privacy cost of grid search is many times higher than that of our method at each value of ε, because we do not account for the pri- vacy cost of grid search to illustrate that even when our method has to account for the privacy cost of HPO and the oracle (grid search) does not, our method is competitive. Our method finds near-optimal hyperparameters with just a fraction of the runtime and privacy cost of grid search. 5.3. Empirical Analysis of Linear Scaling We now consider different architectures and validate our HPO method in the presence of distribution shifts. Full results can be found in Appendix B.1. Architecture Search. In Table 4 we apply our method to Table 4.We compare the best private and best non-private perfor- mances of all models on all datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Model Dataset ε = 1 ε = ∞ Gap beitv2 CIFAR10 98.90 99 .00 0 .10 CIFAR100 89.10 91 .57 2 .47 FMNIST 91.02 91 .53 0 .51 STL10 99.69 99 .81 0 .12 EMNIST 81.77 82 .00 0 .23 convnext CIFAR10 96.75 97 .22 0 .47 CIFAR100 83.47 86 .59 3 .12 FMNIST 90.23 91 .13 0 .9 STL10 99.61 99 .71 0 .10 EMNIST 78.38 79 .05 0 .67 beit CIFAR10 98.19 98 .51 0 .32 CIFAR100 87.1 90 .08 2 .98 FMNIST 90.55 91 .6 1 .05 STL10 99.62 99 .78 0 .16 EMNIST 81.48 83 .25 1 .77 vit-L CIFAR10 98.29 98 .44 0 .40 CIFAR100 86.18 89 .72 3 .54 FMNIST 90.58 91 .37 0 .79 STL10 99.62 99 .76 0 .14 different architectures that can serve as good backbones for high-accuracy DP classification across CIFAR10, CI- FAR100, FMNIST, STL10, and EMNIST. The private-non private utility gap diminishes with model accuracy. One architecture, beitv2, performs the best on all benchmarks and also has the highest non-private zero-shot ImageNet accuracy (Wightman, 2019). We conclude that architecture search can be done without any privacy cost by selecting the model with the best zero-shot performance on a repre- sentative benchmark such as ImageNet. Distribution Shift. A concern in DP fine-tuning is that the pretraining datasets are too similar to the downstream tasks, which can violate privacy (Tram`er et al., 2022). In Table 5 we evaluate the robustness to distribution shift of models trained with our private HPO to non-private models, in the absence of any explicit regularization methods or any infor- mation about the distribution shift. These datasets are con- sidered benchmark tasks for distribution shifts (Kumar et al., 2022b;c; Mehta et al., 2022) and include data that is not in- distribution of the training data, making for a more realistic evaluation of the capabilities of our method to solve chal- lenging tasks. We show that DP-SGD provides robustness to covariate, subpopulation and label distribution shifts for synthetic and natural datasets. Full details in Appendix B.1. On Waterbirds, DP degrades the ID performance but actu- ally improves the OOD performance. On fMoW and Came- lyon17 that are datasets with a significant distribution shift from ImageNet and very different subgroups, DP does not significantly degrade performance and does not exacerbate 7A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 5.Evaluating our DP-HPO method on datasets with distribution shifts at ε = 1. Waterbirds fMoW Camelyon C10 → STL C10 → C10p1 C10 → C10C C100 → C100C ID 92.31 45.44 93.91 98.90 98.90 98.90 89.65 OOD 91.59 35.31 93.55 98.82 97.85 89.98 68.69 Table 6.Linear scaling holds for GLUE tasks when training the full RoBERTa-base model Task ε Acc r = η × T SST-2 0.1 90.60 0.975 0.2 90.83 1.95 0.7 91.06 5.07 QNLI 0.1 82.54 3.9 0.2 84.00 4.68 1.3 86.25 26.52 QQP 0.1 81.07 11.7 0.2 82.21 17.55 1.2 84.69 64.35 MNLI(m/mm) 0.1 77.52(78.24) 11.7 0.2 79.40(79.98) 17.55 1.2 81.86(82.76) 64.35 disparities among subgroups. We also show that we can train models on CIFAR10 with DP and do zero-shot transfer to STL and CIFAR10p1. Finally, we evaluate the robustness of CIFAR-DP-trained models to the common corruptions of CIFAR10C/CIFAR100C, and note that DP training achieves some measure of intrinsic robustness to image corruptions. 5.4. Linear Scaling for language modeling Prior work has generally focused on either CV or NLP because the methods used in DP fine-tuning differ greatly across data modalities (Li et al., 2022b; Mehta et al., 2023a); here we show that our method extends to NLP by validat- ing on text classification and language modeling tasks with LoRA (Hu et al., 2021) and full fine-tuning. We fine-tune GPT-2 (Radford et al., 2019) with our method for three lan- guage modeling tasks that have been benchmarked in prior works (Li et al., 2022b; Shi et al., 2022; Gupta et al., 2022) on private fine-tuning: Persona-Chat (Zhang et al., 2018b), WikiText-2 (Merity et al., 2017) and Enron Emails (Klimt & Yang, 2004). We also fine-tune RoBERTa-base on four tasks in the GLUE benchmark: SST-2, QNLI, QQP and MNLI(m/mm) in Table 6. While prior works mainly fo- cus on ε in {3, 8}, in this work we are also interested in smaller εs like 0.1. Appendix C.2 includes the details for the experimental set-up. Linear scaling succeeds when random search fails. We consider the challenging setting from Malladi et al. (2024) of fine-tuning an OPT-13B model on just 1000 samples from SQuADv2 with DP-SGD-LoRA. Random search runs sometimes do not improve much over zero shot performance, because the search space is so large and the viable set so small. In the initial phases of our method, the trials do not always succeed. Regardless, our method achieves78%±3% close to the oracle 82%, a RERR of 87.5%. Random search performs poorly for NLP tasks because HPO is generally more challenging (Li et al., 2022b). In the rest of the NLP datasets we consider, we compare our performance to prior work that doesn’t consider the privacy cost of HPO. Linear scaling holds for NLP tasks We analyze the per- formance gap between estimated total step size and optimal total step size by grid search to understand how well linear scaling performs on language modeling tasks. Fig. 4 plots the optimal perplexity and perplexity by estimated total step size at different values ofε on Enron emails. We can see that the linear scaling rule generalizes well for reported values of ε and the perplexity by the estimated total step size is close to the optimal perplexity. From Table 6 we can see that our method also works for the GLUE benchmark. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 10 11 12 13 14Perplexity Perplexity by linear rule Perplexity by grid search (a) Pareto Frontier for ε vs Test Perplexity. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 5 10Total step size ( × T)  T otal step size by linear scaling T otal step size by grid search (b) Pareto Frontier for ε vs Total Step Size. Figure 4.The linear scaling rule (accounting for the privacy cost of hyperparameter tuning) is competitive with grid search (non- private, doing N trials each with the given ε) on the Enron Emails dataset. Left: y-axis is Perplexity (lower is better). The linear scaling rule outperforms prior results on dif- ferentially private language modeling tasks.We first run a qualitative evaluation on the previous benchmark SOTA (Li et al., 2022b) on PersonaChat trained with DP-SGD by fol- lowing the linear scaling rule to increase the number of epochs. We can see in Table 7 that we can push the per- plexity under 18 for ε = 3 and ε = 8; this performance is competitive with the non-private baseline. Furthermore, even when pushing for a stricter privacy guarantee ε = 0.5, we can still get perplexity of 21.25, that is better than the result of ε = 8 in (Li et al., 2022b). We also report the results of ablating these hyper-parameters and varying the 8A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 7.Linear scaling holds when fine-tuning all layers of GPT2 on PersonaChat and outperforms Li et al. (2022b) ε (δ = 1 2|Dtrain|) 0.7 3 ∞ Li et al. (2022b) - 24.59 18 .52 Our Work 21.25 - 17.69 Table 8.Finetuning GPT-2 on WikiText-2 (δ = 10−6) and Enron (δ = 1 2|Dtrain|) with DP-SGD. Ppl is perplexity and TSS is Total Step Size. (∗ means estimated). Previously reported best perplexity of GPT-2 on WikiText-2 atε = 3is 28.84 in (Shi et al., 2022). ε 0.1 0 .2 0 .5 1 .4 2 .2 3 .0 WikiText-2 Ppl - 28.81 28.37 28.15 27.98 27 .69 TSS - 0.008 0 .02 0 .04∗ 0.08∗ 0.12∗ ε 0.1 0 .2 0 .7 1 .1 1 .9 2 .7 Enron Ppl 14.35 12.50 11.56 10.91 10.45 10 .14 TSS 0.10 0 .58 2 .02∗ 4.41∗ 9.19∗ 13.98∗ number of layers trained in Appendix C.3. We quantitatively validate the linear scaling rule on WikiText-2 and report the result in Table 8. For WikiText-2, a key observation is that when we compare our results to the best prior reported re- sults in (Shi et al., 2022), for the same number of passes over the training data (20), we obtain lower perplexity for ε = 0.2 than they report for ε = 3. That is, by just increas- ing the effective step size from ∼ 8 × 10−6 to ∼ 8 × 10−3 we can strengthen the privacy guarantee without degrading performance. 5.5. Additional Ablations We can apply our method to methods other than DP- SGD. Tang et al. (2024) recently proposed DP-ZO, a method for DP zeroth-order optimization, that privatizes the zeroth- order update to the model weights in the form of a scalar rather than the first-order gradient update. In Table 9 we find that our method can also optimize HPs for DP-ZO. Although our method fits a 1-d polynomial with 2 points, we can in principle fit any degree-d polynomial with d + 1 points. to approximate the relationship between r and ε. However, because using more points to fit the polynomial imposes more privacy cost for HPO, we use the same num- ber of points and degree throughout all experiments. It is likely that for some datasets, it’s important to tune the Method Mean Accuracy (Std) Random Search 82.53 (1.01) Our Private HPO 83.02 (0.86) Grid Search 83.87 (0.50) Table 9.Our method works beyond DP-SGD. Method Mean Accuracy (Std) Linear (2) 86.69 (0.86) Linear (3) 87.81 (0.86) Quadratic (3) 86.64 (1.08) Table 10.Using more points or a higher order approximation can improve performance. Method GPU Hours Wall-clock time (hours) Random 1 1 Grid 100 1 Ours 7 3 Table 11.Our method trades off runtime for performance with random search and grid search. privacy budget allocated to the smaller trials, the number of points, etc. However, we are not interested in tuning the hyperparameters of our hyperparameter optimization method. In Table 10 we evaluate the linear fit with 2 and 3 points for evaluation, and the quadratic fit with 3 points for evaluation, on ImageNet. Throughout the paper we search for the two main HPs of interest η, Tand fix other HPs such as batch size B and clipping threshold C. However, we can search for these as well. We can incorporate new HPs by updating the decom- position of r so that we optimize for the joint product of the HPs being optimized. We change it from r = η × T to r = η × T × B × C. We evaluate on CIFAR100. The performance of our method when optimizing η, T, B, Cis 87.9 ± 1.9, which is worse than the 89.10 where we opti- mized η, Tand fixed B, C. One limitation of our method is the runtime, shown in Ta- ble 11.. We have worse runtime than random search and worse parallelization than grid search, which is embarrass- ingly parallel while our method requires serial runs. Here the base time for a single HP trial is just 1 hour; this can change based on the task, but these proportions should remain similar. Random search just does 1 run so it has both the lowest GPU hours and wall-clock time. The oracle does a number of runs equal to the granularity of the search space, which here we approximate as 100. In the setting where 100 GPUs are available for the oracle, which may be realistic for large companies but is not realistic for our academic compute setting, these can all be done in parallel, so it uses 100 GPU hours but just 1 hour in wall-clock time. Our method typically does 7 runs: 3 for ε1, 3 for ε2, and 1 for εf , so the total number of GPU hours is 7. The serial dependency of our method requires that εf runs after ε1 9A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization and ε2, but the 3 runs for ε1, ε2 can be parallelized so the wall-clock time is just 3 hours. 6. Related Work and Discussion Related Work. We have performed detailed quantitative and qualitative comparisons to prior private HPO meth- ods (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023). These build on earlier work by Liu & Talwar (2018) that can do HPO by increasing the privacy cost roughly threefold. We improve over these works by significantly reducing the privacy budget required by HPO and adopting a robust prior on our hyperparameter search. Many non-private HPO methods have been used by prior DP papers that do not report the privacy cost of HPO, and a valuable future task would be to consider privatizing these. Multiple prior works (De et al., 2022; Cattan et al., 2022; Bu et al., 2022a;b; Mehta et al., 2023a;b; Berrada et al., 2023; Li et al., 2022b;a; Hu et al., 2021) consider the task of max- imizing the privacy utility tradeoff of finetuning pretrained models. Although the main focus of our paper is private HPO, we also critically evaluate the efficacy of a range of techniques that have been proposed by these works such as data augmentation, fine-tuning the embedding layer, and weight averaging. A detailed discussion of these techniques is deferred to Appendix B.2 and Appendix C. Golatkar et al. (2022); Nasr et al. (2023); Amid et al. (2022) treat< 10% of the private training dataset and public and use it to improve DP-SGD. Although we do not use any private data during pretraining, future work can tackle applying our method to this alternate threat model. Sander et al. (2022) suggest doing HPO with smaller batch sizes and then scaling up the HPs to the full batch update. This idea is similar in spirit to the adaptive scaling that we propose, because the HP trials are cheaper from a runtime perspective than the final run. However, our approach is not only compute-efficient but also accounts for the privacy cost of HPO. Kuo et al. (2023) find that noisy HPO in the federated setting suffers, and suggest doing HPO on public proxy data (whose existence we don’t assume) and transferring it to the private dataset. Wang et al. (2023) propose a method for private adaptive HPO that provides an RDP guarantee for Bayesian HPO. They compare their method to random search under a total privacy budget of ε = 15 , where at each iteration they sample a new set of HPs from their prior, and update their prior, and at each iteration random search samples a new set of HPs uniformly from the search space; each run has a base privacy cost, and it takes many runs for the distribution to converge. Their method can be seen as a version of ours that lacks the linear scaling prior and does not use cheap trials to find the parameters for the prior. As a result, they use much more budget in order to find a good distribution for the HPs. This more flexible approach can be superior to ours in settings where the HPs are not linear. Discussion. DP researchers commonly confront the compute-intensive, privacy-expensive task of doing grid search with hundreds of trials to optimize the privacy-utility tradeoff of DP methods. Our work provides an alternative HPO method that reduces the compute and privacy costs of grid search by an order of magnitude without compromising accuracy across 20 tasks. Researchers using our method can accelerate the pace of research by reducing the compute needed to produce good results, and address the open ques- tion of accounting for the privacy cost of hyperparameter tuning, whether they are doing transfer learning in the pres- ence of domain shifts, training from scratch, or applying PEFT methods to LLMs. 7. Impact Statement This paper presents work whose goal is to improve the qual- ity of models trained with differential privacy. Privacy is at the heart of many ongoing debates about AI. We submit that any work that makes it easier for organizations to train and release models with DP guarantees will positively benefit society. Additionally, this paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communi- cations Security. ACM, oct 2016. doi: 10.1145/2976749. 2978318. Amid, E., Ganesh, A., Mathews, R., Ramaswamy, S., Song, S., Steinke, T., Steinke, T., Suriyakumar, V . M., Thakkar, O., and Thakurta, A. Public data-assisted mirror descent for private model training. In Proceedings of the 39th International Conference on Machine Learning, pp. 517– 535. PMLR, 2022. Bao, H., Dong, L., Piao, S., and Wei, F. Beit: Bert pre- training of image transformers, 2021. URL https:// arxiv.org/abs/2106.08254. Basu, P., Roy, T. S., Naidu, R., Muftuoglu, Z., Singh, S., and Mireshghallah, F. Benchmarking differential privacy and federated learning for bert models, 2022. Berrada, L., De, S., Shen, J. H., Hayes, J., Stanforth, R., Stutz, D., Kohli, P., Smith, S. L., and Balle, B. Unlock- ing accuracy and fairness in differentially private image classification, 2023. 10A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Bu, Z., Mao, J., and Xu, S. Scalable and efficient training of large convolutional neural networks with differential privacy. arXiv preprint arXiv:2205.10683, 2022a. Bu, Z., Wang, Y .-X., Zha, S., and Karypis, G. Differentially private bias-term only fine-tuning of foundation models, 2022b. B´andi, P., Geessink, O., Manson, Q., Van Dijk, M., Balken- hol, M., Hermsen, M., Ehteshami Bejnordi, B., Lee, B., Paeng, K., Zhong, A., Li, Q., Zanjani, F. G., Zinger, S., Fukuta, K., Komura, D., Ovtcharov, V ., Cheng, S., Zeng, S., Thagaard, J., Dahl, A. B., Lin, H., Chen, H., Jacob- sson, L., Hedlund, M., C ¸etin, M., Halıcı, E., Jackson, H., Chen, R., Both, F., Franke, J., K ¨usters-Vandevelde, H., Vreuls, W., Bult, P., van Ginneken, B., van der Laak, J., and Litjens, G. From detection of individual metas- tases to classification of lymph node status at the pa- tient level: The camelyon17 challenge. IEEE Transac- tions on Medical Imaging , 38(2):550–560, 2019. doi: 10.1109/TMI.2018.2867350. Cattan, Y ., Choquette-Choo, C. A., Papernot, N., and Thakurta, A. Fine-tuning with differential privacy neces- sitates an additional hyperparameter search, 2022. URL https://arxiv.org/abs/2210.02156. Christie, G., Fendley, N., Wilson, J., and Mukherjee, R. Functional map of the world, 2017. URL https:// arxiv.org/abs/1711.07846. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011. Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. Emnist: an extension of mnist to handwritten letters, 2017. URL https://arxiv.org/abs/1702.05373. Croce, F., Andriushchenko, M., Sehwag, V ., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P., and Hein, M. Robustbench: a standardized adversarial robustness benchmark, 2021. Cutkosky, A. and Mehta, H. Momentum improves nor- malized SGD. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 2260–2268. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/cutkosky20b.html. De, S., Berrada, L., Hayes, J., Smith, S. L., and Balle, B. Unlocking high-accuracy differentially private im- age classification through scale, 2022. URL https: //arxiv.org/abs/2204.13650. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Diffenderfer, J., Bartoldson, B. R., Chaganti, S., Zhang, J., and Kailkhura, B. A winning hand: Compress- ing deep networks can improve out-of-distribution ro- bustness, 2021. URL https://arxiv.org/abs/ 2106.09129. Dong, J., Roth, A., and Su, W. J. Gaussian differential privacy, 2019. URL https://arxiv.org/abs/ 1905.02383. Dong, J., Roth, A., Su, W. J., et al. Gaussian differential privacy. Journal of the Royal Statistical Society Series B, 84(1):3–37, 2022. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2020. URL https: //arxiv.org/abs/2010.11929. Dwork, C., McSherry, F., Nissim, K., and Smith, A. Cali- brating noise to sensitivity in private data analysis. InPro- ceedings of the Third Conference on Theory of Cryptog- raphy, TCC’06, pp. 265–284, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN 3540327312. Dwork, C., Kohli, N., and Mulligan, D. Differential privacy in practice: Expose your epsilons! Journal of Privacy and Confidentiality, 9, 10 2019. doi: 10.29012/jpc.689. Erlingsson, U., Feldman, V ., Mironov, I., Raghunathan, A., Talwar, K., and Thakurta, A. Amplification by shuffling: From local to central differential privacy via anonymity, 2018. URL https://arxiv.org/abs/ 1811.12469. Fang, H., Li, X., Fan, C., and Li, P. Improved convergence of differential private SGD with gradient clipping. In The Eleventh International Conference on Learning Represen- tations, 2023a. URL https://openreview.net/ forum?id=FRLswckPXQ5. Fang, Y ., Sun, Q., Wang, X., Huang, T., Wang, X., and Cao, Y . Eva-02: A visual representation for neon genesis, 2023b. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained cnns are biased towards texture; increasing shape bias im- proves accuracy and robustness, 2018. URL https: //arxiv.org/abs/1811.12231. 11A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Ghalebikesabi, S., Berrada, L., Gowal, S., Ktena, I., Stan- forth, R., Hayes, J., De, S., Smith, S. L., Wiles, O., and Balle, B. Differentially private diffusion models generate useful synthetic images, 2023. Golatkar, A., Achille, A., Wang, Y .-X., Roth, A., Kearns, M., and Soatto, S. Mixed differential privacy in computer vision, 2022. Gopi, S., Lee, Y . T., and Wutschitz, L. Numerical com- position of differential privacy, 2021. URL https: //arxiv.org/abs/2106.02848. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017. URL https://arxiv.org/abs/ 1706.02677. Gupta, S., Huang, Y ., Zhong, Z., Gao, T., Li, K., and Chen, D. Recovering private text in federated learning of lan- guage models. In Advances in Neural Information Pro- cessing Systems (NeurIPS), 2022. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations, 2019. URL https://arxiv.org/abs/ 1903.12261. Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D. Augment your batch: better training with larger batches, 2019. URL https://arxiv.org/ abs/1901.09335. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. Hulkund, N., Suriyakumar, V . M., Killian, T. W., and Ghas- semi, M. Limits of algorithmic stability for distributional generalization, 2023. URL https://openreview. net/forum?id=PoU_NgCStE5. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A. G. Averaging weights leads to wider op- tima and better generalization, 2018. URL https: //arxiv.org/abs/1803.05407. Kingma, D. P. and Ba, J. Adam: A method for stochastic op- timization, 2014. URL https://arxiv.org/abs/ 1412.6980. Klimt, B. and Yang, Y . The enron corpus: A new dataset for email classification research. In European conference on machine learning, pp. 217–226. Springer, 2004. Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B. A., Haque, I. S., Beery, S., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C., and Liang, P. Wilds: A benchmark of in-the-wild distribu- tion shifts, 2020. URL https://arxiv.org/abs/ 2012.07421. Koskela, A. and Kulkarni, T. Practical differentially private hyperparameter tuning with subsampling, 2023. Krizhevsky, A. et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Kulynych, B., Yang, Y .-Y ., Yu, Y ., Błasiok, J., and Nakkiran, P. What you see is what you get: Principled deep learning via distributional generalization, 2022. Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. Fine-tuning can distort pretrained features and un- derperform out-of-distribution, 2022a. URL https: //arxiv.org/abs/2202.10054. Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. Fine-tuning can distort pretrained features and un- derperform out-of-distribution, 2022b. URL https: //arxiv.org/abs/2202.10054. Kumar, A., Shen, R., Bubeck, S., and Gunasekar, S. How to fine-tune vision models with sgd, 2022c. URL https: //arxiv.org/abs/2211.09359. Kuo, K., Thaker, P., Khodak, M., Nguyen, J., Jiang, D., Talwalkar, A., and Smith, V . On noisy evaluation in federated hyperparameter tuning, 2023. Li, H., Chaudhari, P., Yang, H., Lam, M., Ravichan- dran, A., Bhotika, R., and Soatto, S. Rethinking the hyperparameters for fine-tuning, 2020. URL https: //arxiv.org/abs/2002.11770. Li, T., Zaheer, M., Liu, K. Z., Reddi, S. J., McMahan, H. B., and Smith, V . Differentially private adaptive optimization with delayed preconditioners, 2023. Li, X., Liu, D., Hashimoto, T., Inan, H. A., Kulkarni, J., Lee, Y ., and Thakurta, A. G. When does differentially private learning not suffer in high dimensions? In Advances in Neural Information Processing Systems , pp. 28616– 28630, 2022a. Li, X., Tram`er, F., Liang, P., and Hashimoto, T. Large lan- guage models can be strong differentially private learners. In International Conference on Learning Representations, 2022b. Liu, J. and Talwar, K. Private selection from private candi- dates, 2018. 12A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Liu, Z., Mao, H., Wu, C.-Y ., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s, 2022. URL https://arxiv.org/abs/2201.03545. Malladi, S., Lyu, K., Panigrahi, A., and Arora, S. On the SDEs and scaling rules for adaptive gradient algo- rithms. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=F2mhzjHkQP. Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D., Chen, D., and Arora, S. Fine-tuning language models with just forward passes, 2024. McMahan, H. B., Ramage, D., Talwar, K., and Zhang, L. Learning differentially private recurrent language mod- els, 2017. URL https://arxiv.org/abs/1710. 06963. Mehta, H., Krichene, W., Thakurta, A. G., Kurakin, A., and Cutkosky, A. Differentially private image classifica- tion from features. Transactions on Machine Learning Research, 2023a. ISSN 2835-8856. URL https:// openreview.net/forum?id=Cj6pLclmwT. Ex- pert Certification. Mehta, H., Thakurta, A. G., Kurakin, A., and Cutkosky, A. Towards large scale transfer learning for differentially pri- vate image classification.Transactions on Machine Learn- ing Research, 2023b. ISSN 2835-8856. URL https:// openreview.net/forum?id=Uu8WwCFpQv. Ex- pert Certification. Mehta, R., Albiero, V ., Chen, L., Evtimov, I., Glaser, T., Li, Z., and Hassner, T. You only need a good embeddings extractor to fix spurious correlations, 2022. URLhttps: //arxiv.org/abs/2212.06254. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Mohapatra, S., Sasy, S., He, X., Kamath, G., and Thakkar, O. The role of adaptive optimizers for honest private hyperparameter selection, 2021. Nasr, M., Mahloujifar, S., Tang, X., Mittal, P., and Houmansadr, A. Effectively using public data in pri- vacy preserving machine learning. In Proceedings of the 40th International Conference on Machine Learning, pp. 25718–25732. PMLR, 2023. Panda, A., Mahloujifar, S., Bhagoji, A. N., Chakraborty, S., and Mittal, P. Sparsefed: Mitigating model poisoning attacks in federated learning with sparsification, 2021. URL https://arxiv.org/abs/2112.06274. Papernot, N. and Steinke, T. Hyperparameter tuning with renyi differential privacy, 2021. URL https: //arxiv.org/abs/2110.03620. Peng, Z., Dong, L., Bao, H., Ye, Q., and Wei, F. Beit v2: Masked image modeling with vector-quantized visual tokenizers, 2022. URL https://arxiv.org/abs/ 2208.06366. Polyak, B. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. Siam Journal on Control and Optimization, 30:838–855, 1992. Qian, N. On the momentum term in gradient descent learn- ing algorithms. Neural networks, 12(1):145–151, 1999. Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A. Micro-batch training with batch-channel normalization and weight standardization, 2019. URL https:// arxiv.org/abs/1903.10520. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Rahimian, H. and Mehrotra, S. Frameworks and results in distributionally robust optimization. Open Journal of Mathematical Optimization, 3:1–85, jul 2022. doi: 10. 5802/ojmo.15. URL https://doi.org/10.5802% 2Fojmo.15. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383– 2392, 2016. Ryu, E. K. and Boyd, S. P. A primer on monotone operator methods. 2015. Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case gen- eralization, 2019. URL https://arxiv.org/abs/ 1911.08731. Sander, T., Stock, P., and Sablayrolles, A. Tan without a burn: Scaling laws of dp-sgd, 2022. URL https: //arxiv.org/abs/2210.03403. Shejwalkar, V ., Ganesh, A., Mathews, R., Thakkar, O., and Thakurta, A. Recycling scraps: Improving private learn- ing by leveraging intermediate checkpoints, 2022. URL https://arxiv.org/abs/2210.01864. Shi, W., Chen, S., Zhang, C., Jia, R., and Yu, Z. Just fine- tune twice: Selective differential privacy for large lan- guage models. arXiv preprint arXiv:2204.07667, 2022. 13A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):60, Jul 2019. ISSN 2196-1115. doi: 10.1186/ s40537-019-0197-0. URL https://doi.org/10. 1186/s40537-019-0197-0 . Song, S., Chaudhuri, K., and Sarwate, A. D. Stochastic gradient descent with differentially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pp. 245–248, 2013. doi: 10.1109/GlobalSIP. 2013.6736861. Sun, Z., Suresh, A. T., and Menon, A. K. The importance of feature preprocessing for differentially private linear optimization. In International Conference on Learning Representations, 2024. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learn- ing. In Dasgupta, S. and McAllester, D. (eds.), Proceed- ings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learn- ing Research, pp. 1139–1147, Atlanta, Georgia, USA, 17– 19 Jun 2013. PMLR. URL https://proceedings. mlr.press/v28/sutskever13.html. Tang, X., Panda, A., Sehwag, V ., and Mittal, P. Differen- tially private image classification by learning priors from random processes. In Advances in Neural Information Processing Systems, 2023. Tang, X., Panda, A., Nasr, M., Mahloujifar, S., and Mittal, P. Private fine-tuning of large language models with zeroth- order optimization, 2024. Team, A. Learning with privacy at scale, 2017. URL https://docs-assets.developer. apple.com/ml-research/papers/ learning-with-privacy-at-scale.pdf . Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transform- ers; distillation through attention. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Confer- ence on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10347–10357. PMLR, 18–24 Jul 2021. URLhttps://proceedings.mlr. press/v139/touvron21a.html. Tram`er, F., Kamath, G., and Carlini, N. Considerations for differentially private learning with large-scale public pretraining, 2022. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019a. URL https://openreview.net/forum? id=rJ4km2R5t7. Wang, H., Gao, S., Zhang, H., Su, W. J., and Shen, M. DP- hyPO: An adaptive private framework for hyperparameter optimization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=3Py8A1j5N3. Wang, Y .-X., Balle, B., and Kasiviswanathan, S. P. Sub- sampled renyi differential privacy and analytical mo- ments accountant. In Chaudhuri, K. and Sugiyama, M. (eds.), Proceedings of the Twenty-Second Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 89 of Proceedings of Machine Learning Research, pp. 1226–1235. PMLR, 16–18 Apr 2019b. URL https://proceedings.mlr.press/v89/ wang19b.html. Wightman, R. Pytorch image models. https://github. com/rwightman/pytorch-image-models, 2019. Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y ., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Huggingface’s transformers: State-of-the-art natural language processing, 2019. URL https://arxiv.org/abs/1910.03771. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. URL https://arxiv.org/abs/ 1708.07747. Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K., Malek, M., Nguyen, J., Ghosh, S., Bharadwaj, A., Zhao, J., Cormode, G., and Mironov, I. Opacus: User- friendly differential privacy library in pytorch, 2021. URL https://arxiv.org/abs/2109.12298. Yu, D., Zhang, H., Chen, W., Yin, J., and Liu, T.-Y . Large scale private learning via low-rank reparametrization, 2021. Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. Personalizing dialogue agents: I have a dog, do you have pets too?, 2018a. Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. Personalizing dialogue agents: I have a dog, do you have pets too?, 2018b. URL https: //arxiv.org/abs/1801.07243. Zhu, Y . and Wang, Y .-X. Poission subsampled r´enyi dif- ferential privacy. In Proceedings of the 36th Interna- tional Conference on Machine Learning, pp. 7634–7642. PMLR, 2019. 14A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization A. Further Results for Computer Vision Tasks Our code is available at the following URL: https://anonymous.4open.science/r/dp-custom-32B9/README.md A.1. Experimental Set-up Hyperparameter Search Space. We use a logarithmic grid for the learning rate η ∈ [10−7, 10−4]. We use the same grid for the CIFAR training from scratch experiments, and the NLP experiments. We scale the learning rate by the batch size (the original linear scaling rule). The number of epochs depends on the maximum number of iterations that we can do in the provided time. ImageNet details. The architecture is a modernized ViT (Fang et al., 2023b) pretrained on IN-21k Deng et al. (2009) with CLIP. We use a resource-efficient finetuning approach where we create a linear layer aggregating the intermediate representations from each Transformer block, following Tang et al. (2023). We apply the method from Sun et al. (2024) to preprocess the features, allocating a budget of ε = 0.05 for the private mean estimation. For the HPO, we do 3 runs at ε = 0.1, followed by 3 runs at ε = 0.2 and a final run at ε = 0.88, which produces a cumulative privacy cost including HPO of ε = 1.0. Here we express the privacy values in terms of ε for brevity, but the actual expressions are in terms of µ, the parameter for f-DP. Because of the nature of composition, the hyperparameter search only costs us the difference in performance between ε = 0.88 and ε = 1.0, which is minimal. We search across values of T ranging from 1 (a single epoch) to 20 (the most we can do in the maximum amount of time a job will run on our cluster). (Berrada et al., 2023) report that fine-tuning the full architecture produces better results than linear probing. However, we lack the computational resources to do full fine-tuning of large transformers, but we can do linear probing of the extracted features in under an hour on a single A100. CIFAR training from scratch. We use the model from Tang et al. (2023), a WideResNet-16-4, and train only the last layer on the extracted features from previous layers. The model is pretrained on synthetic data that does not resemble real-world data. We choose this model because it is the SOTA model for CIFAR training from scratch, and we want to validate that our private HPO can produce competitive results in a setting where the zero-shot performance is poor (indeed, the zero-shot performance of this model is just random chance, because it has never seen any real images before) but the ceiling for performance is quite high. Comparison to DPAdamWOSM details. We implement DPAdamWOSM (Mohapatra et al., 2021) to the best of our ability in wosm impl.py since there is no code available, and tune the necessary hyperparameter T (# of epochs) between 1 and 200 and report the performance for the best value of T without accounting for the privacy cost of this tuning. The rest of the hyperparameter choices and model architecture mirror our own. At a high level, our linear scaling rule attempts to do a data-dependent learning rate selection, while DPAdamWOSM does a data-independent learning rate selection. It is natural that for hard tasks (ImageNet) the data-independent choice may not work well. We note that while DPAdamWOSM does not require tuning the learning rate, we still need to tune the number of epochs. Therefore, even if further tuning for DPAdamWOSM could match the utility of the linear scaling rule, it would not match the privacy guarantee. Ultimately we think these works are compatible, because we can use our HPO to tune the number of epochs in DPAdamWOSM. Comparison to Koskela & Kulkarni (2023) details. . We implement the ability to train on a subset of ImageNet in our codebase by passing the flags start idx, end idx. As an initial test, we tried doing HPO on half the dataset by passing start idx=0, end idx=625. Our code will produce a random permutation of the chunks of ImageNet (1251 total) and then load the first half. We compare η = 0.01, η= 1.0 on this half-dataset. On the full dataset, these produce very different performance; η = 0.01 achieves 81% at ε = 1.0, while η = 1.0 achieves 87%. However, on the half dataset, the first learning rate achieves 43.2% performance, while the second achieves 41.4%. Inspecting the loss curves, we find that both learning rates overfit the training dataset and do not generalize to the validation set, but the second learning rate overfits more. We then try training two models on disjoint sets of the dataset and combining them via parallel composition. This achieves 83%, which is worse than training on the entire dataset. This may be an interesting direction for future work. We tried a number of other strategies to try and scale the idea of Koskela & Kulkarni (2023) to ImageNet scale, such as weight decay, smaller learning rate, single-epoch training, etc. but were unable to produce a recipe where performance on the half-dataset was consistently positively correlated with performance on the full dataset. We suspect that there is a factor of the number of classes that is needed to properly calibrate the subsampling. 15A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 12.Linear Scaling on ImageNet is competitive with (Mehta et al., 2023b) and (Mehta et al., 2023a) ε (Mehta et al., 2023b) (Mehta et al., 2023a) Ours r = η × T 0.25 75.6 - 79.0 250 0.50 79.4 86.1 81.6 750 1.00 81.1 86.8 83.2 1100 2.00 81.5 87.4 84.2 2000 10.0 81.7 - 85.4 2000 ∞ 86.9 88.9 85.7 2000 Models. We evaluate five models: two masked-image modeling transformers, beit (Bao et al., 2021) and beitv2 (Peng et al., 2022), their backbone architecture ViT (Dosovitskiy et al., 2020) at both the base and large scales, and the pure convolutional architecture convnext (Liu et al., 2022). All models are pretrained on ImageNet-21k (Deng et al., 2009). These models span a range of input resolutions: beitv2 (224x224), convnext, vit-base, vit-large (384x384), and beit (512x512) and we upsample images to the necessary input size. For text generation we use GPT-2 (Radford et al., 2019) at the smallest scale, and RoBERTa-base. Availability. Our results tune open source models from the PyTorch timm package (Wightman, 2019) using existing privacy accounting from (Gopi et al., 2021) and per-sample clipping code in (Yousefpour et al., 2021), and can be reproduced in minutes. B. Further ImageNet Results. We perform additional experiments on ImageNet with the same architecture as prior work to better understand the tradeoffs of our method. We use a ViT-g that was pretrained on laion-2b, to compare to the ViT-g models in prior work that were pretrained on JFT-4b. It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work (Mehta et al., 2023a). We find that use of our new rule can unlock significant improvements for a range of ε when we hold both approaches accountable for the privacy cost of hyperparameter tuning. We apply linear scaling to the ViT model used in (Mehta et al., 2023a) on CIFAR100. Although (Mehta et al., 2023a) do not directly state the hyperparameters for their best results, they specify that they use 200 hyperparameter trials with Bayesian optimization. While they obtain RDP guarantees, these guarantees do not include the privacy cost of non-privately tuning hyperparameters. We apply the linear scaling rule to extrapolate a value ofr from ε = 0.1 to ε = 1, obtaining r = 20 = η(0.2) × T(100). We recover performance of82.7% for ε = 1, a 2% improvement over the best result for DP-Adam in (Mehta et al., 2023a) while accounting for the privacy cost of hyperparameter tuning. They obtain their best result for DP-Adam at T = 10, but we cannot compute the corresponding value of r because they do not provide η. However, because they use a clipping norm of 0.005 we can reasonably infer that their value of r is ≈ 1000× smaller than ours. This is farther from the optimal non-private training, as evidenced by the performance gap. Linear Scaling scales to ImageNet In Table 12 we do a granular comparison between our method and (Mehta et al., 2023b;a). We observe that our method is competitive with (Mehta et al., 2023a) even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of r = η × T. The non-private accuracy of their closed-source model is 3.2% higher than our open-source model, and so the private accuracy at ε = 2 is also 3.2% higher. However, ultimately our method and the method of Mehta et al. (2023a) are complementary, because their method introduces new hyperparameters that we intuit our linear scaling rule can optimize. We attempted to validate this intuition empirically but were unable to reproduce the results of Mehta et al. (2023a) because they and Mehta et al. (2023b) pretrain on the closed- source JFT dataset with billions of images. We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in (Mehta et al., 2023a), but for sufficiently small values of ε on harder datasets the second-order methods they propose provide better performance. We note that the method in Mehta et al. (2023a) only works for vision tasks, whereas our approach works for both vision and language tasks. 16A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization ε1 ε2 εf Acc Std - - 1.0 99.00 0.01 0.01 0.05 0.99 98.88 0.01 0.05 0.1 0.96 98.85 0.03 0.05 0.2 0.9 98.81 0.01 0.1 0.2 0.88 98.81 0.01 0.2 0.3 0.7 98.79 0.03 Table 13.The marginal cost of our HPO method is low. The first row represents the oracle. The dataset is CIFAR10. The marginal cost of linear scaling is low. Table 13 shows that the marginal cost of our HPO method is low. In the case of CIFAR10, this is because the oracle at ε = 0.1 achieves > 98% accuracy. Linear Scaling produces robust results. In Fig. 3 we report that following Algorithm 2 produces new state-of-the-art results for all values of ε, shown in Table 7. In Appendix A.1 we provide detailed computations of the linear interpolation for multiple datasets and in Appendix B.4 we provide full results across the entire hyperparameter search space. 5 10 20 30 40 50 60 70 80 90 100 epochs 0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 lr  = 0.05 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 Figure 5.Heatmaps for beit on CIFAR100. ε increases from 0.05 → 1.0 left to right on the grid-axis, iterationsT increases from 5 → 100 left to right on the individual plot axis, and the learning rate η increases from 0.05 ↓ 1 top to bottom on the individual plot axis. As ε increases, left to right, the optimal value of η × T increases in accordance with the new linear scaling rule. Prior work has generally operated in the top-left regime, that is often suboptimal. Decomposing r into η, T One of the advantages of our search method is that we combine the parameters that we need to search into one meta-parameter, the radius r, which allows us to perform linear interpolation and also allows us to improve the runtime of the intermediate HP trials. We uniformly sample r from the search space defined by rmin = ηmin ×Tmin, (rmax = ηmax ×Tmax. We evaluate 3 methods for decomposing r. 1) We decompose r by sampling η, Tfrom their search spaces until their product is close to the target r. 2) We sample T uniformly, then get η = r/T . 3) We sample η, Tuniformly from their search spaces. We don’t observe any significant difference between these methods. Note that the product of uniform distributions is not uniform. The robustness of the rule that “combinations of η, Tthat evaluate to the same product perform similarly” is crucial to the success of our method, because it enables us to fit a line rather than a more complex function that might require more evaluations. In Figure 6 and Figure 5 our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations T by a constant factor and decreasing the learning rate η by the same factor (or vice versa). We find that any inaccuracy incurred by estimating the best value of r with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of r, but does reduce the privacy cost of hyperparameter tuning immensely. Method to Reduce ε ε = 0.7 Degradation from ε = 1.0 Subsampling 41.34% 45.66% Increasing Noise 84.07% 2.93% Table 14.Comparing methods to reduce privacy cost (ε) on ImageNet. Increasing noise is more effective than subsampling for reducing ε with minimal performance degradation. 17A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 20 40 60 80 100 R = × T 0 1 2 3 4 5T est Accuracy Gap(Relative)    R vs TestAccuracy =[0.1,0.2,0.3,0.4,0.5] =0.05 =0.10 =0.20 =0.30 =0.40 =0.50 Figure 6.A scatter plot of r = η × T the total step size vs the relative gap in test accuracy on CIFAR100 Beitv2; this gap is measured as the difference between the test accuracy at the plotted value of r and the optimal value of r. Optimizing r for any value of ε and transferring this, e.g. via the linear scaling rule, will not reduce accuracy by much compared to the optimal hyperparameters. Figure 7.We compare the best private and best non-private performances of all models on all datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Model Dataset ε = 1 ε = ∞ Gap beitv2 CIFAR10 98.90 99 .00 0 .10 CIFAR100 89.10 91 .57 2 .47 FMNIST 91.02 91 .53 0 .51 STL10 99.69 99 .81 0 .12 EMNIST 81.77 82 .00 0 .23 convnext CIFAR10 96.75 97 .22 0 .47 CIFAR100 83.47 86 .59 3 .12 FMNIST 90.23 91 .13 0 .9 STL10 99.61 99 .71 0 .10 EMNIST 78.38 79 .05 0 .67 beit CIFAR10 98.19 98 .51 0 .32 CIFAR100 87.1 90 .08 2 .98 FMNIST 90.55 91 .6 1 .05 STL10 99.62 99 .78 0 .16 EMNIST 81.48 83 .25 1 .77 vit-L CIFAR10 98.29 98 .44 0 .40 CIFAR100 86.18 89 .72 3 .54 FMNIST 90.58 91 .37 0 .79 STL10 99.62 99 .76 0 .14 B.1. Linear Scaling enables empirical analysis Many interesting questions in DP fine-tuning remain unanswered because of the immense computational overhead of evaluating hundreds of hyperparameter trials for each privacy budget, model architecture and dataset (Mehta et al., 2023a). We now employ the linear scaling rule to efficiently answer key questions in DP fine-tuning for vision tasks. Impact of model architectures on differential privacy Many pretrained model architectures are available (Wolf et al., 2019) but prior work has generally engaged with a single architecture, e.g. beit (Bu et al., 2022a) or ViT (Mehta et al., 2023b). We leverage our method to answer three questions: • What model architectures can provide good DP classifiers? • Is the best model task-specific, e.g., is an architecture search required? • Does the private-non private utility gap depend on the model architecture? We report our findings in Tab. 7. We evaluate multiple transformer architectures in ViT (Dosovitskiy et al., 2020), beitv1 (Bao et al., 2021) and beitv2 (Peng et al., 2022), as well as the purely convolutional architecture Convnext (Liu et al., 2022). We 18A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Figure 8.In-distribution (ID) and out-of-distribution (OOD) performance on benchmark distribution shift datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Dataset ε = 1.0 ID(OOD) Prior ( ε = ∞) Waterbirds 92.31 (91.59) 98.3(80.4) fMoW 45.44 (35.31) 49.1 (36.6) Camelyon 93.91 (93.55) 99.5 (96.5) C10 → STL 99.0 (98.82) 97.5 (90.7) C10 → C10p1 99.0 (97.85) 97.5 (93.5) C10 → C10C 99.0 (89.98) 96.56 (92.78) C100 → C100C 89.65 (68.69) 81.16 (72.06) find that all architectures can serve as good backbones for high-accuracy DP classification. This is somewhat surprising because the different inductive biases of transformers and purely convolutional architectures tend to produce differently structured features, but we reason that the noise added by DP will ‘smooth out’ these decision boundaries regardless of architecture. We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy (Wightman, 2019). We therefore recommend that practitioners do not worry about architecture search when fine-tuning as this can incur further privacy costs, and instead pick the best model available. We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time lossless privacy of 99.0% on CIFAR10 at ε = 1 (without considering the cost of HPO) and the gap is only < 0.10% if we consider the cost of HPO. We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning. We harness these insights for our next analyses. DP models are robust to distribution shifts. If we assume the existence of some publicly available data for pretraining and then do DP fine-tuning on the private data, it’s crucial that there is no privacy leakage between the public data and private data. There is only 0 distribution shift when public = private, and this violates the key assumption (no privacy leakage because public and private data are sufficiently different) in DP fine-tuning. If the public data is so different from the private data that it can be used for pretraining without privacy leakage, there must be some distribution shift. Benchmarking performance on datasets with distribution shifts is increasingly important because real-world problems almost always contain distribution shift between model training and inference (Rahimian & Mehrotra, 2022). We show that DP-SGD provides robustness to covariate, subpopulation and label distribution shifts for synthetic and natural datasets. We compare to other methods that consider this question. Details on OOD Experiments We specify exact details for all OOD experiments. Our training details are drawn from prior work (Kumar et al., 2022c;b; Diffenderfer et al., 2021). Waterbirds: the ID→OOD contains a well-studied spurious correlation in the binary classification problem. (Mehta et al., 2022) evaluate vision transformers without using group knowledge and obtain ≈ 80 % ID accuracy, but much worse (≈ 60%) OOD accuracy, and (Kumar et al., 2022c) tailor their method to this task and get the reported results. Surprisingly, just fine-tuning a linear model on the extracted features outperforms both works for OOD accuracy for ε = 0.1. This trend (sacrificing ID accuracy for increased OOD robustness) is seen in other OOD results, and we hypothesize that this is due to the inherent regularization present in DP-SGD. Fmow: we train on region 3 (ID) and evaluate on regions 1,2 (OOD), following (Kumar et al., 2022b). Camelyon17: we again follow (Kumar et al., 2022b). CIFAR10 → STL10, CIFAR10p1: We train privately on CIFAR10 using our best hyperparameters returned from the linear scaling rule and then transfer this to STL10/CIFAR10p1, with the label reassignment following (Kumar et al., 2022c). Common Corruptions: We evaluate on the average severity of the ’gaussian blur’ corruption. We leverage our method to answer three questions: • Can DP help when there is a domain shift from private fine-tuning to test? 19A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization • Can DP help when there is a domain shift from public data to private fine-tuning? • Can DP fine-tuned models perform well in the zero-shot setting? In Table 8 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is yes. The Waterbirds dataset is a well-known benchmark for evaluating the robustness of models to spurious correlations. There is a domain shift between the private training data and the private test data created by class imbalance. We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually improves performance on the OOD split. We hypothesize that the lackluster OOD non-private performance is caused by the model overfitting to the spurious correlation in the training data, and that the inherent regularization of DP prevents the model from memorizing this spurious correlation. By comparing our results to Mehta et al. (2022) we determine that this robustness is unique to DP rather than an artifact of the pretrained model. Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test. Because our central assumption in DP fine-tuning is that there is no privacy leakage from the pretraining data to the private training data, it is important to understand how DP fine-tuning performs when there is a distribution shift between public data and private data. fMoW (Christie et al., 2017) and Camelyon17 (B ´andi et al., 2019) are two datasets that represent a signficant distribution from the pretraining data (ImageNet). We observe a similar relationship between ID and OOD degradation as above, where the OOD degradation is somewhat mitigated by DP. If we compare our results on Camelyon to the best results in Ghalebikesabi et al. (2023) we find that we can improve their best performance from 91.1% at ε = 10 to 93.91% at ε = 1. Although performance on fMoW remains quite poor, we note that it is not significantly worse than in the non-private setting. We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data. We finally consider the zero-shot setting, where we fine-tune a model on CIFAR and then transfer it without updating any parameters to private test datasets that once again represent a distribution shift from CIFAR. We report the performance in the OOD column. For the more minute distribution shifts of STL and CIFAR10p1 we find that the fine-tuned classifier can achieve remarkable performance without ever updating parameters on these datasets; that is, we just remap the labels as per (Kumar et al., 2022b). CIFAR10C and CIFAR100C represent larger distribution shifts and are used to benchmark the robustness of models to commonly reported image corruptions (Hendrycks & Dietterich, 2019). Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a> 20% degradation. Although this is lower than the top result on the RobustBench leaderboard (Croce et al., 2021) obtains 85% accuracy, we note that once again we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification. Comparison to other works on distribution shift under DP. Prior work in distributionally robust optimization (DRO) has addressed this problem by using knowledge of the relative imbalances between groups, but recent work with vision transformers has shown that linear probing can perform well on datasets with distribution shifts (Mehta et al., 2022; Kumar et al., 2022a;c). Kulynych et al. (2022) proposes DP-IS-SGD that improves the robustness of DP-SGD by removing per-sample gradient clipping (therefore removing the introduced bias but also losing the privacy guarantee; see 4.2) and uses knowledge of the groups to sample subpopulations at different rates to improve robustness. Because our method uses DP-GD to maximize the signal-to-noise ratio of updates and requires clipping (because our primary goal is the privacy guarantee, unlike Kulynych et al. (2022) which focuses on DRO) and we do not assume knowledge of groups, we cannot make use of DP-IS-SGD. Hulkund et al. (2023) concludes that ”[DP-SGD] is not a good candidate for improving robustness under covariate or subpopulation shift, as it comes at a major cost to accuracy.” This conclusion runs counter to our findings, and we believe the reason is because their numerical findings are not conclusive. Our interpretation of their results is that because their DP-SGD degrades accuracy, it should also increase robustness; however we find that even when DP-SGD does not degrade accuracy it still improves robustness. B.2. Detailed Ablations In this subsection we deal with detailed ablations of each step in the method that we use. We ablate each step and show their individual benefits in Table 15. At a high level, we want to maximize the signal-to-noise ratio of updates, accelerate training to minimize the impact of noise on the optimization trajectory, and apply the linear scaling rule to select the best 20A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 15.Our method fixes six design choices: the architecture and initialization (for CV tasks only), the batch size (full batch), the optimizer (SGD with momentum=0.9), the accounting method (PLV where all prior HPO methods use RDP), and the clipping norm (unit clipping). We report the improvement derived from following each of these techniques with respect to a competitive baseline from prior work on CIFAR100 at ε = 0.1. Method Baseline Baseline Accuracy Improvement Classifier (no bias) (Mehta et al., 2023b) 71.3 0 .36 Zero Initialization Random Initialization (De et al., 2022) 64.85 6 .81 Gradient Descent SGD(Batch=4096) (De et al., 2022) 70.2 1 .46 Momentum (ρ = 0.9) ρ = 0 (Bu et al., 2022a) 69.02 2 .09 PLV Accounting RDP (De et al., 2022) 68.43 3 .23 Unit Clipping (C = 1) C ≪ 1 (Mehta et al., 2023a) 71.2 0 .46 hyperparameters while maintaining a given overall privacy budget. 1) Extract features from a private dataset using an open source feature extractor pretrained on a public dataset. A valid criticism of this approach in private fine-tuning is that the fine-tuning dataset can be in-distribution with the training dataset, and this may violate privacy. To address this we evaluate our method on eight datasets that have been used as distribution shift benchmarks in Sec. 5. 2) Zero-initialize a linear classifier that maps features to classes. Prior work has studied full network fine-tuning (Cattan et al., 2022; Bu et al., 2022a; De et al., 2022) but we find that by doing logistic regression on a linear classifier we minimize the number of parameters, and mitigate the curse of dimensionality. We further simplify the choice of initialization by initializing all parameters to zero. 3) Apply linear scaling to privately select the step size and number of steps. We propose a new linear scaling rule: increase either the step size η or number of steps T so that the total step size r = η × T is linear in ε. This reduces the hyperparameter search to a binary search in r. Furthermore we can do a hyperparameter search for r using a small privacy budget, and then linearly scale up this value to minimize the cost of hyperparameter search(Alg. 2). Using privacy loss accounting enables us to get competitive accuracy for privacy budgets as small as ε = 0.01, so these low-cost trials can inform better hyperparameters. our method already minimizes the private-nonprivate performance gap atε = 1.0 as we show in Table 7, so spending ε = 0.1 for hyperparameter tuning does not significantly degrade accuracy. Unless stated explicitly otherwise, all privacy-utility tradeoffs reported for our method in the main body include the privacy cost of hyperparameter tuning via the linear scaling rule. 4) Compute the full batch gradient. This optimizes the signal-to-noise ratio of the update and enables use of large step sizes (Goyal et al., 2017). We achieve 91.52% accuracy on CIFAR10 (|D| = 5e4) for ε = 0.01 when training for 100 epochs with noise multiplier σ = 2561. When the noise is divided by the batch size, the effective noise multiplier is σ |B|=5e4 ≈ 0.05 and the SNR is 1 0.05 = 20. When we use subsampling with sampling probability p = 0.2 and train for the same number of epochs under the same privacy budget, our effective noise multiplier is σ |B| = 1145 1e4 = 0.114, and the corresponding SNR of 1 0.114 = 8.7 is much worse than in the full batch setting. 5) Clip per-sample gradients to unit norm. As per Eq. 1 reducing the per-sample gradient below 1 is equivalent to reducing η (and thus reducing the step size) while simultaneously biasing optimization. By setting c = 1 we can simplify r = η × T × c to r = η × T. 6) Use privacy loss variable accounting. Gopi et al. (2021) provides a tool to calibrate Gaussian noise for the given privacy budget and add noise to the gradient: this enables budgeting for small values of ε without underestimating privacy expenditure. 7) Use momentum. Acceleration has a host of well-known benefits for optimization and is ubiquitous in non-private optimization (Qian, 1999; Kingma & Ba, 2014), but prior work has not always used momentum because it can lead DP-SGD astray when the SNR of updates is low (De et al., 2022). Because we optimize the SNR of individual updates in (4), we can make use of momentum. 21A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Acc Momentum Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 = 0 = 0.9 (a) Momentum Acceleration 40 42 44 46 48 50 Epochs 65 66 67 68 69 70 71 72 73T est Acc Momentum Post-Processing.   Dataset=CIFAR100, Arch=beit, =0.1 T est Accuracy Free Step (b) Momentum Post-Processing Figure 9.Ablation of momentum parameter during training (left) and post processing of the parameter exponential moving average stored in the momentum buffer to take an extra step ’for free’ (right). Use of both methods increases performance slightly. Momentum Accelerates Convergence. Despite the exhaustive study of the acceleration of gradient descent with momen- tum done by prior work (Sutskever et al., 2013; Qian, 1999) work on DP-SGD generally eschews the use of a momentum term. A notable exception (Mehta et al., 2023a) use AdamW rather than SGD with momentum; in a later section we discuss the reason to prefer SGD with momentum. The reason to use momentum to accelerated the convergence of DP-SGD is straightforward: the exponentially moving average of noisy gradients will have higher SNR than individual gradients. Furthermore, momentum is shown to provably benefit normalized SGD (Cutkosky & Mehta, 2020). In Fig. 9 we observe that momentum complements our new linear scaling rule and accelerates convergence. Separately, we report the improvement of taking a step ’for free’ in the direction of the exponential moving average stored during training in the momentum buffer. Note that this exponential moving average is in no way tied to momentum, and it is equivalent to perform DP-SGD without acceleration, store an exponential moving average of gradients with decay parameter γ = 0.9, and then take an additional step in the direction of the stored gradient average after training has finished; we only use the momentum buffer for ease of implementation. As we discuss above when introducing the new linear scaling rule, we maximize performance by maximizing SNR and terminating training while the model is still improving. Intuitively we therefore expect that the momentum buffer will contain a good estimate of the direction of the next step that we would have taken had we continued training, and taking a step in this direction with our usual learning rate should only improve performance without any privacy loss. We use momentum with ρ = 0.9 in all other experiments and also take a ’free step’ at the end of private training. 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Acc Batch Size Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 10e3 Full Batch 1e3 5e3 (a) Batch Size 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Accuracy Batch Size Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 SGD GD (b) Gradient Descent vs SGD Figure 10.Ablation of batch size. Left: We vary the batch size using the learning rate and number of iterations tuned for full batch; all other batch sizes perform much worse. Right: We compare SGD and GD. For SGD we tune the batch size jointly with learning rate and number of iterations, arriving at a batch size of 4096 and plot the best performing run against full batch. 22A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Full Batches Optimize Signal-to-Noise Ratio. Since its inception, the use of privacy amplification via Poisson subsam- pling and RDP has been a mainstay in the DP community (Zhu & Wang, 2019; Wang et al., 2019b; Erlingsson et al., 2018). Prior work almost universally uses privacy amplification via subsampling, but as early as (McMahan et al., 2017), and more recently in (De et al., 2022) it has become apparent that DP-SGD can actually benefit from large batch sizes because the signal-to-noise ratio (SNR) improves. Note that the noise term in 1 is divided by the batch size, so if we are willing to give up amplification via subsampling entirely, we can reduce the noise by a factor of5e4 for the benchmark computer vision tasks. In Fig. 10 we report the improvement of full-batch DP-GD over Poisson subsampled DP-SGD. We attribute the success of DP-GD to the improvement in SNR. For example, we achieve 91.52% accuracy on CIFAR10 for ε = 0.01 when training for 100 epochs with learning rate η = 0.01 and noise multiplier σ = 2561. When the noise is divided by the batch size, the effective noise multiplier is σ |B|=5e4 = 0.05 and the SNR is 1 0.051 = 20. When we use subsampling with sampling probability p = 0.2 and train for the same number of epochs under the same privacy budget, our effective noise multiplier is σ |B| = 1145 1e4 = 0.114, and the corresponding SNR of 1 0.114 = 8.7 is much worse than in the full batch setting. Although at first glance our analysis merely supports the typical conclusion that large batches are better in DP-SGD, (De et al., 2022) observe that DP-SGD is still preferrable to DP-GD because minibatching produces the optimal choice of noise multiplier. Our findings run counter to this: as discussed above, we contend that performance depends not only on the optimal noise multiplier but on our new linear scaling rule, and DP-GD unlocks the use of larger step sizes (Goyal et al., 2017). We use DP-GD instead of DP-SGD in all other experiments, removing the batch size from the hyperparameter tuning process and improving the overall privacy cost of deploying our baselines (Papernot & Steinke, 2021). B.3. A Critical Evaluation of Proposed Techniques for Fine-Tuning Prior work has proposed a number of ad-hoc techniques that improve performance in DP fine-tuning. Here we critically evaluate these techniques in the our method regime, and analyze why they reduce performance in our setting. Small Clipping Norms Bias Optimization. The standard deviation of the noise added in DP-SGD scales with the sensitivity of the update, defined by the clipping norm parameter. To decrease the amount of noise added, prior work has used very strict clipping (Mehta et al., 2023a; Bu et al., 2022a). Intuitively, if the clipping norm parameter is already chosen to be some value smaller than the norm of the unclipped gradient, the gradient estimator is no longer unbiased and this may have a negative impact on optimization. In Fig. 12 we observe that decreasing the clipping norm below 1 only degrades performance. As we can see in equation 1, further decreasing the clipping norm is equivalent to training with a smaller learning rate, and this is suboptimal because Fig. 17 indicates that we can prefer to use larger learning rates. We use a clipping norm of 1 in all other experiments. Initializing Weights to Zero Mitigates Variance in DP-GD. (Qiao et al., 2019) propose initializing the model parameters to very small values to improve the stability of micro-batch training, and (De et al., 2022) find that applying this technique to DP-SGD improves performance. In Fig. 11 we ablate the effectiveness of zero initialization with standard He initialization and find that the best performance comes from initializing the weights uniformly to zero. We initialize the classifier weights to zero in all other experiments. Weight Averaging Cannot Catch Up To Accelerated Fine-Tuning. (Shejwalkar et al., 2022) perform an in-depth empirical analysis and find that averaging the intermediate model checkpoints reduces the variance of DP-SGD and improves model performance. (De et al., 2022) first proposed the use of an Exponential Moving Average (EMA) to mitigate the noise introduced by DP-SGD. Previously, methods that use stochastic weight averaging (SWA) during SGD have been proposed and are even available by default in PyTorch (Izmailov et al., 2018). The idea of averaging weights to increase acceleration was first proposed by (Polyak & Juditsky, 1992), and is theoretically well-founded. In Fig. 13 we compare EMA and SW A with no averaging and find that no averaging performs the best. This is because weight averaging methods work well when optimization has converged and the model is plotting a trajectory that orbits around a local minima in the loss landscape (Izmailov et al., 2018). That is to say, the model’s distance from the initialization does not continually increase and at some point stabilizes so that the weight averaging method can ’catch up’. However, as discussed in Fig. 3 the optimal number of iterations for our method is to train for longer epochs without decaying the learning rate for convergence, because when the model converges the SNR decays. This is corroborated by Fig. 13, where we see that the distance from initialization is monotonically increasing. Our findings run counter to those of (Shejwalkar et al., 2022) for hyperparameters in line with our proposed linear scaling rule because we find that the best optimization regime for our method is precisely one where weight averaging can never catch up to the optimization trajectory. Therefore, the averaging methods only serve 23A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 10 20 30 40 50 60 Epochs 0 10 20 30 40 50 60 70T est Accuracy Weight Initialization Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 He Zero (a) Weight Initialization 30 35 40 45 50 55 60 Epochs 60 62 64 66 68 70 72T est Acc Weight Decay Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 0.05 5e-05 0.005 0.0005 0 (b) Weight Decay Figure 11.Ablation of two previously proposed methods: zero initialization of parameters and weight decay. Zero initialization increases accuracy in all experiments, but weight decay only degrades performance. 50 52 54 56 58 60 Epochs 68.0 68.5 69.0 69.5 70.0 70.5 71.0 71.5 72.0 72.5T est Acc Clipping Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 1.0 0.1 0.01 0.001 Figure 12.Because reducing the clipping norm is equivalent to reducing the learning rate, reducing the clipping norm below 1 only degrades performance on CIFAR100 for the beit architecture at ε = 0.1. 50 51 52 53 54 55 56 57 58 59 Epochs 65 66 67 68 69 70 71 72Accuracy Weight Averaging.  Dataset=CIFAR100, Arch=beit, =0.1 model_acc ema_acc swa_acc (a) Weight Averaging 0 2 4 6 8 Iteration 0 5 10 15 20Distance from Initialization Weight Trajectory   Dataset=CIFAR100, = 0.1 beit beitv2 (b) Weight Trajectory Figure 13.Left:Ablation of Weight Averaging. Right: Plot of distance from initialization. Weight Averaging does not improve performance because the model is monotonically moving away from the initialization and weight averaging cannot ’catch up’. 24A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization to lag one step behind no averaging. Data Augmentation Does Not Work When Freezing Embeddings. Data augmentation is used during training to bias the model towards selecting features that are invariant to the rotations we use in the augmentations. (Geirhos et al., 2018) find that feature extractors pretrained on ImageNet are naturally biased towards texture features. (De et al., 2022) eschew traditional data augmentation and instead propose the use of multiple dataset augmentations or ”batch augmentation”, first introduced by (Hoffer et al., 2019), to mitigate the variance of DP-SGD. In Fig. 14 we ablate the effectiveness of batch augmentation and find that it does not noticeably improve accuracy during transfer learning. This is because dataset augmentation changes the prior of the model when training the entire network (Shorten & Khoshgoftaar, 2019), but when we freeze all layers but the classifier, the model does not have the capacity to change to optimize for the prior introduced by data augmentation, because the embedding layer is frozen. 0 2 4 6 8 10 12 Epochs 10 20 30 40 50 60T est Acc Data Augmultation Ablation.   Dataset=CIFAR100, Arch=beitv2, =0.1 No augmult Augmult=16 (a) CIFAR100 (hard) 0 2 4 6 8 10 12 14 Epochs 40 50 60 70 80 90 100T est Acc Data Augmultation Ablation.   Dataset=CIFAR10, Arch=beitv2, =0.1 No augmult Augmult=16 (b) CIFAR10 (easy) Figure 14.Ablation of Data Augmultation on two datasets. On both datasets, Data Augmultation lags behind the baseline because there is much more training data, and even at the end, Data Augmultation does not have a noticeable improvement. Weight Decay Is Not Needed When Freezing Embeddings. Regularization methods such as weight decay are commonly used during pretraining to prevent overfitting, and the feature extractors we use are pretrained with AdamW (Dosovitskiy et al., 2020). One of the benefits of weight decay during fine-tuning is limiting the change of the embedding layer to not overfit and thus retain the features learned during pretraining (Kumar et al., 2022b). In the ongoing debate on whether to use weight decay during fine-tuning (Touvron et al., 2021), we submit that weight decay should not be used in private fine-tuning. In Fig. 11 we ablate a range of values of the weight decay parameter and observe that increasing the weight decay beyond a negligible amount (the gradient norm is ≈ 1e − 2) only decreases accuracy, and no value of the weight decay increases accuracy. There are two reasons for this. The first is that we initialize the weights of the model to zero, so we do not expect the gradients to be large. The second is that we only train the last layer, and therefore there is no need to regularize the training of the embedding layer. This supports the conclusion of (Kumar et al., 2022c) that SGD with momentum is outperforms AdamW as long as the embedding layer is not updated. B.4. Hyperparameter Ablations We provide full heatmaps and pareto frontiers for all datasets and the 3 best performing models (we do not perform a full evaluation on the ViT in order to minimize any knowledge leak for the evaluation of the linear scaling rule with the strategy in (Mehta et al., 2023a)). We note that while all of these datasets are arguably in-distribution, our focus is on comparing the regime of optimization preferred by our method to those of other works, and this is achieved by producing results on benchmark tasks. We further note that STL10 is explicitly in-distribution for the pretraining dataset (ImageNet); we only use this dataset as a temporary stand-in for evaluation on ImageNet-1k, a common benchmark in prior work (Mehta et al., 2023a) to minimize the computational burden. Hyperparameter Tuning and Selecting Epsilon. Prior work often uses unrealistic values of ε that provide no real privacy guarantee. While some prior work makes the case that hyperparameters need to be tuned even for non-private learning 25A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 16.We compare the best private and best non-private test accuracy performances of our method to prior work using models pretrained on ImageNet-21k and fine-tuned on CIFAR10 and CIFAR100. Full results are in Section 5. Model Dataset ε = 0.1 ε = 1 ε = ∞ Gap (1 − ∞) our method CIFAR10 98.65 99.00 99.00 0.00 CIFAR100 81.9 89.81 91.57 1 .76 (Mehta et al., 2023a) CIFAR10 95.8 96 .3 96 .6 0 .3 CIFAR100 78.5 82 .7 85 .29 2 .59 (Bu et al., 2022a) CIFAR10 - 96.7 97 .4 0 .7 CIFAR100 - 83.0 88 .4 5 .4 (Cattan et al., 2022) CIFAR10 - 95.0 96 .4 1 .4 CIFAR100 - 73.7 82 .1 8 .4 (De et al., 2022) CIFAR10 - 94.8 96 .6 1 .8 CIFAR100 - 67.4 81 .8 14 .4 and can be chosen beforehand, we show that this is not the case. Not only are the optimal choices of key hyperparameters different between training from scratch and transfer learning (Li et al., 2020), they are also different for non-private and private transfer learning (Li et al., 2022b; De et al., 2022). We now provide guidelines for selectingε and broad intuition behind our choice to design a system that minimizes dependence on hyperparameters. For a decade the standard values of ε proposed for privacy preserving statistics queries have fallen in the range of 0.1 in line with eε ≈ 1 + ε for ε ≪ 1 (Dwork et al., 2006), and recently surveyed DP deployments generally abide by the rule of selecting ε ≈ 0.1 (Dwork et al., 2019). We know that while all small values of ε generally behave the same, every large value of ε is fundamentally different in a unique way (Dwork et al., 2019). In line with these guidelines, we only evaluate ε ∈ [0.01, 1.0] and perform most of our ablations on the most challenging task where we can see a range of performance: CIFAR100 for ε = 0.1. B.5. Theory Proposition B.1. The model training subroutine in 2 is ( √ T /σ)-GDP . Corollary B.2. Algorithm 2 is (ϵ, Φ(−ϵ·σ/ √ T + √ T /2σ))−eϵ ·Φ(−ϵ·σ/ √ T − √ T /2σ))-DP . Also, forn-fold repetition, the algorithm is (ϵ, Φ(−ϵ · σ/ √ n · T + √ n · T /2σ)) − eϵ · Φ(−ϵ · σ/ √ n · T − √ n · T /2σ))-DP Proof of Proposition 4.1: Proof. Since we are using the full batch, each iteration of the algorithm is an instantiation of the Gaussian mechanism with sensitivity of 1 and Gaussian noise with standard deviation of σ. Hence, each iteration of the mechanism is (1/σ)-GDP by Theorem 3.7 in (Dong et al., 2019). Then, since we have the adaptive composition of T of these mechanisms, the algorithm is ( √ T /σ)-GDP overall, using the composition theorem for GDP, as stated in Corollary 3.3 in (Dong et al., 2019). Proof of Corollary B.2: Proof. This directly follows from the GDP to DP conversion as stated in Corollary 2.13 in (Dong et al., 2019).Why does our HPO have low privacy cost? Our HPO has low privacy cost because of the nature of composition under GDP. Consider one sweep of our method with n = 3 that evaluates some (T1, η1, σ1), (T2, η2, σ2)) and we extrapolate (Tf , ηf , σf ), that works out to ε1 = 0.1, ε2 = 0.2, εf = 0.88. The composition for this according to (Dong et al., 2022) isµf = q nµ2 1 + nµ2 2 + µ2 f for µ1 = p T1/σ2 1. If we convert µf to εf , δ= 1e −5-DP, we arrive at a final guarantee of(1, 1e −5)-DP. The cost of HPO here in terms of the privacy utility tradeoff is actually just the marginal utility between εf = 0.88 and εt = 1.0. As we will show in Section 5, in many cases this marginal utility is negligible, and the value of cheap one-time measures that improve the performance of the rest of training such as HPO is very much worth it due to the nature of composition under GDP. Proof of Thm. 3.1 The main idea of the proof is similar to the main result in Fang et al. (2023a) but is simpler because we only prove the result for linear models. 26A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Proof. We first apply (Ryu & Boyd, 2015) to see that gradient descent with step size 2 β > η > 2 α+β on a α-strongly convex, β-smooth function is a max(1 − ηβ, 1 − ηα)-contraction. Call this latter quantity c. Now consider a sequence of benign updates from gradient descent wt b and a sequence of noisy updates for the same dataset wt. Given the contractive property of GD , we have the following: \f\f\f(wt b − η∇f(wt b)) − (wt − η∇f(w(t))) \f\f\f ≤ c \f\f\fwt b − wt−1 b \f\f\f (1) We apply the update rule in 1 and use Eq.1 w(t+1) = w(t) − η(∇f(w(t)) + σξ) (2)\f\f\fwt+1 b − wt+1 \f\f\f = (3) = \f\f\fwt b − η∇f(wt b) − w(t) + η∇f(w(t)) − σξ \f\f\f (4) ≤ c \f\f\fwt b − w(t) \f\f\f + ηρ (5) Now we have the following \f\f\fwt − wt b \f\f\f ≤ c \f\f\fwt−1 − wt−1 b \f\f\f + ρη (6) We now proceed via induction. Assume for T − 1 the statement of Thm. 3.1 holds. By Eq.6 and the induction hypothesis we have \f\f\fwT−1 − wT−1 b \f\f\f ≤ ρη × ( T−2X i ci) (7) \f\f\fwT − wT b \f\f\f ≤ c(ρη × ( T−2X i ci)) + ρη (8) \f\f\fwT − wT b \f\f\f ≤ ρη × ( T−1X i ci). (9) ρη × ( T−1X i ci) = ρη(1 − cT ) 1 − c ρη 1 − cT 1 − c = ρη(1 − cT ) η · min(α, β) = ρ(1 − cT ) min(α, β) The intuition is clear: at iteration 0 there is no divergence. At iteration 1 there is ηρ divergence. At iteration 2 the previous divergence contracts by c and increases by ηρ, so the divergence is c1ηρ + ηρ. At iteration 3 the divergence is c2ηρ + c1ηρ + ηρ = ηρ(c2 + c + 1). It remains to show that the conditions for convexity and smoothness are satisfied for the problem at hand. For the case of, ex, training a single linear layer on top of extracted features with GD, this is easy to prove. We defer to the analysis from Panda et al. (2021), which we reproduce here for the reader’s convenience. Example B.3 (Computing the Lipschitz constant for single-layer SGD training ( Panda et al. (2021))). We compute the coordinatewise Lipschitz constant of the SGD protocol for a single layer neural network defined asσ(θx), where σ is the softmax function and θ ∈ Rd are the network parameters. For cross-entropy loss-based training using datasetD, we show that the constant c = 1 4 . Formally, sup D∈Z,θ1,θ2∈M |G(θ1, D)[i] − G(θ2, D)[i]|1 ≤ 1 4|θ1 − θ2|1 for any coordinate index i ∈ [d] 27A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Without loss of generality, we assume that dataset D is comprised of samples of the form (x, y), where x ∈ [0, 1]m, and y ∈ {0, 1}C is the one-hot encoded representation of any of the C classes. For the single layer neural network, the model parameters are denoted by θ ∈ RC×m, and the softmax layer by the function σ(·). The neural network can thus be represented as Φ(x, θ) = σ(θx). We define g(θ, x) = ∂L(Φ(x,θ),y) ∂θ where L is the softmax cross entropy loss function. For the SGD protocol,A(u) = u, and G(θ, D) = g(θ, x). Our goal is to find a Lipschitz constant L such that, for all indices i ∈ [C] and j ∈ [m], sup x∈D,θ1,θ2 |g(θ1, x)ij − g(θ2, x)ij|1 |θ1 − θ2|1 ≤ L (10) We define intermediate variable z = θx and the neural network output distribution p = σ(z), such that both p, z∈ RC. Note, for a given target class t, the cross entropy loss function L(p, y) = −log pt where pt = eztP j ezj . Thus, g(θ, x)ij = ∂L ∂θij = CX c=1 ∂L ∂zc ∂zc ∂θij . (11) Computing the terms of (11), we have ∂L ∂zc = pt − 1 for c = t; and ∂L ∂zc = pc otherwise; and ∂zc ∂θij = xj. Thus, g(x, θ)ij = xj(pt − 1) for i = t = xjpi for i ̸= t (12) We compute the Hessian of g(x, θ)ij as: ∂g(x, θ)ij ∂θkl = xjpt(1 − pt)xl for k = t = xjpk(1 − pk)xl for k ̸= t (13) where k ∈ [C], l∈ [m]. The maximum value of the Hessian in (13), occurs at xj = xl = 1, and pt = pk = 1 2 . Thus, max i,j,k,l ∂g(x, θ)ij ∂θkl ≤ 1 4 for k = t ≤ 1 4 for k ̸= t (14) To obtain the Lipschitz constant, we first define the function h(t) = g((1 − t)θ1 + tθ2, x)ij where t ∈ [0, 1] Thus, h(0) = g(θ1, x)ij and h(1) = g(θ2, x)ij. Since, the function h(t) is differentiable everywhere in (0, 1), using Mean Value Theorem, we know that there exists a pointt∗ ∈ (0, 1) such that: h(1) − h(0) ≤ h′(t∗) where h′(t) = (θ2 − θ1)g′((1 − t)θ1 + tθ2, x)ijkl. (15) Rewriting (10), we get sup x∈D,θ1,θ2 |g(θ1, x) − g(θ2, x)|1 ≤ sup x∈D,θ1,θ2 |max i,j {g(θ1, x)ij − g(θ2, x)ij}|1 Let i∗, j∗ correspond to the indices where the maximum in the above equation occurs. Combining (14) and (15), we get: sup x∈D,θ1,θ2 |g(θ1, x)i∗j∗ − g(θ2, x)i∗j∗ |1 ≤ 1 4|θ1 − θ2|1 (16) Comparing (16) with (10) we get c = 1 4 . 28A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization C. Furthur Results for Language Modeling Tasks In general it is not feasible to do full-batch experiments for the NLP tasks because the memory requirements of LLMs are very large. We therefore do the composition with the PoissonSubsampledGaussianMechanism class in the PLD accountant (Gopi et al., 2021), ensuring that our method still accounts for the privacy cost of HPO. C.1. Related Works Li et al. (2022b) provide methods for fine-tuning large language models under DP-SGD by proposing new clipping methods to mitigate the memory burden of per-sample gradient clipping. However, they do not achieve performance comparable to non-private models when fine-tuning a pretrained model on the PersonaChat dataset. We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset. Yu et al. (2021) report compelling results by only updating a sparse subset of the LLMs with LoRA (Hu et al., 2021). We fine-tune GPT2 and RoBerta; Basu et al. (2022) also fine-tune BERT models. C.2. Experimental Set-up for Finetuning Language Models Persona-Chat: We write code based on winners of ConvAI2 competition1 and private-transformers library.2 We first do clipping norm [0.1, 0.2, 0.5, 1.0], learning rate in [2, 5, 10, 20, 50] × 10−5, batch size 64 and epochs [3, 10, 20] at ε = 3 and ε = 8 and find that the clipping norm in this range achieves almost same perplexity with other hyperparams fixed. We then do hyperparameter tuning as reported in Table 17 to finetune GPT-2. Table 17.Set of hyper-parameters used in the finetuning GPT-2. Parameter Values Clipping Norm 0.1 Learning Rate [2, 5, 10, 20, 50, 100] × 10−5 Batch Size [64, 128, 256, 512, 1024, 2048] Epochs [3, 10, 20] WikiText-2: We write code based on the HuggingFace transformers library GPT-2 example,3 source code by (Shi et al., 2022)4 and private-transformers library. The hyperparameter range for grid search is reported in Table 18. Table 18.Set of hyper-parameters for grid search to finetune GPT-2 on WikiText-2.δ = 10−6. Parameter Values Clipping Norm 1 Batch Size 2048 (Full Batch) Epochs 20 Learning Rate for ε = 0.2 [2 , 4, 6, 8, 10] × 10−4 Learning Rate for ε = 0.5 [0 .8, 1, 2] × 10−3 Enron Email: For Enron email dataset, we use the preprocessed dataset in (Gupta et al., 2022), where the non-private baseline of finetuned GPT-2 on this dataset is 7.09. The hyperparameter range for grid search is reported in Table 19. C.3. Additional Results on Persona-Chat We report the perplexity of GPT-2 on the Persona-Chat dataset at different epochs and batch size in Figure 15 (with tuned learning rate in Table 17) and we can see that larger batch size and longer epochs can achieve better perplexity, which is 1https://github.com/huggingface/transfer-learning-conv-ai. 2https://github.com/lxuechen/private-transformers. 3HuggingFace transformers GPT-2 example code. 4https://github.com/wyshi/sdp transformers 29A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 19.Set of hyper-parameters for grid search to finetune GPT-2 on Enron Email dataset. δ = 1 2|Dtrain|. Parameter Values Clipping Norm 1 Batch Size 1024 Epochs 5 Learning Rate for ε = 0.1 [2 , 3, 4, 5, 6, 7, 8, 9, 10] × 10−4 Learning Rate for ε = 0.2 [0 .6, 0.8, 1, 2, 3, 4, 6, 7] × 10−3 Learning Rate for ε = 0.5 [0 .4, 0.6, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.8, 2] × 10−2 Learning Rate for ε = 1.0 [1 , 2, 3, 4, 5, 6, 7, 8] × 10−2 Learning Rate for ε = 2.0 [2 , 3, 4, 5, 6, 7, 8, 9, 10] × 10−2 Learning Rate for ε = 3.0 [0 .6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.6, 1.8, 2.0] × 10−1 consistent with our linear scale rule. Besides, we also investigate fine-tuning multiple layers. With letting the embedding layer and last LayerNorm layer in transformer trainable, we consider fine-tuning only last block in transformer, first and last block in transformer and report the result in Table 20 and we can see that the best perplexity is achieved by fine-tuning the whole model. 64 128 256 512 1024 Batch Size 20 10 3 Epochs 19.56 18.98 18.39 17.94 17.91 20.34 19.69 19.07 18.68 18.33 22.53 21.68 21.34 20.28 20.25 Ppl of finetuned GPT2 on Persona-Chat ( =3) 18 19 20 21 22 (a) ε = 3 64 128 256 512 1024 Batch Size 20 10 3 Epochs 18.95 18.40 17.83 17.34 17.27 19.61 19.01 18.43 18.03 17.65 21.36 20.60 20.38 19.41 19.33 Ppl of finetuned GPT2 on Persona-Chat ( =8) 17.5 18.0 18.5 19.0 19.5 20.0 20.5 21.0 (b) ε = 8 Figure 15.Comparison of perplexity at different batch size and epochs of GPT-2 on Persona-Chat dataset. Table 20.Finetuning GPT-2 on Persona-Chat dataset including full model and different layers of model. We also include non-private baseline. ε 3 8 Full 17.91 17 .27 Last Block 19.80 19 .20 First-Last-Block 18.93 18 .26 C.4. Addtional Results on WikiText-2 We run the grid-search experiment for ε ∈ {0.2, 0.5, 1, 2, 3} to evaluate the performance gap between the optimal total step size and the estimated total step size.5) and present the result in Figure 16. The linear rule scales well from ε ∈ {0.2, 0.5} to ε = 1. Though for ε ∈ {2, 3} the perplexity of total step size by linear scale rule is slightly higher than the optimal perplexity of total step size by grid search, the result by linear scale is better than previous SOTA (Shi et al., 2022), which is 28.84 at (ε = 3, δ= 10−6) by training 20 iterations. 5Due to the limit of computation resources, all experiments are done by training for 20 iterations. Further increasing the number of iterations will help improve the utility as shown by previous study (Li et al., 2022b; Shi et al., 2022), we leave longer iterations for further study. 30A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.5 1.0 1.5 2.0 2.5 3.0 27.5 28.0 28.5Perplexity Perplexity by linear scaling Perplexity by grid search (a) Pareto Frontier for ε vs Test Perplexity 0.5 1.0 1.5 2.0 2.5 3.0 0.00 0.05 0.10 0.15 0.20 0.25Total step size ( × T)  T otal step size by linear scaling T otal step size by grid search (b) Pareto Frontier for ε vs Total Step Size Figure 16.The linear scaling rule (accounting for the privacy cost of hyperparameter tuning) is competitive with grid search (non-private, doing N trials each with the given ε) in range [0.2, 1.0] on the WikiText-2 dataset. Left: y-axis is Perplexity (lower is better). 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (a) CIFAR10 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (b) CIFAR10 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (c) CIFAR10 Convnext 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (d) CIFAR100 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (e) CIFAR100 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (f) CIFAR100 Convnext Figure 17.Heatmaps for the reported datasets and architectures; lighter is better. Note that the scale of the axes differs from the heatmaps in the main body; this will be fixed in a future update. ε increases left to right with a different value for each heatmap according to: [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], epochs increase from left to right on the x-axis of each heatmap according to: [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], and the learning increases from top to bottom on the y-axis of each heatmap according to: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1.0]. As ε increases, left to right, the optimal hyperparameters trend towards longer training with lower learning rates (bottom right). 31A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (a) STL10 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (b) STL10 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (c) STL10 Convnext 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (d) FashionMNIST Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (e) FashionMNIST Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (f) FashionMNIST Convnext Figure 18.Heatmaps for the reported datasets and architectures; lighter is better. Note that the scale of the axes differs from the heatmaps in the main body; this will be fixed in a future update. ε increases left to right with a different value for each heatmap according to: [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], epochs increase from left to right on the x-axis of each heatmap according to: [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], and the learning increases from top to bottom on the y-axis of each heatmap according to: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1.0]. As ε increases, left to right, the optimal hyperparameters trend towards longer training with lower learning rates (bottom right). 32A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80T est Accuracy  vs Test Accuracy on Dataset=CIFAR100 beit beitv2 (a) CIFAR100 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 10 20 30 40 50 60 70 80Total Step Size (  × T)  vs Total Step Size on Dataset=CIFAR100 beit beitv2 (b) CIFAR100 Total Step Size 0.0 0.2 0.4 0.6 0.8 1.0 86 88 90 92 94 96 98T est Accuracy  vs Test Accuracy on Dataset=CIFAR10 beit convnext beitv2 (c) CIFAR10 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80Total Step Size (  × T)  vs Total Step Size on Dataset=CIFAR10 beit convnext beitv2 (d) CIFAR10 Total Step Size Figure 19.Pareto frontier for ε vs test accuracy and total step size for CIFAR10, and CIFAR100. Beitv2 excels for larger values ofε but beit and convnext are better for smaller values of ε. The inflection point varies across datasets. 33A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.0 0.2 0.4 0.6 0.8 1.0 80 82 84 86 88 90T est Accuracy  vs Test Accuracy on Dataset=FashionMNIST beit convnext beitv2 (a) FashionMNIST Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80Total Step Size (  × T)  vs Total Step Size on Dataset=FashionMNIST beit convnext beitv2 (b) FashionMNIST Total Step Size 0.0 0.2 0.4 0.6 0.8 1.0 20 30 40 50 60 70 80 90 100T est Accuracy  vs Test Accuracy on Dataset=STL10 beit convnext beitv2 (c) STL10 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 2 4 6 8Total Step Size (  × T)  vs Total Step Size on Dataset=STL10 beit convnext beitv2 (d) STL10 Total Step Size Figure 20.Pareto frontier for ε vs test accuracy and total step size for STL10 and FashionMNIST. Beitv2 excels for larger values of ε but beit and convnext are better for smaller values of ε. The inflection point varies across datasets. 34",
      "meta_data": {
        "arxiv_id": "2212.04486v3",
        "authors": [
          "Ashwinee Panda",
          "Xinyu Tang",
          "Saeed Mahloujifar",
          "Vikash Sehwag",
          "Prateek Mittal"
        ],
        "published_date": "2022-12-08T18:56:37Z",
        "pdf_url": "https://arxiv.org/pdf/2212.04486v3.pdf",
        "github_url": "https://github.com/lxuechen/private-transformers"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the open problem of hyperparameter optimization (HPO) in differentially private deep learning, which is computationally expensive and complicates privacy accounting. It proposes a novel adaptive HPO method based on a 'new linear scaling rule.' This method uses low-privacy-cost trials (small epsilon) to estimate optimal hyperparameters and then scales them linearly to larger privacy budgets. This approach significantly reduces the computation and privacy cost of HPO, achieves state-of-the-art performance on 22 benchmark tasks across computer vision and natural language processing, and accounts for the privacy cost of HPO, outperforming prior methods.",
        "methodology": "The core methodology is the 'new linear scaling rule,' which simplifies HPO by reducing the search for optimal learning rate (η) and number of steps (T) to a single scalar variable, r = η × T. The method estimates r for small privacy budgets (ε) using random search to find two empirical points (ε1, r(ε1)) and (ε2, r(ε2)). It then fits a linear approximation to these points to estimate the optimal r for any desired target εf. The theoretical intuition is motivated by analyzing the excess empirical risk of private Gradient Descent (GD) and bounding the 'noisy radius,' the distance between noisy and non-noisy iterates, which is shown to be proportional to η × T. The method incorporates specific design choices to maximize the signal-to-noise ratio and accelerate training: full batch gradient computation (DP-GD), unit clipping (C=1), zero initialization of linear classifiers, SGD with momentum (ρ=0.9), and Privacy Loss Variable (PLV) accounting, which offers tighter privacy guarantees than RDP.",
        "experimental_setup": "The method was evaluated on 22 benchmark tasks spanning computer vision (ImageNet, CIFAR10/100, FashionMNIST, STL10, EMNIST) and natural language processing (SQuAD, GLUE benchmark tasks: SST-2, QNLI, QQP, MNLI, PersonaChat, WikiText-2, Enron Emails). It also included distribution shift datasets (CIFAR10 -> STL, CIFAR10p1, CIFAR10C, CIFAR100 -> CIFAR100C, Waterbirds, FMoW, Camelyon17). Models evaluated included ResNets and various Transformer architectures (beit, beitv2, convnext, ViT-L, GPT-2, RoBERTa-base), covering both pretraining and fine-tuning scenarios. The privacy budgets (ε) ranged widely from 0.01 to 8.0. Performance was compared against random search, grid search (without accounting for HPO privacy cost), and several prior private HPO methods, including those based on Rényi DP (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023), DPAdamWOSM, and methods by Mehta et al. (2023b). Evaluation metrics included accuracy for classification tasks and perplexity for language modeling.",
        "limitations": "The theoretical assumptions (e.g., strong convexity, smoothness, bounded gradients) do not universally hold for training complex neural networks, though empirical results support the method's success. The HPO method requires more runtime than simple random search (e.g., 7 GPU hours compared to 1 for random search) and exhibits worse parallelization than grid search due to its sequential adaptive nature. For extremely small privacy budgets (e.g., ε < 0.5 on challenging datasets like ImageNet), the privacy cost of HPO itself can be too high, preventing cheap trials from yielding useful information for the final run. While the method primarily optimizes learning rate (η) and number of steps (T), incorporating additional hyperparameters (e.g., batch size, clipping threshold) can degrade performance. The paper does not focus on tuning the hyperparameters of the HPO method itself (e.g., number of points for the linear approximation).",
        "future_research_directions": "Future work could extend the theoretical analysis to incorporate additional factors like momentum acceleration and the bias introduced by gradient clipping, as well as generalize it to more complex neural network settings. Investigating Rényi Differential Privacy (RDP) analysis for the adaptive method is another potential direction. Applying the proposed HPO method to alternate threat models, such as scenarios involving small fractions of private training data combined with public data, could be explored. The paper also suggests privatizing other non-private HPO methods and using this HPO approach to tune the number of epochs in parameter-free optimizers like DPAdamWOSM. While the current work primarily uses linear approximation, exploring higher-degree polynomial approximations for the relationship between r and ε could be considered, albeit with increased privacy cost.",
        "experimental_code": "import collections\nimport logging\nimport math\nimport types\nfrom typing import Callable, Dict, Optional, Sequence, Union\n\nimport torch\nfrom ml_swissknife import utils\nfrom torch import nn\n\nfrom . import autograd_grad_sample, transformers_support\nfrom .accounting import accounting_manager\nfrom .settings import AccountingMode, BackwardHookMode, ClippingMode, SUPPORTED_TRANSFORMERS\n\nlogger = logging.get_logger(__name__)\n\n\nclass PrivacyEngine(object):\n    \"\"\"Differentially-private optimization engine that works gracefully with Hugging Face transformers.\n\n    Supports ghost clipping as described in\n        Li, X., Tramèr, F., Liang, P., & Hashimoto, T. (2021).\n        Large Language Models Can Be Strong Differentially Private Learners.\n        arXiv preprint arXiv:2110.05679.\n\n    Implicitly assumes inputs are in batch first format.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: nn.Module,\n        *,\n        batch_size: int,\n        sample_size: int,\n        max_grad_norm: float,\n        epochs: Optional[Union[int, float]] = None,\n        noise_multiplier: Optional[float] = None,\n        target_epsilon: Optional[float] = None,\n        target_delta: Optional[float] = None,\n        alphas: Sequence[float] = accounting_manager.DEFAULT_ALPHAS,\n        record_snr: bool = True,\n        named_params: Optional[Sequence] = None,\n        numerical_stability_constant=1e-6,\n        clipping_mode=ClippingMode.default,\n        accounting_mode=\"rdp\",\n        eps_error=0.05,\n        skip_checks=False,\n        **unused_kwargs,\n    ):\n        \"\"\"Initialize the engine.\n\n        Args:\n            module: The PyTorch module for which per-sample gradient is required.\n                Setting the `requires_grad` attribute of a parameter to False\n                disables the per-sample gradient accumulation.\n            batch_size: The expected size of Poisson-sampled batch, i.e., the lot size.\n            sample_size: Size of dataset.\n            max_grad_norm: The maximum 2-norm for gradient clipping.\n            epochs: The number of epochs for training.\n            noise_multiplier: The extra multiplier for DP-SGD noise.\n            target_epsilon: The target privacy spending.\n                Only used to estimate the `noise_multiplier` if it is not set.\n            target_delta: The target failure probability.\n                Defaults to sample_size ** -1.1 if not set.\n            alphas: The RDP orders for (ε, δ)-DP conversion. Useless if not accounting in RDP.\n            record_snr: Record and report the signal-to-noise ratio --\n                ratio between norm of summed clipped gradient and norm of noise vector.\n            named_params: Specifies which parameters need gradients;s\n                defaults to use parameters which require grad in module.\n            numerical_stability_constant: Small constant to avoid division by 0 when clipping.\n            clipping_mode: The clipping mode to use. One of 'default', 'ghost', 'per_layer', 'per_layer_percentile'.\n            accounting_mode: The method of accounting privacy. One of (`rdp`, `glw`, `all`).\n                Meanings of shorthands:\n                    - rdp: Account loss with RDP but perform conversion to approx-DP with a procedure defined in\n                        \"The Discrete Gaussian for Differential Privacy\". https://arxiv.org/abs/2004.00010\n                    - glw: Account loss by numerically composing tradeoff functions in f-DP; defined in\n                        \"Numerical composition of differential privacy\". https://arxiv.org/abs/2106.02848\n                    - all: Report loss with all methods listed above.\n            eps_error: Error threshold for upper and lower bound in the GLW accounting procedure.\n            skip_checks: Skips the model type validation test if True.\n        \"\"\"\n        utils.handle_unused_kwargs(unused_kwargs)\n        del unused_kwargs\n        super(PrivacyEngine, self).__init__()\n\n        if clipping_mode not in ClippingMode.all():\n            raise ValueError(f\"Unknown clipping mode {clipping_mode}. Expected one of {ClippingMode.all()}.\")\n        if accounting_mode not in AccountingMode.all():\n            raise ValueError(f\"Unknown accounting mode: {accounting_mode}. Expected one of {AccountingMode.all()}.\")\n        if epochs <= 0.0:\n            raise ValueError(f\"Number of training epochs cannot be non-positive, but found epochs={epochs}\")\n\n        # Privacy parameters.\n        sample_rate = batch_size / sample_size\n        if target_delta is None:\n            target_delta = sample_size ** -1.1\n        if noise_multiplier is None:\n            if target_epsilon is None or epochs is None:\n                raise ValueError(\n                    f\"`target_epsilon` and `epochs` must be specified when `noise_multiplier` is `None`.\"\n                )\n            if accounting_mode in (\"rdp\", \"all\"):\n                manager = accounting_manager.RDPManager(alphas=alphas)\n            else:  # \"glw\"\n                manager = accounting_manager.GLWManager(eps_error=eps_error)\n            noise_multiplier = manager.compute_sigma(\n                target_epsilon=target_epsilon, target_delta=target_delta, sample_rate=sample_rate, epochs=epochs,\n            )\n\n        self.batch_size = batch_size\n        self.sample_size = sample_size\n        self.sample_rate = sample_rate\n        self.max_grad_norm = max_grad_norm\n\n        self.epochs = epochs\n        self.noise_multiplier = noise_multiplier\n        self.effective_noise_multiplier = noise_multiplier / batch_size\n        self.target_epsilon = target_epsilon\n        self.target_delta = target_delta\n        self.alphas = alphas\n        self.eps_error = eps_error\n        self.accounting_mode = accounting_mode\n        self.record_snr = record_snr\n\n        # Internals.\n        self.steps = 0  # Tracks privacy spending.\n\n        # Recording.\n        self.max_clip = None\n        self.min_clip = None\n        self.med_clip = None\n        self.signal = None\n        self.noise = None\n        self.snr = None\n        self.noise_limit = None\n\n        # Record parameters.\n        self.module = module\n        if named_params is None:\n            self.named_params = tuple(\n                (name, param) for (name, param) in module.named_parameters() if param.requires_grad\n            )\n        else:\n            self.named_params = named_params\n        self.num_params = sum(param.numel() for _, param in self.named_params)\n\n        self._locked = False  # Lock the part where noisy gradients is created (in `self.step`) if True.\n        self.numerical_stability_constant = numerical_stability_constant\n        self.clipping_mode = clipping_mode\n        if clipping_mode == ClippingMode.ghost:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)  # Prepare for first backward.\n        else:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.default)  # Extra guard.\n\n        if not isinstance(module, SUPPORTED_TRANSFORMERS) and not skip_checks:\n            raise ValueError(\n                f\"Model type {type(module)} is not supported. Please file an issue if you want this model to be added.\\n\"\n                f\"Currently supported transformers are: {SUPPORTED_TRANSFORMERS}\"\n            )\n        transformers_support.forward_swapper(module=module)  # Fix the position embeddings broadcast issue.\n\n    def lock(self):\n        \"\"\"Run this after noisy clipped gradient is created to prevent tampering with it before parameter update.\"\"\"\n        self._locked = True\n\n    def unlock(self):\n        \"\"\"Run this after parameter update to allow creation of noisy gradient for next step\"\"\"\n        self._locked = False\n\n    def attach(self, optimizer):\n        # `loss_reduction=\"sum\"` super important.\n        autograd_grad_sample.add_hooks(model=self.module, loss_reduction=\"sum\")\n\n        # Override zero grad.\n        def dp_zero_grad(_self, *args, **kwargs):\n            _self.privacy_engine.zero_grad()\n\n        # Override step.\n        def dp_step(_self, **kwargs):\n            closure = kwargs.pop(\"closure\", None)\n\n            _self.privacy_engine.step(**kwargs)\n            _self.original_step(closure=closure)\n            _self.privacy_engine.unlock()  # Only enable creating new grads once parameters are updated.\n            _self.privacy_engine.steps += 1\n\n        def virtual_step(_self, **kwargs):\n            _self.privacy_engine.virtual_step(**kwargs)\n\n        def get_privacy_spent(_self, **kwargs):\n            return _self.privacy_engine.get_privacy_spent(**kwargs)\n\n        def get_training_stats(_self, **kwargs):\n            return _self.privacy_engine.get_training_stats(**kwargs)\n\n        optimizer.privacy_engine = self\n\n        optimizer.original_step = optimizer.step\n        optimizer.step = types.MethodType(dp_step, optimizer)\n\n        optimizer.original_zero_grad = optimizer.zero_grad\n        optimizer.zero_grad = types.MethodType(dp_zero_grad, optimizer)\n\n        optimizer.virtual_step = types.MethodType(virtual_step, optimizer)\n\n        # Make getting info easier.\n        optimizer.get_privacy_spent = types.MethodType(get_privacy_spent, optimizer)\n        optimizer.get_training_stats = types.MethodType(get_training_stats, optimizer)\n\n        self.module.privacy_engine = self\n\n        # Just to be safe, we also override `zero_grad` for module.\n        self.module.original_zero_grad = self.module.zero_grad\n        self.module.zero_grad = types.MethodType(dp_zero_grad, self.module)\n\n        # For easy detaching.\n        self.optimizer = optimizer\n\n    def detach(self):\n        optimizer = self.optimizer\n        optimizer.step = optimizer.original_step\n        optimizer.zero_grad = optimizer.original_zero_grad\n        delattr(optimizer, \"privacy_engine\")\n        delattr(optimizer, \"original_step\")\n        delattr(optimizer, \"original_zero_grad\")\n        delattr(optimizer, \"virtual_step\")\n        delattr(optimizer, \"get_privacy_spent\")\n        delattr(optimizer, \"get_training_stats\")\n\n        module = self.module\n        autograd_grad_sample.remove_hooks(module)\n        autograd_grad_sample.set_hooks_mode(\"default\")  # This is super important when there are multiple attaches!\n        module.zero_grad(skip_grad=True)  # noqa\n        module.zero_grad = module.original_zero_grad\n        delattr(module, \"original_zero_grad\")\n\n    @torch.no_grad()\n    def step(\n        self,\n        loss: torch.Tensor,\n        scale=1.,\n        # Function that takes in named_params and does something.\n        # This option was included to help with another spectrum analysis project.\n        callback: Optional[Callable] = None,\n    ):\n        if loss.dim() != 1:\n            raise ValueError(\n                f\"Expected `loss` to be the per-example loss 1-D tensor, but got a tensor with dims={loss.dim()}.\n            )\n\n        if self.clipping_mode == ClippingMode.ghost:\n            if callback is not None:\n                raise ValueError(\"Ghost clipping does not support `callback` in `optimizer.step`.\")\n            if scale != 1.:\n                raise ValueError(\"Ghost clipping does not support mixed-precision training.\")\n            self._ghost_step(loss=loss)\n        else:\n            self._step(loss=loss, scale=scale, callback=callback)\n\n    @torch.no_grad()\n    def virtual_step(self, loss: torch.Tensor, scale=1.):\n        \"\"\"Virtual step function when there's gradient accumulation.\"\"\"\n        if self.clipping_mode == ClippingMode.ghost:\n            self._ghost_virtual_step(loss=loss)\n        else:\n            self._virtual_step(loss=loss, scale=scale)\n\n    def zero_grad(self, skip_grad=False):\n        for name, param in self.named_params:\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"summed_grad\"):\n                del param.summed_grad\n            if not skip_grad:\n                if hasattr(param, \"grad\"):\n                    del param.grad\n\n    def _create_noisy_clipped_gradient(self):\n        \"\"\"Create noisy clipped gradient for `optimizer.step`.\n\n        Add noise and scale by inverse batch size.\n\n        Notes:\n            In ghost clipping, `summed_grad` stores previous micro-batches; `grad` stores current micro-batch.\n            In default clipping, `summed_grad` stores summed clipped gradients for all micro-batches.\n        \"\"\"\n\n        signals, noises = [], []\n        for name, param in self.named_params:\n            assert hasattr(param, 'summed_grad'), (\n                f\"Internal error: PrivacyEngine should not reach here; \"\n                f\"this means either \"\n                f\"1) there is parameter which requires gradient, but was not used in the computational graph, \"\n                f\"or 2) the backward hook registry failed to find the corresponding module to register.\"\n            )\n            param.grad = param.summed_grad  # Ultra important to override `.grad`.\n\n            if self.record_snr:\n                signals.append(param.grad.reshape(-1).norm(2))\n\n            if self.noise_multiplier > 0 and self.max_grad_norm > 0:\n                noise = torch.normal(\n                    mean=0,\n                    std=self.noise_multiplier * self.max_grad_norm,\n                    size=param.size(),\n                    device=param.device,\n                    dtype=param.dtype,\n                )\n                param.grad += noise\n                if self.record_snr:\n                    noises.append(noise.reshape(-1).norm(2))\n                del noise\n\n            param.grad /= self.batch_size\n\n        if self.record_snr and len(noises) > 0:\n            self.signal, self.noise = tuple(torch.stack(lst).norm(2).item() for lst in (signals, noises))\n            self.noise_limit = math.sqrt(self.num_params) * self.noise_multiplier * self.max_grad_norm\n            self.snr = self.signal / self.noise\n        else:\n            self.snr = math.inf  # Undefined!\n\n        self.lock()  # Make creating new gradients impossible, unless optimizer.step is called.\n\n    # --- ghost clipping ---\n    def _ghost_step(self, loss: torch.Tensor):\n        \"\"\"Run double-backward on per-example loss, then sum up all gradients and noise it.\"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        self._ghost_virtual_step(loss)\n        self._create_noisy_clipped_gradient()\n\n    @torch.no_grad()\n    def _ghost_virtual_step(self, loss: torch.Tensor):\n        \"\"\"Backward twice to accumulate summed clipped gradients in `.summed_grad`.\n\n        We accumulate gradients in `.summed_grad` for micro-batching.\n        All of this copying actually creates a new 2x memory overhead.\n        \"\"\"\n        self._double_backward(loss)\n\n        for name, param in self.named_params:\n            if hasattr(param, 'summed_grad'):\n                param.summed_grad += param.grad\n            else:\n                param.summed_grad = param.grad\n\n            if hasattr(param, \"grad\"):\n                del param.grad\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n\n    @torch.enable_grad()\n    def _double_backward(self, loss: torch.Tensor):\n        \"\"\"Given per-example losses, backward twice to accumulate summed clipped gradients in `.grad`.\"\"\"\n        first_loss = loss.sum()\n        first_loss.backward(retain_graph=True)\n\n        # Prepare for second backward.\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_grad)\n\n        # The first backward might have accumulated things we don't need into `.grad`;\n        # remove it before the second pass to avoid accumulating garbage.\n        for name, param in self.named_params:\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        coef_sample = self.get_coef_sample()\n        second_loss = (coef_sample * loss).sum(dim=0)\n        second_loss.backward()\n\n        # Prepare for first backward (in the next round).\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)\n\n    def get_coef_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient scaling factor for clipping.\"\"\"\n        norm_sample = self.get_norm_sample()\n        return torch.clamp_max(self.max_grad_norm / (norm_sample + self.numerical_stability_constant), 1.)\n\n    def get_norm_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient norms.\"\"\"\n        norm_sample = torch.stack([param.norm_sample for name, param in self.named_params], dim=0).norm(2, dim=0)\n        return norm_sample\n\n    # --- default clipping ---\n    def _step(\n        self,\n        loss,\n        scale,\n        callback,\n    ):\n        \"\"\"Create noisy gradients.\n\n        Should be run right before you call `optimizer.step`.\n\n        This function does 3 things:\n            1) call `loss.backward()`\n            2) clip the current `.grad_sample` and add that to `.summed_grad`\n            3) noise the gradients\n        In mixed-precision training (with amp), the last two steps require knowing the loss scaling factor.\n\n        Args:\n            loss: The per-example loss; a 1-D tensor.\n            scale: The loss up-scaling factor in amp. In full precision, this arg isn't useful.\n        \"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        norm_sample, coef_sample = self._accumulate_summed_grad(loss=loss, scale=scale)\n        # Collect stats for debugging.\n        self.max_clip = coef_sample.max().item()\n        self.min_clip = coef_sample.min().item()\n        self.med_clip = coef_sample.median().item()\n\n        if callback is not None:\n            callback(self)\n        self._create_noisy_clipped_gradient()\n\n    def _virtual_step(self, loss, scale):\n        self._accumulate_summed_grad(loss=loss, scale=scale)\n\n    @torch.no_grad()\n    def _accumulate_summed_grad(self, loss, scale):\n        \"\"\"Accumulate signal by summing clipped gradients.\n\n        Removes `.grad_sample` and `.grad` for each variable that requires grad at the end.\n        \"\"\"\n        with torch.enable_grad():\n            loss.sum(dim=0).backward()\n\n        norm_sample = []\n        for name, param in self.named_params:\n            try:\n                batch_size = param.grad_sample.size(0)\n            except AttributeError as error:\n                args = error.args\n                extra_msg = f\"\\n *** {name} parameter has no grad_sample attribute ***\"\n                error.args = (args[0] + extra_msg, *args[1:])\n                raise error\n            norm = param.grad_sample.reshape(batch_size, -1).norm(2, dim=1)\n            norm_sample.append(norm)\n\n        # The stack operation here is prone to error, thus clarify where the error is.\n        try:\n            norm_sample = torch.stack(norm_sample, dim=0).norm(2, dim=0)\n        except RuntimeError as runtime_error:\n            args = runtime_error.args\n\n            # Get the major shape.\n            shapes = collections.defaultdict(int)\n            for tensor in norm_sample:\n                shapes[tensor.size()] += 1\n\n            # Get the shape that most tensors have.\n            major_shape, major_count = max(shapes.items(), key=lambda x: x[1])\n\n            # Check which tensors don't have the major shape!\n            extra_msg = f\" \\n*** Major shape: {major_shape}\"\n            for (name, param), tensor in zip(list(self.named_params), norm_sample):\n                if tensor.size() != major_shape:\n                    extra_msg += f\", {name} wrong shape: {tensor.size()}\"\n            extra_msg += \" ***\"\n\n            runtime_error.args = (args[0] + extra_msg, *args[1:])\n            raise runtime_error\n\n        coef_sample = torch.clamp_max(\n            self.max_grad_norm * scale / (norm_sample + self.numerical_stability_constant), 1.\n        )\n        for name, param in self.named_params:\n            if not hasattr(param, 'summed_grad'):\n                param.summed_grad = 0.\n            current_device = param.grad_sample.device\n            param.summed_grad += torch.einsum(\"i,i...->...\", coef_sample.to(current_device), param.grad_sample)\n\n            # Aggressive memory saving -- delete everything except `.summed_grad` to save memory!\n            if hasattr(param, \"grad_sample\"):\n                # This must be deleted due to how `privacy_utils::supported_layers_grad_samplers.py` works!\n                #   When a parameter with `.grad_sample` is reused, the per-sample gradients are accumulated!\n                del param.grad_sample\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        return norm_sample, coef_sample\n\n    def get_privacy_spent(\n        self,\n        steps: Optional[int] = None,\n        accounting_mode: Optional[str] = None,\n        lenient=False\n    ) -> Dict:\n        if steps is None:\n            steps = self.steps\n        if accounting_mode is None:\n            accounting_mode = self.accounting_mode\n\n        privacy_results = {}  # Contains stats from all modes.\n        if accounting_mode in (AccountingMode.all_, AccountingMode.rdp):\n            try:\n                manager = accounting_manager.RDPManager(alphas=self.alphas)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps,\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\"RDP accounting failed! Double check privacy parameters.\")\n                if not lenient:\n                    raise err\n\n        if accounting_mode in (AccountingMode.all_, AccountingMode.glw):\n            try:\n                manager = accounting_manager.GLWManager(eps_error=self.eps_error)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\n                    \"Numerical composition of tradeoff functions failed! Double check privacy parameters.\"\n                )\n                if not lenient:\n                    raise err\n\n        return privacy_results\n\n    def get_training_stats():\n        \"\"\"Get the clipping, signal, and noise statistics.\"\"\"\n        return {\n            \"med_clip\": self.med_clip,\n            \"max_clip\": self.max_clip,\n            \"min_clip\": self.min_clip,\n            \"snr\": self.snr,\n            \"signal\": self.signal,\n            \"noise\": self.noise,\n            \"noise_limit\": self.noise_limit,\n        }\n\n    def __repr__(self):\n        return (\n            f\"PrivacyEngine(\\n\"\n            f\"  target_epsilon={self.target_epsilon:.6f}, \\n\"\n            f\"  target_delta={self.target_delta:.6f}, \\n\"\n            f\"  noise_multiplier={self.noise_multiplier:.6f}, \\n\"\n            f\"  effective_noise_multiplier={self.effective_noise_multiplier:.6f}, \\n\"\n            f\"  epochs={self.epochs}, \\n\"\n            f\"  max_grad_norm={self.max_grad_norm}, \\n\"\n            f\"  sample_rate={self.sample_rate}, \\n\"\n            f\"  batch_size={self.batch_size}, \\n\"\n            f\"  accounting_mode={self.accounting_mode}, \\n\"\n            f\"  clipping_mode={self.clipping_mode}\\n\"\n            f\")\"\n        )\n\n\n# File: private_transformers/accounting/accounting_manager.py\nimport abc\nimport math\nfrom typing import Dict, Optional, Union\n\nfrom . import rdp_accounting\n\nDEFAULT_ALPHAS = tuple(1 + x / 10.0 for x in range(1, 100)) + tuple(range(12, 64))  # RDP.\n\n\nclass AccountingManager(abc.ABC):\n    def _get_sigma_with_target_epsilon(\n        self,\n        target_epsilon,\n        target_delta,\n        sample_rate,\n        steps,\n        threshold,\n        sigma_hi_init,\n        sigma_lo_init,\n    ):\n        \"\"\"Binary search σ given ε and δ.\"\"\"\n        if sigma_lo_init > sigma_hi_init:\n            raise ValueError(\"`sigma_lo` should be smaller than `sigma_hi`.\")\n\n        # Find an appropriate region for binary search.\n        sigma_hi = sigma_hi_init\n        sigma_lo = sigma_lo_init\n\n        # Ensure sigma_hi isn't too small.\n        while True:\n            eps = self._compute_epsilon_from_sigma(sigma_hi, sample_rate, target_delta, steps)\n            if eps < target_epsilon:\n                break\n            sigma_hi *= 2\n\n        # Ensure sigma_lo isn't too large.\n        while True:\n            eps = self._compute_epsilon_from_sigma(sigma_lo, sample_rate, target_delta, steps)\n            if eps > target_epsilon:\n                break\n            sigma_lo /= 2\n\n        # Binary search.\n        while sigma_hi - sigma_lo > threshold:\n            sigma = (sigma_hi + sigma_lo) / 2\n            eps = self._compute_epsilon_from_sigma(sigma, sample_rate, target_delta, steps)\n            if eps < target_epsilon:\n                sigma_hi = sigma\n            else:\n                sigma_lo = sigma\n\n        # Conservative estimate.\n        return sigma_hi\n\n    @abc.abstractmethod\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        \"\"\"Override for reporting results.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps) -> float:\n        \"\"\"Override for binary sigma search.\"\"\"\n        raise NotImplementedError\n\n    def compute_sigma(\n        self,\n        target_epsilon: float,\n        target_delta: float,\n        sample_rate: float,\n        epochs: Optional[Union[float, int]] = None,\n        steps=None,\n        threshold=1e-3,\n        sigma_hi_init=4,\n        sigma_lo_init=0.1,\n    ) -> float:\n        if steps is None:\n            if epochs is None:\n                raise ValueError(\"Epochs and steps cannot both be None.\")\n            steps = math.ceil(epochs / sample_rate)\n        return self._get_sigma_with_target_epsilon(\n            target_epsilon=target_epsilon,\n            target_delta=target_delta,\n            sample_rate=sample_rate,\n            steps=steps,\n            threshold=threshold,\n            sigma_hi_init=sigma_hi_init,\n            sigma_lo_init=sigma_lo_init,\n        )\n\n\nclass RDPManager(AccountingManager):\n    def __init__(self, alphas):\n        super(RDPManager, self).__init__()\n        self._alphas = alphas\n\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):\n        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)[\"eps_rdp\"]\n\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        \"\"\"Compute RDP as usual, but convert to (ε, δ)-DP based on the result by Canonne, Kamath, Steinke.\"\"\"\n        rdp = rdp_accounting.compute_rdp(q=sample_rate, noise_multiplier=sigma, steps=steps, orders=self._alphas)\n        eps, alpha = rdp_accounting.get_privacy_spent(orders=self._alphas, rdp=rdp, delta=target_delta)\n        return dict(eps_rdp=eps, alpha_rdp=alpha)\n\n\nclass GLWManager(AccountingManager):\n    def __init__(self, eps_error=0.05):\n        super(GLWManager, self).__init__()\n        self._eps_error = eps_error\n\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):\n        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)[\"eps_upper\"]  # Be conservative.\n\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        if steps == 0:\n            return dict(eps_low=None, eps_estimate=None, eps_upper=None)\n\n        from prv_accountant import Accountant\n        accountant = Accountant(\n            noise_multiplier=sigma,\n            sampling_probability=sample_rate,\n            delta=target_delta,\n            eps_error=self._eps_error,\n            max_compositions=steps\n        )\n        eps_low, eps_estimate, eps_upper = accountant.compute_epsilon(num_compositions=steps)\n        return dict(eps_low=eps_low, eps_estimate=eps_estimate, eps_upper=eps_upper)\n\n\n# File: private_transformers/autograd_grad_sample.py\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .settings import BackwardHookMode\nfrom .supported_layers_grad_samplers import _supported_layers_grad_samplers\n\n_hooks_disabled: bool = False\n_hooks_mode = BackwardHookMode.default\n\n\ndef set_hooks_mode(mode):\n    if mode not in BackwardHookMode.all():\n        raise ValueError(f\"Unknown mode for hooks: {mode}; expected one of {BackwardHookMode.all()}.\")\n\n    global _hooks_mode\n    _hooks_mode = mode  # Set mode.\n\n    if _hooks_mode == BackwardHookMode.ghost_grad:  # Second backward pass of ghost clipping doesn't need hooks.\n        disable_hooks()\n    elif _hooks_mode == BackwardHookMode.ghost_norm:  # First backward pass of ghost clipping needs to accumulate norms.\n        enable_hooks()\n\n\ndef get_hooks_mode():\n    global _hooks_mode\n    return _hooks_mode\n\n\ndef requires_grad(module: nn.Module, recurse: bool = False) -> bool:\n    \"\"\"\n    Checks if any parameters in a specified module require gradients.\n\n    Args:\n        module: PyTorch module whose parameters are examined\n        recurse: Flag specifying if the gradient requirement check should\n            be applied recursively to sub-modules of the specified module\n\n    Returns:\n        Flag indicate if any parameters require gradients\n    \"\"\"\n    return any(p.requires_grad for p in module.parameters(recurse))\n\n\ndef add_hooks(model: nn.Module, loss_reduction: str = \"mean\"):\n    r\"\"\"\n    Adds hooks to model to save activations and backprop values.\n    The hooks will\n\n    1. save activations into ``param.activations`` during forward pass.\n    2. compute per-sample gradients and save them in ``param.grad_sample`` during backward pass.\n\n    Args:\n        model: Model to which hooks are added.\n        loss_reduction: Indicates if the loss reduction (for aggregating the gradients) is a sum or a mean operation.\n            Can take values ``sum`` or ``mean``.\n    \"\"\"\n    if hasattr(model, \"autograd_grad_sample_hooks\"):\n        raise ValueError(\"Trying to add hooks twice to the same model\")\n\n    enable_hooks()\n\n    handles = []\n    for name, layer in model.named_modules():\n        if type(layer) in _supported_layers_grad_samplers:\n            if requires_grad(layer, recurse=False):\n                handles.append(layer.register_forward_hook(_capture_activations))\n\n                def this_backward(this_layer, grad_input, grad_output):\n                    return _capture_backprops(this_layer, grad_input, grad_output, loss_reduction)\n\n                # Starting with 1.8.0, use `register_full_backward_hook`.\n                handles.append(layer.register_backward_hook(this_backward))\n\n    model.__dict__.setdefault(\"autograd_grad_sample_hooks\", []).extend(handles)\n\n\ndef remove_hooks(model: nn.Module):\n    \"\"\"Removes hooks added by `add_hooks()`.\"\"\"\n    if not hasattr(model, \"autograd_grad_sample_hooks\"):\n        raise ValueError(\"Asked to remove hooks, but no hooks found\")\n    else:\n        for handle in model.autograd_grad_sample_hooks:\n            handle.remove()\n        del model.autograd_grad_sample_hooks\n\n\ndef disable_hooks():\n    \"\"\"Globally disables all hooks installed by this library.\"\"\"\n    global _hooks_disabled\n    _hooks_disabled = True\n\n\ndef enable_hooks():\n    \"\"\"Globally enables all hooks installed by this library.\"\"\"\n    global _hooks_disabled\n    _hooks_disabled = False\n\n\ndef _capture_activations(layer: nn.Module, inputs: Tuple, outputs: Tuple):\n    \"\"\"Forward hook handler captures and saves activations.\"\"\"\n    if not requires_grad(layer) or not layer.training or _hooks_disabled:\n        return\n\n    if not hasattr(layer, \"activations\"):\n        layer.activations = []\n\n    # This improves on original Opacus and supports additional arguments on top of the (first) activation tensor.\n    stored_inputs = tuple(input_i.detach() if torch.is_tensor(input_i) else input_i for input_i in inputs)\n    layer.activations.append(stored_inputs)\n\n\ndef _capture_backprops(\n    layer: nn.Module,\n    inputs: Tuple[torch.Tensor],\n    outputs: Tuple[torch.Tensor],\n    loss_reduction: str\n):\n    \"\"\"Backward hook handler captures grad_outputs.\"\"\"\n    # This improves on the original Opacus codebase and supports multiple outputs.\n    backprops = tuple(output_i.detach() if torch.is_tensor(output_i) else output_i for output_i in outputs)\n    _compute_grad_sample(layer, backprops, loss_reduction)\n\n\ndef _compute_grad_sample(layer: nn.Module, backprops: Tuple, loss_reduction: str):\n    \"\"\"Computes per-sample gradients with respect to the parameters.\"\"\"\n    if not requires_grad(layer) or not layer.training or _hooks_disabled:\n        return\n\n    if not hasattr(layer, \"activations\"):\n        raise ValueError(f\"No activations detected for {type(layer)}, run forward after add_hooks(model)\")\n\n    # Outside of the LSTM there is \"batch_first\" but not for the Linear inside the LSTM\n    if isinstance(layer.activations, list):\n        A = layer.activations.pop()\n    else:\n        A = layer.activations\n\n    if not hasattr(layer, \"max_batch_len\"):\n        assert torch.is_tensor(A[0]), f\"Internal error: first input of the following layer isn't a Tensor. \\n{layer}\"\n        layer.max_batch_len = _get_batch_size(layer, A[0])\n\n    n = layer.max_batch_len\n    if loss_reduction == \"mean\":\n        B = tuple(B_i * n if torch.is_tensor(B_i) else B_i for B_i in backprops)\n    elif loss_reduction == \"sum\":\n        B = backprops\n    else:\n        raise ValueError(f\"loss_reduction = {loss_reduction}. Only 'sum' and 'mean' losses are supported\")\n\n    # compute grad sample for individual layers\n    compute_layer_grad_sample = _supported_layers_grad_samplers.get(type(layer))\n    compute_layer_grad_sample(layer, A, B)\n\n    if (not isinstance(layer.activations, list) or len(layer.activations) == 0) and hasattr(layer, \"max_batch_len\"):\n        del layer.max_batch_len\n\n\ndef _get_batch_size(layer: nn.Module, grad_sample: torch.Tensor) -> int:\n    r\"\"\"\n    Computes and returns the maximum batch size which is the maximum of the dimension values\n    along 'batch_dim' axis over layer.activations + [grad_sample], where layer.activations is\n    a list. If layer.activations is a not a list, then return grad_sample.shape[batch_dim].\n    \"\"\"\n\n    batch_dim = 0\n    max_batch_len = 0\n    if isinstance(layer.activations, list):\n        for out in layer.activations:\n            assert torch.is_tensor(out[0]), (\n                f\"Internal error: first input of the following layer isn't a Tensor. \\n{layer}\"\n            )\n            if out[0].shape[batch_dim] > max_batch_len:\n                max_batch_len = out[0].shape[batch_dim]\n\n    max_batch_len = max(max_batch_len, grad_sample.shape[batch_dim])\n    return max_batch_len\n\n\n# File: examples/classification/run_classification.py\nimport collections\nimport copy\nimport dataclasses\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport gc\nimport json\nimport logging\nimport os\nfrom typing import Callable, Dict, Optional\n\nfrom filelock import FileLock\nimport numpy as np\nfrom ml_swissknife import utils\nimport torch\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction\nfrom transformers import GlueDataTrainingArguments as DataTrainingArguments\nfrom transformers import GlueDataset\nfrom transformers import HfArgumentParser, set_seed\n\nfrom private_transformers import PrivacyEngine\nfrom .src.common import true_tags\nfrom .src.compiled_args import PrivacyArguments, TrainingArguments, AuxiliaryArguments\nfrom .src.dataset import FewShotDataset\nfrom .src.models import (\n    BertForPromptFinetuning, RobertaForPromptFinetuning, AlbertForPromptFinetuning, DistilBertForPromptFinetuning,\n    resize_token_type_embeddings\n)\nfrom .src.processors import num_labels_mapping, output_modes_mapping, compute_metrics_mapping, bound_mapping\nfrom .src.trainer import Trainer\n\nlogger = logging.getLogger(__name__)\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n    )\n    # Few-shot type\n    #   - finetune: standard fine-tuning\n    #   - prompt: prompt-based fine-tuning\n    #   - prompt-demo: prompt-based fine-tuning with demonstrations\n    few_shot_type: str = field(\n        default='prompt-demo',\n        metadata={\"help\": \"Few-shot learning model type. Choice: finetune, prompt, prompt-demo\"}\n    )\n\n    # Only for BERT-type model\n    random_segment: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to reinitialize the token type embeddings (only for BERT).\"}\n    )\n\n    static_embedding: str = field(\n        default=\"no\"\n    )\n    static_lm_head: str = field(\n        default=\"no\"\n    )\n    attention_only: str = field(\n        default=\"no\"\n    )\n\n    randomly_initialize: str = field(\n        default=\"no\",\n        metadata={\"help\": \"Randomly initialize the model; useful only for ablation studies.\"}\n    )\n\n    def __post_init__(self):\n        self.static_embedding = self.static_embedding.lower() in true_tags  # noqa\n        self.static_lm_head = self.static_lm_head.lower() in true_tags  # noqa\n        self.attention_only = self.attention_only.lower() in true_tags  # noqa\n        self.randomly_initialize = self.randomly_initialize.lower() in true_tags  # noqa\n\n\n@dataclass\nclass DynamicDataTrainingArguments(DataTrainingArguments):\n    \"\"\"\n    Arguments for dynamic training.\n    \"\"\"\n    num_k: Optional[int] = field(\n        default=16,\n        metadata={\"help\": \"Number of training instances per class\"}\n    )\n\n    num_sample: Optional[int] = field(\n        default=16,\n        metadata={\"help\": \"Number of samples (for inference) in fine-tuning with demonstrations\"}\n    )\n\n    num_demo: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"Number of demonstrations from each class\"}\n    )\n\n    auto_demo: bool = field(\n        default=True,\n        metadata={\"help\": \"Automatically generate template for using demonstrations\"}\n    )\n\n    # For prompting\n    template: str = field(\n        default=None,\n        metadata={\"help\": \"Template\"}\n    )\n\n    mapping: str = field(\n        default=None,\n        metadata={\"help\": \"Label word mapping\"}\n    )\n\n    template_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to a txt file that stores all the templates, one per line. Do not set this when prompt_path \"\n                    \"is used\"}\n    )\n\n    mapping_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to a txt file that stores all the label word mappings, one per line. Do not set this when \"\n                    \"prompt_path is used\"}\n    )\n\n    prompt_path: str = field(\n        default=None,\n        metadata={\"help\": \"Path to a txt file that stores all the prompts (templates and mappings), one per line\"}\n    )\n\n    template_id: int = field(\n        default=None,\n        metadata={\"help\": \"Template id if using template_path\"}\n    )\n\n    mapping_id: int = field(\n        default=None,\n        metadata={\"help\": \"Mapping id if using template_path\"}\n    )\n\n    prompt_id: int = field(\n        default=None,\n        metadata={\"help\": \"Prompt id if using prompt_path\"}\n    )\n\n    top_n_template: int = field(\n        default=None,\n        metadata={\"help\": \"Use top-n template in the template path\"}\n    )\n\n    # For logging\n    tag: str = field(\n        default='',\n        metadata={\"help\": \"Set the tag and find the result easier in the log.\"}\n    )\n\n    # For filtering when using demonstrations\n    demo_filter: bool = field(\n        default=False,\n        metadata={\"help\": \"Only use similar instances in demonstrations\"}\n    )\n\n    demo_filter_rate: float = field(\n        default=0.5,\n        metadata={\"help\": \"Only use top-x\\% similar instances in demonstrations\"}\n    )\n\n    demo_filter_model: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Model name for demonstration filter embeddings. Will load embeddings based on the model name.\"}\n    )\n\n    debug_mode: bool = field(\n        default=False,\n        metadata={\"help\": \"Debug mode\"}\n    )\n\n    # For max length\n    double_demo: bool = field(\n        default=False,\n        metadata={\"help\": \"Use double length for using demonstrations\"}\n    )\n\n    first_sent_limit: int = field(\n        default=None,\n        metadata={\"help\": \"Limit the length of the first sentence (i.e., sent_0)\"}\n    )\n\n    other_sent_limit: int = field(\n        default=None,\n        metadata={\"help\": \"Limit the length of sentences other than the first sentence\"}\n    )\n\n    use_full_length: bool = field(\n        default=None,\n        metadata={\"help\": \"Use the full length (512)\"}\n    )\n\n    # GPT-3's in-context learning\n    gpt3_in_context_head: bool = field(\n        default=False,\n        metadata={\"help\": \"GPT-3's in-context learning (context at the beginning)\"}\n    )\n\n    gpt3_in_context_tail: bool = field(\n        default=False,\n        metadata={\"help\": \"GPT-3's in-context learning (context at the end)\"}\n    )\n\n    gpt3_in_context_num: int = field(\n        default=32,\n        metadata={\"help\": \"Number of context examples\"}\n    )\n\n    truncate_head: bool = field(\n        default=False,\n        metadata={\"help\": \"When exceeding the maximum length, truncate the head instead of the tail.\"}\n    )\n\n    # Do not set up the following fields. They are set up automatically.\n    prompt: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use prompt-based fine-tuning\"}\n    )\n    template_list: tuple = field(\n        default=None,\n        metadata={\"help\": \"(DO NOT List of templates (only initialized after the program starts.\"}\n    )\n\n    inference_time_demo: bool = field(\n        default=False,\n        metadata={\"help\": \"Do not use demonstrations during inference time; \"\n                          \"the original paper attaches to each test example a few training examples as demo -- \"\n                          \"apparently this breaks privacy. We turn this off by default here.\"}\n    )\n\n\n@dataclass\nclass DynamicTrainingArguments(TrainingArguments):\n    # For ensemble\n    array_id: int = field(\n        default=-1,\n        metadata={\"help\": \"Array ID (contains seed and hyper-paramter search) to idenfity the model\"}\n    )\n\n    model_id: int = field(\n        default=-1,\n        metadata={\"help\": \"Model ID (contains template information) to identify the model\"}\n    )\n\n    save_logit: bool = field(\n        default=False,\n        metadata={\"help\": \"Save test file logit with name $TASK-$MODEL_ID-$ARRAY_ID.npy\"}\n    )\n\n    save_logit_dir: str = field(\n        default=None,\n        metadata={\"help\": \"Where to save the prediction result\"}\n    )\n\n    # Regularization\n    fix_layers: int = field(\n        default=0,\n        metadata={\"help\": \"Fix bottom-n layers when optimizing\"}\n    )\n\n    # Training\n    save_at_last: bool = field(\n        default=False,\n        metadata={\"help\": \"Instead of saving the best (dev performance) checkpoint, save the last checkpoint\"}\n    )\n\n    # Turn off train/test\n    no_train: bool = field(\n        default=False,\n        metadata={\"help\": \"No training\"}\n    )\n    no_predict: bool = field(\n        default=False,\n        metadata={\"help\": \"No test\"}\n    )\n\n    evaluate_after_training: bool = field(\n        default=True, metadata={\"help\": \"Always run evaluation after training ends.\"}\n    )\n\n    def __post_init__(self):\n        super(DynamicTrainingArguments, self).__post_init__()\n\n\ndef main():\n    parser = HfArgumentParser(\n        (ModelArguments, DynamicDataTrainingArguments, DynamicTrainingArguments, PrivacyArguments, AuxiliaryArguments)\n    )\n    model_args, data_args, training_args, privacy_args, auxiliary_args = parser.parse_args_into_dataclasses()\n\n    if not os.path.exists(training_args.output_dir):\n        print(f\"output_dir doesn't exists, mkdir now: {training_args.output_dir}\")\n        os.makedirs(training_args.output_dir)\n\n    if 'prompt' in model_args.few_shot_type:\n        data_args.prompt = True\n\n    if training_args.no_train:\n        training_args.do_train = False\n    if training_args.no_predict:\n        training_args.do_predict = False\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n    )\n\n    # TODO: Hacky mapping creation. Refactor this in the future.\n    #  Currently gets replace if mapping_id and mapping_path is set.\n    if data_args.task_name == \"sst-2\":\n        data_args.mapping = \"{'0':'terrible','1':'great'}\"\n    elif data_args.task_name == \"mnli\":\n        data_args.mapping = \"{'contradiction': 'no', 'entailment': 'yes', 'neutral': 'maybe'}\"\n    elif data_args.task_name == \"qnli\":\n        data_args.mapping = \"{'not_entailment': 'no', 'entailment': 'yes'}\"\n    elif data_args.task_name == \"qqp\":\n        data_args.mapping = \"{'1': 'yes', '0': 'no'}\"  # 1 -- equivalent, 0 -- not equivalent.\n    else:\n        raise ValueError(f\"Unknown task: {data_args.task_name}\")\n\n    # Load prompt/template/mapping file\n    if data_args.prompt:\n        if data_args.prompt_path is not None:\n            assert data_args.prompt_id is not None\n            prompt_list = []\n            with open(data_args.prompt_path) as f:\n                for line in f:\n                    line = line.strip()\n                    template, mapping = line.split('\\t')\n                    prompt_list.append((template, mapping))\n\n            data_args.template, data_args.mapping = prompt_list[data_args.prompt_id]\n            logger.info(\n                \"Specify load the %d-th prompt: %s | %s\" % (data_args.prompt_id, data_args.template, data_args.mapping))\n        else:\n            if data_args.template_path is not None:\n                with open(data_args.template_path) as f:\n                    data_args.template_list = []\n                    for line in f:\n                        line = line.strip()\n                        if len(line) > 0:\n                            data_args.template_list.append(line)\n\n                # Load top-n templates\n                if data_args.top_n_template is not None:\n                    data_args.template_list = data_args.template_list[:data_args.top_n_template]\n                logger.info(\"Load top-%d templates from %s\" % (len(data_args.template_list), data_args.template_path))\n\n                # ... or load i-th template\n                if data_args.template_id is not None:\n                    data_args.template = data_args.template_list[data_args.template_id]\n                    data_args.template_list = None\n                    logger.info(\"Specify load the %d-th template: %s\" % (data_args.template_id, data_args.template))\n\n            if data_args.mapping_path is not None:\n                assert data_args.mapping_id is not None  # Only can use one label word mapping\n                with open(data_args.mapping_path) as f:\n                    mapping_list = []\n                    for line in f:\n                        line = line.strip()\n                        mapping_list.append(line)\n\n                data_args.mapping = mapping_list[data_args.mapping_id]\n                logger.info(\"Specify using the %d-th mapping: %s\" % (data_args.mapping_id, data_args.mapping))\n\n    # Check save path\n    if (\n        os.path.exists(training_args.output_dir)\n        and os.listdir(training_args.output_dir)\n        and training_args.do_train\n        and not training_args.overwrite_output_dir\n    ):\n        raise ValueError(f\"Output directory ({training_args.output_dir}) already exists.\")\n\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        training_args.local_rank,\n        training_args.device,\n        training_args.n_gpu,\n        bool(training_args.local_rank != -1),\n        training_args.fp16,\n    )\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n\n    # Set seed\n    set_seed(training_args.seed)\n\n    try:\n        num_labels = num_labels_mapping[data_args.task_name]\n        output_mode = output_modes_mapping[data_args.task_name]\n        logger.info(\n            \"Task name: {}, number of labels: {}, output mode: {}\".format(data_args.task_name, num_labels, output_mode))\n    except KeyError:\n        raise ValueError(\"Task not found: %s\" % (data_args.task_name))\n\n    # Automatically generate template for using demonstrations\n    if data_args.auto_demo and model_args.few_shot_type == 'prompt-demo':\n        # GPT-3's in-context learning\n        if data_args.gpt3_in_context_head or data_args.gpt3_in_context_tail:\n            logger.info(\"Automatically convert the template to GPT-3's in-context learning.\")\n            assert data_args.template_list is None\n\n            old_template = data_args.template\n            new_template = old_template + ''\n            old_template = old_template.replace('*cls*', '')\n            # Single sentence or sentence pair?\n            sent_num = 1\n            if \"_1\" in old_template:\n                sent_num = 2\n            for instance_id in range(data_args.gpt3_in_context_num):\n                sub_template = old_template + ''\n                # Replace sent_id\n                for sent_id in range(sent_num):\n                    sub_template = sub_template.replace(\"_{}*\".format(sent_id),\n                                                        \"_{}*\".format(sent_num + sent_num * instance_id + sent_id))\n                # Replace mask\n                sub_template = sub_template.replace(\"*mask*\", \"*labelx_{}*\".format(instance_id))\n                if data_args.gpt3_in_context_tail:\n                    new_template = new_template + sub_template  # Put context at the end\n                else:\n                    new_template = sub_template + new_template  # Put context at the beginning\n            logger.info(\"| {} => {}\".format(data_args.template, new_template))\n            data_args.template = new_template\n        else:\n            logger.info(\"Automatically convert the template to using demonstrations.\")\n            if data_args.template_list is not None:\n                for i in range(len(data_args.template_list)):\n                    old_template = data_args.template_list[i]\n                    new_template = old_template + ''\n                    old_template = old_template.replace('*cls*', '')\n                    # Single sentence or sentence pair?\n                    sent_num = 1\n                    if \"_1\" in old_template:\n                        sent_num = 2\n                    for label_id in range(num_labels):\n                        sub_template = old_template + ''\n                        # Replace sent id\n                        for sent_id in range(sent_num):\n                            sub_template = sub_template.replace(\"_{}*\".format(sent_id),\n                                                                \"_{}*\".format(sent_num + sent_num * label_id + sent_id))\n                        # Replace mask\n                        sub_template = sub_template.replace(\"*mask*\", \"*label_{}*\".format(label_id))\n                        new_template = new_template + sub_template\n                    logger.info(\"| {} => {}\".format(data_args.template_list[i], new_template))\n                    data_args.template_list[i] = new_template\n            else:\n                old_template = data_args.template\n                new_template = old_template + ''\n                old_template = old_template.replace('*cls*', '')\n                # Single sentence or sentence pair?\n                sent_num = 1\n                if \"_1\" in old_template:\n                    sent_num = 2\n                for label_id in range(num_labels):\n                    sub_template = old_template + ''\n                    # Replace sent id\n                    for sent_id in range(sent_num):\n                        sub_template = sub_template.replace(\"_{}\".format(sent_id),\n                                                            \"_{}\".format(sent_num + sent_num * label_id + sent_id))\n                    # Replace mask\n                    sub_template = sub_template.replace(\"*mask*\", \"*label_{}*\".format(label_id))\n                    new_template = new_template + sub_template\n                logger.info(\"| {} => {}\".format(data_args.template, new_template))\n                data_args.template = new_template\n\n    # Create config\n    config = AutoConfig.from_pretrained(\n        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n        num_labels=num_labels,\n        finetuning_task=data_args.task_name,\n        cache_dir=model_args.cache_dir,\n    )\n\n    if 'prompt' in model_args.few_shot_type:\n        if config.model_type == 'roberta':\n            model_fn = RobertaForPromptFinetuning\n        elif config.model_type == 'bert':\n            model_fn = BertForPromptFinetuning\n        elif config.model_type == 'albert':\n            model_fn = AlbertForPromptFinetuning\n        elif config.model_type == 'distilbert':\n            model_fn = DistilBertForPromptFinetuning\n        else:\n            raise NotImplementedError\n    elif model_args.few_shot_type == 'finetune':\n        model_fn = AutoModelForSequenceClassification\n    else:\n        raise NotImplementedError\n    special_tokens = []\n\n    # Create tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n        additional_special_tokens=special_tokens,\n        cache_dir=model_args.cache_dir, use_fast=False\n    )\n    print(f' | tokenizer: {tokenizer}, size: {len(tokenizer)} \\n\\n\\n')\n\n    # Get our special datasets.\n    if model_args.few_shot_type == \"finetune\":\n        assert data_args.num_sample == 1\n        train_dataset = GlueDataset(data_args, tokenizer, mode=\"train\")\n        eval_dataset = (\n            GlueDataset(data_args, tokenizer, mode=\"dev\")\n            if training_args.do_eval else None\n        )\n        test_dataset = (\n            GlueDataset(data_args, tokenizer, mode=\"test\")\n            if training_args.do_predict or training_args.evaluate_test_split\n            else None\n        )\n\n        if eval_dataset is not None:\n            eval_dataset.num_sample = 1\n        if test_dataset is not None:\n            test_dataset.num_sample = 1\n    else:\n        use_demo = \"demo\" in model_args.few_shot_type\n        train_dataset = FewShotDataset(data_args, tokenizer=tokenizer, mode=\"train\", use_demo=use_demo)\n        eval_dataset = (\n            FewShotDataset(data_args, tokenizer=tokenizer, mode=\"dev\", use_demo=use_demo)\n            if training_args.do_eval else None\n        )\n        test_dataset = (\n            FewShotDataset(data_args, tokenizer=tokenizer, mode=\"test\", use_demo=use_demo)\n            if training_args.do_predict or training_args.evaluate_test_split else None\n        )\n    print(f\" *** dataset sizes: \")\n    for _tag, _ds in zip((\"train\", \"valid\", \"test\"), (train_dataset, eval_dataset, test_dataset)):\n        if _ds is not None:\n            print(f'{_tag}: {len(_ds)}')\n    print(f\" ***\")\n\n    set_seed(training_args.seed)\n\n    model = model_fn.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n    )\n    print(\" | model type: \")\n    print(type(model))\n\n    if model_args.attention_only:\n        model.requires_grad_(False)\n        for name, param in model.named_parameters():\n            if 'query' in name or 'value' in name or 'classifier' in name or 'lm_head' in name:\n                param.requires_grad_(True)\n        if model_args.static_lm_head and hasattr(model, 'lm_head'):\n            model.lm_head.requires_grad_(False)\n    else:\n        model.requires_grad_(True)\n        if model_args.static_embedding:\n            model.get_input_embeddings().requires_grad_(False)\n\n    if model_args.randomly_initialize:\n        # Only reinit the params which require gradients.\n        model_old = copy.deepcopy(model)  # Copy pretrained model.\n        model.init_weights()\n\n        params = tuple(model.parameters())\n        params_old = tuple(model_old.parameters())\n        for param, param_old in utils.zip_(params, params_old):\n            if not param.requires_grad:\n                param.data.copy_(param_old.data)\n\n        del model_old\n        gc.collect()\n        torch.cuda.empty_cache()\n    print(f\"attention_only: {model_args.attention_only} | randomly_initialize: {model_args.randomly_initialize}\")\n\n    named_params = [(name, param) for name, param in model.named_parameters() if param.requires_grad]\n    print('Params to update: ')\n    print(json.dumps([name for name, param in named_params], indent=4))\n    num_differentiable_params = utils.count_parameters(model, only_differentiable=True)\n    print(f'Number of differentiable params: {num_differentiable_params / 1e6:.3f} million')\n\n    # For BERT, increase the size of the segment (token type) embeddings\n    if config.model_type == 'bert':\n        model.resize_token_embeddings(len(tokenizer))\n        resize_token_type_embeddings(model, new_num_types=10, random_segment=model_args.random_segment)\n\n    # Pass dataset and argument information to the model\n    if data_args.prompt:\n        model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()\n        print(f\" | Classification label_word_list: {model.label_word_list}\")\n        print(f\"   converted words: {tokenizer.convert_ids_to_tokens(model.label_word_list)}\")\n    if output_modes_mapping[data_args.task_name] == 'regression':\n        # lower / upper bounds\n        model.lb, model.ub = bound_mapping[data_args.task_name]\n        print(f\" | Regression lb: {model.lb}, ub: {model.ub}\")\n    model.model_args = model_args\n    model.data_args = data_args\n    model.tokenizer = tokenizer\n\n    # Build metric\n    def build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:\n        def compute_metrics_fn(p: EvalPrediction):\n            # Note: the eval dataloader is sequential, so the examples are in order.\n            # We average the logits over each sample for using demonstrations.\n            predictions = p.predictions\n            num_logits = predictions.shape[-1]\n            logits = predictions.reshape([eval_dataset.num_sample, -1, num_logits])\n            logits = logits.mean(axis=0)\n\n            if num_logits == 1:\n                preds = np.squeeze(logits)\n            else:\n                preds = np.argmax(logits, axis=1)\n\n            # Just for sanity, assert label ids are the same.\n            label_ids = p.label_ids.reshape([eval_dataset.num_sample, -1])\n            label_ids_avg = label_ids.mean(axis=0)\n            label_ids_avg = label_ids_avg.astype(p.label_ids.dtype)\n            assert (label_ids_avg - label_ids[0]).mean() < 1e-2\n            label_ids = label_ids[0]\n\n            return compute_metrics_mapping[task_name](task_name, preds, label_ids)\n\n        return compute_metrics_fn\n\n    # Initialize our Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        model_args=model_args,\n        privacy_args=privacy_args,\n        auxiliary_args=auxiliary_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=build_compute_metrics_fn(data_args.task_name)\n    )\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)],\n         'weight_decay': training_args.weight_decay},\n        {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = trainer.optimizer = torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=training_args.learning_rate,\n        betas=(training_args.adam_beta1, training_args.adam_beta2),\n        eps=training_args.adam_epsilon,\n    )\n    if training_args.lr_decay:  # Default linear decay.\n        training_setup = trainer.get_training_setup()\n        t_total = training_setup[\"t_total\"]\n        # `trainer.optimizer` is not None here, so no optimizer is created.\n        trainer.create_optimizer_and_scheduler(num_training_steps=t_total)\n    else:\n        trainer.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(trainer.optimizer, lambda _: 1.)\n\n    if privacy_args.non_private:\n        privacy_args.noise_multiplier = 0.\n        privacy_args.per_example_max_grad_norm = None\n    else:\n        total_train_batch_size = training_args.gradient_accumulation_steps * training_args.per_device_train_batch_size\n        privacy_engine = PrivacyEngine(\n            module=model,\n            batch_size=total_train_batch_size,\n            sample_size=len(train_dataset),\n            epochs=training_args.num_train_epochs,\n            max_grad_norm=privacy_args.per_example_max_grad_norm,\n            noise_multiplier=privacy_args.noise_multiplier,\n            target_epsilon=privacy_args.target_epsilon,\n            target_delta=privacy_args.target_delta,\n            accounting_mode=privacy_args.accounting_mode,\n            clipping_mode=privacy_args.clipping_mode,\n            skip_checks=True,\n        )\n        # Originally, it could have been null.\n        privacy_args.noise_multiplier = privacy_engine.noise_multiplier\n        privacy_args.target_delta = privacy_engine.target_delta\n\n        print('privacy_args: ')\n        print(json.dumps(privacy_args.__dict__, indent=4))\n        privacy_engine.attach(optimizer)\n\n    # Training\n    if training_args.do_train:\n        # Write argparse.\n        utils.jdump(\n            {**training_args.__dict__, **model_args.__dict__, **data_args.__dict__, **privacy_args.__dict__},\n            os.path.join(training_args.output_dir, 'argparse.json'),\n            default=lambda x: str(x),\n        )\n        print(data_args.mapping)\n        print(data_args.template)\n\n        # Don't reload.\n        trainer.train(model_path=None)\n        # Use the early stop, so do not save the model in the end (unless specify save_at_last)\n        if training_args.save_at_last:\n            trainer.save_model(training_args.output_dir)\n\n        if trainer.is_world_process_zero():\n            tokenizer.save_pretrained(training_args.output_dir)\n            torch.save(model_args, os.path.join(training_args.output_dir, \"model_args.bin\"))\n            torch.save(data_args, os.path.join(training_args.output_dir, \"data_args.bin\"))\n\n    if training_args.do_eval or training_args.do_predict:\n        # Reload the best checkpoint (for eval or predict).\n        logger.info(\"*** Loading best checkpoint ***\")\n        model = model_fn.from_pretrained(training_args.output_dir)\n        model = model.to(training_args.device)\n        trainer.model = model\n        if data_args.prompt:\n            model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()\n        if output_modes_mapping[data_args.task_name] == 'regression':\n            # lower / upper bounds\n            model.lb, model.ub = bound_mapping[data_args.task_name]\n        model.model_args = model_args\n        model.data_args = data_args\n        model.tokenizer = tokenizer\n\n    # Evaluation\n    final_result = {'time': str(datetime.today())}\n\n    eval_results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Validate ***\")\n\n        eval_datasets = []\n        eval_task_names = []\n        eval_splits = []\n        for split, dataset in zip(('dev', 'test'), (eval_dataset, test_dataset)):\n            if split == \"test\" and not training_args.evaluate_test_split:\n                continue\n\n            eval_datasets.append(dataset)\n            eval_task_names.append(data_args.task_name)\n            eval_splits.append(split)\n\n            # --- lxuechen: This block depends on `split`.\n            if data_args.task_name == \"mnli\":\n                mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n                eval_task_names.append(mnli_mm_data_args.task_name)\n                eval_splits.append(split)\n                if model_args.few_shot_type == \"finetune\":\n                    mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode=split)\n                    mnli_mm_dataset.num_sample = 1\n                    eval_datasets.append(mnli_mm_dataset)\n                else:\n                    eval_datasets.append(\n                        FewShotDataset(\n                            mnli_mm_data_args, tokenizer=tokenizer, mode=split,\n                            use_demo=('demo' in model_args.few_shot_type),\n                        )\n                    )\n            # ---\n\n        results_json = collections.defaultdict(dict)\n        for eval_dataset, eval_task_name, eval_split in zip(eval_datasets, eval_task_names, eval_splits):\n            trainer.compute_metrics = build_compute_metrics_fn(eval_dataset.args.task_name)\n            output = trainer.evaluate(eval_dataset=eval_dataset)\n            eval_result = output.metrics\n\n            # --- lxuechen: My evaluation procedure.\n            if eval_result is not None:\n                if not privacy_args.non_private:\n                    privacy_spent = privacy_engine.get_privacy_spent(accounting_mode=\"all\", lenient=True)\n                    to_record_dict = {**eval_result, **privacy_spent}\n                else:\n                    to_record_dict = eval_result\n\n                if training_args.evaluate_test_split:\n                    results_json[eval_split][eval_task_name] = to_record_dict\n                else:\n                    results_json[eval_task_name] = to_record_dict\n            # ---\n\n        output_path = os.path.join(training_args.output_dir, \"final_results.json\")\n        utils.jdump(results_json, output_path)\n\n    test_results = {}\n    if training_args.do_predict:\n        logging.info(\"*** Test ***\")\n\n        test_datasets = [test_dataset]\n        if data_args.task_name == \"mnli\":\n            mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n            if model_args.few_shot_type == \"finetune\":\n                mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode=\"test\")\n                mnli_mm_dataset.num_sample = 1\n                test_datasets.append(mnli_mm_dataset)\n            else:\n                test_datasets.append(\n                    FewShotDataset(\n                        mnli_mm_data_args,\n                        tokenizer=tokenizer, mode=\"test\", use_demo=('demo' in model_args.few_shot_type)\n                    )\n                )\n\n        for test_dataset in test_datasets:\n            trainer.compute_metrics = build_compute_metrics_fn(test_dataset.args.task_name)\n            output = trainer.evaluate(eval_dataset=test_dataset)\n            test_result = output.metrics\n\n            output_test_file = os.path.join(\n                training_args.output_dir, f\"test_results_{test_dataset.args.task_name}.txt\"\n            )\n            if trainer.is_world_process_zero():\n                with open(output_test_file, \"w\") as writer:\n                    logger.info(\"***** Test results {} *****\".format(test_dataset.args.task_name))\n                    for key, value in test_result.items():\n                        logger.info(\"  %s = %s\", key, value)\n                        writer.write(\"%s = %s\\n\" % (key, value))\n                        final_result[test_dataset.args.task_name + '_test_' + key] = value\n\n                if training_args.save_logit:\n                    predictions = output.predictions\n                    num_logits = predictions.shape[-1]\n                    logits = predictions.reshape([test_dataset.num_sample, -1, num_logits]).mean(axis=0)\n                    np.save(os.path.join(training_args.save_logit_dir,\n                                         \"{}-{}-{}.npy\".format(test_dataset.task_name, training_args.model_id,\n                                                               training_args.array_id)), logits)\n\n            test_results.update(test_result)\n\n    with FileLock('log.lock'):\n        with open('log', 'a') as f:\n            final_result.update(vars(model_args))\n            final_result.update(vars(training_args))\n            final_result.update(vars(data_args))\n            if 'evaluation_strategy' in final_result:\n                final_result.pop('evaluation_strategy')\n            f.write(str(final_result) + '\\n')\n\n    return eval_results\n\n\nif __name__ == \"__main__\":\n    main()\n\n",
        "experimental_info": "- **Hyperparameter Optimization (HPO) for learning rate (η) and number of steps (T) via `r = η × T`**: The `PrivacyEngine` is designed to calculate the `noise_multiplier` (sigma) given a target epsilon and number of steps/epochs (`steps = math.ceil(epochs / sample_rate)`), which serves as the building block for the proposed linear scaling rule. The method describes an HPO strategy involving fitting a linear approximation to empirical points (ε, r(ε)), which is a high-level conceptual strategy not directly implemented as a single function within the provided code, but facilitated by the `PrivacyEngine`'s ability to calculate `sigma` based on target `epsilon`.\n- **Full batch gradient computation (DP-GD)**: Achieved through gradient accumulation. The total effective batch size for one update (`batch_size`) is determined by a base batch size scaled by a factor related to the task (`batch_size = int(base_batch_size * factor)`). `gradient_accumulation_steps` are calculated as `batch_size // per_device_train_batch_size`.\n- **Unit clipping (C=1)**: The maximum per-example gradient 2-norm, `per_example_max_grad_norm`, is set to `0.1` in `examples/classification/run_wrapper.py`, not 1.\n- **Zero initialization of linear classifiers**: The code typically loads pre-trained models. For language models, the `lm_head` is explicitly initialized by *cloning* the embedding weights, not zeroing them. For classification tasks, the new classifier head might be randomly initialized by `from_pretrained`.\n- **SGD with momentum (ρ=0.9)**: The optimization is performed using `torch.optim.AdamW`. The `adam_beta1` parameter, which controls the momentum, defaults to `0.9` in `transformers.TrainingArguments`, aligning with the specified momentum.\n- **Privacy Loss Variable (PLV) accounting**: Implemented using `accounting_mode=\"glw\"` (Generalized L-W, also known as f-DP accounting), configurable via `PrivacyArguments`. A `target_epsilon` of `8` and a `target_delta` of `sample_size ** -1.1` (default if not explicitly set) are used.\n- **Noise Multiplier**: Automatically computed via a binary search in `PrivacyEngine`'s `compute_sigma` method if a `target_epsilon` is provided.\n- **Clipping Mode**: The `clipping_mode` can be configured as \"ghost\" or \"default\". \"ghost\" clipping is utilized for improved efficiency.\n- **Learning Rate Schedule**: A linear learning rate decay (`lr_decay=\"yes\"`) is applied by default."
      }
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
      "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are\npowerful in attaining state-of-the-art machine learning models, with Bayesian\noptimization (BO) standing out as a mainstream method. Extending BO into the\nmulti-fidelity setting has been an emerging research topic, but faces the\nchallenge of determining an appropriate fidelity for each hyperparameter\nconfiguration to fit the surrogate model. To tackle the challenge, we propose a\nmulti-fidelity BO method named FastBO, which adaptively decides the fidelity\nfor each configuration and efficiently offers strong performance. The\nadvantages are achieved based on the novel concepts of efficient point and\nsaturation point for each configuration.We also show that our adaptive fidelity\nidentification strategy provides a way to extend any single-fidelity method to\nthe multi-fidelity setting, highlighting its generality and applicability.",
      "full_text": "FastBO: Fast HPO and NAS with Adaptive Fidelity Identification Jiantong Jiang and Ajmal Mian The University of Western Australia, Perth WA 6009, Australia {jiantong.jiang@research.,ajmal.mian@}uwa.edu.au Abstract. Hyperparameter optimization (HPO) and neural architec- ture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts ofefficient pointand saturation point for each configuration. We also show that our adaptive fidelity iden- tification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability. Keywords: HPO · NAS · Multi-fidelity 1 Introduction HPO [9] and NAS [7] aim to find the hyperparameter configuration or archi- tecture λ∗ that minimizesf(λ), the performance obtained by configurationλ. BO [2,10,30] is an effective model-based method for HPO and NAS. It maintains asurrogate modelof the performancebasedon past evaluations ofconfigurations, which guides the choice of promising configurations to evaluate. Recent studies onBOhaveexploredexpertpriors[11,20,26,29],derivativeinformation[1,27,35], and enhancing the interpretability [5,36–39] of HPO and NAS [3,24,25]. However, standard BO requires full evaluations of configurations, which in- curssignificantcosts,especiallyconsideringtheescalatingmodelevaluationover- head. Despite efforts to accelerate model evaluation [13,15–17], smart strategies are required to widely adopt HPO and NAS. Thus, multi-fidelity methods have been proposed [4,12,21,22], where the fidelities mean the performance levels obtained under various resource levels. They follow the idea of successive halv- ing (SHA) [12]: initially, they evaluate many random configurations using few resources; then, based on the low-fidelity performances, only the well-performing ones successively continue to be evaluated with increasing resources. Follow-up studies [8,19,23,28,33] propose model-based multi-fidelity meth- ods, replacing random sampling with more informed models to improve samplearXiv:2409.00584v1  [cs.LG]  1 Sep 20242 J. Jiang and A. Mian efficiency. However, they are based on SHA, which assumes that learning curves of different configurations rarely intersect - a condition that often fails in prac- tice [32], i.e., early performance observations cannot always indicate the final fidelity performance. This raises a key challenge:What is the appropriate fidelity for each configuration to fit the surrogate model?In other words, which fidelity can offer observations that reliably indicate the final fidelity performance? Cur- rent methods struggle with this. Hyper-Tune [23] and BOHB [8] fit separate models for different fidelities, missing inter-fidelity correlations. FTBO [31] and A-BOHB [19] fit joint models but require strong assumptions. Salinas et al. [28] use the last observed fidelity performance, which may get inaccurate surrogate models as it widens the gap between poorly- and well-performing configurations. This paper is an extended abstract of our conference paper [14], highlighting key ideas and the main experimental results, while omitting finer details. 2 Key Idea of FastBO We propose a multi-fidelity extension of BO, namely FastBO, which tackles the challenge of deciding the appropriate fidelity for each configuration to fit the surrogate model. Here, we first propose the key concepts ofefficient point and saturation point, which are crucial in the optimization process. Then, we briefly describe the process of FastBO and highlight its generality. 2.1 Efficient Point and Saturation Point We first formally define the efficient point as follows. Definition 1 (Efficient point). For a given learning curveCi(r) of hyper- parameter configuration or architectureλi, wherer represents the resource level (also referred to as fidelity), the efficient pointei of λi is defined as:ei = min{r | Ci(r) − Ci(2r) < δ1}, whereδ1 is a predefined small threshold. The semantic of Definition 1 is that starting from the efficient point onwards, whentheresourcesaredoubled,theperformanceimprovementfallsbelowasmall threshold. Consequently, this point signifies a fidelity of performance achieved with comparably efficient resource usage. Thus, we make the following remark. Remark 1. The efficient points of the configurations can serve as their ap- propriate fidelities used for fitting the surrogate model. This is due to their (i) optimal resource-to-performance balance, (ii) ability to capture valuable learning curve trends, and (iii) customization for different hyperparameter configurations. We elaborate on the reasons as follows. Firstly, efficient points balance the trade- off between computational cost and result quality. Beyond the efficient point, allocating additional resources becomes less efficient. Secondly, efficient points capture valuable behaviors within the learning curves, enabling more informed decision-making. Thirdly, the ability to customize the fidelity for each specific configuration is an advantage. This adaptive approach is more reasonable than previous studies that use a fixed fidelity for all the configurations.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 3 1 FastBO Methodology•Process overviewWarm-up Learning curve modelingEfficient & saturation pointsConfig !! •Evaluate !! to the warm-up point to get early observation set.•Stop some bad configs. •Estimate !!’s learning curve from its observation set. Post-processing……•Adaptively extract the points from the learning curve. •Stop evaluating at efficient point.•Fit surrogate model. •Resume some best-performing configs to saturation point.Optimal config Fig. 1:Main process of FastBO. FastBO involves estimating efficient and saturation points, modeling learning curves, and auxiliary stages of warm-up and post-processing. Besides efficient points, we identify saturation points for all configurations as well. We provide the definition of the saturation point as follows. Definition 2 (Saturation point).For a given learning curveCi(r) of config- uration λi, wherer represents the resource level (also referred to as fidelity), the saturation pointsi of λi is defined as:si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}, where δ2 is a predefined small threshold. The semantic of Definition 2 is that beyond the saturation point, the observed performance no longer exhibits notable variations with more resources. Thus, this point characterizes the fidelity at which the performance of a configuration stabilizes. Building on the above definition, we make the following remark. Remark 2. The saturation points of the configurations can serve as their ap- proximate final fidelities, as they provide performance results that meet predefined quality thresholds while reducing resource wastage. 2.2 FastBO Process and Generalization With the two crucial points, we show the main process of FastBO in Fig. 1. Each configurationλi first enters a warm-up stage to get its early observation set. Some configurations are terminated here if they are detected consecutive performance deterioration. Then, FastBO estimates the learning curve ofλi from its observation set. Thus, the efficient point and saturation points are adaptively extracted. After that, λi continues to be evaluated to its efficient point; the result is used to update the surrogate model. Finally, the post-processing stage let a small set of promising configurations resume evaluating to their saturation points, and the optimal configurations can be obtained. Generalizing FastBO to single-fidelity methods.The inefficiency of single- fidelity methods like BO stems from their reliance on expensive final fidelity evaluations. Notably, low-fidelity evaluations provide informative insights but are computationally cheaper. Therefore, we can extend single-fidelity methods to the multi-fidelity setting by acquiring the low-fidelity performance for each configuration to fit the surrogate model. To do this, we need to determine the fidelity used to fit the surrogate model. FastBO adaptively determines this fi- delity for each configuration by identifying its efficient point. While this adaptive identification strategy is described in the context of model-based methods, it can4 J. Jiang and A. Mian Fashion-MNIST Airlines Albert CovertypeChristine Fig. 2:Anytime performance on the LCBench benchmark. Slice CIFAR-10 CIFAR-100 ProteinImageNet16-120 (a) NAS-Bench-201 benchmark (b) FCNet benchmark Fig. 3:Anytime performance on(a) NAS-Bench-201 and(b) FCNet. be generalized to various single-fidelity methods. For example, when evaluating configurations within the population for an evolutionary algorithm-based HPO method, we can similarly evaluate the efficient point performances instead of the final performances and integrate them in the subsequent processes, such as selection and variation. To conclude, the proposed strategy provides a simple but effective way to bridge the gap between single- and multi-fidelity methods. 3 Experimental Evaluation We compare the performance of FastBO with random search (RS), standard BO [30], ASHA [21], Hyperband [22], PASHA [4], A-BOHB [19], A-CQR [28], BOHB [8], DyHPO [34], and Hyper-Tune [23]. The results on the LCBench [40], NAS-Bench-201 [6], and FCNet [18] benchmarks are shown in Figs. 2 and 3. Overall, FastBO can handle various performance metrics and shows strong any- time performance. We can observe that FastBO gains an advantage earlier than other methods, rapidly converging to the global optimum after the initial phase. 4 Conclusion and Discussion We propose FastBO, a model-based multi-fidelity HPO method, which excels in adaptively identifying the fidelity for each configuration to fit the surrogate modelandefficientlyprovidinghigh-qualityperformance.Theproposed adaptive fidelity identification strategy also provides a simple way to extend any single- fidelity method to the multi-fidelity setting. While this paper provides a strong foundation on HPO and NAS, we see challenges that demand future improve- ments. Future work could refine and expand Fast-BO to larger search spaces and distributed computing systems to improve its applicability and scalablity.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 5 References 1. Ament, S.E., Gomes, C.P.: Scalable first-order Bayesian Optimization via struc- tured automatic differentiation. In: International Conference on Machine Learning. pp. 500–516. PMLR (2022) 1 2. Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter optimization. Advances in Neural Information Processing Systems24 (2011) 1 3. Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A.L., et al.: Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisci- plinary Reviews: Data Mining and Knowledge Discovery13(2), e1484 (2023) 1 4. Bohdal, O., Balles, L., Wistuba, M., Ermis, B., Archambeau, C., Zappella, G.: PASHA: efficient HPO and NAS with progressive resource allocation. In: Interna- tional Conference on Learning Representations. OpenReview.net (2023) 1, 4 5. Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.: This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems32 (2019) 1 6. Dong, X., Yang, Y.: NAS-Bench-201: Extending the scope of reproducible neu- ral architecture search. In: International Conference on Learning Representations (2020) 4 7. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. The Journal of Machine Learning Research20(1), 1997–2017 (2019) 1 8. Falkner, S., Klein, A., Hutter, F.: BOHB: Robust and efficient hyperparameter optimization at scale. In: International Conference on Machine Learning. pp. 1437– 1446. PMLR (2018) 1, 2, 4 9. Feurer, M., Hutter, F.: Hyperparameter optimization. Automated Machine Learn- ing: Methods, Systems, Challenges pp. 3–33 (2019) 1 10. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. In: Learning and Intelligent Optimization. pp. 507–523. Springer (2011) 1 11. Hvarfner,C.,Stoll,D.,Souza,A.L.F.,Lindauer,M.,Hutter,F.,Nardi,L.:$\\pi$BO: Augmenting acquisition functions with user beliefs for bayesian optimization. In: International Conference on Learning Representations. OpenReview.net (2022) 1 12. Jamieson, K., Talwalkar, A.: Non-stochastic best arm identification and hyperpa- rameter optimization. In: Artificial Intelligence and Statistics. pp. 240–248. PMLR (2016) 1 13. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast parallel exact inference on Bayesian networks. In: ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. pp. 425–426 (2023) 1 14. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Efficient hyperparameter optimization with adaptive fidelity identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 26181–26190 (2024) 2 15. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast inference for probabilistic graphical models. In: 2024 USENIX Annual Technical Conference (USENIX ATC 24) (2024) 1 16. Jiang, J., Wen, Z., Mian, A.: Fast parallel bayesian network structure learning. In: IEEE International Parallel and Distributed Processing Symposium. pp. 617–627. IEEE (2022) 1 17. Jiang, J., Wen, Z., Yang, P., Mansoor, A., Mian, A.: Fast-pgm: Fast probabilistic graphical model learning and inference. arXiv preprint arXiv:2405.15605 (2024) 16 J. Jiang and A. Mian 18. Klein, A., Hutter, F.: Tabular benchmarks for joint architecture and hyperparam- eter optimization. arXiv preprint arXiv:1905.04970 (2019) 4 19. Klein, A., Tiao, L.C., Lienart, T., Archambeau, C., Seeger, M.: Model-based asynchronous hyperparameter and neural architecture search. arXiv preprint arXiv:2003.10865 (2020) 1, 2, 4 20. Li, C., Rana, S., Gupta, S., Nguyen, V., Venkatesh, S., Sutti, A., de Celis Leal, D.R.,Slezak,T.,Height,M.,Mohammed,M.,Gibson,I.:Acceleratingexperimental design by incorporating experimenter hunches. In: International Conference on Data Mining. pp. 257–266. IEEE Computer Society (2018) 1 21. Li,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,E.,Ben-Tzur,J.,Hardt,M.,Recht, B., Talwalkar, A.: A system for massively parallel hyperparameter tuning. Proceed- ings of Machine Learning and Systems2, 230–246 (2020) 1, 4 22. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., Talwalkar, A.: Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research18(1), 6765–6816 (2017) 1, 4 23. Li, Y., Shen, Y., Jiang, H., Zhang, W., Li, J., Liu, J., Zhang, C., Cui, B.: Hyper- tune: Towards efficient hyper-parameter tuning at scale. Proceedings of the VLDB Endowment 15(6), 1256–1265 (2022) 1, 2, 4 24. Moosbauer, J., Casalicchio, G., Lindauer, M., Bischl, B.: Improving accuracy of interpretability measures in hyperparameter optimization via Bayesian algorithm execution. arXiv preprint arXiv:2206.05447 (2022) 1 25. Moosbauer, J., Herbinger, J., Casalicchio, G., Lindauer, M., Bischl, B.: Explaining hyperparameter optimization via partial dependence plots. Advances in Neural Information Processing Systems34, 2280–2291 (2021) 1 26. Oh, C., Gavves, E., Welling, M.: BOCK: Bayesian optimization with cylindrical kernels. In: International Conference on Machine Learning. pp. 3868–3877. PMLR (2018) 1 27. Padidar, M., Zhu, X., Huang, L., Gardner, J., Bindel, D.: Scaling gaussian pro- cesses with derivative information using variational inference. Advances in Neural Information Processing Systems34, 6442–6453 (2021) 1 28. Salinas, D., Golebiowski, J., Klein, A., Seeger, M.W., Archambeau, C.: Optimizing hyperparameters with conformal quantile regression. In: International Conference on Machine Learning. vol. 202, pp. 29876–29893. PMLR (2023) 1, 2, 4 29. Shahriari, B., Bouchard-Côté, A., Freitas, N.: Unbounded Bayesian Optimization via regularization. In: Artificial intelligence and statistics. pp. 1168–1176. PMLR (2016) 1 30. Snoek, J., Larochelle, H., Adams, R.P.: Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems25 (2012) 1, 4 31. Swersky, K., Snoek, J., Adams, R.P.: Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896 (2014) 2 32. Viering, T., Loog, M.: The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 2 33. Wang, J., Xu, J., Wang, X.: Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. arXiv preprint arXiv:1801.01596 (2018) 1 34. Wistuba, M., Kadra, A., Grabocka, J.: Supervising the multi-fidelity race of hy- perparameter configurations. Advances in Neural Information Processing Systems 35, 13470–13484 (2022) 4 35. Wu, J., Poloczek, M., Wilson, A.G., Frazier, P.: Bayesian optimization with gradi- ents. Advances in neural information processing systems30 (2017) 1FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 7 36. Yang,P.,Akhtar,N.,Jiang,J.,Mian,A.:Backdoor-basedexplainableaibenchmark forhighfidelityevaluationofattributionmethods.arXivpreprintarXiv:2405.02344 (2024) 1 37. Yang, P., Akhtar, N., Shah, M., Mian, A.: Regulating model reliance on non-robust features by smoothing input marginal density. arXiv preprint arXiv:2407.04370 (2024) 1 38. Yang, P., Akhtar, N., Wen, Z., Mian, A.: Local path integration for attribution. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 3173–3180 (2023) 1 39. Yang, P., Akhtar, N., Wen, Z., Shah, M., Mian, A.: Re-calibrating feature attri- butions for model interpretation. In: International Conference on Learning Repre- sentations (2022) 1 40. Zimmer, L., Lindauer, M.T., Hutter, F.: Auto-Pytorch: Multi-fidelity metalearn- ing for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence43, 3079–3090 (2021) 4",
      "meta_data": {
        "arxiv_id": "2409.00584v1",
        "authors": [
          "Jiantong Jiang",
          "Ajmal Mian"
        ],
        "published_date": "2024-09-01T02:40:04Z",
        "pdf_url": "https://arxiv.org/pdf/2409.00584v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "FastBO proposes a novel multi-fidelity Bayesian optimization (BO) method for Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS). Its main contribution is adaptively identifying the appropriate fidelity for each hyperparameter configuration to fit the surrogate model, addressing a key challenge in existing multi-fidelity methods. This is achieved through the introduction of \"efficient point\" and \"saturation point\" concepts. FastBO's adaptive fidelity identification strategy also provides a generalizable way to extend any single-fidelity method to a multi-fidelity setting, offering strong anytime performance.",
        "methodology": "FastBO's methodology centers on the concepts of efficient point and saturation point. An 'efficient point' for a configuration is defined as the minimum resource level where doubling resources yields performance improvement below a small threshold (delta1), indicating optimal resource-to-performance balance. A 'saturation point' is defined as the minimum resource level beyond which performance no longer exhibits notable variations (variation below delta2), signifying performance stabilization. The FastBO process involves a warm-up stage to gather early observations, learning curve modeling to estimate these points, evaluating configurations up to their efficient points to update the surrogate model, and a post-processing stage where a small set of promising configurations are resumed to their saturation points to obtain optimal results. This adaptive fidelity identification is adaptable to other single-fidelity methods by evaluating configurations at their efficient points.",
        "experimental_setup": "FastBO's performance was experimentally evaluated against a range of methods including Random Search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The evaluation was conducted on three established benchmarks: LCBench, NAS-Bench-201, and FCNet. The validation method focused on comparing 'anytime performance,' demonstrating FastBO's ability to gain an advantage earlier and rapidly converge to the global optimum after the initial phase.",
        "limitations": "The paper does not explicitly detail inherent limitations of FastBO in a dedicated section. However, the future research directions implicitly suggest current constraints or areas for improvement, indicating that FastBO could be refined and expanded to handle larger search spaces and distributed computing systems more effectively, implying potential limitations in its current applicability and scalability within these complex environments.",
        "future_research_directions": "Future research directions for FastBO include refining and expanding the method to handle larger search spaces and integrating it with distributed computing systems. These extensions aim to significantly improve FastBO's applicability and scalability in more complex and resource-intensive HPO and NAS scenarios."
      }
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of expensive hyperparameter tuning for iterative deep learning (DL) and deep reinforcement learning (DRL) systems, where traditional Bayesian Optimization (BO) methods ignore valuable intermediate training information. The main contributions include: 1) proposing an algorithm (BOIL) to optimize the learning curve of an ML algorithm using training curve compression, rather than just final averaged performance; 2) introducing an approach to learn the optimal compression curve parameters from data and a data augmentation technique for increased sample-efficiency; and 3) demonstrating BOIL's effectiveness in tuning hyperparameters for DRL agents and convolutional neural networks, outperforming existing baselines in identifying optimal hyperparameters in minimal wall-clock time.",
        "methodology": "The proposed Bayesian Optimization for Iterative Learning (BOIL) approach exploits the iterative structure of learning algorithms. It models the cost-sensitive black-box function, which takes both hyperparameters (x) and training iterations (t) as input, using a Gaussian Process (GP) with a product kernel. The core idea is to compress the entire learning curve into a single numeric score using a user-preferred Sigmoid (Logistic) function, parameterized by a growth parameter (g0) and a middle point (m0). These parameters (g*0, m*0) are learned dynamically by maximizing the GP's log marginal likelihood. The training data is augmented by selectively including intermediate observations from the learning curve, chosen to maximize GP predictive uncertainty while ensuring the GP covariance matrix remains well-conditioned (condition number below a threshold). The cost function for evaluations is approximated by a linear regressor. The acquisition function selects the next hyperparameter and iteration count by balancing expected improvement (utility) against estimated computation cost.",
        "experimental_setup": "The algorithm was evaluated on two deep reinforcement learning (DRL) agents and a convolutional neural network (CNN). For DRL, experiments included a Dueling DQN (DDQN) agent on the CartPole-v0 environment and Advantage Actor Critic (A2C) agents on the InvertedPendulum-v2 and Reacher-v2 environments (from OpenAI gym and Mujoco). For DL, hyperparameters for a CNN were tuned on the SVHN dataset and CIFAR10 dataset. All experimental results were averaged over 20 independent runs with different random seeds. The evaluations were conducted on NVIDIA 1080 GTX GPUs using the TensorFlow-GPU Python package. Baselines for comparison included Hyperband and Continuous Multi-Task/Multi-Fidelity BO (CM-T/F-BO). Square-exponential kernels were used for the GP, with parameters estimated by maximizing marginal likelihood. A maximum of 15 augmented points were used, with a natural log of GP condition number threshold of 20.",
        "limitations": "A primary limitation, though effectively managed by the proposed method, is the potential for redundancy and numerical instability (ill-conditioning of the GP covariance matrix) when naively augmenting training data with the entire learning curve. While BOIL actively selects a subset of points to mitigate this, it highlights a constraint in data handling. Additionally, the cost function is approximated by a linear regressor; for scenarios with more complex cost dependencies on hyperparameters and iterations, a more sophisticated model (e.g., a second GP) might be more appropriate. The inherent noisiness and unpredictable fluctuations of DRL reward curves also pose a general challenge for hyperparameter optimization, which BOIL aims to address more robustly than prior methods like Freeze-Thaw BO.",
        "future_research_directions": "The authors suggest that their BOIL framework is not limited to machine learning algorithms and can be applied more generally to any process exhibiting an iterative structure that can be exploited. A specific example given is the optimization of manufacturing pipelines, where factory settings are adjusted to increase productivity. Further research could also explore the use of various acquisition functions and kernel choices within the BOIL framework, as these were explicitly omitted from comparison in the current study but could easily be integrated.",
        "experimental_code": "import numpy as np\nfrom bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import ProductGaussianProcess\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ncounter = 0\n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \n        gp_params:                  GP parameters\n        gp_params.theta:            to compute the kernel\n        gp_params.delta:            to compute the kernel\n        \n        func_params:                function to optimize\n        func_params.init bound:     initial SearchSpace for parameters\n        func_params.SearchSpace:        SearchSpace on parameters        \n        func_params.func:           a function to be optimized\n        \n        \n        acq_params:            acquisition function, \n        acq_params.acq_func['name']=['ei','ucb','poi']\n        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.S\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
        "experimental_info": "BOIL (Bayesian Optimization for Iterative Learning) settings:\n- Gaussian Process (GP) with Product Kernel (RBF for hyperparameters and time):\n  - Initial GP hyperparameters (`gp.hyper`):\n    - `var`: 1 (standardized data)\n    - `lengthscale_x`: 0.02 (for hyperparameters)\n    - `lengthscale_t`: 0.2 (for time/iterations)\n  - GP noise parameter (`noise_delta`): 5e-4\n  - Upper bound for GP noise (`noise_upperbound`): 1e-2\n  - GP hyperparameters (`lengthscale_x`, `lengthscale_t`) and Logistic hyperparameters (`midpoint`, `growth`) are optimized dynamically by maximizing the GP's log marginal likelihood. Optimization is performed every `2 * dim` iterations.\n  - Bounds for lengthscales during optimization: `[0.03, 0.3]` for `lengthscale_x`, `[0.3, 0.6]` for `lengthscale_t` (derived from `[10*SearchSpace_l_min,2*SearchSpace_l_max]` for `lengthscale_t` and `SearchSpace_l_min=0.03, SearchSpace_l_max=0.3` in `ProductGaussianProcess.optimize_lengthscale_SE_logistic_hyper`).\n- Learning Curve Compression (Sigmoid/Logistic function):\n  - Initial Logistic hyperparameters (`gp.logistic_hyper`):\n    - `midpoint`: 0.0\n    - `growth`: 1.0\n  - Bounds for Logistic hyperparameters during optimization: `midpoint` `[-2, 3]`, `growth` `[0.5, 2]`.\n  - The Logistic function is `1.0 / (1 + np.exp(-growth * (x - midpoint)))`.\n- Acquisition Function (Cost-sensitive Expected Improvement):\n  - Type: `ei_mu_max` (Expected Improvement using the maximum of the GP mean function as incumbent).\n  - Balances expected improvement (`np.log(utility)`) against estimated computation cost (`np.log(mean_cost)`).\n  - The acquisition value is calculated as `np.log(utility) - np.log(mean_cost)` when using 'ei' based acquisition, or `np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))` otherwise. The objective function is then minimized (multiplied by -1).\n  - Acquisition function maximization uses `L-BFGS-B` optimizer with `maxiter=30*dim` and `maxfun=30*dim` (derived from `optimize_lengthscale_SE_logistic_hyper` which uses `myopts`). The search space is `self.scaleSearchSpace`.\n- Data Augmentation (Virtual Observations):\n  - Intermediate observations from learning curves are selectively included.\n  - Selection criteria: Maximize GP predictive uncertainty (`pure_exploration` acquisition function).\n  - Constraint: GP covariance matrix condition number must be below a threshold (`threshold_cond=15`). Augmentation stops if `log(condition_number) > threshold_cond` or predictive variance is too low (`pred_var_value < (self.gp.noise_delta + 1e-3)`).\n  - Maximum number of augmentations (`max_n_augmentation`): 10 times the current iteration count `t_max` (normalized).\n  - Cost of virtual observations is interpolated linearly based on the real observation's cost and the scaled iteration count.\n- Cost Model:\n  - Approximated by a `sklearn.linear_model.LinearRegression` on the combined input (`X`, `T`).\n  - Cost values are normalized to `[0,1]` scale before fitting the linear regressor.\n- Input Scaling: Hyperparameters (X) and time (T) are scaled to `[0,1]` using `MinMaxScaler`."
      }
    },
    {
      "title": "Scaling Laws for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization is an important subfield of machine learning that\nfocuses on tuning the hyperparameters of a chosen algorithm to achieve peak\nperformance. Recently, there has been a stream of methods that tackle the issue\nof hyperparameter optimization, however, most of the methods do not exploit the\ndominant power law nature of learning curves for Bayesian optimization. In this\nwork, we propose Deep Power Laws (DPL), an ensemble of neural network models\nconditioned to yield predictions that follow a power-law scaling pattern. Our\nmethod dynamically decides which configurations to pause and train\nincrementally by making use of gray-box evaluations. We compare our method\nagainst 7 state-of-the-art competitors on 3 benchmarks related to tabular,\nimage, and NLP datasets covering 59 diverse tasks. Our method achieves the best\nresults across all benchmarks by obtaining the best any-time results compared\nto all competitors.",
      "full_text": "Scaling Laws for Hyperparameter Optimization Arlind Kadra Representation Learning Lab University of Freiburg kadraa@cs.uni-freiburg.de Maciej Janowski Representation Learning Lab University of Freiburg janowski@cs.uni-freiburg.de Martin Wistuba Amazon Web Services Amazon Berlin marwistu@amazon.com Josif Grabocka Representation Learning Lab University of Freiburg grabocka@cs.uni-freiburg.de Abstract Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the dominant power law nature of learning curves for Bayesian optimization. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incre- mentally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors. 1 Introduction Hyperparameter Optimization (HPO) is a major challenge for the Machine Learning community. Unfortunately, HPO is not yet feasible for Deep Learning (DL) methods due to the high cost of evaluating multiple configurations. Recently multi-fidelity HPO (a sub-problem of gray-box HPO) has emerged as a promising paradigm for HPO in DL, by discarding poorly-performing hyperparameter configurations after observing the validation error on the low-level fidelities of the optimization procedure [28, 9, 1, 29]. The advantage of multi-fidelity HPO compared to online HPO [7, 38], or meta-gradient HPO [32, 10, 31] is the ability to tune all types of hyperparameters. In recent years, a stream of papers highlights the fact that the performance of DL methods is predictable [14], concretely, that the validation error rate is a power law function of the model size, or dataset size [41, 40]. Such a power law relationship has been subsequently validated in the domain of NLP, too [11]. In this paper, we demonstrate that the power-law principle has the potential to be a game-changer in HPO, because we can evaluate hyperparameter configurations in low-budget regimes (e.g. after a few epochs), then estimate the performance on the full budget using dataset-specific power law models. Concretely, we hypothesize and empirically demonstrate that optimization curves (training epochs versus accuracy, or loss) can be efficiently modeled as simple power law functions. As a result, we introduce Deep Power Law (DPL) ensembles, a probabilistic surrogate for Bayesian optimization (BO) that estimates the performance of a hyperparameter configuration at future budgets using ensembles of deep power law functions. Subsequently, our novel formulation of BO dynami- cally decides which configurations to pause and train incrementally by relying on the performance 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2302.00441v3  [cs.LG]  25 Oct 2023estimations of the surrogate. We demonstrate that our method achieves the new state-of-the-art in HPO for DL by comparing against 7 strong HPO baselines, and 59 datasets of three diverse modalities (tabular, image, and natural language processing). Our experimental protocol contains state-of-the-art deep learning architectures such as Transformers [44], XFormer [26], ResNeXt [49] and large language models like GPT-2 [39]. As a result, we believe the proposed method has the potential to finally make HPO for DL a feasible reality. Overall, our contributions can be summarized as follows: • We introduce a novel probabilistic surrogate for multi-fidelity HPO based on ensembles of deep power law functions. • We derive a simple mechanism to combine our surrogate with Bayesian optimization. • Finally, we demonstrate the empirical superiority of our method against the current state-of- the-art in HPO for Deep Learning, with a large-scale HPO experimental protocol. 2 Related Work Multi-fidelity HPO assumes a method has access to the learning curve of a hyperparameter con- figuration. Such a learning curve is the function that maps either training time or dataset size, to the validation performance. The early performance of configurations (i.e. first segment of the learning curve) is used to discard unpromising configurations, before waiting for full convergence. Successive halving [17] is a widely used multi-fidelity method that randomly samples hyperparameter configurations, starts evaluating them, and ends a fraction of them upon reaching a predefined budget. Afterwards, the budget is divided by the fraction of discarded hyperparameter configurations and the process continues until the maximum budget is reached. Although the method relies only on the last observed value of the learning curve, it is very efficient. In recent years, various flavors of successive halving have been suggested, including Hyperband [28], which effectively runs successive halving in parallel with different settings. A major improvement to Hyperband is replacing random search with a more efficient sampling strategy [1, 9]. A more efficient approach is to allocate the budget dynamically to the most promising configurations [47]. However, the only assumption these methods make about the learning curve is that it will improve over time, while recent work [ 4] exploits a power law assumption on the curves. Similarly, we fit surrogates that exploit a power law assumption, however, our method is able to estimate the performance of unobserved configurations through probabilistic surrogates learned jointly for all hyperparameter configurations. Learning curve prediction is a related topic, where the performance of a configuration is predicted based on a partially observed learning curve [36]. Typically, the assumptions about the learning curve are much stronger than those described above. The prediction is often based on the assumption that the performance increases at the beginning and then flattened towards the end. One way to model this behavior is to define a weighted set of parametric functions [8, 23]. Then, the parameters of all functions are determined so that the resulting prediction best matches the observed learning curve. Another approach is to use learning curves from already evaluated configurations and to find an affine transformation that leads to a well-matched learning curve [6]. A more data-driven approach is to learn the typical learning curve behaviour directly from learning curves across different datasets [48]. Learning curve prediction algorithms can be combined with successive halving [2]. In contrast to this research line, we fit ensembles of power law surrogates for conducting multi-fidelity HPO with Bayesian optimization. Scaling laws describe the relationship between the performance of deep learning models as a function of dataset size or model size as a power law [ 14, 16, 41, 40, 11, 20, 15, 33]. Another work tunes hyperparameters on a small-scale model and then transfers them to a large-scale version [ 50]. In contrast to these papers, we directly use the power law assumption for training surrogates in Bayesian optimization for HPO. 3 Preliminaries Hyperparameter Optimization (HPO) demands finding the configurations λ ∈ Λ of a Machine Learning method that achieve the lowest validation loss L(Val) of a model (e.g. a neural network), which is parameterized with θ ∈ Θ and learned to minimize the training loss L(Train) as: 2λ∗ := arg min λ∈Λ L(V al) (λ, θ∗ (λ)) , s.t. θ∗ (λ) := arg min θ∈Θ L(Train ) (λ, θ) (1) For simplicity, we denote the validation loss as our function of interestf(λ) := L(V al) (λ, θ∗ (λ)). The optimal hyperparameter configurations λ∗ of Equation 1 are found via an HPO policy A ∈ P(also called an HPO method) that given a history of N evaluated configurations H(N) := {λi, f(λi)}N i=1 suggests the (N + 1)-th configuration to evaluate as λN+1 := A \u0000 H(N)\u0001 where A : [Λ × R+]N → Λ. The search for an optimal HPO policy is a bi-objective problem in itself, aiming at (i) finding a configuration out of N evaluations that achieves the smallest validation loss f(λ), and (ii) ensuring that the costs of evaluating the N configurations do not exceed a total budget Ω, as shown in Equation 2. arg min A∈P min i∈{1,...,N} f \u0010 λi := A \u0010 H(i−1) \u0011\u0011 , (2) where: H(i) := ( {(λj, f(λj))}i j=1 i >0 ∅ i = 0 , subject to: Ω ≥ NX i=1 cost (f (λi)) Bayesian optimization (BO) is the most popular type of policy for HPO, due to its ability to balance the exploration and exploitation aspects of minimizing the validation loss f in terms of hyperparame- ters λ ∈ Λ. Technically speaking, BO fits a surrogate ˆf(λ; ϕ) parametrized with ϕ ∈ Φ to approximate the observed loss f(λ) using the history H(N), as ϕ∗ := arg maxϕ∈Φ E(λ,f(λ)) ∼pH(N) p(f(λ)|λ, ϕ), where, p represents the probability. Afterwards, BO uses an acquisition/utility function a : Λ → R+ to recommend the next configuration as λN+1 := A \u0000 H(N)\u0001 = arg maxλ∈Λ a \u0010 ˆf(λ; ϕ∗) \u0011 . Multi-fidelity HPO refers to the case where an approximation of the validation loss can be measured at a lower budget b ∈ B, where B := (0, bmax]. For instance, in Deep Learning we can measure the validation loss after a few epochs ( 0 < b < ϵ), rather than wait for a full convergence ( b = bmax). Throughout this paper, the term budget refers to a learning curve step. The evaluation of a configuration λ for a budget b is defined as f (λ, b) : Λ × B → R+. 4 Power Law Surrogates for Bayesian Optimization Prior work has demonstrated that the performance of Machine Learning methods as a function of budgets (i.e. dataset size, number of optimization epochs, model size, image resolution) follows a power law relationship [41, 40, 11, 20, 15]. In this work, we employ this power law dependence between the validation loss and the number of optimization epochs in Deep Learning. We propose a novel multi-fidelity Hyperparameter Optimization method which is based on power law surrogates. We assume that every learning curve f (λ, ·) can be described by a power law function defined by (α, β, γ). Concretely, we define a power law function for the validation loss of a configuration λ at a budget b (a.k.a. the number of epochs) as shown in Equation 3. ˆf (λ, b) := αλ + βλ b−γλ, αλ, βλ, γλ ∈ R (3) Instead of fitting one separate power law function to each learning curve, we fit a single shared power law function across all configurations by conditioning the power law coefficients α, β, γon λ using a parametric neural network g that maps a configuration to the power law coefficients of its learning curve as g : Λ → R3. The network g has three output nodes, corresponding to the power law coefficients, denoted as g(λ)α, g(λ)β, g(λ)γ, as defined in Equation 4. ˆf (λ, b) := g(λ)α + g(λ)β b−g(λ)γ , g : Λ → R3 (4) 3Using a history of learning curve evaluations H(N) := {(λi, bi, f(λi, bi))}N i=1 we can train the power law surrogate to minimize the following loss function using stochastic gradient descent: arg min ˆf E(λ,b,f(λ,b))∼pH(N) \f\f\ff (λi, bi) − ˆf (λi, bi) \f\f\f (5) BO surrogates need to be probabilistic regression models because the acquisition functions require the posterior variance of the predictions. As a result, we train an ensemble of K diverse surrogates ˆf(1)(λ, b), . . . ,ˆf(K)(λ, b) by initializing each surrogate with different weights and by training with a different sequence of mini-batches as in prior work [25, 22]. The posterior mean µ and the posterior variance σ2 of the power law ensemble are trivially computed as: µ ˆf (λ, b) := 1 K KX k=1 ˆf(k)(λ, b), σ 2 ˆf (λ, b) := 1 K KX k=1 \u0010 ˆf(k)(λ, b) − µ ˆf (λ, b) \u00112 (6) A commonly used acquisition function in the domain is Expected Improvement (EI) [ 35] which incorporates both the mean and uncertainty of predictions, applying a trade-off between exploration and exploitation. Consequently, in our work, we use the EI acquisition with the estimated full budget’s (bmax) posterior mean and variance. We briefly define the acquisition function in Equation 7: λN+1 := arg max λ∈Λ EI \u0010 λ, bmax|H(N) \u0011 , (7) EI(λ, bmax|H) := Eˆf(λ,bmax)∼N \u0010 µ ˆf (λ,bmax),σ2 ˆf (λ,bmax) \u0011 h max n ˆf(λ, bmax) − f (λbest, bmax) , 0 oi where, f (λbest, bmax) corresponds to the best observed loss for any budget b′ ≤ bmax from the history H(N). After selecting a configuration with our variant of the EI acquisition, we do not naively run it until convergence. Instead, we propose a novel multi-fidelity strategy that advances the selected λN+1 of Equation 7 by a small budget of bstep, e.g. 1 epoch of training. Therefore, the selected λN+1 will be evaluated at bN+1 as defined in Equation 8. Notice, our proposed strategy also covers new configurations with no learning curve evaluations in H(N). bN+1 := (bstep, ∄λN+1 : (λN+1, ·, ·) ∈ H(N) bstep + max (λN+1,b,·)∈H(N) b, otherwise (8) We provide the detailed pseudocode of our method at Algorithm 1. 5 Experimental Protocol 5.1 Benchmarks LCBench: A benchmark that features 2,000 hyperparameter configurations that parametrize the archi- tecture of simple feedforward neural networks, as well as, the training pipeline [51]. The benchmark features 7 numerical hyperparameters and 35 different datasets from the AutoML benchmark [12]. PD1: A deep learning benchmark [45] that consists of recent DL (including Transformers) architec- tures run on large vision datasets such as CIFAR-10, CIFAR-100, ImageNet, as well as statistical modeling corpora and protein sequence datasets from bioinformatics. Every search space includes varying learning curve lengths, ranging from 5 to 1414, and a different number of evaluated hy- perparameter configurations ranging from 807 to 2807. The search space includes hyperparameter configurations that parametrize the learning rate, the learning rate scheduler, and the momentum. TaskSet: A benchmark that features different optimization tasks evaluated in 5 different search spaces [34]. For our work, we focus on the Adam8p search space, which is among the largest search spaces 4Algorithm 1: Multi-Fidelity HPO with Deep Power Laws Input : Search space Λ, initial design H(init), budget increment bstep Output :Best hyperparameter configuration λ∗ 1 Iteration i ← 0; Evaluate initial configurations and budgets H(i) := H(init); 2 while still budget do 3 Fit a DPL ensemble ˆf(1)(λ, b), . . . ,ˆf(K)(λ, b) from history H(i) using Equation 5; 4 Recommend the next configuration λi+1 and its budget bi+1 using Equation 6, 7 and 8; 5 Train λi+1 until bi+1 and measure the validation loss f (λi+1, bi+1); 6 Append to history Hi+1 ← Hi ∪ {(λi+1, bi+1, f(λi+1, bi+1))}; 7 i ← i + 1 ; 8 end 9 return Best configuration λ∗ with the smallest validation loss min (λ∗,b,f(λ∗,b))∈Hi f (λ∗, b) ; in the benchmark with 1000 hyperparameter configurations. Every hyperparameter configuration features 8 continuous hyperparameters. The hyperparameters control the learning rate, the learning rate schedulers, and the optimizer. For variety among our benchmarks, we focus on 12 RNN text classification tasks that feature different RNN cells, embedding sizes, and batch sizes. For a more detailed explanation of the benchmarks, we refer the reader to Appendix F. 5.2 Baselines Random Search stochastically samples hyperparameter configurations for the largest possible budget. Hyperband uses multiple brackets with different trade-offs of the initial budget and number of epochs to initially train [28]. It then applies Successive Halving (SH) [17] on every bracket. ASHA is an asynchronous version of SH [ 27] that does not wait for all configurations to finish in an SH bracket before starting the next one. Furthermore, BOHB is a follow-up of Hyperband that uses TPE [3] to sample the initial hyperparameter configurations of a bracket [9]. DEHB, on the other hand, modifies Hyperband by using evolutionary strategies to sample the initial hyperparameter configurations [1]. Similarly, multi-fidelity SMAC extends Hyperband but uses random forests to sample the initial hyperparameter configurations for a bracket [ 30]. We also use the Dragonfly Library [19] to compare against BOCA [18], a multi-fidelity method that uses Gaussian Processes to predict the next hyperparameter to evaluate, as well as the fidelity for which it should be evaluated. For all the baselines, we use their official public implementations. We provide additional details in Appendix G. 5.3 Architecture & Training For our method, we use an ensemble of 5 models, where every model consists of a 2-layer feedforward neural network with 128 units per layer and Leaky ReLU for the non-linearity. The architecture of our method is motivated by prior work [25]. Our network has 3 output units, that are then combined with the budget b to yield the power law output. We apply the GLU non-linearity activation only on the β and γ output units, allowing α to take any value. We use the L1 loss to train our network, coupled with Adam featuring an initial learning rate of 10−3. For the first 10 iterations of our multi-fidelity HPO method in Algorithm 1 we train every network of our ensemble for 250 epochs with randomly sampled initial weights. This choice helps the convergence of the weights in the early stage of HPO. Next, we continuously refine the model for 20 epochs every HPO iteration. However, if the optimization stagnates (surrogate fitting loss does not improve) for more than the LC Length + a buffer of 0.2 × LC Length, the training procedure is restarted with random weights, where every model is trained again for 250 epochs and then only refined. During the refining phase, the new data point at an HPO iteration (Line 9 at Algorithm 1) is sampled with repeat on every batch, to learn new data points equally compared to older data points. Since we are working with discrete search spaces, we evaluate the acquisition function exhaustively 5on every hyperparameter configuration. When dealing with continuous search spaces, the acquisition function can either be evaluated exhaustively on a discretized version of the search space, or in a gradient-based way. Our implementation of DPL is publicly available.1 5.4 Protocol In our experiments, we standardize the hyperparameter values by performing min-max scaling for our method and every baseline. If a baseline has a specific preprocessing protocol, we do not apply min-max scaling but we apply the protocol as suggested by the authors. The benchmarks do not support a common evaluation metric for configurations (i.e. the function f). As a consequence, the evaluation metric for LCBench is the balanced accuracy, for TaskSet the log-likelihood loss, while for PD1 the accuracy. Moreover, the benchmarks do not offer learning curves with a common step size. For LCBench and PD1, one step size is equivalent to one epoch, while for TaskSet one step size is 200 batches. The HPO budget is defined as the maximum number of steps needed to fully evaluate 20 hyperparameter configurations. In that context, one unit step of the HPO budget signifies training a particular configuration for one more optimization step (e.g. 200 batches in TaskSet or 1 epoch in LCBench). In the following experiments, for all methods, we report the regret of the best-found configuration as shown in Equation 9: R = f (λbest, bmax) − f (λoracle, bmax) (9) where the oracle is given as f (λoracle, bmax) := min \b f (λ, b) | (λ, b, f(λ, b)) ∈ H(D) ∧ b ≤ bmax\t , and H(D) corresponds to the set of all the exhaustively-evaluated hyperparameter configurations’ performances on a dataset D. If the oracle configuration is not known in advance for the search space, H(D) can be replaced with the history H(N) at the end of the HPO procedure. The only difference between f (λbest, bmax) and f (λoracle, bmax) is that the former only considers the history at the HPO step for which we are reporting the results. In short, the regret is the difference in the evaluation metric performance from the best-found hyperparameter configuration during optimization to the best possible hyperparameter configuration (oracle) on the dataset (in a minimization setting). On a dataset level, we report the average regret across 10 repetitions with different seeds. To be able to aggregate results over datasets, we report the averaged normalized regret. As normalization, we divide the regret by the difference between the performances of the best and the worst hyperparameter configuration on a dataset. Then we compute the mean of the normalized regrets across all the datasets of a benchmark. Moreover, in the experiments that report the average normalized regret over time, we provide results over normalized wall clock time. The wall clock time includes both the method’s overhead (i.e. training the surrogate ˆf and selecting the next hyperparameter configuration to evaluate) and the time taken to evaluate the selected hyperparameter configuration (i.e. evaluating f). Since the methods have different run times, we normalize the individual times by the time it took Random Search (the fastest non-model-based method) to complete the HPO optimization process. To provide a fair any-time comparison, we report results until the time it took Random Search to evaluate 20 hyperparameter configurations. Furthermore, when reporting the learning curve (LC) length fraction, we imply the fraction of the total learning curve length. LCBench and TaskSet have LCs of a fixed length for all datasets, corresponding to 51 epochs for LCBench and 50 epochs for TaskSet. In contrast, PD1 has varying LC lengths for different datasets. In our experiments, all methods start with a history H(init) of 1 randomly sampled hyperparameter configuration evaluated for 1 step/epoch in the case of multi-fidelity techniques (Hyperband, BOHB, DEHB, SMAC, ASHA, Dragonfly; descriptions in Section 5.2), or for the full budget for the black- box technique (Random Search). We ran experiments on a CPU cluster, where every node contains two Intel Xeon E5-2630v4 CPUs with 20 CPU cores running at 2.2 GHz. The total memory of every node is 120GB, and every experiment is limited to 2 cores which offer 12GB. 1https://github.com/releaunifreiburg/DPL 66 Research Hypotheses and Experiments /uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018 /uni0000002f/uni00000026/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000001d/uni00000003/uni00000028/uni00000056/uni00000057/uni00000011/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000037/uni00000055/uni00000058/uni00000048 /uni00000027/uni00000033/uni0000002f/uni00000026/uni00000052/uni00000051/uni00000047/uni00000003/uni00000031/uni00000031/uni00000033/uni0000002f/uni00000031/uni00000031/uni0000002a/uni00000033 Figure 1: Rank correlations of learning curve fore- casting models, which are given a fraction of the learning curve and estimate the remaining curve segment. DPL: Deep Power Law, Cond NN: Con- ditioned neural network,PL: Power Law,NN: Neu- ral Network, GP: Gaussian processes. Hypothesis 1: The power law assumption im- proves the quality of learning curve forecasting. In this experiment, we evaluate the predictive performance of forecasting models that given a fraction of the observed learning curve, esti- mate the remaining unobserved segment of the curve, on the LCBench benchmark. The results of Figure 1 compare three different forecasting models, concretely, neural networks (NN), Gaus- sian Processes (GP), and Power Law functions (PL). For the three variants (PL, NN, GP) we fitted one model on every learning curve of each hyperparameter configuration (i.e. given b in the x-axis estimate one ˆf(b) separately for every λ). For the other two variants (DPL and Cond NN) we fit a single forecasting model (not an ensem- ble) for all configurations, by conditioning the surrogate on the configuration (i.e. given b and λ, estimate ˆf (λ, b)). The purpose of the experiment is to assess whether a power law function regressor leads to superior predictive accuracy, compared to generic forecasting models, such as neural networks, or Gaussian processes. The evaluation metric of the experiment highlighted in Figure 1 is the rank correlation between the estimated performances at the end of the learning curve and the true performances. We notice that although a Power Law regressor has significantly fewer parameters than a neural network (3 to 288 parameters), PL still achieves higher predictive performance than NN. Furthermore, our conditioning of the power law function to the hyperparameter configuration further improves the predictive quality, as the difference between DLP and PL demonstrates. Lastly, we refer the reader to Appendix H, where we provide an analysis of the distributions for the absolute error rate between the DPL predictions and the ground truth values over the different LC length fractions, showing that DPL does not only retain the ranks but, it also accurately predicts the final performance. Based on the results, we consider Hypothesis 1 to be validated and that DPL is accurate in terms of learning curve forecasting. What about learning curves that do not follow a power law pattern? Although the provided empirical evidence in this section strongly suggests that the presented power law model can accurately forecast learning curves, it is also true that some learning curves have divergent behaviour that does not follow the power law assumption. As a consequence, we investigate two different ways to handle curves that do not follow the power law assumption: i) recently-proposed power law functions that include breaking points [ 5], or shifts in the curve [ 8], and ii) min-smoothing the learning curves to transform them into monotonically decreasing time series. In Appendix A we provide ample empirical evidence showing that although the alternative formulations achieve comparable performances, still they do not outperform our simpler power law formulation. The findings indicate that even though not all learning curves are power laws, most of them are, therefore a power law surrogate is efficient in forecasting the final performance of a partially-observed learning curve. As a result, the forthcoming experiments will provide further empirical evidence that our power law surrogates lead to state-of-the-art HPO results when deployed in the proposed multi-fidelity Bayesian optimization setup. Hypothesis 2: Our method DPL achieves state-of-the-art results in HPO. In Figure 2, we show the performance of the considered methods over the HPO budget, where DPL manages to outperform all the rival baselines. In the case of LCBench, DPL quickly finds well-performing hyperparameter configurations compared to the competitor methods and continues to discover even better configurations until the HPO process ends. Furthermore, we observe the same trend with TaskSet and PD1, where after ca. 25% of the HPO budget, our method DPL converges to a better regret compared to the baselines and increases the lead until HPO ends. For a detailed overview of the performances of all methods on all individual datasets, we point the reader to Appendix H. 7/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000033/uni00000027/uni00000014 /uni00000027/uni00000033/uni0000002f/uni00000024/uni00000036/uni0000002b/uni00000024/uni00000025/uni00000032/uni0000002b/uni00000025/uni00000027/uni00000028/uni0000002b/uni00000025/uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000036/uni00000030/uni00000024/uni00000026 Figure 2: DPL discovers better hyperparameter configurations than all rival baselines in terms of regret (distance to oracle). Solid curves and shaded regions represent the mean and standard error of the averaged normalized regret. /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000036/uni00000030/uni00000024/uni00000026/uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000036/uni00000030/uni00000024/uni00000026/uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000033/uni00000027/uni00000014/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000027/uni00000028/uni0000002b/uni00000025/uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000033/uni0000002f /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000027/uni00000028/uni0000002b/uni00000025/uni00000036/uni00000030/uni00000024/uni00000026 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000024/uni00000036/uni0000002b/uni00000024/uni00000036/uni00000030/uni00000024/uni00000026 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000033/uni0000002f /uni00000033/uni00000027/uni00000014/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 Figure 3: The critical difference diagrams at 50% and 100% of the HPO budget. The ranks indicate the sorted position in terms of regret, aggregated across datasets (the lower the better). Thick horizontal lines highlight differences that are not statistically significant. /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000017/uni00000013/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni0000003a/uni00000044/uni0000004f/uni0000004f/uni00000057/uni0000004c/uni00000050/uni00000048 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000033/uni00000027/uni00000014 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 4: The average normalized regret of DPL and rival methods over the normalized time for all the considered benchmarks. Solid curves and shaded regions represent the mean and standard average normalized regret. In addition, Figure 3 provides the critical differ- ence diagrams of the per-dataset regret ranks at 50% and 100% of the HPO budget. Our method DPL outperforms all baselines in 5 out of 6 cases (in 4 of which with a statistically significant mar- gin), while being second best only at the 50% of the HPO budget on the PD1 benchmark. We investigate the lack of statistical significance in PD1, by analyzing the individual dataset perfor- mances where DPL performs worse compared to other baselines. We notice that the datasets have a skewed distribution of hyperparameter configuration performances, where, a majority of the configurations achieve top performance. Based on the results, we conclude that a lack of statistical significance is the outcome of a search space that includes relatively simple optimiza- tion tasks and not a specific failure state of our method. We provide the detailed results of our analysis in Appendix D. Lastly, we analyse the performance of DPL over time in Figure 4. As it can be observed, DPL manages to outperform the competitors even when the method’s overhead time is included, showing that the overhead of DPL (i.e. fitting surrogate and running the acquisition) is negligible in terms of the quality of the HPO results. For a more detailed information, regarding the DPL overhead time, we point to Appendix E. TaskSet is not included in Figure 4 since the benchmark does not offer runtimes. Given the results, we conclude that Hypothesis 2 is validated and that DPL achieves state-of-the-art performance in HPO. 80.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.2 0.4 0.6 Precision of Top Candidates LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.05 0.10 0.15Average Regret LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Precision of Top Candidates TaskSet 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.15 0.20 0.25 0.30Average Regret TaskSet 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.1 0.2 0.3 0.4 0.5 Fraction of Poor Performer Promotions TaskSet DPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC BaselineDPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC Baseline Figure 5: Post-hoc analysis to study DPL’s efficiency. Left: Share of the best candidates selected during training. Middle: Average regret of configurations chosen to be trained at each budget. Right: Share of top third configurations at a given budget which were bottom two third configurations at a previous budget. Hypothesis 3: DPL explores the search space more efficiently compared to the baselines. We conduct further analyses to understand the source of the efficiency of DPL versus the baselines. As a result, we analyze two important aspects, the quality of the evaluated configurations, as well as the exploration capability of our multi-fidelity HPO method. Initially, we measure what fraction of the top 1% configurations (ranked by accuracy) can our method discover. Figure 5 (left) shows that until convergence our method can discover significantly more top configurations compared to the baselines. The middle plots of Figure 5, show the average regret for each configuration promoted to the respective budget. According to the plot, DPL is more efficient and assigns the budget only to configurations with lower regret compared to the other methods. The precision and regret plots demonstrate that the quality of the evaluated configurations is largely better than all baselines, therefore, giving our method a significant lift in the performance rank. Last but not least, the right plot shows the percentage of configurations that were performing poorly in an earlier epoch (i.e. accuracy-wise in the bottom 2/3 of configurations up to the epoch indicated at the x-axis) but performed better at later epochs (i.e. at the top 1/3 of configurations). Furthermore, we added a line labeled with \"Baseline\", which represents the fraction of previously poor-performing configurations of all configurations. This behavior is observed often with learning curves, where for instance, strongly regularized networks converge slowly. For the same analysis regarding the PD1 benchmark, we point the reader to Appendix H. The results indicate that our method explores well the unpromising early configurations, by consider- ing them through the uncertainty estimation of our ensemble and the respective Bayesian optimization mechanism. The results validate Hypothesis 3 and confirm that DPL explores the search space more efficiently. Hypothesis 4: Our method DPL offers an effective tool for HPO in Large Language Models. In this experiment, we consider the case of tuning the hyperparameters of transformers in Large Language Models. To this end, we computed a tabular benchmark by training a smaller GPT-2 [39] model on the OpenWebText dataset [13] for a series of different hyperparameter configurations. We tune three learning rate hyperparameters: the fraction of warmup steps, the maximum learning rate at the end of warmup, and the minimum learning rate at the end of the decay. We repeat the experiments for seven model sizes ranging from 0.3M to 30M total parameters, ablating the embedding size of the multi-head attention layers (details in Appendix B). We follow the common practice of conducting HPO with small transformers and then deploying the discovered optimal configuration on the full-scale transformers [50]. As a result, we search for the optimal hyperparameters of small transformers (embedding size of {6, 12, . . . ,96, 192}) and then evaluate the discovered configurations at a full-scale transformer with an embedding size of 384. 90 3 6 9 12 6.20 6.40 6.60 6.80 7.00Validation Error Embedding size = 12 0 4 8 12 16 HPO Budget [GPU-hours] 6.00 7.00 Embedding size = 48 0 6 12 18 24 5.00 6.00 7.00 Embedding size = 192 12 48 192 3.90 3.92 3.95 3.98Error at Largest Scale Normalized HPO budget = 25% 12 48 192 Small Transformer Scale [embedding size] 3.90 3.92 3.95 3.98 Normalized HPO budget = 50% 12 48 192 3.90 3.92 3.94 3.96 Normalized HPO budget = 100% DPL BOHB Random oracle at small scale oracle at largest scale Figure 6: HPO for Transformer architectures. Top: HPO on small-scale transformers in terms of the embedding size. Bottom: Error on the full-scale transformer, using the hyperparameter configuration discovered by conducting HPO using the small transformers. We present three analyses, ablating the HPO time on the small-scale transformer up to the HPO budget of 2 full function evaluations. Figure 6 shows the HPO results of DPL against Random Search and BOHB (a rival multi-fidelity HPO baseline). In the top row of plots, we observe the performance of the discovered configurations at the small transformers for the indicated embedding size. We observe that our method finds better configurations than the baselines at any proxy space with small embedding sizes. On the other hand, the bottom row of plots presents the performance of the discovered configurations in the small embedding space, by applying such hyperparameter configurations to the full-scale transformers. We observe that the configurations discovered by DPL on the small search space achieve very competitive results on the full-scale transformers, finding the oracle configuration of the full-scale transformers in the majority of cases. It takes DPL 3.6 hours to find the oracle configuration for the largest model via HPO for the smallest model. In turn, it takes 21.52 hours to train the largest model only once. For more details, we refer the reader to Appendix B. The results validate Hypothesis 4 and confirm that DPL is an efficient HPO technique for tuning the hyperparameters of large language models when the HPO is conducted using smaller transformer model sizes. 7 Conclusions Summary. In this work, we introduce Deep Power Law (DPL), a probabilistic surrogate based on an ensemble of power law functions. The proposed surrogate is used within a novel multi-fidelity Hyperparameter Optimization (HPO) method based on Bayesian optimization. In contrast to the prior work, we exploit scaling laws for estimating the performance of Deep Learning (DL) models. Through extensive experiments comprising 7 baselines, 59 datasets, and search spaces of diverse deep learning architectures, we show that DPL outperforms strong HPO baselines for DL by a large margin. As an overarching contribution, we advance the state-of-the-art in the important field of HPO for DL. Limitations and future work. Contrary to the common perception, we experienced that the uncertainty estimation arising from the Deep Ensemble approach [ 25] is suboptimal compared to standard BO surrogates such as Gaussian Processes. In addition, having to train an ensemble has additional computational costs, due to the necessity of training multiple power law models. In the future, we plan to investigate the combination of power laws with Gaussian Processes, as well as investigate additional fidelity types. 10Acknowledgements JG, AK and MJ would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, JG and AK acknowledge the support of the BrainLinks-BrainTools center of excellence. References [1] Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. [2] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural archi- tecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. [3] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [4] Benedetto J. Buratti and Eli Upfal. Ordalia: Deep learning hyperparameter search via general- ization error bounds extrapolation. In 2019 IEEE International Conference on Big Data (Big Data), pages 180–187, 2019. [5] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws, 2022. [6] Akshay Chandrashekaran and Ian R. Lane. Speeding up hyper-parameter optimization by extrapolation of learning curves using previous builds. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18-22, 2017, Proceedings, Part I , volume 10534 of Lecture Notes in Computer Science, pages 477–492. Springer, 2017. [7] Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. [8] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. [9] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. [10] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , pages 1165–1173, 2017. [11] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [12] Pieter Gijsbers, Erin LeDell, Janek Thomas, Sébastien Poirier, Bernd Bischl, and Joaquin Vanschoren. An open source automl benchmark. arXiv preprint arXiv:1907.00909, 2019. 11[13] Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. [14] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. CoRR, abs/1712.00409, 2017. [15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [16] Achin Jain, Gurumurthy Swaminathan, Paolo Favaro, Hao Yang, Avinash Ravichandran, Hrayr Harutyunyan, Alessandro Achille, Onkar Dabeer, Bernt Schiele, Ashwin Swaminathan, and Stefano Soatto. A meta-learning approach to predicting performance and data requirements. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3623–3632, June 2023. [17] Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyper- parameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 240–248, 2016. [18] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi- fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. [19] Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christo- pher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21:81:1–81:27, 2020. [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [21] Andrej Karpathy. nanoGPT. https://github.com/karpathy/nanoGPT, 2023. [22] Abdus Salam Khazi, Sebastian Pineda Arango, and Josif Grabocka. Deep ranking ensembles for hyperparameter optimization. In The Eleventh International Conference on Learning Representations, 2023. [23] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Rep- resentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre- dictive uncertainty estimation using deep ensembles. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Informa- tion Processing Systems, volume 30. Curran Associates, Inc., 2017. [26] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022. 12[27] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 5, 2018. [28] Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18:185:1–185:52, 2017. [29] Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [30] Marius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, René Sass, and Frank Hutter. Smac3: A versatile bayesian optimization package for hyperparameter optimization. Journal of Machine Learning Research (JMLR) – MLOSS, 23(54):1–9, 2022. [31] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. [32] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter optimization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2113–2122, 2015. [33] Rafid Mahmood, James Lucas, Jose M. Alvarez, Sanja Fidler, and Marc Law. Optimizing data collection for machine learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 29915–29928. Curran Associates, Inc., 2022. [34] Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. [35] Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2(117-129):2, 1978. [36] Felix Mohr and Jan N van Rijn. Learning curves for decision making in supervised machine learning–a survey. arXiv preprint arXiv:2201.12150, 2022. [37] OpenAI. Gpt-4 technical report, 2023. [38] Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparam- eter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [39] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [40] Jonathan S. Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of pruning across scales. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 9075–9083. PMLR, 2021. [41] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [42] David Salinas, Matthias Seeger, Aaron Klein, Valerio Perrone, Martin Wistuba, and Cedric Archambeau. Syne tune: A library for large scale hyperparameter tuning and reproducible research. In First Conference on Automated Machine Learning (Main Track), 2022. 13[43] Mingxing Tan and Quoc Le. EfficientNetV2: Smaller models and faster training. github.com/ google/automl/tree/master/efficientnetv2, 2021. [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [45] Zi Wang, George E Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-training helps bayesian optimization too. arXiv preprint arXiv:2207.03084, 2022. [46] Ross Wightman. PyTorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. [47] Martin Wistuba, Arlind Kadra, and Josif Grabocka. Supervising the multi-fidelity race of hyperparameter configurations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [48] Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 10303–10312, 2020. [49] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017. [50] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer. CoRR, abs/2203.03466, 2022. [51] Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch tabular: Multi-fidelity metalearning for efficient and robust autodl. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(9):3079 – 3090, 2021. 14A Modeling /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000026/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000014 /uni00000026/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000015 /uni00000025/uni00000055/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000002f/uni00000044/uni0000005a /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000036/uni00000033/uni0000002f Figure 7: Left: The performance of different power law formulations, as well as, the baselines on the LCBench benchmark over the HPO budget. Right: The performance of the power law formulation when min-smoothing is applied to the learning curve to ensure a monotonically decreasing learning curve. Label Formula DPL α+β · b−γ Candidate 1 α−β · (b +d)−γ Candidate 2 α−β · (e · b +d)−γ Broken Law α+β · b−γ · \u0010 1 + \u0010 b d 1 f \u0011\u0011−c·f Table 1: Alternative power law formulations for the DPL surrogate. To inspect the modeling choices of the power law functions used as our surrogate, we con- sider different formulations for the ensemble members of our surrogate as shown in Table 1. Initially, we consider Candidate 1 which can handle shifts in the learning curve by introduc- ing an additional parameter d. Furthermore, we consider a more complex version, Candidate 2, that allows us to additionally scale the budget, by introducing variable e. Lastly, we consider Broken Laws [5] (BPL), which can handle breaking points in the learning curve. We use a version that can handle one breaking point since the authors of the method suggest it as a sufficient formulation to approximate the majority of cases. We run the DPL surrogate with every individual formulation on the LCBench benchmark to investigate their empirical performance. Figure 7 presents the results, where, our chosen surrogate formulation (DPL) despite being the simplest, outperforms all other formulations. The Candidate 2 formulation does not manage to outperform all competitor methods, the Candidate 1 formulation manages to outperform all rival methods, however, only with a marginal difference. The Broken Law formulation manages to outperform all the rival baselines by a considerable margin, however, it still performs worse than the simple power law formulation used for DPL. We would like to point out that the alternative power law formulations are more difficult to opti- mize/run since they are prone to diverge and fall into a dead state given different combinations of parameter values. The most common is division by zero for e.g. d term in the Broken Law formulation, taking the root of a negative number b + d term in Candidate 1, e · b + d for Candidate 2 and the d term in Broken Law. We additionally consider min-smoothing the learning curve y, by taking at each step b of the learning curve the minimal value of the learning curve ysmooth b where, ysmooth b = min (y0, y1, ..., yb). The min-smoothing allows diverging curves to be formulated as power laws since the diverging phase will be substituted with stagnation. Figure 7 shows that incorporating learning curve min-smoothing for our surrogate (SPL) performs comparably to DPL without learning curve smoothing and manages to beat the other HPO baselines. 15B nanoGPT-Bench In recent years, deep learning research has increasingly focused on large-scale models, particularly Large Language Models (LLMs) like the Generative Pre-trained Transformers (GPTs). To evaluate the effectiveness of search algorithms, we propose a benchmark based on the nanoGPT model [21], reproducing the performance of the small GPT-2 [39] model trained on the OpenWebText dataset [13]. Our experimental setup is designed with the practicalities of real-world hardware constraints in mind. The common practice in the field is to perform Hyperparameter Optimization (HPO) on an informative proxy task that adheres to model size scaling laws [37], and then to apply these optimized parameters to larger models. This pragmatic approach is necessitated by the fact that training larger models would require significantly more expensive computational resources and time. Within these constraints, we focus our experiments on NVIDIA RTX 2080 GPUs. Warmup Max Steps Learning Rate Min Max Figure 8: nanoGPT-Bench search space parametrization. Baseline: We train a small nanoGPT model, a scaled-down variant of the small GPT-2, reducing the parameter count to approximately 30 M from the original 124 M parameters. The model architecture includes 6 transformer layers and 6 attention heads, and the embedding size is set to384. The AdamW optimizer is utilized for training, with the first and second moment estimates configured to 0.9 and 0.98, respectively. The weight decay is set to 10−1, and we apply gradient clipping at a value of 1.0 to prevent large gradients from causing instability in the model training. For the training process, we optimize the cross-entropy loss for next-token prediction. The process involves 350 steps, with each step encompassing 1000 random samples, with a batch size of 12. Each data point has a context size of 512 tokens, encoded using OpenAI’s token embeddings (sized of 50304). This procedure ensures that even the most resource-intensive experiments stay within the limits of a single GPU day. HP Values Max LR [10−5; 10−4; 10−3] Min LR [1%; 10%] of Max LR Warmup Steps [10%; 20%] of Budget Table 2: Search space of nanoGPT- Bench. Search Space: Our hyperparameter search space con- struction involves varying the number of warmup steps, along with the maximum and minimum learning rates for the cosine scheduler. The specific parametrization of the scheduler is illustrated in Figure 8. The discretized choices are presented in detail in Table 2. Fidelity Space: To construct the fidelity space, we focus on two key dimensions: the number of training steps and the transformer’s embedding size, serving as a proxy for model size. In this exploration, the natural fidelity of the number of training steps is visualized by the validation curves during model training as depicted in Figure 9. On the other hand, the end performance correlation between the different fidelities is reflected by the Pearson correlation in Table 3. We establish proxy tasks by sampling embedding size from a log scale,{6, 12, . . . ,96, 192}, with the maximum being 384. Consequently, each configuration has 6 proxy and one target task, leading to a total of 84 unique configurations in our full benchmark. Every configuration is trained for 350 steps. 1650 200 350 8 10 Embedding size = 6 50 200 350 Steps 6 8 10 Embedding size = 12 50 200 350 6 8 10 Embedding size = 24 50 200 350 6 8 10Validation Error Embedding size = 48 50 200 350 Steps 6 8 10 Embedding size = 96 50 200 350 4 6 8 10 Embedding size = 192 50 200 350 Steps 4 6 8 10 Embedding size = 384 Figure 9: Validation loss curves during model training for all nanoGPT-Bench configurations across model fidelity values. 6 48 96 192 384 0 1 2 3Size [bytes] 1e7 6 48 96 192 384 Small Transformer Scale [embedding size] 0.00 0.25 0.50 0.75 1.00FLOPS 1e11 6 48 96 192 384 10 15 20Runtime [GPU-hours] Figure 10: Scaling of model size in relation to bytes, FLOPS, and runtime based on average values across all nanoGPT-Bench configurations. Due to the GPU under-utilization of small model sizes, the runtime scales linearly as the model size scales exponentially. This relationship can be observed in Figure 10 and Figure 11. We expect the runtime to scale in a linear proportion to the model size when larger models are considered. Embedding Size Correlation 6 0 .951 12 0 .880 24 0 .971 48 0 .955 96 0 .987 192 0 .994 384 1 .000 Table 3: Pearson correlation across 7 fidelities. Figure 12 illustrates the effectiveness of DPL, particularly when the number of training steps is considered as a fidelity dimension. Vertical dotted lines denote the iteration at which an algo- rithm identifies the oracle value with an absolute tolerance of 0.01. Figure 13 depicts the results over the different values of the embedding fidelity. We utilized DPL, BOHB, and random search in proxy tasks, incrementing the budget allocation over each run, up to a horizon of 6 full-function evalua- tions. From these proxy tasks, we extracted the incumbent hyperparameters and evaluated their performance on the target task, that correponds to the maximum embedding size of 384. Despite operating within short regimes, DPL consistently outperforms baselines in terms of the mean incumbent value and the explored regime, as evidenced by error bars indicating the range between best and worst incumbents across 10 seeds. It should be noted, however, that the correlation between 176.7 6.8 6.9 7.0 7.10 1 2 3 Embedding size = 6 7.0 7.1 7.2 7.3 7.4 Runtime [GPU-hours] 0 2 4 Embedding size = 12 7.2 7.3 7.4 7.50 1 2 3 Embedding size = 24 7.95 8.00 8.05 8.10 8.150 1 2 3Count Embedding size = 48 8.90 8.95 9.00 9.05 9.10 9.15 Runtime [GPU-hours] 0 1 2 3 Embedding size = 96 13.6 13.7 13.8 13.9 14.00 1 2 3 Embedding size = 192 21.4 21.6 21.8 Runtime [GPU-hours] 0 1 2 3 Embedding size = 384 Figure 11: Distribution of GPU-hours required for training across different model fidelity values. 0 10 20 30 40 6.60 6.70 6.80 6.90 7.00 Embedding size = 6 0 10 20 30 40 HPO Budget [GPU-hours] 6.20 6.40 6.60 6.80 7.00 Embedding size = 12 0 10 20 30 40 6.00 7.00 Embedding size = 24 0 10 20 30 40 6.00 7.00Validation Error Embedding size = 48 0 15 30 45 HPO Budget [GPU-hours] 5.00 6.00 7.00 Embedding size = 96 0 20 40 60 80 5.00 6.00 7.00 Embedding size = 192 0 30 60 90 120 HPO Budget [GPU-hours] 4.00 5.00 6.00 7.00 Embedding size = 384 DPL BOHB Random oracle at small scale Figure 12: The incumbent performance of DPL and other baselines during the HPO budget of 6 full function evaluations for different values of the embedding size fidelity. Dashed lines indicate the point at which the oracle has been evaluated for every algorithm. Solid curves and shaded areas stand for mean value across runs and standard error. proxy and target tasks is not always perfect. This can result in a proxy task incumbent that does not translate to the oracle in the target task, which can be observed particularly at lower fidelity levels. 186 12 24 48 96 192 4.00 4.20 4.40Error at Largest Scale HPO budget = 1 full func evals 6 12 24 48 96 192 Small Transformer Scale [embedding size] 3.90 3.95 4.00 HPO budget = 2 full func evals 6 12 24 48 96 192 3.90 3.95 4.00 HPO budget = 3 full func evals 6 12 24 48 96 192 3.90 3.95 4.00Error at Largest Scale HPO budget = 4 full func evals 6 12 24 48 96 192 Small Transformer Scale [embedding size] 3.90 3.95 4.00 HPO budget = 5 full func evals 6 12 24 48 96 192 3.90 3.95 4.00 HPO budget = 6 full func evals DPL BOHB Random oracle at largest scale Figure 13: The target task performance distribution for DPL and other methods over different HPO budgets ranging from 1 − 6 full function evaluations. C Continuous Search Space 0.00 0.25 0.50 0.75 1.00 HPO Budget 0.1 0.3 0.6 0.9Incumbent Error DPL BOHB Random SMAC Figure 14: The incumbent error of DPL, as well as, the baselines on the CIFAR10 task over the HPO budget. The primary objective of this study is to investigate the efficacy of Deep Power Laws (DPL) in trading off exploration vs exploitation in a continuous HPO search space. In this study, we do not make use of pre-computed tabular tables, but instead we optimize the hyperparameters of an EfficientNetV2 model online, by iteratively pausing unpromising configurations and moving forward only promising hyperparameter configurations during the HPO optimization procedure. To benchmark our findings, we contrast the results against established baseline algorithms such as random search, BOHB, and SMAC. Baseline: We employ EfficientNetV2 [43] as a benchmarking model and train it on the CIFAR10 dataset [24]. Specifically, we train the lightweight variant of EfficientNetV2-b0 from scratch for 50 epochs, using the RMSprop optimizer. The learning rate is initiated at 10−6 and gradually increased over a span of five warmup epochs to reach the learning rate value of5 ·10−4. Following the warmup phase, we employ a cosine learning rate scheduler, with a decay factor of 0.97 applied every 10 epochs. The weight decay is set at 10−5, with no momentum used. Furthermore, the dropout rate is configured to be 10−6 and the model’s moving average exponential decay is set at0.9996. During the training phase, the batch size is set to 64, while for the validation phase, it is reduced to 8. All experiments are performed using the timm library [46]. 19HP Values LR [10−5, 10−2] weight decay [0, 10−1] Table 4: Search space of CIFAR10 task. Search Space: In our experiment, we concen- trate on optimizing the two most critical hy- perparameters, learning rate, and weight decay, while keeping the remaining hyperparameters fixed as per the baseline model. We construct a search space for these two hyperparameters in accordance with common practices (Table 4). To emulate a continuous search space for the acquisition function, we generate 100 equally-sized steps on a logarithmic scale from the lower bound to the upper bound of each dimension. This process yields a search space comprising 104 potential configurations. Our method demonstrates a substantial speedup in terms of anytime performance when compared to baseline algorithms (Figure 14). Acknowledging the practical constraints of evaluating large-scale models, we pragmatically allocated an HPO budget for a maximum of 3 full-function evaluations, equivalent to 150 epochs. The results of our exploration underline the compelling potential of the DPL algorithm to effectively manage HPO tasks within a continuous search space. Exhibiting significant speedup gains, DPL proves itself to be not only viable but an efficient method for identifying optimal hyperparameters. The findings further underscore the adaptability and efficacy of DPL in addressing complex HPO tasks, reinforcing its standing as a valuable tool in the machine learning toolbox. D PD1 Investigation /uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013 /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000056/uni00000059/uni0000004b/uni00000051/uni00000042/uni00000051/uni00000052/uni00000042/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000013/uni00000015/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000046/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000013/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 Figure 15: Configuration test performance histograms for datasets where DPL does not outperform baselines. We investigate the datasets: fashion_mnist_max_pooling_cnn_tanh_batch_size_256, cifar100_wide_resnet_batch_size_256, svhn_no_extra_wide_resnet_batch_size_1024, where DPL does not outperform other methods for the PD1 benchmark as shown in Figure 23. We analyze the test performance of the individual hyperparameter configurations that belong to the aforementioned datasets. Figure 15 shows that the search spaces of the datasets have a skewed distribution of performances, where, there exist a large number of hyperparameter configurations that achieve top performance. In such datasets, even a non-model based technique will quickly find a well-performing configuration, since, there is a high chance for a randomly-sampled configuration to achieve the top performance. For this reason, there is little room for sophisticated HPO techniques in these datasets. To further validate our hypothesis, we investigate two additional datasetsdionis from the LCBench benchmark and uniref50_transformer _batch_size_128 from the PD1 benchmark, where DPL achieves a strong performance compared to baseline HPO methods. The results in Figure 16 show that DPL excels on tasks that are complex and that require optimizations to find hyperparameter configurations that achieve top performance. Based on the results, we conclude that the lack of statistical significance in PD1 is not a specific failure mode of DPL, but a consequence of multiple PD1 datasets where the majority of configurations achieve the top performance. 20/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000047/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056 /uni00000013/uni00000011/uni00000014/uni0000001a/uni00000013/uni00000011/uni00000014/uni0000001b/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000014 /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000058/uni00000051/uni0000004c/uni00000055/uni00000048/uni00000049/uni00000018/uni00000013/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000015/uni0000001b Figure 16: Histograms of the distribution of performances for datasets where DPL performs strongly. 0.0 0.5 1.0 0 20 Embedding size = 6 0.0 0.5 1.0 HPO Budget 0 20 Embedding size = 12 0.0 0.5 1.0 0 20 Embedding size = 24 0.0 0.5 1.0 0 20Percentage of Iteration Time Embedding size = 48 0.0 0.5 1.0 HPO Budget 0 10 Embedding size = 96 0.0 0.5 1.0 0 10 Embedding size = 192 0.0 0.5 1.0 HPO Budget 0 10 Embedding size = 384 Figure 17: The percentage taken by the DPL overhead in the total time per iteration for the different embedding sizes in nanoGPT-Bench. E DPL Overhead To investigate the efficiency of DPL in terms of method runtime, we investigate the percentage of time that the DPL overhead contributes in the total time taken to perform one HPO iteration in the nanoGPT-Bench. The total time taken constitutes of the DPL overhead (training the DPL surrogate, calling the acquisition function to suggest the next hyperparameter configuration) and the time taken to run the target algorithm for one more step. Figure 17 shows that the impact of the DPL overhead is negligible in the total time taken. For the smallest embedding of size 6, DPL takes only 10% of the total time taken to perform one HPO iteration after spending half of the optimization budget. At the end, after circa 40 hours of HPO optimization, DPL has an impact of 20% in the total time taken to perform one HPO iteration. The impact is even smaller for the largest embedding of size 384, where DPL has an impact of only 5% in the total time taken per iteration after spending half of the optimization budget and it has an impact of only 10% in the total time per iteration after more than 120 hours of HPO optimization. The findings validate our claim that DPL has a minor time overhead in performing hyperparameter optimization, which explains the strong any-time performance of our method. 21F Details of Considered Benchmarks LCBench: We use the official implementation as the interface for the LCBench benchmark2. As suggested by the authors, we use the benchmark information starting from the second step and we skip the last step of the curve since it is a repeat of the preceding step. TaskSet: The TaskSet benchmark features 1000 diverse tasks. We decide to focus on only 12 NLP tasks from the TaskSet benchmark to add variety to our entire collection of datasets. Our limitation on the number of included tasks is related to the limited compute power, as we are unable to run for the entire suite of tasks offered in TaskSet. TaskSet features a set of 8 hyperparameters, that consists of i) optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate, β1 and β2, and Adam’s constant for numerical stability ε, ii) hyperparameters that control the linear and exponential decay schedulers for the learning rate decay, and lastly iii) hyperparameters that control the L1 and L2 regularization terms. Every hyperparameter in TaskSet except β1 and β2 is sampled logarithmically. PD1: We use the synetune library [42] for our interface to the PD1 benchmark. From the benchmark, we only include datasets that have a learning curve of length greater than 10. We furthermore only include datasets that have a learning curve lower or equal to 50 to have a fair comparison between all benchmarks by having approximately 20 full-function evaluations. PD1 features 4 numerical hyper- parameters, lr_initial_value, lr_power, lr_decay_steps_factor and one_minus_momentum, where lr_initial_value and one_minus_momentum are log sampled. The learning rate decay is applied based on a polynomial schedule, its hyperparameters taken from the search space. G Baselines Random Search: We implemented random search by randomly sampling hyperparameter configura- tions from the benchmarks with the maximal budget. Hyperband, BOHB, LCNet: We use version 0.7.4 of the HpBandSter library as a common codebase for all 3 baselines 3. For the last approach mentioned, despite heavy hyperparameter tuning of the method, we could not get stable results across all the benchmarks and hence dropped the method from our comparison. ASHA: For the implementation of ASHA we use the public implementation from the optuna library, version 2.10.0. DEHB: We use the public implementation offered by the authors 4. MF-DNN: In our experiments we used the official implementation from the authors 5. However, the method crashes which does not allow for full results on all benchmarks. SMAC: For our experiment with SMAC we used the official code base from the authors 6. Dragonfly: We use version 0.1.6 of the publicly available Dragonfly library. For all the multi-fidelity methods considered in the experiments, we use the same minimal and maximal fidelities. In more detail, for the LCBench, TaskSet and PD1 benchmarks we use a minimal fidelity lower bound of 1 and a maximal fidelity lower bound equal to the max budget. H Plots In Hypothesis 1, we prove that DPL achieves a better performance in comparison to other models in estimating the final performance for different hyperparameter configurations based on partial observations. Our analysis shows that DPL manages to retain the ranks of different hyperparameter 2https://github.com/automl/LCBench 3https://github.com/automl/HpBandSter 4https://github.com/automl/DEHB/ 5https://github.com/shib0li/DNN-MFBO 6https://github.com/automl/SMAC3 22/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018 /uni0000002f/uni00000026/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017/uni00000024/uni00000045/uni00000056/uni00000052/uni0000004f/uni00000058/uni00000057/uni00000048/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055 Figure 18: The absolute relative error distribution of DPL over the different learning curve fractions in the LCBench benchmark. The distribution is calculated from the ground truth and prediction values, averaged over all configurations of a dataset. Every distribution is over the datasets. configurations. We complement Hypothesis 1 by additionally investigating the absolute relative error. We measure the difference between the DPL estimation of the final learning curve value for different fractions of available partial observations vs the actual end performance. Figure 18 shows that DPL does not only retain the ranks of the final performances of different hyperparameter configurations, but it also correctly estimates the final performance by attaining a small relative error, where the error is reduced the more partial observations we have from the learning curve. In Hypothesis 3, we investigate the efficiency of DPL in exploring more promising configurations compared to other HPO methods. In Figure 19 we provide the same comparison with regards to the PD1 benchmark. Based on the results, we conclude that DPL explores more promising configurations compared to other HPO methods. 0.005 0.010 0.015 0.020 LC Length Fraction 0.00 0.05 0.10 0.15 0.20 Precision of T op Candidates PD1 0.005 0.010 0.015 0.020 LC Length Fraction 0.02 0.04 0.06 0.08Average Regret PD1 0.005 0.010 0.015 0.020 LC Length Fraction 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions PD1 DPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC Baseline Figure 19: Post-hoc analysis to study DPL’s efficiency.Left: Share of the best candidates selected during training. Middle: Average regret of configurations chosen to be trained at each budget. Right: Share of top third configurations at a given budget which were bottom two third configurations at a previous budget. Lastly, we provide the per-dataset performances of all methods, where we present the mean regret of the incumbent trajectory and the standard error over 10 runs in LCBench (Figure 20 and 21), TaskSet (Figure 22), and PD1 (Figure 23). The results show that DPL consistently outperforms other methods in the majority of cases achieving strong any-time performance and not only a strong final performance. 23/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000033/uni00000036/uni00000029/uni00000044/uni0000004c/uni0000004f/uni00000058/uni00000055/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000050/uni00000044/uni0000005d/uni00000052/uni00000051/uni00000042/uni00000048/uni00000050/uni00000053/uni0000004f/uni00000052/uni0000005c/uni00000048/uni00000048/uni00000042/uni00000044/uni00000046/uni00000046/uni00000048/uni00000056/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000058/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni0000004c/uni00000044/uni00000051 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002e/uni00000027/uni00000027/uni00000026/uni00000058/uni00000053/uni00000013/uni0000001c/uni00000042/uni00000044/uni00000053/uni00000053/uni00000048/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000025/uni00000052/uni00000052/uni00000031/uni00000028 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni00000047/uni00000058/uni0000004f/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni0000004c/uni00000055/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni0000004f/uni00000045/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000045/uni00000044/uni00000051/uni0000004e/uni00000010/uni00000050/uni00000044/uni00000055/uni0000004e/uni00000048/uni00000057/uni0000004c/uni00000051/uni0000004a /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000045/uni0000004f/uni00000052/uni00000052/uni00000047/uni00000010/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000058/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000056/uni00000048/uni00000055/uni00000059/uni0000004c/uni00000046/uni00000048/uni00000010/uni00000046/uni00000048/uni00000051/uni00000057/uni00000048/uni00000055 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000044/uni00000055 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni0000004b/uni00000055/uni0000004c/uni00000056/uni00000057/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000051/uni00000044/uni00000048/uni00000010/uni0000001c /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000052/uni00000051/uni00000051/uni00000048/uni00000046/uni00000057/uni00000010/uni00000017 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000052/uni00000059/uni00000048/uni00000055/uni00000057/uni0000005c/uni00000053/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000057/uni00000010/uni0000004a /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000047/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 20: Performance comparison over the number of epochs on a dataset level for LCBench. We plot the mean value over 10 runs and the standard error. 24/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000045/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004b/uni00000048/uni0000004f/uni00000048/uni00000051/uni00000044 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004b/uni0000004c/uni0000004a/uni0000004a/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000044/uni00000051/uni00000051/uni0000004c/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000044/uni00000056/uni00000050/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000058/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000042/uni00000046/uni0000004b/uni00000048/uni00000056/uni00000056/uni00000042/uni00000015/uni00000053/uni00000046/uni00000056/uni00000042/uni00000055/uni00000044/uni0000005a/uni00000042/uni00000048/uni00000051/uni00000047/uni0000004a/uni00000044/uni00000050/uni00000048/uni00000042/uni00000046/uni00000052/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000057/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004e/uni00000046/uni00000014 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004e/uni00000055/uni00000010/uni00000059/uni00000056/uni00000010/uni0000004e/uni00000053 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000049/uni00000048/uni00000044/uni00000057/uni00000010/uni00000049/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000051/uni00000052/uni00000050/uni00000044/uni00000052 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000051/uni00000058/uni00000050/uni00000048/uni00000055/uni00000044/uni0000004c/uni00000015/uni0000001b/uni00000011/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048/uni00000050/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni00000048/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni0000004b/uni00000058/uni00000057/uni00000057/uni0000004f/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni0000005c/uni0000004f/uni00000059/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000059/uni00000048/uni0000004b/uni0000004c/uni00000046/uni0000004f/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000059/uni00000052/uni0000004f/uni0000004e/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 21: Performance comparison over the number of epochs on a dataset level for LCBench (cont.). We plot the mean value over 10 runs and the standard error. 25/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014/uni00000017 /uni00000014/uni00000013/uni00000014/uni00000014 /uni00000014/uni00000013/uni0000001b /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000013/uni00000015 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000048/uni00000050/uni00000045/uni00000048/uni00000047/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002a/uni00000035/uni00000038/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002a/uni00000035/uni00000038/uni00000019/uni00000017/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002c/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002c/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000028/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000013 /uni00000015/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000016/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000017/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000019/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 22: Performance comparison over the number of steps on a dataset level for TaskSet. We plot the mean value over 10 runs and the standard error. 26/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000013/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000056/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004f/uni00000050/uni00000014/uni00000045/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000013/uni00000017/uni0000001b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000056/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni00000059/uni0000004b/uni00000051/uni00000042/uni00000051/uni00000052/uni00000042/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000013/uni00000015/uni00000017 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000058/uni00000051/uni0000004c/uni00000055/uni00000048/uni00000049/uni00000018/uni00000013/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000015/uni0000001b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 23: Performance comparison over the fraction of the total optimization iterations on a dataset level for PD1. We plot the mean value over 10 runs and the standard error. 27",
      "meta_data": {
        "arxiv_id": "2302.00441v3",
        "authors": [
          "Arlind Kadra",
          "Maciej Janowski",
          "Martin Wistuba",
          "Josif Grabocka"
        ],
        "published_date": "2023-02-01T13:39:07Z",
        "pdf_url": "https://arxiv.org/pdf/2302.00441v3.pdf",
        "github_url": "https://github.com/releaunifreiburg/DPL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Deep Power Laws (DPL), a novel multi-fidelity Hyperparameter Optimization (HPO) method that leverages the power-law nature of learning curves to efficiently tune hyperparameters for Deep Learning (DL) models. DPL proposes a probabilistic surrogate based on an ensemble of deep power law functions, which, when combined with Bayesian optimization, dynamically decides which configurations to pause and train incrementally. The method achieves state-of-the-art results in HPO for DL across diverse modalities and architectures, making HPO for DL more feasible.",
        "methodology": "DPL models learning curves as power law functions: `f̂(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`, where `g` is a parametric neural network mapping hyperparameter configurations `λ` to the power law coefficients `α, β, γ`, and `b` is the budget (epochs). An ensemble of K such neural networks is trained to provide probabilistic predictions (mean `μ` and variance `σ²`). Bayesian Optimization (BO) with an Expected Improvement (EI) acquisition function is used to recommend the next hyperparameter configuration `λN+1`. Instead of running `λN+1` to full convergence, a multi-fidelity strategy incrementally advances it by a small budget `b_step` (e.g., one epoch). The network is a 2-layer feedforward neural network with 128 units per layer, Leaky ReLU, and GLU non-linearity on `β` and `γ` output units, trained with L1 loss and Adam.",
        "experimental_setup": "DPL was evaluated against 7 state-of-the-art HPO baselines (Random Search, Hyperband, ASHA, BOHB, DEHB, SMAC, BOCA from Dragonfly Library) on 3 benchmarks covering 59 diverse tasks: LCBench (2,000 configurations, 7 numerical hyperparameters, 35 datasets from AutoML benchmark for feedforward neural networks), PD1 (deep learning benchmark with Transformers, ResNeXt on large vision, statistical modeling, and protein sequence datasets, varying learning curve lengths and 807-2807 configurations), and TaskSet (12 RNN text classification tasks from Adam8p search space with 1000 configurations and 8 continuous hyperparameters). Additionally, experiments were conducted on nanoGPT-Bench for Large Language Models (GPT-2 on OpenWebText, tuning learning rates and warmup steps across 7 model sizes (0.3M to 30M parameters) using embedding size as a fidelity dimension, evaluated on an NVIDIA RTX 2080 GPU, then transferring optimal HPs to a full-scale transformer). A continuous search space experiment on CIFAR10 with EfficientNetV2 was also performed, optimizing learning rate and weight decay.",
        "limitations": "The uncertainty estimation derived from the Deep Ensemble approach was found to be suboptimal compared to standard BO surrogates like Gaussian Processes. Additionally, training an ensemble incurs extra computational costs due to the necessity of training multiple power law models.",
        "future_research_directions": "Future work includes investigating the combination of power laws with Gaussian Processes to improve uncertainty estimation, and exploring additional types of fidelities beyond training epochs and model size.",
        "experimental_code": "import torch\nimport torch.nn as nn\n\n\nclass ConditionedPowerLaw(nn.Module):\n\n    def __init__(\n        self,\n        nr_initial_features=10,\n        nr_units=200,\n        nr_layers=3,\n        use_learning_curve: bool = True,\n        kernel_size: int = 3,\n        nr_filters: int = 4,\n        nr_cnn_layers: int = 2,\n    ):\n        \"\"\"\n        Args:\n            nr_initial_features: int\n                The number of features per example.\n            nr_units: int\n                The number of units for every layer.\n            nr_layers: int\n                The number of layers for the neural network.\n            use_learning_curve: bool\n                If the learning curve should be use in the network.\n            kernel_size: int\n                The size of the kernel that is applied in the cnn layer.\n            nr_filters: int\n                The number of filters that are used in the cnn layers.\n            nr_cnn_layers: int\n                The number of cnn layers to be used.\n        \"\"\"\n        super(ConditionedPowerLaw, self).__init__()\n\n        self.use_learning_curve = use_learning_curve\n        self.kernel_size = kernel_size\n        self.nr_filters = nr_filters\n        self.nr_cnn_layers = nr_cnn_layers\n\n        self.act_func = torch.nn.LeakyReLU()\n        self.last_act_func = torch.nn.GLU()\n        self.tan_func = torch.nn.Tanh()\n        self.batch_norm = torch.nn.BatchNorm1d\n\n        layers = []\n        # adding one since we concatenate the features with the budget\n        nr_initial_features = nr_initial_features\n        if self.use_learning_curve:\n            nr_initial_features = nr_initial_features + nr_filters\n\n        layers.append(nn.Linear(nr_initial_features, nr_units))\n        layers.append(self.act_func)\n\n        for i in range(2, nr_layers + 1):\n            layers.append(nn.Linear(nr_units, nr_units))\n            layers.append(self.act_func)\n\n        last_layer = nn.Linear(nr_units, 3)\n        layers.append(last_layer)\n\n        self.layers = torch.nn.Sequential(*layers)\n\n        cnn_part = []\n        if use_learning_curve:\n            cnn_part.append(\n                nn.Conv1d(\n                    in_channels=2,\n                    kernel_size=(self.kernel_size,),\n                    out_channels=self.nr_filters,\n                ),\n            )\n            for i in range(1, self.nr_cnn_layers):\n                cnn_part.append(self.act_func)\n                cnn_part.append(\n                    nn.Conv1d(\n                        in_channels=self.nr_filters,\n                        kernel_size=(self.kernel_size,),\n                        out_channels=self.nr_filters,\n                    ),\n                ),\n            cnn_part.append(nn.AdaptiveAvgPool1d(1))\n\n        self.cnn = nn.Sequential(*cnn_part)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        predict_budgets: torch.Tensor,\n        evaluated_budgets: torch.Tensor,\n        learning_curves: torch.Tensor,\n    ):\n        \"\"\"\n        Args:\n            x: torch.Tensor\n                The examples.\n            predict_budgets: torch.Tensor\n                The budgets for which the performance will be predicted for the\n                hyperparameter configurations.\n            evaluated_budgets: torch.Tensor\n                The budgets for which the hyperparameter configurations have been\n                evaluated so far.\n            learning_curves: torch.Tensor\n                The learning curves for the hyperparameter configurations.\n        \"\"\"\n        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)\n        if self.use_learning_curve:\n            lc_features = self.cnn(learning_curves)\n            # revert the output from the cnn into nr_rows x nr_kernels.\n            lc_features = torch.squeeze(lc_features, 2)\n            x = torch.cat((x, lc_features), dim=1)\n\n        x = self.layers(x)\n        alphas = x[:, 0]\n        betas = x[:, 1]\n        gammas = x[:, 2]\n\n        output = torch.add(\n            alphas,\n            torch.mul(\n                self.last_act_func(torch.cat((betas, betas))),\n                torch.pow(\n                    predict_budgets,\n                    torch.mul(self.last_act_func(torch.cat((gammas, gammas))), -1)\n                )\n            ),\n        )\n\n        return output\n\nimport logging\nimport os\nimport time\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom scipy.stats import norm\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom data_loader.tabular_data_loader import WrappedDataLoader\nfrom dataset.tabular_dataset import TabularDataset\n\n# Assuming ConditionedPowerLaw is imported from models.conditioned_power_law\n# from models.conditioned_power_law import ConditionedPowerLaw\n\nclass PowerLawSurrogate:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        surrogate_configs: dict = None,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        ensemble_size: int = 5,\n        nr_epochs: int = 250,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 1000,\n        device: str = None,\n        output_path: str = '.',\n        dataset_name: str = 'unknown',\n        pretrain: bool = False,\n        backbone: str = 'power_law',\n        max_value: float = 100,\n        min_value: float = 0,\n        fill_value: str = 'zero',\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for a given dataset.\n            surrogate_configs: dict\n                The model configurations for the surrogate.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            ensemble_size: int\n                The number of members in the ensemble.\n            nr_epochs: int\n                Number of epochs for which the surrogate should be\n                trained.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If for the evaluation metric, the lower the value the better.\n            total_budget: int\n                The total budget given. Used to calculate the initialization\n                percentage.\n            device: str\n                The device where the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            pretrain: bool\n                If the surrogate will be pretrained before with a synthetic\n                curve.\n            backbone: str\n                The backbone, which can either be 'power_law' or 'nn'.\n            max_value: float\n                The maximal value for the dataset.\n            min_value: float\n                The minimal value for the dataset.\n            fill_value: str = 'zero',\n                The filling strategy for when learning curves are used.\n                Either 'zero' or 'last' where last represents the last value.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n        self.total_budget = total_budget\n        self.fill_value = fill_value\n        self.max_value = max_value\n        self.min_value = min_value\n        self.backbone = backbone\n\n        self.model_instances = [\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n        ]\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.learning_rate = 0.001\n        self.batch_size = 64\n        self.refine_batch_size = 64\n\n        self.criterion = torch.nn.L1Loss()\n        self.hp_candidates = hp_candidates\n\n        self.minimization = minimization\n        self.seed = seed\n\n        self.logger = logging.getLogger('power_law')\n        logging.basicConfig(\n            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',\n            level=logging.INFO,\n            force=True,\n        )\n\n        self.fraction_random_configs = 0.1\n        self.iteration_probabilities = np.random.rand(self.total_budget)\n\n        self.examples = dict()\n        self.performances = dict()\n\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.seeds = np.random.choice(100, ensemble_size, replace=False)\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.ensemble_size = ensemble_size\n        self.nr_epochs = nr_epochs\n        self.refine_nr_epochs = 20\n        self.fantasize_step = fantasize_step\n\n        self.pretrain = pretrain\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        init_budgets = [i for i in range(1, conf_individual_budget + 1)]\n\n        self.rand_init_conf_indices = []\n        self.rand_init_budgets = []\n\n        for config_index in init_conf_indices:\n            for config_budget in init_budgets:\n                self.rand_init_conf_indices.append(config_index)\n                self.rand_init_budgets.append(config_budget)\n\n        self.initial_random_index = 0\n\n        if surrogate_configs is None:\n\n            self.surrogate_configs = []\n            for i in range(0, self.ensemble_size):\n                self.surrogate_configs.append(\n                    {\n                        'nr_units': 128,\n                        'nr_layers': 2,\n                        'kernel_size': 3,\n                        'nr_filters': 4,\n                        'nr_cnn_layers': 2,\n                        'use_learning_curve': False,\n                    }\n                )\n        else:\n            self.surrogate_configs = surrogate_configs\n\n        self.nr_features = self.hp_candidates.shape[1]\n        self.best_value_observed = np.inf\n\n        self.diverged_configs = set()\n\n        self.models = []\n        self.last_point = None\n\n        self.initial_full_training_trials = 10\n\n        self.train = True\n\n        self.refine_counter = 0\n        self.iterations_counter = 0\n        self.info_dict = dict()\n\n        self.suggest_time_duration = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n    def _prepare_dataset(self) -> TabularDataset:\n        \"\"\"This method is called to prepare the necessary training dataset\n        for training a model.\n\n        Returns:\n            train_dataset: A dataset consisting of examples, labels, budgets\n                and learning curves.\n        \"\"\"\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_curves = self.prepare_training_curves(train_budgets, train_curves)\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_dataset = TabularDataset(\n            train_examples,\n            train_labels,\n            train_budgets,\n            train_curves,\n        )\n\n        return train_dataset\n\n    def _refine_surrogate(self):\n        \"\"\"Refine the surrogate model.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started refining model with index: {model_index}')\n            refined_model = self.train_pipeline(\n                model_index,\n                train_dataset,\n                nr_epochs=self.refine_nr_epochs,\n                refine=True,\n                weight_new_example=True,\n                batch_size=self.refine_batch_size,\n            )\n\n            self.models[model_index] = refined_model\n\n    def _train_surrogate(self, pretrain: bool = False):\n        \"\"\"Train the surrogate model.\n\n        Trains all the models of the ensemble\n        with different initializations and different\n        data orders.\n\n        Args:\n            pretrain: bool\n                If we have pretrained weights and we will just\n                refine the models.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started training model with index: {model_index}')\n\n            if pretrain:\n                # refine the models that were already pretrained\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.refine_nr_epochs,\n                    refine=True,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.refine_nr_epochs, # basically no early stopping\n                )\n                self.models[model_index] = trained_model\n            else:\n                # train the models for the first time\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.nr_epochs,\n                    refine=False,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.nr_epochs, # basically no early stopping\n                )\n                self.models.append(trained_model)\n\n    def train_pipeline(\n        self,\n        model_index: int,\n        train_dataset: TabularDataset,\n        nr_epochs: int,\n        refine: bool = False,\n        weight_new_example: bool = True,\n        batch_size: int = 64,\n        early_stopping_it: int = 10,\n        activate_early_stopping: bool = False,\n    ) -> torch.nn.Module:\n        \"\"\"Train an algorithm to predict the performance\n        of the hyperparameter configuration based on the budget.\n\n        Args:\n            model_index: int\n                The index of the model.\n            train_dataset: TabularDataset\n                The tabular dataset featuring the examples, labels,\n                budgets and curves.\n            nr_epochs: int\n                The number of epochs to train the model for.\n            refine: bool\n                If an existing model will be refined or if the training\n                will start from scratch.\n            weight_new_example: bool\n                If the last example that was added should be weighted more\n                by being included in every batch. This is only applicable\n                when refine is True.\n            batch_size: int\n                The batch size to be used for training.\n            early_stopping_it: int\n                The early stopping iteration patience.\n            activate_early_stopping: bool\n                Flag controlling the activation.\n\n        Returns:\n            model: torch.nn.Module\n                A trained model.\n        \"\"\"\n        if model_index == 0:\n            self.iterations_counter += 1\n            self.logger.info(f'Iteration number: {self.iterations_counter}')\n\n        surrogate_config = self.surrogate_configs[model_index]\n        seed = self.seeds[model_index]\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if refine:\n            model = self.models[model_index]\n        else:\n            model = self.model_instances[model_index](\n                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,\n                nr_units=surrogate_config['nr_units'],\n                nr_layers=surrogate_config['nr_layers'],\n                use_learning_curve=surrogate_config['use_learning_curve'],\n                kernel_size=surrogate_config['kernel_size'],\n                nr_filters=surrogate_config['nr_filters'],\n                nr_cnn_layers=surrogate_config['nr_cnn_layers'],\n            )\n            model.to(self.dev)\n\n        # make the training dataset here\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n        )\n        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)\n        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n\n        patience_rounds = 0\n        best_loss = np.inf\n        from copy import deepcopy\n        best_state = deepcopy(model.state_dict())\n\n        for epoch in range(0, nr_epochs):\n            running_loss = 0\n            model.train()\n\n            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:\n\n                nr_examples_batch = batch_examples.shape[0]\n                if nr_examples_batch == 1:\n                    continue\n\n                optimizer.zero_grad(set_to_none=True)\n\n                # in case we are refining, we add the new example to every\n                # batch to give it more importance.\n                if refine and weight_new_example:\n                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point\n                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)\n                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])\n                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs\n                    newp_performance = np.array([newp_performance], dtype=np.single)\n                    from copy import deepcopy\n                    modified_curve = deepcopy(newp_curve)\n\n                    difference = self.max_benchmark_epochs - len(modified_curve) - 1\n                    if difference > 0:\n                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)\n\n                    modified_curve = np.array([modified_curve], dtype=np.single)\n                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)\n\n                    # add depth dimension to the train_curves array and missing_value_matrix\n                    modified_curve = np.expand_dims(modified_curve, 1)\n                    newp_missing_values = np.expand_dims(newp_missing_values, 1)\n                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)\n\n                    new_example = torch.tensor(new_example, device=self.dev)\n                    newp_budget = torch.tensor(newp_budget, device=self.dev)\n                    newp_performance = torch.tensor(newp_performance, device=self.dev)\n                    modified_curve = torch.tensor(modified_curve, device=self.dev)\n\n                    batch_examples = torch.cat((batch_examples, new_example))\n                    batch_budgets = torch.cat((batch_budgets, newp_budget))\n                    batch_labels = torch.cat((batch_labels, newp_performance))\n                    batch_curves = torch.cat((batch_curves, modified_curve))\n\n                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)\n                loss = self.criterion(outputs, batch_labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n\n            running_loss = running_loss / len(train_dataloader)\n            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')\n\n            if activate_early_stopping:\n                if running_loss < best_loss:\n                    best_state = deepcopy(model.state_dict())\n                    best_loss = running_loss\n                    patience_rounds = 0\n                elif running_loss > best_loss:\n                    patience_rounds += 1\n                    if patience_rounds == early_stopping_it:\n                        model.load_state_dict(best_state)\n                        self.logger.info(f'Stopping training since validation loss is not improving')\n                        break\n\n        if activate_early_stopping:\n            model.load_state_dict(best_state)\n\n        return model\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the ensemble.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, real_budgets:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices and budgets.\n\n        \"\"\"\n        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()\n        # scale budgets to [0, 1]\n        budgets = np.array(budgets, dtype=np.single)\n        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)\n        budgets = budgets / self.max_benchmark_epochs\n        real_budgets = np.array(real_budgets, dtype=np.single)\n        configurations = np.array(configurations, dtype=np.single)\n\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n        hp_curves = torch.tensor(hp_curves)\n        hp_curves = hp_curves.to(device=self.dev)\n        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)\n        network_real_budgets.to(device=self.dev)\n        all_predictions = []\n\n        for model in self.models:\n            model = model.eval()\n            predictions = model(configurations, budgets, network_real_budgets, hp_curves)\n            all_predictions.append(predictions.detach().cpu().numpy())\n\n        mean_predictions = np.mean(all_predictions, axis=0)\n        std_predictions = np.std(all_predictions, axis=0)\n\n        return mean_predictions, std_predictions, hp_indices, real_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"Suggest a hyperparameter configuration and a budget\n        to evaluate.\n\n        Returns:\n            suggested_hp_index, budget: Tuple[int, int]\n                The index of the hyperparamter configuration to be evaluated\n                and the budget for what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n\n        if self.initial_random_index < len(self.rand_init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. \\n'\n                'Returning randomly sampled configuration'\n            )\n            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]\n            budget = self.rand_init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n        else:\n            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n            )\n            # actually do the mapping between the configuration indices and the best prediction\n            # index\n            suggested_hp_index = hp_indices[best_prediction_index]\n\n            if suggested_hp_index in self.examples:\n                evaluated_budgets = self.examples[suggested_hp_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        return suggested_hp_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        hp_curve: List[float],\n    ):\n        \"\"\"Receive information regarding the performance of a hyperparameter\n        configuration that was suggested.\n\n        Args:\n            hp_index: int\n                The index of the evaluated hyperparameter configuration.\n            b: int\n                The budget for which the hyperparameter configuration was evaluated.\n            hp_curve: List\n                The performance of the hyperparameter configuration.\n        \"\"\"\n        for index, curve_element in enumerate(hp_curve):\n            if np.isnan(curve_element):\n                self.diverged_configs.add(hp_index)\n                # only use the non-nan part of the curve and the corresponding\n                # budget to still have the information in the network\n                hp_curve = hp_curve[0:index + 1]\n                b = index\n                break\n\n        from copy import deepcopy\n        if not self.minimization:\n            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)\n            hp_curve = hp_curve.tolist()\n\n        best_curve_value = min(hp_curve)\n\n        self.examples[hp_index] = np.arange(1, b + 1)\n        self.performances[hp_index] = hp_curve\n\n        if self.best_value_observed > best_curve_value:\n            self.best_value_observed = best_curve_value\n            self.no_improvement_patience = 0\n            self.logger.info(f'New Incumbent value found '\n                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')\n        else:\n            self.no_improvement_patience += 1\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.train = True\n                self.no_improvement_patience = 0\n                self.logger.info(\n                    'No improvement in the incumbent value threshold reached, '\n                    'restarting training from scratch'\n                )\n\n        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0\n        if self.initial_random_index >= len(self.rand_init_conf_indices):\n            performance = self.performances[hp_index]\n            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])\n\n            if self.train:\n                # delete the previously stored models\n                self.models = []\n                if self.pretrain:\n                    # TODO Load the pregiven weights.\n                    pass\n\n                self._train_surrogate(pretrain=self.pretrain)\n\n                if self.iterations_counter <= self.initial_full_training_trials:\n                    self.train = True\n                else:\n                    self.train = False\n            else:\n                self.refine_counter += 1\n                self._refine_surrogate()\n\n    @staticmethod\n    def acq(\n        best_values: np.ndarray,\n        mean_predictions: np.ndarray,\n        std_predictions: np.ndarray,\n        explore_factor: float = 0.25,\n        acq_choice: str = 'ei',\n    ) -> np.ndarray:\n        \"\"\"\n        Calculate the acquisition function based on the network predictions.\n\n        Args:\n        -----\n        best_values: np.ndarray\n            An array with the best value for every configuration.\n            Depending on the implementation it can be different for every\n            configuration.\n        mean_predictions: np.ndarray\n            The mean values of the model predictions.\n        std_predictions: np.ndarray\n            The standard deviation values of the model predictions.\n        explore_factor: float\n            The explore factor, when ucb is used as an acquisition\n            function.\n        acq_choice: str\n            The choice for the acquisition function to use.\n\n        Returns\n        -------\n        acq_values: np.ndarray\n            The values of the acquisition function for every configuration.\n        \"\"\"\n        if acq_choice == 'ei':\n            z = (np.subtract(best_values, mean_predictions))\n            difference = deepcopy(z)\n            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]\n            zero_std_indicator = np.invert(not_zero_std_indicator)\n            z = np.divide(z, std_predictions, where=not_zero_std_indicator)\n            np.place(z, zero_std_indicator, 0)\n            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))\n        elif acq_choice == 'ucb':\n            # we are working with error rates so we multiply the mean with -1\n            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)\n        elif acq_choice == 'thompson':\n            acq_values = np.random.normal(mean_predictions, std_predictions)\n        else:\n            acq_values = mean_predictions\n\n        return acq_values\n\n    def find_suggested_config(\n            self,\n            mean_predictions: np.ndarray,\n            mean_stds: np.ndarray,\n    ) -> int:\n        \"\"\"Return the hyperparameter with the highest acq function value.\n\n        Given the mean predictions and mean standard deviations from the DPL\n        ensemble for every hyperparameter configuraiton, return the hyperparameter\n        configuration that has the highest acquisition function value.\n\n        Args:\n            mean_predictions: np.ndarray\n                The mean predictions of the ensemble for every hyperparameter\n                configuration.\n            mean_stds: np.ndarray\n                The standard deviation predictions of the ensemble for every\n                hyperparameter configuration.\n\n        Returns:\n            max_value_index: int\n                the index of the maximal value.\n\n        \"\"\"\n        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])\n        acq_func_values = self.acq(\n            best_values,\n            mean_predictions,\n            mean_stds,\n            acq_choice='ei',\n        )\n\n        max_value_index = np.argmax(acq_func_values)\n\n        return max_value_index\n\n    # other helper methods for PowerLawSurrogate are omitted for brevity",
        "experimental_info": "DPL models learning curves as power law functions: `f̂(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`. The function `g(λ)` is implemented as a neural network named `ConditionedPowerLaw`.\n\n**Neural Network (`ConditionedPowerLaw`) Settings:**\n*   **Architecture:** 2-layer feedforward neural network.\n*   **Units per layer:** 128 units.\n*   **Activation Functions:** Leaky ReLU for hidden layers, GLU non-linearity applied to the `β` and `γ` output units (implicitly handled by `self.last_act_func(torch.cat((betas, betas)))` and `self.last_act_func(torch.cat((gammas, gammas)))` in the forward pass).\n*   **Output Units:** 3 outputs corresponding to `α`, `β`, and `γ` coefficients.\n\n**Ensemble Settings:**\n*   **Ensemble Size (K):** 5 neural networks.\n\n**Training Settings:**\n*   **Loss Function:** L1 loss (`torch.nn.L1Loss`).\n*   **Optimizer:** Adam optimizer (`torch.optim.Adam`).\n*   **Learning Rate:** 0.001.\n*   **Batch Size:** 64 for initial training, 64 for refinement.\n*   **Initial Training Epochs:** 250 epochs.\n*   **Refinement Epochs:** 20 epochs.\n\n**Bayesian Optimization (BO) and Multi-fidelity Strategy:**\n*   **Acquisition Function:** Expected Improvement (EI).\n*   **Budget Step (`b_step`):** 1 epoch. New hyperparameter configurations are incrementally advanced by 1 epoch.\n*   **Total HPO Budget:** 1000 HPO iterations (total number of epochs across all configurations).\n*   **Device:** CPU."
      }
    },
    {
      "title": "Deep Ranking Ensembles for Hyperparameter Optimization",
      "abstract": "Automatically optimizing the hyperparameters of Machine Learning algorithms\nis one of the primary open questions in AI. Existing work in Hyperparameter\nOptimization (HPO) trains surrogate models for approximating the response\nsurface of hyperparameters as a regression task. In contrast, we hypothesize\nthat the optimal strategy for training surrogates is to preserve the ranks of\nthe performances of hyperparameter configurations as a Learning to Rank\nproblem. As a result, we present a novel method that meta-learns neural network\nsurrogates optimized for ranking the configurations' performances while\nmodeling their uncertainty via ensembling. In a large-scale experimental\nprotocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks,\nwe demonstrate that our method achieves new state-of-the-art results in HPO.",
      "full_text": "Published as a conference paper at ICLR 2023 DEEP RANKING ENSEMBLES FOR HYPERPARAMETER OPTIMIZATION Abdus Salam Khazi∗, Sebastian Pineda Arango∗, Josif Grabocka University of Freiburg Correspondence to Sebastian Pineda Arango: pineda@cs.uni-freiburg.edu ABSTRACT Automatically optimizing the hyperparameters of Machine Learning algorithms is one of the primary open questions in AI. Existing work in Hyperparameter Op- timization (HPO) trains surrogate models for approximating the response surface of hyperparameters as a regression task. In contrast, we hypothesize that the op- timal strategy for training surrogates is to preserve the ranks of the performances of hyperparameter conﬁgurations as a Learning to Rank problem. As a result, we present a novel method that meta-learns neural network surrogates optimized for ranking the conﬁgurations’ performances while modeling their uncertainty via en- sembling. In a large-scale experimental protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks, we demonstrate that our method achieves new state-of-the-art results in HPO. 1 I NTRODUCTION Hyperparameter Optimization (HPO) is a crucial ingredient in training state-of-the-art Machine Learning (ML) algorithms. The three popular families of HPO techniques are Bayesian Optimiza- tion (Hutter et al., 2019), Evolutionary Algorithms (Awad et al., 2021a), and Reinforcement Learn- ing (Wu & Frazier, 2019; Jomaa et al., 2019). Among these paradigms, Bayesian Optimization (BO) stands out as the most popular approach to guide the HPO search. At its core, BO ﬁts a paramet- ric function (called a surrogate) to estimate the evaluated performances (e.g. validation error rates) of a set of hyperparameter conﬁgurations. The task of ﬁtting the surrogate to the observed data points is treated as a probabilistic regression, where the common choice for the surrogate is Gaus- sian Processes (GP) (Snoek et al., 2012). Consequently, BO uses the probabilistic predictions of the conﬁgurations’ performances for exploring the search space of hyperparameters. For an introduction to BO, we refer the interested reader to Hutter et al. (2019). In this paper, we highlight that the current BO approach of training surrogates through a regression task is sub-optimal. We furthermore hypothesize that ﬁtting a surrogate to evaluated conﬁgurations is instead a learning-to-rank (L2R) problem (Burges et al., 2005). The evaluation criterion for HPO is the performance of the top-ranked conﬁguration. In contrast, the regression loss measures the surrogate’s ability to estimate all observed performances and does not pay any special consideration to the top-performing conﬁguration(s). We propose that BO surrogates must be learned to estimate the ranks of the conﬁgurations with a special emphasis on correctly predicting the ranks of the top-performing conﬁgurations. Unfortunately, the current BO machinery cannot be naively extended for L2R, because Gaussian Processes (GP) are not directly applicable to ranking. In this paper, we propose a novel paradigm to train probabilistic surrogates for learning to rank in HPO with neural network ensembles 1. Our networks are learned to minimize L2R listwise losses (Cao et al., 2007), and the ensemble’s uncer- tainty estimation is modeled by training diverse networks via the Deep Ensemble paradigm (Laksh- minarayanan et al., 2017). While there have been a few HPO-related works using ﬂavors of basic ranking losses (Bardenet et al., 2013; Wistuba & Pedapati, 2020;¨Ozt¨urk et al., 2022), ours is the ﬁrst ∗Equal contribution 1Our code is available in the following repository: https://github.com/releaunifreiburg/ DeepRankingEnsembles 1 arXiv:2303.15212v2  [cs.LG]  21 May 2023Published as a conference paper at ICLR 2023 systematic treatment of HPO through a methodologically-principled L2R formulation. To achieve state-of-the-art HPO results, we follow the established practice of transfer-learning the ranking sur- rogates from evaluations on previous datasets (Wistuba & Grabocka, 2021). Furthermore, we boost the transfer quality by using dataset meta-features as an extra source of information (Jomaa et al., 2021a). We conducted large-scale experiments using HPO-B (Pineda Arango et al., 2021), the largest pub- lic HPO benchmark and compared them against 12 state-of-the-art HPO baselines. We ultimately demonstrate that our method Deep Ranking Ensembles (DRE) sets the new state-of-the-art in HPO by a statistically-signiﬁcant margin. This paper introduces three main technical contributions: • We introduce a novel neural network BO surrogate (named Deep Ranking Ensembles) optimized with Learning-to-Rank (L2R) losses; • We propose a new technique for meta-learning our ensemble surrogate from large-scale public meta-datasets; • Deep Ranking Ensembles achieve the new state-of-the-art in HPO, demonstrated through a very large-scale experimental protocol. 2 R ELATED WORK Hyperparameter Optimization (HPO)is a problem that has been well elaborated on during the last decade. The mainstream HPO strategies are Reinforcement Learning (RL) (Wu & Frazier, 2019), evolutionary search (Awad et al., 2021b), and Bayesian optimization (BO) (Hutter et al., 2019). The latter comprises two main components: a surrogate function that approximates the response function given some observations, and an acquisition function that leverages the probabilistic output of the surrogate to explore the search space, ultimately deciding which point to observe next. Previous work covers various choices for the surrogate model family, including Gaussian Processes (Snoek et al., 2012), and Bayesian Neural Networks (Springenberg et al., 2016a). Other authors report the advantages of using ensembles as a surrogate, such as Random Forests Hutter et al. (2011), or ensembles of neural networks White et al. (2021). In contrast, we train BO surrogates using a learning-to-rank problem deﬁnition (Cao et al., 2007). Transfer HPO refers to the problem deﬁnition of speeding up HPO by transferring knowledge from evaluations of hyperparameter conﬁgurations on other auxiliary datasets (Wistuba & Grabocka, 2021; Feurer et al., 2015; 2018). For example, the hyper-parameters of a Gaussian Process can be meta-learned on previous datasets and then transferred to new tasks (Wang et al., 2021). Similarly, a deep GP’s kernel parameters can also be meta-learned across auxiliary tasks (Wistuba & Grabocka, 2021). Another method trains ensembles of GPs weighted proportionally to the similarity between the new task and the auxiliary ones (Wistuba et al., 2016). When performing transfer HPO, it is use- ful to embed additional information about the dataset. Some approaches use dataset meta-features to warm-initialize the HPO (Feurer et al., 2015; Wistuba et al., 2015), or to condition the surrogate during pre-training (Bardenet et al., 2013). Recent works propose an attention mechanism to train dataset-aware surrogates (Wei et al., 2019), or utilize deep sets to extract meta-features (Jomaa et al., 2021b). In complement to the prior work, we meta-learn ranking surrogates with meta-features. Learning to Rank (L2R)is a problem deﬁnition that demands estimating the rank (a.k.a. relevance, or importance) of an instance in a set (Burges et al., 2005). The primary application domain for L2R is information retrieval (ranking websites in a search engine) (Ai et al., 2018), or e-commerce systems (ranking recommended products or advertisements) (Tang & Wang, 2018; Wu et al., 2018). However, L2R is applicable in diverse applications, from learning distance functions among images in computer vision (Cakir et al., 2019), up to ranking ﬁnancial events (Feng et al., 2021). In this paper, we emphasize the link between HPO and L2R and train neural surrogates for BO with L2R. Learning to Rank for HPO is a strategy for conducting HPO with an L2R optimization approach. There exist some literature on transfer-learning HPO methods that employ ranking objective within their transfer mechanisms. SCoT uses a surrogate-based ranking mechanism for transferring hyper- parameter conﬁgurations across datasets (Bardenet et al., 2013). On the other hand, Feurer et al. (2018) use a weighted ensemble of Gaussian Processes with one GP per auxiliary dataset, while the ensemble weights are learned with a pairwise ranking-based loss. Modeling the ranks of the learning 2Published as a conference paper at ICLR 2023 ... ... ... ... ...  Figure 1: The neural architecture of our Deep Ranking Ensembles (DRE) with inputs x (query points) and z (meta-features). curves also helps estimate the performance of conﬁgurations in a multi-ﬁdelity transfer setup (Wis- tuba & Pedapati, 2020). Recent work has demonstrated that pair-wise ranking losses can be used for transfer-learning surrogates in a zero-shot HPO protocol ( ¨Ozt¨urk et al., 2022). However, none of these approaches extensively study the core HPO problem with L2R, nor do they analyze which ranking loss types enable us to learn accurate BO surrogates. 3 D EEP RANKING ENSEMBLES (DRE) 3.1 P RELIMINARIES Hyperparameter Optimization is deﬁned as the problem of tuning the hyperparameters x ∈X of a ML algorithm to minimize the validation error achieved on a datasetD as arg minx∈XLVal (x,D ). The mainstream approach for tuning hyperparameters is Bayesian Optimization (BO), an introduc- tion of which is offered by Hutter et al. (2019). BO relies on ﬁtting a surrogate function for approx- imating the validation error on evaluated hyperparameter conﬁgurations. Consider having evaluated N conﬁgurations on a dataset and their respective validation errors as H = {(x i,y i)}N i=1, where y i = LVal (x,D ). We train a surrogate function ˆy (x i) =f (x i; θ ), typically a Gaussian Process, to estimate the observed y as arg maxθ E(xi,yi)∼pH log p (y i|x i,H/ {(x i,y i)}; θ ). Learning to Rank (L2R) differs from a standard supervised regression because instead of directly estimating the target variable it learns to estimate the rank of the target values. In the context of HPO, we deﬁne the rank of a conﬁguration as r (x i, {y 1,...,y N}) := ∑N j=1 1 yj≤yi. The core of a typical L2R method (Burges et al., 2005) includes training a parametric ranker ˆr (x i) :=f (x i; θ ) that correctly estimates the ranks of observed conﬁgurations’ validation errors. Instead of naively estimating the ranks as a direct regression task, i.e. arg maxθ E(xi,ri)∼pH log p (r i|x i; θ ), L2R techniques prioritize estimating the ranks of top-performing conﬁgurations more than bottom- performing ones (Cao et al., 2007). In general ranking losses can be deﬁned on the basis of sin- gle objects (point-wise approach), pairs of objects (pair-wise approach) or the whole list of objects (list-wise approach) (Chen et al., 2009). 3.2 D EEP RANKING ENSEMBLE (DRE) S URROGATE In this paper, we introduce a novel ranking model based on an ensemble of diverse neural net- works optimized for L2R. We aim to learn neural networks that output the ranking score of a hyperparameter conﬁguration s : X →R. The ranks of the estimated scores should match the true ranks ∑N j=1 1 yj≤yi ≈∑N j=1 1 s(xj;θ)≥s(xi;θ), however, with a higher priority in approximat- ing the ranks of the top-performing conﬁgurations using a weighted list-wise L2R loss (Cao et al., 2007). First of all, we deﬁne the indices of the ranked/ordered conﬁgurations as π : {1,...,N }→ {1,...,N }. Concretely, the ℓ -th observed conﬁguration is the k -th ranked conﬁguration π (ℓ ) =k if k = ∑N j=1 1 yj≤yℓ. Ultimately, we train the scoring network using the following loss: arg min θ N∑ i=1 L(x i,y i,y,θ ) , where L(x i,y i,y,θ ) =−w (π (i )) ·log exps(xπ(i);θ) ∑N j=iexps(xπ(j);θ) (1) 3Published as a conference paper at ICLR 2023 Algorithm 1: Meta-learning the Deep Ranking Ensembles Input : Set of datasets D, Number of iterations J, Number of ensemble scorers M Output: DRE parameters θ1,...,θ M, Meta-feature network parameters φ 1 Initialize scorer networks with parameters θ1,...,θ M ; 2 Initialize the parameters φof the meta-feature network zfrom Jomaa et al. (2021a) ; 3 for j = 1to J do 4 Sample dataset index i∈{1,...,D }, sample scorer network index m∈{1,...,M }; 5 Sample a query set H(s) := {( x(s) 1 ,y(s) 1 ) ,..., ( x(s) N(s) ,y(s) N(s) )} from Di ; 6 Sample a support set H(z) from Di \\H(s) ; 7 Compute meta-features z(H(z); φ) ; 8 Compute rank scores for the query set si = s ( x(s) i ,z ( H(z); φ ) ; θm ) ,i = 1,...,N (s) ; 9 Compute true ranks π(1) ,...,π ( N(s)) ; 10 Compute loss L(π,s; θm,φ) using Equation 1 ; 11 Update the meta-feature network φ←φ−ηφ ∂L(π,s; θm,φ) ∂φ ; 12 Update the ranker network θm ←θm −ηθm ∂L(π,s; θm,φ) ∂θm ; 13 end 14 return θ1,...,θ M,φ ; The weighting functions w : {1,...,N }→ R+ is deﬁned as w(π(i)) = 1 log(π(i)+1) and is used to assign a higher penalty to the top-performing hyper-parameter conﬁgurations, whose correct rank is more important in HPO (Chen et al., 2017). After having trained the scoring model of Equation 1 we estimate the rank of an unobserved conﬁguration as ˆr(x; θ) = ∑N j=1 1 s(xj;θ)≥s(x;θ). Further- more, Bayesian Optimization (BO) needs uncertainty estimates to be able to explore the search space (Hutter et al., 2019). As a result, we model uncertainty by training M diverse neural scorers s1(x,θ1),...,s M(x,θM) with stochastic gradient descent. The diversity of the ensemble scorers is ensured through the established mechanism of applying different per-scorer seeds for sampling mini-batches of hyperparameter conﬁgurations (Lakshminarayanan et al., 2017). Finally, the pos- terior mean and variance of the estimated ranks is computed trivially as µ(x) = 1 N ∑N i=1 ˆr(x; θi) and σ2(x) = 1 N ∑N i=1(ˆr(x; θi) −µ(x))2. The BO pseudo-code and the details for using our Deep Rankers in HPO are explained in Appendix A. 3.3 M ETA-LEARNING THE DEEP RANKING ENSEMBLES HPO is a very challenging problem due to the limited number of evaluated hyperparameter con- ﬁgurations. As a result, the current best practice in HPO relies on transfer-learning the knowledge of hyperparameters2 from evaluations on previous datasets (Wistuba & Grabocka, 2021; Wistuba et al., 2016; Salinas et al., 2020). In this paper, we meta-learn our ranker from Kdatasets assuming we have a set of observations H(k) := {( x(k) 1 ,y(k) 1 ) ,..., ( x(k) Nk,y(k) Nk )} ; k = 1,...,K with Nk evaluated hyperparameter conﬁgurations on the k-th dataset. We meta-learn our ensemble of M Deep Rankers with the meta-learning objective in Equation 2, where we learn to estimate the ranks of all observations on all evaluations for all the previous datasets using the loss of Equation 1. arg min θ1,...,θM K∑ k=1 Nk∑ n=1 M∑ m=1 L ( x(k) n ,y(k) n ,y(k); θm ) (2) 2Even in manually-designed ML systems, experts start their initial guess about hyper-parameters by transfer-learning the conﬁgurations that worked well on past projects (a.k.a. datasets). 4Published as a conference paper at ICLR 2023 −10 0 10 −1 0 1 BO Step 1 −10 0 10 BO Step 2 −10 0 10 BO Step 3 −10 0 10 BO Step 4 −10 0 10 BO Step 5 True Function Expected Improvement Observed Samples Next Sample Figure 2: BO Steps Example with a Random Initialized DRE. EI is scaled and shifted for clarity. −10 0 10 −4 −2 0 2 Scorers’ Output −10 0 10 0 50 100 150 200 Rankers’ Output 1 2 3 4 5 6 7 8 9 10 −4 −2 0 2 Distributions of Scorers’ Output 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 Distributions of Rankers’ Output −10 0 10 0 50 100 150 200 Uncertainty Estimations Figure 3: Understanding the outputs of DRE’s modules. Transfer-learning for HPO suffers from the negative-transfer phenomenon, where the distribution of the validation errors given hyperparameters changes across datasets. In such cases, using dataset meta-features helps condition the transfer only from evaluations on similar datasets (Rakotoarison et al., 2022; Jomaa et al., 2021a). We use the meta-features of Jomaa et al. (2021a) which are based on a deep set formulation (Zaheer et al., 2017) of the pairwise interactions between hyperparameters and their validation errors. The meta-feature network with parameterφtakes a history of evaluations H = {(xi,yi)}N i=1 as its input and outputs aL-dimensional representation of the history asz(H,φ) : (X× R)N →RL. Afterward, the scorer function becomes s(x,z(H; φ); θ) : X× RL →R. In other words, the dataset meta-features are additional features to the scorers. A graphical depiction of our architecture is shown in Figure 1. We update all the scorer networks of the ensemble independently using the loss of Equation 1. The pseudo-code of Algorithm 1 draws an evaluation set (called a query set) which is used as the training batch for updating the parameters of the sampled scorer network. We also meta-learn the meta- feature network (Jomaa et al., 2021a), however, by using a different batch of evaluations (called a support set). We do not meta-learn both the scorer and the meta-feature networks using the same batch of evaluations in order to promote generalization. 4 E XPERIMENTS AND RESULTS 4.1 M OTIVATING EXAMPLE We demonstrate our DRE with 10 base models on a simple sinusoid function y = sin(x+π 2 ) for x ∈[−10,10] sampled with a step size of 0.1 in equally spaced intervals. Further details on the architecture are explained in Section B. In Figure 2, we conduct BO with a variant of DRE without meta-learning, and we start the HPO with 3 initial random observations. We observe that a BO procedure with the Expected Improvement acquisition reaches an optimum after 8 observations. Furthermore, we plot the scorers’ and rankers’ outputs of the second BO step in Figure 3. The analysis illustrates that the distributions of the scorers’ outputs have different ranges because the loss function in Equation 1 models only the target rank, but not the scale of the target. However, the outputs of the rankers display similar distributions in the rank space, which is more adequate for 5Published as a conference paper at ICLR 2023 −10 −5 0 5 10 −1.0 −0.5 0.0 0.5 1.0 −10 −5 0 5 10 −1.5 −1.0 −0.5 0.0 0.5 1.0 Meta-Train Task 1 Meta-Train Task 2 Meta-Train Task 3 Meta-Train Task 4 Meta-Train Task 5 Meta-Test Task Expected Improvement Observed Samples Next Sample Figure 4: Meta-train and Meta-test Tasks (left) for optimizing the function. The meta-learned DRE ﬁnds the optimum in one step (right). computing the ranks’ uncertainties. Moreover, the rank distributions differ in certain regions of the search space, enabling BO to conduct exploration. To showcase the power of transfer-learning, we meta-learn DRE on 5 auxiliary tasks, corresponding to different sinusoidal functions y = sin(x+π 2 + β) with varying β ∈{11,.., 15}, as illustrated in Figure 4 (left). Subsequently, we deploy a meta-learned DRE surrogate on a test task (blue line with β = 8) which was not part of the meta-training set. Figure 4 reveals that DRE directly discovers a global optimum within one BO step (4 total observations). The success is attributed to the fact that the surrogate has been meta-learned to recognize sinusoidal shapes given the 3 initial observations in green, as is clearly shown by the acquisition in Figure 4 (right). 4.2 D ATASETS AND BASELINES We base our experiments on HPO-B (Pineda Arango et al., 2021), the largest public benchmark for HPO. It contains 16 search spaces, each of which comprises a meta-train, meta-test, and meta- validation split. Every split is a set of datasets, and for every dataset, the benchmark contains the validation errors of evaluated hyperparameter conﬁgurations. The benchmark also includes the re- sults of several HPO methods run in those datasets, including transfer and non-transfer algorithms3. Moreover, we generated new results for three additional state-of-the-art baselines (GCP, HEBO, and DKLM) that are not released by HPO-B. The benchmark provides 5 sets of 5 initial random seeds for every task in the meta-test split (86 in total). We use the meta-test datasets to compare the per- formance of the Deep Ranker Ensembles against the baselines. Speciﬁcally, our non-transfer HPO baselines are listed below: • Random Search (RS) (Bergstra & Bengio, 2012) is a simple yet strong baseline that selects a random conﬁguration at every step. • Gaussian Processes (GP) (Snoek et al., 2012) model the response function by computing the posterior distribution of functions induced by the observed data. • DNGO (Snoek et al., 2015) uses a neural network that models the uncertainty with a Bayesian linear regression on the last network layer. • BOHAMIANN (Springenberg et al., 2016b) is also a Bayesian neural network that per- forms Bayesian inference via Hamiltonian Monte Carlo. • Deep-Kernel Gaussian Processes (DKGP) (Wilson et al., 2016) learn a latent representa- tion of the features that are fed to a GP kernel function. • HEBO (Cowen-Rivers et al., 2020) is a state-of-the-art Bayesian optimization method. It combines input and output transformations and a multi-objective acquisition function. We use the implementation contained in the original repository.4 Transfer HPO methods use the evaluations of the tasks included in the meta-train split to meta-learn surrogates, that are subsequently applied for HPO on the meta-test tasks within the same search space. We consider the following baselines: 3Available in https://github.com/releaunifreiburg/HPO-B 4Available in https://github.com/huawei-noah/HEBO 6Published as a conference paper at ICLR 2023 • TST (Wistuba et al., 2016) constructs an ensemble of Gaussian Processes aggregated with a kernel-weighted average. Alternatively,TAFbuilds an ensemble of acquisition functions. • RGPE (Feurer et al., 2018) trains a Gaussian Process per each meta-train task and then combines for a new task through a weighting scheme, which accounts for the ranking per- formance of every base GP model. • FSBO (Wistuba & Grabocka, 2021) pre-trains a Deep Kernel Gaussian Process using meta- train tasks and then ﬁne-tunes the parameters when observations for new tasks are available. • GCP (Salinas et al., 2020) pre-trains a neural network to predict the residual performance on the auxiliary tasks and applies Gaussian Copulas to combine results for a new task. • DKLM (Jomaa et al., 2021b) adds a Deep Set as task contextualization on top of FSBO. We use the same hyperparameters as suggested in the original paper. 4.3 DRE-E XPERIMENTAL SETUP The meta-feature extractor zis based on the Deep Set architecture proposed by Jomaa et al. (2021a) with ﬁve hidden layers and 32 neurons per layer. The ensemble of scorers is composed of 10 MLPs with identical architectures: four layers and 32 neurons that we selected using the meta-validation split from HPO-B. We meta-learn DRE for 5000 epochs with Adam optimizer, learning rate 0.001 and batch size 100. Every element of the batch is a list of 100 elements. We select 20% of the samples in each list as input to the meta-feature extractor. During meta-test in every BO iteration, we update the pre-trained weights for 1000 epochs. For DRE-RI, we initialize randomly the scorers and train them for 1000 epochs using Adam Optimizer with a learning rate of 0.02. Every epoch, we use 20% of the observations to feed the meta-feature extractor. 4.4 R ESEARCH HYPOTHESIS AND EXPERIMENTAL RESULTS (a) Transfer Methods 5 20 35 50 65 80 95 Number of Evaluations 4 5 6 7Average Rank DRE FSBO RGPE TST TAF HEBO DKLM GCP Random (b) Non-Transfer Methods 5 20 35 50 65 80 95 Number of Evaluations 3.5 4.0 4.5 5.0 5.5 Average Rank DRE-RI HEBO GP DKGP BOHA. DNGO Random Figure 5: Results for Transfer and Non-transfer methods. Hypothesis 1. Deep Ranking Ensembles (DRE) achieve state-of-the-art results in transfer HPO. We compare against the transfer HPO baselines listed in Section 4.2 and report the average ranks across all the tasks in the meta-test split of all the HPO-B search spaces. Our protocol uses 5 initial conﬁgurations plus 100 BO iterations across 16 search spaces (the default HPO-B protocol). Our method uses meta-features (Jomaa et al., 2021a) and the scorer parameters are ﬁne-tuned after each BO observation. Figure 5 (left) shows that DRE clearly outperforms all baselines over 100 BO iterations based on the rank among the HPO methods averaged among 86 datasets and 5 runs. We compute the critical dif- ference diagram (Demˇsar, 2006) for 25, 50, and 100 iterations, and show the statistical signiﬁcance 7Published as a conference paper at ICLR 2023 (a) Ranking Loss 5 20 35 50 65 80 95 Number of Evaluations 2.5 3.0 3.5 4.0 Average Rank List W-List Pair Point Reg (b) Meta-features 5 20 35 50 65 80 95 Number of Evaluations 2.50 2.75 3.00 3.25 3.50 Average Rank F,M,T F,M F,T M,T T (c) Acquisition Function 5 20 35 50 65 80 95 Number of Evaluations 2.25 2.50 2.75 3.00 3.25 Average Rank Avg EI UCB Random Figure 6: Results after testing our hypothesis 3-5. of the results in Figure 9a (Appendix E). HEBO is not a transfer HPO method but is presented as a reference. These results demonstrate the advantage of training neural ensembles with L2R since our method outperforms other rivals which also meta-train neural networks (FSBO, DKLM), or en- sembles of neural networks (TST, TAF, RGPE). DRE also attains competitive results in individual search space, as shown in Figure 13, at Appendix E. Hypothesis 2. The randomly-initialized DRE performs competitively in non-transfer HPO. We test the hypothesis by comparing the performance of DRE against the non-transfer baselines mentioned in Section 4.2. Similar to Experiment 1, we compute the average rank over 100 BO iterations, aggregating across all the meta-test tasks of all the search spaces in HPO-B. The results of Figure 5 (right) show that a random initialized DRE (i.e. non meta-learned) is still a competitive surrogate for HPO. It exhibits good performance for up to 30 iterations compared to the other baselines and is second only to HEBO (notice our meta-learned DRE actually outperforms HEBO, Figure 5 (left)). This demonstrates the usefulness of deep ensembles with L2R as general- purpose HPO surrogates. Interestingly, DRE outperforms other surrogates using neural networks, such as BOHAMIANN, DNGO, and DKGP. We present the statistical signiﬁcance of the results after 25, 50, and 100 BO iterations in Figure 9b. Hypothesis 3. A weighted list-wise ranking loss is the best L2R strategy for DRE. We test DRE (meta-learned) with three different L2R losses: point-wise, pair-wise, and list-wise (weighted and non-weighted) ranking losses. Additionally, we compare to a surrogate predicting the performance in the original scale using Mean Squared Error as loss, i.e. a regression. Moreover, we compare the performance to a DRE trained with a regression loss. We omit the meta-features from all variants to avoid confounding factors from the analysis and use Expected Improvement as the acquisition function. The results in Figures 6a and 11a (Appendix E) show the advantage of the list-wise ranking losses over the other type of ranking losses. Moreover, the results highlight the advantage of list-weighted ranking losses, as it attained the best performance over the average rank among 100 BO iterations. Additionally, we observe that pairwise-losses also give a boost in performance compared to point- wise estimations. The message is: ”Any L2R loss is better than the regression one”. Hypothesis 4. Meta-features help the transfer HPO performance of DRE. We evaluate DRE with and without the meta-features extracted by the DeepSet module (Jomaa et al., 2021a), ablating the scenarios with and without meta-learning. Again we use all 16 search spaces from HPO-B for 100 BO iterations, starting with 5 random initial conﬁgurations. DRE uses the weighted list-wise loss, and Expected Improvement as the acquisition. 8Published as a conference paper at ICLR 2023 Figure 6b shows the performance obtained with meta-features (F) considering meta-learning (M) and ﬁne-tuning (T). A missing capital letter in the label stands for an experiment without that aspect (e.g. no M means no meta-learning, etc). The results indicate that the meta-features help DRE achieve better performance, both with and without meta-learning. The results also highlight that ﬁne-tuning (i.e. updating the scorer network’s parameters on the target tasks after each BO step) the meta-learned surrogate is important for achieving the best HPO performance. Further evidence of the signiﬁcance of these results is showcased in Figure 11b (Appendix E). Hypothesis 5. Expected Improvement is the best acquisition function for BO with DRE. We run experiments to address how DRE performs with different acquisition functions, which use DRE’s estimated rank uncertainty to explore the search space with Bayesian Optimization. Con- cretely, we ablate the Upper Conﬁdence Bound (UCB) and Expected Improvement (EI) acquisitions. Additionally, we added Average Rank (Avg) which simply recommends the conﬁguration with the highest estimated average rank, without using the posterior variance of the rank. We also add Ran- dom Search as a reference baseline. Further details on how we apply acquisitions in the BO loop are discussed in Appendix A. In this experiment, we use meta-features and weighted list-wise ranking losses. The results in Figures 6c and 10b (Appendix E) demonstrate that EI is the best choice for the ac- quisition function. As UCB and EI attained overall better performances than the simple average rank (no uncertainty), we conclude the uncertainties computed by DRE are effective in exploring the search space. 4.5 D ISCUSSION ON DRE HYPERPARAMETERS 5 20 35 50 65 80 95 Number of Evaluations 3.5 4.0 4.5 5.0 5.5 Average Rank Scorer Size 16x2 32x2 32x3 32x4 48x2 64x2 Random Figure 7: Average Rank on the meta-validation split from HPO- B. Given that DRE achieves state-of-the-art results across all the 16 search spaces (see Figure 12 in Appendix E) of HPO-B by using the same conﬁguration (e.g. number of layers for the scor- ers, number of layers for meta-feature extractor), we assume our settings (hyper-hyperparameters) are applicable straightfor- wardly to new search spaces. Such a generalization of the hyper- hyperparameters is desirable for any HPO method and liberates practitioners and researchers from having to tune DRE hyper- hyperparameters. In Figure 7 we show an ablation study compar- ing the performance of DRE for different numbers of layers (2, 3, 4), and different numbers of neurons per layer (16, 32, 64) on all the tasks of the meta-validation split from HPO-B. Given the critical difference diagram in Figure 10a, we observe the perfor- mance does not change signiﬁcantly when we vary any of these hyper-hyperparameters. However, we notice that the depth of the scorer is slightly more important to tune than the number of neu- rons per layer. We also notice that even an expressive ensemble of scorers (32x4) is able to generalize well on the meta-test split, as we have shown in our previous experiments. 5 C ONCLUSION The presented empirical results based on a very large-scale experimental protocol provide strong evidence of the state-of-the-art performance of deep ensembles optimized through learning to rank. We demonstrated that our method outperforms a large number of 11 baselines in both transfer and non-transfer HPO. In addition, we validated the design choices of our method through detailed ablations and analyses. Particularly, the results indicate the power of meta-learning surrogates from evaluations on other datasets. Overall, we believe that this paper will set a new trend in the HPO community for moving away from regression-learned surrogate functions in Bayesian Optimization. Finally, our surrogate DRE opens up an effective way to improve the HPO performance in different sub-problems, such as multi-ﬁdelity HPO, multi-objective HPO, or neural architecture search. 9Published as a conference paper at ICLR 2023 ACKNOWLEDGEMENTS This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foun- dation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools Center of Excel- lence. REFERENCES Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. Learning a deep listwise context model for ranking reﬁnement. In The 41st International ACM SIGIR Conference on Research and De- velopment in Information Retrieval, SIGIR ’18, pp. 135–144, New York, NY , USA, 2018. Asso- ciation for Computing Machinery. ISBN 9781450356572. doi: 10.1145/3209978.3209985. URL https://doi.org/10.1145/3209978.3209985. N. Awad, N. Mallik, and F. Hutter. DEHB: Evolutionary hyberband for scalable, robust and efﬁcient hyperparameter optimization. In Z. Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence, IJCAI-21, pp. 2147–2153. ijcai.org, 2021a. Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efﬁcient hyperparameter optimization. CoRR, abs/2105.09821, 2021b. R´emi Bardenet, M ´aty´as Brendel, Bal ´azs K´egl, and Mich `ele Sebag. Collaborative hyperparameter tuning. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 199–207, 2013. James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach. Learn. Res., 13:281–305, 2012. Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hul- lender. Learning to rank using gradient descent. In Proceedings of the 22nd international confer- ence on Machine learning, pp. 89–96, 2005. Fatih Cakir, Kun He, Xide Xia, Brian Kulis, and Stan Sclaroff. Deep metric learning to rank. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach to listwise approach. In Zoubin Ghahramani (ed.), Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20- 24, 2007, volume 227 of ACM International Conference Proceeding Series, pp. 129–136. ACM, 2007. URL https://doi.org/10.1145/1273496.1273513. Huadong Chen, Shujian Huang, David Chiang, Xinyu Dai, and Jiajun Chen. Top-rank enhanced listwise optimization for statistical machine translation. arXiv preprint arXiv:1707.05438, 2017. Wei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, and Hang Li. Ranking measures and loss functions in learning to rank. In Y . Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta (eds.), Advances in Neural Information Processing Systems , volume 22. Curran As- sociates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2009/file/ 2f55707d4193dc27118a0f19a1985716-Paper.pdf. Alexander Imani Cowen-Rivers, Wenlong Lyu, Zhi Wang, Rasul Tutunov, Jianye Hao, Jun Wang, and Haitham Bou-Ammar. HEBO: heteroscedastic evolutionary bayesian optimisation. CoRR, abs/2012.03826, 2020. Janez Demˇsar. Statistical comparisons of classiﬁers over multiple data sets. J. Mach. Learn. Res., 7:1–30, dec 2006. ISSN 1532-4435. Fuli Feng, Moxin Li, Cheng Luo, Ritchie Ng, and Tat-Seng Chua. Hybrid learning to rank for ﬁnan- cial event ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’21, pp. 233–243, New York, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450380379. doi: 10.1145/3404835.3462969. URL https://doi.org/10.1145/3404835.3462969. 10Published as a conference paper at ICLR 2023 Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparameter optimization via meta-learning. In Blai Bonet and Sven Koenig (eds.),Proceedings of the Twenty- Ninth AAAI Conference on Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas, USA , pp. 1128–1135. AAAI Press, 2015. Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for bayesian opti- mization using ranking-weighted gaussian process ensembles. In AutoML Workshop at ICML , volume 7, 2018. Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In Learning and Intelligent Optimization - 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers, pp. 507–523, 2011. doi: 10.1007/978-3-642-25566-3 \\40. Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (eds.). Automated Machine Learning - Meth- ods, Systems, Challenges . The Springer Series on Challenges in Machine Learning. Springer, 2019. ISBN 978-3-030-05317-8. doi: 10.1007/978-3-030-05318-5. Hadi S. Jomaa, Josif Grabocka, and Lars Schmidt-Thieme. Hyp-rl : Hyperparameter optimization by reinforcement learning. CoRR, abs/1906.11527, 2019. URL http://arxiv.org/abs/ 1906.11527. Hadi S Jomaa, Lars Schmidt-Thieme, and Josif Grabocka. Dataset2vec: Learning dataset meta- features. Data Mining and Knowledge Discovery, pp. 1–22, 2021a. Hadi Samer Jomaa, Sebastian Pineda Arango, Lars Schmidt-Thieme, and Josif Grabocka. Transfer learning for bayesian hpo with end-to-end landmark meta-features. In Fifth Workshop on Meta- Learning at the Conference on Neural Information Processing Systems, 2021b. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic- tive uncertainty estimation using deep ensembles. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett (eds.), Ad- vances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6402–6413, 2017. Ekrem ¨Ozt¨urk, Fabio Ferreira, Hadi S. Jomaa, Lars Schmidt-Thieme, Josif Grabocka, and Frank Hutter. Zero-shot automl with pretrained models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv ´ari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pp. 17138–17155. PMLR, 2022. URL https: //proceedings.mlr.press/v162/ozturk22a.html. Sebastian Pineda Arango, Hadi Jomaa, Martin Wistuba, and Josif Grabocka. Hpo- b: A large-scale reproducible benchmark for black-box hpo based on openml. In J. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Process- ing Systems Track on Datasets and Benchmarks , volume 1, 2021. URL https: //datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/ ec8956637a99787bd197eacd77acce5e-Paper-round2.pdf. Herilalaina Rakotoarison, Louisot Milijaona, Andry RASOANAIVO, Michele Sebag, and Marc Schoenauer. Learning meta-features for autoML. In International Conference on Learning Rep- resentations, 2022. URL https://openreview.net/forum?id=DTkEfj0Ygb8. David Salinas, Huibin Shen, and Valerio Perrone. A quantile-based approach for hyperparameter transfer learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp. 8438–8448, 2020. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held De- cember 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 2960–2968, 2012. 11Published as a conference paper at ICLR 2023 Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using deep neural networks. InProceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 2171–2180, 2015. Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Asso- ciates, Inc., 2016a. URL https://proceedings.neurips.cc/paper/2016/file/ a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf. Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 4134–4142, 2016b. Jiaxi Tang and Ke Wang. Ranking distillation: Learning compact ranking models with high performance for recommender system. In Proceedings of the 24th ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining , KDD ’18, pp. 2289–2298, New York, NY , USA, 2018. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/3219819.3220021. URL https://doi.org/10.1145/3219819.3220021. Zi Wang, George E Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained gaussian processes for bayesian optimization. arXiv preprint arXiv:2109.08215, 2021. Ying Wei, Peilin Zhao, Huaxiu Yao, and Junzhou Huang. Transferable neural processes for hy- perparameter optimization. CoRR, abs/1909.03209, 2019. URL http://arxiv.org/abs/ 1909.03209. Colin White, Willie Neiswanger, and Yash Savani. BANANAS: bayesian optimization with neural architectures for neural architecture search. In Thirty-Fifth AAAI Conference on Artiﬁcial Intelli- gence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 10293–10301, 2021. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th International Con- ference on Artiﬁcial Intelligence and Statistics , volume 51 of Proceedings of Machine Learning Research, pp. 370–378, 2016. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML’20. JMLR.org, 2020. Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimiza- tion initializations. In 2015 IEEE International Conference on Data Science and Advanced Ana- lytics, DSAA 2015, Campus des Cordeliers, Paris, France, October 19-21, 2015, pp. 1–10, 2015. doi: 10.1109/DSAA.2015.7344817. Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Two-stage transfer surrogate model for automatic hyperparameter optimization. In Paolo Frasconi, Niels Landwehr, Giuseppe Manco, and Jilles Vreeken (eds.), Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I, volume 9851 of Lecture Notes in Computer Science, pp. 199–214. Springer, 2016. Jian Wu and Peter I. Frazier. Practical Two-Step Look-Ahead Bayesian Optimization. Curran Asso- ciates Inc., Red Hook, NY , USA, 2019. 12Published as a conference paper at ICLR 2023 Liang Wu, Diane Hu, Liangjie Hong, and Huan Liu. Turning clicks into purchases: Revenue opti- mization for product search in e-commerce. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 365–374, 2018. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/ paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf. 13Published as a conference paper at ICLR 2023 A B AYESIAN OPTIMIZATION WITH DEEP RANKING ENSEMBLES Once the Deep Ensembles are trained, we aggregate the predictions for an input x following the procedure explained in Section 3.2 to obtain µ(x),σ(x) and conditioning to a set of observations Ds. For the sake of simplicity, we omit this conditioning in our notation. These outputs can be fed in several types of acquisition functions and decide for the next point xto observe from the set of pending points to evaluate X. Notice that the lower rank, the better the conﬁguration, therefore we formulate the cast the acquisition function as a minimization problem. Speciﬁcally, we consider: • Average Rank: α(xj) =µ(xj) • Lower Conﬁdence Bound: α(xj) =µ(xj) −β·σ(xj) • Expected Improvement: α(xj) =− ∫ rmax (0,µ(xk) −r) N(r; µ(xj),σ(xj)) Where βis a factor that trades of exploitation and exploration andxk is the best-observed conﬁgura- tion, i.e. k = arg mini∈{1,...,|Ds|}yi and µ(xk) is the average rank predicted for that conﬁguration and yk is its validation error. The previous formulation assumes a minimization, thus to choose the next query point you apply: x= arg minxj∈Xα(xj). Algorithm 2: Bayesian Optimization with DRE Input : A prior distribution over datasets p(D), initial observations H = {(x1,y1),..., (xN,yN)}, pending points X, number of BO iterations K, black-box function to optimize f Output: Best observed conﬁguration x∗ 1 Train ensemble of MLP scorers following Algorithm 1 and prior p(D); 2 for j ←1 to Kdo 3 Fine-tune/Train MLP scorers ; 4 Suggest next candidate x= arg minxj∈Xα(xj,H) ; 5 Observe response y= f(x) ; 6 Update history H = H∪{(x,y)}; 7 end 8 Return top performing conﬁguration: arg min(xi,yi)∈H yi B E XPERIMENTAL SETUP FOR DEEP RANKING ENSEMBLES Meta-Feature Extractor The DRE model has two conﬁgurable components: the meta-feature net- work and the scorers. The meta-feature extractor is a DeepSet with an architecture similar to the one used by Jomaa et al. (2021a). However, we used 2 fully connected layers with 32 neurons each for both φand ρ(Deep Set parameters) instead of 3 fully connected layers. The output size is set to 16 by default. Ensemble of Scorers The ensemble of scorers is a group of 10 MLP (Multilayer Perceptrons) with identical architectures. Each neural network has 4 hidden layers and each hidden layer has 32 neurons. The neural networks are initialized independently and randomly (for DRE-RI) or warm- initialized with the meta-learned weights. The input size of each neural network is 16 (the dimesi- ionality of the meta-features), plus the HP search space dimensionality. their output size is 1. Setup for Motivating Example. For the creation of the Figure 2, we use as scorer network an MLP with 2 hidden layers and 10 neurons per layer. The meta-feature extractor has 4 layers and 10 neurons, and output dimensions equal to 10. The network is meta-trained for 1000 epochs, with batch size 10, learning rate 0.001, Adam Optimizer, and 10 models in the ensemble. For the meta-learning example, we do not ﬁne-tune the networks, while we ﬁne-tune the networks for the non-meta-learned example for 500 iterations. 14Published as a conference paper at ICLR 2023 Table 1: Average Cost per BO Step (in seconds) 4796 (3 Dims) 5636 (6 Dims) 5527 (8 Dims) 5965 (10 Dims) 5906 (16 Dims) HEBO 0.27 ±0.18 3.11 ±1.68 2.66 ±0.95 3.21 ±1.78 2.85 ±2.43 FSBO 10.49 ±2.92 10.13 ±1.51 10.61 ±4.47 11.45 ±4.35 12.13 ±6.41 DRE 22.29 ±3.81 18.8 ±3.57 22.61 ±3.85 19.39 ±3.81 22.29 ±3.79 C D ISCUSSION ON LIST SIZE AND LIST WEIGHTS (a) List Size Ablation 5 20 35 50 65 80 95 Number of Evaluations 3.0 3.5 4.0 Average Rank Random L=10 L=50 L=100 L=200 (b) List Weights 5 20 35 50 65 80 95 Position 0.0 0.5 1.0 1.5 Weight I. Log I. Linear PDA Figure 8: Effect of parameters in list-wise loss We present an additional ablation on the list size. We report the aver- age rank on the meta-validation split for different list sizes during meta- training on Figure 8a. Notice, that a small list size (10) leads to an un- derperforming setting. Therefore, it is important to consider relative large list sizes (100 ≤n). During meta-testing i.e. by perform- ing BO, there is no signiﬁcant over- head in terms of having a larger list size, because the true rank is derived from the observed validation accu- racy of conﬁgurations. During both meta-training, as well as the BO step, we ﬁt our surrogate to estimate the rank of previously observed conﬁgu- rations that have been already evalu- ated. Given nobservations, comput- ing the true rank is a simpleO(n·log(n)) sorting operation. Notice that in BO settingsnis typically small. There are several weighting schemes. Two alternatives to the weighting factor we use (inverse log weighting) are inverse linear weighting and position-dependent attention (PDA) (Chen et al., 2017). As you can see in Figure 8b, inverse linear gives very small weight to lower ranks, while the position- dependent gives too much importance. In this plot, PDA weights were scaled to make it comparable to the other schemes. We decided to use the inverse log weighting because it gives neither too low nor too high weight to lower ranks. For the j-th position in a list with kelements, these weights can be described as follows: • Inverse Log: w(j) = 1 log(j+1) • Inverse Linear: w(j) =1 j • Position-dependent attention: w(j) =k−j+1∑k t=1 t D D ISCUSSION ON COMPUTATIONAL COST We provide here a cost comparison between DRE, FSBO and HEBO. In the Table 1, we provide the average cost per BO step ( ±standard deviation) for different search spaces (with different di- mensions). DRE effectively incurs a cost higher than FSBO and HEBO, but <30 seconds, which is a very small overhead compared to the cost of actually evaluating hyperparameter conﬁgurations (evaluation means the expensive process of training classiﬁers given the hyperparameter conﬁgura- tions and computing the validation accuracy). 15Published as a conference paper at ICLR 2023 E A DDITIONAL PLOTS We present additional results on the critical difference diagrams fori) Transfer methods results (Fig- ure 9a), ii) Non-Transfer (Figure 9b, iii) Scorer size (Figure 10a, iv) Acquisition Function (Figure 10b, v Ranking Loss (Figure 11a) and vi Meta-features (Figure 11b). These CD plots show the com- parison of the performance at different number of trials (e.g. at 25 trials = Rank@25). The vertical lines connecting two methods indicate that their performances are not signiﬁcantly different. (a) Comparison vs. transfer methods  (b) Comparison vs. non-transfer methods Figure 9: Critical Difference Diagram for a) Transfer and b) Non-transfer. (a) Ablation of the DRE Scorer size  (b) Ablation of the Acquisition Function Figure 10: Critical Difference Diagram for the results of the ablation of DRE hyperparameters in (a) and the choice of the acquisition function from Hypothesis 5 in (b). 16Published as a conference paper at ICLR 2023 (a) Ablation of Ranking Loss  (b) Ablation of Meta-Features Figure 11: Critical Difference Diagrams for the results of Hypothesis 3 in a) and Hypothesis 4 in b). Figure 12: Average Rank per Search Space (Transfer Methods) 17Published as a conference paper at ICLR 2023 Figure 13: Average Rank per Search Space (Non-Transfer Methods) 18",
      "meta_data": {
        "arxiv_id": "2303.15212v2",
        "authors": [
          "Abdus Salam Khazi",
          "Sebastian Pineda Arango",
          "Josif Grabocka"
        ],
        "published_date": "2023-03-27T13:52:40Z",
        "pdf_url": "https://arxiv.org/pdf/2303.15212v2.pdf",
        "github_url": "https://github.com/huawei-noah/HEBO"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Deep Ranking Ensembles (DRE), a novel neural network-based Bayesian Optimization (BO) surrogate optimized with Learning-to-Rank (L2R) losses for Hyperparameter Optimization (HPO). Unlike traditional methods that treat surrogate training as a regression task, DRE formulates it as an L2R problem, prioritizing the ranks of top-performing hyperparameter configurations and modeling uncertainty via deep ensembles. The method integrates a meta-learning technique for transferring knowledge from large-scale public meta-datasets, enhanced by dataset meta-features. DRE achieves new state-of-the-art results in HPO across a broad range of benchmarks.",
        "methodology": "The core methodology involves training an ensemble of diverse neural networks (DRE) to act as a probabilistic surrogate for HPO, focusing on learning the ranks of hyperparameter configurations' performances. The neural networks are optimized using a weighted list-wise L2R loss, which assigns higher penalties to errors in ranking top-performing configurations. Uncertainty is estimated by training multiple diverse scorers within the ensemble using stochastic gradient descent with different per-scorer seeds. The DRE surrogate is meta-learned from evaluations on previous datasets, and its transfer quality is boosted by incorporating dataset meta-features extracted using a deep set formulation, which condition the scorer functions. During the BO loop, the ensemble's posterior mean and variance of estimated ranks are used by acquisition functions like Expected Improvement (EI) or Lower Confidence Bound (UCB) to guide the search for the next optimal hyperparameter configuration.",
        "experimental_setup": "Experiments were conducted on HPO-B, the largest public HPO benchmark, comprising 16 search spaces and 86 meta-test datasets/tasks. The evaluation involved 5 initial configurations followed by 100 BO iterations, with 5 runs per task. DRE's performance was compared against 12 state-of-the-art HPO baselines, including non-transfer methods (e.g., Random Search, GP, HEBO) and transfer methods (e.g., TST, RGPE, FSBO). The DRE architecture consisted of a Deep Set-based meta-feature extractor (five hidden layers, 32 neurons per layer) and an ensemble of 10 MLPs (four hidden layers, 32 neurons each). DRE was meta-learned for 5000 epochs using Adam (LR 0.001, batch size 100) and fine-tuned for 1000 epochs per BO iteration. Ablation studies investigated different L2R losses (point-wise, pair-wise, list-wise), the impact of meta-features, and various acquisition functions (EI, UCB, Average Rank), as well as scorer network size and list parameters.",
        "limitations": "The primary limitation noted is that DRE incurs a higher computational cost per BO step compared to some baselines like FSBO and HEBO (though deemed a 'very small overhead' relative to the overall cost of evaluating hyperparameter configurations). The paper also implicitly acknowledges the general challenges of HPO, such as a limited number of evaluated configurations and the negative-transfer phenomenon, which DRE aims to mitigate through its design choices (meta-learning, meta-features), but these are inherent difficulties of the problem space rather than specific weaknesses of DRE itself.",
        "future_research_directions": "Future research directions include extending the DRE surrogate to improve HPO performance in various sub-problems, such as multi-fidelity HPO, multi-objective HPO, and neural architecture search (NAS).",
        "experimental_code": "import torchimport torch.nn as nnfrom torch.nn import functional as Ffrom collections.abc import Iterableimport mathdef compute_conv2d_output_shape(h_in, w_in, kernel_size, padding, stride, dilation=1):    if isinstance(kernel_size, Iterable) is False:        kernel_size = (kernel_size, kernel_size)    if isinstance(padding, Iterable) is False:        padding = (padding, padding)    if isinstance(stride, Iterable) is False:        stride = (stride, stride)    if isinstance(dilation, Iterable) is False:        dilation = (dilation, dilation)    h_out = int(math.floor(        (h_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1    ))    w_out = int(math.floor(        (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1    ))    return (h_out, w_out)def compute_conv2dTranspose_output_shape(h_in, w_in, kernel_size, padding, stride,                                          output_padding=0, dilation=1):    if isinstance(kernel_size, Iterable) is False:        kernel_size = (kernel_size, kernel_size)    if isinstance(padding, Iterable) is False:        padding = (padding, padding)    if isinstance(stride, Iterable) is False:        stride = (stride, stride)    if isinstance(dilation, Iterable) is False:        dilation = (dilation, dilation)    if isinstance(output_padding, Iterable) is False:        output_padding = (output_padding, output_padding)    h_out = (h_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) \\            + output_padding[0] + 1    w_out = (w_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) \\            + output_padding[1] + 1    return (h_out, w_out)class MLP(nn.Module):    def __init__(self, input_dim, hidden_dims, output_dim, activation=\"leaky_relu\",                 act_in_last_layer=False, skip_connection=False, norm_method=None):        super(MLP, self).__init__()        self.input_dim = input_dim        self.output_dim = output_dim        self.hidden_dims = hidden_dims        self.layer_dims = [input_dim, *hidden_dims, output_dim]        self.layer_num = len(self.layer_dims) - 1        self.act_in_last_layer = act_in_last_layer        self.skip_connection = skip_connection        self.norm_method = norm_method        self.layers = []        for i in range(self.layer_num):            linear_layer = nn.Linear(self.layer_dims[i], self.layer_dims[i + 1])            if self.norm_method is not None and i != self.layer_num - 1:                if self.norm_method == 'spectral_norm':                    sn_l = nn.utils.spectral_norm(linear_layer)                    self.layers.append(sn_l)                elif self.norm_method == 'batch_norm':                    bn = nn.BatchNorm1d(self.layer_dims[i+1])                    self.layers.append(linear_layer)                    self.layers.append(bn)                else:                    raise ValueError(\"Invalid norm_method:\", self.norm_method)            else:                self.layers.append(linear_layer)            if i != self.layer_num - 1 or self.act_in_last_layer:                if activation == \"relu\":                    self.layers.append(torch.nn.ReLU())                elif activation == \"tanh\":                    self.layers.append(torch.nn.Tanh())                elif activation == \"sigmoid\":                    self.layers.append(torch.nn.Sigmoid())                elif activation == \"leaky_relu\":                    self.layers.append(torch.nn.LeakyReLU())                else:                    raise ValueError(\"Unsupported activation type: \", activation)        self.model = nn.Sequential(*self.layers)    def forward(self, x):        if self.skip_connection:            o = x + self.model(x)        else:            o = self.model(x)        return oclass CopyModule(nn.Module):    def __init__(self):        super(CopyModule, self).__init__()    def forward(self, x):        return ximport torchfrom gpytorch.kernels.kernel import Kernelfrom typing import Optional, Tuplefrom gpytorch.priors import Priorfrom gpytorch.constraints import Intervalimport torchdef postprocess_rbf(dist_mat):    return dist_mat.div_(-2).exp_()def default_postprocess_script(x):    return xclass ExpectedRBFKernel(Kernel):    has_lengthscale = True    def __init__(self, ard_num_dims: Optional[int] = None,                 batch_shape: Optional[torch.Size] = torch.Size([]),                 active_dims: Optional[Tuple[int, ...]] = None,                 lengthscale_prior: Optional[Prior] = None,                 lengthscale_constraint: Optional[Interval] = None,                 eps: Optional[float] = 1e-6,                 **kwargs):        super(ExpectedRBFKernel, self).__init__(            ard_num_dims, batch_shape, active_dims,            lengthscale_prior, lengthscale_constraint, eps        )    def forward(self, x1, x2, diag=False, **params):        B = x1.shape[:-2] if x1.ndim >= 3 else None        M = x1.shape[-2]        N = x2.shape[-2]        D = x1.shape[-1] // 2        dtype = x1.dtype        device = x1.device        x1_mean = x1[..., 0:D]        x1_var = x1[..., D:] ** 2.0        x2_mean = x2[..., 0:D]        x2_var = x2[..., D:] ** 2.0        Var1 = torch.diag_embed(x1_var)        Var2 = torch.diag_embed(x2_var)        if self.ard_num_dims is None:            ls_vec = self.lengthscale.repeat(1, D).view(-1)        else:            ls_vec = self.lengthscale.view(-1)        W = torch.diag(ls_vec ** 2.0)        AB = x1_mean.unsqueeze(-2) - x2_mean.unsqueeze(-3)        VAVB = Var1.unsqueeze(-3) + Var2.unsqueeze(-4)        Z = W.unsqueeze(0).unsqueeze(0) if B is None \\            else W.unsqueeze(0).unsqueeze(0).unsqueeze(            0) + VAVB        Z_inv = torch.inverse(Z)        ABZ_eq = 'mnpd,mndd->mnpd' if B is None else 'bmnpd,bmndd->bmnpd'        ABZ = torch.einsum(ABZ_eq, [AB.unsqueeze(-2), Z_inv])        nu_eq = 'mnpd,mndq->mnpq' if B is None else 'bmnpd,bmndq->bmnpq'        nu = torch.einsum(nu_eq, [ABZ, AB.unsqueeze(-1)]).squeeze(-1).squeeze(            -1)        x1_eq_x2 = torch.equal(x1, x2)        if diag:            if x1_eq_x2:                res = torch.zeros(*x1.shape[:-1], x1.shape[-2], dtype=x1.dtype, device=x1.device)                res = postprocess_rbf(res)                return res            else:                res = torch.norm(x1 - x2, p=2, dim=-1)                res = res.pow(2)                res = postprocess_rbf(res)                return res        else:            W_inv = W.inverse().unsqueeze(0).unsqueeze(0)            if B is not None:                W_inv = W_inv.unsqueeze(0)            de_coeff = (W_inv.matmul(VAVB)).det().sqrt()            if x1_eq_x2:                zero_diag_identity = (torch.ones((M, M), dtype=dtype).to(device) -                                       torch.eye(M, M, dtype=dtype).to(device))                one_diag = torch.eye(M, M, dtype=dtype).to(device)                if B is not None:                    zero_diag_identity = zero_diag_identity.unsqueeze(0)                    one_diag = one_diag.unsqueeze(0)                de = de_coeff * zero_diag_identity + one_diag            else:                de = de_coeff            covar = nu / de            return postprocess_rbf(covar)import gpytorch.kernels.kernel as Kernelimport gcfrom utils import commons as cmfrom functools import partialimport torchdef postprocess_linear(dist_mat):    sym_dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)    return sym_dist_matdef integral_KME_batch(X, Y, kernel: Kernel):    dist_mat = None    try:        B = X.shape[:-3]        M, C, D = X.shape[-3:]        N, H, D = Y.shape[-3:]        _original_shape = kernel.batch_shape        kernel.batch_shape = torch.Size([M, N])        dist_mat = kernel(            X.unsqueeze(-3),            Y.unsqueeze(-4)        ).evaluate()        kme = dist_mat.mean(dim=[-1, -2])        kernel.batch_shape = _original_shape    finally:        cm.free_memory(            [                dist_mat,            ],            debug=False        )    return kmedef estimate_KME_in_chunks(X, Y, estimator, chunk_size=10):    N = Y.shape[-3]    retry_i = 0    success = False    _practicable_chunk_size = chunk_size    kme_results = None    while not success:        kme_results = []        _Y = None        _kme = None        try:            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)            for bs in range(0, N, _practicable_chunk_size):                be = min(N, bs + _practicable_chunk_size)                _Y = Y[..., bs:be, :, :]                _kme = estimator(X, _Y)                kme_results.append(_kme)                gc.collect()                torch.cuda.empty_cache()            success = True        except RuntimeError as e:            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:                if _practicable_chunk_size > 1:                    print(f'Chunk size {_practicable_chunk_size} is too large, '                          f'reduce it by a half:', e)                    retry_i += 1                    if len(kme_results) > 0:                        cm.free_memory(kme_results)                        for _m in kme_results:                            del _m                        del kme_results                    if '_kme' in locals():                        cm.free_memory([_kme])                    if '_Y' in locals():                        cm.free_memory([_Y])                else:                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '                                     'try cpu.')            else:                raise e        finally:            gc.collect()            torch.cuda.empty_cache()    kme = torch.concat(kme_results, dim=-1)    return kmeclass KMEKernel(Kernel):    has_lengthscale = True    def __init__(self, base_kernel, **kwargs):        super(KMEKernel, self).__init__(**kwargs)        self.base_kernel = base_kernel        self.chunk_size = kwargs.get('chunk_size', 100)        self.estimator = kwargs.get('estimator', 'integral')        self.estimation_trials = kwargs.get('estimation_trials', 1)        if self.estimator == 'integral':            self.estimation_func = partial(                integral_KME_batch, kernel=self.base_kernel            )        else:            raise ValueError('Unsupported estimator name', self.estimator)    @property    def is_stationary(self) -> bool:        return self.base_kernel.is_stationary    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):        chunk_size = params.get('chunk_size', self.chunk_size)        avg_dist_mat = None        for _ in range(self.estimation_trials):            dist_mat = estimate_KME_in_chunks(x1, x2, self.estimation_func, chunk_size)            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat        avg_dist_mat = avg_dist_mat / self.estimation_trials        cov_mat = postprocess_linear(avg_dist_mat.div(self.lengthscale ** 2.0))        return avg_dist_mat, cov_mat    def forward(self, x1, x2, diag=False, **params):        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)        return cov_matimport gpytorch as gpytfrom gpytorch.kernels.kernel import Kernelimport torchfrom tqdm.auto import trangeimport gcfrom utils import commons as cmfrom functools import partialdef additive_RQ_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):    assert len(alphas) > 0    _k_list = []    for a in alphas:        _k = gpyt.kernels.RQKernel()        _k.alpha = a        _k.lengthscale = ls        _k.raw_lengthscale.require_grad = learnable_ls        _k_list.append(_k)    k = gpyt.kernels.AdditiveKernel(*_k_list)    return kdef combo_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):    assert len(alphas) > 0    _k_list = []    for a in alphas:        _k = gpyt.kernels.RQKernel()        _k.alpha = a        _k.lengthscale = ls        _k.raw_lengthscale.require_grad = learnable_ls        _k_list.append(_k)    _k_list.append(gpyt.kernels.LinearKernel())    k = gpyt.kernels.AdditiveKernel(*_k_list)    return kdef postprocess_mmd(dist_mat):    sym_dist_mat = (dist_mat + dist_mat.T) * 0.5    _dist_mat = sym_dist_mat.clamp_(min=0) ** 0.5    return torch.clamp(1.0 - _dist_mat, 0.0, 1.0)def postprocess_mmd_rbf(dist_mat):    _dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)    return _dist_mat.div_(-2).exp_()def nystrom_mmd(X, Y, kernel: Kernel, sub_samp_size: int = 100):    B, D = X.shape    x_sub_inds = torch.randperm(B)[:sub_samp_size]    X_sub = X[x_sub_inds, :]    H, D = Y.shape    y_sub_inds = torch.randperm(H)[:sub_samp_size]    Y_sub = Y[y_sub_inds, :]    k_m_x = kernel(X_sub, X_sub).evaluate()    k_m_x_inv = torch.linalg.pinv(k_m_x)    k_mn_x = kernel(X_sub, X).evaluate()    alpha_x = (k_m_x_inv @ k_mn_x @ torch.ones(X.shape[0], 1).type(X.dtype).to(X.device)) \\              / X.shape[0]    k_m_y = kernel(Y_sub, Y_sub).evaluate()    k_m_y_inv = torch.linalg.pinv(k_m_y)    k_mn_y = kernel(Y_sub, Y).evaluate()    alpha_y = (k_m_y_inv @ k_mn_y @ torch.ones(Y.shape[0], 1).type(Y.dtype).to(Y.device)) \\              / Y.shape[0]    part1 = alpha_x.T @ k_m_x @ alpha_x    part2 = alpha_y.T @ k_m_y @ alpha_y    part3 = alpha_x.T @ kernel(X_sub, Y_sub).evaluate() @ alpha_y * -2    mmd2 = part1 + part2 + part3    return mmd2def nystrom_mmd_batch(X, Y, kernel: Kernel, sub_samp_size: int = 100):    km_x_inv = None    kmn_x = None    ones_x = None    km_y_inv = None    kmn_y = None    ones_y = None    km_x, km_y = None, None    alpha_x, km_xy = None, None    alpha_y, X_sub, Y_sub = None, None, None    part1, part2, part3 = None, None, None    try:        B = X.shape[:-3]        M, C, D = X.shape[-3:]        N, H, D = Y.shape[-3:]        _original_shape = kernel.batch_shape        x_sub_inds = torch.randperm(C)[:sub_samp_size]        X_sub = X[..., x_sub_inds, :]        y_sub_inds = torch.randperm(H)[:sub_samp_size]        Y_sub = Y[..., y_sub_inds, :]        kernel.batch_shape = torch.Size([M])        km_x = kernel(X_sub, X_sub).evaluate()        km_x_inv = torch.linalg.pinv(km_x)        kmn_x = kernel(X_sub, X).evaluate()        ones_x = torch.ones(M, C, 1).type(X.dtype).to(X.device)        alpha_x = (km_x_inv @ kmn_x @ ones_x) / C        kernel.batch_shape = torch.Size([N])        km_y = kernel(Y_sub, Y_sub).evaluate()        km_y_inv = torch.linalg.pinv(km_y)        kmn_y = kernel(Y_sub, Y).evaluate()        ones_y = torch.ones(N, H, 1).type(Y.dtype).to(Y.device)        alpha_y = (km_y_inv @ kmn_y @ ones_y) / H        part1 = (alpha_x.transpose(-2, -1) @ km_x @ alpha_x).view(*B, M, 1)        part2 = (alpha_y.transpose(-2, -1) @ km_y @ alpha_y).view(*B, 1, N)        kernel.batch_shape = torch.Size([M, N])        km_xy = kernel(X_sub.unsqueeze(-3),                       Y_sub.unsqueeze(-4)).evaluate()        part3 = (alpha_x.unsqueeze(-3).transpose(-2, -1) @ km_xy @ alpha_y.unsqueeze(-4)).view(*B,                                                                                                M, N)        mmd2 = part1 + part2 - part3 * 2.0        kernel.batch_shape = _original_shape    finally:        cm.free_memory(            [                km_x_inv, kmn_x, ones_x,                km_y_inv, kmn_y, ones_y,                km_x, km_y,                alpha_x, km_xy, alpha_y, X_sub, Y_sub, part1, part2, part3,            ],            debug=False        )    return mmd2def empirical_mmd(X, Y, kernel: Kernel):    cm_xx = kernel(X, X).evaluate()    avg_xx_mmd = (cm_xx.sum() - torch.diagonal(cm_xx).sum()) / (X.shape[0] * (X.shape[0] - 1))    cm_yy = kernel(Y, Y).evaluate()    avg_yy_mmd = (cm_yy.sum() - torch.diagonal(cm_yy).sum()) / (Y.shape[0] * (Y.shape[0] - 1))    cm_xy = kernel(X, Y).evaluate()    avg_xy_mmd = cm_xy.sum() / (X.shape[0] * Y.shape[0])    mmd = avg_xx_mmd + avg_yy_mmd - 2.0 * avg_xy_mmd    return mmddef estimate_mmd_in_chunks(X, Y, estimator, chunk_size=10):    N = Y.shape[-3]    retry_i = 0    success = False    _practicable_chunk_size = chunk_size    while not success:        try:            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)            mmd_results = []            for bs in range(0, N, _practicable_chunk_size):                be = min(N, bs + _practicable_chunk_size)                _Y = Y[..., bs:be, :, :]                _mmd = estimator(X, _Y)                mmd_results.append(_mmd)                gc.collect()                torch.cuda.empty_cache()            success = True        except RuntimeError as e:            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:                if _practicable_chunk_size > 1:                    print(f'Chunk size {_practicable_chunk_size} is too large, '                          f'reduce it by a half:', e)                    retry_i += 1                    if len(mmd_results) > 0:                        cm.free_memory(mmd_results)                        for _m in mmd_results:                            del _m                        del mmd_results                    if '_mmd' in locals():                        cm.free_memory([_mmd])                    if '_Y' in locals():                        cm.free_memory([_Y])                else:                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '                                     'try cpu.')            else:                raise e        finally:            gc.collect()            torch.cuda.empty_cache()    mmd = torch.concat(mmd_results, dim=-1)    return mmddef empirical_mmd_batch(X, Y, kernel: gpyt.kernels.Kernel):    B = X.shape[:-3]    M, C, D = X.shape[-3:]    N, H, D = Y.shape[-3:]    _original_shape = kernel.batch_shape    cm_xx = None    cm_yy = None    cm_xy = None    try:        kernel.batch_shape = torch.Size([M])        cm_xx = kernel(X, X).evaluate()        avg_xx_mmd = (cm_xx.sum((-2, -1)) - torch.diagonal(cm_xx, dim1=-2, dim2=-1).sum(-1)) \\                     / (C * (C - 1))        kernel.batch_shape = torch.Size([N])        cm_yy = kernel(Y, Y).evaluate()        avg_yy_mmd = (cm_yy.sum((-2, -1)) - torch.diagonal(cm_yy, dim1=-2, dim2=-1).sum(-1)) \\                     / (H * (H - 1))        kernel.batch_shape = torch.Size([M, N])        cm_xy = kernel(X.unsqueeze(-3), Y.unsqueeze(-4)).evaluate()        avg_xy_mmd = cm_xy.sum((-2, -1)) / (C * H)        mmd2 = avg_xx_mmd.unsqueeze(-1) + avg_yy_mmd.unsqueeze(-2) - 2.0 * avg_xy_mmd    finally:        kernel.batch_shape = _original_shape        cm.free_memory(            [cm_xx, cm_yy, cm_xy, ],            debug=False        )    return mmd2class MMDKernel(Kernel):    has_lengthscale = True    def __init__(self, base_kernel, **kwargs):        super(MMDKernel, self).__init__(**kwargs)        self.base_kernel = base_kernel        self.chunk_size = kwargs.get('chunk_size', 100)        self.estimator = kwargs.get('estimator', 'nystrom')        self.sub_samp_size = kwargs.get('sub_samp_size', 100)        self.estimation_trials = kwargs.get('estimation_trials', 1)        if self.estimator == 'nystrom':            self.estimation_func = partial(                nystrom_mmd_batch, kernel=self.base_kernel, sub_samp_size=self.sub_samp_size            )        elif self.estimator == 'empirical':            self.estimation_func = partial(empirical_mmd_batch, kernel=self.base_kernel)        else:            raise ValueError('Unsupported estimator name', self.estimator)    @property    def is_stationary(self) -> bool:        return self.base_kernel.is_stationary    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):        chunk_size = params.get('chunk_size', self.chunk_size)        avg_dist_mat = None        for _ in range(self.estimation_trials):            dist_mat = estimate_mmd_in_chunks(x1, x2, self.estimation_func, chunk_size)            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat        avg_dist_mat = avg_dist_mat / self.estimation_trials        cov_mat = postprocess_mmd_rbf(avg_dist_mat.div(self.lengthscale ** 2.0))        return avg_dist_mat, cov_mat    def forward(self, x1, x2, diag=False, **params):        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)        return cov_matimport numpy as npimport torchimport botorch.models.transforms as transfrom torch.nn import ModuleDictfrom collections import OrderedDictfrom typing import Tuplefrom torch import Tensorfrom botorch.exceptions.errors import BotorchTensorDimensionErrorfrom typing import Any, Callable, Dict, List, Optional, Uniondef additional_std(X, std):    ret = torch.ones_like(X) * torch.tensor(std, dtype=X.dtype, device=X.device) \\        if isinstance(X, torch.Tensor) else np.ones_like(X) * std    return retdef additional_xc_samples(X, n_sample, n_var, sampling_func, sampling_cfg={}, **kwargs):    batch_shape = X.shape[:-1]    noises = sampling_func(        **{**sampling_cfg, 'x': X}, size=(*batch_shape, n_sample)    ).reshape(*batch_shape, n_sample, n_var)    if isinstance(X, torch.Tensor):        noises = torch.tensor(noises, dtype=X.dtype, device=X.device)    samples = X[..., None, :] + noises    return samplesdef add_noise(X, sampling_func, sampling_cfg={}, **kwargs):    batch_shape = X.shape[:-1]    event_dim = X.shape[-1]    noise = sampling_func(**sampling_cfg, size=(*batch_shape, 1) ).reshape(*batch_shape, event_dim)    if isinstance(X, torch.Tensor):        noise = torch.tensor(noise, dtype=X.dtype, device=X.device)    return X + noiseclass AdditionalFeatures(trans.input.AppendFeatures):    def transform(self, X):        expanded_features = self._f(X[..., self.indices], **self.fkwargs)        return X, expanded_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,                                                 device=X.device)class TransformFeature(trans.input.AppendFeatures):    def transform(self, X):        transformed_features = self._f(X[..., self.indices], **self.fkwargs)        return transformed_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,                                                 device=X.device)class SelectMultiInputs(trans.input.InputTransform, torch.nn.Module):    def __init__(            self,            sel_indices,            transform_on_train: bool = True,            transform_on_eval: bool = True,            transform_on_fantasize: bool = True,    ) -> None:        super().__init__()        self.transform_on_train = transform_on_train        self.transform_on_eval = transform_on_eval        self.transform_on_fantasize = transform_on_fantasize        self.register_buffer(\"sel_indices\", sel_indices)    def transform(self, X):        return tuple(X[i] for i in self.sel_indices)class MultiInputTransform(trans.input.InputTransform, ModuleDict):    r\"\"\"An input transform representing the chaining of individual transforms.\"\"\"    def __init__(self, **transforms) -> None:        r\"\"\"Chaining of input transforms.        Args:            transforms: The transforms to chain. Internally, the names of the                kwargs are used as the keys for accessing the individual                transforms on the module.        \"\"\"        super().__init__(OrderedDict(transforms))        self.transform_on_train = False        self.transform_on_eval = False        self.transform_on_fantasize = False        for tf in transforms.values():            self.is_one_to_many |= tf.is_one_to_many            self.transform_on_train |= tf.transform_on_train            self.transform_on_eval |= tf.transform_on_eval            self.transform_on_fantasize |= tf.transform_on_fantasize    def transform(self, X):        ret = tuple([tf.transform(X[ind]) for ind, tf in enumerate(self.values())])        return ret    def untransform(self, X):        ret = tuple([tf.untransform(X[ind]) for ind, tf in enumerate(self.values())])        return ret    def equals(self, other: trans.input.InputTransform) -> bool:        return super().equals(other=other) and all(            t1.equals(t2) for t1, t2 in zip(self.values(), other.values())        )    def preprocess_transform(self, X):        ret = tuple([tf.preprocess_transform(X[ind]) for ind, tf in enumerate(self.values())])        return retclass DummyTransform(trans.input.ReversibleInputTransform, torch.nn.Module):    def __init__(            self,            transform_on_train: bool = True,            transform_on_eval: bool = True,            transform_on_fantasize: bool = True,            reverse: bool = False,    ) -> None:        super().__init__()        self.transform_on_train = transform_on_train        self.transform_on_eval = transform_on_eval        self.transform_on_fantasize = transform_on_fantasize        self.reverse = reverse    def _transform(self, X: Tensor) -> Tensor:        return X    def _untransform(self, X: Tensor) -> Tensor:        return Xclass ScaleTransform(trans.input.Normalize):    def _transform(self, X: Tensor) -> Tensor:        return X / self.coefficient    def _untransform(self, X: Tensor) -> Tensor:        return X * self.coefficientimport model_utils.input_transform as tfxfrom sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScalerimport numpy as npimport torchimport torch.nn as nnfrom collections.abc import Iterablefrom typing import Callabledef safe_scaling(y, scaler=None, scaling_method=\"standardize\"):    try:        if scaler is None:            if scaling_method == \"standardize\":                scaler = StandardScaler()                y_scaled = scaler.fit_transform(y)            elif scaling_method == \"power_transform\":                if y.min() <= 0:                    scaler = PowerTransformer(method='johnson', standardize=True)                    y_scaled = scaler.fit_transform(y)                else:                    scaler = PowerTransformer(method='box-cox', standardize=True)                    y_scaled = scaler.fit_transform(y)                    if y_scaled.std() < 0.5:                        scaler = PowerTransformer(method='yeo-johnson', standardize=True)                        y_scaled = scaler.fit_transform(y)                    if y_scaled.std() < 0.5:                        raise RuntimeError('Power transformation failed')            elif scaling_method == \"min_max\":                scaler = MinMaxScaler()                y_scaled = scaler.fit_transform(y)            else:                raise ValueError(\"Unknown scaling method:\", scaling_method)        else:            y_scaled = scaler.transform(y)    except Exception as e:        print(f\"[Warn] scaling fails:\", e)        y_scaled = y.copy()        scaler = None    return y_scaled, scalerdef filter_nan(x, xe, y, keep_rule='any'):    assert x is None or np.isfinite(x).all()    assert xe is None or np.isfinite(xe).all()    assert torch.isfinite(y).any(), \"No valid data in the dataset\"    if keep_rule == 'any':        valid_id = torch.isfinite(y).any(dim=1)    else:        valid_id = torch.isfinite(y).all(dim=1)    x_filtered = x[valid_id] if x is not None else None    xe_filtered = xe[valid_id] if xe is not None else None    y_filtered = y[valid_id]    return x_filtered, xe_filtered, y_filtereddef get_gp_prediction(model, x, scaler, **kwargs):    pred = model.predict(x, **kwargs)    pred_lcb, pred_ucb = pred.confidence_region()    pred_mean = pred.mean    if scaler is not None:        mean = scaler.inverse_transform(pred_mean.detach().numpy().reshape(-1, 1)).flatten()        lcb = scaler.inverse_transform(pred_lcb.detach().numpy().reshape(-1, 1)).flatten()        ucb = scaler.inverse_transform(pred_ucb.detach().numpy().reshape(-1, 1)).flatten()    else:        mean = pred_mean.detach().numpy().flatten()        lcb = pred_lcb.detach().numpy().flatten()        ucb = pred_ucb.detach().numpy().flatten()    return pred, mean, lcb, ucbclass OneHotTransform(torch.nn.Module):    def __init__(self, num_uniqs):        super().__init__()        self.num_uniqs = num_uniqs    @property    def num_out(self) -> int:        return sum(self.num_uniqs)    def forward(self, xe):        return torch.cat(            [torch.nn.functional.one_hot(xe[:, i].long(), self.num_uniqs[i])             for i in range(xe.shape[1])], dim=1        ).float()class EmbTransform(nn.Module):    def __init__(self, num_uniqs, **conf):        super().__init__()        self.emb_sizes = conf.get('emb_sizes')        if self.emb_sizes is None:            self.emb_sizes = [min(50, 1 + v // 2) for v in num_uniqs]        self.emb = nn.ModuleList([])        for num_uniq, emb_size in zip(num_uniqs, self.emb_sizes):            self.emb.append(nn.Embedding(num_uniq, emb_size))    @property    def num_out(self) -> int:        return sum(self.emb_sizes)    def forward(self, xe):        return torch.cat(            [self.emb[i](xe[:, i]).view(xe.shape[0], -1) for i in range(len(self.emb))], dim=1)def get_model_prediction(model, Xc_te, support_decomposed_pred):    preds = []    if support_decomposed_pred:        py_m0, ps2_m0 = model.predict(            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=0        )    else:        py_m0, ps2_m0 = model.predict(torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0))    ucb_m0 = py_m0 + (torch.sqrt(ps2_m0) * 2.0)    lcb_m0 = py_m0 - (torch.sqrt(ps2_m0) * 2.0)    preds.append(        (py_m0.detach().numpy(),         ps2_m0.detach().numpy(),         lcb_m0.detach().numpy(),         ucb_m0.detach().numpy(),         )    )    if support_decomposed_pred:        py_m1, ps2_m1 = model.predict(            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=1        )        ucb_m1 = py_m1 + (torch.sqrt(ps2_m1) * 2.0)        lcb_m1 = py_m1 - (torch.sqrt(ps2_m1) * 2.0)        preds.append(            (py_m1.detach().numpy(),             ps2_m1.detach().numpy(),             lcb_m1.detach().numpy(),             ucb_m1.detach().numpy(),             )        )        py_m2, ps2_m2 = model.predict(            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=2        )        ucb_m2 = py_m2 + (torch.sqrt(ps2_m2) * 2.0)        lcb_m2 = py_m2 - (torch.sqrt(ps2_m2) * 2.0)        preds.append(            (py_m2.detach().numpy(),             ps2_m2.detach().numpy(),             lcb_m2.detach().numpy(),             ucb_m2.detach().numpy(),             )        )    return predsdef get_kernel_lengthscale(kern_model):    ls = None    km = kern_model    while ls is None and getattr(km, 'base_kernel', None) is not None:        km = km.base_kernel        ls = km.lengthscale    if ls is not None:        ls = ls.detach().cpu().numpy().flatten()    if isinstance(ls, Iterable) and len(ls) >= 1:        str_output = [f'{i:.3f}' for i in ls] if len(ls) > 1 else f'{ls[0]:.3f}'    else:        str_output = f'{ls}'    return ls, str_outputdef get_kernel_output_scale(kernel):    oscale = kernel.outputscale.item() if hasattr(kernel, 'outputscale') else None    oscale_str = f'{oscale:.3f}' if oscale is not None else f'{oscale}'    return oscale, oscale_strdef prepare_data(input_type: str,                 n_var: int, raw_input_mean: [float, np.array], raw_input_std: [float, np.array],                 xc_sample_size: int, input_sampling_func: Callable,                 xc_raw: np.array, y: np.array,                 dtype: torch.dtype, device: torch.device,                 **data_cfg):    if input_type == 'exact_input' or input_type == 'mean_input':        x_ts = torch.tensor(xc_raw, dtype=dtype, device=device)    elif input_type == 'sample_input':        tf_add_xsamp = tfx.AdditionalFeatures(            f=tfx.additional_xc_samples, transform_on_train=False,            fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,                     'sampling_func': input_sampling_func}        )        x_ts = tf_add_xsamp.transform(torch.tensor(xc_raw, dtype=dtype, device=device))    elif input_type == 'distribution_input':        tf_add_std = tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,                                            fkwargs={'std': raw_input_std})        x_ts = tf_add_std.transform(            torch.tensor(                xc_raw + raw_input_mean if raw_input_mean is not None else xc_raw,                dtype=dtype, device=device            )        )    else:        raise ValueError('Unknown input type:', input_type)    y_ts = torch.tensor(y.reshape(-1, 1), dtype=dtype, device=device)    return x_ts, y_tsimport gpytorch as gpytimport botorch as botfrom botorch.models.gpytorch import GPyTorchModelfrom botorch.models import transforms as tffrom typing import Any, Optional, Unionimport torchfrom botorch.acquisition.objective import PosteriorTransformfrom botorch.models.utils import gpt_posterior_settingsfrom botorch.posteriors.gpytorch import GPyTorchPosteriorfrom torch import Tensorimport warningsfrom botorch.posteriors.transformed import TransformedPosteriorclass RobustGP(gpyt.models.ExactGP, GPyTorchModel):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 **kwargs):        _in_tf = input_transform if input_transform is not None \\            else self.define_default_input_transform(**kwargs)        _out_tf = outcome_transform if outcome_transform is not None \\            else self.define_default_outcome_transform(**kwargs)        with torch.no_grad():            _in_tf.transform(train_inputs)        if _out_tf is not None:            train_targets, _ = _out_tf(train_targets)            train_targets = train_targets.squeeze(-1)        gpyt.models.ExactGP.__init__(self, train_inputs, train_targets, likelihood)        self.input_transform = _in_tf        self.outcome_transform = _out_tf        self.num_inputs = num_inputs        self.additional_transform = additional_transform if additional_transform is not None \\            else self.define_additional_transform(**kwargs)        self.mean_module = self.define_mean_module(**kwargs)        self.covar_module = self.define_covar_module(**kwargs)        self.kwargs = kwargs    def define_default_input_transform(self, **kwargs):        n_var = kwargs['n_var']        return tf.Normalize(d=n_var, transform_on_train=True)    def define_default_outcome_transform(self, m=1, **kwargs):        return tf.outcome.Standardize(m=m)    def define_mean_module(self, **kwargs):        return gpyt.means.ConstantMean()    def define_covar_module(self, **kwargs):        return gpyt.kernels.ScaleKernel(gpyt.kernels.MaternKernel())    def define_additional_transform(self, **kwargs):        return None    def transform_inputs_additional(self, X):        n_inputs = 1 if isinstance(X, torch.Tensor) else len(X)        if n_inputs != self.num_inputs:            if self.additional_transform is not None:                X = self.additional_transform(X)            else:                raise ValueError(f\"Expect {self.num_inputs} inputs but found {len(X)} \"                                 f\"and no additional transformer.\")        return X    def forward(self, X):        if self.training:            X = self.transform_inputs(X)        xc_raw = X        mean_x = self.mean_module(xc_raw)        covar_x = self.covar_module(xc_raw)        return gpyt.distributions.MultivariateNormal(mean_x, covar_x)    def posterior(            self, X,            observation_noise: Union[bool, Tensor] = False,            posterior_transform: Optional[PosteriorTransform] = None,            **kwargs: Any,    ) -> Union[GPyTorchPosterior, TransformedPosterior]:        self.eval()        X = self.transform_inputs_additional(X)        X = self.transform_inputs(X)        with gpt_posterior_settings():            mvn = self(*X) if self.num_inputs > 1 else self(X)            if observation_noise is not False:                if isinstance(observation_noise, torch.Tensor):                    self._validate_tensor_args(X=X, Y=observation_noise)                    if observation_noise.size(-1) == 1:                        observation_noise = observation_noise.squeeze(-1)                    mvn = self.likelihood(mvn, X, noise=observation_noise)                else:                    mvn = self.likelihood(mvn, X)        posterior = GPyTorchPosterior(distribution=mvn)        if hasattr(self, \"outcome_transform\"):            posterior = self.outcome_transform.untransform_posterior(posterior)        if posterior_transform is not None:            return posterior_transform(posterior)        return posterior    def _set_transformed_inputs(self) -> None:        if hasattr(self, \"input_transform\") and not self._has_transformed_inputs:            if hasattr(self, \"train_inputs\"):                self._original_train_inputs = self.train_inputs[0] if self.num_inputs == 1 \\                    else self.train_inputs                with torch.no_grad():                    X_tf = self.input_transform.preprocess_transform(                        self.train_inputs[0] if self.num_inputs == 1 else self.train_inputs                    )                self.set_train_data(X_tf, strict=False)                self._has_transformed_inputs = True            else:                warnings.warn(                    \"Could not update `train_inputs` with transformed inputs \"                    f\"since {self.__class__.__name__} does not have a `train_inputs` \"                    \"attribute. Make sure that the `input_transform` is applied to \"                    \"both the train inputs and test inputs.\",                    RuntimeWarning,                )class RobustGPModel():    def __init__(self, m_cls, num_inputs=1, **kwargs):        self.model = None        self.likelihood = None        self.mll = None        self.optimizer = None        self.m_cls = m_cls        self.num_inputs = num_inputs        self.noise_free = kwargs.get('noise_free', False)        self.dtype = kwargs.get('dtype', torch.float)        self.device = kwargs.get('device', torch.device('cpu'))        self.kwargs = kwargs    def define_optimizer(self, model: torch.nn.Module, **kwargs):        optimizer = None        if not kwargs.get('fit_with_scipy', False):            lr = kwargs.get('lr', 1e-2)            optimizer = torch.optim.Adam(                params=[{'params': model.parameters()}],                lr=lr            )        return optimizer    def define_likelihood(self, **kwargs):        noise_free = kwargs.get(\"noise_free\", False)        if noise_free:            lkh = gpyt.likelihoods.GaussianLikelihood()            lkh.noise = 1e-4            lkh.raw_noise.requires_grad = False        else:            noise_prior = kwargs.get(\"noise_prior\", None)            noise_constr = kwargs.get(\"noise_constr\", None)            lkh = gpyt.likelihoods.GaussianLikelihood(                noise_prior=noise_prior, noise_constraint=noise_constr            )        return lkh    def define_model(self, tr_x, tr_y, likelihood, **kwargs):        model = self.m_cls(tr_x, tr_y, likelihood, self.num_inputs, **kwargs)        return model    def define_mll(self, likelihood, model):        return gpyt.mlls.ExactMarginalLogLikelihood(likelihood, model)    def post_initialize(self, tr_x, tr_y):        self.likelihood = self.define_likelihood(**self.kwargs)        self.model = self.define_model(tr_x, tr_y, self.likelihood, **self.kwargs)        self.mll = self.define_mll(self.likelihood, self.model)        self.likelihood = self.likelihood.to(self.dtype).to(self.device)        self.model = self.model.to(self.dtype).to(self.device)    def fit(self, **kwargs):        assert (self.model is not None and self.mll is not None and self.likelihood is not None)        tr_hist = None        success = False        max_retries = kwargs.get('max_retries', 5)        n_retry = 0        while not success:            try:                with bot.settings.debug(True):                    tr_hist = self.do_fit(**kwargs)                success = True            except Exception as e:                if n_retry < max_retries:                    n_retry += 1                    print(f\"[Warn] Model fit fails, retry cnt={n_retry}.\", e)                    success = False                else:                    raise e        return tr_hist    def do_fit(self, **kwargs):        self.model.train()        fit_with_scipy = kwargs.get('fit_with_scipy', True)        tr_history = None        if fit_with_scipy:            bot.fit_gpytorch_mll(self.mll)        else:            tr_history = []            epoch_num = kwargs.get('epoch_num', 100)            verbose = kwargs.get('verbose', True)            print_every = kwargs.get('print_every', 10)            if self.optimizer is None:                self.optimizer = self.define_optimizer(self.model, **kwargs)            for ep_i in range(epoch_num):                def closure():                    self.optimizer.zero_grad()                    output = self.model(self.model.train_inputs[0]) if self.num_inputs == 1 \\                        else self.model(*self.model.train_inputs)                    loss = -self.mll(output, self.model.train_targets)                    loss.backward()                    return loss                loss = self.optimizer.step(closure)                xc_ls, xc_ls_str = get_kernel_lengthscale(self.model.covar_module)                xc_os, xc_os_str = get_kernel_output_scale(self.model.covar_module)                y_noise = self.model.likelihood.noise.item()                tr_history.append((ep_i, loss.item(), xc_ls, xc_os, y_noise))                if verbose and ((ep_i % print_every == 0) or (ep_i == epoch_num - 1)):                    print(f\"[epoch{ep_i}] loss={loss.item():.3f}, \"                          f\"xc_lscale={xc_ls_str}, \"                          f\"xc_oscale={xc_os_str}, \"                          f\"y_noise={y_noise:.3f}\")        return tr_history    def predict(self, X):        pred = self.model.posterior(X)        return pred.mean, pred.variance    def get_posterior(self, X):        return self.model.posterior(X)from models.mmd_gp import MMDGPfrom kernels.kme_kernel import KMEKernelimport gpytorch as gpytclass UGP(MMDGP):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 hidden_dims=(4, 2), latent_dim=1, **kwargs):        super(UGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,                                         input_transform, outcome_transform,                                         additional_transform, **kwargs)    def define_covar_module(self, **kwargs):        xc_kern_params = {}        xc_lscale_constr = kwargs.get('xc_ls_constr', None)        if xc_lscale_constr is not None:            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr        xc_kme_inner_k = kwargs.get('base_kernel', None)        if xc_kme_inner_k is None:            xc_kme_inner_k = gpyt.kernels.RBFKernel(**xc_kern_params)        estimator_name = kwargs.get('estimator_name', 'integral')        chunk_size = kwargs.get('chunk_size', 100)        sub_samp_size = kwargs.get('sub_samp_size', 100)        covar_module = gpyt.kernels.ScaleKernel(            KMEKernel(xc_kme_inner_k, estimator=estimator_name, chunk_size=chunk_size)        )        return covar_modulefrom model_utils import input_transform as tfxfrom gpytorch.kernels import GaussianSymmetrizedKLKernel, RBFKernelfrom kernels.expected_rbf_kernel import ExpectedRBFKernelfrom models.robust_gp import RobustGPimport gpytorch as gpytfrom botorch.models import transforms as tfimport torchKN_EXPECTED_RBF = 'ERBF'KN_SKL = \"SKL\"KN_RBF = 'rbf'class UncertainGP(RobustGP):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 **kwargs):        super(UncertainGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,                                          input_transform, outcome_transform,                                          additional_transform, **kwargs)        self.kernel_name = kwargs.get('kernel_name', KN_SKL)    def define_covar_module(self, **kwargs):        n_var = kwargs['n_var']        self.kernel_name = kwargs['kernel_name']        xc_lscale_constr = kwargs.get('xc_lscale_constr', None)        xc_kern_params = {'ard_num_dims': n_var}        if xc_lscale_constr is not None:            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr        if self.kernel_name == KN_EXPECTED_RBF:            xc_kern_params['ard_num_dims'] = None            xc_covar_module = gpyt.kernels.ScaleKernel(ExpectedRBFKernel(**xc_kern_params))        elif self.kernel_name == KN_SKL:            xc_kern_params['ard_num_dims'] = None            xc_covar_module = gpyt.kernels.ScaleKernel(                GaussianSymmetrizedKLKernel(**xc_kern_params)            )        elif self.kernel_name == KN_RBF:            xc_covar_module = gpyt.kernels.ScaleKernel(RBFKernel(**xc_kern_params))        else:            raise ValueError(\"Unsupported kernel type:\", self.kernel_name)        return xc_covar_module    def define_default_input_transform(self, **kwargs):        n_var = kwargs['n_var']        input_bounds = kwargs['input_bounds']        return tfx.MultiInputTransform(            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),            tf2=tfx.ScaleTransform(d=n_var, bounds=input_bounds, transform_on_train=True),            tf3=tfx.DummyTransform(transform_on_train=True),        )    def define_additional_transform(self, **kwargs):        raw_input_std = kwargs['raw_input_std']        return tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,                                      fkwargs={'std': raw_input_std})    def compute_mean_cov(self, X, **kwargs):        xc_raw, xc_std, xe = X        mean_x, covar = None, None        if self.kernel_name == KN_SKL:            xc_input = torch.concat((xc_raw, xc_std.pow(2).log()), dim=-1)        elif self.kernel_name == KN_EXPECTED_RBF:            xc_input = torch.concat((xc_raw, xc_std.pow(2)), dim=-1)        else:            xc_input = xc_raw        mean_x = self.mean_module(xc_raw)        covar_x = self.covar_module(xc_input, **kwargs)        return mean_x, covar_x    def forward(self, xc_raw, xc_std, xe):        X = (xc_raw, xc_std, xe)        if self.training:            X = self.transform_inputs(X)        mean_x, covar = self.compute_mean_cov(X)        return gpyt.distributions.MultivariateNormal(mean_x, covar)from model_utils import input_transform as tfxfrom model_utils.common_model_parts import MLP, CopyModulefrom kernels.mmd_kernel import MMDKernel, additive_RQ_kernelimport gpytorch as gpytfrom botorch.models import transforms as tfimport torchimport warningsfrom gpytorch import settingsfrom gpytorch.utils.warnings import GPInputWarningfrom gpytorch.models.exact_prediction_strategies import prediction_strategyfrom gpytorch.models import ExactGPfrom gpytorch.distributions import MultivariateNormalclass MMDGP(RobustGP):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 hidden_dims=(4, 2), latent_dim=1, **kwargs):        super(MMDGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,                                     input_transform, outcome_transform,                                     additional_transform, **kwargs)        self.norm_method = kwargs.get('latent_norm_method', None)        self.skip_conn = kwargs.get('skip_conn', False)        if hidden_dims is not None:            self.latent_dim = latent_dim            self.latent_mapping_module = MLP(                train_inputs[0].shape[-1], hidden_dims, latent_dim,                norm_method=self.norm_method            )        else:            self.latent_dim = train_inputs[0].shape[1] + train_inputs[0].shape[1]            self.latent_mapping_module = CopyModule()    def define_default_input_transform(self, **kwargs):        n_var = kwargs['n_var']        input_bounds = kwargs['input_bounds']        return tfx.MultiInputTransform(            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),            tf2=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),            tf3=tfx.DummyTransform(transform_on_train=True),        )    def define_covar_module(self, **kwargs):        xc_kern_params = {}        xc_lscale_constr = kwargs.get('xc_ls_constr', None)        if xc_lscale_constr is not None:            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr        xc_mmd_inner_k = kwargs.get('base_kernel', None)        if xc_mmd_inner_k is None:            xc_mmd_inner_k = additive_RQ_kernel(                alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False            )        estimator_name = kwargs.get('estimator_name', 'nystrom')        chunk_size = kwargs.get('chunk_size', 100)        sub_samp_size = kwargs.get('sub_samp_size', 100)        covar_module = gpyt.kernels.ScaleKernel(            MMDKernel(xc_mmd_inner_k, estimator=estimator_name, sub_samp_size=sub_samp_size,                      chunk_size=chunk_size)        )        return covar_module    def forward(self, xc_raw, xc_samp, xe):        X = (xc_raw, xc_samp, xe)        if self.training:            X = self.transform_inputs(X)        mean_x, covar = self.compute_mean_cov(X)        return gpyt.distributions.MultivariateNormal(mean_x, covar)    def compute_mean_cov(self, x, **kwargs):        Xc_raw, Xc_samples, Xe = x        Xe_trans = Xe        mean_x, covar = None, None        if Xc_raw.shape[-1] > 0 and Xc_samples.shape[-1] > 0:            _s = Xc_samples.shape[:-1]            D = Xc_samples.shape[-1]            proj_X_samples = self.latent_mapping_module(Xc_samples.view(-1, D)).view(*_s, -1)            with gpyt.settings.debug(True) and gpyt.settings.lazily_evaluate_kernels(False):                k_c = self.covar_module(proj_X_samples, **kwargs)            covar = k_c if covar is None else (k_c * covar)            Xc_samples_mean = Xc_samples.mean(dim=-2)            proj_X_raw = self.latent_mapping_module(Xc_samples_mean)            mean_x = self.mean_module(proj_X_raw)        if Xe.shape[-1] > 0:            Xe_trans = self.xe_transformer(Xe)            k_e = self.xe_covar_module(Xe_trans, **kwargs)            covar = k_e if covar is None else (k_e * covar)        return mean_x, covar    def define_additional_transform(self, **kwargs):        xc_sample_size = kwargs.get('xc_sample_size', 1000)        input_sampling_func = kwargs['input_sampling_func']        n_var = kwargs['n_var']        return tfx.AdditionalFeatures(f=tfx.additional_xc_samples, transform_on_train=False,                                      fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,                                               'sampling_func': input_sampling_func})    def __call__(self, *args, **kwargs):        train_inputs = list(self.train_inputs) if self.train_inputs is not None else []        inputs = [i.unsqueeze(-1) if i.ndimension() == 1 else i for i in args]        if self.training:            if self.train_inputs is None:                raise RuntimeError(                    \"train_inputs, train_targets cannot be None in training mode. \"                    \"Call .eval() for prior predictions, or call .set_train_data() to add training data.\"                )            if settings.debug.on():                if not all(torch.equal(train_input, input) for train_input, input in                                   zip(train_inputs, inputs)):                    raise RuntimeError(\"You must train on the training inputs!\")            res = super().__call__(*inputs, **kwargs)            return res        elif settings.prior_mode.on() or self.train_inputs is None or self.train_targets is None:            full_inputs = args            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)            if settings.debug().on():                if not isinstance(full_output, MultivariateNormal):                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")            return full_output        else:            if settings.debug.on():                if all(torch.equal(train_input, input) for train_input, input in                                   zip(train_inputs, inputs)):                    warnings.warn(                        \"The input matches the stored training data. Did you forget to call model.train()?\",                        GPInputWarning,                    )            if self.prediction_strategy is None:                train_output = super(ExactGP, self).__call__(*train_inputs, **kwargs)                self.prediction_strategy = prediction_strategy(                    train_inputs=train_inputs,                    train_prior_dist=train_output,                    train_labels=self.train_targets,                    likelihood=self.likelihood,                )            full_inputs = []            batch_shape = train_inputs[0].shape[:-2]            for i, (train_input, input) in enumerate(zip(train_inputs, inputs)):                dim_2_concat = -3 if i == 1 else -2                batch_reserved_dim = -3 if i == 1 else -2                batch_shape = train_inputs[i].shape[:batch_reserved_dim]                if batch_shape != train_input.shape[:batch_reserved_dim]:                    batch_shape = torch.broadcast_shapes(batch_shape,                                                         train_input.shape[:batch_reserved_dim])                    train_input = train_input.expand(*batch_shape,                                                     *train_input.shape[batch_reserved_dim:])                if batch_shape != input.shape[:batch_reserved_dim]:                    batch_shape = torch.broadcast_shapes(batch_shape,                                                         input.shape[:batch_reserved_dim])                    train_input = train_input.expand(*batch_shape,                                                     *train_input.shape[batch_reserved_dim:])                    input = input.expand(*batch_shape, *input.shape[batch_reserved_dim:])                full_inputs.append(torch.cat([train_input, input], dim=dim_2_concat))            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)            if settings.debug().on():                if not isinstance(full_output, MultivariateNormal):                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")            full_mean, full_covar = full_output.loc, full_output.lazy_covariance_matrix            batch_shape = full_output.batch_shape            joint_shape = full_output.event_shape            tasks_shape = joint_shape[1:]            test_shape = torch.Size(                [joint_shape[0] - self.prediction_strategy.train_shape[0], *tasks_shape])            with settings.cg_tolerance(settings.eval_cg_tolerance.value()):                predictive_mean, predictive_covar = self.prediction_strategy.exact_prediction(                    full_mean, full_covar)            predictive_mean = predictive_mean.view(*batch_shape, *test_shape).contiguous()            return full_output.__class__(predictive_mean, predictive_covar)",
        "experimental_info": "The experiments compare robust optimization performance across various surrogate models and input uncertainty distributions. The setup involves:1.  **General Configuration**    *   `n_trial`: 10 (or 2 in debug mode)    *   `n_iter`: 100 (or 5 in debug mode)    *   `xc_sample_size`: 160 (sample size for each uncertain input sample)    *   `sub_samp_size`: 10 (sub-sampling size for Nystrom estimator)    *   `init_samp_num`: 10 (number of initial samples)2.  **Problem Setup**    *   `raw_fname`: 'RKHS-S', 'CustomK', 'BumpedBowlHD' (benchmark function names)    *   `n_var`: 1 to 3 (variable dimension, depends on function)    *   `minimization`: True (problem is minimization)    *   `num_expectation_eval`: 500 (number of evaluations for true expectation)3.  **Input Uncertainty Distributions**    *   **`input_uncertainty_type`**:        *   `gmm`: Gaussian Mixture Model (`GMMInputDistribution`) with 2 components, specific means, covariances, and weights.        *   `norm`: Gaussian (`ScipyInputDistribution`) for `n_var=1` or Multivariate Normal for `n_var > 1` with a specified `raw_input_std`.        *   `beta`: Beta distribution (`ScipyInputDistribution`) with parameters `a=0.4`, `b=0.2`, and `scale=raw_input_std`.        *   `chi2`: Chi-squared distribution (`ScipyInputDistribution`) with `df=3` and `scale=raw_input_std`.        *   `step_chi2`: Step-wise Chi-squared (`StepInputDistribution`) with two `chi2` distributions, switching based on input `x`.        *   `varying_beta`: Varying Beta distribution (`VaryingInputDistribution`) where parameters depend on `x`.        *   `uniform`: Uniform distribution (`ScipyInputDistribution`) with specified `loc` and `scale=raw_input_std`.        *   `concated_circular`: Concatenated distribution (`ConcatDistribution`) combining `Circular2Distribution` and `ScipyInputDistribution` (multivariate normal).    *   `raw_input_std`: 0.01 (standard deviation of input uncertainty)4.  **Model Configuration**    *   `default_dtype`: `torch.double` (if `use_double_precision`) or `torch.float32`.    *   `default_device`: `cuda:0` (if `use_gpu`) or `cpu`.    *   `x_bounds`: Determined by the problem's input bounds.    *   **`model_candidates` (surrogate models evaluated)**:        *   **`MMDGP-nystrom`**: `MMDGP` with `input_type='sample_input'`, `estimator_name='nystrom'`, `xc_sample_size=160`, `sub_samp_size=10`. Utilizes `MLP` as `latent_mapping_module`.        *   **`uGP`**: `UGP` (inherits from `MMDGP`) with `input_type='sample_input'`, `estimator_name='integral'`, `xc_sample_size` set to `int((xc_sample_size * sub_samp_size) ** 0.5)`.        *   **`GP`**: `RobustGP` (standard GP) with `input_type='exact_input'`.        *   **`skl`**: `UncertainGP` with `kernel_name='SKL'` (Gaussian Symmetrized KL Kernel) and `input_type='distribution_input'`.        *   **`ERBF`**: `UncertainGP` with `kernel_name='ERBF'` (Expected RBF Kernel) and `input_type='distribution_input'`.        *   _(Optional, sometimes included)_ `MMDGP-raw-S`, `MMDGP-raw-L`: `MMDGP` with `estimator_name='empirical'` and different `xc_sample_size`.    *   `default_fit_cfg`: `epoch_num` (150 or 1 in debug), `lr=5e-2`, `fit_with_scipy=False`, `print_every=10`.5.  **Optimization Settings**    *   `batch_size`: 1 (for acquisition function optimization)    *   `n_restarts`: 10 (for acquisition function optimization)    *   `raw_samples`: 512 (for acquisition function optimization)    *   **Acquisition Functions**:        *   `ExpectedImprovement` (EI)        *   `UpperConfidenceBound` (UCB)    *   **Optimum Finding Methods**:        *   `OPTIMUM_BOV`: Best Observed Value        *   `OPTIMUM_BE`: Best Expectation6.  **Infrastructure**    *   `use_gpu`: Optional GPU usage.    *   `use_double_precision`: Optional double precision.    *   `use_multiprocess`: Optional parallel execution for trials."
      }
    },
    {
      "title": "PASHA: Efficient HPO and NAS with Progressive Resource Allocation",
      "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are\nmethods of choice to obtain the best-in-class machine learning models, but in\npractice they can be costly to run. When models are trained on large datasets,\ntuning them with HPO or NAS rapidly becomes prohibitively expensive for\npractitioners, even when efficient multi-fidelity methods are employed. We\npropose an approach to tackle the challenge of tuning machine learning models\ntrained on large datasets with limited computational resources. Our approach,\nnamed PASHA, extends ASHA and is able to dynamically allocate maximum resources\nfor the tuning procedure depending on the need. The experimental comparison\nshows that PASHA identifies well-performing hyperparameter configurations and\narchitectures while consuming significantly fewer computational resources than\nASHA.",
      "full_text": "Published as a conference paper at ICLR 2023 PASHA: E FFICIENT HPO AND NAS WITH PROGRESSIVE RESOURCE ALLOCATION Ondrej Bohdal1∗, Lukas Balles2, Martin Wistuba2, Beyza Ermis3, C´edric Archambeau2, Giovanni Zappella2 1The University of Edinburgh 2AWS, Berlin 3Cohere for AI 1ondrej.bohdal@ed.ac.uk 3beyza@cohere.com 2{balleslb,marwistu,cedrica,zappella}@amazon.com ABSTRACT Hyperparameter optimization (HPO) and neural architecture search (NAS) are methods of choice to obtain the best-in-class machine learning models, but in practice they can be costly to run. When models are trained on large datasets, tuning them with HPO or NAS rapidly becomes prohibitively expensive for practitioners, even when efﬁcient multi-ﬁdelity methods are employed. We propose an approach to tackle the challenge of tuning machine learning models trained on large datasets with limited computational resources. Our approach, named PASHA, extends ASHA and is able to dynamically allocate maximum resources for the tuning procedure depending on the need. The experimental comparison shows that PASHA identiﬁes well-performing hyperparameter conﬁgurations and architectures while consuming signiﬁcantly fewer computational resources than ASHA. 1 I NTRODUCTION Hyperparameter optimization (HPO) and neural architecture search (NAS) yield state-of-the-art models, but often are a very costly endeavor, especially when working with large datasets and models. For example, using the results of (Sharir et al., 2020) we can estimate that evaluating 50 conﬁgurations for a 340-million-parameter BERT model (Devlin et al., 2019) on the 15GB Wikipedia and Book corpora would cost around $500,000. To make HPO and NAS more efﬁcient, researchers explored how we can learn from cheaper evaluations (e.g. on a subset of the data) to later allocate more resources only to promising conﬁgurations. This created a family of methods often described as multi- ﬁdelity methods. Two well-known algorithms in this family are Successive Halving (SH) (Jamieson & Talwalkar, 2016; Karnin et al., 2013) and Hyperband (HB) (Li et al., 2018). Multi-ﬁdelity methods signiﬁcantly lower the cost of the tuning. Li et al. (2018) reported speedups up to 30x compared to standard Bayesian Optimization (BO) and up to 70x compared to random search. Unfortunately, the cost of current multi-ﬁdelity methods is still too high for many practitioners, also because of the large datasets used for training the models. As a workaround, they need to design heuristics which can select a set of hyperparameters or an architecture with a cost comparable to training a single conﬁguration, for example, by training the model with multiple conﬁgurations for a single epoch and then selecting the best-performing candidate. On one hand, such heuristics lack robustness and need to be adapted to the speciﬁc use-cases in order to provide good results. On the other hand, they build on an extensive amount of practical experience suggesting that multi-ﬁdelity methods are often not sufﬁciently aggressive in leveraging early performance measurements and that identifying the best performing set of hyperparameters (or the best architecture) does not require training a model until convergence. For example, Bornschein et al. (2020) show that it is possible to ﬁnd the best hyperparameter – number of channels in ResNet- 101 architecture (He et al., 2015) for ImageNet (Deng et al., 2009) – using only one tenth of the data. However, it is not known beforehand that one tenth of data is sufﬁcient for the task. Our aim is to design a method that consumes fewer resources than standard multi-ﬁdelity algorithms such as Hyperband (Li et al., 2018) or ASHA (Li et al., 2020), and yet is able to identify conﬁgurations ∗Work done during an internship at AWS, Berlin. 1 arXiv:2207.06940v2  [cs.LG]  8 Mar 2023Published as a conference paper at ICLR 2023 that produce models with a similar predictive performance after full retraining from scratch. Models are commonly retrained on a combination of training and validation sets to obtain the best performance after optimizing the hyperparameters. To achieve the speedup, we propose a variant of ASHA, called Progressive ASHA (PASHA), that starts with a small amount of initial maximum resources and gradually increases them as needed. ASHA in contrast has a ﬁxed amount of maximum resources, which is a hyperparameter deﬁned by the user and is difﬁcult to select. Our empirical evaluation shows PASHA can save a signiﬁcant amount of resources while ﬁnding similarly well-performing conﬁgurations as conventional ASHA, reducing the entry barrier to do HPO and NAS. To summarize, our contributions are as follows: 1) We introduce a new approach called PASHA that dynamically selects the amount of maximum resources to allocate for HPO or NAS (up to a certain budget), 2) Our empirical evaluation shows the approach signiﬁcantly speeds up HPO and NAS without sacriﬁcing the performance, and 3) We show the approach can be successfully combined with sample-efﬁcient strategies based on Bayesian Optimization, highlighting the generality of our approach. Our implementation is based on the Syne Tune library (Salinas et al., 2022). 2 R ELATED WORK Real-world machine learning systems often rely on a large number of hyperparameters and require testing many combinations to identify suitable values. This makes data-inefﬁcient techniques such as Grid Search or Random Search (Bergstra & Bengio, 2012) very expensive in most practical scenarios. Various approaches have been proposed to ﬁnd good parameters more quickly, and they can be classiﬁed into two main families: 1) Bayesian Optimization: evaluates the most promising conﬁgurations by modelling their performance. The methods are sample-efﬁcient but often designed for environments with limited amount of parallelism; 2) Multi-ﬁdelity: sequentially allocates more resources to conﬁgurations with better performance and allows high level of parallelism during the tuning. Multi-ﬁdelity methods have typically been faster when run at scale and will be the focus of this work. Ideas from these two families can also be combined together, for example as done in BOHB by Falkner et al. (2018), and we will test a similar method in our experiments. Successive Halving (SH) (Karnin et al., 2013; Jamieson & Talwalkar, 2016) is conceptually the simplest multi-ﬁdelity method. Its key idea is to run all conﬁgurations using a small amount of resources and then successively promote only a fraction of the most promising conﬁgurations to be trained using more resources. Another popular multi-ﬁdelity method, called Hyperband (Li et al., 2018), performs SH with different early schedules and number of candidate conﬁgurations. ASHA (Li et al., 2020) extends the simple and very efﬁcient idea of successive halving by introducing asynchronous evaluation of different conﬁgurations, which leads to further practical speedups thanks to better utilisation of workers in a parallel setting. Related to the problem of efﬁciency in HPO, cost-aware HPO explicitly accounts for the cost of the evaluations of different conﬁgurations. Previous work on cost-aware HPO for multi-ﬁdelity algorithms such as CAHB (Ivkin et al., 2021) keeps a tight control on the budget spent during the HPO process. This is different from our work, as we reduce the budget spent by terminating the HPO procedure early instead of allocating the compute budget in its entirety. Moreover, PASHA could be combined with CAHB to leverage the cost-based resources allocation. Recently, researchers considered dataset subsampling to speedup HPO and NAS. Shim et al. (2021) have combined coresets with PC-DARTS (Xu et al., 2020) and showed that they can ﬁnd well- performing architectures using only 10% of the data and 8.8x less search time. Similarly, Visalpara et al. (2021) have combined subset selection methods with the Tree-structured Parzen Estimator (TPE) for HPO (Bergstra et al., 2011). With a 5% subset they obtained between an 8x to 10x speedup compared to standard TPE. However, in both cases it is difﬁcult to say in advance what subsampling ratio to use. For example, the 10% ratio in (Shim et al., 2021) incurs no decrease in accuracy, while reducing further to 2% leads to a substantial (2.6%) drop in accuracy. In practice, it is difﬁcult to ﬁnd a trade-off between the time required for tuning (proportional to the subset size) and the loss of performance for the ﬁnal model because these change, sometimes wildly, between datasets. Further, Zhou et al. (2020) have observed that for a ﬁxed number of iterations, rank consistency is better if we use more training samples and fewer epochs rather than fewer training samples and more epochs. This observation gives further motivation for using the whole dataset for HPO/NAS and design new approaches, like PASHA, to save computational resources. 2Published as a conference paper at ICLR 2023 3 P ROBLEM SETUP The problem of selecting the best conﬁguration of a machine learning algorithm to be trained is formalized in (Jamieson & Talwalkar, 2016) as a non-stochastic bandit problem. In this setting the learner (the hyperparameter optimizer) receives N hyperparameter conﬁgurations and it has to identify the best performing one with the constraint of not spending more than a ﬁxed amount of resources R(e.g. total number of training epochs) on a speciﬁc conﬁguration. Ris considered given, but in practice users do not have a good way for selecting it, which can have undesirable consequences: if the value is too small, the model performance will be sub-optimal, while if the budget is too large, the user will incur a signiﬁcant cost without any practical return. This leads users to overestimate R, setting it to a large amount of resources in order to guarantee the convergence of the model. We maintain the concept of maximum amount of resources in our algorithm but we prefer to interpret Ras a “safety net”, a cost not to be surpassed (e.g. in case an error prevents a normal behaviour of the algorithm), instead of the exact amount of resources spent for the optimization. This setting could be extended with additional assumptions, based on empirical observation, removing some extreme cases and leading to a more practical setup. In particular, when working with large datasets we observe that the curve of the loss for conﬁgurations (called arms in the bandit literature) continuously decreases (in expectation). Moreover, “crossing points” between the curves are rare (excluding noise), and they are almost always in the very initial part of the training procedure. Viering & Loog (2021); Mohr & van Rijn (2022) provide an analysis of learning curves and note that in practice most learning curves are well-behaved, with Bornschein et al. (2020); Domhan et al. (2015) reporting similar ﬁndings. More formally, let us deﬁne Ras the maximum number of resources needed to train an ML algorithm to convergence. Given πm(i) the ranking of conﬁguration iafter using mresources for training, there exists minimum R∗much smaller than Rsuch that for all amounts of resources rlarger than R∗the rankings of conﬁgurations trained with r resources remain the same: ∃R∗ ≪R : ∀i ∈ {conﬁgurations},∀r > R∗,πR∗(i) =πr(i). The existence of such a quantity, limited to the best performing conﬁguration, is also assumed by Jamieson & Talwalkar (2016), and it is leveraged to quantify the budget required to identify the best performing conﬁguration. If we knew R∗, it would be sufﬁcient to run all conﬁgurations with exactly that amount of resources to identify the best one and then just train the model from scratch with all the data using that conﬁguration. Unfortunately that quantity is unknown and can only be estimated during the optimization procedure. Note that in practice there is noise involved in training of neural networks, so similarly performing conﬁgurations will repeatedly swap their ranks. 4 M ETHOD PASHA is an extension of ASHA (Li et al., 2020) inspired by the “doubling trick” (Auer et al., 1995). PASHA targets improvements for hyperparameter tuning on large datasets by hinging on the assumptions made about the crossing points of the learning curves in Section 3. The algorithm starts with a small initial amount of resources and progressively increases them if the ranking of the conﬁgurations in the top two rungs (rounds of promotion) has not stabilized. The ability of our approach to stop early automatically is the key beneﬁt. We illustrate the approach in Figure 1, showing how we stop evaluating conﬁgurations for additional rungs if rankings are stable. We describe the details of our proposed approach in Algorithm 1. Given η, a hyperparameter used both in ASHA and PASHA to control the fraction of conﬁgurations to prune, PASHA sets the current maximum resources Rt to be used for evaluating a conﬁguration using the reduction factor ηand the minimum amount of resources rto be used (Kt is the current maximum rung). The approach increases the maximum number of resources allocated to promising conﬁgurations each time the ranking of conﬁgurations in the top two rungs becomes inconsistent. For example, if we can currently train conﬁgurations up to rung 2 and the ranking of conﬁgurations in rung 1 and rung 2 is not consistent, then we allow training part of the conﬁgurations up to rung 3, i.e. one additional rung. The minimum amount of resources ris a hyperparameter to be set by the user. It is signiﬁcantly easier to set compared to Ras ris the minimum amount of resources required to see a meaningful difference in the performance of the models, and it can be easily estimated empirically by running a few small-scale experiments. 3Published as a conference paper at ICLR 2023 Figure 1: Illustration of how PASHA stops early if the ranking of conﬁgurations has stabilized. Left: the ranking of the conﬁgurations (displayed inside the circles) has stabilized, so we can select the best conﬁguration and stop the search. Right: the ranking has not stabilized, so we continue. Algorithm 1Progressive Asynchronous Successive Halving (PASHA) 1: input minimum resource r, reduction factor η 2: function PASHA() 3: t= 0,R0 = ηr, K0 = ⌊logη(R0/r)⌋ 4: while desired do 5: for each free worker do 6: (θ,k) =get job() 7: run then return val loss(θ,rηk) 8: end for 9: for completed job (θ, k) with loss ldo 10: Update conﬁguration θin rung kwith loss l 11: if k≥Kt −1 then 12: πk = configuration ranking(k) 13: end if 14: if k= Kt and πk ̸≡πk−1 then 15: t= t+ 1 16: Rt = ηtR0 17: Kt = ⌊logη(Rt/r)⌋ 18: end if 19: end for 20: end while 21: end function 22: function get job() 23: // Check if there is a promotable config 24: for k= Kt −1,..., 1,0 do 25: candidates = top k(rung k,|rung k|/η) 26: promotable = {c∈candidates : cnot promoted} 27: if |promotable|>0 then 28: return promotable[0],k + 1 29: end if 30: // If not, grow bottom rung 31: Draw random conﬁguration θ 32: return θ,0 33: end for 34: end function We also set a maximum amount of resources Rso that PASHA can default to ASHA if needed and avoid increasing the resources indeﬁnitely. While it is not generally reached, it provides a safety net. 4.1 S OFT RANKING Due to the noise present in the training process, negligible differences in the measured predictive performance of different conﬁgurations can lead to signiﬁcantly different rankings. For these reasons we adopt what we call “soft ranking”. In soft ranking, conﬁgurations are still sorted by predictive performance but are considered equivalent if the performance difference is smaller 4Published as a conference paper at ICLR 2023 than a value ϵ (or equal to it). Instead of producing a sorted list of conﬁguration, this provides a list of lists where for every position of the ranking there is a list of equivalent conﬁgurations. The concept is explained graphically in Figure 2, and we also provide a formal deﬁnition. For a set of n conﬁgurations c1,c2,··· ,ci,··· ,cn and performance metric f (e.g. accuracy) with f(c1) ≤f(c2) ≤···≤ f(ci) ≤···≤ f(cn), soft rank at position iis deﬁned as soft ranki = {cj ∈conﬁgurations : |f(ci) −f(cj)|≤ ϵ}. When deciding on if to increase the resources, we go through the ranked list of conﬁgurations in the top rung and check if the current conﬁguration at the given rank was in the list of conﬁgurations for that rank in the previous rung. If there is a conﬁguration which does not satisfy the condition, we increase resources. Figure 2: Illustration of soft ranking. There are three lists with the ﬁrst two containing two items because the scores of the two conﬁgurations are closer to each other than ϵ. 4.2 A UTOMATIC ESTIMATION OF ϵBY MEASURING NOISE IN RANKINGS Every operation involving randomization gives slightly different results when repeated, the training process and the measurement of performance on the validation set are no exception. In an ideal world, we could repeat the process multiple times to compute empirical mean and variance to make a better decision. Unfortunately this is not possible in our case since the repeating portions of the training process will defeat the purpose of our work: speeding up the tuning process. Understanding when the differences between the performance measured for different conﬁgurations are “signiﬁcant” is crucial for ranking them correctly. We devise a method to estimate a threshold below which differences are meaningless. Our intuition is that conﬁgurations with different performance maintain their relative ranking over time. On the other hand, conﬁgurations that repeatedly swap their rankings perform similarly well and the performance difference in the current epoch or rung is simply due to noise. We want to measure this noise and use it to automatically estimate the threshold value ϵto be used in the soft-ranking described above. Formally we can deﬁne a set of pairs of conﬁgurations that perform similarly well by the following: S : {(c,c′) : ( πrj (c) >πrj (c′) ∧πrk (c) <πrk (c′) ∧πrl (c) >πrl (c′) ) ∨ ( πrj (c) <πrj (c′) ∧πrk (c) >πrk (c′) ∧πrl (c) <πrl (c′) ) }, (1) for resource levels (e.g. epochs – not rungs) rj > rk > rl, using the same notation as earlier to refer to resources. In practice we have per-epoch validation performance statistics and use these to ﬁnd resource levels rj,rk,rl that have conﬁgurations with the criss-crossing behaviour (there can be several epochs between such resource levels). We only consider conﬁgurations (c,c′) that made it to the latest rung, so rηKt−1 ≥rj >rη Kt−2. However, we allow for the criss-crossing to happen across epochs from any rungs. The value of ϵcan then be calculated as the N-th percentile of distances between the performances of conﬁgurations in S: ϵ= PN,(c,c′)∈S|frj (c) −frj (c′)|. The exact value of rj depends on the considered pair of conﬁgurations (c,c′). To uniquely deﬁne frj , we take the maximum resources rj currently available for both conﬁgurations in the consid- ered pair (c,c′). Let us consider the following example setup: the top rung has 8 epochs and 5Published as a conference paper at ICLR 2023 the next one has 4 epochs, there are three conﬁgurations ca,cb,cc that made it to the top rung and were trained for 8, 8 and 6 epochs so far respectively. Assuming there was criss-crossing within each pair (ca,cb), (ca,cc) and (cb,cc), the set of distances between conﬁgurations in S is {|f8(ca) −f8(cb)|,|f6(ca) −f6(cc)|,|f6(cb) −f6(cc)|}. The value of ϵis recalculated every time we receive new information about the performances of conﬁgurations. Initially the value of ϵis set to 0, which means that we check for exact ranking if we cannot yet calculate the value of ϵ. 5 E XPERIMENTS In this section we empirically evaluate the performance of PASHA. Its goal is not to provide a model with a higher accuracy, but to identify the best conﬁguration in a shorter amount of time so that we can then re-train the model from scratch. Overall, we target a signiﬁcantly faster tuning time and on-par predictive performance when comparing with the models identiﬁed by state-of-the-art optimizers like ASHA. Re-training after HPO or NAS is important because HPO and NAS in general require to reserve a signiﬁcant part of the data (often around 20 or 30%) to be used as a validation set. Training with fewer data is not desirable because in practice it is observed that training a model on the union of training and validation sets provides better results. We tested our method on two different sets of experiments. The ﬁrst set evaluates the algorithm on NAS problems and uses NASBench201 (Dong & Yang, 2020), while the second set focuses on HPO and was run on two large-scale tasks from PD1 benchmark (Wang et al., 2021). 5.1 S ETUP Our experimental setup consists of two phases: 1) run the hyperparameter optimizer until N = 256 candidate conﬁgurations are evaluated; and 2) use the best conﬁguration identiﬁed in the ﬁrst phase to re-train the model from scratch. For the purpose of these experiments we re-train all the models using only the training set. This avoids introducing an arbitrary choice on the validation set size and allows us to leverage standard benchmarks such as NASBench201. In real-world applications the model can be trained on both training and validation sets. All our results report only the time invested in identifying the best conﬁguration since the re-training time is comparable for all optimizers. All results are averaged over multiple repetitions, with the details speciﬁed for each set of experiments separately. We use N = 90-th percentile when calculating the value of ϵ. We use 4 workers to perform parallel and asynchronous evaluations. The choice of Ris sensitive for ASHA as it can make the optimizer consume too many resources and penalize the performance. For a fair comparison, we make Rdataset-dependent taking the maximum amount of resources in the considered benchmarks. ris also dataset-dependent and η, the halving factor, is set to 3 unless otherwise speciﬁed. The same values are used for both ASHA and PASHA. Runtime reported is the time spent on HPO (without retraining), including the time for computing validation set performance. We compare PASHA with ASHA (Li et al., 2020), a widely-adopted approach for multi-ﬁdelity HPO, and other relevant baselines. In particular, we consider “one-epoch baseline” that trains all conﬁgurations for one epoch (the minimum available resources) and then selects the most promising conﬁguration, and “random baseline” that randomly selects the conﬁguration without any training. For both one-epoch and random baselines we sample N = 256conﬁgurations, using the same scheduler and seeds as for PASHA and ASHA. All reported accuracies are after retraining for R= 200epochs. In addition, two, three and ﬁve-epoch baselines are evaluated in Appendix A. 5.2 NAS EXPERIMENTS For our NAS experiments we leverage the well-known NASBench201 (Dong & Yang, 2020) bench- mark. The task is to identify the network structure providing the best accuracy on three different datasets (CIFAR-10, CIFAR-100 and ImageNet16-120) independently. We use r = 1epoch and R= 200epochs. We repeat the experiments using 5 random seeds for the scheduler and 3 random seeds for NASBench201 (all that are available), resulting in 15 repetitions. Some conﬁgurations in NASBench201 do not have all seeds available, so we impute them by averaging over the available seeds. To measure the predictive performance we report the best accuracy on the combined validation and test set provided by the creators of the benchmark. 6Published as a conference paper at ICLR 2023 Table 1: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h±0.6h 1.0x 200.0 ±0.0 PASHA 93.57 ±0.75 1.3h±0.6h 2.3x 36.1 ±50.0 One-epoch baseline 93.30±0.61 0.3h±0.0h 8.5x 1.0 ±0.0 Random baseline 72.88±19.20 0.0h±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 One-epoch baseline 65.57±5.53 0.3h±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83±18.20 0.0h±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h±2.2h 1.0x 200.0 ±0.0 PASHA 45.13 ±1.51 2.9h±1.7h 3.1x 21.3 ±48.1 One-epoch baseline 41.42±4.98 1.0h±0.0h 8.8x 1.0 ±0.0 Random baseline 20.75±9.97 0.0h±0.0h N/A 0.0 ±0.0 The results in Table 1 suggest PASHA consistently leads to strong improvements in runtime, while achieving similar accuracy values as ASHA. The one-epoch baseline has noticeably worse accuracies than ASHA or PASHA, suggesting that PASHA does a good job of deciding when to continue increasing the resources – it does not stop too early. Random baseline is a lot worse than the one- epoch baseline, so there is value in performing NAS. We also report the maximum resources used to ﬁnd how early the ranking becomes stable in PASHA. The large variances are caused by stopping HPO at different rung levels for different seeds (e.g. 27 and 81 epochs). Note that the time required to train a model is about 1.3h for CIFAR-10 and CIFAR-100, and about 4.1h for ImageNet16-120, making the total tuning time of PASHA comparable or faster than the training time. We also ran additional experiments testing PASHA with a reduction factor ofη= 2and η= 4instead of η = 3, the usage of PASHA as a scheduler in MOBSTER (Klein et al., 2020) and alternative ranking functions. These experiments provided similar ﬁndings as the above and are described next. 5.2.1 R EDUCTION FACTOR An important parameter for the performance of multi-ﬁdelity algorithms like ASHA is the reduction factor. This hyperparameter controls the fraction of pruned candidates at every rung. The optimal theoretical value is eand it is typically set to 2 or 3. In Table 2 we report the results of the different algorithms ran with η= 2and η= 4on CIFAR-100 (the full set of results is in Appendix B). The gains are consistent also for η= 2and η= 4, with a larger speedup when using η= 2as that allows PASHA to make more decisions and identify earlier that it can stop the search. Table 2: NASBench201 – CIFAR-100 results with various reduction factorsη. The speedup is large for both η= 2and η= 4, and accuracy similar to ASHA is retained. Dataset Reduction factor Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-100 η= 2 ASHA 71.67 ±0.84 3.8h ±1.0h 1.0x 200.0 ±0.0 PASHA 71.65±1.42 0.9h ±0.1h 4.2x 5.9 ±2.0 η= 4 ASHA 71.43 ±1.13 2.7h ±0.9h 1.0x 200.0 ±0.0 PASHA 72.09±1.22 1.0h ±0.4h 2.8x 25.1 ±49.0 5.2.2 B AYESIAN OPTIMIZATION Bayesian Optimization combined with multi-ﬁdelity methods such as Successive Halving can improve the predictive performance of the ﬁnal model (Klein et al., 2020). In this set of experiments, we verify PASHA can speedup also these kinds of methods. Our results are reported in Table 3, where we can clearly see PASHA obtains a similar accuracy result as ASHA with signiﬁcant speedup. 7Published as a conference paper at ICLR 2023 Table 3: NASBench201 results for ASHA with Bayesian Optimization searcher – MOBSTER (Klein et al., 2020) and similarly extended version of PASHA. The results show PASHA can be successfully combined with a smarter conﬁguration selection strategy. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 MOBSTER 94.21±0.28 5.0h ±1.1h 1.0x 200.0 ±0.0 PASHA BO 94.00±0.20 2.6h ±1.8h 2.0x 70.7 ±81.6 CIFAR-100 MOBSTER 72.79±0.68 5.7h ±1.4h 1.0x 200.0 ±0.0 PASHA BO 72.16±1.07 1.6h ±0.5h 3.7x 13.0 ±8.7 ImageNet16-120MOBSTER 46.21±0.70 15.1h±4.0h 1.0x 200.0 ±0.0 PASHA BO 45.36±1.06 3.9h ±1.2h 3.9x 11.8 ±7.9 5.2.3 A LTERNATIVE RANKING FUNCTIONS We have considered a variety of alternative ranking functions in addition to the soft ranking function that automatically estimates the value of ϵby measuring noise in rankings. These include simple ranking (equivalent to soft ranking withϵ= 0.0), soft ranking with ﬁxed values of ϵor obtained using various heuristics (for example based on the standard deviation of objective values in the previous rung), Rank Biased Overlap (RBO) (Webber et al., 2010), and our own reciprocal rank regret metric (RRR) that considers the objective values of conﬁgurations. Details of the ranking functions and additional results are in Appendix C. Table 4 shows a selection of the results on CIFAR-100 with full results in the appendix. We can see there are also other ranking functions that work well and that simple ranking is not sufﬁciently robust – some benevolence is needed. However, the ranking function that estimates the value of ϵ by measuring noise in rankings (to which we refer simply as PASHA) remains the easiest to use, is well-motivated and offers both excellent performance and large speedup. Table 4: NASBench201 – CIFAR-100 results for a variety of ranking functions, showing there are also other well-performing options, even though those are harder to use and are less interpretable. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 PASHA direct ranking 71.69 ±1.05 2.8h±0.7h 1.1x 200.0 ±0.0 PASHA soft rankingϵ= 2.5% 71.41±1.15 1.5h±0.7h 2.1x 88.3 ±74.4 PASHA soft rankingϵ= 2σ 71.14±0.97 1.9h±0.7h 1.7x 136.4 ±75.8 PASHA RBO 71.51 ±0.93 2.4h±0.7h 1.3x 180.5 ±50.6 PASHA RRR 71.42 ±1.51 1.2h±0.5h 2.6x 39.3 ±51.4 One-epoch baseline 65.57 ±5.53 0.3h±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83 ±18.20 0.0h±0.0h N/A 0.0 ±0.0 5.3 HPO EXPERIMENTS We further utilize the PD1 HPO benchmark (Wang et al., 2021) to show the usefulness of PASHA in large-scale settings. In particular, we take WMT15 German-English (Bojar et al., 2015) and ImageNet (Deng et al., 2009) datasets that use xformer (Lefaudeux et al., 2021) and ResNet50 (He et al., 2015) models. Both of them are datasets with a large amount of training examples, in particular WMT15 German-English has about 4.5M examples, while ImageNet has about 1.3M examples. In PD1 we optimize four hyperparameters: base learning rate η∈ [ 10−5,10.0 ] (log scale), momen- tum 1 −β ∈ [ 10−3,1.0 ] (log scale), polynomial learning rate decay schedule power p∈[0.1,2.0] (linear scale) and decay steps fraction λ∈[0.01,0.99] (linear scale). The minibatch size used for WMT experiments is 64, while the minibatch size for ImageNet experiments is 512. There are 1414 epochs available for WMT and 251 for ImageNet. There are also other datasets in PD1, but these only have a small number of epochs with 1 epoch being the minimum amount of resources. As a result there would not be enough rungs to see beneﬁts of the early stopping provided by PASHA. 8Published as a conference paper at ICLR 2023 If resources could be deﬁned in terms of fractions of epochs, PASHA could be beneﬁcial there too. Most public benchmarks have resources deﬁned in terms of epochs, but in practice it is possible to deﬁne resources also in alternative ways. We use 1-NN as a surrogate model for the PD1 benchmark. We repeat our experiments using 5 random seeds and there is only one dataset seed available. The results in Table 5 show that PASHA leads to large speedups on both WMT and ImageNet datasets. The speedup is particularly impressive for the signiﬁcantly larger WMT dataset where it is about 15.5x, highlighting how PASHA can signiﬁcantly accelerate the HPO search on datasets with millions of training examples (WMT has about 4.5M training examples). The one-epoch baseline obtains similar accuracy as ASHA and PASHA for WMT, but performs signiﬁcantly worse on ImageNet dataset. This result suggests that simple approaches such as the one-epoch baseline are not robust and solutions such as PASHA are needed (which we also saw on NASBench201). Selecting the hyperparameters randomly leads to signiﬁcantly worse performance than any of the other approaches. Table 5: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ±1.41 43.7h±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ±2.05 2.8h ±0.6h 15.5x 37.8 ±21.6 One-epoch baseline 62.36±1.40 0.6h ±0.0h 67.3x 1.0 ±0.0 Random baseline 33.93±21.96 0.0h ±0.0h N/A 0.0 ±0.0 ImageNet ASHA 75.10 ±2.03 7.3h ±1.2h 1.0x 251.0 ±0.0 PASHA 73.37 ±2.71 3.8h ±1.0h 1.9x 45.0 ±30.1 One-epoch baseline 63.40±9.91 1.1h ±0.0h 6.7x 1.0 ±0.0 Random baseline 36.94±31.05 0.0h ±0.0h N/A 0.0 ±0.0 6 L IMITATIONS PASHA is designed to speed up ﬁnding the best conﬁguration, making HPO and NAS more accessible. To do so, PASHA interrupts the tuning process when it considers the ranking of conﬁgurations to be sufﬁciently stable, not spending resources on evaluating conﬁgurations in later rungs. However, the beneﬁts of such mechanism will be small in some circumstances. When the number of rungs is small, there will be few opportunities for PASHA to interrupt the tuning and provide large speedups. This phenomenon is demonstrated in Appendix D on the LCBench benchmark (Zimmer et al., 2021). Public benchmarks usually ﬁx the minimum resources to one epoch, while the maximum is benchmark-dependent (e.g. 200 epochs for NASBench201 and 50 for LCBench), leaving little control for algorithms like PASHA in some cases. Appendix E analyses the impact of these choices. For practical usage, we recommend having a maximum amount of resources at least 100 times larger than the minimum amount of resources when using η= 3(default). This can be achieved by measuring resources with higher granularity (e.g. in terms of gradient updates) if needed. 7 C ONCLUSIONS In this work we have introduced a new variant of Successive Halving called PASHA. Despite its simplicity, PASHA leads to strong improvements in the tuning time. For example, in many cases it reduces the time needed to about one third compared to ASHA without a noticeable impact on the quality of the found conﬁguration. For benchmarks with a small number of rungs (LCBench), PASHA provides more modest speedups but this limitation can be mitigated in practice by adopting a more granular unit of measure for resources. Further work could investigate the deﬁnition of rungs and resource levels, with the aim of understanding how they impact the decisions of the algorithm. More broadly this applies not only to PASHA but also to multi-ﬁdelity algorithms in general. PASHA can also be successfully combined with more advanced search strategies based on Bayesian Optimization to obtain improvements in accuracy at a fraction of the time. In the future, we would like to test combinations of PASHA with transfer-learning techniques for multi-ﬁdelity such as RUSH (Zappella et al., 2021) to further decrease the tuning time. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT We include the code for our approach at https://github.com/ondrejbohdal/pasha, including details for how to run the experiments. We use pre-computed benchmarks that make it possible to run the NAS and HPO experiments even without large computational resources. In addition, PASHA is available as part of the Syne Tune library (Salinas et al., 2022). ACKNOWLEDGEMENTS We would like to thank the Syne Tune developers for providing us with a library to easily extend and use in our experiments. We would like to thank Aaron Klein, Matthias Seeger and David Salinas for their support on questions regarding Syne Tune and hyperparameter optimization more broadly. We would also like to thank Valerio Perrone, Sanyam Kapoor and Aditya Rawal for insightful discussions when working on the project. Further, we are thankful to the anonymous reviewers for helping us improve our paper. REFERENCES Peter Auer, Nicol´o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Annual Foundations of Computer Science, 1995. James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. JMLR, 2012. James Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter optimization. In NIPS, 2011. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, 2015. Jorg Bornschein, Francesco Visin Visin, and Simon Osindero. Small data, big decisions: Model selection in the small-data regime. In ICML, 2020. Jia Deng, Wei Dong, Richard Socher, Li-jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical image database, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In IJCAI, 2015. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In ICLR, 2020. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efﬁcient hyperparameter opti- mization at scale. In ICML, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2015. Nikita Ivkin, Zohar Karnin, Valerio Perrone, and Giovanni Zappella. Cost-aware adversarial best arm identiﬁcation. In ICLR NAS Workshop, 2021. Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter optimization. In AISTATS, 2016. Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In ICML, 2013. 10Published as a conference paper at ICLR 2023 Aaron Klein, Louis C. Tiao, Thibaut Lienart, Cedric Archambeau, and Matthias Seeger. Model-based asynchronous hyperparameter and neural architecture search. In arXiv, 2020. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, and Susan Zhang. xformers: A modular and hackable transformer modelling library, 2021. Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In MLSys, 2020. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. JMLR, 2018. Felix Mohr and Jan N. van Rijn. Learning curves for decision making in supervised machine learning - a survey. In arXiv, 2022. David Salinas, Matthias Seeger, Aaron Klein, Valerio Perrone, Martin Wistuba, and Cedric Archam- beau. Syne tune: A library for large scale hyperparameter tuning and reproducible research. In First Conference on Automated Machine Learning (Main Track), 2022. Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training nlp models: A concise overview. In arXiv, 2020. Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. Core-set sampling for efﬁcient neural architecture search. In ICML Workshop on Subset Selection in ML, 2021. Tom Viering and Marco Loog. The shape of learning curves: a review. In arXiv, 2021. Savan Visalpara, Krishnateja Killamsetty, and Rishabh Iyer. A data subset selection framework for efﬁcient hyper-parameter tuning and automatic machine learning. In ICML Workshop on Subset Selection in ML, 2021. Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained Gaussian processes for Bayesian Optimization. In arXiv, 2021. William Webber, Alistair Moffat, and Justin Zobel. A similarity measure for indeﬁnite rankings. In ACM Transactions on Information Systems, 2010. Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. PC-DARTS: Partial channel connections for memory-efﬁcient architecture search. In ICLR, 2020. Giovanni Zappella, David Salinas, and C´edric Archambeau. A resource-efﬁcient method for repeated HPO and NAS problems. In ICML AutoML Workshop, 2021. Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang, and Wanli Ouyang. EcoNAS: Finding proxies for economical neural architecture search. In CVPR, 2020. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-PyTorch Tabular: Multi-ﬁdelity metalearn- ing for efﬁcient and robust AutoDL. TPAMI, 2021. 11Published as a conference paper at ICLR 2023 A A DDITIONAL BASELINES We consider additional baselines that evaluate how good two, three and ﬁve-epoch baselines are compared to PASHA. From Table 6 and 7 we see that while these usually get closer to the performance of ASHA and PASHA than the one-epoch baseline, they are still relatively far compared to PASHA. Moreover, it is crucial to observe that such baselines cannot dynamically allocate resources and decide when to stop, and as a result PASHA can outperform them both in terms of speedup and the quality of the found conﬁguration. Table 6: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h ±0.6h 1.0x 200.0 ±0.0 PASHA 93.57 ±0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 One-epoch baseline 93.30±0.61 0.3h ±0.0h 8.5x 1.0 ±0.0 Two epoch baseline 92.75±0.91 0.7h ±0.0h 4.2x 2.0 ±0.0 Three epoch baseline 93.47±0.71 1.0h ±0.0h 2.8x 3.0 ±0.0 Five epoch baseline 93.38±0.90 1.7h ±0.0h 1.7x 5.0 ±0.0 Random baseline 72.88 ±19.20 0.0h±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h ±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h ±0.4h 3.4x 20.5 ±48.3 One-epoch baseline 65.57±5.53 0.3h ±0.0h 9.2x 1.0 ±0.0 Two-epoch baseline 68.28±4.25 0.7h ±0.0h 4.6x 2.0 ±0.0 Three-epoch baseline 70.47±1.60 1.0h ±0.0h 3.1x 3.0 ±0.0 Five-epoch baseline 70.95±0.95 1.7h ±0.0h 1.8x 5.0 ±0.0 Random baseline 42.83 ±18.20 0.0h±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h ±2.2h 1.0x 200.0 ±0.0 PASHA 45.13 ±1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 One-epoch baseline 41.42±4.98 1.0h ±0.0h 8.8x 1.0 ±0.0 Two-epoch baseline 42.99±1.89 2.0h ±0.0h 4.4x 2.0 ±0.0 Three-epoch baseline 44.65±0.95 3.0h ±0.0h 2.9x 3.0 ±0.0 Five-epoch baseline 44.75±1.03 5.0h ±0.1h 1.8x 5.0 ±0.0 Random baseline 20.75 ±9.97 0.0h ±0.0h N/A 0.0 ±0.0 Table 7: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ± 1.41 43.7h ±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ± 2.05 2.8h ± 0.6h 15.5x 37.8 ±21.6 One-epoch baseline 62.36 ± 1.40 0.6h ± 0.0h 67.3x 1.0 ± 0.0 Two-epoch baseline 60.16± 3.58 1.1h ± 0.0h 39.1x 2.0 ± 0.0 Three-epoch baseline 61.12± 3.47 1.6h ± 0.0h 27.6x 3.0 ± 0.0 Five-epoch baseline 57.89 ± 5.33 2.5h ± 0.0h 17.3x 5.0 ± 0.0 Random baseline 33.93 ±21.96 0.0h ± 0.0h N/A 0.0 ± 0.0 ImageNet ASHA 75.10 ± 2.03 7.3h ± 1.2h 1.0x 251.0 ± 0.0 PASHA 73.37 ± 2.71 3.8h ± 1.0h 1.9x 45.0 ±30.1 One-epoch baseline 63.40 ± 9.91 1.1h ± 0.0h 6.7x 1.0 ± 0.0 Two-epoch baseline 64.61±10.81 1.7h ± 0.0h 4.2x 2.0 ± 0.0 Three-epoch baseline 68.74± 3.79 2.3h ± 0.1h 3.2x 3.0 ± 0.0 Five-epoch baseline 65.91 ± 3.99 3.7h ± 0.1h 2.0x 5.0 ± 0.0 Random baseline 36.94 ±31.05 0.0h ± 0.0h N/A 0.0 ± 0.0 B R EDUCTION FACTOR Table 8 shows the full set of results for our experiments with different reduction factors. The behaviour is the same across all cases. 12Published as a conference paper at ICLR 2023 Table 8: NASBench201 results with various reduction factors η. Dataset Reduction factor Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 η= 2 ASHA 93.88 ±0.27 3.6h ±1.1h 1.0x 200.0 ±0.0 PASHA 93.53 ±0.76 1.0h ±0.3h 3.5x 9.1 ±8.1 η= 4 ASHA 93.75 ±0.28 2.4h ±0.6h 1.0x 200.0 ±0.0 PASHA 93.65 ±0.65 1.1h ±0.5h 2.3x 32.3 ±50.2 CIFAR-100 η= 2 ASHA 71.67 ±0.84 3.8h ±1.0h 1.0x 200.0 ±0.0 PASHA 71.65 ±1.42 0.9h ±0.1h 4.2x 5.9 ±2.0 η= 4 ASHA 71.43 ±1.13 2.7h ±0.9h 1.0x 200.0 ±0.0 PASHA 72.09 ±1.22 1.0h ±0.4h 2.8x 25.1 ±49.0 ImageNet16-120 η= 2 ASHA 46.09 ±0.68 11.9h ±4.0h 1.0x 200.0 ±0.0 PASHA 45.35 ±1.52 2.8h ±0.6h 4.2x 9.3 ±7.1 η= 4 ASHA 45.43 ±0.98 7.9h ±3.0h 1.0x 200.0 ±0.0 PASHA 45.52 ±1.30 2.9h ±1.1h 2.8x 18.4 ±18.7 C E XPERIMENTS WITH ALTERNATIVE RANKING FUNCTIONS C.1 D ESCRIPTION PASHA employs a ranking function whose choice is completely arbitrary. In our main set of experiments we used soft ranking that automatically estimates the value of ϵby measuring noise in rankings. In this set of experiments we would like to evaluate different criteria to deﬁne the ranking of the candidates. We describe the functions considered next. C.1.1 D IRECT RANKING As a baseline, we study if we can use the simple ranking of conﬁgurations by predictive performance (e.g., sorting from the ones with the highest accuracy to the ones with the lowest). If any of the conﬁgurations change their order, we consider the ranking unstable and increase the resources. C.1.2 S OFT RANKING VARIATIONS We consider several variations of soft ranking. The ﬁrst variation is to ﬁx the value of the ϵparameter. We have considered values 0.01, 0.02, 0.025, 0.03, 0.05. The second set of variations aim to estimate the value of ϵautomatically, using various heuristics. The heuristics we have evaluated include: • Standard deviation: calculate the standard deviation of the considered performance measure (e.g. accuracy) of the conﬁgurations in the previous rung and set a multiple of it as the value of ϵ– we tried multiples of 1, 2 and 3. • Mean distance: value of ϵis set as the mean distance between the score of the conﬁgurations in the previous rung. • Median distance: similar to the mean distance, but using the median distance. There are various beneﬁts for estimating the value of ϵby measuring noise in rankings, as presented in our paper: • There is no need to set the value of ϵmanually. • Estimation of ϵhas an intuitive motivation that makes sense. • The value of ϵcan dynamically adapt to the different stages of hyperparameter optimization. • The approach works well in practice. C.1.3 R ANK BIASED OVERLAP (RBO) (W EBBER ET AL ., 2010) A score that can be broadly interpreted as a weighted correlation between rankings. We can specify how much we want to prioritize the top of the ranking using parameter pthat is between 0.0 and 1.0, 13Published as a conference paper at ICLR 2023 with a smaller value giving more priority to the top of the ranking. The best value is 1.0, while it gives value of 0.0 for rankings that are completely the opposite. We compute the RBO value and then compare it to the selected threshold t, increasing the resources if the value is less than the threshold. C.1.4 R ECIPROCAL RANK REGRET (RRR) A key insight is that conﬁgurations can be very similar to each other and differences in their rankings will not affect the quality of the found solution signiﬁcantly. To account for this we look at the objective values of the conﬁgurations (e.g. accuracy) and compute the relative regret that we would pay at the current rung if we would have assumed the ranking at the previous rung correct. We deﬁne reciprocal rank regret (RRR) as: RRR = n−1∑ i=0 (fi −f′ i) fi wi, where f represents the ordered scores in the top rung, f′represents the reordered scores from the top rung according to the second rung, nis the number of conﬁgurations in the top rung and pis the parameter that says how much attention to give to the top of the ranking. The weights wi sum to 1 and can be selected in different ways to e.g. give more priority to the top of the ranking. For example, we could use the following weights: wi = pi ∑n−1 i=0 pi The metric has an intuitive interpretation: it is the average relative regret with priority on top of the ranking. The best value of RRR is 0.0, while the worst possible value is 1.0. We also consider a version of RRR which considers the absolute values of the differences in the objectives - Absolute RRR (ARRR). We have evaluated these additional ranking functions using NASBench201 benchmark. C.2 R ESULTS We report the results in Table 9, 10 and 11. We see there are also several other variations that achieve strong results across a variety of datasets within NASBench201, most notably soft ranking 2σand variations based on RRR. In these cases we obtain similar performance as ASHA, but at a signiﬁcantly shorter time. We additionally also give a similar analysis in Table 12 (analogous to Table 4), where we analyse a selection of the most interesting ranking functions for the PD1 benchmark. 14Published as a conference paper at ICLR 2023 Table 9: NASBench201 – CIFAR-10 results for a variety of ranking functions. Approach Accuracy (%) Runtime Speedup factor Max resources ASHA 93.85 ± 0.25 3.0h ±0.6h 1.0x 200.0 ± 0.0 PASHA 93.57 ± 0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 PASHA direct ranking 93.79 ± 0.26 2.7h ±0.6h 1.1x 198.4 ± 6.0 PASHA soft rankingϵ= 0.01 93.79± 0.26 2.6h ±0.5h 1.1x 194.3 ±21.2 PASHA soft rankingϵ= 0.02 93.78± 0.31 2.4h ±0.5h 1.2x 152.4 ±58.3 PASHA soft rankingϵ= 0.025 93.78± 0.31 2.3h ±0.5h 1.3x 144.5 ±59.4 PASHA soft rankingϵ= 0.03 93.78± 0.32 2.2h ±0.6h 1.3x 128.6 ±58.3 PASHA soft rankingϵ= 0.05 93.79± 0.49 1.8h ±0.7h 1.6x 76.0 ±66.0 PASHA soft ranking1σ 93.75± 0.32 2.4h ±0.5h 1.2x 186.4 ±35.2 PASHA soft ranking2σ 93.88± 0.28 1.9h ±0.5h 1.5x 132.7 ±68.7 PASHA soft ranking3σ 93.56± 0.69 0.9h ±0.3h 3.1x 16.2 ±19.9 PASHA soft ranking mean distance 93.73± 0.52 2.3h ±0.4h 1.3x 184.1 ±40.5 PASHA soft ranking median distance 93.82± 0.26 2.3h ±0.5h 1.3x 169.2 ±51.2 PASHA RBO p=1.0, t=0.5 93.49 ± 0.78 0.7h ±0.1h 4.2x 4.6 ± 6.0 PASHA RBO p=0.5, t=0.5 93.77 ± 0.35 2.2h ±0.6h 1.3x 144.0 ±71.2 PASHA RRR p=1.0, t=0.05 93.49 ± 0.78 0.7h ±0.0h 4.4x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 93.76 ± 0.31 2.1h ±0.6h 1.4x 140.9 ±69.7 PASHA ARRR p=1.0, t=0.05 93.71 ± 0.35 2.4h ±0.4h 1.2x 179.0 ±42.9 PASHA ARRR p=0.5, t=0.05 93.81 ± 0.30 2.5h ±0.4h 1.2x 181.0 ±40.9 One-epoch baseline 93.30 ± 0.61 0.3h ±0.0h 8.5x 1.0 ± 0.0 Random baseline 72.88 ±19.20 0.0h ±0.0h N/A 0.0 ± 0.0 Table 10: NASBench201 – CIFAR-100 results for a variety of ranking functions. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 71.69 ± 1.05 3.2h ±0.9h 1.0x 200.0 ± 0.0 PASHA 71.84 ± 1.41 0.9h ±0.4h 3.4x 20.5 ±48.3 PASHA direct ranking 71.69 ± 1.05 2.8h ±0.7h 1.1x 200.0 ± 0.0 PASHA soft rankingϵ= 0.01 71.55± 1.04 2.5h ±0.7h 1.3x 198.3 ± 6.5 PASHA soft rankingϵ= 0.02 70.94± 0.85 2.0h ±0.5h 1.6x 160.5 ±62.9 PASHA soft rankingϵ= 0.025 71.41± 1.15 1.5h ±0.7h 2.1x 88.3 ±74.4 PASHA soft rankingϵ= 0.03 71.00± 1.38 1.0h ±0.5h 3.2x 39.4 ±63.4 PASHA soft rankingϵ= 0.05 70.71± 1.66 0.7h ±0.0h 4.9x 3.0 ± 0.0 PASHA soft ranking1σ 71.56± 1.03 2.5h ±0.6h 1.3x 184.1 ±40.5 PASHA soft ranking2σ 71.14± 0.97 1.9h ±0.7h 1.7x 136.4 ±75.8 PASHA soft ranking3σ 71.63± 1.60 1.0h ±0.3h 3.3x 20.2 ±25.3 PASHA soft ranking mean distance 71.51± 0.99 2.4h ±0.5h 1.4x 189.8 ±30.3 PASHA soft ranking median distance 71.52± 0.98 2.4h ±0.6h 1.3x 189.5 ±30.6 PASHA RBO p=1.0, t=0.5 70.69 ± 1.67 0.7h ±0.1h 4.6x 3.8 ± 2.0 PASHA RBO p=0.5, t=0.5 71.51 ± 0.93 2.4h ±0.7h 1.3x 180.5 ±50.6 PASHA RRR p=1.0, t=0.05 70.71 ± 1.66 0.7h ±0.0h 4.9x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 71.42 ± 1.51 1.2h ±0.5h 2.6x 39.3 ±51.4 PASHA ARRR p=1.0, t=0.05 70.80 ± 1.70 0.8h ±0.4h 3.8x 22.9 ±51.3 PASHA ARRR p=0.5, t=0.05 71.41 ± 1.05 1.8h ±0.6h 1.7x 110.0 ±68.7 One-epoch baseline 65.57 ± 5.53 0.3h ±0.0h 9.2x 1.0 ± 0.0 Random baseline 42.83 ±18.20 0.0h ±0.0h N/A 0.0 ± 0.0 15Published as a conference paper at ICLR 2023 Table 11: NASBench201 – ImageNet16-120 results for a variety of ranking functions. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 45.63 ± 0.81 8.8h ±2.2h 1.0x 200.0 ± 0.0 PASHA 45.13 ± 1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 PASHA direct ranking 45.63 ± 0.81 8.3h ±2.5h 1.1x 200.0 ± 0.0 PASHA soft rankingϵ= 0.01 45.52± 0.89 7.0h ±1.5h 1.3x 185.7 ±36.1 PASHA soft rankingϵ= 0.02 45.79± 1.16 4.4h ±1.4h 2.0x 71.4 ±50.8 PASHA soft rankingϵ= 0.025 46.01± 1.00 3.2h ±1.0h 2.8x 28.6 ±27.7 PASHA soft rankingϵ= 0.03 45.62± 1.48 2.4h ±0.7h 3.6x 11.0 ±10.0 PASHA soft rankingϵ= 0.05 44.90± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA soft ranking1σ 45.63± 0.89 6.5h ±1.3h 1.4x 177.1 ±44.2 PASHA soft ranking2σ 45.39± 1.22 4.5h ±1.4h 1.9x 91.2 ±58.0 PASHA soft ranking3σ 44.90± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA soft ranking mean distance 45.50± 1.12 6.2h ±1.5h 1.4x 157.7 ±54.7 PASHA soft ranking median distance 45.67± 0.95 6.3h ±1.6h 1.4x 156.3 ±52.2 PASHA RBO p=1.0, t=0.5 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA RBO p=0.5, t=0.5 45.24 ± 1.13 6.4h ±1.3h 1.4x 148.3 ±56.9 PASHA RRR p=1.0, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA ARRR p=1.0, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA ARRR p=0.5, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 One-epoch baseline 41.42 ± 4.98 1.0h ±0.0h 8.8x 1.0 ± 0.0 Random baseline 20.75 ± 9.97 0.0h ±0.0h N/A 0.0 ± 0.0 Table 12: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark, using a selection of the most interesting candidates for ranking functions. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ±1.41 43.7h±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ±2.05 2.8h ±0.6h 15.5x 37.8 ±21.6 PASHA direct ranking 62.16 ±1.75 39.3h±38.3h 1.1x 1024.0 ±466.6 PASHA soft rankingϵ= 2.5% 62.09±2.04 1.3h ±0.4h 33.4x 4.2 ± 2.4 PASHA soft ranking2σ 62.52±2.18 1.1h ±0.1h 38.8x 3.0 ± 0.0 PASHA RBO p=0.5, t=0.5 61.44±1.23 6.7h ±7.8h 6.5x 147.6 ±113.2 PASHA RRR p=0.5, t=0.05 62.52±2.18 1.1h ±0.1h 38.8x 3.0 ± 0.0 One-epoch baseline 62.36 ±1.40 0.6h ±0.0h 67.3x 1.0 ± 0.0 Random baseline 33.93 ±21.96 0.0h ±0.0h N/A 0.0 ± 0.0 ImageNet ASHA 75.10 ±2.03 7.3h ±1.2h 1.0x 251.0 ± 0.0 PASHA 73.37 ±2.71 3.8h ±1.0h 1.9x 45.0 ±30.1 PASHA direct ranking 75.10 ±2.03 6.8h ±0.7h 1.1x 247.8 ± 3.9 PASHA soft rankingϵ= 2.5% 74.73±1.99 4.3h ±2.5h 1.7x 140.4 ±112.8 PASHA soft ranking2σ 75.82±0.82 5.0h ±1.6h 1.5x 133.0 ±96.8 PASHA RBO p=0.5, t=0.5 74.80±2.19 4.4h ±2.1h 1.6x 117.4 ±109.4 PASHA RRR p=0.5, t=0.05 74.98±2.12 1.6h ±0.0h 4.7x 3.0 ± 0.0 One-epoch baseline 63.40 ±9.91 1.1h ±0.0h 6.7x 1.0 ± 0.0 Random baseline 36.94 ±31.05 0.0h ±0.0h N/A 0.0 ± 0.0 16Published as a conference paper at ICLR 2023 D A DDITIONAL RESULTS ON LCB ENCH We additionally evaluate PASHA on the LCBench benchmark (Zimmer et al., 2021) where only modest speedups can be expected due to a small number of epochs (and hence rungs) available. LCBench limits the maximum amount of resources per conﬁguration to 50 epochs, so when using and setting the minimum resource level to 1 epoch, it is a challenging testbed for an algorithm like PASHA. The hyperparameters optimized include number of layers ∈[1,5], max. number of units ∈[64,1024] (log scale), batch size ∈[16,512] (log scale), learning rate ∈ [ 10−4,10−1] (log scale), weight decay ∈ [ 10−5,10−1] , momentum ∈[0.1,0.99] and max. value of dropout ∈[0.0,1.0]. Similarly as in our other experiments, we use η= 3and stop after sampling 256 candidates. Overall, the results in Table 13 conﬁrm an accuracy level on-par with ASHA. While, as expected, the speedup is reduced compared to the experiments on NASBench, in several cases PASHA achieves a 20+% speedup with peaks around 40%. If only a small number of epochs is sufﬁcient for training the model on the given dataset, then HPO can be performed on a sub-epoch basis, e.g. deﬁning the rung levels in terms of iterations instead of epochs. PASHA would then be able to give a large speedup even in cases with smaller numbers of epochs – an example of which is LCBench. Table 13: Results of the HPO experiments on the LCBench benchmark. Mean and std of the test accuracy across ﬁve random seeds. PASHA achieves similar accuracies as ASHA, but gives only modest speedups because of the limited number of rung levels and opportunities to stop the HPO early. To enable large speedup from PASHA, we could redeﬁne the rung levels in terms of neural network weights updates rather than epochs. Dataset ASHA accuracy (%) PASHA accuracy (%) PASHA speedup APSFailure 97.52 ± 0.92 97.01 ±0.75 1.3x Amazonemployeeaccess 94.01 ± 0.40 94.21 ±0.00 1.1x Australian 83.35 ± 0.33 83.35 ±0.51 1.1x Fashion-MNIST 86.70 ± 1.87 86.34 ±1.32 1.1x KDDCup09appetency 98.22 ± 0.00 98.22 ±0.00 1.1x MiniBooNE 86.13 ± 1.57 86.24 ±1.62 1.4x Adult 79.14 ± 0.85 79.14 ±0.85 1.2x Airlines 59.57 ± 1.32 59.22 ±0.76 1.4x Albert 64.31 ± 0.99 64.23 ±0.61 1.2x Bank-marketing 88.34 ± 0.07 88.38 ±0.00 1.2x Blood-transfusion-service-center 79.92 ± 0.20 76.99 ±6.00 1.1x Car 86.60 ± 6.41 86.60 ±6.41 1.1x Christine 71.05 ± 1.17 70.15 ±1.85 1.2x Cnae-9 94.10 ± 0.31 94.44 ±0.11 1.0x Connect-4 62.28 ± 6.87 65.69 ±0.04 1.2x Covertype 59.76 ± 3.24 61.64 ±1.64 1.2x Credit-g 70.30 ± 0.84 70.79 ±0.68 1.1x Dionis 64.58 ± 9.89 64.58 ±9.89 1.1x Fabert 56.11 ±10.89 53.47 ±9.75 1.1x Helena 19.16 ± 3.20 19.16 ±3.20 1.1x Higgs 66.48 ± 3.16 66.48 ±3.16 1.1x Jannis 58.92 ± 2.38 59.63 ±2.81 1.4x Jasmine 75.85 ± 0.36 75.55 ±0.68 1.0x Junglechess2pcsrawendgamecomplete 72.86 ± 4.69 74.94 ±7.84 1.3x Kc1 80.32 ± 4.37 80.86 ±3.37 1.2x Kr-vs-kp 92.50 ± 3.93 90.95 ±4.70 1.0x Mfeat-factors 98.21 ± 0.15 98.15 ±0.15 1.1x Nomao 94.12 ± 0.60 94.25 ±0.64 1.1x Numerai28.6 52.03 ± 0.55 52.30 ±0.24 1.3x Phoneme 76.65 ± 2.65 75.42 ±2.87 1.1x Segment 83.15 ± 2.54 83.15 ±2.54 1.0x Sylvine 90.57 ± 1.87 90.89 ±2.04 1.0x Vehicle 71.76 ± 2.57 71.76 ±2.57 1.1x V olkert 50.72 ± 1.91 50.72 ±1.91 1.1x 17Published as a conference paper at ICLR 2023 E I NVESTIGATION WITH VARIABLE MAXIMUM RESOURCES We analyse the impact of variable maximum resources (number of epochs) on how large speedup PASHA provides over ASHA. More speciﬁcally, we change the maximum resources available for ASHA and also the upper boundary on maximum resources for PASHA. We utilize NASBench201 benchmark for these experiments and set the number of epochs to 200 (default) or 50 (other details are the same as earlier). The results in Table 14 conﬁrm that PASHA leads to larger speedups when there are more epochs (and rung levels) available. This analysis also explains the modest speedups on LCBench analysed earlier. If the model is trained for a small number of epochs, it is worth redesigning the HPO so that there are more rung levels available, enabling PASHA to give larger speedups. This can be achieved by using sub-epoch resource levels – specifying the rung levels and the minimum resources in terms of the number of iterations (neural network weights updates). Based on the results observed across various benchmarks, we would recommend having at least 5 rung levels in ASHA, with more rung levels leading to larger speedups from PASHA over ASHA. Table 14: NASBench201 results. PASHA leads to larger speedups if the models are trained with more epochs. Dataset Number of epochs Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 200 ASHA 93.85 ±0.25 3.0h±0.6h 1.0x 200.0 ±0.0 PASHA 93.57±0.75 1.3h±0.6h 2.3x 36.1 ±50.0 50 ASHA 93.78 ±0.39 1.8h±0.2h 1.0x 50.0 ±0.0 PASHA 93.58±0.75 1.2h±0.4h 1.5x 22.0 ±16.8 CIFAR-100 200 ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 50 ASHA 72.24 ±0.87 1.8h±0.3h 1.0x 50.0 ±0.0 PASHA 71.91±1.32 0.9h±0.3h 2.0x 10.5 ±12.1 ImageNet16-120 200 ASHA 45.63 ±0.81 8.8h±2.2h 1.0x 200.0 ±0.0 PASHA 45.13±1.51 2.9h±1.7h 3.1x 21.3 ±48.1 50 ASHA 45.97 ±0.99 5.2h±0.7h 1.0x 50.0 ±0.0 PASHA 45.09±1.52 2.7h±1.0h 1.9x 11.3 ±11.7 F A NALYSIS OF LEARNING CURVES We analyse the NASBench201 learning curves in Figure 3 and 4. To make the analysis realistic and easier to grasp, we ﬁrst sample a random subset of 256 conﬁgurations, similarly as we do for our NAS experiments. Figure 3 shows the learning curves of the top three conﬁgurations (selected in terms of their ﬁnal performance). We see that these learning curves are very close to each other and frequently cross due to noise in the training, allowing us to estimate a meaningful value of ϵ parameter (conﬁgurations that repeatedly swap their order are very likely to be similarly good, so we can select any of them because the goal is to ﬁnd a strong conﬁguration quickly rather than the very best one). Figure 4 shows all learning curves from the same random sample of 256 conﬁgurations. In this case we can see that the learning curves are relatively well-behaved (especially the ones at the top), and any exceptions are rare. 18Published as a conference paper at ICLR 2023 0 50 100 150 200 Epoch 40 50 60 70 80 90Validation accuracy (%) CIFAR-10 0 50 100 150 200 Epoch 20 40 60Validation accuracy (%) CIFAR-100 0 50 100 150 200 Epoch 10 20 30 40Validation accuracy (%) ImageNet16-120 Figure 3: Analysis of how the learning curves of the top three conﬁgurations (in terms of ﬁnal validation accuracy; from a random sample of 256 conﬁgurations) evolve across epochs. We see that such similar conﬁgurations frequently change their ranks, enabling us to calculate a meaningful value of ϵparameter. 0 50 100 150 200 Epoch 20 40 60 80Validation accuracy (%) CIFAR-10 0 50 100 150 200 Epoch 0 20 40 60Validation accuracy (%) CIFAR-100 0 50 100 150 200 Epoch 0 10 20 30 40Validation accuracy (%) ImageNet16-120 Figure 4: Analysis of what the learning curves look like for a random sample of 256 conﬁgurations. We see that the learning curves are relatively well-behaved (especially the ones at the top), and any exceptions are rare. G I NVESTIGATION OF HOW VALUE ϵEVOLVES We analyse how the value of ϵthat is used for calculating soft ranking develops during the HPO process. We show the results in Figure 5 for the three different datasets available in NASBench201 (taking one seed). The results show the obtained values of ϵare relatively small. 0 500 1000 1500 2000 Number of updates received 0.00 0.05 0.10 0.15 0.20 0.25 0.30Value of  CIFAR-10 0 200 400 600 Number of updates received 0.00 0.05 0.10 0.15Value of  CIFAR-100 0 200 400 600 800 Number of updates received 0.00 0.02 0.04 0.06Value of  ImageNet16-120 Figure 5: Analysis of how the value of ϵevolves as we receive additional updates about the perfor- mances of candidate conﬁgurations. Note that most of the updates are obtained in the top rung due to how multi-ﬁdelity methods work. 19Published as a conference paper at ICLR 2023 H I NVESTIGATION OF PERCENTILE VALUE N We investigate the impact of using various percentile valuesN used for estimating the value of ϵin Table 15. The intuition is that we want to take some value on the top end rather than the maximum distance in case there are some outliers. We see that the results are relatively stable, even though larger value of N can lead to further speedups. However, from the point of view of a practitioner we would still take N = 90in case there are any outliers in the speciﬁc new use-case. Table 15: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Investigation of various percentile values ( N) to use for calculating parameter ϵ. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h ±0.6h 1.0x 200.0 ±0.0 PASHAN= 100%93.70±0.61 1.0h ±0.4h 3.0x 13.8 ±19.5 PASHAN= 95% 93.64±0.59 1.0h ±0.4h 2.8x 15.4 ±19.5 PASHAN= 90% 93.57±0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 PASHAN= 80% 93.86±0.53 1.5h ±0.6h 1.9x 60.9 ±60.7 One-epoch baseline 93.30±0.61 0.3h ±0.0h 8.5x 1.0 ±0.0 Random baseline 72.88±19.20 0.0h ±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h ±0.9h 1.0x 200.0 ±0.0 PASHAN= 100%71.84±1.41 0.8h ±0.1h 3.9x 6.6 ±2.9 PASHAN= 95% 71.84±1.41 0.8h ±0.1h 3.9x 6.6 ±2.9 PASHAN= 90% 71.91±1.32 0.9h ±0.3h 3.5x 12.6 ±19.2 PASHAN= 80% 71.78±1.31 1.2h ±0.6h 2.6x 56.0 ±76.2 One-epoch baseline 65.57±5.53 0.3h ±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83±18.20 0.0h ±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h ±2.2h 1.0x 200.0 ±0.0 PASHAN= 100%45.09±1.61 2.3h ±0.4h 3.7x 7.0 ±2.8 PASHAN= 95% 45.26±1.58 2.4h ±0.4h 3.7x 7.4 ±2.7 PASHAN= 90% 45.13±1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 PASHAN= 80% 45.36±1.38 3.6h ±1.2h 2.5x 40.5 ±47.7 One-epoch baseline 41.42±4.98 1.0h ±0.0h 8.8x 1.0 ±0.0 Random baseline 20.75±9.97 0.0h ±0.0h N/A 0.0 ±0.0 20",
      "meta_data": {
        "arxiv_id": "2207.06940v2",
        "authors": [
          "Ondrej Bohdal",
          "Lukas Balles",
          "Martin Wistuba",
          "Beyza Ermis",
          "Cédric Archambeau",
          "Giovanni Zappella"
        ],
        "published_date": "2022-07-14T14:06:15Z",
        "pdf_url": "https://arxiv.org/pdf/2207.06940v2.pdf",
        "github_url": "https://github.com/ondrejbohdal/pasha"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the high computational cost of Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS), especially with large datasets, even when using multi-fidelity methods. It proposes PASHA (Progressive ASHA), an extension of ASHA that dynamically allocates maximum resources for the tuning procedure based on the need. PASHA significantly reduces computational resources and tuning time compared to ASHA while maintaining similar predictive performance. Key contributions include: 1) Introducing PASHA for dynamic resource allocation in HPO/NAS, 2) Demonstrating significant speedups without sacrificing performance through empirical evaluation, and 3) Showing PASHA's compatibility with sample-efficient strategies like Bayesian Optimization.",
        "methodology": "PASHA extends ASHA and is inspired by the 'doubling trick' concept. It starts with a small initial amount of resources and progressively increases them only if the ranking of configurations in the top two rungs (rounds of promotion) has not stabilized. This allows for early stopping when the relative performance of promising configurations becomes consistent. To handle noise in the training process, PASHA employs a 'soft ranking' approach where configurations are considered equivalent if their performance difference is below a threshold \\u03b5. This \\u03b5 value is automatically estimated by identifying pairs of configurations that repeatedly swap ranks across different resource levels (epochs/iterations), calculating it as the N-th percentile (default 90th) of the performance differences among these 'criss-crossing' configurations. The algorithm maintains a maximum resource 'safety net' to prevent indefinite resource increases.",
        "experimental_setup": "The method was empirically evaluated on two main sets of experiments. For Neural Architecture Search (NAS), it used the NASBench201 benchmark across three datasets: CIFAR-10, CIFAR-100, and ImageNet16-120. For Hyperparameter Optimization (HPO), it used two large-scale tasks from the PD1 benchmark: WMT15 German-English (with xformer) and ImageNet (with ResNet50). All experiments involved two phases: 1) running the optimizer until 256 candidate configurations were evaluated, and 2) retraining the best identified configuration from scratch (using only the training set for experiments). Performance was measured by runtime (HPO phase only) and accuracy after retraining. Default parameters included a minimum resource 'r' of 1 epoch, a maximum resource 'R' (for ASHA or as a safety net for PASHA) of 200 epochs for NASBench201 and 1414/251 epochs for PD1 tasks, and a reduction factor \\u03b7=3. Experiments were averaged over multiple random seeds (e.g., 5 scheduler seeds, 3 NASBench201 seeds, totaling 15 repetitions). Baselines included ASHA, 'one-epoch', 'two-epoch', 'three-epoch', 'five-epoch' and 'random' baselines, and a combination with Bayesian Optimization (MOBSTER). Various alternative ranking functions were also tested, including direct ranking, soft ranking with fixed or heuristic-estimated \\u03b5, Rank Biased Overlap (RBO), and Reciprocal Rank Regret (RRR).",
        "limitations": "PASHA's benefits in speedup are limited when the number of available rungs (resource levels) is small. This is observed in benchmarks like LCBench, which has a maximum of 50 epochs and a minimum of 1 epoch, offering few opportunities for early stopping. Public benchmarks often fix resource granularities (e.g., in epochs), which can constrain PASHA's ability to demonstrate large speedups. The authors recommend having a maximum amount of resources at least 100 times larger than the minimum amount (for \\u03b7=3) to leverage PASHA effectively. If the number of epochs is small, achieving this might require defining resources with higher granularity, such as in terms of gradient updates or iterations, rather than epochs.",
        "future_research_directions": "Future work could involve investigating the optimal definition of rungs and resource levels, not just for PASHA but for multi-fidelity algorithms in general, to understand their impact on algorithmic decisions. Another promising direction is to test combinations of PASHA with transfer-learning techniques specifically designed for multi-fidelity problems, such as RUSH (Zappella et al., 2021), with the goal of further decreasing the overall tuning time.",
        "experimental_code": "class PASHA(HyperbandScheduler):\n    def __init__(self, config_space: Dict, metric: str, resource_attr: str, **kwargs):\n        _assert_max_resource_args(kwargs)\n        super(PASHA, self).__init__(\n            config_space=config_space,\n            metric=metric,\n            searcher=\"random\",\n            resource_attr=resource_attr,\n            type=\"pasha\",\n            **kwargs,\n        )\n\nimport numpy as np\nfrom syne_tune.optimizer.schedulers.hyperband_promotion import PromotionRungSystem\nimport itertools\n\nclass PASHARungSystem(PromotionRungSystem):\n    def __init__(\n        self,\n        rung_levels,\n        promote_quantiles,\n        metric,\n        mode,\n        resource_attr,\n        max_t,\n        ranking_criterion,\n        epsilon,\n        epsilon_scaling,\n    ):\n        super().__init__(\n            rung_levels, promote_quantiles, metric, mode, resource_attr, max_t\n        )\n        self.ranking_criterion = ranking_criterion\n        self.current_rung_idx = 2\n        self.rung_levels = rung_levels\n        self.current_max_t = rung_levels[self.current_rung_idx - 1]\n        self.epsilon = epsilon\n        self.epsilon_scaling = epsilon_scaling\n        if ranking_criterion == 'soft_ranking_auto':\n            self.per_epoch_results = {}\n            self.epoch_to_trials = {}\n            self.current_max_epoch = -1\n\n    def _effective_max_t(self):\n        return self.current_max_t\n\n    def _get_top_rungs_rankings(self, num_rungs=2):\n        rankings = []\n        rungs = [self._rungs[-self.current_rung_idx + e] for e in range(num_rungs)]\n        for rung in rungs:\n            if rung.data != {}:\n                trial_ids = rung.data.keys()\n                values = []\n                for trial_id in trial_ids:\n                    values.append(rung.data[trial_id][0])\n                values_order = np.array(values).argsort()\n                values_ranking = values_order.argsort()\n                ranking = list(zip(trial_ids, values_ranking, values))\n\n                rankings.append(ranking)\n\n        return rankings\n\n    def _get_sorted_top_rungs(self, rankings):\n        top_rung_keys = set([e[0] for e in rankings[0]])\n        corresponding_previous_rung_trials = filter(\n            lambda e: e[0] in top_rung_keys, rankings[1]\n        )\n        if self._mode == \"max\":\n            reverse = True\n        else:\n            reverse = False\n\n        sorted_top_rung = sorted(rankings[0], key=lambda e: e[1], reverse=reverse)\n        sorted_previous_rung = sorted(\n            corresponding_previous_rung_trials, key=lambda e: e[1], reverse=reverse\n        )\n        return sorted_top_rung, sorted_previous_rung\n\n    def _evaluate_soft_ranking(self, sorted_top_rung, sorted_previous_rung) -> bool:\n        keep_current_budget = True\n        if len(sorted_previous_rung) < 2:\n            epsilon = 0.0\n        elif self.ranking_criterion == \"soft_ranking_std\":\n            epsilon = (\n                np.std([e[2] for e in sorted_previous_rung]) * self.epsilon_scaling\n            )\n        elif (\n            self.ranking_criterion == \"soft_ranking_median_dst\"\n            or self.ranking_criterion == \"soft_ranking_mean_dst\"\n        ):\n            scores = [e[2] for e in sorted_previous_rung]\n            distances = [\n                abs(e1 - e2)\n                for idx1, e1 in enumerate(scores)\n                for idx2, e2 in enumerate(scores)\n                if idx1 != idx2\n            ]\n            if self.ranking_criterion == \"soft_ranking_mean_dst\":\n                epsilon = np.mean(distances) * self.epsilon_scaling\n            elif self.ranking_criterion == \"soft_ranking_median_dst\":\n                epsilon = np.median(distances) * self.epsilon_scaling\n            else:\n                raise ValueError(\n                    \"Ranking criterion {} is not supported\".format(\n                        self.ranking_criterion\n                    )\n                )\n        else:\n            epsilon = self.epsilon\n\n        previous_rung_groups = []\n        for idx, item in enumerate(sorted_previous_rung):\n            current_rung_group = [item[0]]\n            for idx_after in range(idx + 1, len(sorted_previous_rung)):\n                new_item = sorted_previous_rung[idx_after]\n\n                if self._mode == \"max\":\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                else:\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            for idx_before in range(idx - 1, -1, -1):\n                new_item = sorted_previous_rung[idx_before]\n                if self._mode == \"max\":\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                else:\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            previous_rung_groups.append(set(current_rung_group))\n\n        for idx, item in enumerate(sorted_top_rung):\n            if item[0] not in previous_rung_groups[idx]:\n                keep_current_budget = False\n                break\n\n        return keep_current_budget\n\n    def _update_epsilon(self):\n        seen_pairs = set()\n        noisy_cfg_distances = []\n        top_epoch = min(self.current_max_epoch, self._rungs[-self.current_rung_idx].level)\n        bottom_epoch = min(self._rungs[-self.current_rung_idx+1].level, self.current_max_epoch)\n        for epoch in range(top_epoch, bottom_epoch, -1):\n            if len(self.epoch_to_trials[epoch]) > 1:\n                for pair in itertools.combinations(self.epoch_to_trials[epoch], 2):\n                    c1, c2 = pair[0], pair[1]\n                    if (c1, c2) not in seen_pairs:\n                        seen_pairs.add((c1, c2))\n                        p1, p2 = self.per_epoch_results[c1][epoch], self.per_epoch_results[c2][epoch]\n                        cond = p1 > p2\n\n                        opposite_order = False\n                        same_order_after_opposite = False\n                        for prev_epoch in range(epoch - 1, 0, -1):\n                            pp1, pp2 = self.per_epoch_results[c1][prev_epoch], self.per_epoch_results[c2][prev_epoch]\n                            p_cond = pp1 > pp2\n                            if p_cond == (not cond):\n                                opposite_order = True\n                            if opposite_order and p_cond == cond:\n                                same_order_after_opposite = True\n                                break\n\n                        if opposite_order and same_order_after_opposite:\n                            noisy_cfg_distances.append(abs(p1 - p2))\n\n        if len(noisy_cfg_distances) > 0:\n            self.epsilon = np.percentile(noisy_cfg_distances, 90)\n            if str(self.epsilon) == 'nan':\n                raise ValueError('Epsilon became nan') \n\n    def _update_per_epoch_results(self, trial_id, result):\n        if trial_id not in self.per_epoch_results:\n            self.per_epoch_results[trial_id] = {}\n        self.per_epoch_results[trial_id][result[self._resource_attr]] = result[self._metric]\n\n        if result[self._resource_attr] not in self.epoch_to_trials:\n            self.epoch_to_trials[result[self._resource_attr]] = set() \n        self.epoch_to_trials[result[self._resource_attr]].add(trial_id)\n\n        if result[self._resource_attr] > self.current_max_epoch:\n            self.current_max_epoch = result[self._resource_attr]\n\n    def _decide_resource_increase(self, rankings) -> bool:\n        if len(rankings) == 2:\n            sorted_top_rung, sorted_previous_rung = self._get_sorted_top_rungs(rankings)\n        else:\n            return False\n\n        keep_current_budget = self._evaluate_soft_ranking(\n            sorted_top_rung, sorted_previous_rung\n        )\n\n        return not keep_current_budget\n\n    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:\n        ret_dict = super().on_task_report(trial_id, result, skip_rungs)\n\n        if self.ranking_criterion == \"soft_ranking_auto\":\n            self._update_per_epoch_results(trial_id, result)\n            self._update_epsilon()\n\n        rankings = self._get_top_rungs_rankings(num_rungs=2)\n        increase_resources = self._decide_resource_increase(rankings)\n\n        if increase_resources:\n            if self.current_rung_idx < len(self._rungs):\n                self.current_rung_idx += 1\n                self.current_max_t = self.rung_levels[self.current_rung_idx - 1]\n            else:\n                self.current_max_t = self.max_t\n\n        return ret_dict\n\n# From benchmarking/cli/scheduler_factory.py\n        if scheduler == \"hyperband_pasha\":\n            rung_system_kwargs = scheduler_options.get(\"rung_system_kwargs\", dict())\n            for name, tp in (\n                (\"ranking_criterion\", str),\n                (\"epsilon\", float),\n                (\"epsilon_scaling\", float),\n            ):\n                name_cl = \"pasha_\" + name\n                v = params.get(name_cl)\n                if v is not None:\n                    rung_system_kwargs[name] = tp(v)\n            if rung_system_kwargs:\n                scheduler_options[\"rung_system_kwargs\"] = rung_system_kwargs\n\n# From notebooks/run_bo_experiments.py\n    if hpo_approach == 'pasha':\n        scheduler = baselines_dict['PASHA'](\n            config_space,\n            max_t=max_t,\n            grace_period=default_params['grace_period'],\n            reduction_factor=reduction_factor,\n            resource_attr=resource_attr,\n            mode=mode,\n            metric=metric,\n            random_seed=random_seed,\n            rung_system_kwargs=rung_system_kwargs)\n    elif hpo_approach == 'pasha-bo':\n        scheduler = HyperbandScheduler(\n            config_space,\n            max_t=max_t,\n            grace_period=default_params['grace_period'],\n            reduction_factor=reduction_factor,\n            resource_attr=resource_attr,\n            mode=mode,\n            searcher='bayesopt',\n            type='pasha',\n            metric=metric,\n            random_seed=random_seed,\n            rung_system_kwargs=rung_system_kwargs)",
        "experimental_info": "PASHA (Progressive ASynchronous HAlving) extends ASHA by dynamically increasing resource allocation only if the ranking of configurations within the top two rungs has not stabilized. It incorporates a 'soft ranking' approach, considering configurations equivalent if their performance difference is below a threshold \\u03b5. This \\u03b5 value can either be manually specified (`--pasha_epsilon`) or automatically estimated. Automatic estimation calculates \\u03b5 as the N-th percentile (default 90th) of performance differences among 'criss-crossing' configurations (those that repeatedly swap ranks across resource levels). The estimation method can be specified via `--pasha_ranking_criterion` (options include 'soft_ranking_std', 'soft_ranking_median_dst', 'soft_ranking_mean_dst', 'soft_ranking_auto'), and an optional `--pasha_epsilon_scaling` factor can be applied to the automatically estimated \\u03b5.\n\nKey configurable parameters include:\n- `pasha_ranking_criterion`: (string) Specifies the strategy for deciding ranking stability and resource increase. Options: 'soft_ranking', 'soft_ranking_std', 'soft_ranking_median_dst', 'soft_ranking_mean_dst', 'soft_ranking_auto'.\n- `pasha_epsilon`: (float) Threshold for soft ranking when `ranking_criterion` is 'soft_ranking'.\n- `pasha_epsilon_scaling`: (float) Scaling factor for automatically estimated epsilon.\n\nExample setting observed: `rung_system_kwargs = {'ranking_criterion': 'soft_ranking_auto', 'epsilon': 0.0}`, where `epsilon=0.0` suggests a placeholder when `soft_ranking_auto` is used to determine epsilon dynamically."
      }
    },
    {
      "title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks",
      "abstract": "Bayesian optimization (BO) is a popular framework to optimize black-box\nfunctions. In many applications, the objective function can be evaluated at\nmultiple fidelities to enable a trade-off between the cost and accuracy. To\nreduce the optimization cost, many multi-fidelity BO methods have been\nproposed. Despite their success, these methods either ignore or over-simplify\nthe strong, complex correlations across the fidelities, and hence can be\ninefficient in estimating the objective function. To address this issue, we\npropose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO)\nthat can flexibly capture all kinds of complicated relationships between the\nfidelities to improve the objective function estimation and hence the\noptimization performance. We use sequential, fidelity-wise Gauss-Hermite\nquadrature and moment-matching to fulfill a mutual information-based\nacquisition function, which is computationally tractable and efficient. We show\nthe advantages of our method in both synthetic benchmark datasets and\nreal-world applications in engineering design.",
      "full_text": "arXiv:2007.03117v4  [cs.LG]  10 Dec 2020 Multi-Fidelity Bayesian Optimization via Deep Neural Networks Shibo Li School of Computing University of Utah Salt Lake City, UT 84112 shibo@cs.utah.edu W ei Xing Scientiﬁc Computing and Imaging Institute University of Utah Salt Lake City, UT 84112 wxing@sci.utah.edu Robert M. Kirby School of Computing University of Utah Salt Lake City, UT 84112 kirby@cs.utah.edu Shandian Zhe School of Computing University of Utah Salt Lake City, UT 84112 zhe@cs.utah.edu Abstract Bayesian optimization (BO) is a popular framework for optim izing black-box functions. In many applications, the objective function ca n be evaluated at mul- tiple ﬁdelities to enable a trade-off between the cost and ac curacy. T o reduce the optimization cost, many multi-ﬁdelity BO methods have b een proposed. De- spite their success, these methods either ignore or over-si mplify the strong, com- plex correlations across the ﬁdelities. While the acquisit ion function is therefore easy and convenient to calculate, these methods can be inefﬁ cient in estimating the objective function. T o address this issue, we propose De ep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can ﬂexibly capture all kinds of complicated relationships between the ﬁdelities t o improve the objective function estimation and hence the optimization performanc e. W e use sequential, ﬁdelity-wise Gauss-Hermite quadrature and moment-matchi ng to compute a mu- tual information based acquisition function in a tractable and highly efﬁcient way. W e show the advantages of our method in both synthetic benchm ark datasets and real-world applications in engineering design. 1 Introduction Bayesian optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a general and powerful ap- proach for optimizing black-box functions. It uses a probab ilistic surrogate model (typically Gaus- sian process (GP) (Rasmussen and Williams, 2006)) to estima te the objective function. By repeat- edly maximizing an acquisition function computed with the i nformation of the surrogate model, BO ﬁnds and queries at new input locations that are closer and cl oser to the optimum; meanwhile the new training examples are incorporated into the surrogate m odel to improve the objective estimation. In practice, many applications allow us to query the objecti ve function at different ﬁdelities, where low ﬁdelity queries are cheap yet inaccurate, and high ﬁdeli ty queries more accurate but costly. For example, in physical simulation (Peherstorfer et al., 2018 ), the computation of an objective ( e.g., the elasticity of a part or energy of a system) often involves solving partial differential equations. Running a numerical solver with coarse meshes gives a quick y et rough result; using dense meshes substantially improves the accuracy but dramatically incr eases the computational cost. The multi- ﬁdelity queries enable us to choose a trade-off between the c ost and accuracy. 34th Conference on Neural Information Processing Systems ( NeurIPS 2020), V ancouver, Canada.Accordingly, to reduce the optimization cost, many multi-ﬁ delity BO methods (Huang et al., 2006; Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; T akeno et al., 2019) have been pro- posed to jointly select the input locations and ﬁdelities to best balance the optimization progress and query cost, i.e., the beneﬁt-cost ratio. Despite their success, these method s often ignore the strong, complex correlations between the function outputs at diffe rent ﬁdelities, and learn an independent GP for each ﬁdelity (Lam et al., 2015; Kandasamy et al., 2016) . Recent works use multi-output GPs to capture the ﬁdelity correlations. However, to avoid intr actable computation of the acquisition function, they have to impose simpliﬁed correlation struct ures. For example, T akeno et al. (2019) assume a linear correlation between the ﬁdelities; Zhang et al. (2017) use kernel convolution to con- struct the cross-covariance function, and have to choose si mple, smooth kernels ( e.g., Gaussian) to ensure a tractable convolution. Therefore, the existing me thods can be inefﬁcient and inaccurate in estimating the objective function, which further lowers th e optimization efﬁciency and increases the cost. T o address these issues, we propose DNN-MFBO, a deep neural n etwork based multi-ﬁdelity Bayesian optimization that is ﬂexible enough to capture all kinds of complex (possibly highly nonlin- ear and nonstationary) relationships between the ﬁdelitie s, and exploit these relationships to jointly estimate the objective function in all the ﬁdelities to impr ove the optimization performance. Speciﬁ- cally, we stack a set of neural networks (NNs) where each NN mo dels one ﬁdelity. In each ﬁdelity, we feed both the original input (to the objective) and output from the previous ﬁdelity into the NN to propagate information throughout and to estimate the com plex relationships across the ﬁdelities. Then, the most challenging part is the calculation of the acq uisition function. For efﬁcient inference and tractable computation, we consider the NN weights in the output layer as random variables and all the other weights as hyper-parameters. W e develop a stoc hastic variational learning algorithm to jointly estimate the posterior of the random weights and h yper-parameters. Next, we sequen- tially perform Gauss-Hermite quadrature and moment matchi ng to approximate the posterior and conditional posterior of the output in each ﬁdelity, based o n which we calculate and optimize an information based acquisition function, which is not only c omputationally tractable and efﬁcient, but also conducts maximum entropy search (W ang and Jegelka, 2017), the state-of-the-art criterion in BO. For evaluation, we examined DNN-MFBO in three benchmark functions and two real-world applica- tions in engineering design that requires physical simulat ions. The results consistently demonstrate that DNN-MFBO can optimize the objective function (in the hi ghest ﬁdelity) more effectively, mean- while with smaller query cost, as compared with state-of-th e-art multi-ﬁdelity and single ﬁdelity BO algorithms. 2 Background Bayesian optimization. T o optimize a black-box objective function f : X → R, BO learns a probabilistic surrogate model to predict the function valu es across the input domain X and quantiﬁes the uncertainty of the predictions. This information is use d to calculate an acquisition function that measures the utility of querying at different input locatio ns, which usually encodes a exploration- exploitation trade-off. By maximizing the acquisition fun ction, BO ﬁnds new input locations at which to query, which are supposed to be closer to the optimum ; meanwhile the new examples are added into the training set to improve the accuracy of the surrogate model. The most commonly used surrogate model is Gaussian process (GP) (Rasmussen an d Williams, 2006). Given the training inputs X = [ x1, . . . , xN ]⊤ and (noisy) outputs y = [ y1, . . . , y N ]⊤, GP assumes the outputs follow a multivariate Gaussian distribution, p(y|X) = N (y|m, K + σ2I) where m are the values of the mean function at the inputs X, K is a kernel matrix on X, [K]ij = k(xi, xj ) (k(·, ·) is the kernel function), and σ2 is the noise variance. The mean function is usually set to the constant function 0 and so m = 0. Due to the multi-variate Gaussian form, given a new input x∗, the posterior distribution of the function output, p ( f(x∗)|x∗, X, y ) is a closed-form conditional Gaussian, and hence is convenient to quantify the uncertainty and calcula te the acquisition function. There are a variety of commonly used acquisition functions, such as expected improvement (EI) (Jones et al., 1998), upper conﬁdent bound (UCB) (Srini vas et al., 2010), entropy search (ES) (Hennig and Schuler, 2012), and predictive entropy sea rch (PES) (Hernández-Lobato et al., 2014). A particularly successful recent addition is the max -value entropy search (MES) (W ang and Jegelka, 2017), which not only enjoys a global util ity measure (like ES and PES), but also is computationally efﬁcient (because it calculates th e entropy of the function output rather than input like in ES/PES). Speciﬁcally, MES maximizes the mutua l information between the function 2value and its maximum f∗ to ﬁnd the next input at which to query, a(x) = I ( f(x), f ∗|D ) = H ( f(x)|D ) − Ep(f∗|D)[H ( f(x)|f∗, D ) ], (1) where I(·, ·) is the mutual information, H(·) the entropy, and D the training examples collected so far. Note that the function values and extremes are consider ed as generated from the posterior in the surrogate model, which includes all the knowledge we have fo r the black-box objective function. Multi-ﬁdelity Bayesian optimization . Many applications allow multi-ﬁdelity queries of the obje c- tive function, {f1(x), . . . , f M (x)}, where the higher (larger) the ﬁdelity m, the more accurate yet costly the query of fm(·). Many studies have extended BO for multi-ﬁdelity settings. For exam- ple, MF-GP-UCB (Kandasamy et al., 2016) starts from the lowe st ﬁdelity ( m = 1 ), and queries the objective at each ﬁdelity until the conﬁdence band excee ds a particular threshold. Despite its effectiveness and theoretical guarantees, MF-GP-UCB lear ns an independent GP surrogate for each ﬁdelity and ignores the strong correlations between the ﬁde lities. Recent works use a multi-output GP to model the ﬁdelity correlations. For example, MF-PES (Z hang et al., 2017) introduces a shared latent function, and uses kernel convolution to derive the c ross-covariance between the ﬁdelities. The most recent work, MF-MES (T akeno et al., 2019) introduces C kernel functions {κc(·, ·)} and, for each ﬁdelity m, C latent features {ωcm}. The covariance function is deﬁned as k ( fm(x), fm′ (x′) ) = ∑ C c=1 (ωcmωcm′ + τcmδmm′ )κc(x, x′), (2) where τcm > 0, δmm′ = 1 if and only if m = m′, and each kernel κc(·, ·) is usually assumed to be stationary, e.g., Gaussian kernel. 3 Multi-Fidelity Modeling with Deep Neural Networks Despite the success of existing multi-ﬁdelity BO methods, they either overlook the strong, complex correlations between different ﬁdelities ( e.g., MF-GP-UCB) or model these correlations with an over-simpliﬁed structure. For example, the convolved GP in MF-PES has to employ simple/smooth kernels (typically Gaussian) for both the latent function a nd convolution operation to obtain an ana- lytical cross-covariance function, which has limited expr essiveness. MF-MES essentially adopts a linear correlation assumption between the ﬁdelities. Acc ording to (2), if we choose each κc as a Gaussian kernel (with amplitude one), we have k ( fm(x), fm′ (x) ) = ω⊤ mωm′ + δmm′ τm where ωm = [ ω1m, . . . , ω Cm ]⊤ and τm = ∑ C c=1 τcm. These correlation structures might be over-simpliﬁed and insufﬁcient to estimate the complicate d relationships between the ﬁdelities ( e.g., highly nonlinear and nonstationary). Hence, they can limit the accuracy of the surrogate model and lower the optimization efﬁciency while increasing the quer y cost. T o address this issue, we use deep neural networks to build a m ulti-ﬁdelity model that is ﬂexible enough to capture all kinds of complicated relationships be tween the ﬁdelities, taking advantage of the relationships to promote the accuracy of the surrogat e model. Speciﬁcally, for each ﬁdelity m > 1, we introduce a neural network (NN) parameterized by {wm, θm}, where wm are the weights in the output layer and θm the weights in all the other layers. Denote the NN input by xm, the output by fm(x) and the noisy observation by ym(x). The model is deﬁned as xm = [ x; fm−1(x)], f m(x) = w⊤ mφθ m (xm), y m(x) = fm(x) + ǫm, (3) where x is the original input to the objective function, φθ m (xm) is the output vector of the second last layer (hence parameterized by θm) which can be viewed as a set of nonlinear basis functions, an d ǫm ∼ N (ǫm|0, σ2 m) is a Gaussian noise. The input xm is obtained by appending the output from the previous ﬁdelity to the original input. Through a series of l inear and nonlinear transformations inside the NN, we obtain the output fm(x). In this way, we digest the information from the lower ﬁdelit ies, and capture the complex relationships between the current a nd previous ﬁdelities by learning a nonlinear mapping fm(x) = h(x, fm−1(x)), where h(·) is fulﬁlled by the NN. When m = 1 , we set xm = x. A graphical representation of our model is given in Fig. 1 of the supplementary material. W e assign a standard normal prior over each wm. Following (Snoek et al., 2015), we con- sider all the remaining NN parameters as hyper-parameters. Given the training set D = {{(xnm, ynm)}Nm n=1}M m=1, the joint probability of our model is p(W, Y|X , Θ , s) = ∏ M m=1 N (wm|0, I) ∏ Nm n=1 N ( ynm|fm(xnm), σ2 m ) , (4) 3where W = {wm}, Θ = {θm}, s = [ σ2 1, . . . , σ 2 M ]⊤, and X , Y are the inputs and outputs in D. In order to obtain the posterior distribution of our model (w hich is in turn used to compute the ac- quisition function), we develop a stochastic variational l earning algorithm. Speciﬁcally, for each wm, we introduce a multivariate Gaussian posterior, q(wm) = N (wm|µm, Σ m). W e further pa- rameterize Σ m with its Cholesky decomposition to ensure the positive deﬁn iteness, Σ m = LmL⊤ m where Lm is a lower triangular matrix. W e assume q(W) = ∏ M m=1 q(wm), and construct a varia- tional model evidence lower bound (ELBO), L ( q(W), Θ , s ) = Eq[log(p(W, Y|X , Θ , s)/q(W))]. W e then maximize the ELBO to jointly estimate the variationa l posterior q(W) and all the other hyper-parameters. The ELBO is analytically intracta ble, and we use the reparameterization trick (Kingma and W elling, 2013) to conduct efﬁcient stocha stic optimization. The details are given in the supplementary material (Sec. 3). 4 Multi-Fidelity Optimization with Max-V alue Entropy Search W e now consider an acquisition function to select both the ﬁd elities and input locations at which we query during optimization. Following (T akeno et al., 2019) , we deﬁne the acquisition function as a(x, m) = 1 λm I (f∗, fm(x)|D) = 1 λm ( H ( fm(x)|D ) − Ep(f∗|D) [ H ( fm(x)|f∗, D )]) (5) where λm > 0 is the cost of querying with ﬁdelity m. In each step, we maximize the acquisition function to ﬁnd a pair of input location and ﬁdelity that prov ides the largest beneﬁt-cost ratio. However, given the model inference result, i.e., p(W|D) ≈ q(W), a critical challenge is to compute the posterior distribution of the output in each ﬁdelity, p(fm(x)|D), and use them to compute the acquisition function. Due to the nonlinear coupling of the o utputs in different ﬁdelities (see (3)), the computation is analytically intractable. T o address th is issue, we conduct ﬁdelity-wise moment matching and Gauss-Hermite quadrature to approximate each p(fm(x)|D) as a Gaussian distribu- tion. 4.1 Computing Output Posteriors Speciﬁcally, we ﬁrst assume that we have obtained the posterior of the output for ﬁdelity m − 1, p ( fm−1(x)|D ) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . For convenience, we slightly abuse the notation and use fm−1 and fm to denote fm−1(x) and fm(x), respectively. Now we consider calculat- ing p(fm|D). According to (3), we have fm = w⊤ mφθ m ([x; fm−1]). Based on our variational posterior q(wm) = N (wm|µm, LmL⊤ m), we can immediately derive the conditional posterior p(fm|fm−1, D) = N ( fm|u(fm−1, x), γ(fm−1, x) ) where u(fm−1, x) = µ⊤ mφθ m ([x; fm−1]) and γ(fm−1, x) = ∥L⊤ mφθ m ([x; fm−1])∥2. Here ∥ · ∥ 2 is the square norm. W e can thereby read out the ﬁrst and second conditional moments, E[fm|fm−1, D] = u(fm−1, x), E[f2 m|fm−1, D] = γ(fm−1, x) + u(fm−1, x)2. (6) T o obtain the moments, we need to take the expectation of the c onditional moments w .r.t p(fm−1|D) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . While the conditional moments are nonlinear to fm−1 and their expectation is not analytical, we can use Gauss-He rmite quadrature to give an accu- rate, closed-form approximation, E[fm|D] = Ep(fm−1|D)E[fm|fm−1, D] ≈ ∑ k gk · u(tk, x), E[f2 m|D] = Ep(fm−1|D)E[f2 m|fm−1, D] ≈ ∑ k gk · [γ(tk, x) + u(tk, x)2], (7) where {gk} and {tk} are quadrature weights and nodes, respectively. Note that e ach node tk is determined by αm−1(x) and ηm−1(x). W e then use these moments to construct a Gaus- sian posterior approximation, p(fm|D) ≈ N ( fm|αm(x), ηm(x) ) where αm(x) = E[fm|D] and ηm(x) = E[f2 m|D] − E[fm|D]2. This is called moment matching, which is widely used and ver y successful in approximate Bayesian inference, such as expe ctation-propagation (Minka, 2001). One may concern if the quadrature will give a positive variance. This is guaranteed by the follow lemma. Lemma 4.1. As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7), is positive. 4The proof is given in the supplementary material. Following the same procedure, we can compute the posterior of the output in ﬁdelity m + 1. Note that when m = 1 , we do not need quadrature because the input of the NN is the same as the original input, not inclu ding other NN outputs. Hence, we can derive the Gaussian posterior outright from q(w1) — p(f1(x)|D) = N ( f1(x)|α1(x), η1(x) ) , where α1(x) = µ⊤ 1φθ 1 (x) and η1(x) = ∥L⊤ 1φθ 1 (x)∥2. 4.2 Computing Acquisition Function Given the posterior of the NN output in each ﬁdelity,p ( fm(x)|D) ≈ N (fm(x)|αm(x), ηm(x) ) (1 ≤ m ≤ M), we consider how to compute the acquisition function (5). Du e to the Gaussian posterior, the ﬁrst entropy term is straightforward, H ( fm(x)|D ) = 1 2 log ( 2πeηm(x) ) . The second term — a conditional entropy, however, is intractable. Hence, we f ollow (W ang and Jegelka, 2017) to use a Monte-Carlo approximation, Ep(f∗|D)[H ( fm(x)|f∗, D ) ] ≈ 1 |F| ∑ f∗∈F∗ H ( fm(x)|f∗, D ) , where F∗ are a collection of independent samples of the function maxi mums based on the posterior distribution of our model. T o obtain a sample of the function maximum, we ﬁrst generate a posterior sample for each wm, according to q(wm) = N (wm|µm, LmL⊤ m). W e replace each wm by their sample in calculating fM (x) so as to obtain a posterior sample of the objective function. W e then maximize this sample function to obtain one instance of f∗. W e use L-BFGS (Liu and Nocedal, 1989) for optimization. Given f∗, the computation of H ( fm(x)|f∗, D ) = H ( fm(x)| max fM (x) = f∗, D ) is still in- tractable. W e then follow (W ang and Jegelka, 2017) to calcul ate H ( fm(x)|fM (x) ≤ f∗, D ) instead as a reasonable approximation. For m = M, the entropy is based on a truncated Gaussian distribu- tion, p(fM (x)|fM (x) ≤ f∗, D) ∝ N ( fM (x)|αM (x), ηM (x) ) 1(fM (x) ≤ f∗) where 1(·) is the indicator function, and is given by H ( fm(x)|fM (x) ≤ f∗, D ) = log ( √ 2πeηM (x)Φ( β) ) − β · N (β|0, 1)/ ( 2Φ( β) ) , (8) where Φ( ·) is the cumulative density function (CDF) of the standard nor mal distribution, and β =( f∗ − αM (x) ) / √ ηM (x). When m < M , the entropy is based on the conditional distribution p(fm(x)|fM (x) ≤ f∗, D) = 1 Z · p ( fm(x)|D ) p(fM (x) ≤ f∗|fm(x), D) ≈ 1 Z · N ( fm(x)|αm(x), ηm(x) ) p(fM (x) ≤ f∗|fm(x), D). (9) where Z is the normalizer. T o obtain p(fM (x) ≤ f∗|fm(x), D), we ﬁrst consider how to compute p(fM (x)|fm(x), D). According to (3), it is trivial to derive that p(fm+1(x)|fm(x), D) = N ( fm+1|ˆαm+1(x, fm), ˆηm+1(x, fm) ) , where ˆαm+1(x, fm) = µ⊤ m+1φθ m+1 ([x; fm]) and ˆηm+1(x, fm) = ∥L⊤ m+1φθ m+1 ([x; fm])∥2. Note that we again use fm+1 and fm to denote fm+1(x) and fm(x) for convenience. Next, we follow the same method as in Section 4.1 to sequentially obtain the c onditional posterior for each higher ﬁdelity, p(fm+k|fm, D)(1 < k ≤ M − m). In more detail, we ﬁrst base on q(wm+k) to derive the conditional moments E(fm+k|fm+k−1, fm, D) and E(f2 m+k|fm+k−1, fm, D). They are calculated in the same way as in (6), because fm+k are independent to fm conditioned on fm+k−1. Then we take the expectation of the conditional moments w .r.t p(fm+k−1|fm, D) (that is Gaussian) to obtain E(fm+k|fm, D) and E(f2 m+k|fm, D). This again can be done by Gauss-Hermite quadrature. Finally, we use these moments to construct a Gaussian approx imation to the conditional posterior, p(fm+k|fm, D) ≈ N ( fm+k|ˆαm+k(x, fm), ˆηm+k(x, fm) ) , (10) where ˆαm+k(x, fm) = E(fm+k|fm, D) and ˆηm+k(x, fm) = E(f2 m+k|fm, D) − E(fm+k|fm, D)2. According to Lemma 4.1, we guarantee ˆηm+k(x, fm) > 0. Now we can obtain p(fm(x)|fM (x) ≤ f∗, D) ≈ 1 Z · N ( fm|αm(x), ηm(x) ) Φ ( f∗ − ˆαM (x, fm)√ ˆηM (x, fm) ) . (11) 5In order to compute the entropy analytically, we use moment m atching again to approximate this distribution as a Gaussian distribution. T o this end, w e use Gauss-Hermite quadrature to compute three integrals, Z = ∫ R(fm) · N ( fm|αm(x), ηm(x) ) dfm, Z1 = ∫ fmR(fm) · N ( fm|αm(x), ηm(x) ) dfm, and Z2 = ∫ f2 mR(fm) · N ( fm|αm(x), ηm(x) ) dfm, where R(fm) = Φ ( (f∗ − ˆαM (x, fm))/ √ ˆηM (x, fm) ) . Then we can obtain E[fm|fM ≤ f∗, D] = Z1/Z and E[f2 m|fM ≤ f∗, D] = Z2/Z, based on which we approximate p(fm(x)|fM (x) ≤ f∗, D) ≈ N ( fm|Z1/Z, Z2/Z − Z2 1 /Z2) . (12) Following the same idea to prove Lemma 4.1, we can show that th e variance is non-negative. See the details in the supplementary material (Sec. 5). With the Gaussian form, we can analytically compute the entropy, H(fm(x)|fM (x) ≤ f∗, D) = 1 2 log ( 2πe(Z2/Z − Z2 1 /Z2) ) . Although our calculation of the acquisition function is qui te complex, due to the analytical form, we can use automatic differentiation libraries (Baydin et al. , 2017), to compute the gradient efﬁciently and robustly for optimization. In our experiments, we used T ensorFlow (Abadi et al., 2016) and L-BFGS to maximize the acquisition function to ﬁnd the ﬁdeli ty and input location we query at in the next step. Our multi-ﬁdelity Bayesian optimization alg orithm is summarized in Algorithm 1. Algorithm 1 DNN-MFBO ( D, M, T , {λm}M m=1 ) 1: Learn the DNN-based multi-ﬁdelity model (4) on Dwith stochastic variational learning. 2: for t = 1, . . . , T do 3: Generate F∗ from the variational posterior q(W) and the NN output at ﬁdelity M, i.e., fM (x) 4: (xt, m t) = argmaxx∈X ,1≤ m≤ M MutualInfo(x, m, λ m, F∗ , D, M ) 5: D←D∪{ (xt, m t)} 6: Re-train the DNN-based multi-ﬁdelity model on D 7: end for Algorithm 2 MutualInfo(x, m, λm, F∗, D, M) 1: Compute each p(fm(x)|D) ≈N ( fm|α m(x), η m(x) ) (Sec. 4.1) 2: H0 ← 1 2 log(2πeη m(x)), H1 ← 0 3: for f∗ ∈F ∗ do 4: if m = M then 5: Use (8) to compute H(fm|fM ≤ f∗ , D) and add it to H1 6: else 7: Compute p(fm(x)|fM (x), D) following (10) and p(fm(x)|fM (x) ≤ f∗ , D) with (12) 8: H1 ← H1 + 1 2 log ( 2πe (Z2/Z − Z2 1 /Z 2) ) 9: end if 10: end for 11: return (H0 − H1/ |F∗ |)/λ m 5 Related W ork Most surrogate models used in Bayesian optimization (BO) (Mockus, 2012; Snoek et al., 2012) are based on Gaussian processes (GPs) (Rasmussen and Williams, 2006), partly because their closed- form posteriors (Gaussian) are convenient to quantify the u ncertainty and calculate the acquisition functions. However, GPs are known to be costly for training, and the exact inference takes O(N3) time complexity ( N is the number of samples). Recently, Snoek et al. (2015) show ed deep neural networks (NNs) can also be used in BO and performs very well. T he training of NNs are much more efﬁcient ( O(N)). T o conveniently quantify the uncertainty, Snoek et al. (2 015) consider the NN weights in the output layer as random variables and all the ot her weights as hyper-parameters (like the kernel parameters in GPs). They ﬁrst obtain a point estim ation of the hyper-parameters (typically through stochastic training). Then they ﬁx the hyper-param eters and compute the posterior distribu- tion of the random weights (in the last layer) and NN output — t his can be viewed as the inference for Bayesian linear regression. In our multi-ﬁdelity model , we also only consider the NN weights in the output layer of each ﬁdelity as random variables. Howe ver, we jointly estimate the hyper- parameters and posterior distribution of the random weight s. Since the NN outputs in successive ﬁdelities are coupled non-linearly, we use the variational estimation framework (W ainwright et al., 2008). 6Many multi-ﬁdelity BO algorithms have been proposed. For ex ample, Huang et al. (2006); Lam et al. (2015); Picheny et al. (2013) augmented the standa rd EI for the multi-ﬁdelity settings. Kandasamy et al. (2016, 2017) extended GP upper conﬁdence bo und (GP-UCB) (Srinivas et al., 2010). Poloczek et al. (2017); Wu and Frazier (2017) develop ed multi-ﬁdelity BO with knowledge gradients (Frazier et al., 2008). EI is a local measure of the utility and UCB requires us to explicitly tune the exploit-exploration trade-off. The recent works a lso extend the information-based acqui- sition functions to enjoy a global utility for multi-ﬁdelit y optimization, e.g., (Swersky et al., 2013; Klein et al., 2017) using entropy search (ES), (Zhang et al., 2017; McLeod et al., 2017) (PES) using predictive entropy search (PES), and (Song et al., 2019; T ak eno et al., 2019) using max-value en- tropy search (MES). Note that ES and PES are computationally more expensive than MES because the former calculate the entropy of the input (vector) and la tter the output scalar. Despite the great success of the existing methods, they either ignore or overs implify the complex correlations across the ﬁdelities, and hence might hurt the accuracy of the surro gate model and further the optimiza- tion performance. For example, Picheny et al. (2013); Lam et al. (2015); Kandasamy et al. (2016); Poloczek et al. (2017) train an independent GP for each ﬁdeli ty; Song et al. (2019) combined all the examples indiscriminately to train a single GP; Huang et al. (2006); T akeno et al. (2019) assume a linear correlation structure between ﬁdelities, and Zhang et al. (2017) used the convolution opera- tion to construct the covariance and so the involved kernels have to be simple and smooth enough (yet less expressive) to obtain an analytical form. T o overc ome these limitations, we propose an NN-based multi-ﬁdelity model, which is ﬂexible enough to ca pture arbitrarily complex relation- ships between the ﬁdelities and to promote the performance o f the surrogate model. Recently, a NN-based multi-task model (Perrone et al., 2018) was also de veloped for BO and hyper-parameter transfer learning. The model uses an NN to construct a shared feature map ( i.e., bases) across the tasks, and generates the output of each task by a linear combi nation of the latent features. While this model can also be used for multi-ﬁdelity BO (each task co rresponds to one ﬁdelity), it views each ﬁdelity as symmetric and does not reﬂect the monotonici ty of function accuracy/importance along with the ﬁdelities. More important, the model does not capture the correlation between ﬁdeli- ties — given the shared bases, different ﬁdelities are assum ed to be independent. Finally, while a few algorithms deal with continuous ﬁdelities, e.g., (Kandasamy et al., 2017; McLeod et al., 2017; Wu and Frazier, 2017), we focus on discrete ﬁdelities in this work. 6 Experiment 6.1 Synthetic Benchmarks W e ﬁrst evaluated DNN-MFBO in three popular synthetic bench mark tasks. (1) Branin func- tion (Forrester et al., 2008; Perdikaris et al., 2017) with t hree ﬁdelities. The input is two dimensional and ranges from [−5, 10] × [0, 15]. (2) P ark1function (Park, 1991) with two ﬁdelities. The input is four dimensional and each dimension is in [0, 1]. (3) Levy function (Laguna and Martí, 2005), having three ﬁdelities and two dimensional inputs. The doma in is [−10, 10] × [−10, 10]. For each objective function, between ﬁdelities can be nonlinear and /or nonstationary transformations. The detailed deﬁnitions are given in the supplementary materia l (Sec. 1). Competing Methods. W e compared with the following popular and state-of-the-ar t multi- ﬁdelity BO algorithms: (1) Multi-Fidelity Sequential Krig ing (MF-SKO) (Huang et al., 2006) that models the function of the current ﬁdelity as the functi on of the previous ﬁdelity plus a GP , (2) MF-GP-UCB (Kandasamy et al., 2016), (3) Multi- Fidelity Predictive Entropy Search (MF-PES) (Zhang et al., 2017) and (4) Multi-Fidelity Maximum Entropy Search (MF- MES) (T akeno et al., 2019). These algorithms extend the stan dard BO with EI, UCB, PES and MES principles respectively. W e also compared with (5) mult i-task NN based BO (MTNN-BO) by Perrone et al. (2018), where a set of latent bases (generat ed by an NN) are shared across the tasks, and the output of each task ( i.e., ﬁdelity) is predicted by a linear combination of the bases. W e tested the single ﬁdelity BO with MES, named as (6) SF-MES (W a ng and Jegelka, 2017). SF-MES only queries the objective at the highest ﬁdelity. Settings and Results.W e implemented our method and MTNN-BO with T ensorFlow . W e used the original Matlab implementatio n for MF-GP-UCB ( https://github.com/kirthevasank/mf-gp-ucb), MF-PES ( https://github.com/YehongZ/MixedTypeBO) and SF- MES ( https://github.com/zi-w/Max-value-Entropy-Search/ ), and 7500 1000 1500 2000 2500 Total Cos  10−1 100 101 102 103 Simple Regre  DNN -MFBO MF-MES MF-PES MF-SKO MF-GP-UCB SF-MES MTNN-BO (a) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−4 10−2 100 (b) P ark1 500 1000 1500 2000 2500 Total Cost 10 0 10 1 10 2 (c) Levy 100 200 300 400 500 Total Cost 170 180 190190 200 210 220Queried Maximum (d) V ibration Plate 500 1000 1500 2000 2500 Total Cost 10 −2 10 −1 10 0 10 1 10 2 10 3 Inference Regret (e) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−3 10−1 100 101 (f) P ark1 500 1000 1500 2000 2500 Total Cost 10−2 10−1 100 101 102 (g) Levy 100 200 300 400 500 Total cost 1.05 1.10 1.15 1.20 1.25 1.3 × 100 1.35 × 100 1.4 × 100 Queried Minimum (h) Thermal Conductor Figure 1: Simple and Inference regrets on three synthetic benchmark t asks (a-c, e-g) and the optimum queried function values (d, h) along with the query cost. Python/Numpy implementation for MF-MES. MF-SKO was implemented with Python as well. W e used the default settings in their implementations . SF-MES and MF-GP-UCB used the Squared Exponential (SE) kernel. MF-PES used the Automa tic Relevance Determination (ARD) kernel. MF-MES and MF-SKO used the Radial Basis (RBF) k ernel (within each ﬁdelity). For DNN-MFBO and MTNN-BO, we used ReLU activation. T o identi fy the architecture of the neural network in each ﬁdelity and learning rate, we ﬁrst ran the AutoML tool SMAC3 (https://github.com/automl/SMAC3) on the initial training dataset (we randomly split the data into half for training and the other half for test, an d repeated multiple times to obtain a cross-validation accuracy to guide the search) and then man ually tuned these hyper-parameters. The depth and width of each network were chosen from [2, 12] and [32, 512], and the learning rate [10−5, 10−1]. W e used ADAM (Kingma and Ba, 2014) for stochastic training. The number of epochs was set to 5, 000, which is enough for convergence. T o optimize the acquisiti on function, MF-MES and MF-PES ﬁrst run a global optimization algorithm D IRECT (Jones et al., 1993; Gablonsky et al., 2001) and then use the results as the initia lization to run L-BFGS. SF-MES uses a grid search ﬁrst and then runs L-BFGS. DNN-MFBO and MTNN-BO d irectly use L-BFGS with a random initialization. T o obtain the initial training poin ts, we randomly query in each ﬁdelity. For Branin and Levy, we generated 20, 20 and 2 training samples for the ﬁrst, second and third ﬁdelity, respectively. For P ark1, we generated 5 and 2 examples for the ﬁrst and second ﬁdelity. The query costs is (λ1, λ2, λ3) = (1 , 10, 100). W e examined the simple regret (SR) and inference regret (IR ). SR is deﬁned as the difference between the global optimum and the best queried function value so far: maxx∈X fM (x) − maxi∈{i|i∈[t],mi=M} fM (xi); IR is the difference between the global optimum and the optimum estimated by the surrogate model: maxx∈X fM (x) − maxx∈X ˆfM (x) where ˆfM (·) is the estimated objective. W e repeated the experiment for ﬁ ve times, and report on average how the simple and inference regrets vary along with the query cost in Fig. 1 (a-c, e-g). W e also show the standard error bars. As we can see, in all the t hree tasks, DNN-MFBO achieves the best regrets with much smaller or comparable querying co sts. The best regrets obtained by our method are much smaller (often orders of magnitude) than the baselines. In particular, DNN-MFBO almost achieved the global optimum after querying one point (IR < 10−6) (Fig. 1f). These results demonstrate our DNN based surrogate model is more ac curate in estimating the objective. Furthermore, our method spends less or comparable cost to ac hieve the best regrets, showing a much better beneﬁt/cost ratio. 6.2 Real-W orld Applications in Engineering Design Mechanical Plate Vibration Design.W e aim to optimize three material properties, Y oung’s modu- lus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]), to maximize the fourth vibration mode frequency of a 3-D simp ly supported, square, elastic plate, of size 10 × 10 × 1. T o evaluate the frequency, we need to run a numerical solver on the discretized 8DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (a) Branin DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200 250Time (seconds) (b) P ark1 DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (c) Levy DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (d) V ibration Plate DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (e) Heat Conductor Figure 2: The average query time on three synthetic tasks (a- c) and two real-world applications (d-e). plate. W e considered two ﬁdelities, one with a coarse mesh an d the other a dense mesh. The details about the settings of the solvers are provided the supplemen tary document. Thermal Conductor Design. Given the property of a particular thermal conductor, our go al is to optimize the shape of the central hole where we install/ﬁx th e conductor to make the heat conduction (from left to right) to be as as fast as possible. The shape of t he hole (an ellipse) is described by three parameters: x-radius, y-radius and angle. W e used the time to reach 70 degrees as the objective function value and we want to minimize the objective. W e need to run numerical solvers to calculate the objective. W e considered two ﬁdelities. The details are given in the supplementary material. For both problems, we randomly queried at 20 and 5 inputs in th e low and high ﬁdelities respectively, at the beginning. The query cost is (λ1, λ2) = (1 , 10). W e then ran each algorithm until convergence. W e repeated the experiments for ﬁve times. Since we do not kno w the ground-truth of the global optimum, we report how the average of the best function value s queried improves along with the cost. The results are shown in Fig. 1d and h. As we can see, in bo th applications, DNN-MFBO reaches the maximum/minimum function values with a smaller cost than all the competing methods, which is consistent with results in the synthetic benchmark tasks. Finally, we examined the average query time of each multi-ﬁd elity BO method, which is spent in calculating and optimizing the acquisition function to ﬁnd new inputs and ﬁdelities to query at in each step. For a fair comparison, we ran all the methods on a Li nux workstation with a 16-core Intel(R) Xeon(R) CPU E5-2670 and 16GB RAM. As shown in Fig. 2, DNN-MFBO spends much less time than MF-MES and MF-PES that are based on multi-outp ut GPs, and the speed of DNN- MFBO is close or comparable to MF-GP-UCB and MF-SKO, which us e independent and additive GPs for each ﬁdelity, respectively. On average, DNN-MFBO ac hieves 25x and 60x speedup over MF-MES and MF-PES. One reason might be that DNN-MFBO simply a dopts a random initializa- tion for L-BFGS rather than runs an expensive global optimiz ation (so does MTNN-BO). However, as we can see from Fig. 1, DNN-MFBO still obtains new input and ﬁdelities that achieve much better beneﬁt/cost ratio. On the other hand, the close speed to MF-GP-UCB and MF-SKO also demonstrate that our method is efﬁcient in acquisition func tion calculation, despite its seemingly complex approximations. 7 Conclusion W e have presented DNN-MFBO, a deep neural network based mult i-ﬁdelity Bayesian optimization algorithm. Our DNN surrogate model is ﬂexible enough to capt ure the strong and complicated relationships between ﬁdelities and promote objective est imation. Our information based acquisition function not only enjoys a global utility measure, but also i s computationally tractable and efﬁcient. Acknowledgments This work has been supported by DARP A TRADES A ward HR0011-17 -2-0016 and NSF IIS- 1910983. Broader Impact This work can be used in a variety of engineering design probl ems that involve intensive computa- tion, e.g., ﬁnite elements or differences. Hence, the work has potentia l positive impacts in the society if it is used to design passenger aircrafts, biomedical devi ces, automobiles, and all the other devices 9or machines that can beneﬁt human lives. At the same time, thi s work may have some negative consequences if it is used to design weapons or weapon parts. References Abadi, M., Barham, P ., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. (2016). T ensorﬂow: A system for large-s cale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) , pages 265–283. Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595–5637. Forrester, A., Sobester, A., and Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons. Frazier, P . I., Powell, W . B., and Dayanik, S. (2008). A knowl edge-gradient policy for sequential information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439. Gablonsky, J. M. et al. (2001). Modiﬁcations of the DIRECT Algorithm. PhD thesis. Hennig, P . and Schuler, C. J. (2012). Entropy search for info rmation-efﬁcient global optimization. Journal of Machine Learning Research, 13(Jun):1809–1837. Hernández-Lobato, J. M., Hoffman, M. W ., and Ghahramani, Z. (2014). Predictive entropy search for efﬁcient global optimization of black-box functions. I n Advances in neural information processing systems, pages 918–926. Huang, D., Allen, T . T ., Notz, W . I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382. Incropera, F . P ., Lavine, A. S., Bergman, T . L., and DeWitt, D . P . (2007). Fundamentals of heat and mass transfer. Wiley. Jones, D. R., Perttunen, C. D., and Stuckman, B. E. (1993). Li pschitzian optimization without the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157–181. Jones, D. R., Schonlau, M., and W elch, W . J. (1998). Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492. Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., a nd Póczos, B. (2016). Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems, pages 992–1000. Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. (2017). Multi-ﬁdelity bayesian optimi- sation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1799–1808. JMLR. org. Kingma, D. P . and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kingma, D. P . and W elling, M. (2013). Auto-encoding variati onal bayes. arXiv preprint arXiv:1312.6114. Klein, A., Falkner, S., Bartels, S., Hennig, P ., and Hutter, F . (2017). Fast bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536. Laguna, M. and Martí, R. (2005). Experimental testing of adv anced scatter search designs for global optimization of multimodal functions. Journal of Global Optimization, 33(2):235–255. Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdel ity optimization using statistical surrogate modeling for non-hierarchical information sour ces. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, page 0143. 10Liu, D. C. and Nocedal, J. (1989). On the limited memory bfgs m ethod for large scale optimization. Mathematical programming, 45(1-3):503–528. McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practi cal bayesian optimization for variable cost objectives. arXiv preprint arXiv:1703.04335. Minka, T . P . (2001). Expectation propagation for approxima te bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence, pages 362–369. Mockus, J. (2012). Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media. Mockus, J., Tiesis, V ., and Zilinskas, A. (1978). The applic ation of Bayesian methods for seeking the extremum. T owards global optimization, 2(117-129):2. Park, J. S. (1991). Tuning complex computer codes to data and optimal designs. Peherstorfer, B., Willcox, K., and Gunzburger, M. (2018). S urvey of multiﬁdelity methods in uncer- tainty propagation, inference, and optimization. Siam Review, 60(3):550–591. Perdikaris, P ., Raissi, M., Damianou, A., Lawrence, N., and Karniadakis, G. E. (2017). Nonlinear in- formation fusion algorithms for data-efﬁcient multi-ﬁdel ity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751. Perrone, V ., Jenatton, R., Seeger, M. W ., and Archambeau, C. (2018). Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855. Picheny, V ., Ginsbourger, D., Richet, Y ., and Caplin, G. (20 13). Quantile-based optimization of noisy computer experiments with tunable precision. T echnometrics, 55(1):2–13. Poloczek, M., W ang, J., and Frazier, P . (2017). Multi-infor mation source optimization. In Advances in Neural Information Processing Systems, pages 4288–4298. Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. Snoek, J., Larochelle, H., and Adams, R. P . (2012). Practica l bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Su ndaram, N., Patwary, M., Prabhat, M., and Adams, R. (2015). Scalable bayesian optimization us ing deep neural networks. In International conference on machine learning, pages 2171–2180. Song, J., Chen, Y ., and Y ue, Y . (2019). A general framework fo r multi-ﬁdelity bayesian optimization with gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010). Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1015–1022. Swersky, K., Snoek, J., and Adams, R. P . (2013). Multi-task b ayesian optimization. In Advances in neural information processing systems, pages 2004–2012. T akeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T ., Shiga, M., T akeuchi, I., and Karasuyama, M. (2019). Multi-ﬁdelity bayesian optimization with max-val ue entropy search. arXiv preprint arXiv:1901.08275. W ainwright, M. J., Jordan, M. I., et al. (2008). Graphical mo dels, exponential families, and varia- tional inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305. W ang, Z. and Jegelka, S. (2017). Max-value entropy search fo r efﬁcient bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning-V olume 70 , pages 3627– 3635. JMLR. org. 11Wu, J. and Frazier, P . I. (2017). Continuous-ﬁdelity bayesi an optimization with knowledge gradient. In NIPS W orkshop on Bayesian Optimization. Zhang, Y ., Hoang, T . N., Low , B. K. H., and Kankanhalli, M. (20 17). Information-based multi- ﬁdelity bayesian optimization. In NIPS W orkshop on Bayesian Optimization. Zienkiewicz, O. C., T aylor, R. L., Zienkiewicz, O. C., and T a ylor, R. L. (1977). The ﬁnite element method, volume 36. McGraw-hill London. 12Supplementary Material . . .x x x f1(x) f2(x) fM (x) Figure 3: Graphical representation of the DNN based multi-ﬁdelity su rrogate model. The output in each ﬁdelity fm(x) (1 ≤ m ≤ M) is fulﬁlled by a (deep) neural network. 1 Deﬁnitions of Synthetic Benchmark Functions In the experiments, we used three synthetic benchmark tasksto evaluate our method. The deﬁnitions of the objective functions are given as follows. 1.1 Branin Function The input is two dimensional, x = [ x1, x2] ∈ [−5, 10] × [0, 15]. W e have three ﬁdelities to query the function, which, from high to low , are given by f3(x) = − ( −1.275x2 1 π2 + 5x1 π + x2 − 6 ) 2 − ( 10 − 5 4π ) cos(x1) − 10, f2(x) = −10 √ −f3(x − 2) − 2(x1 − 0.5) + 3(3 x2 − 1) + 1 , f1(x) = −f2 ( 1.2(x + 2) ) + 3x2 − 1. (13) W e can see that between ﬁdelities are nonlinear transformat ions and non-uniform scaling and shifts. The global maximum is -0.3979 at (−π, 12.275), (π, 2.275) and (9.425, 2.475). 1.2 Park1 Function The input is four dimensional, x = [ x1, x2, x3, x4] ∈ [0, 1]4. W e have two ﬁdelities, f2(x) = x1 2 [ √ 1 + ( x2 + x2 3)x4 x2 1 − 1 ] + (x1 + 3x4) exp[1 + sin( x3)], f1(x) = [ 1 + sin(x1) 10 ] f2(x) − 2x1 + x2 2+ x2 3+ 0.5. (14) The global maximum is at 25.5893 at (1.0, 1.0, 1.0, 1.0). 1.3 Levy Function The input is two dimensional, x = [ x1, x2] ∈ [−10, 10]2. The query has three ﬁdelities, f3(x) = − sin2(3πx1) − (x1 − 1)2[1 + sin 2(3πx2)] − (x2 − 1)2[1 + sin 2(2πx2)], f2(x) = − exp(0.1 · √ −f3(x)) − 0.1 · √ 1 + f2 3 (x), f1(x) = − √ 1 + f2 3 (x). (15) The global maximum is 0.0 at (1.0, 1.0). 2 Details of Real-W orld Applications 2.1 Mechanical Plate Vibration Design In this application, we want to make a 3-D simply supported, square, elastic plate, of size 10×10×1, as shown in Fig. 4. The goal is to ﬁnd materials that can maximi ze the fourth vibration mode 13frequency (so as to avoid resonance with other parts which ca uses damages). The materials are parameterized by three properties, Y oung’s modulus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]). T o compute the frequency, we discretize the plate with quadr atic tetrahedral elements (see Fig. 4). W e consider two ﬁdelities. The low-ﬁdelity solution is o btained from setting a maximum mesh edge length to 1.2, while the high-ﬁdelity 0.6. W e then use the ﬁnite ﬁnite element method (Zienkiewicz et al., 1977) to solve for the ﬁrst 4th vibratio n mode and compute the frequency as our objective. Figure 4: The plate discretized with quadratic tetrahedral elements (the maximum mesh edge length is 1. 2). 2.2 Thermal Conductor Design In the second application, we consider the design of a thermal conductor, shown in Fig. 5a. The heat source is on the left, where the temperature is zero at th e beginning and ramps to 100 degrees in 0.5 seconds. The heat runs through the conductor to the right end . The size and properties of the conductor are ﬁxed: the thermal conductivity and mass densi ty are both 1. W e need to bore a hole in the centre to install the conductor. The edges on the top, bot tom and inside the hole are all insulated, i.e., no heat is transferred across these edges. Note that the size and the angle of the hole determine the speed of the heat transfusion. The hole in general is an el lipse, described by three parameters, x-radius, y-radius and angle. The goal is to make the heat con duction (from left to right) as fast as possible. Hence, we use the time to reach 70 degrees on the r ight end as the objective function value. T o compute the time, we discretize the conductor with quadratic tetrahedral elements, and apply the ﬁnite element methods to solve a transient heat tra nsfer problem (Incropera et al., 2007) to obtain a response heat curve on the right edge. An example is g iven in Fig. 5b. The response curve is a function of time, from which we can calculate when the tem perature reaches 70 degrees. W e consider queries of two ﬁdelities. The low ﬁdelity queries a re computed with the maximum mesh edge length being 0.8 in solving the heat transfer problem; t he high ﬁdelity queries are computed with the maximum mesh edge length being 0.2. 3 Details of Stochastic V ariational Learning W e develop a stochastic variational learning algorithm to j ointly estimate the posterior of W = {wm} — the NN weights in the output layer in each ﬁdelity, and the hy perparameters, including all the other NN weights Θ = {θm} and noise variance s = [ σ2 1 , . . . , σ 2 M ]⊤. T o this end, we assume q(W) = ∏ M m=1 q(wm) where each q(wm) = N (wm|µm, Σ m). W e parameterize Σ m with its Cholesky decomposition to ensure the positive deﬁniteness , Σ m = LmL⊤ mwhere Lm is a lower triangular matrix. W e then construct a variational model ev idence lower bound (ELBO) from the 14-0.5 0 0.5 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 88 90 92 94 96 98 100 (a) Conductor 0 1 2 3 4 5 Time (seconds) -20 0 20 40 60 80 100Temperature (degrees-Celsius) (b) Heat Response Curve Figure 5: The thermal conductor with one transient heat solution (a), and the heat responsive curve on the right edge (b). The white triangles in (a) are the ﬁnite eleme nts used to discretize the conductor to compute the solution. joint probability of our model (see (4) of the main paper), L ( q(W), Θ , s ) = Eq [ log(p(W, Y|X , Θ , s) q(W) ] = − M∑ m=1 KL ( q(wm)∥p(wm) ) + M∑ m=1 Nm∑ n=1 Eq [ log ( N (ynm|fm(xnm), σ2 m) )] , (16) where p(wm) = N (wm|0, I) and KL (·∥·) is the Kullback Leibler divergence. W e maximize L to estimate q(W), Θ and s jointly. However, since the NN outputs fm(·) in each ﬁdelity are coupled in a highly nonlinear way (see (3) of the main paper), the expect ation terms in L is analytical intractable. T o address this issue, we apply stochastic optimization. Sp eciﬁcally, we use the reparameterization trick (Kingma and W elling, 2013) and for each wm generate parameterized samples from their variational posterior, ˆwm = µm + Lmǫ where ǫ ∼ N (·|0, I). W e then substitute each sample ˆwm for wm in computing all log ( N (ynm|fm(xnm), σ2 m) ) in (16) and remove the expectation in front of them. W e therefore obtain ˆL, an unbiased estimate of ELBO, which is analytically tracta ble. Next, we compute ∇ ˆL, which is an unbiased estimate of the ∇L and hence can be used to maximize L. W e can use any stochastic optimization algorithm. 4 Proof of Lemma 4.1 Lemma 4.1.As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7) of the main paper , ispositive. Proof. First, for brevity, we denote u(tk, x) and γ(tk, x) in (7) of the main paper by uk and γk, respectively. Then from the quadrature results, we compute the variance V ar(fm|D) = ∑ k gkγk + ∑ k gku2 k− ( ∑ k gkuk)2. 15Since γk > 0, the ﬁrst summation ∑ k gkγk > 0. Note that the quadrature weights have all gk > 0 and ∑ k gk = 1 . W e deﬁne ¯u = ∑ k gkuk. Next, we derive that ∑ k gku2 k− ( ∑ k gkuk)2 = ∑ k gku2 k− ¯u2 = ∑ k gku2 k+ ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2 ∑ k gkuk ¯u = ∑ k gk(u2 k+ ¯u2 − 2uk ¯u) = ∑ k gk(uk − ¯u)2 ≥ 0. (17) Therefore, V ar(fm|D) > 0. 5 Proof of Nonnegative V ariance in (12) of the Main Paper W e show the variance in (12) of the main paper, computed by qua drature, is non-negative. The proof is very similar to that of Lemma 4.1 (Section 4). W e denote the quadrature weights and nodes by {gk} and {tk}. Then we have Z = ∑ k gkR(tk), Z 1 = ∑ k gktkR(tk), Z 2 = ∑ k gkt2 kR(tk). (18) Therefore, Z1 Z = ∑ k tk gkR(tk)∑ j gjR(tj ) = ∑ k tkνk, Z2 Z = ∑ k t2 k gkR(tk) ∑ j gjR(tj ) = ∑ k t2 kνk (19) where νk = gk R(tk) ∑ j gj R(tj ) > 0 and ∑ k νk = 1 . Following the same derivation as in (17), we can immediately show that the variance Z2/Z − Z2 1 /Z2 = ∑ k νk(tk − ¯t)2 ≥ 0 where ¯t = Z1/Z = ∑ k tkνk. 16",
      "meta_data": {
        "arxiv_id": "2007.03117v4",
        "authors": [
          "Shibo Li",
          "Wei Xing",
          "Mike Kirby",
          "Shandian Zhe"
        ],
        "published_date": "2020-07-06T23:28:40Z",
        "pdf_url": "https://arxiv.org/pdf/2007.03117v4.pdf",
        "github_url": "https://github.com/zi-w/Max-value-Entropy-Search"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) to address the limitations of existing multi-fidelity BO methods that either ignore or oversimplify the strong, complex correlations across different fidelities. DNN-MFBO employs deep neural networks to flexibly capture all kinds of complicated (potentially nonlinear and nonstationary) relationships between fidelities, thereby improving the estimation of the objective function and overall optimization performance. A tractable and efficient mutual information based acquisition function is computed using sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching. The method demonstrates superior optimization effectiveness and lower query costs compared to state-of-the-art multi-fidelity and single-fidelity BO algorithms on both synthetic benchmarks and real-world engineering design applications.",
        "methodology": "DNN-MFBO constructs a multi-fidelity model by stacking a set of deep neural networks, where each NN models one fidelity. For fidelities m > 1, the input to the NN is the concatenation of the original input and the output from the previous fidelity, allowing information propagation and capturing complex inter-fidelity relationships. The model assumes output layer weights (wm) as random variables with Gaussian priors and other NN weights (θm) as hyper-parameters. A stochastic variational learning algorithm is developed to jointly estimate the posterior distribution of the random weights and the hyper-parameters by maximizing an analytically intractable Evidence Lower Bound (ELBO) using the reparameterization trick. For optimization, a Max-Value Entropy Search (MES) acquisition function is used, which is defined as the mutual information between the objective function's maximum and the function output at a given fidelity, normalized by query cost. The output posterior distributions for each fidelity are approximated as Gaussian via fidelity-wise moment matching and Gauss-Hermite quadrature. The conditional entropy term in the acquisition function is approximated using Monte-Carlo sampling for function maxima (f*) and further approximated by considering the entropy given fM(x) <= f*, where fM(x) is the highest fidelity output. This conditional entropy is also computed using Gauss-Hermite quadrature and moment matching to achieve a Gaussian approximation. The acquisition function is optimized using L-BFGS with random initialization.",
        "experimental_setup": "DNN-MFBO was evaluated against several popular and state-of-the-art multi-fidelity BO algorithms, including MF-SKO, MF-GP-UCB, MF-PES, MF-MES, and MTNN-BO, as well as single-fidelity SF-MES. Experiments were conducted on three synthetic benchmark functions: Branin (3 fidelities, 2D), Park1 (2 fidelities, 4D), and Levy (3 fidelities, 2D), and two real-world engineering design problems: Mechanical Plate Vibration Design (3 material properties, 2 fidelities) and Thermal Conductor Design (3 shape parameters, 2 fidelities). DNN-MFBO and MTNN-BO were implemented in TensorFlow, while other methods used their original implementations. Neural network architectures (depths from 2 to 12, widths from 32 to 512) and learning rates (10^-5 to 10^-1) were tuned using SMAC3 and manual adjustments. ADAM optimizer was used for 5,000 epochs of stochastic training. Initial training points were randomly queried (e.g., 20, 20, 2 for Branin/Levy; 5, 2 for Park1; 20, 5 for real-world problems). Query costs were set as (1, 10, 100) for three-fidelity tasks and (1, 10) for two-fidelity tasks. Performance was measured by simple regret (SR), inference regret (IR) for synthetic tasks, and best queried function values for real-world tasks, along with average query time. All query time comparisons were performed on a Linux workstation with a 16-core Intel(R) Xeon(R) CPU E5-2670 and 16GB RAM. Experiments were repeated five times, and average results with standard error bars were reported.",
        "limitations": "The calculation of the acquisition function, while analytically tractable due to its form, is noted to be quite complex, involving multiple layers of approximation through Gauss-Hermite quadrature, moment matching, and Monte-Carlo sampling. Specifically, the method approximates the conditional entropy H(fm(x)|f*, D) using H(fm(x)|fM(x) <= f*, D), which is a reasonable but not exact substitution. The current work explicitly focuses on discrete fidelities and does not address continuous fidelity settings. The initial tuning of neural network hyper-parameters relied on AutoML (SMAC3) followed by manual adjustments, which could be resource-intensive or require expert domain knowledge for different applications. Although DNN-MFBO showed efficiency in query time, part of this efficiency might stem from using random initialization for L-BFGS for acquisition function optimization, in contrast to some competitors that employ more expensive global optimization strategies.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "#!/usr/bin/env python# Author: Ari Anders and Zi Wangfrom Box2D import *from Box2D.b2 import *import numpy as npimport pygameimport scipy.iofrom numpy import linalg as LA# this just makes pygame show what's going on    class guiWorld:    def __init__(self, fps):        self.SCREEN_WIDTH, self.SCREEN_HEIGHT = 1000, 1000        self.TARGET_FPS = fps        self.PPM = 10.0 # pixels per meter        self.screen = pygame.display.set_mode((self.SCREEN_WIDTH, self.SCREEN_HEIGHT), 0, 32)        pygame.display.set_caption('push simulator')        self.clock = pygame.time.Clock()        self.screen_origin = b2Vec2(self.SCREEN_WIDTH/(2*self.PPM), self.SCREEN_HEIGHT/(self.PPM*2))        self.colors = {            b2_staticBody : (255,255,255,255),             b2_dynamicBody : (163,209,224,255)            }    def draw(self, bodies, bg_color=(64,64,64,0)):    #def draw(self, bodies, bg_color=(0,0,0,0)):        def my_draw_polygon(polygon, body, fixture):            vertices=[(self.screen_origin + body.transform*v)*self.PPM for v in polygon.vertices]            vertices=[(v[0], self.SCREEN_HEIGHT-v[1]) for v in vertices]            color = self.colors[body.type]            if body.userData == \"obs\":                color = (123,128,120,0)            if body.userData == \"hand\":                color = (174,136,218,0)            pygame.draw.polygon(self.screen, color, vertices)                    def my_draw_circle(circle, body, fixture):            position=(self.screen_origin + body.transform*circle.pos)*self.PPM            position=(position[0], self.SCREEN_HEIGHT-position[1])            color = self.colors[body.type]            if body.userData == \"hand\":                color = (174,136,218,0)            pygame.draw.circle(self.screen, color, [int(x) for x in            position], int(circle.radius*self.PPM))        b2PolygonShape.draw=my_draw_polygon        b2CircleShape.draw=my_draw_circle        # draw the world        self.screen.fill(bg_color)        self.clock.tick(self.TARGET_FPS)         for body in bodies:            for fixture in body.fixtures:                fixture.shape.draw(body,fixture)        pygame.display.flip()# this is the interface to pybox2dclass b2WorldInterface:    def __init__(self, do_gui=True):        self.world = b2World(gravity=(0.0,0.0), doSleep=True)        self.do_gui = do_gui        self.TARGET_FPS = 100        self.TIME_STEP = 1.0/self.TARGET_FPS        self.VEL_ITERS, self.POS_ITERS =10,10        self.bodies = []        if do_gui:            self.gui_world  = guiWorld(self.TARGET_FPS)            #raw_input()        else:            self.gui_world = None    def initialize_gui(self):        if self.gui_world == None:            self.gui_world = guiWorld(self.TARGET_FPS)        self.do_gui = True    def stop_gui(self):        self.do_gui = False    def add_bodies(self, new_bodies):        \"\"\" add a single b2Body or list of b2Bodies to the world\"\"\"        if type(new_bodies) == list:            self.bodies += new_bodies        else:            self.bodies.append(new_bodies)    def step(self, show_display=True, idx=0):        self.world.Step(self.TIME_STEP, self.VEL_ITERS, self.POS_ITERS)        if show_display and self.do_gui:            self.gui_world.draw(self.bodies)            #if idx % 10 == 0:            #    pygame.image.save(self.gui_world.screen,'tmp_images/'+str(int(sm.ttt*100)+idx)+'.bmp')class end_effector:    def __init__(self, b2world_interface, init_pos, base, init_angle, hand_shape='rectangle', hand_size=(0.3,1)):        world= b2world_interface.world        self.hand = world.CreateDynamicBody(position=init_pos,angle=init_angle)        self.hand_shape = hand_shape        self.hand_size = hand_size        # forceunit for circle and rect        if hand_shape == 'rectangle':            rshape = b2PolygonShape(box=hand_size)            self.forceunit = 30.0        elif hand_shape == 'circle':            rshape = b2CircleShape(radius=hand_size)            self.forceunit = 100.0        elif hand_shape == 'polygon':            rshape = b2PolygonShape(vertices=hand_size)        else:            raise Exception(\"%s is not a correct shape\" % hand_shape)        self.hand.CreateFixture(            shape = rshape,            density = .1,            friction = .1            )        self.hand.userData = \"hand\"                friction_joint = world.CreateFrictionJoint(            bodyA = base,            bodyB = self.hand,            maxForce = 2,            maxTorque = 2,            )        b2world_interface.add_bodies(self.hand)            def set_pos(self, pos, angle):        self.hand.position = pos        self.hand.angle = angle    def apply_wrench(self, rlvel=(0,0), ravel=0):        #self.hand.ApplyForce(force, self.hand.position,wake=True)        #if avel != 0:                avel = self.hand.angularVelocity        delta_avel = ravel - avel        torque = self.hand.mass*delta_avel*30.0        self.hand.ApplyTorque(torque, wake=True)                #else:        lvel = self.hand.linearVelocity        delta_lvel = b2Vec2(rlvel) - b2Vec2(lvel)        force = self.hand.mass*delta_lvel*self.forceunit        self.hand.ApplyForce(force, self.hand.position,wake=True)            def get_state(self, verbose=False):        state = list(self.hand.position) + [ self.hand.angle] +  \nYou must remove any control tokens (such as \n and \t) from the generated string values. I will fix it by removing all control tokens from the extracted string values.\n",
        "experimental_info": "The method described is DNN-MFBO, a multi-fidelity Bayesian Optimization approach. The provided repository content implements a physics-based 'push world' simulator, which represents the objective function (or a set of fidelities) that DNN-MFBO would optimize. Therefore, the extracted 'experimental_code' refers to the simulator's implementation, and 'experimental_info' details its settings for potential use with DNN-MFBO.Experimental settings for the 'push world' simulator include: Input parameters (potentially optimized by DNN-MFBO): rx, ry (robot 1 initial position), xvel, yvel (robot 1 pushing velocity components), simu_steps (robot 1 simulation steps/duration, directly related to query cost), init_angle (robot 1 initial orientation), rx2, ry2 (robot 2 initial position), xvel2, yvel2 (robot 2 pushing velocity components), simu_steps2 (robot 2 simulation steps/duration, also related to query cost), init_angle2 (robot 2 initial orientation), rtor, rtor2 (robot angular torques), gx, gy (goal position for object 1), gx2, gy2 (goal position for object 2).Fixed environment parameters: The simulation environment is configured with specific object shapes (e.g., 'rectangle', 'circle'), sizes, frictions (e.g., ofriction=0.01, bfriction=0.01), densities (e.g., odensity=0.05), and robot hand shapes and sizes (e.g., hand_shape='rectangle', hand_size=(1,0.3)). The overall objective function being evaluated by this simulation is the sum of Euclidean distances between the final positions of two pushed objects and their respective goal positions."
      }
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
      "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are\npowerful in attaining state-of-the-art machine learning models, with Bayesian\noptimization (BO) standing out as a mainstream method. Extending BO into the\nmulti-fidelity setting has been an emerging research topic, but faces the\nchallenge of determining an appropriate fidelity for each hyperparameter\nconfiguration to fit the surrogate model. To tackle the challenge, we propose a\nmulti-fidelity BO method named FastBO, which adaptively decides the fidelity\nfor each configuration and efficiently offers strong performance. The\nadvantages are achieved based on the novel concepts of efficient point and\nsaturation point for each configuration.We also show that our adaptive fidelity\nidentification strategy provides a way to extend any single-fidelity method to\nthe multi-fidelity setting, highlighting its generality and applicability.",
      "full_text": "FastBO: Fast HPO and NAS with Adaptive Fidelity Identification Jiantong Jiang and Ajmal Mian The University of Western Australia, Perth WA 6009, Australia {jiantong.jiang@research.,ajmal.mian@}uwa.edu.au Abstract. Hyperparameter optimization (HPO) and neural architec- ture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts ofefficient pointand saturation point for each configuration. We also show that our adaptive fidelity iden- tification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability. Keywords: HPO · NAS · Multi-fidelity 1 Introduction HPO [9] and NAS [7] aim to find the hyperparameter configuration or archi- tecture λ∗ that minimizesf(λ), the performance obtained by configurationλ. BO [2,10,30] is an effective model-based method for HPO and NAS. It maintains asurrogate modelof the performancebasedon past evaluations ofconfigurations, which guides the choice of promising configurations to evaluate. Recent studies onBOhaveexploredexpertpriors[11,20,26,29],derivativeinformation[1,27,35], and enhancing the interpretability [5,36–39] of HPO and NAS [3,24,25]. However, standard BO requires full evaluations of configurations, which in- curssignificantcosts,especiallyconsideringtheescalatingmodelevaluationover- head. Despite efforts to accelerate model evaluation [13,15–17], smart strategies are required to widely adopt HPO and NAS. Thus, multi-fidelity methods have been proposed [4,12,21,22], where the fidelities mean the performance levels obtained under various resource levels. They follow the idea of successive halv- ing (SHA) [12]: initially, they evaluate many random configurations using few resources; then, based on the low-fidelity performances, only the well-performing ones successively continue to be evaluated with increasing resources. Follow-up studies [8,19,23,28,33] propose model-based multi-fidelity meth- ods, replacing random sampling with more informed models to improve samplearXiv:2409.00584v1  [cs.LG]  1 Sep 20242 J. Jiang and A. Mian efficiency. However, they are based on SHA, which assumes that learning curves of different configurations rarely intersect - a condition that often fails in prac- tice [32], i.e., early performance observations cannot always indicate the final fidelity performance. This raises a key challenge:What is the appropriate fidelity for each configuration to fit the surrogate model?In other words, which fidelity can offer observations that reliably indicate the final fidelity performance? Cur- rent methods struggle with this. Hyper-Tune [23] and BOHB [8] fit separate models for different fidelities, missing inter-fidelity correlations. FTBO [31] and A-BOHB [19] fit joint models but require strong assumptions. Salinas et al. [28] use the last observed fidelity performance, which may get inaccurate surrogate models as it widens the gap between poorly- and well-performing configurations. This paper is an extended abstract of our conference paper [14], highlighting key ideas and the main experimental results, while omitting finer details. 2 Key Idea of FastBO We propose a multi-fidelity extension of BO, namely FastBO, which tackles the challenge of deciding the appropriate fidelity for each configuration to fit the surrogate model. Here, we first propose the key concepts ofefficient point and saturation point, which are crucial in the optimization process. Then, we briefly describe the process of FastBO and highlight its generality. 2.1 Efficient Point and Saturation Point We first formally define the efficient point as follows. Definition 1 (Efficient point). For a given learning curveCi(r) of hyper- parameter configuration or architectureλi, wherer represents the resource level (also referred to as fidelity), the efficient pointei of λi is defined as:ei = min{r | Ci(r) − Ci(2r) < δ1}, whereδ1 is a predefined small threshold. The semantic of Definition 1 is that starting from the efficient point onwards, whentheresourcesaredoubled,theperformanceimprovementfallsbelowasmall threshold. Consequently, this point signifies a fidelity of performance achieved with comparably efficient resource usage. Thus, we make the following remark. Remark 1. The efficient points of the configurations can serve as their ap- propriate fidelities used for fitting the surrogate model. This is due to their (i) optimal resource-to-performance balance, (ii) ability to capture valuable learning curve trends, and (iii) customization for different hyperparameter configurations. We elaborate on the reasons as follows. Firstly, efficient points balance the trade- off between computational cost and result quality. Beyond the efficient point, allocating additional resources becomes less efficient. Secondly, efficient points capture valuable behaviors within the learning curves, enabling more informed decision-making. Thirdly, the ability to customize the fidelity for each specific configuration is an advantage. This adaptive approach is more reasonable than previous studies that use a fixed fidelity for all the configurations.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 3 1 FastBO Methodology•Process overviewWarm-up Learning curve modelingEfficient & saturation pointsConfig !! •Evaluate !! to the warm-up point to get early observation set.•Stop some bad configs. •Estimate !!’s learning curve from its observation set. Post-processing……•Adaptively extract the points from the learning curve. •Stop evaluating at efficient point.•Fit surrogate model. •Resume some best-performing configs to saturation point.Optimal config Fig. 1:Main process of FastBO. FastBO involves estimating efficient and saturation points, modeling learning curves, and auxiliary stages of warm-up and post-processing. Besides efficient points, we identify saturation points for all configurations as well. We provide the definition of the saturation point as follows. Definition 2 (Saturation point).For a given learning curveCi(r) of config- uration λi, wherer represents the resource level (also referred to as fidelity), the saturation pointsi of λi is defined as:si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}, where δ2 is a predefined small threshold. The semantic of Definition 2 is that beyond the saturation point, the observed performance no longer exhibits notable variations with more resources. Thus, this point characterizes the fidelity at which the performance of a configuration stabilizes. Building on the above definition, we make the following remark. Remark 2. The saturation points of the configurations can serve as their ap- proximate final fidelities, as they provide performance results that meet predefined quality thresholds while reducing resource wastage. 2.2 FastBO Process and Generalization With the two crucial points, we show the main process of FastBO in Fig. 1. Each configurationλi first enters a warm-up stage to get its early observation set. Some configurations are terminated here if they are detected consecutive performance deterioration. Then, FastBO estimates the learning curve ofλi from its observation set. Thus, the efficient point and saturation points are adaptively extracted. After that, λi continues to be evaluated to its efficient point; the result is used to update the surrogate model. Finally, the post-processing stage let a small set of promising configurations resume evaluating to their saturation points, and the optimal configurations can be obtained. Generalizing FastBO to single-fidelity methods.The inefficiency of single- fidelity methods like BO stems from their reliance on expensive final fidelity evaluations. Notably, low-fidelity evaluations provide informative insights but are computationally cheaper. Therefore, we can extend single-fidelity methods to the multi-fidelity setting by acquiring the low-fidelity performance for each configuration to fit the surrogate model. To do this, we need to determine the fidelity used to fit the surrogate model. FastBO adaptively determines this fi- delity for each configuration by identifying its efficient point. While this adaptive identification strategy is described in the context of model-based methods, it can4 J. Jiang and A. Mian Fashion-MNIST Airlines Albert CovertypeChristine Fig. 2:Anytime performance on the LCBench benchmark. Slice CIFAR-10 CIFAR-100 ProteinImageNet16-120 (a) NAS-Bench-201 benchmark (b) FCNet benchmark Fig. 3:Anytime performance on(a) NAS-Bench-201 and(b) FCNet. be generalized to various single-fidelity methods. For example, when evaluating configurations within the population for an evolutionary algorithm-based HPO method, we can similarly evaluate the efficient point performances instead of the final performances and integrate them in the subsequent processes, such as selection and variation. To conclude, the proposed strategy provides a simple but effective way to bridge the gap between single- and multi-fidelity methods. 3 Experimental Evaluation We compare the performance of FastBO with random search (RS), standard BO [30], ASHA [21], Hyperband [22], PASHA [4], A-BOHB [19], A-CQR [28], BOHB [8], DyHPO [34], and Hyper-Tune [23]. The results on the LCBench [40], NAS-Bench-201 [6], and FCNet [18] benchmarks are shown in Figs. 2 and 3. Overall, FastBO can handle various performance metrics and shows strong any- time performance. We can observe that FastBO gains an advantage earlier than other methods, rapidly converging to the global optimum after the initial phase. 4 Conclusion and Discussion We propose FastBO, a model-based multi-fidelity HPO method, which excels in adaptively identifying the fidelity for each configuration to fit the surrogate modelandefficientlyprovidinghigh-qualityperformance.Theproposed adaptive fidelity identification strategy also provides a simple way to extend any single- fidelity method to the multi-fidelity setting. While this paper provides a strong foundation on HPO and NAS, we see challenges that demand future improve- ments. Future work could refine and expand Fast-BO to larger search spaces and distributed computing systems to improve its applicability and scalablity.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 5 References 1. Ament, S.E., Gomes, C.P.: Scalable first-order Bayesian Optimization via struc- tured automatic differentiation. In: International Conference on Machine Learning. pp. 500–516. PMLR (2022) 1 2. Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter optimization. Advances in Neural Information Processing Systems24 (2011) 1 3. Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A.L., et al.: Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisci- plinary Reviews: Data Mining and Knowledge Discovery13(2), e1484 (2023) 1 4. Bohdal, O., Balles, L., Wistuba, M., Ermis, B., Archambeau, C., Zappella, G.: PASHA: efficient HPO and NAS with progressive resource allocation. In: Interna- tional Conference on Learning Representations. OpenReview.net (2023) 1, 4 5. Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.: This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems32 (2019) 1 6. Dong, X., Yang, Y.: NAS-Bench-201: Extending the scope of reproducible neu- ral architecture search. In: International Conference on Learning Representations (2020) 4 7. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. The Journal of Machine Learning Research20(1), 1997–2017 (2019) 1 8. Falkner, S., Klein, A., Hutter, F.: BOHB: Robust and efficient hyperparameter optimization at scale. In: International Conference on Machine Learning. pp. 1437– 1446. PMLR (2018) 1, 2, 4 9. Feurer, M., Hutter, F.: Hyperparameter optimization. Automated Machine Learn- ing: Methods, Systems, Challenges pp. 3–33 (2019) 1 10. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. In: Learning and Intelligent Optimization. pp. 507–523. Springer (2011) 1 11. Hvarfner,C.,Stoll,D.,Souza,A.L.F.,Lindauer,M.,Hutter,F.,Nardi,L.:$\\pi$BO: Augmenting acquisition functions with user beliefs for bayesian optimization. In: International Conference on Learning Representations. OpenReview.net (2022) 1 12. Jamieson, K., Talwalkar, A.: Non-stochastic best arm identification and hyperpa- rameter optimization. In: Artificial Intelligence and Statistics. pp. 240–248. PMLR (2016) 1 13. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast parallel exact inference on Bayesian networks. In: ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. pp. 425–426 (2023) 1 14. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Efficient hyperparameter optimization with adaptive fidelity identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 26181–26190 (2024) 2 15. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast inference for probabilistic graphical models. In: 2024 USENIX Annual Technical Conference (USENIX ATC 24) (2024) 1 16. Jiang, J., Wen, Z., Mian, A.: Fast parallel bayesian network structure learning. In: IEEE International Parallel and Distributed Processing Symposium. pp. 617–627. IEEE (2022) 1 17. Jiang, J., Wen, Z., Yang, P., Mansoor, A., Mian, A.: Fast-pgm: Fast probabilistic graphical model learning and inference. arXiv preprint arXiv:2405.15605 (2024) 16 J. Jiang and A. Mian 18. Klein, A., Hutter, F.: Tabular benchmarks for joint architecture and hyperparam- eter optimization. arXiv preprint arXiv:1905.04970 (2019) 4 19. Klein, A., Tiao, L.C., Lienart, T., Archambeau, C., Seeger, M.: Model-based asynchronous hyperparameter and neural architecture search. arXiv preprint arXiv:2003.10865 (2020) 1, 2, 4 20. Li, C., Rana, S., Gupta, S., Nguyen, V., Venkatesh, S., Sutti, A., de Celis Leal, D.R.,Slezak,T.,Height,M.,Mohammed,M.,Gibson,I.:Acceleratingexperimental design by incorporating experimenter hunches. In: International Conference on Data Mining. pp. 257–266. IEEE Computer Society (2018) 1 21. Li,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,E.,Ben-Tzur,J.,Hardt,M.,Recht, B., Talwalkar, A.: A system for massively parallel hyperparameter tuning. Proceed- ings of Machine Learning and Systems2, 230–246 (2020) 1, 4 22. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., Talwalkar, A.: Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research18(1), 6765–6816 (2017) 1, 4 23. Li, Y., Shen, Y., Jiang, H., Zhang, W., Li, J., Liu, J., Zhang, C., Cui, B.: Hyper- tune: Towards efficient hyper-parameter tuning at scale. Proceedings of the VLDB Endowment 15(6), 1256–1265 (2022) 1, 2, 4 24. Moosbauer, J., Casalicchio, G., Lindauer, M., Bischl, B.: Improving accuracy of interpretability measures in hyperparameter optimization via Bayesian algorithm execution. arXiv preprint arXiv:2206.05447 (2022) 1 25. Moosbauer, J., Herbinger, J., Casalicchio, G., Lindauer, M., Bischl, B.: Explaining hyperparameter optimization via partial dependence plots. Advances in Neural Information Processing Systems34, 2280–2291 (2021) 1 26. Oh, C., Gavves, E., Welling, M.: BOCK: Bayesian optimization with cylindrical kernels. In: International Conference on Machine Learning. pp. 3868–3877. PMLR (2018) 1 27. Padidar, M., Zhu, X., Huang, L., Gardner, J., Bindel, D.: Scaling gaussian pro- cesses with derivative information using variational inference. Advances in Neural Information Processing Systems34, 6442–6453 (2021) 1 28. Salinas, D., Golebiowski, J., Klein, A., Seeger, M.W., Archambeau, C.: Optimizing hyperparameters with conformal quantile regression. In: International Conference on Machine Learning. vol. 202, pp. 29876–29893. PMLR (2023) 1, 2, 4 29. Shahriari, B., Bouchard-Côté, A., Freitas, N.: Unbounded Bayesian Optimization via regularization. In: Artificial intelligence and statistics. pp. 1168–1176. PMLR (2016) 1 30. Snoek, J., Larochelle, H., Adams, R.P.: Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems25 (2012) 1, 4 31. Swersky, K., Snoek, J., Adams, R.P.: Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896 (2014) 2 32. Viering, T., Loog, M.: The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 2 33. Wang, J., Xu, J., Wang, X.: Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. arXiv preprint arXiv:1801.01596 (2018) 1 34. Wistuba, M., Kadra, A., Grabocka, J.: Supervising the multi-fidelity race of hy- perparameter configurations. Advances in Neural Information Processing Systems 35, 13470–13484 (2022) 4 35. Wu, J., Poloczek, M., Wilson, A.G., Frazier, P.: Bayesian optimization with gradi- ents. Advances in neural information processing systems30 (2017) 1FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 7 36. Yang,P.,Akhtar,N.,Jiang,J.,Mian,A.:Backdoor-basedexplainableaibenchmark forhighfidelityevaluationofattributionmethods.arXivpreprintarXiv:2405.02344 (2024) 1 37. Yang, P., Akhtar, N., Shah, M., Mian, A.: Regulating model reliance on non-robust features by smoothing input marginal density. arXiv preprint arXiv:2407.04370 (2024) 1 38. Yang, P., Akhtar, N., Wen, Z., Mian, A.: Local path integration for attribution. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 3173–3180 (2023) 1 39. Yang, P., Akhtar, N., Wen, Z., Shah, M., Mian, A.: Re-calibrating feature attri- butions for model interpretation. In: International Conference on Learning Repre- sentations (2022) 1 40. Zimmer, L., Lindauer, M.T., Hutter, F.: Auto-Pytorch: Multi-fidelity metalearn- ing for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence43, 3079–3090 (2021) 4",
      "meta_data": {
        "arxiv_id": "2409.00584v1",
        "authors": [
          "Jiantong Jiang",
          "Ajmal Mian"
        ],
        "published_date": "2024-09-01T02:40:04Z",
        "pdf_url": "https://arxiv.org/pdf/2409.00584v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes FastBO, a multi-fidelity Bayesian Optimization (BO) method for Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS). It tackles the challenge of adaptively deciding the appropriate fidelity for each hyperparameter configuration to fit the surrogate model. This is achieved by introducing novel concepts of 'efficient point' (optimal resource-to-performance balance for surrogate model fitting) and 'saturation point' (approximate final fidelity). FastBO's adaptive fidelity identification strategy offers a general way to extend any single-fidelity method to a multi-fidelity setting and demonstrates strong anytime performance, converging rapidly to the global optimum.",
        "methodology": "FastBO extends Bayesian Optimization (BO) by adaptively determining the fidelity for each configuration. The core methodology involves: 1) Defining an 'efficient point' for a configuration `λi` as `ei = min{r | Ci(r) − Ci(2r) < δ1}`, where `r` is resource level, `Ci(r)` is performance, and `δ1` is a small threshold. This point signifies efficient resource usage and is used to fit the surrogate model. 2) Defining a 'saturation point' as `si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}`, where `δ2` is a small threshold, indicating performance stabilization. The FastBO process includes a warm-up stage, learning curve modeling to estimate efficient and saturation points, evaluating configurations up to their efficient points to update the surrogate model, and a post-processing stage where promising configurations are evaluated up to their saturation points to find the optimal configuration. This adaptive strategy is generalizable to other single-fidelity methods.",
        "experimental_setup": "FastBO's performance was evaluated against a suite of state-of-the-art multi-fidelity HPO methods, including Random Search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The experiments were conducted on three established benchmarks: LCBench, NAS-Bench-201, and FCNet. The primary validation method was a comparison of 'anytime performance,' which assesses how quickly a method converges to the global optimum.",
        "limitations": "The paper is presented as an extended abstract, which inherently omits finer details of the method and experiments. The applicability and scalability of FastBO to larger search spaces and distributed computing systems are identified as areas for future improvement, implying potential limitations in these aspects for the current version. The method relies on predefined small thresholds, `δ1` and `δ2`, for defining efficient and saturation points, whose optimal selection or sensitivity is not explicitly discussed in this abstract.",
        "future_research_directions": "Future work could focus on refining and expanding FastBO to handle larger search spaces and integrate with distributed computing systems. The goal for these directions is to improve FastBO's overall applicability and scalability."
      }
    },
    {
      "title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks",
      "abstract": "Bayesian optimization (BO) is a powerful approach for optimizing black-box,\nexpensive-to-evaluate functions. To enable a flexible trade-off between the\ncost and accuracy, many applications allow the function to be evaluated at\ndifferent fidelities. In order to reduce the optimization cost while maximizing\nthe benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian\nOptimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of\nBayesian neural networks to construct a fully auto-regressive model, which is\nexpressive enough to capture strong yet complex relationships across all the\nfidelities, so as to improve the surrogate learning and optimization\nperformance. Furthermore, to enhance the quality and diversity of queries, we\ndevelop a simple yet efficient batch querying method, without any combinatorial\nsearch over the fidelities. We propose a batch acquisition function based on\nMax-value Entropy Search (MES) principle, which penalizes highly correlated\nqueries and encourages diversity. We use posterior samples and moment matching\nto fulfill efficient computation of the acquisition function and conduct\nalternating optimization over every fidelity-input pair, which guarantees an\nimprovement at each step. We demonstrate the advantage of our approach on four\nreal-world hyperparameter optimization applications.",
      "full_text": "Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks Shibo Li, Robert M. Kirby, and Shandian Zhe School of Computing, University of Utah Salt Lake City, UT 84112 shibo@cs.utah.edu, kirby@cs.utah.edu, zhe@cs.utah.edu Abstract Bayesian optimization (BO) is a powerful approach for optimizing black-box, expensive-to-evaluate functions. To enable a ﬂexible trade-off between the cost and accuracy, many applications allow the function to be evaluated at different ﬁdelities. In order to reduce the optimization cost while maximizing the beneﬁt- cost ratio, in this paper we propose Batch Multi-ﬁdelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of Bayesian neural networks to construct a fully auto-regressive model, which is expressive enough to capture strong yet complex relationships across all the ﬁdelities, so as to improve the surrogate learning and optimization performance. Furthermore, to enhance the quality and diversity of queries, we develop a simple yet efﬁcient batch querying method, without any combinatorial search over the ﬁdelities. We propose a batch acquisition function based on Max-value Entropy Search (MES) principle, which penalizes highly correlated queries and encourages diversity. We use posterior samples and moment matching to fulﬁll efﬁcient computation of the acquisition function, and conduct alternating optimization over every ﬁdelity-input pair, which guarantees an improvement at each step. We demonstrate the advantage of our approach on four real-world hyperparameter optimization applications. 1 Introduction Many applications demand we optimize a complex function of an unknown form that is expensive to evaluate. Bayesian optimization (Mockus, 2012; Snoek et al., 2012) is a powerful approach to optimize such functions. The key idea is to use a probabilistic surrogate model, typically Gaussian processes (Rasmussen and Williams, 2006), to iteratively approximate the target function, integrate the posterior information to compute and maximize an acquisition function so as to generate new inputs at which to query, update the model with new examples, and meanwhile approach the optimum. In practice, to enable a ﬂexible trade-off between the computational cost and accuracy, many appli- cations allow us to evaluate the target function at different ﬁdelities. For example, to evaluate the performance of the hyperparameters for a machine learning model, we can train the model thoroughly, i.e., with sufﬁcient iterations/epochs, to obtain the accurate evaluation (high-ﬁdelity yet often costly) or just run a few iterations/epochs to obtain a rough estimate (low-ﬁdelity but much cheaper). Many multi-ﬁdelity BO algorithms (Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; Song et al., 2019; Takeno et al., 2019) have therefore been proposed to identify both the ﬁdelities and inputs at which to query, so as to reduce the cost and achieve a good beneﬁt-cost balance. Notwithstanding their success, these methods often overlook the strong yet complex relationships between different ﬁdelities or adopt an over-simpliﬁed assumption, (partly) for the sake of convenience in calculating/maximizing the acquisition function considering ﬁdelities. This, however, can restrict the performance of the surrogate model, impair the optimization efﬁciency and increase the cost. For 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.09884v2  [cs.LG]  25 Oct 2021example, Lam et al. (2015); Kandasamy et al. (2016) learned an independent GP for each ﬁdelity, Zhang et al. (2017) used multitask GPs with a convolved kernel for multi-ﬁdelity modeling and have to use a simple smoothing kernel (e.g., Gaussian) for tractable convolutions. The recent work (Takeno et al., 2019) imposes a linear correlation across different ﬁdelities. In addition, the standard one-by- one querying strategy needs to sequentially run each query and cannot utilize parallel computing resources to accelerate, e.g., multi-core CPUs/GPUs and clusters. While incrementally absorbing more information, it does not explicitly account for the correlation between different queries, hence still has a risk to bring in highly correlated examples that includes redundant information. To address these issues, we propose BMBO-DARN, a novel batch multi-ﬁdelity Bayesian optimization method. First, we develop a deep auto-regressive model to integrate training examples at various ﬁdelities. Each ﬁdelity is modeled by a Bayesian neural network (NN), where the output predicts the objective function value at that ﬁdelity and the input consists of the original inputs and the outputs of all the previous ﬁdelities. In this way, our model is adequate to capture the complex, strong correlations ( e.g., nonstationary, highly nonlinear) across all the ﬁdelities to enhance the surrogate learning. We use Hamiltonian Monte-Carlo (HMC) sampling for posterior inference. Next, to improve the quality of the queries, we develop a simple yet efﬁcient method to jointly fetch a batch of inputs and ﬁdelities. Speciﬁcally, we propose a batch acquisition function based on the state-of-the-art Max-value Entropy Search (MES) principle (Wang and Jegelka, 2017). The batch acquisition function explicitly penalizes highly correlated queries and encourages diversity. To efﬁciently compute the acquisition function, we use the posterior samples of the NN weights and moment matching to construct a multi-variate Gaussian posterior for all the ﬁdelity outputs and the function optimum. To prevent a combinatorial search over multiple ﬁdelities in maximizing the acquisition function, we develop an alternating optimization algorithm to cyclically update each pair of input and ﬁdelity, which is much more efﬁcient and guarantees an improvement at each step. For evaluation, we examined BMBO-DARN in both synthetic benchmarks and real-world applications. The synthetic benchmark tasks show that given a small number of training examples, our deep auto- regressive model can learn a more accurate surrogate of the target function than other state-of-the-art multi-ﬁdelity BO models. We then evaluated BMBO-DARN on four popular machine learning models (CNN, online LDA, XGBoost and Physics informed NNs) for hyperparameter optimization. BMBO-DARN can ﬁnd more effective hyperparameters leading to superior predictive performance, and meanwhile spends smaller total evaluation costs, as compared with state-of-the-art multi-ﬁdelity BO algorithms and other popular hyperparameter tuning methods. 2 Background Bayesian Optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a popular approach for optimizing black-box functions that are often costly to evaluate and cannot provide exact gradient information. BO learns a probabilistic surrogate model to predict the function value across the input space and quantiﬁes the predictive uncertainty. At each step, we use this information to compute an acquisition function to measure the utility of querying at different inputs. By maximizing the acquisition function, we ﬁnd the next input at which to query, which is supposed to be closer to the optimum. Then we add the new example into the training set to improve the accuracy of the surrogate model. The procedure is repeated until we ﬁnd the optimal input or the maximum number of queries have been ﬁnished. There are a variety of acquisition functions, such as Expected Improvement (EI) (Mockus et al., 1978) and Upper Conﬁdence Bound (UCB) (Srinivas et al., 2010). The recent state-of-the-art addition is Maximum-value Entropy Search (MES) (Wang and Jegelka, 2017), a(x) = I ( f(x),f∗|D ) , (1) where I(·,·) is the mutual information, f(x) is the objective function value at x, f∗the minimum, and Dthe training data collected so far for the surrogate model. Note that both f(x) and f∗are considered as generated by the posterior of the surrogate model given D; they are random variables. The most commonly used class of surrogate models is Gaussian process (GP) (Rasmussen and Williams, 2006). Given the training dataset X = [ x⊤ 1 ,..., x⊤ N]⊤and y = [ y1,...,y N]⊤, a GP assumes the outputs y follow a multivariate Gaussian distribution, p(y|X) = N(y|m,K + vI), where m is the mean function values at the inputs X, often set to 0, vis the noise variance, and K is a kernel matrix on X. Each [K]ij = κ(xi,xj), where κ(·,·) is a kernel function. For example, a popular one is the RBF kernel,κ(xi,xj) = exp ( −β−1∥xi −xj∥2) . An important advantage of GPs 2f1(θ) f2(θ) . . . f3(θ) fM (θ)θ Figure 1: Graphical representation of the deep auto-regressive model in BMBO-DARN. The output at each ﬁdelity fm(x) (1 ≤ m ≤ M) is calculated by a (deep) neural network. is their convenience in uncertainty quantiﬁcation. Since GPs assume any ﬁnite set of function values follow a multi-variate Gaussian distribution, given a test input ˆx, we can compute the predictive (or posterior) distribution p(f(ˆx)|ˆx,X,y) via a conditional Gaussian distribution, which is simple and analytical. Multi-Fidelity BO. Since evaluating the exact value of the object function is often expensive, many practical applications provide multi-ﬁdelity evaluations {f1(x),...,f M(x)}to allow us to choose a trade-off between the accuracy and cost. Accordingly, many multi-ﬁdelity BO algorithms have been developed to select both the inputs and ﬁdelities to reduce the cost and to achieve a good balance between the optimization progress and cost, i.e., the beneﬁt-cost ratio. For instance, MF- GP-UCB (Kandasamy et al., 2016) sequentially queries at each ﬁdelity (from the lowest one, i.e., m= 1) until the conﬁdence band is over a given threshold. In spite of its great success and guarantees in theory, MF-GP-UCB uses a set of independent GPs to estimate the objective at each ﬁdelity, and hence ignores the valuable correlations between different ﬁdelities. MF-PES (Zhang et al., 2017) uses a multi-task GP surrogate where each task corresponds to one ﬁdelity, and convolves a smoothing kernel with the kernel of a shared latent function to obtain the cross-covariance. The recent MF-MES (Takeno et al., 2019) also builds a multi-task GP surrogate, where the covariance function is κ(fm(x),fm′ (x′)) = ∑d j=1 (umjum′j + 1(m= m′) ·αmj) ρj(x1,x2), (2) where αmj >0, 1(·) is the indicator function, {umj}d j=1 is dlatent features for each ﬁdelity m, and {ρj(·,·)}are dbases kernels, usually chosen as a commonly used stationary kernel, e.g., RBF. 3 Deep Auto-Regressive Model for Multi-Fidelity Surrogate Learning Notwithstanding the elegance and success of the existing multi-ﬁdelity BO methods, they often ignore or oversimplify the complex, strong correlations between different ﬁdelities, and hence can be inefﬁcient for surrogate learning, which might further lower the optimization efﬁciency and incur more expenses. For example, the state-of-the-art methods MF-GP-UCB (Kandasamy et al., 2016) estimate a GP surrogate for each ﬁdelity independently; MF-PES (Zhang et al., 2017) has to adopt a simple form for both the smoothing and latent function kernel (e.g., Gaussian and delta) to achieve an analytically tractable convolution, which might limit the expressivity in estimating the cross-ﬁdelity covariance; MF-MES (Takeno et al., 2019) essentially imposes a linear correlation structure between different ﬁdelities — for any input x, κ(fm(x),fm′ (x)) = u⊤ m1 um2 + αm where um = [um1,...,u md] and ˜αm = ∑d j=1 αmj if we use a RBF basis kernel (see (2)). To overcome this limitation, we develop a deep auto-regressive model for multi-ﬁdelity surrogate learning. Our model is expressive enough to capture the strong, possibly very complex (e.g., highly nonlinear, nonstationary) relationships between all the ﬁdelities to improve the prediction (at the highest ﬁdelity). As such, our model can more effectively integrate multi-ﬁdelity training information to better estimate the objective function. Speciﬁcally, given M ﬁdelities, we introduce a chain of M neural networks, each of which models one ﬁdelity and predicts the target function at that ﬁdelity. Denote by xm, Wm, and ψWm(·) the NN input, parameters and output mapping at each ﬁdelity m. Our model is deﬁned as follows, xm = [x; f1(x); ... ; fm−1(x)], fm(x) = ψWm(xm), y m(x) = fm(x) + ϵm, (3) where x1 = x, fm(x) is the prediction (i.e., NN output) at the m-th ﬁdelity, ym(x) is the observed function value, and ϵm is a random noise, ϵm ∼N (ϵm|0,τ−1 m ). We can see that each input xm 3consists of not only the original input x of the objective function, but also the outputs from all the previous ﬁdelities. Via a series of linear projection and nonlinear activation from the NN, we obtain the output at ﬁdelity m. In this way, our model fully exploits the information from the lower ﬁdelities and can ﬂexibly capture arbitrarily complex relationships between the current and all the previous ﬁdelities by learning an NN mapping, fm(x) = ψWm ( xm,f1(x),...,f m−1(x) ) . We assign a standard Gaussian prior distribution over each element of the NN parameters W= {W1,..., WM}, and a Gamma prior over each noise precision, p(τm) = Gam(τm|a0,b0). Given the dataset D= {{(xnm,ynm)}Nm n=1}M m=1, the joint probability of our model is given by p(W,τ, Y,S|X) = N(vec(W)|0,I) M∏ m=1 Gam(τm|a0,b0) M∏ m=1 Nm∏ n=1 N ( ynm|fm(xnm),τ−1 m ) , (4) where τ = [ τ1,...,τ M], X = {xnm}, Y = {ynm}, and vec(·) is vectorization. The graphical representation of our model is given in Fig. 1. We use Hamiltonian Monte Carlo (HMC) (Neal et al., 2011) sampling to perform posterior inference due to its unbiased, high-quality uncertainty quantiﬁcation, which is critical to calculate the acquisition function. However, our method allows us to readily switch to other approximate inference approaches as needed (see Sec. 4), e.g., stochastic gradient HMC (Chen et al., 2014) used in the excellent work of Springenberg et al. (2016). 4 Batch Acquisition for Multi-Fidelity Optimization Given the posterior of our model, we aim to compute and optimize an acquisition function to identify the input and ﬁdelity at which to query next. Popular BO methods query at one input each time and then update the surrogate model. While successful, this one-by-one strategy has to run each query sequentially and cannot take advantage of parallel computing resources (that are often available in practice) to further accelerate, such as multi-core CPU and GPU workstations and computer clusters. In addition, the one-by-one strategy although gradually integrates more data information, it lacks an explicit mechanism to take into account the correlation across different queries, hence still has a risk to bring in highly correlated examples with redundant information, especially in the multi-ﬁdelity setting, e.g., querying at the same input with another ﬁdelity. To allow parallel query and to improve the query quality and diversity, we develop a batch acquiring approach to jointly identify a set of inputs and ﬁdelities at a time, presented as follows. 4.1 Batch Acquisition Function We ﬁrst propose a batch acquisition function based on the MES principle (Zhang et al., 2017) (see (1)). Denote by Bthe batch size and by {λ1,...,λ M}the cost of querying at M ﬁdelities. We want to jointly identify Bpairs of inputs and ﬁdelities (x1,m1),..., (xB,mB) at which to query. The batch acquisition function is given by abatch(X,m) = I({fm1 (x1),...,f mB (xB)},f∗|D)∑B k=1 λmk , (5) where X = {x1,..., xB}and m = [m1,...,m B]. As we can see, our batch acquisition function explicitly penalizes highly correlated queries, encouraging joint effectiveness and diversity — if between the outputs {fmk(xk)}B k=1 are high correlations, the mutual information in the numerator will decrease. Furthermore, by dividing the total querying cost in (5), the batch acquisition function expresses a balance between the beneﬁt of these queries (in probing the optimum) and the price, i.e., beneﬁt-cost ratio. When we set B = 1, our batch acquisition function is reduced to the single one used in (Takeno et al., 2019). 4.2 Efﬁcient Computation Given X and m, the computation of (5) is challenging, because it involves the mutual information between a set of NN outputs and the function optimum. To address this challenge, we use posterior samples and moment matching to approximate p(f,f∗|D) as a multi-variate Gaussian distribution, where f = [fm1 (x1),...,f mB (xB)]. Speciﬁcally, we ﬁrst draw a posterior sample of the NN weights Wfrom our model. We then calculate the output at each input and ﬁdelity to obtain a sample of f, and maximize (or minimize) fM(·) to obtain a sample of f∗. We use L-BFGS (Liu and Nocedal, 41989) for optimization. After we collect Lindependent samples {(ˆf1, ˆf∗ 1 ),..., (ˆfL, ˆf∗ L)}, we can estimate the ﬁrst and second moments of h = [f; f∗], namely, mean and covariance matrix, µ = 1 L L∑ j=1 ˆhj, Σ = 1 L−1 L∑ j=1 (ˆhj −µ)(ˆhj −µ)⊤, where each ˆhj = [ˆfj; ˆf∗ j]. We then use these moments to match a multivariate Gaussian posterior, p(h|D) ≈N(h|µ,Σ). Then the mutual information can be computed with a closed form, I(f,f∗|D) = H(f|D) + H(f∗|D) −H(f,f∗|D) ≈1 2 log |Σﬀ |+ 1 2 log σ∗∗−1 2 log |Σ|, (6) where Σﬀ = Σ[1 : B,1 : B], i.e., the ﬁrst B×B sub-matrix along the diagonal, which is the posterior covariance of f, and σ∗∗= Σ[B+ 1,B + 1], i.e., the posterior variance of f∗. The batch acquisition function is therefore calculated from abatch(X,m) ≈ 1 2 ∑B k=1 λmk (log |Σﬀ |+ logσ∗∗−log |Σ|) . (7) Note that Σ is a function of the inputs X and ﬁdelities m and hence so are its submatrix and elements, Σﬀ and σ∗∗. To obtain a reliable estimate of the moments, we set L= 100 in our experiments. Note that our method can be applied along with any posterior inference algorithm, such as variational inference and SGHMC (Chen et al., 2014), as long as we can generate posterior samples of the NN weights, not restricted to the HMC adopted in our paper. 4.3 Optimizing a Batch of Fidelities and Inputs Now, we consider maximizing (7) to identify Binputs X and their ﬁdelities m at which to query. However, since the optimization involves a mix of continuous inputs and discrete ﬁdelities, it is quite challenging. A straightforward approach would be to enumerate all possible conﬁgurations of m, for each particular conﬁguration, run a gradient based optimization algorithm to ﬁnd the optimal inputs, and then pick the conﬁguration and its optimal inputs that give the largest value of the acquisition function. However, doing so is essentially conducting a combinatorial search over Bﬁdelities, and the search space grows exponentially with B, i.e., O(MB) = O(eBlog M). Hence, it will be very costly, even infeasible for a moderate choice of B. To address this issue, we develop an alternating optimization algorithm. Speciﬁcally, we ﬁrst initialize all the Bqueries, Q= {(x1,m1),..., (xB,mB)}, say, randomly. Then each time, we only optimize one pair of the input and ﬁdelity (xk,mk)(1 ≤k≤B), while ﬁxing the others. We cyclically update each pair, where each update is much cheaper but guarantees to increase abatch. Speciﬁcally, each time, we maximize abatch,k(x,m) = I(F¬k ∪{fm(x)},f∗|D) λm + ∑ j̸=kλmj , (8) where F¬k = {fmj (xj)|j ̸= k}. Note that the computation of(8) still follows(7). We set(xk,mk) to the optimum (x∗,m∗), and then proceed to optimize the next input location and ﬁdelity(xk+1,mk+1) in Qwith the others ﬁxed. We continues this until we ﬁnish updating all the queries in Q, which corresponds to one iteration. We can keep running iterations until the increase of the batch acquisition function is less than a tolerance level or a maximum number of iterations has been done. Suppose we ran Giterations, the time complexity is O(GMB), which is linear in the number of ﬁdelities and batch size, and hence is much more efﬁcient than the naive combinatorial search. Our multi-ﬁdelity BO approach is summarized in Algorithm 1. 5 Related Work Most Bayesian optimization (BO) (Mockus, 2012; Snoek et al., 2012) methods are based on Gaussian processes (GPs) and a variety of acquisition functions, such as (Mockus et al., 1978; Auer, 2002; Srinivas et al., 2010; Hennig and Schuler, 2012; Hernández-Lobato et al., 2014; Wang and Jegelka, 2017; Kandasamy et al., 2017b; Garrido-Merchán and Hernández-Lobato, 2020). Snoek et al. (2015) showed Bayesian neural networks (NNs) can also be used as a general surrogate model, and has 5Algorithm 1 BMBO-DARN (D, B, M, T, {λm}M m=1 ) Learn the deep auto-regressive model (4) on Dwith HMC. for t= 1,...,T do Collect a batch of Bqueries, Q= {(xk,mk)}B k=1, with Algorithm 2. Query the objective function value at each input xk and ﬁdelity mk in Q D←D∪{ (xk,yk,mk)|1 ≤k≤B}. Re-train the deep auto-regressive model on Dwith HMC. end for Algorithm 2 BatchAcquisition({λm}, B, L, G, ξ) Initialize Q= {(x1,m1),..., (xB,mB)}randomly. Collect Lindependent posterior samples of the NN weights. repeat for k= 1,...,B do Use the posterior samples to calculate and optimize (8), (x∗,m∗) = argmax x∈Ω,1≤m≤M abatch,k(x,m), where Ω is the input domain. (xk,mk) ←(x∗,m∗). end for until Giterations are done or the increase of abatch in (7) is less than ξ Return Q. excellent performance. Moreover, the training of NNs is scalable, not suffering from O(N3) time complexity (N is the number of examples) of training exact GPs. Springenberg et al. (2016) further used scale adaption to develop a robust stochastic gradient HMC for the posterior inference in the NN based BO. Recent works that deal with discrete inputs (Baptista and Poloczek, 2018) or mixed discrete and continuous inputs (Daxberger et al., 2019) use an explicit nonlinear feature mapping and Bayesian linear regression, which can be viewed as one-layer Bayesian NNs. There have been many studies in multi-ﬁdelity (MF) BO, e.g., (Huang et al., 2006; Swersky et al., 2013; Lam et al., 2015; Picheny et al., 2013; Kandasamy et al., 2016, 2017a; Poloczek et al., 2017; McLeod et al., 2017; Wu and Frazier, 2017). While successful, these methods either ignore or oversimplify the strong, complex correlations between different ﬁdelities, and hence might be inefﬁcient in surrogate learning. For example, Picheny et al. (2013); Lam et al. (2015); Kandasamy et al. (2016); Poloczek et al. (2017) learned an independent GP for each ﬁdelity; Song et al. (2019) used all the examples without discrimination to train one single GP; Huang et al. (2006); Takeno et al. (2019) imposed a linear correlation across ﬁdelities, while Zhang et al. (2017) constructed a convolutional kernel as the cross-ﬁdelity covariance and so the involved kernels in the convolution must be simple and smooth enough (yet less expressive) to obtain a closed form. Recently, Perrone et al. (2018) developed an NN-based multi-task BO method for hyper-parameter transfer learning. Their model constructs an NN feature mapping shared by all the tasks, and uses an independent linear combination of the mapped features to predict each task output. While we can consider each task as evaluating the objective at a particular ﬁdelity, the model does not explicitly capture and exploit the correlations across different tasks — given the shared (latent) features, the predictions of these tasks (ﬁdelities) are independent. The most recent work (Li et al., 2020) also developed an NN-based multi-ﬁdelity BO method, which differs from our work in that (1) their model only estimates the relationship between successive ﬁdelities, and hence has less capacity, (2) their work uses a recursive one-dimensional quadrature to calculate the acquisition function, and is difﬁcult to extend to batch acquisitions. In a high level, the chain structure of Li et al. (2020)’s model also resembles deep GP based multi-ﬁdelity models (Perdikaris et al., 2017; Cutajar et al., 2019). Quite a few batch BO algorithms have been developed, such as (González et al., 2016; Wu and Frazier, 2016; Hernández-Lobato et al., 2017; Kandasamy et al., 2017b). However, they work with single-ﬁdelity queries and are not easily extended to multi-ﬁdelity optimization tasks. Takeno et al. (2019) proposed two batch querying strategies for their MF-BO framework. Both strategies 6leverage the property that the covariance of a conditional Gaussian does not rely on the values of the conditioned variables; so, there is no need to worry about conditioning on function values that are still in query. The asynchronous version generates new queries conditioned on different sets of function values in query (asynchronously). However, if the conditional parts are signiﬁcantly overlapping, which might not be uncommon in practice, there is a risk of generating redundant or even collapsed queries. Takeno et al. (2019) also talked about a synchronous version. While they discussed how to compute the information gain between the function maximum and a batch of function values, they did not provide an effective way to optimize it with the multi-ﬁdelity querying costs. Instead, they suggested a simple heuristics to sequentially ﬁnd each query by conditioning on the generated ones. However, there is no guarantee about this heuristics. While in our experiments, we mainly use hyperparameter optimization to evaluate our multi-ﬁdelity BO approach, there are many other excellent works speciﬁcally designed for hyperparameter tuning or selection, e.g., the non-Bayesian, random search based method Hyberband (Li et al., 2017) which also reﬂects the multi-ﬁdelity idea: it starts using few training iterations/epochs (low ﬁdelity) to evaluate many candidates, rank them, iteratively selects the top-ranked ones, and further evaluate them with more iterations/epochs (high ﬁdelity). BOHB (Falkner et al., 2018) is a hybrid of KDE based BO (Bergstra et al., 2011) and Hyperband. Li et al. (2018) further developed an asynchronous successive halving algorithm for parallel random search over hyperparameters. Domhan et al. (2015); Klein et al. (2017b) propose to estimate the learning curves, and early halt the evaluation of ominous hyperparameters according to the learning curve predictions. Swersky et al. (2014) introduced a kernel about the training steps, and developed Freeze-thaw BO (Swersky et al., 2014) that can temporarily pause the model training and explore several promising hyperparameter settings for a while and then continue on to the most promising one. The work in (Klein et al., 2017a) jointly estimates the cost as a function of the data size and training steps, which can be viewed as continuous ﬁdelities, like in (Kandasamy et al., 2017a; Wu and Frazier, 2017). 6 Experiment 6.1 Surrogate Learning Performance We ﬁrst examined if BMBO-DARN can learn a more accurate surrogate of the objective. We used two popular benchmark functions: (1) Levy (Laguna and Martí, 2005) with two-ﬁdelity evaluations, and (2) Branin (Forrester et al., 2008; Perdikaris et al., 2017) with three-ﬁdelity evaluations. Throughout different ﬁdelities are nonlinear/nonstationary transforms. We provide the details in the Appendix. Levy nRMSE MNLL MF-GP-UCB 0.831 ± 0.195 1 .824 ± 0.276 MF-MES 0.581 ± 0.032 1 .401 ± 0.031 SHTL 0.443 ± 0.009 1 .208 ± 0.026 DNN-MFBO 0.365 ± 0.035 1 .081 ± 0.011 BMBO-DARN 0.348 ± 0.021 1 .072 ± 0.016 Branin MF-GP-UCB 0.846 ± 0.147 1 .976 ± 0.208 MF-MES 0.719 ± 0.099 1 .796 ± 0.128 SHTL 0.835 ± 0.218 1 .958 ± 0.646 DNN-MFBO 0.182 ± 0.022 0 .973 ± 0.013 BMBO-DARN 0.158 ± 0.016 0 .965 ± 0.005 Table 1: Surrogate learning performance on Branin function with three-ﬁdelity training examples and Levy function with two-ﬁdelity examples: normalized root- mean-square-error (nRMSE) and mean-negative-log- likelihood (MNLL). The results were averaged over ﬁve runs. Methods. We compared with the following multi-ﬁdelity learning models used in the state- of-the-art BO methods: (1) MF-GP-UCB (Kan- dasamy et al., 2016) that learns an independent GP for each ﬁdelity. (2) MF-MES (Takeno et al., 2019) that uses a multi-output GP with a linear correlation structure across different outputs (ﬁ- delities), (3) Scalable Hyperparameter Transfer Learning (SHTL) (Perrone et al., 2018) that uses an NN to generate latent bases shared by all the tasks (ﬁdelities) and predicts the output of each task with a linear combination of the bases. (4) Deep Neural Network Multi-Fidelity BO (DNN- MFBO) (Li et al., 2020) that uses a chain of NNs to model each ﬁdelity, but only estimates the relationship between successive ﬁdelities. Settings. We implemented our model with PyTorch (Paszke et al., 2019) and HMC sam- pling based on the Hamiltorch library (Cobb and Jalaian, 2021) (https://github.com/ AdamCobb/hamiltorch). For each ﬁdelity, we used two hidden layers with 40 neurons and tanh activation. We ran HMC for 5K steps to reach burn in (by looking at the trace plots) and then produced 200 posterior samples with every 10 steps. To generate each sample proposal, we ran 10 leapfrog steps, and the step size was chosen as 0.012. 7We implemented DNN-MFBO and SHTL with PyTorch as well. For DNN-MFBO, we used the same NN architecture as in BMBO-DARN for each ﬁdelity, and ran HMC with the same setting for model estimation. For SHTL, we used two hidden layers with 40 neurons and an output layer with 32 neurons to generate the shared bases. We used ADAM (Kingma and Ba, 2014) to estimate the model parameters, and the learning rate was chosen from {10−4,5 ×10−4,10−3,5 ×10−3,10−2}. We ran 1K epochs, which are enough for convergence. Note that we also attempted to use L-BFGS to train SHTL, but it often runs into numerical issues. ADAM is far more stable. We used a Python implementation of MF-MES and MF-GP-UCB, both of which use the RBF kernel (consistent with the original papers). Results. We randomly generated{130,65}examples for Levy function at the two increasing ﬁdelities, and {320,130,65}examples for Branin function at its three increasing ﬁdelities. After training, we examined the prediction accuracy of all the models with 100 test samples uniformly sampled from the input space. We calculated the normalized root-mean-square-error (nRMSE) and mean- negative-log-likelihood (MNLL). We repeated the experiment for 5 times, and report their average and standard deviations in Table. 1. As we can see, for both benchmark functions, BMBO-DARN outperforms all the competing models, conﬁrming the advantage of our deep auto-regressive model in surrogate learning. Note that despite using a similar chain structure, DNN-MFBO is still inferior to BMBO-DARN, implying that our fully auto-regressive modeling (see (3)) can better estimate the relationships between the ﬁdelities to facilitate surrogate estimation. 6.2 Real-World Applications Next, we used BMBO-DARN to optimize the hyperparameters of four popular machine learning models: Convolutional Neural Networks (CNN) (Fukushima and Miyake, 1982; LeCun et al., 1990) for image classiﬁcation, Online Latent Dirichlet Allocation (LDA) (Hoffman et al., 2010) for text mining, XGBoost (Chen and Guestrin, 2016) for diabetes diagnosis, and Physics-Informed Neural Networks (PINN) (Raissi et al., 2019) for solving partial differential equations (PDE). Methods and Setting. We compared with the state-of-the-art multi-ﬁdelity BO algorithms men- tioned in Sec. 6.1, (1) MF-GP-UCB, (2) MF-MES, (3) SHTL, and (4) DNN-MFBO. In ad- dition, we compared with (5) MF-MES-Batch (Takeno et al., 2019), the (asynchronous) paral- lel version of MF-MES, (6) SF-Batch (Kandasamy et al., 2017b) ( https://github.com/ kirthevasank/gp-parallel-ts), a single-ﬁdelity GP-based BO that optimizes posterior samples of the objective function to obtain a batch of queries, (7) SMAC3 ( https://github. com/automl/SMAC3), BO based on random forests, (8) Hyperband (Li et al., 2017) ( https: //github.com/automl/HpBandSter) that conducts multi-ﬁdelity random search over the hy- perparameters, (9) BOHB (Falkner et al., 2018) that uses Tree Parzen Estimator (TPE) (Bergstra et al., 2011) to generate hyperparameter candidates in Hyperband iterations. We also tested our method that queries at one input and ﬁdelity each time (B = 1), which we denote by BMBO-DARN-1. We used the same setting as in Sec. 6.1 for all the multi-ﬁdelity methods, except that for SHTL, we ran 2K epochs in surrogate training to ensure the convergence. For our method, we set the maximum number of iterations in optimizing the batch acquisition function (see Algorithm 8) to 100 and tolerance level to 10−3. For the remaining methods, e.g., SMAC3 and Hyperband, we used their original implementations and default settings. For all the batch querying methods, we set the batch size to 5. All the single ﬁdelity methods queried at the highest ﬁdelity. Convolutional Neural Network (CNN). Our ﬁrst application is to train a CNN for image classiﬁca- tion. We used CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html), from which we used 10K images for training and another 10K for evaluation. To optimize the hyperparameters, we considered three ﬁdelities, i.e., training with 1, 10, 50 epochs. We used the average negative log-loss (nLL) to evaluate the prediction accuracy of each method. We considered optimizing the following hyperparameters: # convolutional layers ranging from [1,4], # channels in the ﬁrst ﬁlter ([8, 136]), depth of the dense layers ([1, 8]), width of the dense layers ([32, 2080]), pooling type ([“max”, “average”]), and dropout rate ([10−3, 0.99]). We optimized the dropout rate in the log domain, and used a continuous relaxation of the discrete parameters. Initially, we queried at 10 random hyperparameter settings at each ﬁdelity. All the methods started with these evaluation results and repeatedly identiﬁed new hyperparameters. We used the average running time at each training ﬁdelity as the cost: λ1 : λ2 : λ3 = 1 : 10 : 50 . After each query, we evaluated the performance of the new hyperparameters at the highest 80 1000 2000 3000 4000 5000 6000 7000 8000 Accumulated Cost (Time in seconds) 1.0 1.5 2.0Negative Log Loss (a) CNN 0 200 400 600 800 1000 1200 Accumulated Cost (Time in seconds) 600 800 1000 1200 1400Perplexity (b) Online LDA 0 5 10 15 20 25 30 35 Accumulated Cost (Time in seconds) −0.4 −0.3 −0.2 −0.1Log nRMSE (c) XGBoost 0 1000 2000 3000 4000 5000 6000 7000 8000 Accumulated Cost (Time in seconds) −6 −4 −2 0 Log nRMSE (d) PINN Figure 2: Performance vs. accumulated cost (running time) in Hyperparameter optimization tasks. For fairness, all the batch methods queried new examples sequentially, i.e., no parallel querying was employed. The results were averaged over ﬁve runs. Note that MF-GP-UCB, MF-MES and MF-MES-Batch often obtained very close results and their curves overlap much. training level. We ran each method until 100 queries were issued. We repeated the exper- iment for 5 times and in Fig. 2a report the average accuracy (nLL) and its standard devi- ation for the hyperparameters found by each method throughout the optimization procedure. 0 250 500 750 1000 1250 1500 1750Time(seconds)-Per-Query BMBO-DARN BMBO-DARN-1 DNN-MFBO MF-GP-UCB MF-MES MF-MES-Batch    MF-GP-UCB MF-MES MF-MES-Batch    (a) CNN 0 250 500 750 1000 1250 1500 1750Time(seconds)-Per-Query (b) Online LDA Figure 3: Average time to generate queries (including surrogate training). Online Latent Dirichlet Allocation (LDA). Our second task is to train online LDA (Hoffman et al., 2010) to extract topics from 20NewsGroups corpus ( http://qwone.com/~jason/ 20Newsgroups/). We used 5K documents for training, and 2K for evaluation. We used the implement from the scikit-learn library (https: //scikit-learn.org/stable/). We considered optimizing the following hyperparam- eters: document topic prior α ∈ [10−3,1], topic word prior η ∈ [10−3,1], learn- ing decay κ ∈ [0.51,1], learning offset τ0 ∈[1,2,5,10,20,50,100,200], E-step stopping tolerance ϵ∈[10−5,10−1], document batch size in [2,4,8,16,32,64,128,256], and topic number K ∈[1,64]. We optimized α, η, κand ϵin the log domain, and used a continuous relaxation of the discrete parameters. We considered three ﬁdelities — training with 1, 10 and 50 epochs, and randomly queried 10 examples at each ﬁdelity to start each method. We evaluated the performance of the selected hyperparameters in terms of perplexity (the smaller, the better). In Fig. 2 b, we reported the average perplexity (and its standard deviation) of each method after ﬁve runs of the hyperparameter optimization. XGBoost. Third, we trained an XGBoost model (Chen and Guestrin, 2016) to predict a quantitative measure of the diabetes progression ( https://archive.ics.uci.edu/ml/datasets/ diabetes). The dataset includes 442 examples. We used two-thirds for training and the remaining one-third for evaluation. We used the implementation from the scikit-learn library. We optimized the following hyperparameters: Huber loss parameter α∈[0.01,0.1], the non-negative complexity pruning parameter ([0.01,100]), fraction of samples used to ﬁt individual base learners ( [0.1,1]), 9fraction of features considered to split the tree ( [0.01,1]), splitting criterion ([“MAE”, “MSE”]), minimum number of samples required to split an internal node ( [2,9]), and the maximum depth of individual trees ( [1,16]). The hyperparameter space is 12 dimensional. We considered three ﬁdelities — training XGBoost with 2, 10 and 100 weak learners (trees). The querying cost is therefore λ1 : λ2 : λ3 = 1 : 5 : 50. We started with 10 random queries at each ﬁdelity. We used the log of nRMSE to evaluate the performance. We ran 5 times and report the average log-nRMSE of the identiﬁed hyperparameters by each method in Fig. 2c. Physics-informed Neural Networks (PINN). Our fourth application is to learn a PINN to solve PDEs (Raissi et al., 2019). The key idea of PINN is to use boundary points to construct the training loss, and meanwhile use a set of collocation points in the domain to regularize the NN solver to respect the PDE. With appropriate choices of hyperparameters, PINNs can obtain very accurate solutions. We used PINNs to solve Burger’s equation (Morton and Mayers, 2005) with the viscosity 0.01/π. The solution becomes sharper with bigger time variables (see the Appendix) and hence the learning is quite challenging. We followed (Raissi et al., 2019) to use fully connected networks and L-BFGS for training. The hyperparameters include NN depth ([1,8]), width ([1,64]), and activations (8 choices: Relu, tanh, sigmoid, their variants, etc.). Following (Raissi et al., 2019), we used 100 boundary points as the training set and 10K collocation points for regularization. We used 10K points for evaluation. We chose 3 training ﬁdelities, running L-BFGS with 10, 100, 50K maximum iterations. The querying cost (average training time) is λ1 : λ2 : λ3 = 1 : 10 : 50 . Note that in ﬁdelity 3, L-BFGS usually converged before running 50K iterations. We initially issued 10 random queries at each ﬁdelity. We ran each method for 5 times and reported the average log nRMSE after each step in Fig. 2d. Results. As we can see, in all the applications, BMBO-DARN used the smallest cost (i.e., running time) to ﬁnd the hyperparameters that gives the best learning performance. In general, BMBO-DARN identiﬁed better hyperparameters with the same cost, or equally good hyperparameters with the smallest cost. BMBO-DARN-1 outperformed all the one-by-one querying methods, except that for online LDA (Fig. 2b) and PINN (Fig. 2d), it was worse than DNN-MFBO and Hyperband at the early stage, but ﬁnally obtained better learning performance. We observed that the GP based baselines (MF-MES, MF-GP-UCB, SF-Batch, etc.) are often easier to be stuck in suboptimal hyperparameters, this might because these models are not effective enough to integrate information of multiple ﬁdelities to obtain a good surrogate. Together these results have shown the advantage of our method, especially in our batch querying strategy. Finally, we show the average query generation time of BMBO-DARN for CNN and Online LDA in Fig. 3 (including surrogate training). It turns out BMBO-DARN spends much less time than MF-MES using the global optimization method DIRECT (Jones et al., 1998), and comparable to MF-GP-UCB and DNN-MFBO. Therefore, BMBO-DARN is efﬁcient to update the surrogate model and generate new queries. 7 Conclusion We have presented BMBO-DARN, a batch multi-ﬁdelity Bayesian optimization method. Our deep auto-regressive model can serve as a better surrogate of the black-box objective. Our batch query- ing method not only is efﬁcient, avoiding combinatorial search over discrete ﬁdelities, but also signiﬁcantly reduces the cost while improving the optimization performance. Acknowledgments This work has been supported by MURI AFOSR grant FA9550-20-1-0358. References Auer, P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397–422. Baptista, R. and Poloczek, M. (2018). Bayesian optimization of combinatorial structures. In International Conference on Machine Learning, pages 462–471. Bergstra, J., Bardenet, R., Bengio, Y ., and Kégl, B. (2011). Algorithms for hyper-parameter op- timization. In 25th annual conference on neural information processing systems (NIPS 2011), 10volume 24. Neural Information Processing Systems Foundation. Chen, T., Fox, E., and Guestrin, C. (2014). Stochastic gradient Hamiltonian Monte Carlo. In International conference on machine learning, pages 1683–1691. PMLR. Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm SigKDD international conference on knowledge discovery and data mining, pages 785–794. Chung, T. (2010). Computational ﬂuid dynamics. Cambridge university press. Cobb, A. D. and Jalaian, B. (2021). Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting. Uncertainty in Artiﬁcial Intelligence. Cutajar, K., Pullin, M., Damianou, A., Lawrence, N., and González, J. (2019). Deep gaussian processes for multi-ﬁdelity modeling. arXiv preprint arXiv:1903.07320. Daxberger, E., Makarova, A., Turchetta, M., and Krause, A. (2019). Mixed-variable Bayesian optimization. arXiv preprint arXiv:1907.01329. Domhan, T., Springenberg, J. T., and Hutter, F. (2015). Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-fourth international joint conference on artiﬁcial intelligence. Falkner, S., Klein, A., and Hutter, F. (2018). BOHB: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning, pages 1437–1446. PMLR. Forrester, A., Sobester, A., and Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons. Fukushima, K. and Miyake, S. (1982). Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267–285. Springer. Garrido-Merchán, E. C. and Hernández-Lobato, D. (2020). Dealing with categorical and integer- valued variables in Bayesian optimization with gaussian processes. Neurocomputing, 380:20–35. González, J., Dai, Z., Hennig, P., and Lawrence, N. (2016). Batch Bayesian optimization via local penalization. In Artiﬁcial intelligence and statistics, pages 648–657. PMLR. Hennig, P. and Schuler, C. J. (2012). Entropy search for information-efﬁcient global optimization. Journal of Machine Learning Research, 13(Jun):1809–1837. Hernández-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. (2014). Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in neural information processing systems, pages 918–926. Hernández-Lobato, J. M., Requeima, J., Pyzer-Knapp, E. O., and Aspuru-Guzik, A. (2017). Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In International conference on machine learning, pages 1470–1479. PMLR. Hoffman, M., Bach, F. R., and Blei, D. M. (2010). Online learning for latent dirichlet allocation. In advances in neural information processing systems, pages 856–864. Citeseer. Huang, D., Allen, T. T., Notz, W. I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382. Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492. Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., and Póczos, B. (2016). Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems, pages 992–1000. 11Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. (2017a). Multi-ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1799–1808. JMLR. org. Kandasamy, K., Krishnamurthy, A., Schneider, J., and Poczos, B. (2017b). Asynchronous parallel Bayesian optimisation via Thompson sampling. arXiv preprint arXiv:1705.09236. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. (2017a). Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536. PMLR. Klein, A., Falkner, S., Springenberg, J. T., and Hutter, F. (2017b). Learning curve prediction with Bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Kutluay, S., Bahadir, A., and Özdecs, A. (1999). Numerical solution of one-dimensional burgers equation: explicit and exact-explicit ﬁnite difference methods. Journal of Computational and Applied Mathematics, 103(2):251–261. Laguna, M. and Martí, R. (2005). Experimental testing of advanced scatter search designs for global optimization of multimodal functions. Journal of Global Optimization, 33(2):235–255. Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdelity optimization using statistical surrogate modeling for non-hierarchical information sources. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, page 0143. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. (1990). Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pages 396–404. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. (2017). Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765–6816. Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., and Talwalkar, A. (2018). Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934. Li, S., Xing, W., Kirby, R., and Zhe, S. (2020). Multi-ﬁdelity Bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems. Liu, D. C. and Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528. McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practical Bayesian optimization for variable cost objectives. arXiv preprint arXiv:1703.04335. Mockus, J. (2012). Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media. Mockus, J., Tiesis, V ., and Zilinskas, A. (1978). The application of Bayesian methods for seeking the extremum. Towards global optimization, 2(117-129):2. Morton, K. W. and Mayers, D. F. (2005). Numerical solution of partial differential equations: an introduction. Cambridge university press. Nagel, K. (1996). Particle hopping models and trafﬁc ﬂow theory. Physical review E, 53(5):4655. Neal, R. M. et al. (2011). Mcmc using Hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2. 12Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high- performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc. Perdikaris, P., Raissi, M., Damianou, A., Lawrence, N., and Karniadakis, G. E. (2017). Nonlinear information fusion algorithms for data-efﬁcient multi-ﬁdelity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751. Perrone, V ., Jenatton, R., Seeger, M. W., and Archambeau, C. (2018). Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855. Picheny, V ., Ginsbourger, D., Richet, Y ., and Caplin, G. (2013). Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, 55(1):2–13. Poloczek, M., Wang, J., and Frazier, P. (2017). Multi-information source optimization. In Advances in Neural Information Processing Systems, pages 4288–4298. Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2017). Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561. Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707. Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. Shah, A., Xing, W., and Triantafyllidis, V . (2017). Reduced-order modelling of parameter-dependent, linear and nonlinear dynamic partial differential equation models. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2200):20160809. Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., Prabhat, M., and Adams, R. (2015). Scalable Bayesian optimization using deep neural networks. In International conference on machine learning, pages 2171–2180. Song, J., Chen, Y ., and Yue, Y . (2019). A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167. Springenberg, J. T., Klein, A., Falkner, S., and Hutter, F. (2016). Bayesian optimization with robust Bayesian neural networks. In Advances in neural information processing systems, volume 29, pages 4134–4142. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010). Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1015–1022. Sugimoto, N. (1991). Burgers equation with a fractional derivative; hereditary effects on nonlinear acoustic waves. Journal of ﬂuid mechanics, 225:631–653. Swersky, K., Snoek, J., and Adams, R. P. (2013). Multi-task Bayesian optimization. In Advances in neural information processing systems, pages 2004–2012. Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896. Takeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T., Shiga, M., Takeuchi, I., and Karasuyama, M. (2019). Multi-ﬁdelity Bayesian optimization with max-value entropy search. arXiv preprint arXiv:1901.08275. 13Wang, Z. and Jegelka, S. (2017). Max-value entropy search for efﬁcient Bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 3627– 3635. JMLR. org. Wu, J. and Frazier, P. (2016). The parallel knowledge gradient method for batch Bayesian optimization. Advances in Neural Information Processing Systems, 29:3126–3134. Wu, J. and Frazier, P. I. (2017). Continuous-ﬁdelity Bayesian optimization with knowledge gradient. In NIPS Workshop on Bayesian Optimization. Zhang, Y ., Hoang, T. N., Low, B. K. H., and Kankanhalli, M. (2017). Information-based multi-ﬁdelity Bayesian optimization. In NIPS Workshop on Bayesian Optimization. Appendix 8 Synthetic Benchmark Functions 8.1 Branin Function The input is two dimensional, x = [x1,x2] ∈[−5,10] ×[0,15]. We have three ﬁdelities to evaluate the function, which, from high to low, are given by f3(x) = − (−1.275x2 1 π2 + 5x1 π + x2 −6 )2 − ( 10 − 5 4π ) cos(x1) −10, f2(x) = −10 √ −f3(x−2) −2(x1 −0.5) + 3(3x2 −1) + 1, f1(x) = −f2 ( 1.2(x + 2) ) + 3x2 −1. (9) We can see that between ﬁdelities are nonlinear transformations, nonuniform scaling, and shifts. 8.2 Levy Function The input is two dimensional, x = [x1,x2] ∈[−10,10]2. We have two ﬁdelities, f2(x) = −sin2(3πx1) −(x1 −1)2[1 + sin2(3πx2)] −(x2 −1)2[1 + sin2(2πx2)], f1(x) = − √ 1 + f2 2 (x). (10) 9 Details about Physics Informed Neural Networks Burgers’ equation is a canonical nonlinear hyperbolic PDE, and widely used to characterize a variety of physical phenomena, such as nonlinear acoustics (Sugimoto, 1991), ﬂuid dynamics (Chung, 2010), and trafﬁc ﬂows (Nagel, 1996). Since the solution can develop discontinuities (i.e., shock waves) based on a normal conservation equation, Burger’s equation is often used as a nontrivial benchmark test for numerical solvers and surrogate models (Kutluay et al., 1999; Shah et al., 2017; Raissi et al., 2017). We used physics informed neural networks (PINN) to solve the viscosity version of Burger’s equation, ∂u ∂t + u∂u ∂x = ν∂2u ∂x2 , (11) where uis the volume, xis the spatial location, tis the time, and ν is the viscosity. Note that the smaller ν, the sharper the solution of u. In our experiment, we set ν = 0.01 π , x ∈[−1,1], and t∈[0,1]. The boundary condition is given by u(0,x) = −sin(πx), u(t,−1) = u(t,1) = 0. We use an NNuWto represent the solution. To estimate the NN, we collectedN training points in the boundary, D= {(ti,xi,ui)}N i=1, and M collocation (input) points in the domain, C= {(ˆti,ˆxi)}M i=1. We then minimize the following loss function to estimate uW, L(W) = 1 N N∑ i=1 (uW(ti,xi) −ui)2 + 1 M M∑ i=1 (⏐⏐ψ(uW)(ˆti,ˆxi) ⏐⏐2) , 14where ψ(·) is a functional constructed from the PDE, ψ(u) = ∂u ∂t + u∂u ∂x −ν∂2u ∂x2 . Obviously, the loss consists of two terms, one is the training loss, and the other is a regularization term that enforces the NN solution to respect the PDE. 15",
      "meta_data": {
        "arxiv_id": "2106.09884v2",
        "authors": [
          "Shibo Li",
          "Robert M. Kirby",
          "Shandian Zhe"
        ],
        "published_date": "2021-06-18T02:55:48Z",
        "pdf_url": "https://arxiv.org/pdf/2106.09884v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Batch Multi-fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN) to optimize expensive-to-evaluate black-box functions by leveraging multiple fidelities. Its main contributions are: 1) A deep auto-regressive model using Bayesian neural networks to capture complex, strong, nonstationary, and highly nonlinear relationships across all fidelities, improving surrogate learning. 2) A simple yet efficient batch querying method based on the Max-value Entropy Search (MES) principle, which explicitly penalizes highly correlated queries and encourages diversity, avoiding combinatorial search over fidelities. 3) Efficient computation of the batch acquisition function using posterior samples and moment matching. 4) An alternating optimization algorithm that guarantees improvement at each step to maximize the acquisition function for a batch of inputs and fidelities. 5) Demonstrated superior performance on four real-world hyperparameter optimization tasks compared to state-of-the-art multi-fidelity BO algorithms.",
        "methodology": "BMBO-DARN employs a deep auto-regressive model for multi-fidelity surrogate learning and a novel batch acquisition strategy. For the surrogate model, it uses a chain of M Bayesian Neural Networks (BNNs), one for each fidelity. The input to the BNN for fidelity 'm' includes the original input 'x' and the outputs of all preceding fidelities (f1(x), ..., fm-1(x)), allowing flexible capture of complex inter-fidelity relationships. Standard Gaussian priors are placed on NN parameters and Gamma priors on noise precisions. Posterior inference is performed using Hamiltonian Monte Carlo (HMC) sampling. For batch querying, a batch acquisition function based on the Max-value Entropy Search (MES) principle is proposed. This function quantifies the mutual information between the batch of query outputs and the function optimum, normalized by the total query cost, to balance benefit and cost while encouraging diversity. To compute this function efficiently, posterior samples of NN weights are drawn, and moment matching is used to approximate the joint posterior of the batch outputs and the function optimum as a multivariate Gaussian distribution. Maximizing this acquisition function for mixed continuous inputs and discrete fidelities is done via an alternating optimization algorithm, where each input-fidelity pair in the batch is cyclically updated while others are fixed, avoiding combinatorial search and guaranteeing improvement at each step.",
        "experimental_setup": "The evaluation of BMBO-DARN was conducted in two phases: surrogate learning performance and real-world hyperparameter optimization. For surrogate learning, two benchmark functions were used: Levy (two-fidelity) and Branin (three-fidelity), which feature nonlinear/nonstationary transformations between fidelities. Performance was measured using normalized root-mean-square-error (nRMSE) and mean-negative-log-likelihood (MNLL) on 100 test samples. Training data consisted of randomly generated examples (Levy: {130, 65}; Branin: {320, 130, 65} across fidelities). Comparative models included MF-GP-UCB, MF-MES, SHTL, and DNN-MFBO. For real-world applications, BMBO-DARN was applied to hyperparameter optimization of four machine learning models: 1) Convolutional Neural Networks (CNN) for CIFAR-10 image classification (3 fidelities: 1, 10, 50 epochs, cost ratio 1:10:50, metric: negative log-loss). 2) Online Latent Dirichlet Allocation (LDA) for 20NewsGroups text mining (3 fidelities: 1, 10, 50 epochs, metric: perplexity). 3) XGBoost for diabetes diagnosis (3 fidelities: 2, 10, 100 weak learners, cost ratio 1:5:50, metric: log nRMSE). 4) Physics-Informed Neural Networks (PINN) for solving Burger's equation (3 fidelities: 10, 100, 50K L-BFGS max iterations, cost ratio 1:10:50, metric: log nRMSE). Comparison methods included MF-GP-UCB, MF-MES, SHTL, DNN-MFBO, MF-MES-Batch, SF-Batch, SMAC3, Hyperband, BOHB, and BMBO-DARN-1 (B=1). All batch methods used a batch size of 5. Experiments were repeated 5 times, starting with 10 random initial hyperparameter settings at each fidelity, and run until 100 queries were issued.",
        "limitations": "The paper does not explicitly list limitations of BMBO-DARN itself, but rather emphasizes its advantages over existing methods by highlighting their shortcomings. Implicit limitations of BMBO-DARN could include: 1) The use of Hamiltonian Monte Carlo (HMC) for posterior inference, while providing high-quality uncertainty quantification, can be computationally intensive, although the paper notes the flexibility to switch to approximate inference methods like stochastic gradient HMC. 2) The moment matching technique used to approximate the posterior distribution for efficient acquisition function computation is an approximation and may not perfectly capture the true posterior. 3) The alternating optimization algorithm for maximizing the batch acquisition function guarantees improvement at each step but does not guarantee convergence to a global optimum for the batch.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
      "abstract": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have\nrecently emerged as a promising direction for tuning Deep Learning methods.\nHowever, existing methods suffer from a sub-optimal allocation of the HPO\nbudget to the hyperparameter configurations. In this work, we introduce DyHPO,\na Bayesian Optimization method that learns to decide which hyperparameter\nconfiguration to train further in a dynamic race among all feasible\nconfigurations. We propose a new deep kernel for Gaussian Processes that embeds\nthe learning curve dynamics, and an acquisition function that incorporates\nmulti-budget information. We demonstrate the significant superiority of DyHPO\nagainst state-of-the-art hyperparameter optimization methods through\nlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) and\ndiverse architectures (MLP, CNN/NAS, RNN).",
      "full_text": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations Martin Wistuba∗† Amazon Web Services, Berlin, Germany marwistu@amazon.com Arlind Kadra∗ University of Freiburg, Freiburg, Germany kadraa@cs.uni-freiburg.de Josif Grabocka University of Freiburg, Freiburg, Germany grabocka@cs.uni-freiburg.de Abstract Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO bud- get to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter config- uration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hy- perparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN). 1 Introduction Hyperparameter Optimization (HPO) is arguably an acute open challenge for Deep Learning (DL), especially considering the crucial impact HPO has on achieving state-of-the-art empirical results. Unfortunately, HPO for DL is a relatively under-explored field and most DL researchers still optimize their hyperparameters via obscure trial-and-error practices. On the other hand, traditional Bayesian Optimization HPO methods [Snoek et al., 2012, Bergstra et al., 2011] are not directly applicable to deep networks, due to the infeasibility of evaluating a large number of hyperparameter configurations. In order to scale HPO for DL, three main directions of research have been recently explored. (i) Online HPO methods search for hyperparameters during the optimization process via meta-level controllers [Chen et al., 2017, Parker-Holder et al., 2020], however, this online adaptation can not accommodate all hyperparameters (e.g. related to architectural changes). (ii) Gradient-based HPO techniques, on the other hand, compute the derivative of the validation loss w.r.t. hyperparameters by reversing the training update steps [Maclaurin et al., 2015, Franceschi et al., 2017, Lorraine et al., 2020], however, the reversion is not directly applicable to all cases (e.g. dropout rate). The last direction, (iii) Gray-box HPO techniques discard sub-optimal configurations after evaluating them on lower budgets [Li et al., 2017, Falkner et al., 2018]. In contrast to the online and gradient-based alternatives, gray-box approaches can be deployed in an off-the-shelf manner to all types of hyperparameters and architectures. The gray-box concept is based on the intuition that a poorly-performing hyperparameter configuration can be identified and ∗equal contribution †work does not relate to position at Amazon 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2202.09774v2  [cs.LG]  1 Jun 2023terminated by inspecting the validation loss of the first few epochs, instead of waiting for the full convergence. The most prominent gray-box algorithm is Hyperband [Li et al., 2017], which is based on successive halving. It runs random configurations at different budgets (e.g. number of epochs) and successively halves these configurations by keeping only the top performers. Follow-up works, such as BOHB [Falkner et al., 2018] or DEHB [Awad et al., 2021], replace the random sampling of Hyperband with a sampling based on Bayesian optimization or differentiable evolution. Despite their great practical potential, gray-box methods suffer from a major issue. The low-budget (few epochs) performances are not always a good indicator for the full-budget (full convergence) performances. For example, a properly regularized network converges slower in the first few epochs, however, typically performs better than a non-regularized variant after the full convergence. In other words, there can be a poor rank correlation of the configurations’ performances at different budgets. Figure 1: Top: The learning curve for different hyperparameter configurations. The darker the learning curve, the later it was evaluated during the search. Bottom: The hyperparameter indices in a temporal order as evaluated during the opti- mization and their corresponding curves. We introduceDYHPO, a Bayesian Optimization (BO) approach based on Gaussian Processes (GP), that proposes a novel treatment to the multi-budget (a.k.a. multi-fidelity) setup. In this perspective, we propose a deep kernel GP that captures the learning dynamics. As a result, we train a kernel capable of capturing the similarity of a pair of hyperparameter configurations, even if the pair’s configurations are evaluated at differ- ent budgets. Furthermore, we extend Expected Improvement [Jones et al., 1998] to the multi- budget case, by introducing a new mechanism for the incumbent configuration of a budget. We illustrate the differences between our rac- ing strategy and successive halving with the experiment of Figure 1, where, we showcase the HPO progress of two different methods on the \"Helena\" dataset from the LCBench bench- mark [Zimmer et al., 2021]. Hyperband [Li et al., 2017] is a gray-box approach that stati- cally pre-allocates the budget for a set of candidates (Hyperband bracket) according to a predefined policy. However, DYHPO dynamically adapts the allocation of budgets for configurations after every HPO step (a.k.a. a dynamic race). As a result, DYHPO invests only a small budget on configurations that show little promise as indicated by the intermediate scores. The joint effect of modeling a GP kernel across budgets together with a dedicated acquisition function leads to DYHPO achieving a statistically significant empirical gain against state-of-the-art gray-box baselines [Falkner et al., 2018, Awad et al., 2021], including prior work on multi-budget GPs [Kandasamy et al., 2017, 2020] or neural networks [Li et al., 2020b]. We demonstrate the performance of DYHPO in three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets of three diverse modalities (tabular, image, natural language processing). We believe our method is a step forward toward making HPO for DL practical and feasible. Overall, our contributions can be summarized as follows: • We introduce a novel Bayesian surrogate for gray-box HPO optimization. Our novel surrogate model predicts the validation score of a machine learning model based on both the hyperparameter configuration, the budget information, and the learning curve. • We derive a simple yet robust way to combine this surrogate model with Bayesian opti- mization, reusing most of the existing components currently used in traditional Bayesian optimization methods. • Finally, we demonstrate the efficiency of our method for HPO and neural architecture search tasks compared to the current state-of-the-art methods in HPO, by outperforming seven strong HPO baselines with a statistically significant margin. As an overarching goal, we believe our method is an important step toward scaling HPO for DL. 22 Related Work on Gray-box HPO Multi-Fidelity Bayesian Optimization and Bandits. Bayesian optimization is a black-box func- tion optimization framework that has been successfully applied in optimizing hyperparameter and neural architectures alike [Snoek et al., 2012, Kandasamy et al., 2018, Bergstra et al., 2011]. To further improve Bayesian optimization, several works propose low-fidelity data approximations of hyperparameter configurations by training on a subset of the data [Swersky et al., 2013, Klein et al., 2017a], or by terminating training early [Swersky et al., 2014]. Additionally, several methods extend Bayesian optimization to multi-fidelity data by engineering new kernels suited for this problem [Swer- sky et al., 2013, 2014, Poloczek et al., 2017]. Kandasamy et al. [2016] extends GP-UCB [Srinivas et al., 2010] to the multi-fidelity setting by learning one Gaussian Process (GP) with a standard kernel for each fidelity. Their later work improves upon this method by learning one GP for all fidelities that enables the use of continuous fidelities [Kandasamy et al., 2017]. The work by Takeno et al. [2020] follows a similar idea but proposes to use an acquisition function based on information gain instead of UCB. While most of the works rely on GPs to model the surrogate function, Li et al. [2020b] use a Bayesian neural network that models the complex relationship between fidelities with stacked neural networks, one for each fidelity. Hyperband [Li et al., 2017] is a bandits-based multi-fidelity method for hyperparameter optimization that selects hyperparameter configurations at random and uses successive halving [Jamieson and Tal- walkar, 2016] with different settings to early-stop less promising training runs. Several improvements have been proposed to Hyperband with the aim to replace the random sampling of hyperparameter configurations with a more guided approach [Bertrand et al., 2017, Wang et al., 2018, Wistuba, 2017]. BOHB [Falkner et al., 2018] uses TPE [Bergstra et al., 2011] and builds a surrogate model for every fidelity adhering to a fixed-fidelity selection scheme. DEHB [Awad et al., 2021] samples candidates using differential evolution which handles large and discrete search spaces better than BOHB. Mendes et al. [2021] propose a variant of Hyperband which allows to skip stages. Learning Curve Prediction A variety of methods attempt to extrapolate a partially observed learning curve in order to estimate the probability that a configuration will improve over the current best solution. Domhan et al. [2015] propose to ensemble a set of parametric functions to extrapolate a partial learning curve. While this method is able to extrapolate with a single example, it requires a relatively long learning curve to do so. The work by Klein et al. [2017b] build upon the idea of using a set of parametric functions. The main difference is that they use a heteroscedastic Bayesian model to learn the ensemble weights. Baker et al. [2018] propose to use support vector machines (SVM) as an auto-regressive model. The SVM predicts the next value of a learning curve, the original learning curve is augmented by this value and we keep predicting further values. The work by Gargiani et al. [2019] use a similar idea but makes prediction based on the last K observations only and uses probabilistic models. Wistuba and Pedapati [2020] propose to learn a prediction model across learning curves from different tasks to avoid the costly learning curve collection. In contrast to DYHPO , none of these methods selects configuration but is limited to deciding when to stop a running configuration. Multi-Fidelity Acquisition Functions Klein et al. [2017a] propose an acquisition function which allows for selecting hyperparameter configurations and the dataset subset size. The idea is to reduce training time by considering only a smaller part of the training data. In contrast to EIMF, this acquisition function is designed to select arbitrary subset sizes whereas EIMF is intended to slowly increase the invested budget over time. Mendes et al. [2020] extend the work of Klein et al. [2017a] to take business constraints into account. Deep Kernel Learning with Bayesian Optimization. We are among the first to use deep kernel learning with Bayesian optimization and to the best of our knowledge the first to use it for multi- fidelity Bayesian optimization. Rai et al. [2016] consider the use of a deep kernel instead of a manually designed kernel in the context of standard Bayesian optimization, but, limit their experimentation to synthetic data and do not consider its use for hyperparameter optimization. Perrone et al. [2018], Wistuba and Grabocka [2021] use a pre-trained deep kernel to warm start Bayesian optimization with meta-data from previous optimizations. The aforementioned approaches are multi-task or transfer learning methods that require the availability of meta-data from related tasks. 3In contrast to prior work, we propose a method that introduces deep learning to multi-fidelity HPO with Bayesian Optimization, and captures the learning dynamics across fidelities/budgets, combined with an acquisition function that is tailored for the gray-box setup. 3 Dynamic Multi-Fidelity HPO 3.1 Preliminaries Gray-Box Optimization. The gray-box HPO setting allows querying configurations with a smaller budget compared to the total maximal budget B. Thus, we can query from the response function f : X ×N → R where fi,j = f(xi, j) is the response after spending a budget ofj on configuration xi. As before, these observations are noisy and we observe yi,j = f(xi, j) +εj where εj ∼ N(0, σ2 j,n). Please note, we assume that the budget required to query fi,j+b after having queried fi,j is only b. Furthermore, we use the learning curve Yi,j−1 = (yi,1, . . . , yi,j−1) when predicting fi,j. Gaussian Processes (GP). Given a training data set D = {(xi, yi)}n i=1, the Gaussian Process assumption is that yi is a random variable and the joint distribution of all yi is assumed to be multivariate Gaussian distributed as y ∼ N(m (X) , k(X, X)) . Furthermore, f∗ for test instances x∗ are jointly Gaussian with y as:\u0014 y f∗ \u0015 ∼ N \u0012 m (X, x∗) , \u0012 Kn K∗ KT ∗ K∗∗ \u0013\u0013 . (1) The mean function m is often set to 0 and its covariance function k depends on parameters θ. For notational convenience, we use Kn = k (X, X|θ) +σ2 nI, K∗ = k (X, X∗|θ) and K∗∗ = k (X∗, X∗|θ) to define the kernel matrices. We can derive the posterior predictive distribution with mean and covariance as follows: E[f∗|X, y, X∗] =KT ∗ K−1 n y, cov [f∗|X, X∗] =K∗∗ − KT ∗ K−1 n K∗ (2) Often, the kernel function is manually engineered, one popular example is the squared exponential kernel. However, in this work, we make use of the idea of deep kernel learning [Wilson et al., 2016]. The idea is to model the kernel as a neural network φ and learn the best kernel transformation K (θ, w) :=k(φ(x, w), φ(x′; w)|θ), which allows us to use convolutional operations in our kernel. 3.2 Deep Multi-Fidelity Surrogate We propose to use a Gaussian Process surrogate model that infers the value of fi,j based on the hyperparameter configuration xi, the budget j as well as the past learning curve Yi,j−1. For this purpose, we use a deep kernel as: K (θ, w) :=k(φ(xi, Yi,j−1, j; w), φ(xi′ , Yi′,j′−1, j′; w); θ) (3) xi j Yi,j−1 · 1B P P P Convolution max P P P Figure 2: The feature extractor φ of our kernel. We use a squared exponential kernel for k and the neural network φ is composed of linear and convolutional layers as shown in Figure 2. We normalize the budget j to a range between 0 and 1 by dividing it by the maximum budget B. Afterward, it is concatenated with the hyper- parameter configuration xi and fed to a linear layer. The learning curve Yi,j−1 is transformed by a one-dimensional convolution followed by a global max pooling layer. Finally, both repre- sentations are fed to another linear layer. Its output will be the input to the kernel function k. Both, the kernel k and the neural network φ consist of trainable parameters θ and w, respectively. We find their optimal values by computing the maximum likelihood estimates as: ˆθ, ˆw = arg max θ,w p(y|X, Y, θ, w) ∝ arg min θ,w yTK (θ, w)−1 y + log|K (θ, w)| (4) 4In order to solve this optimization problem, we use gradient descent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1. Given the maximum likelihood estimates, we can approximate the predictive posterior through p \u0010 fi,j|xi, Yi,j−1, j,D, ˆθ, ˆw \u0011 , and ultimately compute the mean and covariance of this Gaussian using Equation 2. 3.3 Multi-Fidelity Expected Improvement Expected improvement [Jones et al., 1998] is a commonly used acquisition function and is defined as: EI(x|D) =E[max {f(x) − ymax, 0}] , (5) where ymax is the largest observed value of f. We propose a multi-fidelity version of it as: EIMF(x, j|D) =E \u0002 max \b f(x, j) − ymax j , 0 \t\u0003 , (6) where: ymax j = \u001amax {y | ((x, ·, j), y) ∈ D} if ((x, ·, j), y) ∈ D max {y | (·, y) ∈ D} otherwise (7) Simply put, ymax j is the largest observed value of f for a budget of j if it exists already, otherwise, it is the largest observed value for any budget. If there is only one possible budget, the multi-fidelity expected improvement is identical to expected improvement. 3.4 The D YHPO Algorithm The DYHPO algorithm looks very similar to many black-box Bayesian optimization algorithms as shown in Algorithm 1. The big difference is that at each step we dynamically decide which candidate configuration to train for a small additional budget. Algorithm 1 DYHPO Algorithm 1: b(x) = 0∀x ∈ X 2: while not converged do 3: xi ← arg maxx∈X EIMF (x, b(x) + 1)(Sec. 3.3) 4: Observe yi,b(xi)+1. 5: b(xi) ← b(xi) + 1 6: D ← D ∪ \b ((xi, Yi,b(xi)−1, b(xi)), yi,b(xi)) \t 7: Update the surrogate on D. (Sec. 3.2) return xi with largest yi,·. Possible candidates are previously un- considered configurations as well as configurations that did not reach the maximum budget. In Line 2, the most promising candidate is chosen using the acquisition function introduced in Section 3.3 and the surrogate model’s predictions. It is important to high- light that we do not maximize the ac- quisition function along the budget di- mensionality. Instead, we set the bud- get b such that it is by exactly one higher than the budget used to eval- uate xi before. This ensures that we explore configurations by slowly increasing the budget. After the candidate and the corresponding budget are selected, the function f is evaluated and we observe yi,j (Line 3). This additional data point is added to D in Line 4. Then in Line 5, the surrogate model is updated according to the training scheme described in Section 3.2. 4 Experimental Protocol 4.1 Experimental Setup We evaluate DYHPO in three different settings on hyperparameter optimization for tabular, text, and image classification against several competitor methods, the details of which are provided in the following subsections. We ran all of our experiments on an Amazon EC2 M5 Instance (m5.xlarge). In our experiments, we report the mean of ten repetitions and we report two common metrics, the regret and the average rank. The regret refers to the absolute difference between the score of the solution found by an optimizer compared to the best possible score. If we report the regret as an aggregate result over multiple datasets, we report the mean over all regrets. The average rank is the metric we use to aggregate rank results over different datasets. We provide further implementation and training details in Appendix A.4. Our implementation of DYHPO is publicly available.3 3https://github.com/releaunifreiburg/DyHPO 54.2 Benchmarks In our experiments, we use the following benchmarks. We provide more details in Appendix A.1. LCBench: A learning curve benchmark [Zimmer et al., 2021] that evaluates neural network architec- tures for tabular datasets. LCBench contains learning curves for 35 different datasets, where 2,000 neural networks per dataset are trained for 50 epochs with Auto-PyTorch. TaskSet: A benchmark that features diverse tasks Metz et al. [2020] from different domains and includes 5 search spaces with different degrees of freedom, where, every search space includes 1000 hyperparameter configurations. In this work, we focus on a subset of NLP tasks (12 tasks) and we use the Adam8p search space with 8 continuous hyperparameters. NAS-Bench-201: A benchmark consisting of 15625 hyperparameter configurations representing different architectures on the CIFAR-10, CIFAR-100 and ImageNet datasets Dong and Yang [2020]. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. 4.3 Baselines Random Search: A random/stochastic black-box search method for HPO. HyperBand: A multi-arm bandit method that extends successive halving by multiple brackets with different combinations of the initial number of configurations, and their initial budget [Li et al., 2017]. BOHB: An extension of Hyperband that replaces the random sampling of the initial configurations for each bracket with recommended configurations from a model-based approach [Falkner et al., 2018]. BOHB builds a model for every fidelity that is considered. DEHB: A method that builds upon Hyperband by exploiting differential evolution to sample the initial candidates of a Hyperband bracket [Awad et al., 2021]. ASHA: An asynchronous version of successive halving (or an asynchronous version of Hyperband if multiple brackets are run). ASHA Li et al. [2020a] does not wait for all configurations to finish inside a successive halving bracket, but, instead promotes configurations to the next successive halving bracket in real-time. MF-DNN: A multi-fidelity Bayesian optimization method that uses deep neural networks to capture the relationships between different fidelities Li et al. [2020b]. Dragonfly: We compare against BOCA [Kandasamy et al., 2017] by using the Dragonfly library Kan- dasamy et al. [2020]. This method suggests the next hyperparameter configuration as well as the budget it should be evaluated for. 4.4 Research Hypotheses and Associated Experiments Hypothesis 1: DYHPO achieves state-of-the-art results in multi-fidelity HPO. Experiment 1: We compare DYHPO against the baselines of Section 4.3 on the benchmarks of Section 4.2 with the experimental setup of Section 4.1. For TaskSet we follow the authors’ recommendation and report the number of steps (every 200 iterations). Hypothesis 2: DYHPO’s runtime overhead has a negligible impact on the quality of results. Experiment 2: We compare DYHPO against the baselines of Section 4.3 over the wallclock time. The wallclock time includes both (i) the optimizer’s runtime overhead for recommending the next hyperparameter configuration, plus (ii) the time needed to evaluate the recommended configuration. In this experiment, we consider all datasets where the average training time per epoch is at least 10 seconds, because, for tasks where the training time is short, there is no practical justification for complex solutions and their overhead. In these cases, we recommend using a random search. We don’t report results for TaskSet because the benchmark lacks training times. Hypothesis 3: DYHPO uses the computational budget more efficiently than baselines. Experiment 3: To further verify that DYHPO is efficient compared to the baselines, we investigate whether competing methods spend their budgets on qualitative candidates. Concretely we: i) calculate 60 500 1000 Number of Epochs 10 2 10 1 Mean Regret LCBench 0 500 1000 Number of Steps 10 1 TaskSet 0 2000 4000 Number of Epochs 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 3: The mean regret for the different benchmarks over the number of epochs or steps (every 200 iterations). The results are aggregated over 35 different datasets for LCBench and aggregated over 12 different NLP tasks for TaskSet. the precision of the top (w.r.t. ground truth) performing configurations that were selected by each method across different budgets, ii) compute the average regret of the selected configurations across budget, and iii) we compare the fraction of top-performing configurations at a given budget that were not top performers at lower budgets, i.e. measure the ability to handle the poor correlation of performances across budgets. 5 Results Experiment 1: DYHPO achieves state-of-the-art results. In our first experiment, we evaluate the various methods on the benchmarks listed in Section 4.2. We show the aggregated results in Figure 3, the results show that DYHPO manages to outperform competitor methods over the set of considered benchmarks by achieving a better mean regret across datasets. Not only does DYHPO achieve a better final performance, it also achieves strong anytime results by converging faster than the competitor methods. For the extended results, related to the performance of all methods on a dataset level, we refer the reader to Appendix B. 12345678 Random Dragonfly Hyperband MF-DNN ASHA BOHB DEHB DyHPO LCBench@50% 12345678 Random Hyperband MF-DNN Dragonfly DEHB BOHB ASHA DyHPO LCBench@100% 12345678 Random MF-DNN ASHA Dragonfly Hyperband DEHB BOHB DyHPO TaskSet@50% 12345678 Random MF-DNN ASHA Dragonfly DEHB Hyperband BOHB DyHPO TaskSet@100% Figure 4: Critical difference diagram for LCBench and TaskSet in terms of the number of HPO steps. The results correspond to results after 500 and 1,000 epochs. Connected ranks via a bold bar indicate that performances are not significantly different (p >0.05). In Figure 4, we provide further evidence thatDYHPO’s improvement over the baselines is statistically significant. The critical difference diagram presents the ranks of all methods and provides information on the pairwise statistical difference between all methods for two fractions of the number of HPO steps (50% and 100%). We included the LCBench and TaskSet benchmarks in our significance plots. NAS-Bench-201 was omitted because it has only 3 datasets and the statistical test cannot be applied. Horizontal lines indicate groupings of methods that are not significantly different. As suggested by the best published practices Demsar [2006], we use the Friedman test to reject the null hypothesis followed by a pairwise post-hoc analysis based on the Wilcoxon signed-rank test (α = 0.05). For LCBench, DYHPO already outperforms the baselines significantly after 50% of the search budget, with a statistically significant margin. As the optimization procedure continues, DYHPO manages to extend its gain in performance and is the only method that has a statistically significant improvement against all the other competitor methods. Similarly, for TaskSet, DYHPO manages to outperform all methods with a statistically significant margin only halfway through the optimization procedure and achieves the best rank over all methods. However, as the optimization procedure continues, BOHB manages to decrease the performance gap with DYHPO , although, it still achieves a worse rank across all datasets. Considering the empirical results, we conclude that Hypothesis 1 is validated and that DYHPO achieves state-of-the-art results on multi-fidelity HPO. Experiment 2: On the impact of DYHPO ’s overhead on the results. We present the results of our second experiment in Figure 5 (left), where, as it can be seen, DYHPO still outperforms the other methods when its overhead is considered. For LCBench, DYHPO manages to get an advantage 70.2 0.4 0.6 0.8 1.0 Normalized Wallclock Time 10 1 Mean Regret LCBench 103 105 Wallclock Time in Seconds 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@50% 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@100% Figure 5: Left: The regret over time for all methods during the optimization procedure for the LCBench benchmark and the ImageNet dataset from the NAS-Bench-201 benchmark. The normalized wall clock time represents the actual run time divided by the total wall clock time ofDYHPO including the overhead of fitting the deep GP. Right: The critical difference diagram for LCBench halfway through the HPO wall-clock time, and in the end. Connected ranks via a bold bar indicate that differences are not significant (p >0.05). fairly quickly and it only increases the gap in performance with the other methods as the optimization process progresses. Similarly, in the case of ImageNet from NAS-Bench-201, DYHPO manages to gain an advantage earlier than other methods during the optimization procedure. Although in the end DYHPO still performs better than all the other methods, we believe most of the methods converge to a good solution and the differences in the final performance are negligible. For the extended results, related to the performance of all methods on a dataset level over time, we refer the reader to the plots in Appendix B. Additionally, in Figure 5 (right), we provide the critical difference diagrams for LCBench that present the ranks and the statistical difference of all methods halfway through the optimization procedure, and in the end. As it can be seen, DYHPO has a better rank with a significant margin with only half of the budget used and it retains the advantage until the end. Experiment 3: On the efficiency of DYHPO . In Figure 6 (left), we plot the precision of every method for different budgets during the optimization procedure, which demonstrates that DYHPO effectively explores the search space and identifies promising candidates. The precision at an epoch i is defined as the number of top 1% candidates that are trained, divided by the number of all candidates trained, both trained for at least i epochs. The higher the precision, the more relevant candidates were considered and the less computational resources were wasted. For small budgets, the precision is low since DYHPO spends budget to consider various candidates, but then, promising candidates are successfully identified and the precision quickly increases. This argument is further supported in Figure 6 (middle), where we visualize the average regret of all the candidates trained for at least the specified number of epochs on the x-axis. In contrast to the regret plots, here we do not show the regret of the best configuration, but the mean regret of all the selected configurations. The analysis deduces a similar finding, our method DYHPO selects more qualitative hyperparameter configurations than all the baselines. An interesting property of multi-fidelity HPO is the phenomenon of poor rank correlations among the validation performance of candidates at different budgets. In other words, a configuration that achieves a poor performance at a small budget can perform better at a larger budget. To analyze this phenomenon, we measure the percentage of \"good\" configurations at a particular budget, that were \"bad\" performers in at least one of the smaller budgets. We define a \"good\" performance at a budget B when a configuration achieves a validation accuracy ranked among the top 1/3 of the validation accuracies belonging to all the other configurations that were run until that budget B. In Figure 6 (right), we analyze the percentage of \"good\" configurations at each budget denoted by the x-axis, that were \"bad\" performers in at least one of the lower budgets. Such a metric is a proxy for the degree of the promotion of \"bad\" configurations towards higher budgets. We present the analysis for all the competing methods of our experimental protocol from Section 4. We have additionally included the ground-truth line annotated as \"Baseline\", which represents the fraction of past poor performers among all the feasible configurations in the search space. In contrast, the respective methods compute the fraction of promotions only among the configurations that those methods have considered (i.e. selected within their HPO trials) until the budget indicated by the x-axis. We see that there is a high degree of \"good\" configurations that were \"bad\" at a previous budget, with fractions of the ground-truth \"Baseline\" going up to 40% for the LCBench benchmark and up to 80% for the NAS-Bench-201 benchmark. 80 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3 0.4Precision of Top Candidates LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.05 0.10 0.15Average Regret LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline 0 50 100 150 200 Number of Epochs 0.05 0.10 0.15Precision of Top Candidates NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.04 0.06 0.08 0.10 0.12Average Regret NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Poor Performer Promotions NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 6: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the LCBench and NAS-Bench-201 benchmark. On the other hand, the analysis demonstrates that our method DYHPO has promoted more \"good\" configurations that were \"bad\" in a lower budget, compared to all the rival methods. In particular, more than 80% of selected configurations from the datasets belonging to either benchmark were \"bad\" performers at a lower budget. The empirical evidence validates Hypothesis 3 and demonstrates that DYHPO efficiently explores qualitative candidates. We provide the results of our analysis for DYHPO’s efficiency on the additional benchmarks (Taskset) in Appendix B. Ablating the impact of the learning curve 102 103 104 105 Training Time in Seconds 10 1 Regret ImageNet16-120 DyHPO DyHPO w/o CNN Random Search Figure 7: Ablating the impact of the learning curve on DYHPO. One of the main differences between DYHPO and similar methods Kandasamy et al. [2017], is that the learning curve is an input to the kernel function. For this reason, we investigate the impact of this design choice. We consider a variation of DYHPO w/o CNN, which is simply DYHPO without the learning curve. It is worth emphasizing that both variants (with and without the learning curve) are multi-fidelity surrogates and both receive the budget informa- tion through the inputted index j in Equation 3. The only difference is that DYHPO additionally incorporates the pattern of the learning curve. We run the ablation on the NAS-Bench-201 benchmark and report the results for ImageNet, the largest dataset in our collection. The ablation results are shown in Figure 7, while the remaining results on the other datasets are shown in Figure 8 of the appendix. Based on the results from our learning curve ablation, we conclude that the use of an explicit learning curve representation leads to significantly better results. 6 Limitations of Our Method Although DYHPO shows a convincing and statistically significant reduction of the HPO time on diverse Deep Learning (DL) experiments, we cautiously characterized our method only as a \"step 9towards\" scaling HPO for DL. The reason for our restrain is the lack of tabular benchmarks for HPO on very large deep learning models, such as Transformers-based architectures [Devlin et al., 2019]. Additionally, the pause and resume part of our training procedure can only be applied when tuning the hyperparameters of parametric models, otherwise, the training of a hyperparameter configuration would have to be restarted. Lastly, for small datasets that can be trained fast, the overhead of model-based techniques would make an approach like random search more appealing. 7 Conclusions In this work, we present DYHPO , a new Bayesian optimization (BO) algorithm for the gray-box setting. We introduced a new surrogate model for BO that uses a learnable deep kernel and takes the learning curve as an explicit input. Furthermore, we motivated a variation of expected improvement for the multi-fidelity setting. Finally, we compared our approach on diverse benchmarks on a total of 50 different tasks against the current state-of-the-art methods on gray-box hyperparameter optimization (HPO). Our method shows significant gains and has the potential to become the de facto standard for HPO in Deep Learning. Acknowledgments Josif Grabocka and Arlind Kadra would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools center of excellence. References Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. doi: 10.24963/ijcai.2021/296. URL https://doi.org/ 10.24963/ijcai.2021/296. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. URL https://openreview.net/forum?id=HJqk3N1vG. James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 2546–2554, 2011. URL https://proceedings.neurips.cc/ paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html. Hadrien Bertrand, Roberto Ardon, Matthieu Perrot, and Isabelle Bloch. Hyperparameter optimization of deep neural networks: Combining hyperband with bayesian model selection. In Conférence sur l’Apprentissage Automatique, 2017. Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. URL http://proceedings. mlr.press/v70/chen17e.html. Janez Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7: 1–30, 2006. URL http://jmlr.org/papers/v7/demsar06a.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 10the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/ 10.18653/v1/n19-1423. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Qiang Yang and Michael J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/487. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://openreview.net/forum?id=HJxyZkBKDr. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimiza- tion at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. URL http://proceedings.mlr.press/v80/falkner18a.html. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1165–1173, 2017. URL http://proceedings.mlr.press/v70/franceschi17a.html. Jacob R. Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wil- son. Gpytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 7587–7597, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 27e8e17134dd7083b050476733207ea1-Abstract.html. Matilde Gargiani, Aaron Klein, Stefan Falkner, and Frank Hutter. Probabilistic rollouts for learning curve extrapolation across hyperparameter settings. CoRR, abs/1910.04522, 2019. URL http: //arxiv.org/abs/1910.04522. Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pages 240–248, 2016. URL http://proceedings.mlr.press/v51/jamieson16.html. Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of ex- pensive black-box functions. J. Global Optimization , 13(4):455–492, 1998. doi: 10.1023/A: 1008306431147. URL https://doi.org/10.1023/A:1008306431147. Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff G. Schneider, and Barn- abás Póczos. Gaussian process bandit optimisation with multi-fidelity evaluations. In Advances in Neural Information Processing Systems 29: Annual Conference on Neu- ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pages 992–1000, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html. Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi-fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. URL http://proceedings.mlr.press/v70/kandasamy17a.html. Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Neural architecture search with bayesian optimisation and optimal transport. In Ad- vances in Neural Information Processing Systems 31: Annual Conference on Neural Infor- mation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 2020–2029, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ f33ba15effa5c10e873bf3842afb46a6-Abstract.html. 11Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21: 81:1–81:27, 2020. URL http://jmlr.org/papers/v21/18-223.html. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages 528–536, 2017a. URL http://proceedings.mlr.press/ v54/klein17a.html. Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017b. URL https://openreview.net/forum?id=S11KBYclx. Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors,Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org, 2020a. URL https://proceedings.mlsys.org/book/303.pdf. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18: 185:1–185:52, 2017. URL http://jmlr.org/papers/v18/16-558.html. Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: An- nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/ 60e1deb043af37db5ea4ce9ae8d2c9ea-Abstract.html. Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. URL http://proceedings.mlr.press/v108/lorraine20a.html. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2113–2122, 2015. URL http://proceedings.mlr.press/v37/maclaurin15.html. Pedro Mendes, Maria Casimiro, Paolo Romano, and David Garlan. Trimtuner: Efficient optimization of machine learning jobs in the cloud via sub-sampling. In 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, MASCOTS 2020, Nice, France, November 17-19, 2020, pages 1–8. IEEE, 2020. doi: 10.1109/MASCOTS50786. 2020.9285971. URL https://doi.org/10.1109/MASCOTS50786.2020.9285971. Pedro Mendes, Maria Casimiro, and Paolo Romano. Hyperjump: Accelerating hyperband via risk modelling. CoRR, abs/2108.02479, 2021. URL https://arxiv.org/abs/2108.02479. Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl- Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. URL https://arxiv.org/abs/2002.11887. Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparameter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html. 12Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, and Cédric Archambeau. Scalable hyper- parameter transfer learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6846–6856, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/14c879f3f5d8ed93a09f6090d77c2cc3-Abstract.html. Matthias Poloczek, Jialei Wang, and Peter I. Frazier. Multi-information source optimiza- tion. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 4288–4298, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ df1f1d20ee86704251795841e6a9405a-Abstract.html. Akshara Rai, Ruta Desai, and Siddharth Goyal. Bayesian optimization with a neural network kernel, 2016. URL http://www.cs.cmu.edu/~rutad/files/BO_NN.pdf. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 2960–2968, 2012. URL https://proceedings. neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html. Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 1015–1022, 2010. URL https://icml.cc/Conferences/2010/papers/422.pdf. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 2004–2012, 2013. URL https://proceedings.neurips. cc/paper/2013/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. URL http://arxiv.org/abs/1406.3896. Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 9334–9345, 2020. URLhttp://proceedings. mlr.press/v119/takeno20a.html. Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. CoRR, abs/1801.01596, 2018. URL http: //arxiv.org/abs/1801.01596. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 370–378, 2016. URL http://proceedings.mlr. press/v51/wilson16.html. Martin Wistuba. Bayesian optimization combined with incremental evaluation for neural network architecture optimization. In AutoML@PKDD/ECML, 2017. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/forum?id=bJxgv5C3sYc. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10303–10312. PMLR, 2020. URL http://proceedings.mlr.press/v119/wistuba20a.html. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl. IEEE Trans. Pattern Anal. Mach. Intell., 43(9):3079–3090, 2021. doi: 10.1109/TPAMI.2021.3067763. URL https://doi.org/10.1109/TPAMI.2021.3067763. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section “Societal Implications” in the Appendix. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] We provide our main algorithm in Section 3 and we additionally provide the detailed implementation details in Appendix A for all methods and benchmarks. We will release the code for the camera-ready version of our work. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please see Appendix A. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] We report the statistical significance of the performance difference between our method and the baselines in Section 5 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4.2 and Section 4.3. (b) Did you mention the license of the assets? [Yes] See Appendix A.1 and A.5 where we provide references to the assets where the license is included. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] The benchmarks and baselines are open-sourced. (e) Did you discuss whether the data you are using/curating contains personally identi- fiable information or offensive content? [N/A] The data does not contain personally identifiable information or offensive content. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14Societal Implications In our work, we use only publicly available data with no privacy concerns. Furthermore, our algorithm reduces the overall time for fitting deep networks, therefore, saving computational resources and yielding a positive impact on the environment. Moreover, our method can help smaller research organizations with limited access to resources to be competitive in the deep learning domain, which reduces the investment costs on hardware. Although our method significantly reduces the time taken for optimizing a machine learning algorithm that achieves peak performance, we warn against running our method for an extended time only to achieve marginal gains in performance, unless it is mission-critical. Last but not least, in order to save energy, we invite the community to create sparse benchmarks with surrogates, instead of dense tabular ones. A Experimental Setup A.1 Benchmarks LCBench. LCBench4 is a feedforward neural network benchmark on tabular data which consists of 2000 configuration settings for each of the 35 datasets. The configurations were evaluated during HPO runs with AutoPyTorch. LCBench features a search space of 7 numerical hyperparameters, where every hyperparameter configuration is trained for 50 epochs. The objective is to optimize seven different hyperparameters of funnel-shaped neural networks, i.e., batch size, learning rate, momentum, weight decay, dropout, number of layers, and maximum number of units per layer. TaskSet. TaskSet5 is a benchmark that features over 1162 diverse tasks from different domains and includes 5 search spaces. In this work, we focus on NLP tasks and we use the Adam8p search space with 8 continuous hyperparameters. We refer to Figure 11 for the exact task names considered in our experiments. The learning curves provided in TaskSet report scores after every 200 iterations. We refer to those as \"steps\". The objective is to optimize eight hyperparameters for a set of different recurrent neural networks (RNN) that differ in embedding size, RNN cell, and other architectural features. The set of hyperparameters consists of optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate of the first and second momentum of Adam, β1 and β2, and Adam’s constant for numerical stability ε. Furthermore, there are two hyperparameters controlling linear and exponential learning rate decays, as well as L1 and L2 regularization terms. NAS-Bench-201. NAS-Bench-2016 is a benchmark that has precomputed about 15,600 archi- tectures trained for 200 epochs for the image classification datasets CIFAR-10, CIFAR-100, and ImageNet. The objective is to select for each of the six operations within the cell of the macro architecture one of five different operations. All other hyperparameters such as learning rate and batch size are kept fixed. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. A.2 Preprocessing In the following, we describe the preprocessing applied to the hyperparameter representation. For LCBench, we apply a log-transform to batch size, learning rate, and weight decay. For TaskSet, we apply it on the learning rate, L1 and L2 regularization terms, epsilon, linear and exponential decay of the learning rate. All continuous hyperparameters are scaled to the range between 0 and 1 using sklearn’s MinMaxScaler. If not mentioned otherwise, we use one-hot encoding for the categorical hyperparameters. As detailed in subsection A.5, some baselines have a specific way of dealing with them. In that case, we use the method recommended by the authors. A.3 Framework The framework contains the evaluated hyperparameters and their corresponding validation curves. The list of candidate hyperparameters is passed to the baseline-specific interface, which in turn, 4https://github.com/automl/LCBench 5https://github.com/google-research/google-research/tree/master/task_set 6https://github.com/D-X-Y/NAS-Bench-201 15optimizes and queries the framework for the hyperparameter configuration that maximizes utility. Our framework in turn responds with the validation curve and the cost of the evaluation. In case a hyperparameter configuration has been evaluated previously up to a budget b and a baseline requires the response for budget b + 1, the cost is calculated accordingly only for the extra budget requested. A.4 Implementation Details We implement the Deep Kernel Gaussian Process using GPyTorch 1.5 [Gardner et al., 2018]. We use an RBF kernel and the dense layers of the transformation functionφ have 128 and 256 units. We used a convolutional layer with a kernel size of three and four filters. All parameters of the Deep Kernel are estimated by maximizing the marginal likelihood. We achieve this by using gradient ascent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1 and batch size of 64. We stop training as soon as the training likelihood is not improving for 10 epochs in a row or we completed 1,000 epochs. For every new data point, we start training the GP with its old parameters to reduce the required effort for training. A.5 Baselines Random Search & Hyperband. Random search and Hyperband sample hyperparameter config- urations at random and therefore the preprocessing is irrelevant. We have implemented both from scratch and use the recommended hyperparameters for Hyperband, i.e. η = 3. BOHB. For our experiments with BOHB, we use version 0.7.4 of the officially-released code7. DEHB. For our experiments with DEHB, we use the official public implementation8. We devel- oped an interface that communicates between our framework and DEHB. In addition to the initial preprocessing common for all methods, we encode categorical hyperparameters with a numerical value in the interval [0, 1]. For a categorical hyperparameter xi, we take Ni equal-sized intervals, where Ni represents the number of unique categorical values for hyperparameter xi and we assign the value for a categorical value n ∈ Ni to the middle of the interval [n, n+ 1]as suggested by the authors. For configuring the DEHB algorithm we used the default values from the library. Dragonfly. We use the publicly available code of Dragonfly9. No special treatment of categorical hyperparameters is required since Dragonfly has its own way to deal with them. We use version 0.1.6 with default settings. MF-DNN. We use the official implementation of MF-DNN by the authors10. Initially, we tried to use multiple incremental fidelity levels like for DYHPO, however, the method runtime was too high and it could not achieve competitive results. For that reason, we use only a few fidelity levels like the authors do in their work Li et al. [2020b]. We use the same fidelity levels as for Hyperband, DEHB, and BOHB to have a fair comparison between the baselines. We also use the same number of initial points as for the other methods to have the same maximal resource allocated for every fidelity level. ASHA-HB. We use the public implementation from the well-known optuna library (version2.10.0). We used the same eta, minimum and maximal budget as for HB, DEHB, and BOHB in our experi- ments, to have a fair comparison. B Additional Plots In Figure 8, we ablate the learning curve input in our kernel, to see the effect it has on performance for the CIFAR-10 and CIFAR-100 datasets from the NAS-Bench-201 benchmark. The results indicate that the learning curve plays an important role in achieving better results by allowing faster convergence and a better anytime performance. 7https://github.com/automl/HpBandSter 8https://github.com/automl/DEHB/ 9https://github.com/dragonfly/dragonfly 10https://github.com/shib0li/DNN-MFBO 16102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar10 DyHPO DyHPO w/o CNN Random Search 102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar100 DyHPO DyHPO w/o CNN Random Search Figure 8: The learning curve ablation for the CIFAR-10 and CIFAR-100 tasks of NAS-Bench-201. 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 9: NAS-Bench-201 regret results over the number of epochs spent during the optimization. Additionally, in Figure 9, we show the performance comparison over the number of epochs of every method for the CIFAR-10 and CIFAR-100 datasets in the NAS-Bench-201 benchmark. While, in Figure 10, we present the performance comparison over time. As can be seen, DYHPO converges faster and has a better performance compared to the other methods over the majority of the time or steps, however, towards the end although it is the optimal method or close to the optimal method, the difference in regret is not significant anymore. Furthermore, Figure 11 shows the performance comparison for the datasets chosen from TaskSet over the number of steps. Looking at the results, DYHPO is outperforming all methods convincingly on the majority of datasets by converging faster and with significant differences in the regret evaluation metric. In Figure 12 and 13, we show the performance comparison for all the datasets from LCBench regarding regret over the number of epochs. Similarly, in Figure 14 and 15, we show the same performance comparison, however, over time. As can be seen, DYHPO manages to outperform the other competitors in the majority of the datasets, and in the datasets that it does not, it is always close to the top-performing method, and the difference between methods is marginal. In Figure 16 we provide the extended results of Experiment 3 for TaskSet. We show the precision, average regret, and promotion percentage for poor-performing configurations for DYHPO and the other competitor methods. Lastly, we explore the behavior of DYHPO after finding the configuration which is returned at the end of the optimization as the best configuration. In Figure 17, we show how the budget is distributed on the configurations considered during that part of the optimization. Clearly, DYHPO is spending very little budget on most configurations. Furthermore, we investigated how many new configurations are considered during this phase. For LCBench, 76.98% of considered configurations are new demonstrating that DYHPO is investigating most of the budget into exploration. These are even more extreme for TaskSet (93.16% and NAS-Bench-201 (97.51%). 17102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 10: NAS-Bench-201 regret results over the total optimization time. The total time includes the method overhead time and the hyperparameter configuration evaluation time. 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_avg_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_embed128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 6 × 10 2 2 × 10 1 3 × 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU64_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 100 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_last_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_E128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 2 × 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN128_tanh_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_tanh_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 11: Performance comparison over the number of steps on a dataset level for TaskSet. 180 200 400 600 800 1000 Number of Epochs 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 100 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 12: Performance comparison over the number of steps on a dataset level for LCBench. 190 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 13: Performance comparison over the number of steps on a dataset level for LCBench (cont.). 200 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 100 200 300 Wallclock Time in Seconds 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2500 5000 7500 10000 12500 Wallclock Time in Seconds 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 10000 Wallclock Time in Seconds 10 2 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 5000 10000 15000 Wallclock Time in Seconds 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 Wallclock Time in Seconds 10 1 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 14: Performance comparison over time on a dataset level for LCBench with the overhead included. 210 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 Wallclock Time in Seconds 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 15: Performance comparison over time on a dataset level for LCBench with the overhead included. (cont.). 220 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3Precision of Top Candidates Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.10 0.15 0.20 0.25 0.30Average Regret Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Steps 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 16: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the Taskset benchmark. 0 10 20 30 40 50 Epochs 0.0 0.2 0.4 0.6Relative Number of Candidates LCBench 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates NAS-Bench-201 0 10 20 30 40 50 Steps 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates TaskSet Figure 17: These plots shed light on how DYHPO behaves after the configuration it finally returns as the best. The plots show how many epochs are spent per candidate. As we can see, for most candidates only a small budget was considered, indicating that DYHPO is mostly exploring at this point. 23",
      "meta_data": {
        "arxiv_id": "2202.09774v2",
        "authors": [
          "Martin Wistuba",
          "Arlind Kadra",
          "Josif Grabocka"
        ],
        "published_date": "2022-02-20T10:28:02Z",
        "pdf_url": "https://arxiv.org/pdf/2202.09774v2.pdf",
        "github_url": "https://github.com/releaunifreiburg/DyHPO"
      },
      "llm_extracted_info": {
        "main_contributions": "Introduced DyHPO, a Bayesian Optimization method for multi-fidelity hyperparameter optimization (HPO) that dynamically allocates computational budget. It addresses the sub-optimal budget allocation and poor rank correlation issues of existing gray-box HPO methods. DyHPO proposes a novel Bayesian surrogate model with a deep kernel that embeds learning curve dynamics and a multi-fidelity acquisition function, demonstrating statistically significant empirical gains over state-of-the-art HPO methods across diverse deep learning architectures and datasets.",
        "methodology": "DyHPO employs a Gaussian Process (GP) surrogate model with a novel deep kernel. This deep kernel (K) uses a neural network (φ) to process the hyperparameter configuration (x), current budget (j), and the past learning curve (Y). The network φ concatenates a linear transformation of normalized budget and hyperparameter configuration with a one-dimensional convolution and max pooling of the learning curve. A squared exponential kernel (k) is then applied to the output of φ. The model parameters are optimized via maximum likelihood using Adam. DyHPO extends Expected Improvement (EI) to Multi-Fidelity Expected Improvement (EIMF), which dynamically selects the next configuration and budget (incrementing by one step) to train based on the surrogate's predictions, prioritizing exploration by slowly increasing the budget.",
        "experimental_setup": "DyHPO was evaluated on 50 datasets across three diverse deep learning architectures and modalities: MLPs on 35 tabular datasets (LCBench), RNNs on 12 NLP tasks (TaskSet, Adam8p search space), and CNNs/NAS on 3 image classification datasets (NAS-Bench-201, CIFAR-10, CIFAR-100, ImageNet). Experiments were run on Amazon EC2 M5 Instances (m5.xlarge). Performance metrics included mean regret and average rank, calculated as the mean of ten repetitions. DyHPO was compared against seven baselines: Random Search, HyperBand, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). Ablation studies were conducted to assess the impact of the learning curve input.",
        "limitations": "The method lacks evaluation on tabular benchmarks for very large deep learning models like Transformer-based architectures. Its pause and resume training procedure is only applicable to parametric models, implying that non-parametric models would require restarting training. For small datasets that train quickly, the computational overhead of DyHPO (as a model-based technique) might make simpler approaches like random search more efficient and appealing.",
        "future_research_directions": "Future research could involve scaling DyHPO to optimize very large deep learning models, such as Transformer-based architectures, and potentially developing new benchmarks tailored for them. Exploring extensions to non-parametric models or adapting the training procedure to handle restarts for such cases is another direction. Additionally, optimizing DyHPO's overhead for small, fast-trained datasets or developing adaptive strategies that choose between complex and simpler HPO methods based on dataset characteristics could be beneficial.",
        "experimental_code": "class DyHPOAlgorithm:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        log_indicator: List,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 500,\n        device: str = None,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        surrogate_config: dict = None,\n        verbose: bool = True,\n    ):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.hp_candidates = hp_candidates\n        self.log_indicator = log_indicator\n\n        self.scaler = MinMaxScaler()\n        self.hp_candidates = self.preprocess_hp_candidates()\n\n        self.minimization = minimization\n        self.seed = seed\n\n        if verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        self.logger = logging.getLogger()\n\n        logging.basicConfig(\n            format='%(levelname)s:%(asctime)s:%(message)s',\n            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',\n            level=logging_level,\n        )\n\n        self.examples = dict()\n        self.performances = dict()\n\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.total_budget = total_budget\n        self.fantasize_step = fantasize_step\n        self.nr_features = self.hp_candidates.shape[1]\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        self.init_budgets = [conf_individual_budget] * initial_configurations_nr\n        self.fraction_random_configs = 0.1\n\n        self.model = None\n        self.initial_random_index = 0\n\n        if surrogate_config is None:\n            self.surrogate_config = {\n                'nr_layers': 2,\n                'nr_initial_features': self.nr_features,\n                'layer1_units': 64,\n                'layer2_units': 128,\n                'cnn_nr_channels': 4,\n                'cnn_kernel_size': 3,\n                'batch_size': 64,\n                'nr_epochs': 1000,\n                'nr_patience_epochs': 10,\n                'learning_rate': 0.001,\n            }\n        else:\n            self.surrogate_config = surrogate_config\n\n        self.best_value_observed = np.NINF\n        self.diverged_configs = set()\n\n        self.info_dict = dict()\n\n        self.suggest_time_duration = 0\n        self.budget_spent = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n\n    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n        train_curves = self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_examples = torch.tensor(train_examples)\n        train_labels = torch.tensor(train_labels)\n        train_budgets = torch.tensor(train_budgets)\n        train_curves = torch.tensor(train_curves)\n\n        train_examples = train_examples.to(device=self.dev)\n        train_labels = train_labels.to(device=self.dev)\n        train_budgets = train_budgets.to(device=self.dev)\n        train_curves = train_curves.to(device=self.dev)\n\n        data = {\n            'X_train': train_examples,\n            'train_budgets': train_budgets,\n            'train_curves': train_curves,\n            'y_train': train_labels,\n        }\n\n        return data\n\n    def _train_surrogate(self):\n        data = self._prepare_dataset_and_budgets()\n        self.logger.info(f'Started training the model')\n\n        self.model.train_pipeline(\n            data,\n            load_checkpoint=False,\n        )\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:\n        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()\n        budgets = np.array(budgets, dtype=np.single)\n        non_scaled_budgets = copy.deepcopy(budgets)\n        budgets = budgets / self.max_benchmark_epochs\n\n        configurations = np.array(configurations, dtype=np.single)\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n\n        learning_curves = self.patch_curves_to_same_length(learning_curves)\n        learning_curves = np.array(learning_curves, dtype=np.single)\n        learning_curves = torch.tensor(learning_curves)\n        learning_curves = learning_curves.to(device=self.dev)\n\n        train_data = self._prepare_dataset_and_budgets()\n        test_data = {\n            'X_test': configurations,\n            'test_budgets': budgets,\n            'test_curves': learning_curves,\n        }\n\n        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)\n\n        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        suggest_time_start = time.time()\n        if self.initial_random_index < len(self.init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. '\n                'Returning randomly sampled configuration'\n            )\n\n            random_indice = self.init_conf_indices[self.initial_random_index]\n            budget = self.init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n\n            return random_indice, budget\n        else:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n                non_scaled_budgets,\n            )\n            best_config_index = hp_indices[best_prediction_index]\n\n            if best_config_index in self.examples:\n                evaluated_budgets = self.examples[best_config_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        self.budget_spent += self.fantasize_step\n\n        if self.budget_spent > self.total_budget:\n            exit(0)\n\n        return best_config_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        learning_curve: np.ndarray,\n        alg_time: Optional[float] = None,\n    ):\n        score = learning_curve[-1]\n        if np.isnan(learning_curve).any():\n            self.update_info_dict(hp_index, b, np.nan, 0)\n            self.diverged_configs.add(hp_index)\n            return\n\n        observe_time_start = time.time()\n\n        self.examples[hp_index] = np.arange(1, b + 1).tolist()\n        self.performances[hp_index] = learning_curve\n\n        if self.best_value_observed < score:\n            self.best_value_observed = score\n            self.no_improvement_patience = 0\n        else:\n            self.no_improvement_patience += 1\n\n        observe_time_end = time.time()\n        train_time_duration = 0\n\n        if self.initial_random_index >= len(self.init_conf_indices):\n            train_time_start = time.time()\n            if self.model is None:\n                self.model = DyHPO(\n                    self.surrogate_config,\n                    self.dev,\n                    self.dataset_name,\n                    self.output_path,\n                    self.seed,\n                )\n\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.model.restart = True\n\n            self._train_surrogate()\n\n            train_time_end = time.time()\n            train_time_duration = train_time_end - train_time_start\n\n        observe_time_duration = observe_time_end - observe_time_start\n        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration\n        if alg_time is not None:\n            total_duration = total_duration + alg_time\n\n        self.update_info_dict(hp_index, b, score, total_duration)\n\n    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        hp_indices = []\n        hp_budgets = []\n        learning_curves = []\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                max_budget = max(budgets)\n                next_budget = max_budget + self.fantasize_step\n                curve = self.performances[hp_index][:max_budget]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)\n                if difference_curve_length > 0:\n                    curve.extend([0.0] * difference_curve_length)\n            else:\n                next_budget = self.fantasize_step\n                curve = [0, 0, 0]\n\n            if next_budget <= self.max_benchmark_epochs:\n                hp_indices.append(hp_index)\n                hp_budgets.append(next_budget)\n                learning_curves.append(curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, learning_curves\n\n    def history_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget, performance in zip(budgets, performances):\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performance)\n                train_curve = performances[:budget - 1] if budget > 1 else [0.0]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)\n                if difference_curve_length > 0:\n                    train_curve.extend([0.0] * difference_curve_length)\n\n                train_curves.append(train_curve)\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    def acq(\n        self,\n        best_value: float,\n        mean: float,\n        std: float,\n        explore_factor: Optional[float] = 0.25,\n        acq_fc: str = 'ei',\n    ) -> float:\n        if acq_fc == 'ei':\n            if std == 0:\n                return 0\n            z = (mean - best_value) / std\n            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n        elif acq_fc == 'ucb':\n            acq_value = mean + explore_factor * std\n        elif acq_fc == 'thompson':\n            acq_value = np.random.normal(mean, std)\n        elif acq_fc == 'exploit':\n            acq_value = mean\n        else:\n            raise NotImplementedError(\n                f'Acquisition function {acq_fc} has not been'\n                f'implemented',\n            )\n\n        return acq_value\n\n    def find_suggested_config(\n        self,\n        mean_predictions: np.ndarray,\n        mean_stds: np.ndarray,\n        budgets: List,\n    ) -> int:\n        highest_acq_value = np.NINF\n        best_index = -1\n\n        index = 0\n        for mean_value, std in zip(mean_predictions, mean_stds):\n            budget = int(budgets[index])\n            best_value = self.calculate_fidelity_ymax(budget)\n            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')\n            if acq_value > highest_acq_value:\n                highest_acq_value = acq_value\n                best_index = index\n\n            index += 1\n\n        return best_index\n\n    def calculate_fidelity_ymax(self, fidelity: int):\n        exact_fidelity_config_values = []\n        lower_fidelity_config_values = []\n\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n                exact_fidelity_config_values.append(performance)\n            except IndexError:\n                learning_curve = self.performances[example_index]\n                lower_fidelity_config_values.append(max(learning_curve))\n\n        if len(exact_fidelity_config_values) > 0:\n            best_value = max(exact_fidelity_config_values)\n        else:\n            best_value = max(lower_fidelity_config_values)\n\n        return best_value\n\n    def preprocess_hp_candidates(self) -> List:\n        log_hp_candidates = []\n\n        for hp_candidate in self.hp_candidates:\n            new_hp_candidate = []\n            for index, hp_value in enumerate(hp_candidate):\n                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)\n\n            log_hp_candidates.append(new_hp_candidate)\n\n        log_hp_candidates = np.array(log_hp_candidates)\n\n        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)\n\n        return log_hp_candidates\n\n    @staticmethod\n    def patch_curves_to_same_length(curves):\n        max_curve_length = 0\n        for curve in curves:\n            if len(curve) > max_curve_length:\n                max_curve_length = len(curve)\n\n        for curve in curves:\n            difference = max_curve_length - len(curve)\n            if difference > 0:\n                curve.extend([0.0] * difference)\n\n        return curves\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, configuration):\n        super(FeatureExtractor, self).__init__()\n\n        self.configuration = configuration\n\n        self.nr_layers = configuration['nr_layers']\n        self.act_func = nn.LeakyReLU()\n        initial_features = configuration['nr_initial_features'] + 1\n        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])\n        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])\n        for i in range(2, self.nr_layers):\n            setattr(\n                self,\n                f'fc{i + 1}',\n                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),\n            )\n            setattr(\n                self,\n                f'bn{i + 1}',\n                nn.BatchNorm1d(configuration[f'layer{i}_units']),\n            )\n\n\n        setattr(\n            self,\n            f'fc{self.nr_layers}',\n            nn.Linear(\n                configuration[f'layer{self.nr_layers - 1}_units'] +\n                configuration['cnn_nr_channels'],\n                configuration[f'layer{self.nr_layers}_units']\n            ),\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),\n            nn.AdaptiveMaxPool1d(1),\n        )\n\n    def forward(self, x, budgets, learning_curves):\n\n        budgets = torch.unsqueeze(budgets, dim=1)\n        x = cat((x, budgets), dim=1)\n        x = self.fc1(x)\n        x = self.act_func(self.bn1(x))\n\n        for i in range(2, self.nr_layers):\n            x = self.act_func(\n                getattr(self, f'bn{i}')(\n                    getattr(self, f'fc{i}')(\n                        x\n                    )\n                )\n            )\n\n        learning_curves = torch.unsqueeze(learning_curves, 1)\n        lc_features = self.cnn(learning_curves)\n        lc_features = torch.squeeze(lc_features, 2)\n\n        x = cat((x, lc_features), dim=1)\n        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))\n\n        return x\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods.GaussianLikelihood,\n    ):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass DyHPO:\n    def __init__(\n        self,\n        configuration: Dict,\n        device: torch.device,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        seed: int = 11,\n    ):\n        super(DyHPO, self).__init__()\n        self.feature_extractor = FeatureExtractor(configuration)\n        self.batch_size = configuration['batch_size']\n        self.nr_epochs = configuration['nr_epochs']\n        self.early_stopping_patience = configuration['nr_patience_epochs']\n        self.refine_epochs = 50\n        self.dev = device\n        self.seed = seed\n        self.model, self.likelihood, self.mll =\n            self.get_model_likelihood_mll(\n                configuration[f'layer{self.feature_extractor.nr_layers}_units']\n            )\n\n        self.model.to(self.dev)\n        self.likelihood.to(self.dev)\n        self.feature_extractor.to(self.dev)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],\n        )\n\n        self.configuration = configuration\n        self.initial_nr_points = 10\n        self.iterations = 0\n        self.restart = True\n\n        self.logger = logging.getLogger(__name__)\n\n        self.checkpoint_path = os.path.join(\n            output_path,\n            'checkpoints',\n            f'{dataset_name}',\n            f'{self.seed}',\n        )\n\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_file = os.path.join(\n            self.checkpoint_path,\n            'checkpoint.pth'\n        )\n\n    def restart_optimization(self):\n        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)\n        self.model, self.likelihood, self.mll =\n            self.get_model_likelihood_mll(\n                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],\n            )\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n    def get_model_likelihood_mll(\n        self,\n        train_size: int,\n    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:\n        train_x = torch.ones(train_size, train_size).to(self.dev)\n        train_y = torch.ones(train_size).to(self.dev)\n\n        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)\n        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)\n\n        return model, likelihood, mll\n\n    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):\n        self.iterations += 1\n        self.logger.debug(f'Starting iteration: {self.iterations}')\n        weights_changed = False\n\n        if load_checkpoint:\n            try:\n                self.load_checkpoint()\n            except FileNotFoundError:\n                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'\n                                  f'Training the GP from the beginning')\n\n        self.model.train()\n        self.likelihood.train()\n        self.feature_extractor.train()\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n        X_train = data['X_train']\n        train_budgets = data['train_budgets']\n        train_curves = data['train_curves']\n        y_train = data['y_train']\n\n        initial_state = self.get_state()\n        training_errored = False\n\n        if self.restart:\n            self.restart_optimization()\n            nr_epochs = self.nr_epochs\n            if self.initial_nr_points <= self.iterations:\n                self.restart = False\n        else:\n            nr_epochs = self.refine_epochs\n\n        mse = 0.0\n\n        for epoch_nr in range(0, nr_epochs):\n\n            nr_examples_batch = X_train.size(dim=0)\n            if nr_examples_batch == 1:\n                continue\n\n            self.optimizer.zero_grad()\n\n            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)\n            self.model.set_train_data(projected_x, y_train, strict=False)\n            output = self.model(projected_x)\n\n            try:\n                loss = -self.mll(output, self.model.train_targets)\n                loss_value = loss.detach().to('cpu').item()\n                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)\n                self.logger.debug(\n                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '\n                    f'Loss: {loss_value:.3f}, '\n                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '\n                    f'noise: {self.model.likelihood.noise.item():.3f}, '\n                )\n                loss.backward()\n                self.optimizer.step()\n            except Exception as training_error:\n                self.logger.error(f'The following error happened while training: {training_error}')\n                self.restart = True\n                training_errored = True\n                break\n\n        if training_errored:\n            self.save_checkpoint(initial_state)\n            self.load_checkpoint()\n\n    def predict_pipeline(\n        self,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        self.model.eval()\n        self.feature_extractor.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad():\n            projected_train_x = self.feature_extractor(\n                train_data['X_train'],\n                train_data['train_budgets'],\n                train_data['train_curves'],\n            )\n            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)\n            projected_test_x = self.feature_extractor(\n                test_data['X_test'],\n                test_data['test_budgets'],\n                test_data['test_curves'],\n            )\n            preds = self.likelihood(self.model(projected_test_x))\n\n        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )\n        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )\n\n        return means, stds\n\n    def load_checkpoint(self):\n        checkpoint = torch.load(self.checkpoint_file)\n        self.model.load_state_dict(checkpoint['gp_state_dict'])\n        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n\n    def save_checkpoint(self, state: Dict =None):\n        if state is None:\n            torch.save(\n                self.get_state(),\n                self.checkpoint_file,\n            )\n        else:\n            torch.save(\n                state,\n                self.checkpoint_file,\n            )\n\n    def get_state(self) -> Dict[str, Dict]:\n        current_state = {\n            'gp_state_dict': deepcopy(self.model.state_dict()),\n            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),\n            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),\n        }\n\n        return current_state",
        "experimental_info": "The DyHPO method is evaluated with the following experimental settings:\n\n**Overall Experiment Setup (from `main_experiment.py` and `hpo_method.py`):**\n*   **Benchmarks:** LCBench and TaskSet. LCBench is a maximization problem (`minimization=False`), while TaskSet is a minimization problem (`minimization=True`).\n*   **Fantasize Step (`fantasize_step`):** 1 (meaning budgets are incremented by one step).\n*   **Total HPO Budget Limit (`total_budget`):** 1000 evaluations.\n*   **Dataset Name:** Defaults to 'covertype' for illustration.\n*   **Seeds:** Experiments are run for 10 seeds (`np.arange(10)`).\n*   **Max Benchmark Epochs (`max_benchmark_epochs`):** 51 for both LCBench and TaskSet.\n*   **Initial Random Configurations:** 1 configuration evaluated at budget 1 (`initial_configurations_nr = 1`, `conf_individual_budget = 1`).\n*   **Fraction of Random Configurations:** 0.1 (though the provided code primarily samples from the model after initial random configs).\n*   **Acquisition Function (`acq_fc`):** Expected Improvement ('ei') is used.\n*   **Exploration Factor (`explore_factor`):** Defaulted to 0.25 in the `acq` function, but primarily relevant for UCB.\n*   **Hyperparameter Preprocessing:** Hyperparameter candidates are preprocessed using `MinMaxScaler` after applying `math.log` for log-sampled hyperparameters. Budgets are scaled to `[0, 1]` by dividing by `max_benchmark_epochs`.\n*   **Learning Curve Handling:** Learning curves are padded with zeros to a consistent length, at least the `cnn_kernel_size`, for input to the convolutional neural network.\n\n**DyHPO Surrogate Model Configuration (from `DyHPOAlgorithm.__init__` and `DyHPO.__init__`):**\n*   **Deep Kernel Neural Network (`FeatureExtractor`) Architecture:**\n    *   `nr_layers`: 2\n    *   `nr_initial_features`: Determined by the number of hyperparameters (`self.nr_features`).\n    *   `layer1_units`: 64\n    *   `layer2_units`: 128\n    *   `cnn_nr_channels`: 4\n    *   `cnn_kernel_size`: 3 (for the 1D convolution on learning curves).\n*   **Gaussian Process Model:** Uses a `gpytorch.kernels.ScaleKernel` with an `RBFKernel` (Squared Exponential Kernel).\n*   **Optimizer:** Adam optimizer.\n    *   `learning_rate`: 0.001.\n    *   `batch_size`: 64.\n*   **Training Strategy:**\n    *   **Full Training Epochs (`nr_epochs`):** 1000 epochs when training from scratch.\n    *   **Refinement Epochs (`refine_epochs`):** 50 epochs after initial full training.\n    *   **Initial Points for Full Retraining (`initial_nr_points`):** 10 (model restarts from scratch for the first 10 iterations).\n    *   **No Improvement Patience (`no_improvement_patience`):** Model restarts (`self.model.restart = True`) if there's no incumbent improvement for `no_improvement_threshold` iterations, where `no_improvement_threshold` is `int(max_benchmark_epochs + 0.2 * max_benchmark_epochs)`."
      }
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
      "abstract": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have\nrecently emerged as a promising direction for tuning Deep Learning methods.\nHowever, existing methods suffer from a sub-optimal allocation of the HPO\nbudget to the hyperparameter configurations. In this work, we introduce DyHPO,\na Bayesian Optimization method that learns to decide which hyperparameter\nconfiguration to train further in a dynamic race among all feasible\nconfigurations. We propose a new deep kernel for Gaussian Processes that embeds\nthe learning curve dynamics, and an acquisition function that incorporates\nmulti-budget information. We demonstrate the significant superiority of DyHPO\nagainst state-of-the-art hyperparameter optimization methods through\nlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) and\ndiverse architectures (MLP, CNN/NAS, RNN).",
      "full_text": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations Martin Wistuba∗† Amazon Web Services, Berlin, Germany marwistu@amazon.com Arlind Kadra∗ University of Freiburg, Freiburg, Germany kadraa@cs.uni-freiburg.de Josif Grabocka University of Freiburg, Freiburg, Germany grabocka@cs.uni-freiburg.de Abstract Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO bud- get to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter config- uration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hy- perparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN). 1 Introduction Hyperparameter Optimization (HPO) is arguably an acute open challenge for Deep Learning (DL), especially considering the crucial impact HPO has on achieving state-of-the-art empirical results. Unfortunately, HPO for DL is a relatively under-explored field and most DL researchers still optimize their hyperparameters via obscure trial-and-error practices. On the other hand, traditional Bayesian Optimization HPO methods [Snoek et al., 2012, Bergstra et al., 2011] are not directly applicable to deep networks, due to the infeasibility of evaluating a large number of hyperparameter configurations. In order to scale HPO for DL, three main directions of research have been recently explored. (i) Online HPO methods search for hyperparameters during the optimization process via meta-level controllers [Chen et al., 2017, Parker-Holder et al., 2020], however, this online adaptation can not accommodate all hyperparameters (e.g. related to architectural changes). (ii) Gradient-based HPO techniques, on the other hand, compute the derivative of the validation loss w.r.t. hyperparameters by reversing the training update steps [Maclaurin et al., 2015, Franceschi et al., 2017, Lorraine et al., 2020], however, the reversion is not directly applicable to all cases (e.g. dropout rate). The last direction, (iii) Gray-box HPO techniques discard sub-optimal configurations after evaluating them on lower budgets [Li et al., 2017, Falkner et al., 2018]. In contrast to the online and gradient-based alternatives, gray-box approaches can be deployed in an off-the-shelf manner to all types of hyperparameters and architectures. The gray-box concept is based on the intuition that a poorly-performing hyperparameter configuration can be identified and ∗equal contribution †work does not relate to position at Amazon 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2202.09774v2  [cs.LG]  1 Jun 2023terminated by inspecting the validation loss of the first few epochs, instead of waiting for the full convergence. The most prominent gray-box algorithm is Hyperband [Li et al., 2017], which is based on successive halving. It runs random configurations at different budgets (e.g. number of epochs) and successively halves these configurations by keeping only the top performers. Follow-up works, such as BOHB [Falkner et al., 2018] or DEHB [Awad et al., 2021], replace the random sampling of Hyperband with a sampling based on Bayesian optimization or differentiable evolution. Despite their great practical potential, gray-box methods suffer from a major issue. The low-budget (few epochs) performances are not always a good indicator for the full-budget (full convergence) performances. For example, a properly regularized network converges slower in the first few epochs, however, typically performs better than a non-regularized variant after the full convergence. In other words, there can be a poor rank correlation of the configurations’ performances at different budgets. Figure 1: Top: The learning curve for different hyperparameter configurations. The darker the learning curve, the later it was evaluated during the search. Bottom: The hyperparameter indices in a temporal order as evaluated during the opti- mization and their corresponding curves. We introduceDYHPO, a Bayesian Optimization (BO) approach based on Gaussian Processes (GP), that proposes a novel treatment to the multi-budget (a.k.a. multi-fidelity) setup. In this perspective, we propose a deep kernel GP that captures the learning dynamics. As a result, we train a kernel capable of capturing the similarity of a pair of hyperparameter configurations, even if the pair’s configurations are evaluated at differ- ent budgets. Furthermore, we extend Expected Improvement [Jones et al., 1998] to the multi- budget case, by introducing a new mechanism for the incumbent configuration of a budget. We illustrate the differences between our rac- ing strategy and successive halving with the experiment of Figure 1, where, we showcase the HPO progress of two different methods on the \"Helena\" dataset from the LCBench bench- mark [Zimmer et al., 2021]. Hyperband [Li et al., 2017] is a gray-box approach that stati- cally pre-allocates the budget for a set of candidates (Hyperband bracket) according to a predefined policy. However, DYHPO dynamically adapts the allocation of budgets for configurations after every HPO step (a.k.a. a dynamic race). As a result, DYHPO invests only a small budget on configurations that show little promise as indicated by the intermediate scores. The joint effect of modeling a GP kernel across budgets together with a dedicated acquisition function leads to DYHPO achieving a statistically significant empirical gain against state-of-the-art gray-box baselines [Falkner et al., 2018, Awad et al., 2021], including prior work on multi-budget GPs [Kandasamy et al., 2017, 2020] or neural networks [Li et al., 2020b]. We demonstrate the performance of DYHPO in three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets of three diverse modalities (tabular, image, natural language processing). We believe our method is a step forward toward making HPO for DL practical and feasible. Overall, our contributions can be summarized as follows: • We introduce a novel Bayesian surrogate for gray-box HPO optimization. Our novel surrogate model predicts the validation score of a machine learning model based on both the hyperparameter configuration, the budget information, and the learning curve. • We derive a simple yet robust way to combine this surrogate model with Bayesian opti- mization, reusing most of the existing components currently used in traditional Bayesian optimization methods. • Finally, we demonstrate the efficiency of our method for HPO and neural architecture search tasks compared to the current state-of-the-art methods in HPO, by outperforming seven strong HPO baselines with a statistically significant margin. As an overarching goal, we believe our method is an important step toward scaling HPO for DL. 22 Related Work on Gray-box HPO Multi-Fidelity Bayesian Optimization and Bandits. Bayesian optimization is a black-box func- tion optimization framework that has been successfully applied in optimizing hyperparameter and neural architectures alike [Snoek et al., 2012, Kandasamy et al., 2018, Bergstra et al., 2011]. To further improve Bayesian optimization, several works propose low-fidelity data approximations of hyperparameter configurations by training on a subset of the data [Swersky et al., 2013, Klein et al., 2017a], or by terminating training early [Swersky et al., 2014]. Additionally, several methods extend Bayesian optimization to multi-fidelity data by engineering new kernels suited for this problem [Swer- sky et al., 2013, 2014, Poloczek et al., 2017]. Kandasamy et al. [2016] extends GP-UCB [Srinivas et al., 2010] to the multi-fidelity setting by learning one Gaussian Process (GP) with a standard kernel for each fidelity. Their later work improves upon this method by learning one GP for all fidelities that enables the use of continuous fidelities [Kandasamy et al., 2017]. The work by Takeno et al. [2020] follows a similar idea but proposes to use an acquisition function based on information gain instead of UCB. While most of the works rely on GPs to model the surrogate function, Li et al. [2020b] use a Bayesian neural network that models the complex relationship between fidelities with stacked neural networks, one for each fidelity. Hyperband [Li et al., 2017] is a bandits-based multi-fidelity method for hyperparameter optimization that selects hyperparameter configurations at random and uses successive halving [Jamieson and Tal- walkar, 2016] with different settings to early-stop less promising training runs. Several improvements have been proposed to Hyperband with the aim to replace the random sampling of hyperparameter configurations with a more guided approach [Bertrand et al., 2017, Wang et al., 2018, Wistuba, 2017]. BOHB [Falkner et al., 2018] uses TPE [Bergstra et al., 2011] and builds a surrogate model for every fidelity adhering to a fixed-fidelity selection scheme. DEHB [Awad et al., 2021] samples candidates using differential evolution which handles large and discrete search spaces better than BOHB. Mendes et al. [2021] propose a variant of Hyperband which allows to skip stages. Learning Curve Prediction A variety of methods attempt to extrapolate a partially observed learning curve in order to estimate the probability that a configuration will improve over the current best solution. Domhan et al. [2015] propose to ensemble a set of parametric functions to extrapolate a partial learning curve. While this method is able to extrapolate with a single example, it requires a relatively long learning curve to do so. The work by Klein et al. [2017b] build upon the idea of using a set of parametric functions. The main difference is that they use a heteroscedastic Bayesian model to learn the ensemble weights. Baker et al. [2018] propose to use support vector machines (SVM) as an auto-regressive model. The SVM predicts the next value of a learning curve, the original learning curve is augmented by this value and we keep predicting further values. The work by Gargiani et al. [2019] use a similar idea but makes prediction based on the last K observations only and uses probabilistic models. Wistuba and Pedapati [2020] propose to learn a prediction model across learning curves from different tasks to avoid the costly learning curve collection. In contrast to DYHPO , none of these methods selects configuration but is limited to deciding when to stop a running configuration. Multi-Fidelity Acquisition Functions Klein et al. [2017a] propose an acquisition function which allows for selecting hyperparameter configurations and the dataset subset size. The idea is to reduce training time by considering only a smaller part of the training data. In contrast to EIMF, this acquisition function is designed to select arbitrary subset sizes whereas EIMF is intended to slowly increase the invested budget over time. Mendes et al. [2020] extend the work of Klein et al. [2017a] to take business constraints into account. Deep Kernel Learning with Bayesian Optimization. We are among the first to use deep kernel learning with Bayesian optimization and to the best of our knowledge the first to use it for multi- fidelity Bayesian optimization. Rai et al. [2016] consider the use of a deep kernel instead of a manually designed kernel in the context of standard Bayesian optimization, but, limit their experimentation to synthetic data and do not consider its use for hyperparameter optimization. Perrone et al. [2018], Wistuba and Grabocka [2021] use a pre-trained deep kernel to warm start Bayesian optimization with meta-data from previous optimizations. The aforementioned approaches are multi-task or transfer learning methods that require the availability of meta-data from related tasks. 3In contrast to prior work, we propose a method that introduces deep learning to multi-fidelity HPO with Bayesian Optimization, and captures the learning dynamics across fidelities/budgets, combined with an acquisition function that is tailored for the gray-box setup. 3 Dynamic Multi-Fidelity HPO 3.1 Preliminaries Gray-Box Optimization. The gray-box HPO setting allows querying configurations with a smaller budget compared to the total maximal budget B. Thus, we can query from the response function f : X ×N → R where fi,j = f(xi, j) is the response after spending a budget ofj on configuration xi. As before, these observations are noisy and we observe yi,j = f(xi, j) +εj where εj ∼ N(0, σ2 j,n). Please note, we assume that the budget required to query fi,j+b after having queried fi,j is only b. Furthermore, we use the learning curve Yi,j−1 = (yi,1, . . . , yi,j−1) when predicting fi,j. Gaussian Processes (GP). Given a training data set D = {(xi, yi)}n i=1, the Gaussian Process assumption is that yi is a random variable and the joint distribution of all yi is assumed to be multivariate Gaussian distributed as y ∼ N(m (X) , k(X, X)) . Furthermore, f∗ for test instances x∗ are jointly Gaussian with y as:\u0014 y f∗ \u0015 ∼ N \u0012 m (X, x∗) , \u0012 Kn K∗ KT ∗ K∗∗ \u0013\u0013 . (1) The mean function m is often set to 0 and its covariance function k depends on parameters θ. For notational convenience, we use Kn = k (X, X|θ) +σ2 nI, K∗ = k (X, X∗|θ) and K∗∗ = k (X∗, X∗|θ) to define the kernel matrices. We can derive the posterior predictive distribution with mean and covariance as follows: E[f∗|X, y, X∗] =KT ∗ K−1 n y, cov [f∗|X, X∗] =K∗∗ − KT ∗ K−1 n K∗ (2) Often, the kernel function is manually engineered, one popular example is the squared exponential kernel. However, in this work, we make use of the idea of deep kernel learning [Wilson et al., 2016]. The idea is to model the kernel as a neural network φ and learn the best kernel transformation K (θ, w) :=k(φ(x, w), φ(x′; w)|θ), which allows us to use convolutional operations in our kernel. 3.2 Deep Multi-Fidelity Surrogate We propose to use a Gaussian Process surrogate model that infers the value of fi,j based on the hyperparameter configuration xi, the budget j as well as the past learning curve Yi,j−1. For this purpose, we use a deep kernel as: K (θ, w) :=k(φ(xi, Yi,j−1, j; w), φ(xi′ , Yi′,j′−1, j′; w); θ) (3) xi j Yi,j−1 · 1B P P P Convolution max P P P Figure 2: The feature extractor φ of our kernel. We use a squared exponential kernel for k and the neural network φ is composed of linear and convolutional layers as shown in Figure 2. We normalize the budget j to a range between 0 and 1 by dividing it by the maximum budget B. Afterward, it is concatenated with the hyper- parameter configuration xi and fed to a linear layer. The learning curve Yi,j−1 is transformed by a one-dimensional convolution followed by a global max pooling layer. Finally, both repre- sentations are fed to another linear layer. Its output will be the input to the kernel function k. Both, the kernel k and the neural network φ consist of trainable parameters θ and w, respectively. We find their optimal values by computing the maximum likelihood estimates as: ˆθ, ˆw = arg max θ,w p(y|X, Y, θ, w) ∝ arg min θ,w yTK (θ, w)−1 y + log|K (θ, w)| (4) 4In order to solve this optimization problem, we use gradient descent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1. Given the maximum likelihood estimates, we can approximate the predictive posterior through p \u0010 fi,j|xi, Yi,j−1, j,D, ˆθ, ˆw \u0011 , and ultimately compute the mean and covariance of this Gaussian using Equation 2. 3.3 Multi-Fidelity Expected Improvement Expected improvement [Jones et al., 1998] is a commonly used acquisition function and is defined as: EI(x|D) =E[max {f(x) − ymax, 0}] , (5) where ymax is the largest observed value of f. We propose a multi-fidelity version of it as: EIMF(x, j|D) =E \u0002 max \b f(x, j) − ymax j , 0 \t\u0003 , (6) where: ymax j = \u001amax {y | ((x, ·, j), y) ∈ D} if ((x, ·, j), y) ∈ D max {y | (·, y) ∈ D} otherwise (7) Simply put, ymax j is the largest observed value of f for a budget of j if it exists already, otherwise, it is the largest observed value for any budget. If there is only one possible budget, the multi-fidelity expected improvement is identical to expected improvement. 3.4 The D YHPO Algorithm The DYHPO algorithm looks very similar to many black-box Bayesian optimization algorithms as shown in Algorithm 1. The big difference is that at each step we dynamically decide which candidate configuration to train for a small additional budget. Algorithm 1 DYHPO Algorithm 1: b(x) = 0∀x ∈ X 2: while not converged do 3: xi ← arg maxx∈X EIMF (x, b(x) + 1)(Sec. 3.3) 4: Observe yi,b(xi)+1. 5: b(xi) ← b(xi) + 1 6: D ← D ∪ \b ((xi, Yi,b(xi)−1, b(xi)), yi,b(xi)) \t 7: Update the surrogate on D. (Sec. 3.2) return xi with largest yi,·. Possible candidates are previously un- considered configurations as well as configurations that did not reach the maximum budget. In Line 2, the most promising candidate is chosen using the acquisition function introduced in Section 3.3 and the surrogate model’s predictions. It is important to high- light that we do not maximize the ac- quisition function along the budget di- mensionality. Instead, we set the bud- get b such that it is by exactly one higher than the budget used to eval- uate xi before. This ensures that we explore configurations by slowly increasing the budget. After the candidate and the corresponding budget are selected, the function f is evaluated and we observe yi,j (Line 3). This additional data point is added to D in Line 4. Then in Line 5, the surrogate model is updated according to the training scheme described in Section 3.2. 4 Experimental Protocol 4.1 Experimental Setup We evaluate DYHPO in three different settings on hyperparameter optimization for tabular, text, and image classification against several competitor methods, the details of which are provided in the following subsections. We ran all of our experiments on an Amazon EC2 M5 Instance (m5.xlarge). In our experiments, we report the mean of ten repetitions and we report two common metrics, the regret and the average rank. The regret refers to the absolute difference between the score of the solution found by an optimizer compared to the best possible score. If we report the regret as an aggregate result over multiple datasets, we report the mean over all regrets. The average rank is the metric we use to aggregate rank results over different datasets. We provide further implementation and training details in Appendix A.4. Our implementation of DYHPO is publicly available.3 3https://github.com/releaunifreiburg/DyHPO 54.2 Benchmarks In our experiments, we use the following benchmarks. We provide more details in Appendix A.1. LCBench: A learning curve benchmark [Zimmer et al., 2021] that evaluates neural network architec- tures for tabular datasets. LCBench contains learning curves for 35 different datasets, where 2,000 neural networks per dataset are trained for 50 epochs with Auto-PyTorch. TaskSet: A benchmark that features diverse tasks Metz et al. [2020] from different domains and includes 5 search spaces with different degrees of freedom, where, every search space includes 1000 hyperparameter configurations. In this work, we focus on a subset of NLP tasks (12 tasks) and we use the Adam8p search space with 8 continuous hyperparameters. NAS-Bench-201: A benchmark consisting of 15625 hyperparameter configurations representing different architectures on the CIFAR-10, CIFAR-100 and ImageNet datasets Dong and Yang [2020]. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. 4.3 Baselines Random Search: A random/stochastic black-box search method for HPO. HyperBand: A multi-arm bandit method that extends successive halving by multiple brackets with different combinations of the initial number of configurations, and their initial budget [Li et al., 2017]. BOHB: An extension of Hyperband that replaces the random sampling of the initial configurations for each bracket with recommended configurations from a model-based approach [Falkner et al., 2018]. BOHB builds a model for every fidelity that is considered. DEHB: A method that builds upon Hyperband by exploiting differential evolution to sample the initial candidates of a Hyperband bracket [Awad et al., 2021]. ASHA: An asynchronous version of successive halving (or an asynchronous version of Hyperband if multiple brackets are run). ASHA Li et al. [2020a] does not wait for all configurations to finish inside a successive halving bracket, but, instead promotes configurations to the next successive halving bracket in real-time. MF-DNN: A multi-fidelity Bayesian optimization method that uses deep neural networks to capture the relationships between different fidelities Li et al. [2020b]. Dragonfly: We compare against BOCA [Kandasamy et al., 2017] by using the Dragonfly library Kan- dasamy et al. [2020]. This method suggests the next hyperparameter configuration as well as the budget it should be evaluated for. 4.4 Research Hypotheses and Associated Experiments Hypothesis 1: DYHPO achieves state-of-the-art results in multi-fidelity HPO. Experiment 1: We compare DYHPO against the baselines of Section 4.3 on the benchmarks of Section 4.2 with the experimental setup of Section 4.1. For TaskSet we follow the authors’ recommendation and report the number of steps (every 200 iterations). Hypothesis 2: DYHPO’s runtime overhead has a negligible impact on the quality of results. Experiment 2: We compare DYHPO against the baselines of Section 4.3 over the wallclock time. The wallclock time includes both (i) the optimizer’s runtime overhead for recommending the next hyperparameter configuration, plus (ii) the time needed to evaluate the recommended configuration. In this experiment, we consider all datasets where the average training time per epoch is at least 10 seconds, because, for tasks where the training time is short, there is no practical justification for complex solutions and their overhead. In these cases, we recommend using a random search. We don’t report results for TaskSet because the benchmark lacks training times. Hypothesis 3: DYHPO uses the computational budget more efficiently than baselines. Experiment 3: To further verify that DYHPO is efficient compared to the baselines, we investigate whether competing methods spend their budgets on qualitative candidates. Concretely we: i) calculate 60 500 1000 Number of Epochs 10 2 10 1 Mean Regret LCBench 0 500 1000 Number of Steps 10 1 TaskSet 0 2000 4000 Number of Epochs 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 3: The mean regret for the different benchmarks over the number of epochs or steps (every 200 iterations). The results are aggregated over 35 different datasets for LCBench and aggregated over 12 different NLP tasks for TaskSet. the precision of the top (w.r.t. ground truth) performing configurations that were selected by each method across different budgets, ii) compute the average regret of the selected configurations across budget, and iii) we compare the fraction of top-performing configurations at a given budget that were not top performers at lower budgets, i.e. measure the ability to handle the poor correlation of performances across budgets. 5 Results Experiment 1: DYHPO achieves state-of-the-art results. In our first experiment, we evaluate the various methods on the benchmarks listed in Section 4.2. We show the aggregated results in Figure 3, the results show that DYHPO manages to outperform competitor methods over the set of considered benchmarks by achieving a better mean regret across datasets. Not only does DYHPO achieve a better final performance, it also achieves strong anytime results by converging faster than the competitor methods. For the extended results, related to the performance of all methods on a dataset level, we refer the reader to Appendix B. 12345678 Random Dragonfly Hyperband MF-DNN ASHA BOHB DEHB DyHPO LCBench@50% 12345678 Random Hyperband MF-DNN Dragonfly DEHB BOHB ASHA DyHPO LCBench@100% 12345678 Random MF-DNN ASHA Dragonfly Hyperband DEHB BOHB DyHPO TaskSet@50% 12345678 Random MF-DNN ASHA Dragonfly DEHB Hyperband BOHB DyHPO TaskSet@100% Figure 4: Critical difference diagram for LCBench and TaskSet in terms of the number of HPO steps. The results correspond to results after 500 and 1,000 epochs. Connected ranks via a bold bar indicate that performances are not significantly different (p >0.05). In Figure 4, we provide further evidence thatDYHPO’s improvement over the baselines is statistically significant. The critical difference diagram presents the ranks of all methods and provides information on the pairwise statistical difference between all methods for two fractions of the number of HPO steps (50% and 100%). We included the LCBench and TaskSet benchmarks in our significance plots. NAS-Bench-201 was omitted because it has only 3 datasets and the statistical test cannot be applied. Horizontal lines indicate groupings of methods that are not significantly different. As suggested by the best published practices Demsar [2006], we use the Friedman test to reject the null hypothesis followed by a pairwise post-hoc analysis based on the Wilcoxon signed-rank test (α = 0.05). For LCBench, DYHPO already outperforms the baselines significantly after 50% of the search budget, with a statistically significant margin. As the optimization procedure continues, DYHPO manages to extend its gain in performance and is the only method that has a statistically significant improvement against all the other competitor methods. Similarly, for TaskSet, DYHPO manages to outperform all methods with a statistically significant margin only halfway through the optimization procedure and achieves the best rank over all methods. However, as the optimization procedure continues, BOHB manages to decrease the performance gap with DYHPO , although, it still achieves a worse rank across all datasets. Considering the empirical results, we conclude that Hypothesis 1 is validated and that DYHPO achieves state-of-the-art results on multi-fidelity HPO. Experiment 2: On the impact of DYHPO ’s overhead on the results. We present the results of our second experiment in Figure 5 (left), where, as it can be seen, DYHPO still outperforms the other methods when its overhead is considered. For LCBench, DYHPO manages to get an advantage 70.2 0.4 0.6 0.8 1.0 Normalized Wallclock Time 10 1 Mean Regret LCBench 103 105 Wallclock Time in Seconds 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@50% 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@100% Figure 5: Left: The regret over time for all methods during the optimization procedure for the LCBench benchmark and the ImageNet dataset from the NAS-Bench-201 benchmark. The normalized wall clock time represents the actual run time divided by the total wall clock time ofDYHPO including the overhead of fitting the deep GP. Right: The critical difference diagram for LCBench halfway through the HPO wall-clock time, and in the end. Connected ranks via a bold bar indicate that differences are not significant (p >0.05). fairly quickly and it only increases the gap in performance with the other methods as the optimization process progresses. Similarly, in the case of ImageNet from NAS-Bench-201, DYHPO manages to gain an advantage earlier than other methods during the optimization procedure. Although in the end DYHPO still performs better than all the other methods, we believe most of the methods converge to a good solution and the differences in the final performance are negligible. For the extended results, related to the performance of all methods on a dataset level over time, we refer the reader to the plots in Appendix B. Additionally, in Figure 5 (right), we provide the critical difference diagrams for LCBench that present the ranks and the statistical difference of all methods halfway through the optimization procedure, and in the end. As it can be seen, DYHPO has a better rank with a significant margin with only half of the budget used and it retains the advantage until the end. Experiment 3: On the efficiency of DYHPO . In Figure 6 (left), we plot the precision of every method for different budgets during the optimization procedure, which demonstrates that DYHPO effectively explores the search space and identifies promising candidates. The precision at an epoch i is defined as the number of top 1% candidates that are trained, divided by the number of all candidates trained, both trained for at least i epochs. The higher the precision, the more relevant candidates were considered and the less computational resources were wasted. For small budgets, the precision is low since DYHPO spends budget to consider various candidates, but then, promising candidates are successfully identified and the precision quickly increases. This argument is further supported in Figure 6 (middle), where we visualize the average regret of all the candidates trained for at least the specified number of epochs on the x-axis. In contrast to the regret plots, here we do not show the regret of the best configuration, but the mean regret of all the selected configurations. The analysis deduces a similar finding, our method DYHPO selects more qualitative hyperparameter configurations than all the baselines. An interesting property of multi-fidelity HPO is the phenomenon of poor rank correlations among the validation performance of candidates at different budgets. In other words, a configuration that achieves a poor performance at a small budget can perform better at a larger budget. To analyze this phenomenon, we measure the percentage of \"good\" configurations at a particular budget, that were \"bad\" performers in at least one of the smaller budgets. We define a \"good\" performance at a budget B when a configuration achieves a validation accuracy ranked among the top 1/3 of the validation accuracies belonging to all the other configurations that were run until that budget B. In Figure 6 (right), we analyze the percentage of \"good\" configurations at each budget denoted by the x-axis, that were \"bad\" performers in at least one of the lower budgets. Such a metric is a proxy for the degree of the promotion of \"bad\" configurations towards higher budgets. We present the analysis for all the competing methods of our experimental protocol from Section 4. We have additionally included the ground-truth line annotated as \"Baseline\", which represents the fraction of past poor performers among all the feasible configurations in the search space. In contrast, the respective methods compute the fraction of promotions only among the configurations that those methods have considered (i.e. selected within their HPO trials) until the budget indicated by the x-axis. We see that there is a high degree of \"good\" configurations that were \"bad\" at a previous budget, with fractions of the ground-truth \"Baseline\" going up to 40% for the LCBench benchmark and up to 80% for the NAS-Bench-201 benchmark. 80 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3 0.4Precision of Top Candidates LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.05 0.10 0.15Average Regret LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline 0 50 100 150 200 Number of Epochs 0.05 0.10 0.15Precision of Top Candidates NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.04 0.06 0.08 0.10 0.12Average Regret NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Poor Performer Promotions NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 6: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the LCBench and NAS-Bench-201 benchmark. On the other hand, the analysis demonstrates that our method DYHPO has promoted more \"good\" configurations that were \"bad\" in a lower budget, compared to all the rival methods. In particular, more than 80% of selected configurations from the datasets belonging to either benchmark were \"bad\" performers at a lower budget. The empirical evidence validates Hypothesis 3 and demonstrates that DYHPO efficiently explores qualitative candidates. We provide the results of our analysis for DYHPO’s efficiency on the additional benchmarks (Taskset) in Appendix B. Ablating the impact of the learning curve 102 103 104 105 Training Time in Seconds 10 1 Regret ImageNet16-120 DyHPO DyHPO w/o CNN Random Search Figure 7: Ablating the impact of the learning curve on DYHPO. One of the main differences between DYHPO and similar methods Kandasamy et al. [2017], is that the learning curve is an input to the kernel function. For this reason, we investigate the impact of this design choice. We consider a variation of DYHPO w/o CNN, which is simply DYHPO without the learning curve. It is worth emphasizing that both variants (with and without the learning curve) are multi-fidelity surrogates and both receive the budget informa- tion through the inputted index j in Equation 3. The only difference is that DYHPO additionally incorporates the pattern of the learning curve. We run the ablation on the NAS-Bench-201 benchmark and report the results for ImageNet, the largest dataset in our collection. The ablation results are shown in Figure 7, while the remaining results on the other datasets are shown in Figure 8 of the appendix. Based on the results from our learning curve ablation, we conclude that the use of an explicit learning curve representation leads to significantly better results. 6 Limitations of Our Method Although DYHPO shows a convincing and statistically significant reduction of the HPO time on diverse Deep Learning (DL) experiments, we cautiously characterized our method only as a \"step 9towards\" scaling HPO for DL. The reason for our restrain is the lack of tabular benchmarks for HPO on very large deep learning models, such as Transformers-based architectures [Devlin et al., 2019]. Additionally, the pause and resume part of our training procedure can only be applied when tuning the hyperparameters of parametric models, otherwise, the training of a hyperparameter configuration would have to be restarted. Lastly, for small datasets that can be trained fast, the overhead of model-based techniques would make an approach like random search more appealing. 7 Conclusions In this work, we present DYHPO , a new Bayesian optimization (BO) algorithm for the gray-box setting. We introduced a new surrogate model for BO that uses a learnable deep kernel and takes the learning curve as an explicit input. Furthermore, we motivated a variation of expected improvement for the multi-fidelity setting. Finally, we compared our approach on diverse benchmarks on a total of 50 different tasks against the current state-of-the-art methods on gray-box hyperparameter optimization (HPO). Our method shows significant gains and has the potential to become the de facto standard for HPO in Deep Learning. Acknowledgments Josif Grabocka and Arlind Kadra would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools center of excellence. References Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. doi: 10.24963/ijcai.2021/296. URL https://doi.org/ 10.24963/ijcai.2021/296. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. URL https://openreview.net/forum?id=HJqk3N1vG. James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 2546–2554, 2011. URL https://proceedings.neurips.cc/ paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html. Hadrien Bertrand, Roberto Ardon, Matthieu Perrot, and Isabelle Bloch. Hyperparameter optimization of deep neural networks: Combining hyperband with bayesian model selection. In Conférence sur l’Apprentissage Automatique, 2017. Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. URL http://proceedings. mlr.press/v70/chen17e.html. Janez Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7: 1–30, 2006. URL http://jmlr.org/papers/v7/demsar06a.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 10the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/ 10.18653/v1/n19-1423. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Qiang Yang and Michael J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/487. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://openreview.net/forum?id=HJxyZkBKDr. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimiza- tion at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. URL http://proceedings.mlr.press/v80/falkner18a.html. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1165–1173, 2017. URL http://proceedings.mlr.press/v70/franceschi17a.html. Jacob R. Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wil- son. Gpytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 7587–7597, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 27e8e17134dd7083b050476733207ea1-Abstract.html. Matilde Gargiani, Aaron Klein, Stefan Falkner, and Frank Hutter. Probabilistic rollouts for learning curve extrapolation across hyperparameter settings. CoRR, abs/1910.04522, 2019. URL http: //arxiv.org/abs/1910.04522. Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pages 240–248, 2016. URL http://proceedings.mlr.press/v51/jamieson16.html. Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of ex- pensive black-box functions. J. Global Optimization , 13(4):455–492, 1998. doi: 10.1023/A: 1008306431147. URL https://doi.org/10.1023/A:1008306431147. Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff G. Schneider, and Barn- abás Póczos. Gaussian process bandit optimisation with multi-fidelity evaluations. In Advances in Neural Information Processing Systems 29: Annual Conference on Neu- ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pages 992–1000, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html. Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi-fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. URL http://proceedings.mlr.press/v70/kandasamy17a.html. Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Neural architecture search with bayesian optimisation and optimal transport. In Ad- vances in Neural Information Processing Systems 31: Annual Conference on Neural Infor- mation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 2020–2029, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ f33ba15effa5c10e873bf3842afb46a6-Abstract.html. 11Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21: 81:1–81:27, 2020. URL http://jmlr.org/papers/v21/18-223.html. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages 528–536, 2017a. URL http://proceedings.mlr.press/ v54/klein17a.html. Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017b. URL https://openreview.net/forum?id=S11KBYclx. Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors,Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org, 2020a. URL https://proceedings.mlsys.org/book/303.pdf. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18: 185:1–185:52, 2017. URL http://jmlr.org/papers/v18/16-558.html. Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: An- nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/ 60e1deb043af37db5ea4ce9ae8d2c9ea-Abstract.html. Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. URL http://proceedings.mlr.press/v108/lorraine20a.html. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2113–2122, 2015. URL http://proceedings.mlr.press/v37/maclaurin15.html. Pedro Mendes, Maria Casimiro, Paolo Romano, and David Garlan. Trimtuner: Efficient optimization of machine learning jobs in the cloud via sub-sampling. In 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, MASCOTS 2020, Nice, France, November 17-19, 2020, pages 1–8. IEEE, 2020. doi: 10.1109/MASCOTS50786. 2020.9285971. URL https://doi.org/10.1109/MASCOTS50786.2020.9285971. Pedro Mendes, Maria Casimiro, and Paolo Romano. Hyperjump: Accelerating hyperband via risk modelling. CoRR, abs/2108.02479, 2021. URL https://arxiv.org/abs/2108.02479. Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl- Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. URL https://arxiv.org/abs/2002.11887. Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparameter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html. 12Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, and Cédric Archambeau. Scalable hyper- parameter transfer learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6846–6856, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/14c879f3f5d8ed93a09f6090d77c2cc3-Abstract.html. Matthias Poloczek, Jialei Wang, and Peter I. Frazier. Multi-information source optimiza- tion. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 4288–4298, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ df1f1d20ee86704251795841e6a9405a-Abstract.html. Akshara Rai, Ruta Desai, and Siddharth Goyal. Bayesian optimization with a neural network kernel, 2016. URL http://www.cs.cmu.edu/~rutad/files/BO_NN.pdf. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 2960–2968, 2012. URL https://proceedings. neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html. Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 1015–1022, 2010. URL https://icml.cc/Conferences/2010/papers/422.pdf. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 2004–2012, 2013. URL https://proceedings.neurips. cc/paper/2013/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. URL http://arxiv.org/abs/1406.3896. Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 9334–9345, 2020. URLhttp://proceedings. mlr.press/v119/takeno20a.html. Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. CoRR, abs/1801.01596, 2018. URL http: //arxiv.org/abs/1801.01596. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 370–378, 2016. URL http://proceedings.mlr. press/v51/wilson16.html. Martin Wistuba. Bayesian optimization combined with incremental evaluation for neural network architecture optimization. In AutoML@PKDD/ECML, 2017. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/forum?id=bJxgv5C3sYc. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10303–10312. PMLR, 2020. URL http://proceedings.mlr.press/v119/wistuba20a.html. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl. IEEE Trans. Pattern Anal. Mach. Intell., 43(9):3079–3090, 2021. doi: 10.1109/TPAMI.2021.3067763. URL https://doi.org/10.1109/TPAMI.2021.3067763. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section “Societal Implications” in the Appendix. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] We provide our main algorithm in Section 3 and we additionally provide the detailed implementation details in Appendix A for all methods and benchmarks. We will release the code for the camera-ready version of our work. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please see Appendix A. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] We report the statistical significance of the performance difference between our method and the baselines in Section 5 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4.2 and Section 4.3. (b) Did you mention the license of the assets? [Yes] See Appendix A.1 and A.5 where we provide references to the assets where the license is included. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] The benchmarks and baselines are open-sourced. (e) Did you discuss whether the data you are using/curating contains personally identi- fiable information or offensive content? [N/A] The data does not contain personally identifiable information or offensive content. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14Societal Implications In our work, we use only publicly available data with no privacy concerns. Furthermore, our algorithm reduces the overall time for fitting deep networks, therefore, saving computational resources and yielding a positive impact on the environment. Moreover, our method can help smaller research organizations with limited access to resources to be competitive in the deep learning domain, which reduces the investment costs on hardware. Although our method significantly reduces the time taken for optimizing a machine learning algorithm that achieves peak performance, we warn against running our method for an extended time only to achieve marginal gains in performance, unless it is mission-critical. Last but not least, in order to save energy, we invite the community to create sparse benchmarks with surrogates, instead of dense tabular ones. A Experimental Setup A.1 Benchmarks LCBench. LCBench4 is a feedforward neural network benchmark on tabular data which consists of 2000 configuration settings for each of the 35 datasets. The configurations were evaluated during HPO runs with AutoPyTorch. LCBench features a search space of 7 numerical hyperparameters, where every hyperparameter configuration is trained for 50 epochs. The objective is to optimize seven different hyperparameters of funnel-shaped neural networks, i.e., batch size, learning rate, momentum, weight decay, dropout, number of layers, and maximum number of units per layer. TaskSet. TaskSet5 is a benchmark that features over 1162 diverse tasks from different domains and includes 5 search spaces. In this work, we focus on NLP tasks and we use the Adam8p search space with 8 continuous hyperparameters. We refer to Figure 11 for the exact task names considered in our experiments. The learning curves provided in TaskSet report scores after every 200 iterations. We refer to those as \"steps\". The objective is to optimize eight hyperparameters for a set of different recurrent neural networks (RNN) that differ in embedding size, RNN cell, and other architectural features. The set of hyperparameters consists of optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate of the first and second momentum of Adam, β1 and β2, and Adam’s constant for numerical stability ε. Furthermore, there are two hyperparameters controlling linear and exponential learning rate decays, as well as L1 and L2 regularization terms. NAS-Bench-201. NAS-Bench-2016 is a benchmark that has precomputed about 15,600 archi- tectures trained for 200 epochs for the image classification datasets CIFAR-10, CIFAR-100, and ImageNet. The objective is to select for each of the six operations within the cell of the macro architecture one of five different operations. All other hyperparameters such as learning rate and batch size are kept fixed. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. A.2 Preprocessing In the following, we describe the preprocessing applied to the hyperparameter representation. For LCBench, we apply a log-transform to batch size, learning rate, and weight decay. For TaskSet, we apply it on the learning rate, L1 and L2 regularization terms, epsilon, linear and exponential decay of the learning rate. All continuous hyperparameters are scaled to the range between 0 and 1 using sklearn’s MinMaxScaler. If not mentioned otherwise, we use one-hot encoding for the categorical hyperparameters. As detailed in subsection A.5, some baselines have a specific way of dealing with them. In that case, we use the method recommended by the authors. A.3 Framework The framework contains the evaluated hyperparameters and their corresponding validation curves. The list of candidate hyperparameters is passed to the baseline-specific interface, which in turn, 4https://github.com/automl/LCBench 5https://github.com/google-research/google-research/tree/master/task_set 6https://github.com/D-X-Y/NAS-Bench-201 15optimizes and queries the framework for the hyperparameter configuration that maximizes utility. Our framework in turn responds with the validation curve and the cost of the evaluation. In case a hyperparameter configuration has been evaluated previously up to a budget b and a baseline requires the response for budget b + 1, the cost is calculated accordingly only for the extra budget requested. A.4 Implementation Details We implement the Deep Kernel Gaussian Process using GPyTorch 1.5 [Gardner et al., 2018]. We use an RBF kernel and the dense layers of the transformation functionφ have 128 and 256 units. We used a convolutional layer with a kernel size of three and four filters. All parameters of the Deep Kernel are estimated by maximizing the marginal likelihood. We achieve this by using gradient ascent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1 and batch size of 64. We stop training as soon as the training likelihood is not improving for 10 epochs in a row or we completed 1,000 epochs. For every new data point, we start training the GP with its old parameters to reduce the required effort for training. A.5 Baselines Random Search & Hyperband. Random search and Hyperband sample hyperparameter config- urations at random and therefore the preprocessing is irrelevant. We have implemented both from scratch and use the recommended hyperparameters for Hyperband, i.e. η = 3. BOHB. For our experiments with BOHB, we use version 0.7.4 of the officially-released code7. DEHB. For our experiments with DEHB, we use the official public implementation8. We devel- oped an interface that communicates between our framework and DEHB. In addition to the initial preprocessing common for all methods, we encode categorical hyperparameters with a numerical value in the interval [0, 1]. For a categorical hyperparameter xi, we take Ni equal-sized intervals, where Ni represents the number of unique categorical values for hyperparameter xi and we assign the value for a categorical value n ∈ Ni to the middle of the interval [n, n+ 1]as suggested by the authors. For configuring the DEHB algorithm we used the default values from the library. Dragonfly. We use the publicly available code of Dragonfly9. No special treatment of categorical hyperparameters is required since Dragonfly has its own way to deal with them. We use version 0.1.6 with default settings. MF-DNN. We use the official implementation of MF-DNN by the authors10. Initially, we tried to use multiple incremental fidelity levels like for DYHPO, however, the method runtime was too high and it could not achieve competitive results. For that reason, we use only a few fidelity levels like the authors do in their work Li et al. [2020b]. We use the same fidelity levels as for Hyperband, DEHB, and BOHB to have a fair comparison between the baselines. We also use the same number of initial points as for the other methods to have the same maximal resource allocated for every fidelity level. ASHA-HB. We use the public implementation from the well-known optuna library (version2.10.0). We used the same eta, minimum and maximal budget as for HB, DEHB, and BOHB in our experi- ments, to have a fair comparison. B Additional Plots In Figure 8, we ablate the learning curve input in our kernel, to see the effect it has on performance for the CIFAR-10 and CIFAR-100 datasets from the NAS-Bench-201 benchmark. The results indicate that the learning curve plays an important role in achieving better results by allowing faster convergence and a better anytime performance. 7https://github.com/automl/HpBandSter 8https://github.com/automl/DEHB/ 9https://github.com/dragonfly/dragonfly 10https://github.com/shib0li/DNN-MFBO 16102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar10 DyHPO DyHPO w/o CNN Random Search 102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar100 DyHPO DyHPO w/o CNN Random Search Figure 8: The learning curve ablation for the CIFAR-10 and CIFAR-100 tasks of NAS-Bench-201. 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 9: NAS-Bench-201 regret results over the number of epochs spent during the optimization. Additionally, in Figure 9, we show the performance comparison over the number of epochs of every method for the CIFAR-10 and CIFAR-100 datasets in the NAS-Bench-201 benchmark. While, in Figure 10, we present the performance comparison over time. As can be seen, DYHPO converges faster and has a better performance compared to the other methods over the majority of the time or steps, however, towards the end although it is the optimal method or close to the optimal method, the difference in regret is not significant anymore. Furthermore, Figure 11 shows the performance comparison for the datasets chosen from TaskSet over the number of steps. Looking at the results, DYHPO is outperforming all methods convincingly on the majority of datasets by converging faster and with significant differences in the regret evaluation metric. In Figure 12 and 13, we show the performance comparison for all the datasets from LCBench regarding regret over the number of epochs. Similarly, in Figure 14 and 15, we show the same performance comparison, however, over time. As can be seen, DYHPO manages to outperform the other competitors in the majority of the datasets, and in the datasets that it does not, it is always close to the top-performing method, and the difference between methods is marginal. In Figure 16 we provide the extended results of Experiment 3 for TaskSet. We show the precision, average regret, and promotion percentage for poor-performing configurations for DYHPO and the other competitor methods. Lastly, we explore the behavior of DYHPO after finding the configuration which is returned at the end of the optimization as the best configuration. In Figure 17, we show how the budget is distributed on the configurations considered during that part of the optimization. Clearly, DYHPO is spending very little budget on most configurations. Furthermore, we investigated how many new configurations are considered during this phase. For LCBench, 76.98% of considered configurations are new demonstrating that DYHPO is investigating most of the budget into exploration. These are even more extreme for TaskSet (93.16% and NAS-Bench-201 (97.51%). 17102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 10: NAS-Bench-201 regret results over the total optimization time. The total time includes the method overhead time and the hyperparameter configuration evaluation time. 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_avg_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_embed128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 6 × 10 2 2 × 10 1 3 × 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU64_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 100 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_last_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_E128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 2 × 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN128_tanh_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_tanh_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 11: Performance comparison over the number of steps on a dataset level for TaskSet. 180 200 400 600 800 1000 Number of Epochs 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 100 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 12: Performance comparison over the number of steps on a dataset level for LCBench. 190 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 13: Performance comparison over the number of steps on a dataset level for LCBench (cont.). 200 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 100 200 300 Wallclock Time in Seconds 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2500 5000 7500 10000 12500 Wallclock Time in Seconds 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 10000 Wallclock Time in Seconds 10 2 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 5000 10000 15000 Wallclock Time in Seconds 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 Wallclock Time in Seconds 10 1 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 14: Performance comparison over time on a dataset level for LCBench with the overhead included. 210 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 Wallclock Time in Seconds 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 15: Performance comparison over time on a dataset level for LCBench with the overhead included. (cont.). 220 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3Precision of Top Candidates Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.10 0.15 0.20 0.25 0.30Average Regret Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Steps 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 16: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the Taskset benchmark. 0 10 20 30 40 50 Epochs 0.0 0.2 0.4 0.6Relative Number of Candidates LCBench 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates NAS-Bench-201 0 10 20 30 40 50 Steps 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates TaskSet Figure 17: These plots shed light on how DYHPO behaves after the configuration it finally returns as the best. The plots show how many epochs are spent per candidate. As we can see, for most candidates only a small budget was considered, indicating that DYHPO is mostly exploring at this point. 23",
      "meta_data": {
        "arxiv_id": "2202.09774v2",
        "authors": [
          "Martin Wistuba",
          "Arlind Kadra",
          "Josif Grabocka"
        ],
        "published_date": "2022-02-20T10:28:02Z",
        "pdf_url": "https://arxiv.org/pdf/2202.09774v2.pdf",
        "github_url": "https://github.com/releaunifreiburg/DyHPO"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces DyHPO, a novel Bayesian Optimization (BO) method for multi-fidelity (gray-box) hyperparameter optimization (HPO) in Deep Learning. It addresses sub-optimal budget allocation in existing methods by dynamically deciding which hyperparameter configurations to train further. DyHPO proposes a new deep kernel for Gaussian Processes that embeds learning curve dynamics and an acquisition function that incorporates multi-budget information. The method is shown to significantly outperform state-of-the-art HPO methods on 50 datasets across tabular, image, and NLP modalities, and diverse architectures (MLP, CNN/NAS, RNN), achieving better mean regret and faster convergence with statistical significance.",
        "methodology": "DyHPO is a Bayesian Optimization approach based on Gaussian Processes (GP). It employs a deep kernel GP (K) that models the validation score (fi,j) based on the hyperparameter configuration (xi), budget (j), and the past learning curve (Yi,j-1). The deep kernel uses a neural network (φ) as a feature extractor, which consists of linear and convolutional layers. The budget is normalized and concatenated with the hyperparameter configuration, while the learning curve is processed by a one-dimensional convolution followed by global max pooling. Both representations are then combined and fed to another linear layer, whose output is used by a squared exponential kernel (k). The optimal parameters for the kernel and neural network are found by maximizing the likelihood. For acquisition, DyHPO introduces a Multi-Fidelity Expected Improvement (EIMF) function, which extends standard EI by dynamically defining the incumbent best score (ymax_j) based on whether observations exist for the current budget j or for any budget. The algorithm dynamically selects the most promising candidate to train for one additional budget step and updates the surrogate model with the new observation.",
        "experimental_setup": "DyHPO was evaluated on hyperparameter optimization tasks for tabular, text, and image classification across three benchmarks: 1. LCBench: 35 tabular datasets with 2,000 neural network configurations each, trained for 50 epochs (7 numerical hyperparameters). 2. TaskSet: A subset of 12 NLP tasks with the Adam8p search space (8 continuous hyperparameters), reporting scores every 200 iterations. 3. NAS-Bench-201: 15,625 precomputed architectures on CIFAR-10, CIFAR-100, and ImageNet datasets, trained for 200 epochs (6 categorical hyperparameters). All experiments were run on an Amazon EC2 M5 Instance (m5.xlarge). Performance was measured by mean regret and average rank over ten repetitions. Validation of statistical significance was performed using the Friedman test followed by a pairwise post-hoc Wilcoxon signed-rank test (α = 0.05). Baselines included Random Search, Hyperband, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). An ablation study was conducted to demonstrate the impact of the learning curve input in the deep kernel by comparing DyHPO with a variant that omits this input (DyHPO w/o CNN).",
        "limitations": "The method lacks evaluation on very large deep learning models, such as Transformer-based architectures, due to the absence of suitable tabular benchmarks. The pause and resume functionality of the training procedure is only applicable to parametric models, meaning non-parametric models would require restarting training. For datasets that train very quickly, the computational overhead of model-based techniques like DyHPO might make simpler approaches like random search more practical and appealing. Additionally, the paper cautions against running the method for extended periods for only marginal performance gains unless mission-critical.",
        "future_research_directions": "Implicit future research directions include scaling HPO for very large deep learning models like Transformers and extending the applicability of the pause-and-resume training procedure to a broader range of models beyond parametric ones. Further optimization of the method's overhead for fast-training tasks could also be explored. The authors also invite the community to create sparse benchmarks with surrogates instead of dense tabular ones to save energy, which could guide future benchmark development.",
        "experimental_code": "class DyHPOAlgorithm:    def __init__(        self,        hp_candidates: np.ndarray,        log_indicator: List,        seed: int = 11,        max_benchmark_epochs: int = 52,        fantasize_step: int = 1,        minimization: bool = True,        total_budget: int = 500,        device: str = None,        dataset_name: str = 'unknown',        output_path: str = '.',        surrogate_config: dict = None,        verbose: bool = True,    ):        torch.backends.cudnn.deterministic = True        torch.backends.cudnn.benchmark = False        torch.manual_seed(seed)        np.random.seed(seed)        if device is None:            self.dev = torch.device(                'cuda') if torch.cuda.is_available() else torch.device('cpu')        else:            self.dev = torch.device(device)        self.hp_candidates = hp_candidates        self.log_indicator = log_indicator        self.scaler = MinMaxScaler()        self.hp_candidates = self.preprocess_hp_candidates()        self.minimization = minimization        self.seed = seed        if verbose:            logging_level = logging.DEBUG        else:            logging_level = logging.INFO        self.logger = logging.getLogger()        logging.basicConfig(            format='%(levelname)s:%(asctime)s:%(message)s',            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',            level=logging_level,        )        self.examples = dict()        self.performances = dict()        torch.manual_seed(seed)        np.random.seed(seed)        self.max_benchmark_epochs = max_benchmark_epochs        self.total_budget = total_budget        self.fantasize_step = fantasize_step        self.nr_features = self.hp_candidates.shape[1]        initial_configurations_nr = 1        conf_individual_budget = 1        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)        self.init_budgets = [conf_individual_budget] * initial_configurations_nr        self.fraction_random_configs = 0.1        self.model = None        self.initial_random_index = 0        if surrogate_config is None:            self.surrogate_config = {                'nr_layers': 2,                'nr_initial_features': self.nr_features,                'layer1_units': 64,                'layer2_units': 128,                'cnn_nr_channels': 4,                'cnn_kernel_size': 3,                'batch_size': 64,                'nr_epochs': 1000,                'nr_patience_epochs': 10,                'learning_rate': 0.001,            }        else:            self.surrogate_config = surrogate_config        self.best_value_observed = np.NINF        self.diverged_configs = set()        self.info_dict = dict()        self.suggest_time_duration = 0        self.budget_spent = 0        self.output_path = output_path        self.dataset_name = dataset_name        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)        self.no_improvement_patience = 0    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()        train_examples = np.array(train_examples, dtype=np.single)        train_labels = np.array(train_labels, dtype=np.single)        train_budgets = np.array(train_budgets, dtype=np.single)        train_curves = self.patch_curves_to_same_length(train_curves)        train_curves = np.array(train_curves, dtype=np.single)        train_budgets = train_budgets / self.max_benchmark_epochs        train_examples = torch.tensor(train_examples)        train_labels = torch.tensor(train_labels)        train_budgets = torch.tensor(train_budgets)        train_curves = torch.tensor(train_curves)        train_examples = train_examples.to(device=self.dev)        train_labels = train_labels.to(device=self.dev)        train_budgets = train_budgets.to(device=self.dev)        train_curves = train_curves.to(device=self.dev)        data = {            'X_train': train_examples,            'train_budgets': train_budgets,            'train_curves': train_curves,            'y_train': train_labels,        }        return data    def _train_surrogate(self):        data = self._prepare_dataset_and_budgets()        self.logger.info(f'Started training the model')        self.model.train_pipeline(            data,            load_checkpoint=False,        )    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()        budgets = np.array(budgets, dtype=np.single)        non_scaled_budgets = copy.deepcopy(budgets)        budgets = budgets / self.max_benchmark_epochs        configurations = np.array(configurations, dtype=np.single)        configurations = torch.tensor(configurations)        configurations = configurations.to(device=self.dev)        budgets = torch.tensor(budgets)        budgets = budgets.to(device=self.dev)        learning_curves = self.patch_curves_to_same_length(learning_curves)        learning_curves = np.array(learning_curves, dtype=np.single)        learning_curves = torch.tensor(learning_curves)        learning_curves = learning_curves.to(device=self.dev)        train_data = self._prepare_dataset_and_budgets()        test_data = {            'X_test': configurations,            'test_budgets': budgets,            'test_curves': learning_curves,        }        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets    def suggest(self) -> Tuple[int, int]:        suggest_time_start = time.time()        if self.initial_random_index < len(self.init_conf_indices):            self.logger.info(                'Not enough configurations to build a model. '                'Returning randomly sampled configuration'            )            random_indice = self.init_conf_indices[self.initial_random_index]            budget = self.init_budgets[self.initial_random_index]            self.initial_random_index += 1            return random_indice, budget        else:            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()            best_prediction_index = self.find_suggested_config(                mean_predictions,                std_predictions,                non_scaled_budgets,            )            best_config_index = hp_indices[best_prediction_index]            if best_config_index in self.examples:                evaluated_budgets = self.examples[best_config_index]                max_budget = max(evaluated_budgets)                budget = max_budget + self.fantasize_step                if budget > self.max_benchmark_epochs:                    budget = self.max_benchmark_epochs            else:                budget = self.fantasize_step        suggest_time_end = time.time()        self.suggest_time_duration = suggest_time_end - suggest_time_start        self.budget_spent += self.fantasize_step        if self.budget_spent > self.total_budget:            exit(0)        return best_config_index, budget    def observe(        self,        hp_index: int,        b: int,        learning_curve: np.ndarray,        alg_time: Optional[float] = None,    ):        score = learning_curve[-1]        if np.isnan(learning_curve).any():            self.update_info_dict(hp_index, b, np.nan, 0)            self.diverged_configs.add(hp_index)            return        observe_time_start = time.time()        self.examples[hp_index] = np.arange(1, b + 1).tolist()        self.performances[hp_index] = learning_curve        if self.best_value_observed < score:            self.best_value_observed = score            self.no_improvement_patience = 0        else:            self.no_improvement_patience += 1        observe_time_end = time.time()        train_time_duration = 0        if self.initial_random_index >= len(self.init_conf_indices):            train_time_start = time.time()            if self.model is None:                self.model = DyHPO(                    self.surrogate_config,                    self.dev,                    self.dataset_name,                    self.output_path,                    self.seed,                )            if self.no_improvement_patience == self.no_improvement_threshold:                self.model.restart = True            self._train_surrogate()            train_time_end = time.time()            train_time_duration = train_time_end - train_time_start        observe_time_duration = observe_time_end - observe_time_start        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration        if alg_time is not None:            total_duration = total_duration + alg_time        self.update_info_dict(hp_index, b, score, total_duration)    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:        examples = []        for hp_index in hp_indices:            examples.append(self.hp_candidates[hp_index])        return examples    def generate_candidate_configurations(        self,    ) -> Tuple[List, List, List, List]:        hp_indices = []        hp_budgets = []        learning_curves = []        for hp_index in range(0, self.hp_candidates.shape[0]):            if hp_index in self.examples:                budgets = self.examples[hp_index]                max_budget = max(budgets)                next_budget = max_budget + self.fantasize_step                curve = self.performances[hp_index][:max_budget]                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)                if difference_curve_length > 0:                    curve.extend([0.0] * difference_curve_length)            else:                next_budget = self.fantasize_step                curve = [0, 0, 0]            if next_budget <= self.max_benchmark_epochs:                hp_indices.append(hp_index)                hp_budgets.append(next_budget)                learning_curves.append(curve)        configurations = self.prepare_examples(hp_indices)        return configurations, hp_indices, hp_budgets, learning_curves    def history_configurations(        self,    ) -> Tuple[List, List, List, List]:        train_examples = []        train_labels = []        train_budgets = []        train_curves = []        for hp_index in self.examples:            budgets = self.examples[hp_index]            performances = self.performances[hp_index]            example = self.hp_candidates[hp_index]            for budget, performance in zip(budgets, performances):                train_examples.append(example)                train_budgets.append(budget)                train_labels.append(performance)                train_curve = performances[:budget - 1] if budget > 1 else [0.0]                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)                if difference_curve_length > 0:                    train_curve.extend([0.0] * difference_curve_length)                train_curves.append(train_curve)        return train_examples, train_labels, train_budgets, train_curves    def acq(        self,        best_value: float,        mean: float,        std: float,        explore_factor: Optional[float] = 0.25,        acq_fc: str = 'ei',    ) -> float:        if acq_fc == 'ei':            if std == 0:                return 0            z = (mean - best_value) / std            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)        elif acq_fc == 'ucb':            acq_value = mean + explore_factor * std        elif acq_fc == 'thompson':            acq_value = np.random.normal(mean, std)        elif acq_fc == 'exploit':            acq_value = mean        else:            raise NotImplementedError(                f'Acquisition function {acq_fc} has not been'                f'implemented',            )        return acq_value    def find_suggested_config(        self,        mean_predictions: np.ndarray,        mean_stds: np.ndarray,        budgets: List,    ) -> int:        highest_acq_value = np.NINF        best_index = -1        index = 0        for mean_value, std in zip(mean_predictions, mean_stds):            budget = int(budgets[index])            best_value = self.calculate_fidelity_ymax(budget)            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')            if acq_value > highest_acq_value:                highest_acq_value = acq_value                best_index = index            index += 1        return best_index    def calculate_fidelity_ymax(self, fidelity: int):        exact_fidelity_config_values = []        lower_fidelity_config_values = []        for example_index in self.examples.keys():            try:                performance = self.performances[example_index][fidelity - 1]                exact_fidelity_config_values.append(performance)            except IndexError:                learning_curve = self.performances[example_index]                lower_fidelity_config_values.append(max(learning_curve))        if len(exact_fidelity_config_values) > 0:            best_value = max(exact_fidelity_config_values)        else:            best_value = max(lower_fidelity_config_values)        return best_value    def update_info_dict(        self,        hp_index: int,        budget: int,        performance: float,        overhead: float,    ):        hp_index = int(hp_index)        if 'hp' in self.info_dict:            self.info_dict['hp'].append(hp_index)        else:            self.info_dict['hp'] = [hp_index]        if 'scores' in self.info_dict:            self.info_dict['scores'].append(performance)        else:            self.info_dict['scores'] = [performance]        if 'curve' in self.info_dict:            self.info_dict['curve'].append(self.best_value_observed)        else:            self.info_dict['curve'] = [self.best_value_observed]        if 'epochs' in self.info_dict:            self.info_dict['epochs'].append(budget)        else:            self.info_dict['epochs'] = [budget]        if 'overhead' in self.info_dict:            self.info_dict['overhead'].append(overhead)        else:            self.info_dict['overhead'] = [overhead]        with open(os.path.join(self.output_path, f'{self.dataset_name}_{self.seed}.json'), 'w') as fp:            json.dump(self.info_dict, fp)    def preprocess_hp_candidates(self) -> List:        log_hp_candidates = []        for hp_candidate in self.hp_candidates:            new_hp_candidate = []            for index, hp_value in enumerate(hp_candidate):                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)        log_hp_candidates = np.array(log_hp_candidates)        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)        return log_hp_candidates    @staticmethod    def patch_curves_to_same_length(curves):        max_curve_length = 0        for curve in curves:            if len(curve) > max_curve_length:                max_curve_length = len(curve)        for curve in curves:            difference = max_curve_length - len(curve)            if difference > 0:                curve.extend([0.0] * difference)        return curves\nclass FeatureExtractor(nn.Module):    def __init__(self, configuration):        super(FeatureExtractor, self).__init__()        self.configuration = configuration        self.nr_layers = configuration['nr_layers']        self.act_func = nn.LeakyReLU()        initial_features = configuration['nr_initial_features'] + 1        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])        for i in range(2, self.nr_layers):            setattr(                self,                f'fc{i + 1}',                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),            )            setattr(                self,                f'bn{i + 1}',                nn.BatchNorm1d(configuration[f'layer{i}_units']),            )        setattr(            self,            f'fc{self.nr_layers}',            nn.Linear(                configuration[f'layer{self.nr_layers - 1}_units'] +                configuration['cnn_nr_channels'],                configuration[f'layer{self.nr_layers}_units']            ),        )        self.cnn = nn.Sequential(            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),            nn.AdaptiveMaxPool1d(1),        )    def forward(self, x, budgets, learning_curves):        budgets = torch.unsqueeze(budgets, dim=1)        x = cat((x, budgets), dim=1)        x = self.fc1(x)        x = self.act_func(self.bn1(x))        for i in range(2, self.nr_layers):            x = self.act_func(                getattr(self, f'bn{i}')(                    getattr(self, f'fc{i}')(                        x                    )                )            )        learning_curves = torch.unsqueeze(learning_curves, 1)        lc_features = self.cnn(learning_curves)        lc_features = torch.squeeze(lc_features, 2)        x = cat((x, lc_features), dim=1)        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))        return xclass GPRegressionModel(gpytorch.models.ExactGP):    def __init__(        self,        train_x: torch.Tensor,        train_y: torch.Tensor,        likelihood: gpytorch.likelihoods.GaussianLikelihood,    ):        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)        self.mean_module = gpytorch.means.ConstantMean()        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())    def forward(self, x):        mean_x = self.mean_module(x)        covar_x = self.covar_module(x)        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)class DyHPO:    def __init__(        self,        configuration: Dict,        device: torch.device,        dataset_name: str = 'unknown',        output_path: str = '.',        seed: int = 11,    ):        super(DyHPO, self).__init__()        self.feature_extractor = FeatureExtractor(configuration)        self.batch_size = configuration['batch_size']        self.nr_epochs = configuration['nr_epochs']        self.early_stopping_patience = configuration['nr_patience_epochs']        self.refine_epochs = 50        self.dev = device        self.seed = seed        self.model, self.likelihood, self.mll = \n            self.get_model_likelihood_mll(                configuration[f'layer{self.feature_extractor.nr_layers}_units']            )        self.model.to(self.dev)        self.likelihood.to(self.dev)        self.feature_extractor.to(self.dev)        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],        )        self.configuration = configuration        self.initial_nr_points = 10        self.iterations = 0        self.restart = True        self.logger = logging.getLogger(__name__)        self.checkpoint_path = os.path.join(            output_path,            'checkpoints',            f'{dataset_name}',            f'{self.seed}',        )        os.makedirs(self.checkpoint_path, exist_ok=True)        self.checkpoint_file = os.path.join(            self.checkpoint_path,            'checkpoint.pth'        )    def restart_optimization(self):        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)        self.model, self.likelihood, self.mll = \n            self.get_model_likelihood_mll(                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],            )        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],        )    def get_model_likelihood_mll(        self,        train_size: int,    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:        train_x = torch.ones(train_size, train_size).to(self.dev)        train_y = torch.ones(train_size).to(self.dev)        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)        return model, likelihood, mll    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):        self.iterations += 1        self.logger.debug(f'Starting iteration: {self.iterations}')        weights_changed = False        if load_checkpoint:            try:                self.load_checkpoint()            except FileNotFoundError:                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'                                  f'Training the GP from the beginning')        self.model.train()        self.likelihood.train()        self.feature_extractor.train()        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],        )        X_train = data['X_train']        train_budgets = data['train_budgets']        train_curves = data['train_curves']        y_train = data['y_train']        initial_state = self.get_state()        training_errored = False        if self.restart:            self.restart_optimization()            nr_epochs = self.nr_epochs            if self.initial_nr_points <= self.iterations:                self.restart = False        else:            nr_epochs = self.refine_epochs        mse = 0.0        for epoch_nr in range(0, nr_epochs):            nr_examples_batch = X_train.size(dim=0)            if nr_examples_batch == 1:                continue            self.optimizer.zero_grad()            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)            self.model.set_train_data(projected_x, y_train, strict=False)            output = self.model(projected_x)            try:                loss = -self.mll(output, self.model.train_targets)                loss_value = loss.detach().to('cpu').item()                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)                self.logger.debug(                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '                    f'Loss: {loss_value:.3f}, '                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '                    f'noise: {self.model.likelihood.noise.item():.3f}, '                )                loss.backward()                self.optimizer.step()            except Exception as training_error:                self.logger.error(f'The following error happened while training: {training_error}')                self.restart = True                training_errored = True                break        if training_errored:            self.save_checkpoint(initial_state)            self.load_checkpoint()    def predict_pipeline(        self,        train_data: Dict[str, torch.Tensor],        test_data: Dict[str, torch.Tensor],    ) -> Tuple[np.ndarray, np.ndarray]:        self.model.eval()        self.feature_extractor.eval()        self.likelihood.eval()        with torch.no_grad():            projected_train_x = self.feature_extractor(                train_data['X_train'],                train_data['train_budgets'],                train_data['train_curves'],            )            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)            projected_test_x = self.feature_extractor(                test_data['X_test'],                test_data['test_budgets'],                test_data['test_curves'],            )            preds = self.likelihood(self.model(projected_test_x))        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )        return means, stds    def load_checkpoint(self):        checkpoint = torch.load(self.checkpoint_file)        self.model.load_state_dict(checkpoint['gp_state_dict'])        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])    def save_checkpoint(self, state: Dict =None):        if state is None:            torch.save(                self.get_state(),                self.checkpoint_file,            )        else:            torch.save(                state,                self.checkpoint_file,            )    def get_state(self) -> Dict[str, Dict]:        current_state = {            'gp_state_dict': deepcopy(self.model.state_dict()),            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),        }        return current_state",
        "experimental_info": "Overall HPO Loop Settings: Total budget for HPO: 1000. Fantasize step (how many budget steps to advance per iteration): 1. Maximization/Minimization: False for LCBench, True for TaskSet. Number of initial random configurations before model training starts: 1 configuration, evaluated at budget 1. Percentage of configurations taken randomly during BO: 10%. Seed range for experiments: 0 to 9. Surrogate Model (DyHPO Deep GP) Training Settings: Max epochs for benchmark budget: 51 (from LCBench/TaskSet `max_budget`). Initial full training points for surrogate: 10 points. Training epochs for full model restart: 1000 epochs. Training epochs for model refinement: 50 epochs. Patience for model restart trigger (`no_improvement_patience`): 10 epochs. Batch size for training the feature extractor: 64. Learning rate for Adam optimizer: 0.001. Model restart trigger: if `no_improvement_patience` reaches `int(max_benchmark_epochs + 0.2 * max_benchmark_epochs)`. Feature Extractor (Neural Network) Architecture: Number of linear layers: 2 (plus a final layer for concatenation with CNN features). Units in first linear layer: 64. Units in second linear layer (before CNN concat): 128. CNN channels: 4. CNN kernel size: 3. Acquisition Function: Multi-Fidelity Expected Improvement (EIMF). Hyperparameter Preprocessing: Log transformation for hyperparameters indicated by `log_indicator`. Min-Max scaling for all hyperparameters. Budget normalization to [0, 1] by dividing by `max_benchmark_epochs`. Learning curve padding with zeros to a uniform length."
      }
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Gradient Descent: The Ultimate Optimizer",
      "abstract": "Working with any gradient-based machine learning algorithm involves the\ntedious task of tuning the optimizer's hyperparameters, such as its step size.\nRecent work has shown how the step size can itself be optimized alongside the\nmodel parameters by manually deriving expressions for \"hypergradients\" ahead of\ntime.\n  We show how to automatically compute hypergradients with a simple and elegant\nmodification to backpropagation. This allows us to easily apply the method to\nother optimizers and hyperparameters (e.g. momentum coefficients). We can even\nrecursively apply the method to its own hyper-hyperparameters, and so on ad\ninfinitum. As these towers of optimizers grow taller, they become less\nsensitive to the initial choice of hyperparameters. We present experiments\nvalidating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch\nimplementation of this algorithm (see\npeople.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).",
      "full_text": "Gradient Descent: The Ultimate Optimizer Kartik Chandra∗ MIT CSAIL† Cambridge, MA kach@csail.mit.edu Audrey Xie∗ MIT CSAIL Cambridge, MA ahx@mit.edu Jonathan Ragan-Kelley MIT CSAIL Cambridge, MA jrk@csail.mit.edu Erik Meijer Meta, Inc. Menlo Park, CA erikm@fb.com Abstract Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer’s hyperparameters, such as its step size. Recent work has shown how the step size can itself be optimized alongside the model parameters by manually deriving expressions for “hypergradients” ahead of time. We show how toautomatically compute hypergradients with a simple and elegant modiﬁcation to backpropagation. This allows us to easily apply the method to other optimizers and hyperparameters (e.g. momentum coefﬁcients). We can even recursively apply the method to its own hyper-hyperparameters, and so on ad in- ﬁnitum. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch implementation of this algorithm (see people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer). 1 Introduction When we train deep neural networks by gradient descent, we have to select a step size αfor our optimizer. If αis too small, the optimizer runs very slowly, whereas ifαis too large, the optimizer fails to converge. Choosing an appropriate αis thus itself an optimization task that machine learning practitioners face every day. Why not apply gradient descent here, too? To do so, we need to compute the derivative of the loss function not only with respect to the neural network’s weights, but also with respect to α. Baydin et al. (2018), applying an insight from Almeida et al. (1999), describe how to efﬁciently compute such “hypergradients” by manually differentiating standard optimizer update rules with respect to the step size hyperparameter. This allows for on-line learning rate adaptation, which generally improves convergence, especially when the initial αis sub-optimal. However, the above method has three limitations: (1) manually differentiating optimizer update rules is tedious and error-prone, and must be re-done for each optimizer variant; (2) the method only tunes the step size hyperparameter, not other hyperparameters such as the momentum coefﬁcient; and (3) the method introduces a new hyperparameter, the hyper-step-size, which must also be tuned. In this paper, we address all three limitations by replacing manual differentiation with automatic differentiation (AD), which (1) automatically computes correct derivatives without any additional human effort, and (2) naturally generalizes to other hyperparameters (e.g. momentum coefﬁcient) for free. As for (3), we observe that AD can be applied to optimize not only the hyperparameters, but also the hyper-hyperparameters, and the hyper-hyper-hyperparameters, and so on. In fact, we can implement arbitrarily tall towers of recursive optimizers, which are increasingly robust to the choice of initial hyperparameter. These “hyperoptimizers” therefore reduce the burden on humans responsible for tuning the hyperparameters. (Such an effect was hypothesized by Baydin et al., but not tested because manual differentiation of complex sequences of nested optimizers is impractical.) ∗Equal contribution. †Work done in part at Meta, Inc. and in part at Stanford University. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:1909.13371v2  [cs.LG]  14 Oct 2022Although “just apply AD” is a seemingly straightforward recipe, an efﬁcient implementation that properly allows for recursive self-application requires some care. To close the loop, we take inspiration from the study of recursion and combinators in programming language theory (and the long tradition of programming language papers named “Lambda: The Ultimate X”). We spell out the details in Section 2, and evaluate our method in Section 3. We ﬁnd that across a variety of architectures (MLPs, CNNs, and RNNs) our hyperoptimizers are robust to suboptimal choices of initial hyperparameters, and that this robustness increases as we grow the stacks of optimizers taller. 2 Implementing hyperoptimizers Consider some loss function f that we want to minimize using gradient descent, and let wi be the weights at the beginning of step i(we will omit subscripts on f, even though it varies at each step due to the stochasticity of minibatches). First, recall the standard weight update rule at step ifor SGD, using some ﬁxed step size α: wi+1 = wi −α∂f(wi) ∂wi We would like to also update αat each step, so we will index it as well with the step number; that is, let αi be the step size at the beginning of step i. At each step, we will ﬁrst update αi to αi+1 using some update rule yet to be derived, and then use the updated step size αi+1 to update the weights from wi to wi+1. αi+1 = αi − adjustment for αi wi+1 = wi −αi+1 ∂f(wi) ∂wi What should the adjustment for αi be? By analogy to w, we want to adjust αi in the direction of the gradient of the loss function with respect to αi, scaled by some hyper-step size κ. In other words, the adjustment should be κ(∂f(wi)/∂αi). Our modiﬁed update rule is therefore: αi+1 = αi −κ∂f(wi) ∂αi (1) wi+1 = wi −αi+1 ∂f(wi) ∂wi (2) All that remains is to compute ∂f(wi)/∂αi in equation (1). Below, we ﬁrst review how Baydin et al. (2018) take this derivative by hand. Then, we show how to obtain the same result automatically and efﬁciently using AD. Finally, we discuss how this automation allows us to generalize the method. 2.1 Computing the step-size update rule by hand One option to compute ∂f(wi)/∂αi, explored by Baydin et al. (2018), is to proceed by direct manual computation of the partial derivative. Applying the chain rule to (1), we have ∂f(wi) ∂αi = ∂f(wi) ∂wi ·∂wi ∂αi = ∂f(wi) ∂wi · ∂ ( wi−1 −αi ∂f(wi−1) ∂wi−1 ) ∂αi (3) = ∂f(wi) ∂wi · ( −∂f(wi−1) ∂wi−1 ) (4) where (3) is obtained by substituting the update rule in (2) for wi and (4) is obtained by observing that wi−1 and f(wi−1) do not depend on αi. As Baydin et al. note, this expression lends itself to a simple and efﬁcient implementation: simply remember the past two gradients from backpropagation, and take their dot product to obtain the hypergradient with respect to the step size. We were able to take this derivative by hand because the update rule for SGD is simply a multiplication by a constant, whose derivative is trivial. What about other optimizers? Consider the Adam optimizer (Kingma and Ba, 2014), which has a much more sophisticated update rule involving the four hyperparameters α,β1,β2,ϵ (though ϵis typically not tuned). Differentiating the update rule by hand, 2we obtain signiﬁcantly more complex expressions for the hypergradients: ∂wi ∂αi = − ˆmi( ϵi + √ˆvi ) ∂wi ∂β1i = − αi ( −∂f(wi−1) ∂wi−1 + mi−1 + iβ1 (i−1) i ˆmi ) ( 1 −β1 i i ) ( ϵi + √ˆvi ) ∂wi ∂ϵi = αiˆmi ( ϵi + √ˆvi )2 ∂wi ∂β2i = αiˆmi √ˆvi ( − ( ∂f(wi−1) ∂wi−1 )2 + vi−1 + iβ2 (i−1) i ˆvi ) 2vi ( ϵi + √ˆvi )2 This manual approach to derive hypergradients simply does not scale: it is tedious and error-prone, and must be repeated for every optimizer variant. However, with a little bit of care, we can compute hypergradients automatically and efﬁciently alongside the regular gradients. 2.2 Computing the step-size update rule automatically In order to compute hypergradients automatically, let us ﬁrst brieﬂy review the mechanics of reverse- mode AD. Differentiable programming systems that provide reverse-mode AD typically build up a computation graph as the function is computed forwardly. For example, when a user computes the function f(wi), the system internally stores a DAG whose leaves are the weights wi, whose internal nodes are intermediate computations, and whose root is the ﬁnal loss. It can then backpropagate through the computation graph starting at this root node, depositing gradients in each internal node as it descends, until the weights wi at the leaf nodes have accumulated their gradients ∂f(wi)/∂wi. Once the gradient ∂f(wi)/∂wi is computed by the backwards pass, we update the weights wi+1 = wi −α·∂f(wi)/∂wi, and repeat the cycle for the next step of gradient descent. An important consideration at this point is for the weights to be “detached” from the computation graph before the next iteration of this algorithm — that is, for the weights to be forcibly converted to leaves of the graph by removing any inbound edges. The effect of the “detach” operation is depicted in Figure 1a. If this step were skipped, backpropagation at the next iteration would continue beyond the current weights and into the previous iteration’s computation graph. Over time the computation graph would grow taller linearly in the number of steps taken; because backpropagation is linear in the size of the graph, the overall training would become quadratic-time and intractable. Let us take PyTorch as an example. In the built-in SGD optimizer (Paszke et al., 2017, optim/sgd.py, commit ff94c9d), this is implemented by wrapping the update in the @torch.no_grad() context manager. Here, we need ﬁner grained control over gradient ﬂow, so will make the .detach() operations explicit. Below is pseudocode for an SGD optimizer that uses .detach() as we have discussed. The highlighted calls to .detach() correspond to detaching the weights and their gradients. def SGD.__init__(self, alpha): self.alpha = alpha def SGD.step(w): d_w = w.grad.detach() w = w.detach() - self.alpha.detach() * d_w Now, in order to have backpropagation deposit the gradient with respect to αi as well as wi, we can simply refrain from detaching αi from the graph, detaching instead its parents. This is depicted in Figure 1b. Because we want to compute ∂f(wi)/∂αi, the edge from αi to wi needs to remain intact. To implement this, instead of calling .detach() on alpha directly, we detach its parents when applying equation (1). This yields the following fully-automated hyperoptimization algorithm: def HyperSGD.step(w): # update alpha using equation (1) d_alpha = self.alpha.grad.detach() self.alpha = self.alpha.detach() - kappa.detach() * d_alpha # update w using equation (2) d_w = w.grad.detach() w = w.detach() - self.alpha.detach() * d_w 3(a) Computation graph of SGD with a single ﬁxed hy- perparameter α. (b) Computation graph of SGD with a continuously- updated hyperparameter αi. Figure 1: Visualizing the computation graphs of SGD and HyperSGD. Since we only extend the computation graph by a little extra amount, corresponding to evaluating the optimizer, the hyperoptimizer’s computational overhead is negligible (see Figure 4f). 2.3 Extending to other optimizers As suggested by Maclaurin et al. (2015), it should be possible to apply gradient-based methods to tune hyperparameters of common variations on SGD such as AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014). The above implementation of HyperSGD generalizes quite easily to these optimizers — we simply replace the last line with the new update rule. Unlike previous work, our method also allows for simultaneously optimizing all hyperparameters of these optimizers (e.g. all of α, β1, and β2 for Adam) “for free.” We simply treat them just likealpha in the implementation. Our evaluation in Section 3.2 demonstrates that this indeed advantageous to do. There are, however, two important subtleties: First, because hyperparameters like β1 and β2 must be strictly in the domain (0,1), we clamp the “raw” values to this domain using a scaled sigmoid. Without this step, we might accidentally adjust these values outside their domains. Second, the Adam optimizer in particular involves the term √ˆvi, which is continuous but not differentiable at ˆvi = 0. Because Adam normally initializes ˆv0 = 0, backpropagation fails on the ﬁrst step due to division by zero. We ﬁx this problem by initializing ˆv0 to ϵrather than 0. 2.4 Stacking hyperoptimizers recursively At this point it is natural to ask whether the hyperoptimizer can itself be optimized; that is, whether the hyper-hyperparameters can be adjusted by a hyper-hyperoptimizer. The possibility of doing so recursively ad inﬁnitum to obtain an optimization algorithm that is highly robust to the human- chosen hyperparameter was hypothesized by Baydin et al. (2018). Computing the gradients of these higher-order hyperparameters by hand is impossible without knowing the exact sequence of stacked optimizers in advance, and, as discussed above, would be extremely tedious and error-prone. However, the ability to compute these gradients automatically by AD makes it possible to realize this vision. To do so, let us revisit our previous implementation of HyperSGD. Notice that there is an opportunity for recursion lurking here: the adjustment to alpha can be factored out with a call to SGD.step, where SGD’s hyperparameter iskappa. def HyperSGD.step(w): # update alpha using Equation (1) SGD(kappa).step(self.alpha) # update w using Equation (2) d_w = w.grad.detach() w = w.detach() - self.alpha * d_w 4Because SGD is already careful to properly detach its parameter (typically w, but in this case α), this implementation is functionally identical to the one above. Indeed, any optimizer that observes this protocol would sufﬁce, so let us abstract out the optimizer as a parameter to HyperSGD: def HyperSGD.__init__(self, alpha, opt): self.alpha = alpha self.optimizer = opt def HyperSGD.step(w): self.optimizer.step(self.alpha) d_w = w.grad.detach() w = w.detach() - self.alpha * d_w opt = HyperSGD(0.01, opt=SGD(kappa)) Finally, after this refactoring, we can recursively feedHyperSGD itself as the optimizer, obtaining a level-2 hyperoptimizer HyperSGD(0.01, HyperSGD(0.01, SGD(0.01))). Similarly, we can imagine taller towers, or towers that mix and match multiple different kinds of optimizers, such as Adam- optimized-by-SGD-optimized-by-Adam. A natural concern is whether this process actually exacerbates the hyperparameter optimization problem by introducing even more hyperparameters. Baydin et al. (2018) predicted that as the towers of hyperoptimizers grew taller, the resulting algorithms would become less sensitive to the human-chosen hyperparameters. This is indeed the case; Section 3.4 presents an empirical evaluation. 3 Experiments In this section we evaluate the hyperoptimizers made possible by our system, exploring in particular the beneﬁts of optimizing hyperparameters beyond just the step size, and of stacking hyperoptimizers to multiple levels. Each of these experiments was conducted on a single NVIDIA TITAN Xp GPU. 3.1 Hyperoptimization for SGD First, we establish some basic properties about hyperoptimizers: (1) whether an SGD hyperoptimizer performs better than a regular SGD optimizer, and (2) whether the ﬁnal learned step size is better than the initial human-chosen step size. We test the latter property by running a fresh regular SGD optimizer with the ﬁnal learned step size of the hyperoptimizer. Following authors of prior work (Maclaurin et al., 2015; Baydin et al., 2018), we conducted initial experiments on the MNIST dataset (Lecun et al., 1998) using a neural network with one fully-connected hidden layer of size 128, tanh activations, and a batch size of 256. We trained all networks for 30 epochs, reporting statistics over 3 runs. As a baseline we used SGD with α= 0.01. Table 1a summarizes the results of our experiments. We ﬁnd that hyperoptimized SGD outper- forms the baseline by a signiﬁcant margin. This holds even if we use other optimizers (e.g. Adam) to adjust the step size of the SGD optimizer. Furthermore, when we re-ran the regular optimizers with the new learned hyperparameters, we found that they performed better than the initial hyperparameter. 3.2 Hyperoptimization for Adam, AdaGrad and RMSProp In Section 2.3, we described how to apply our system to the Adam optimizer, simultaneously optimizing not only the learning rate α, but also the momentum coefﬁcients β1,2. Here, we ask three questions: (1) whether hyperoptimized Adam optimizers perform better than regular Adam optimizers, (2) whether the learned hyperparameters outperform the baseline, and (3) whether there is a beneﬁt to optimizing all the hyperparameters, as opposed to only optimizing the learning rate as Baydin et al. (2018) do. Because Adam has signiﬁcantly faster convergence than SGD, we only run these experiments for 5 epochs to avoid overﬁtting. Table 1b summarizes the results of our experiments. We ﬁnd that indeed the hyperoptimized Adam optimizer outperforms the regular Adam optimizer on its “default” settings. As with SGD, the learned hyperparameters perform better than the initial hyperparameters when re-run with the regular optimizer. Inspecting the learned hyperparameters, we ﬁnd that the algorithm raises the learning rate 5Optimizer Test error SGD 8.99±0.05% SGD / SGD 4.81±0.10% SGD(0.0769) 5.44±0.10% SGD / Adam(0.1) 4.86±0.06% SGD(0.4538) 2.80±0.09% SGD / AdaGrad 4.85±0.21% SGD(0.0836) 5.17±0.03% SGD / RMSprop(0.1) 4.52±0.02% SGD(0.5920) 2.52±0.07% (a) Experiments with SGD (Section 3.1) Optimizer Test error Adam 4.67±0.06% Adam / SGD(10−5) 3.03±0.02% Adam(0.0040, 0.899, 0.999) 3.11±0.06% Adamα / SGD(10−5) 3.12±0.04% Adamα(0.0021) 3.47±0.02% Adam / Adam 3.05±0.09% Adam(0.0038, 0.870, 0.999) 3.24±0.13% Adamα / Adam 3.04±0.08% Adamα(0.0036) 3.08±0.12% (b) Experiments with Adam (Section 3.2) Optimizer Test error AdaGrad 7.40±0.08% AdaGrad / SGD 6.90±0.16% AdaGrad(0.0080) 7.75±0.02% AdaGrad / AdaGrad 5.03±0.23% AdaGrad(0.0151) 6.67±0.08% (c) Experiments with AdaGrad (Section 3.2) Optimizer Test error RMSProp 4.19±0.47% RMSPropα / SGD(10−4) 3.55±0.23% RMSProp(0.0030) 3.93±0.70% RMSpropα,γ / SGD(10−4) 3.33±0.07% RMSProp(0.0032, 0.9899) 3.25±0.09% RMSPropα / RMSProp(10−4) 3.42±0.45% RMSProp(0.0021) 3.60±0.04% RMSPropα,γ / RMSProp(10−4) 2.96±0.11% RMSProp(0.0020, 0.9962) 3.65±0.36% (d) Experiments with RMSProp (Section 3.2) Table 1: Hyperoptimization experiments with MNIST. We denote hyperoptimizers by their constituent optimizers separated by slashes (the leftmost item adjusts the model’s weights). Adamα is an Adam optimizer where only α is optimized as by Baydin et al. (2018); RMSProp α is similar. If not speciﬁed, initial hyperparameters are PyTorch defaults ( 10−2 for learning rates except 10−3 for Adam; β1 = 0.9,β2 = 0.99 for Adam and γ = 0.99 for RMSProp). Each hyperoptimizer experiment is repeated using the ﬁnal hyperparameters (typeset in pink) learned by the algorithm. αand slightly lowers β1, but does not signiﬁcantly affect β2. Nevertheless, learning β1 does help slightly, though not when the top-level optimizer is itself another Adam optimizer. Similarly, we can add any other optimizer to our system with just a few straightforward lines of code. Here, we show results for AdaGrad (Table 1c) and RMSProp (Table 1d; also run to 5 epochs). These experiments took less than an hour each to implement from scratch, and show that every hyperoptimizer stack outperforms the non-hyperoptimized baseline.We remark that AdaGrad is known to “stall” over time as the effective step size goes to zero; inspecting the learned αover time, we ﬁnd that the AdaGrad/AdaGrad hyperoptimizer increases αto make up for this effect. Additionally, we tried to hyperoptimize RMSProp’s newγparameter, which modulates the accumulation of gradient RMS terms. This yielded even better results (compare α to α,γ trials), and required only a 1-line change in our code. 3.3 Hyperoptimization at scale Next, we evaluate our hyperoptimizers on two different real-world neural network architectures. 3.3.1 Convolutional neural networks for computer vision We train a ResNet-20 (He et al., 2016) with and without hyperoptimization on the CIFAR-10 dataset (Krizhevsky, 2012). As a baseline, we replicate the training procedure of He et al. (2016): we 6(a) For a wide range of “bad” initial hyperparameter conﬁgurations, the hyperoptimizer improves on (or at least matches) ﬁnal test accuracy, and often matches or even outperforms the “good” initial hyperparameters. (b) The hyperoptimizer matches performance of the hand-engineered learning rate decay schedule by He et al. (2016), learning a strikingly similar decay schedule (right plot). Figure 2: Training ResNets on CIFAR-10 with hyperoptimizers (Section 3.3.1). use the same network architecture, optimizer (SGD), step size (0.1), momentum (0.9), and weight decay (10−4), though without their custom learning rate decay schedule (which we will address later). Experiments were run for 200 epochs, which takes around 3 hours on our hardware. First, we test how robust the hyperoptimizer is to “bad” initial choices of step size and momentum. We vary the initial step size and the momentum among “small,” “good,” and “large” values (that is, α ∈{0.01,0.1,1.0}and µ ∈{0.09,0.9,0.99}), and add a hyperoptimizer ( αα = α2 ·10−3, αµ = 1/(1−µ)·10−6). The results of this experiment are shown in Figure 2a. In every conﬁguration, the hyperoptimizer matches or outperforms the regular optimizer in ﬁnal test accuracy. Furthermore, in nearly all of the conﬁgurations, the hyperoptimizer matches or exceeds the “good” hyperparameters’ ﬁnal test accuracy. Only when both hyperparameters are bad in the same direction (too small or too large) is it unable to manage this, and even then for the too-large case it dramatically lowers the loss compared to no hyperoptimizer. We conclude that hyperoptimizers are indeed beneﬁcial for tuning both step size and momentum in this real-world setting. Next, we add in the learning rate decay schedule hand-engineered by He et al. (2016): the step size is divided by 10 at epochs 100 and 150. We compare this with a hyperoptimizer initialized with the same starting hyperparameters, training both variants for 500 epochs. Our results are shown in Figure 2b. The hyperoptimizer not only matches the ﬁnal test loss of the hand-engineered learning rate decay schedule, but also learns a decay schedule strikingly similar to one hand-engineered by He et al. Of course, both networks signiﬁcantly outperform the baseline trained with a ﬁxed step size. 3.3.2 Recurrent neural networks for language modeling We train a character-level RNN (“Char-RNN”) on the Tolstoy dataset, as proposed by Karpathy et al. (2015) as a convenient testbed for language models, which is now often used to benchmark optimizers (Schneider et al., 2018; Schmidt et al., 2021). We took the architecture (2-layer LSTM with 128 hidden nodes) and “expert” optimizer (Adam optimizer with α= 2×10−3, run for 50,000 gradient 7Figure 3: Training RNNs with hyperoptimizers (Section 3.3.2). As the initial learning rate is lowered, the regular Adam optimizer’s convergence slows, but the hyperoptimizer is able to accelerate it. The hyperoptimizer also slightly improves convergence when the initial learning rate is too high. descent steps) directly from Johnson (2017) as recommended by Karpathy et al. We compare against our HyperAdam optimizer on a wide range of initial learning rates α ∈{10−4,2 ×10−3,10−2}, with αα = α·10−2. We do not vary initial β1,2 because in our experience these hyperparameters are typically left at their default values. However, we do allow the hyperoptimizer to vary β1,2 over the course of training (with αβ1 = 10−4 and αβ2 = 2×10−4). All runs took around 1 hour to train. The results of this experiment are shown in Figure 3. We ﬁnd that the hyperoptimizer performs comparably to the expert-chosen ﬁxed step size (perplexity 5.41 ±0.26 with hyperoptimizer vs 5.27 ±0.31 without), and improves upon “bad” initial step sizes in both directions (5.45 ±0.76 vs 5.77 ±0.34 when too high; 6.51 ±0.88 vs 8.71 ±0.91 when too low). 3.4 Higher-order hyperoptimization In Section 2.4 we developed an interface for building arbitrarily tall towers of optimizers. Baydin et al. (2018) hypothesized that taller towers would yield hyperoptimizers that were increasingly robust to the initial human-chosen hyperparameters. To validate this behavior of higher-order hyperoptimizers, we ran each of our benchmarks from above (MLP on MNIST, CNN on CIFAR-10, RNN on Tolstoy) with towers of hyperoptimizers of increasing heights, and with bottom-level step sizes αinitialized across many orders of magnitude. In practice we ﬁnd that if the initial hyper-step sizes are too large, the computation diverges for networks larger than the MNIST MLP. So, we initialize each level’s hyperparameter to be smaller than that of the previous level. Speciﬁcally, we use the following scheme: from α= 10−8 to 10−4 the higher layers’ step sizes were initialized to [α·102,α ·100,α ·10−2] respectively, while for α≥10−3 they were initialized to [α·10−3,α ·10−4,10−8] respectively. Figure 4 shows our results. It is indeed the case across these different benchmarks (each of which has a different dataset, architecture, and optimizer type) that the taller the hyperoptimizer stack, the less sensitive the results become to the human-chosen hyperparameters. With a three-level optimizer stack, a single hyperoptimizer design obtains reasonable results in all of our bench- marks across several orders of magnitude of base-level step size. Further tests of scalability To test if our hyperoptimizers continue to work in even larger regimes, we ﬁne-tuned a ResNet-152 (pretrained on ImageNet) to the Caltech-256 dataset Grifﬁn et al. (2007). Figure 4e shows the results: a height-1 hyperoptimizer recovers ≈11% error for both α= 10−6 and α= 10−4 (without a hyperoptimizer, α= 10−6 gives 91.5% error). A height-2 hyperoptimizer is additionally able to make signiﬁcant progress when α= 10−2. 8(a) Results on an MLP (Sec 3.1), where all layers are initialized with the same step size. (b) Results on an MLP (Sec 3.1), where all layers are initialized as in Sec 3.4. (c) Results on a ResNet (Sec 3.3.1)  (d) Results on a Char-RNN (Sec 3.3.2) (e) Results on ﬁne-tuning a pretrained ResNet-152 to the Caltech-256 dataset (Sec 3.4) (f) Our hyperoptimizers have minimal impact on runtime, which scales linearly in height (Sec 3.4) Figure 4: Evaluating higher-order hyperoptimization across a variety of benchmarks (Section 3.4). As we stack more layers of optimizers, the resulting hyperoptimizer is less sensitive to the initial choice of hyperparameters, but costs only 1-2% more in runtime. We stress how lightweight and practical this method is. Figure 4f shows how runtime scales as a function of hyperoptimizer stack height for the above benchmarks. The scaling is linear: each additional level costs only 1-2% in additional runtime above the non-hyperoptimized baseline. 4 Related work Hyperparameter optimization has a long history, and we refer readers to a recent survey by Feurer and Hutter (2019) for the full story. Most existing work on gradient-based hyperparameter optimiza- tion (Bengio, 2000; Domke, 2012; Maclaurin et al., 2015; Pedregosa, 2016; Franceschi et al., 2017) has focused on computing hyperparameter gradients after several iterations of training, which is 9computationally expensive. Baydin et al. (2018), building on a technique ﬁrst published by Almeida et al. (1999), propose instead updating hyperparameters at each step, and Rubio (2017) provides a convergence analysis. Luketina et al. (2016) apply a similar technique to regularization hyperpa- rameters, though they note that their proposed method could work in principle for any continuous hyperparameter. As discussed above, we expand upon this line of work in three directions: (1) by fully automating this process, rather than requiring manual derivative computations; (2) by optimizing hyperparameters beyond just the learning rate; and (3) by realizing the vision of recursive higher-order hyperoptimizers and evaluating the resulting algorithms. We ﬁnd that they are indeed more robust to the initial human-chosen hyperparameter, which relates our work to other learning algorithms that minimize sensitivity to learning rates (Orabona and Tommasi, 2017; Vaswani et al., 2019). 5 Limitations and future work As discussed in Section 3.4, one limitation of hyperoptimizers is that they cannot yet handle initial hyperparameters that are set far too high, because the system is unstable and diverges before the hyperoptimizer can have an effect. Designing hyperoptimizers robust in this regime requires further research, such as a deeper theoretical analysis of convergence. Our implementation also requires some care in avoiding certain bugs related to computation graph management. For example, loggers must detach what is logged to avoid memory leaks because tensors are not garbage collected unless all children are detached. Similarly, certain PyTorch modules (e.g. the built-in LSTM) cannot be used because they silently modify the computation graph, which may lead to incorrect gradients with our system. Further research is needed to design differentiable programming languages where methods like ours can be expressed in a modular and composable manner that minimizes the risk of such bugs. Broader impact Training a modern deep learning system consumes a tremendous amount of energy, and hyperparameter searches can multiply that energy impact by many orders of magnitude (Strubell et al., 2019). We hope that advances in on-line hyperparameter tuning can reduce this impact. 6 Conclusion We presented a technique that enables gradient descent optimizers like SGD and Adam to tune their own hyperparameters. Unlike prior work, our proposed hyperoptimizers require no manual differentiation, learn hyperparameters beyond just learning rates, and can be stacked recursively to many levels. We described an elegant recursive implementation of hyperoptimizers in a reverse-mode AD system and evaluated it on a variety of benchmarks, showing that as the stacks grow taller, they become less sensitive to the initial human-chosen hyperparameter. Acknowledgments and Disclosure of Funding We thank Samantha Andow, Emilio Arroyo-Fang, Irene Dea, Johann George, Melissa Grueter, Basil Hosmer, Stefﬁ Stumpos, Alanna Tempest, and Shannon Yang for early discussions, Krishna Murthy Jatavallabhula and Josh Tenenbaum for their advice when preparing this paper, and the anonymous reviewers for their thoughtful feedback. KC and JRK were supported by NSF Grants #2105806, #CCF-1231216, #CCF-1723445 and #CCF-1846502, and ONR Grant #00010803 at MIT. Additionally, KC was supported by a Hertz Foundation Fellowship, the Paul and Daisy Soros Fellowship for New Americans, and an NSF Graduate Research Fellowship under Grant #2141064, and AX was supported by the MIT Undergraduate Research Opportunities Program (UROP). References L. E. Almeida, T. Langlois, J. F. M. do Amaral, and A. Plakhov. Parameter adaptation in stochastic optimization. In On-Line Learning in Neural Networks, 1999. A. G. Baydin, R. Cornish, D. M. Rubio, M. Schmidt, and F. Wood. Online learning rate adaptation with hypergradient descent. In Sixth International Conference on Learning Representations (ICLR), Vancouver, Canada, April 30 – May 3, 2018, 2018. 10Y . Bengio. Gradient-based optimization of hyperparameters. Neural Computation , 12(8): 1889–1900, 2000. doi: 10.1162/089976600300015187. URL https://doi.org/10.1162/ 089976600300015187. J. Domke. Generic methods for optimization-based modeling. In N. D. Lawrence and M. Girolami, editors, Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statis- tics, volume 22 of Proceedings of Machine Learning Research, pages 318–326, La Palma, Canary Islands, 21–23 Apr 2012. PMLR. URL http://proceedings.mlr.press/v22/domke12. html. J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011. ISSN 1532-4435. URL http: //dl.acm.org/citation.cfm?id=1953048.2021068. M. Feurer and F. Hutter.Hyperparameter Optimization, pages 3–33. Springer International Publishing, Cham, 2019. ISBN 978-3-030-05318-5. doi: 10.1007/978-3-030-05318-5_1. URL https: //doi.org/10.1007/978-3-030-05318-5 _1. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperpa- rameter optimization. In D. Precup and Y . W. Teh, editors,Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1165–1173, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/franceschi17a.html. G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. 2007. URL http: //authors.library.caltech.edu/7694/. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90. J. Johnson. torch-rnn. Github repository, 2017. URL https://github.com/jcjohnson/ torch-rnn. A. Karpathy, J. Johnson, and L. Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015. URL https://arxiv.org/pdf/1506.02078.pdf. D. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 12 2014. A. Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05 2012. Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, Nov 1998. ISSN 0018-9219. doi: 10.1109/5.726791. J. Luketina, M. Berglund, K. Greff, and T. Raiko. Scalable gradient-based tuning of continuous regu- larization hyperparameters. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pages 2952–2960. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390.3045701. D. Maclaurin, D. Duvenaud, and R. P. Adams. Gradient-based hyperparameter optimization through reversible learning. In Proceedings of the 32Nd International Conference on International Con- ference on Machine Learning - Volume 37, ICML’15, pages 2113–2122. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045343. F. Orabona and T. Tommasi. Training deep networks without learning rates through coin betting. Advances in Neural Information Processing Systems, 30, 2017. A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017. 11F. Pedregosa. Hyperparameter optimization with approximate gradient. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pages 737–746. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390. 3045469. D. M. Rubio. Convergence analysis of an adaptive method of gradient descent. University of Oxford, Oxford, M. Sc. thesis, 2017. URL https://damaru2.github.io/convergence_analysis_ hypergradient_descent/dissertation_hypergradients.pdf. R. M. Schmidt, F. Schneider, and P. Hennig. Descending through a crowded valley-benchmarking deep learning optimizers. In International Conference on Machine Learning, pages 9367–9376. PMLR, 2021. F. Schneider, L. Balles, and P. Hennig. Deepobs: A deep learning optimizer benchmark suite. In International Conference on Learning Representations, 2018. E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243, 2019. URL https://arxiv.org/pdf/1906.02243.pdf. S. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S. Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. Advances in neural information processing systems, 32, 2019. M. D. Zeiler. ADADELTA: An adaptive learning rate method.CoRR, abs/1212.5701, 2012. URL http://dblp.uni-trier.de/db/journals/corr/corr1212.html#abs-1212-5701 . 12",
      "meta_data": {
        "arxiv_id": "1909.13371v2",
        "authors": [
          "Kartik Chandra",
          "Audrey Xie",
          "Jonathan Ragan-Kelley",
          "Erik Meijer"
        ],
        "published_date": "2019-09-29T21:41:49Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13371v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a method to automatically compute hypergradients using a simple modification to backpropagation, eliminating the need for manual differentiation of optimizer update rules. This allows for the automatic tuning of various optimizer hyperparameters (e.g., learning rates, momentum coefficients) simultaneously. A key contribution is the ability to recursively stack these hyperoptimizers, creating 'towers' of optimizers that become increasingly robust to the initial human-chosen hyperparameters. The method is validated across MLPs, CNNs, and RNNs, showing improved performance and adaptability to suboptimal initial hyperparameter choices.",
        "methodology": "The core methodology involves adapting reverse-mode automatic differentiation (AD) to compute hypergradients. Instead of manually deriving the partial derivative of the loss with respect to hyperparameters (e.g., ∂f/∂α), the system modifies the computation graph management to allow gradients to flow through the hyperparameters. Specifically, during the optimization step, the hyperparameter (e.g., α) is not 'detached' from the computation graph, while its parents are, enabling backpropagation to compute its gradient. This approach generalizes to various optimizers (SGD, Adam, AdaGrad, RMSProp) and allows for the simultaneous optimization of all their continuous hyperparameters. For hyperparameters with domain constraints (e.g., (0,1) for Adam's β1, β2), a scaled sigmoid is used to clamp values. Recursive stacking of hyperoptimizers is achieved by defining a HyperSGD class where the optimizer for its own hyperparameter (e.g., κ) is itself another optimizer, allowing for arbitrary levels of nested optimization.",
        "experimental_setup": "Experiments were conducted on a single NVIDIA TITAN Xp GPU. The method was evaluated on: (1) An MLP with one hidden layer (128 nodes, tanh activations) on the MNIST dataset, trained for 30 epochs with a batch size of 256. (2) A ResNet-20 on the CIFAR-10 dataset, replicating baseline procedures from prior work (SGD, α=0.1, momentum=0.9, weight decay=10^-4), trained for 200 or 500 epochs. (3) A character-level RNN (Char-RNN) with a 2-layer LSTM (128 hidden nodes) on the Tolstoy dataset, using an Adam optimizer for 50,000 gradient descent steps. (4) Scalability tests involved fine-tuning a pretrained ResNet-152 (on ImageNet) to the Caltech-256 dataset. Various initial hyperparameter settings (e.g., 'small,' 'good,' 'large' step sizes and momentum, different orders of magnitude for base-level step sizes in higher-order stacks) were tested to evaluate robustness. Performance was measured by test error/accuracy and perplexity, and compared against non-hyperoptimized baselines and re-runs with learned hyperparameters.",
        "limitations": "One limitation is the instability of the system when initial hyperparameters are set 'far too high,' causing divergence before the hyperoptimizer can take effect, suggesting a need for deeper theoretical analysis of convergence. The current PyTorch implementation requires careful management of the computation graph to avoid issues like memory leaks (logged tensors must be detached) and incompatibility with certain built-in PyTorch modules (e.g., LSTM) that silently modify the graph, potentially leading to incorrect gradients.",
        "future_research_directions": "Future research should focus on designing hyperoptimizers that are robust to very high initial hyperparameters, possibly through a deeper theoretical analysis of convergence. There is also a need for advancements in differentiable programming languages to enable a more modular and composable expression of methods like this, thereby minimizing the risk of computation graph-related bugs and improving developer experience. The broader impact of reducing energy consumption from hyperparameter searches in deep learning is also highlighted as a motivation for further work in on-line hyperparameter tuning."
      }
    },
    {
      "title": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels",
      "abstract": "Selecting hyperparameters in deep learning greatly impacts its effectiveness\nbut requires manual effort and expertise. Recent works show that Bayesian model\nselection with Laplace approximations can allow to optimize such\nhyperparameters just like standard neural network parameters using gradients\nand on the training data. However, estimating a single hyperparameter gradient\nrequires a pass through the entire dataset, limiting the scalability of such\nalgorithms. In this work, we overcome this issue by introducing lower bounds to\nthe linearized Laplace approximation of the marginal likelihood. In contrast to\nprevious estimators, these bounds are amenable to stochastic-gradient-based\noptimization and allow to trade off estimation accuracy against computational\ncomplexity. We derive them using the function-space form of the linearized\nLaplace, which can be estimated using the neural tangent kernel.\nExperimentally, we show that the estimators can significantly accelerate\ngradient-based hyperparameter optimization.",
      "full_text": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Alexander Immer 1 2 Tycho F.A. van der Ouderaa3 Mark van der Wilk 3 Gunnar R¨atsch 1 Bernhard Sch¨olkopf 2 Abstract Selecting hyperparameters in deep learning greatly impacts its effectiveness but requires man- ual effort and expertise. Recent works show that Bayesian model selection with Laplace approxi- mations can allow to optimize such hyperparame- ters just like standard neural network parameters using gradients and on the training data. How- ever, estimating a single hyperparameter gradi- ent requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approxi- mation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against compu- tational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that the estima- tors can significantly accelerate gradient-based hyperparameter optimization. 1. Introduction Deep learning models have many hyperparameters that need to be chosen properly to achieve good performance. For example, making the right architecture, regularization, or data augmentation choices remains a problem that requires significant human effort and computation. This is even apparent within the field of deep learning itself, where the state-of-the-art settings change frequently. Such manual trial-and-error procedure is inefficient and can hinder the application of deep learning to new problems. A more automatic procedure could be much more sustainable. 1Department of Computer Science, ETH Zurich, Switzerland 2Max Planck Institute for Intelligent Systems, T¨ubingen, Germany 3Imperial College London, UK. Correspondence to: Alexander Immer <alexander.immer@inf.ethz.ch>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Runtime (s) 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 Negative Log Marginal Likelihood Pareto Full GGN NTK KFAC Diag GGN 10 20 50 100 250 500 1000 2500 5000 10000 batch size Figure 1.Pareto frontier between marginal likelihood estimator tightness and runtime for invariance learning on rotated MNIST. Our proposed estimators enable stochastic approximation with varying batch size in contrast to the four previously used full-batch estimators (batch size of 10 000in black). NTK -based estimators on subsets of data perform well at more than 10× speed-up. Automated machine learing (AutoML) methods have the potential to greatly reduce the cost of deploying deep learn- ing for new problems (Hutter et al., 2019). Commonly, black-box algorithms are applied in this setting to optimize hyperparameters towards the best possible validation perfor- mance. For example, neural architectures can be optimized by training each option to convergence and assessing the val- idation performance repeatedly. However, such iterative pro- cedures require training many models to convergence and suffer from high-dimensional hyperparameters due to their black-box nature. We posit that (stochastic-)gradient-based optimization of hyperparameters would be more desirable. Bayesian model selection, where we consider hyperparame- ters and neural network weights jointly as part of a proba- bilistic model, is amenable to gradient-based optimization and also does not require any validation data (MacKay, 2003). Gradient-based optimization of hyperparameters tends to suffer less from high dimensionality than iterative black-box methods (Lorraine et al., 2020). Further, in such integrated procedure, the hyperparameters can be jointly optimized with the neural network weights (Foresee & Ha- gan, 1997; Immer et al., 2021a). While this is theoretically appealing, estimating the hyperparameter gradient is costly as it requires differentiating the marginal likelihood, i.e., the normalization constant of the posterior. 1 arXiv:2306.03968v1  [stat.ML]  6 Jun 2023Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels NC NC M M NC NC N N P P 2 2 P P 2 2P P P P P P NC NC N N Theorem 1. (Structured Parametric Bounds) Theorem 2. (Subset-of-Data Bounds) Theorem 3. (Subset-of-Data Parametric Bounds) number of data pointsN number of parametersP number of outputs (e.g. classes)C batch sizeM number of parameters in  ’th layersP P P s P P Eq. (6) Figure 2.Schematic overview of the derived lower bounds to the linearized Laplace marginal likelihood. The parametric bounds on the top left justify commonly used block-diagonal and diagonal Hessian and Gauss-Newton approximations. The bounds on the right are derived from the dual NTK perspective and give a novel family of lower bounds. These lower bounds enable stochastic marginal likelihood estimation and gradients with both NTK -based estimators and parametric variants, e.g., Gauss-Newton, by translating them back (bottom). To enable Bayesian model selection of hyperparameters in deep learning with marginal likelihood gradients, Immer et al. (2021a) recently revisited the use of the Laplace ap- proximation (MacKay, 1992a) with modern Gauss-Newton approximations. While these structured linearized Laplace approximations can be used for architecture comparison and gradient-based learning of data augmentations and reg- ularization strength (Immer et al., 2021a; 2022a;b; Antor´an et al., 2022b; Daxberger et al., 2021; Hataya & Nakayama, 2021), each hyperparameter gradient requires a pass through the entire training dataset. In particular, the objective does not allow (unbiased) stochastic gradient estimation. In the present work, we derive lower bounds to the Laplace approximation of the marginal likelihood that are amenable to stochastic-gradient-based optimization and show that currently used structured approximations are in fact lower bounds (overview in Figure 2). Instead of a whole pass through the training data, our estimators require only batches of data to estimate hyperparameter gradients, which can greatly increase the scalability of such algorithms. The size of the batches can further be chosen to achieve a trade-off between estimation quality and runtime complex- ity (cf. Figure 1). We derive the lower bounds from the dual form of the linearized Laplace (Khan et al., 2019; Im- mer et al., 2021b), which relies on the (empirical) neural tangent kernel ( NTK , Jacot et al., 2018). By partitioning the NTK using batches, we obtain a lower bound to the linearized Laplace marginal likelihood, which permits un- biased stochastic estimates and gradients. Empirically, our estimators enable up to 25-fold acceleration of hyperparam- eter optimization and application to larger datasets. 2. Background We consider supervised learning tasks with inputsxn ∈ RD and targets yn ∈ RC giving dataset D = {(xn, yn)}N n=1 with N pairs. We model the targets given inputs with a neu- ral network. We parameterize the neural networkf, as usual, with weights w ∈ RP , but additionally make the depen- dence on eventual hyperparameters h, such as architecture, explicit, and have f(x; w, h) ∈ RC. Assuming the data are i.i.d., we can define a probabilistic model with likelihood p(D|w, h) = QN n=1 p(yn|xn, w, h) that depends on the neural network. Further, we have a prior p(w|h). 2.1. Bayesian Model Selection In principle, Bayesian inference makes no distinction be- tween hyperparameters h and weights w and simply in- fers both jointly. With an additional prior p(h), this means that we obtain the joint posterior p(w, h|D) ∝ p(D|w, h)p(w|h)p(h). While there exist methods to ap- proximate the posterior p(w|D, h) efficiently in deep learn- ing, Bayesian inference of the hyperparameters h is compli- cated due to their heterogeneous and complicated structure, e.g., it could require a distribution over neural architectures. Instead, it is common to perform empirical Bayes to select hyperparameters h by optimizing the marginal likelihood, p(D|h) = Z p(D|w, h)p(w|h) dw , (1) which implements Occam’s razor, performing a trade-off between model complexity and accuracy (Rasmussen & Ghahramani, 2001; Bishop, 2006). This point approxima- 2Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels tion can work well unless the number of hyperparameters is too large, which leads to overfitting (Ober et al., 2021). We can optimize the marginal likelihood by gradient ascent on the differentiable hyperparameters, ht+1←ht + ∇h log p(D|h)|h=ht, (2) similarly to how we differentiate the log joint, or regularized log likelihood, to optimize neural network weights w. 2.2. Linearized Laplace Approximation The linearized Laplace ( LA) approximation (MacKay, 1992a) provides an approximation to the log marginal likeli- hood and its scalable variants are currently among the few approximations that have proven effective for deep learning hyperparameter optimization. First, the neural network is linearized at a mode of the posterior ˜w, flin ˜w (x; w, h) def = f(x; ˜w, h) +J˜w(x; h)(w − ˜w), (3) where J denotes the Jacobian of the neural network w.r.t. weights with entries [J˜w]cp = ∂fc(x;w,h) ∂wp |w= ˜w. Further, we have the Hessian of the negative log likelihood w.r.t.f, Λ˜w(x; h) def = −∇2 f log p(y|x, ˜w, h) ∈ RC×C, (4) which is positive semidefinite for common likelihoods used in classification and regression (Murphy, 2012). The linearized Laplace approximation log q(D|h) to the log marginal likelihood log p(D|h) is then given by log q˜w(D|h) = logp(D| ˜w, h) + logp( ˜w|h) − 1 2 log |JT ˜wΛ˜wJ˜w + P0| + P 2 log 2π, (5) where J˜w ∈ RNC ×P denotes stacked Jacobians J˜w(xn; h), and Λ˜w ∈ RNC ×NC is a block-diagonal matrix with blocks Λ˜w(xn; h) for n = 1, .., N, respectively, and we have sup- pressed the dependency on hyperparameters h for nota- tional brevity. The matrix P0 ∈ RP×P is the Hessian w.r.t. weights w of the log prior log p(w|h) and is diagonal here. Because the first two terms of the approximation constitute the log joint, i.e., the training loss, all terms are simple to estimate and differentiate except for the (log-)determinant, which can be written equivalently (Immer et al., 2021a) as |JT ˜wΛ˜wJ˜w + P0| = |J˜wP−1 0 JT ˜wΛ˜w + I||P0|, (6) and allows us to either compute the log-determinant of aP × P or NC ×NC matrix. Unfortunately, the log-determinant prohibits unbiased stochastic gradients on batches of data and is therefore not amenable to optimization with SGD. 2.3. Generalized Gauss-Newton and Neural Tangents The two alternative ways to compute the determinant in Equation 6 use either the generalized Gauss-Newton (GGN ) form or the neural tangent kernel (NTK ) form. In particular, H def = JT ˜wΛ˜wJ˜w and K def = J˜wP−1 0 JT ˜wΛ˜w (7) are the GGN approximation to the Hessian and the NTK with additional scaling by P−1 0 and Λ˜w, respectively.1 To obtain marginal likelihood gradients, we need to estimate and differentiate these two quantities. However, they are intractable for deep neural networks due to large numbers of parameters P and many data points and outputs NC . The GGN can be made tractable by structured block-diagonal and diagonal approximations (Martens & Grosse, 2015). Without loss of generality, we assume ordered parameter in- dices {1, .., P}. With a contiguous partition2 P = {Ps}S s=1 of parameter indices, we have H ≈ HP =   HP1 ··· 0 ... ... ... 0 ··· HPS   = HP1 ⊕...⊕| {z } block-diagonal stacking HPS , (8) a block-diagonal approximation where each block corre- sponds to a subset Ps. For example, if each Ps corresponds to the parameters of one particular layer, we recover a block- diagonal layer-wise approximation like KFAC (Martens & Grosse, 2015). If each set only contains a single parameter index, Ps = {s}, we recover a diagonal approximation. 2.4. Hyperparameter Optimization for Deep Learning To optimize general hyperparameters, we estimate and dif- ferentiate Laplace approximations during neural network training using scalable approximations (Immer et al., 2021a). The form of hyperparameters h governs the complexity of obtaining their gradients. For example, the typical problem of optimizing the prior precision, P0, is relatively simple because it acts linearly on the GGN and NTK . However, gen- eral hyperparameters, such as invariances, affect the forward pass of the neural network and act non-linearly on GGN and NTK . Therefore, they require higher-order differentiation. In the following, we illustrate the derived bounds for both settings, i.e., selecting a scalar or layer-wise prior precision (weight decay) on MNIST, and rotational invariance on rotated MNIST. We use a subset of 1000 random digits and apply a small 3-layer convolutional neural network with linear output layer. This setting ensures that we can compute the exact linearized Laplace in Equation 5 as a reference. 1We recover the standard finite width NTK for a Gaussian like- lihood and prior with unit variance, i.e., K = J˜wJT ˜w. 2A partition of a set S is a set P of disjoint subsets of S whose union is equal to S. It is contiguous if the elements in P are ordered and the order remains preserved in S. 3Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 3. Structured Parametric Laplace Approximations are Lower Bounds Practical linearized Laplace approximations of deep neu- ral networks use structured GGN approximations, such as diagonal and block-diagonal variants, to make the log- determinant computation tractable (Immer et al., 2021a; Daxberger et al., 2021; Antor ´an et al., 2022b). However, it is unclear to what extent these approximations are re- lated to the exact linearized LA. The following Theorem shows that such approximations are in fact lower bounds and therefore justifies their use better. This is similar to vari- ational inference (Blei et al., 2017), where more restrictive approximations, such as a diagonal instead of a multivariate Gaussian, lead to more slack in the evidence lower bound. Theorem 1 (Parametric structured lower bound). For any partitioning P of the parameter indices {1, .., P}, the block- diagonal approximation of HP to H results in a lower bound to the LA log marginal likelihood (Equation 5), i.e., log q˜w(D|h) ≥ log p(D, ˜w|h)− 1 2 log |HP +P0|+c, (9) where c = P 2 log 2π. Further, a refinement3 of the partition P to P′ will result in a lower bound with even more slack. Example 1.1. The block-diagonal Laplace approximation that only considers correlations within layers, similar to the popular KFAC4-Laplace (Ritter et al., 2018; Daxberger et al., 2021), is a lower bound to the linearized Laplace approximation. A diagonal Laplace approximation that only considers marginal variances of parameters is a further lower bound of such block-diagonal approximation. For a proof, refer to App. A. The bound holds for commonly used block-diagonal approximations and is further agnostic to the type of Hessian approximation. That is, it holds for the Hessian, GGN , or (empirical) Fisher variants of the LA alike. In Figure 3, we show the bound for different hyperparameter values h of the prior precision and rotational invariance in the illustrative setting described in Sec. 2.4. For the prior precision, only the diagonal approximation has a large slack and significantly different optimum, which is in line with previous empirical observations (MacKay, 1995; Daxberger et al., 2021). Perhaps surprisingly, it can still be useful to select the right rotational invariance because its shape and thus optimum is in line with the better approximations. While the above bounds justify existing Laplace approxi- mations, they do not improve scalability with dataset size, because we have partitioned the parameters rather than the dataset. To do so, we employ the dual NTK representation. 3A refinement P′ of P is a partition such that for all P′ s ∈ P′ there exists Ps ∈ Psuch that P′ s ⊆ Ps. 4Note that KFAC is a further approximation to the block- diagonal and a bound is not theoretically guaranteed. Nonetheless, it does seem to hold in practice. 10−2 100 102 prior precision −6 −4 −2 log q˜w(D|h) full blockdiag kron diag −3 −2 0 π/2 π rotational invariance −13 −12 Figure 3.Illustration of the parametric bound (Theorem 1) with commonly used approximation structures for prior precision and rotational invariance as hyperparameters h on MNIST with CNN. 4. Stochastic Gradients using the NTK While structured parametric lower bounds to the linearized Laplace marginal likelihood give good performance on model selection, a single hyperparameter gradient still re- quires an entire pass through the training data. This is because the log-determinant is in general not separable across data points and computing it on a subset of data would lead to an uncontrolled upper bound to the marginal likelihood. To overcome this limitation, we use the log- determinant in its dual NTK form in Equation 6 to devise lower bounds that enable batched computation on subsets of data and thus stochastic-gradient-based optimization. Our bounds work for NTK (Sec. 4.1) or parametric and structured GGN (Sec. 4.2) estimators alike and enable tightening the lower bound using larger batches of data or improving the partitioning of data points into batches (Sec. 4.3). 4.1. Subset-of-Data Kernel Bound Using the NTK form of the log-determinant, we construct lower bounds for subsets of data that enable stochastic marginal likelihood gradients and can therefore greatly im- prove the efficiency of hyperparameter optimization. Due to the cubic scaling with the number of data points, the NTK form has so far been only used in special cases (Immer et al., 2021a; Antor´an et al., 2022a). Using our bounds, how- ever, the NTK form on subsets of data becomes a tractable alternative for parametric variants like KFAC. Further, com- puting the NTK can be more architecture-agnostic (Novak et al., 2019; 2022) because it relies on plain Jacobians while structured approximations like KFAC are often non-trivial to compute or extend to custom architectures (Dangel et al., 2019; Osawa, 2021; Botev & Martens, 2022). Instead of partitioning the parameters as in Theorem 1, we partition the N inputs and C outputs and make use of the NTK form, which naively requires computing a NC × NC matrix, to obtain a lower bound for subsets of data. In particular, we have the set of indicesI = {(n, c) | 1 ≤ n ≤ N, 1 ≤ c ≤ C} corresponding to inputs xn and outputs fc of the neural network. The index order is fixed w.r.t. K but arbitrary. With partitioning B = {Bm}M m=1 of I, we have K ≈ KB = KB1 ⊕ ··· ⊕KBM , (10) 4Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 10−2 100 102 prior precision −2 −4 −6 log q˜w(D|h) 0 π/2 π rotational invariance −2 −4 −6 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 4.Illustration of the subset-of-data NTK lower bound (The- orem 2) with varying subset sizes for prior precision and rotational invariance as hyperparameters h on the two MNIST variants. where each KBm is a NTK on a subset of data and out- puts. Mathematically, we have KBm = JBmP−1 0 JT BmΛBm where JBm ∈ R|Bm|×P are the Jacobians for the input- output pairs and ΛBm ∈ R|Bm|×|Bm| the corresponding log-likelihood Hessian terms. Theorem 2 (Data subset lower bound). For any partitioning B of inputs and outputsI, the corresponding block-diagonal NTK approximation KB to K results in a lower bound to the linearized Laplace marginal likelihood, i.e., log q˜w(D|h) ≥ log p(D, ˜w|h) − 1 2 log |KB + I||P0| + c ∝ log p(D, ˜w|h) − 1 2 log |P0| −1 2 PM m=1 log |KBm + I|, (11) where c = P 2 log 2π is left out in the second line indicated by the proportionality. Further, a refinement of partitionB to B′ will result in a lower bound with more slack. We give a proof in App. A. Theorem 2 shows that we can use the NTK on subsets of data and outputs (for multi-output cases like classification) and obtain a valid lower bound. Further, the form of Equation 11 allows unbiased stochastic estimation, gradients, and therefore SGD-based optimiza- tion of hyperparameters using (mini-)batches of data. That is, by uniformly sampling index sets Bm, we have PM m=1 log |KBm +I| = MEm∼U[M][log |KBm +I|]. (12) In Figure 4, we show the tightness of the bound for different subset sizes |Bm| on the two illustrative problems. Despite small subset sizes, the bounds are tighter than the crude diagonal approximation and already around 2% of the input- output pairs can suffice to select good hyperparameters. 4.2. Subset-of-Data Parametric Bounds Computing the NTK on subsets of data can be efficient but is non-trivial to parallelize, in which case parametric approxi- mations like KFAC might be preferable (Osawa et al., 2019). Using the NTK lower bound in Theorem 2, we can devise an equivalent parametric estimator, which gives a lower bound on subsets of data (and parameters). This enables the use of approximations like KFAC with Laplace on batches of data and thus for SGD-based hyperparameter optimization. 10−2 100 102 prior precision −2 −4 −6 log q˜w(D|h) 0 π/2 π rotational invariance −2 −4 −6 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 5.Illustration of the parametric doubly lower bound using the block-diagonal GGN (Corollary 3.1) with varying subset sizes for prior and invariance hyperparameters. Setup as in Figure 4. Defining HBm def = HBm = JT BmΛBmJBm as a form of GGN on the subset of inputs and outputsBm, we have equivalence to the NTK bound, which we show in App. A: Theorem 3 (Parametric data subset bound). The NTK -based lower bound in Equation 11 to the linearized Laplace marginal likelihood is equivalent to a parametric variant, log p(D, ˜w|h)−1 2 log |P0|− 1 2 PM m=1 log |KBm+I| (13) =log p(D, ˜w|h)+ M-1 2 log |P0|-1 2 PM m=1 log |HBm+P0|, where HBm = JT BmΛBmJBm is estimated on a data subset. This Theorem is useful because it shows how to do stochas- tic estimation of the marginal likelihood on a subset of data using a parametric form, which resembles the GGN , instead of the NTK . However, the full GGN is quadratic in the num- ber of parameters and cannot be estimated in deep learning settings. To overcome this, we can further combine it with the parametric lower bound (Theorem 1) to justify struc- tured parametric approximations like the block-diagonal and its scalable approximation, KFAC, on subsets of data. Corollary 3.1 (Parametric doubly lower bound). For any partitioning B of input-output pairs I and P of parameter indices {1, .., P}, respectively, the following lower bound to the linearized Laplace marginal likelihood holds: log q˜w(D|h) ≥log p(D, ˜w|h) +M−1 2 log |P0| (14) − 1 2 PM m=1 log |HBm,P + P0| + P 2 log 2π, where HBm,P is a block-structured GGN approximation on subsets of input-output pairs. This doubly lower bound shows how parametric estimators, the most frequently used ones, can also be used for stochas- tic estimation and optimization of the log marginal likeli- hood. In Figure 5, we show the tightness of such bounds for the block-diagonal GGN . Surprisingly, even with just 5% of the input-output pairs, the bound is tighter then the diagonal approximation. Further, it only incurs a slight increase in slack compared to its NTK -based upper bound in Figure 4. In our experiments, we find that Corollary 3.1 successfully enables stochastic-gradient-based hyperparameter optimiza- tion with KFAC, which previously could only be used in the full-batch setting. 5Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 10−2 100 102 prior precision −2 −4 −6 log q˜w(D|h) 0 π/2 π rotational invariance −2 −4 −6 10 20 50 100 250 500 1000 indices |Bm| Figure 6.NTK Laplace bound using output-wise partitioning leads to tighter bounds and more efficiency. In comparison to Figure 4, the index set sizes are C = 10times smaller corresponding to an improved complexity by a factor of C3 at similar tightness. 4.3. Tighter Bounds with Better Partitions The tightness of the lower bounds derived from the NTK variant can be controlled with the partitioning B into input- output pairs. Bm can be understood as batches like in pa- rameter optimization using SGD. The bounds derived from the NTK motivate various choices of partitioning input data points and output dimensions so as to make the bounds as tight as possible. Mathematically, we want to find the parti- tion B that maximizes the lower bound in Equation 11. We discuss two simple partitioning cases B that can make the bounds derived from the NTK potentially tighter: partition- ing output dimensions and grouping inputs by labels. Partitioning the NTK by outputs means that each partition Bm contains only one particular output and corresponds to an independent kernel as, for example, used in Gaussian processes (Rasmussen & Williams, 2006). Each KBm is then an output-wise NTK , which can be much more efficient to compute than a full NTK . In the parametric space, this corresponds to a GGN for a single output and also greatly im- proves efficiency as it makes the GGN C times cheaper and is theoretically justified through a bound. Figure 6 shows that the output-wise NTK -based marginal likelihood bound is almost as tight as the full one at more thanC times smaller cost. Already 0.2% of the input-output pairs, or equivalently 2% of the data, suffice to learn invariances. Across the entire design space of estimators, it is this class-wise partitioning that provides many Pareto-optimal estimators in Figure 1. Alternatively, we can partition the index setI using the label information we have for each xn. In classification, we have access to labels yn for each xn. Assuming that the correla- tion of the NTK intra-class is greater than the anticorrelation inter-class, it is reasonable to partition I in such a way that data points xn with the same labels are in the same subset(s). In practice, this means that batches are made up from data points within the same class. We find that this approach can perform well but incurs a larger variance (cf. Figure 12 in App. C.1). In future work, it could be interesting to develop methods that track (anti-)correlation between inputs in the NTK to group them optimally during training. This is similar to inducing point optimization (Titsias, 2009). Algorithm 1 Stochastic Marginal Likelihood Estimate 1: Input: dataset D, likelihood, prior, random batch Bm ∈ B(partition of input-output indices; |B| = M), structure (e.g. NTK or GGN ), param partition P (opt.). 2: Let Dm denote data points of input indices in Bm 3: L ←|D| |Dm| log p(Dm|˜w, h) + logp( ˜w|h) − 1 2 log |P0| 4: if structure = NTK then 5: log q˜w(D|h) ← L −M 2 log |KBm +I| 6: else 7: log q˜w(D|h) ← L −M 2 log |HBm,PP−1 0 + I| 8: end if 9: Return log marginal likelihood estimate log ˜q ˜w(D|h). 4.4. Family of Estimators and Algorithm Through the NTK -based lower bound in Theorem 2, we derived estimators that enable stochastic marginal likeli- hood estimates and gradients. This allows us to optimize hyperparameters with SGD, just like neural network param- eters. In particular, Equation 12 shows that we can obtain a stochastic unbiased estimator of the lower bounds on the log-determinants derived from the NTK . The remaining terms are the log likelihood, which allows for an unbiased stochastic estimate naively, and simple terms like the log determinant of the prior and constants. In practice, we use our estimators for interleaved optimiza- tion of neural network weights and hyperparameters as in (Immer et al., 2021a). That is, we take gradient-steps on the hyperparameters every kth epoch after an initial burnin of b epochs. To obtain hyperparameter gradients, we choose a partitioning of the input-output indices I into B, which contains M such batches, and then sample from these uni- formly at random. Further, we resample the partition every epoch. In App. C.1, we show trajectories of this algorithm corresponding to the setting used for illustrating the bounds. To keep the bounds tight, we recommend output-wise parti- tioning since it eliminates complexity scaling in the number of outputs C while empirically maintaining a relatively tight lower bound. In practice, the most scalable bounds for hyperparameter gradients are the NTK -based bounds and doubly bounds using an efficient parametric approximation like KFAC, which even work well with relatively small sub- set sizes. Interestingly, the output-wise approximation can also be applied to KFAC-GGN giving a differentiable and scalable alternative for general hyperparameter learning. Alg. 1 shows how to compute stochastic marginal likelihood estimates. We use automatic differentiation to obtain gradi- ents w.r.t. h to train them with SGD. To make the bound as tight as possible and thus improve the approximations, we choose the partitioning and the size of its batches so as to fully utilize the available memory in practice. 6Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Dataset CIFAR-10 CIFAR-100 h Estimator log q˜w(D|h) log lik. acc. [%] time log q˜w(D|h) log lik. acc. [%] time prior precision BASELINE - -1.22 ± 0.04 84.6 ± 0.3 35% - -3.35 ± 0.14 56.7 ± 1.1 8% NTK -500-1 -0.87 ± 0.02 -0.53 ± 0.01 86.8 ± 0.1 60% -1.98 ± 0.08 -2.12 ± 0.04 61.7 + 0.6 16% KFAC-500-1 -1.41 ± 0.02 -0.38 ± 0.00 88.2 ± 0.1 65% -4.73 ± 0.11 -1.31 ± 0.04 63.9 ± 0.9 24% KFAC-500-10 -1.31 ± 0.01 -0.38 ± 0.00 88.6 ± 0.1 81% -3.95 ± 0.04 -1.26 ± 0.01 67.9 ± 0.2 70% KFAC-N-1 -1.10 ± 0.01 -0.38 ± 0.00 88.8 ± 0.1 52% -4.31 ± 0.09 -1.22 ± 0.02 66.8 ± 0.5 15% KFAC-N-C -0.79 ± 0.01 -0.39 ± 0.00 89.2 ± 0.1 100% -2.38 ± 0.03 -1.65 ± 0.07 64.6 ± 0.4 100% invariance NTK -150-1 -0.56 ± 0.05 -0.42 ± 0.01 90.3 ± 0.2 9% -1.19 ± 0.07 -1.45 ± 0.03 69.5 ± 0.8 4% KFAC-150-1 -0.59 ± 0.08 -0.29 ± 0.00 92.5 ± 0.2 10% -2.22 ± 0.37 -1.40 ± 0.05 71.8 ± 0.4 5% KFAC-N-1 -0.44 ± 0.07 -0.31 ± 0.01 93.0 ± 0.3 23% -3.43 ± 0.56 -1.20 ± 0.03 71.7 ± 0.7 10% KFAC-N-C -0.24 ± 0.02 -0.33 ± 0.01 93.2 ± 0.0 100% - - - ∼100% Table 1.Benchmark of stochastic marginal likelihood estimators on CIFAR classification tasks with a Wide ResNet (16-4) learning layerwise prior precisions (top) and affine invariances (bottom). “500-1” refers to a batch size of 500 and single-output approximation with N and C corresponding to the full-batch setting. The timing is relative to the full-batch estimator, KFAC-N-C, which was previously used for these settings. Our stochastic estimators can perform on par at up to 25-fold speed-up. Due to the number of classes, KFAC-N-C runs out of memory for CIFAR-100. Performance bold per category if standard errors with best overlap from both sides. 5. Experiments We experimentally validate the proposed estimators on vari- ous settings of marginal-likelihood-based hyperparameter optimization for deep learning. Overall, we find that the lower bounds using subsets of data or outputs often provide a better trade-off between performance and computational or memory complexity. In particular, they remain relatively tight even when applied only on small subsets of data and outputs. Therefore, they can greatly accelerate hyperpa- rameter optimization with Laplace approximations, making marginal-likelihood optimization possible at larger scale.5 In our experiments, we optimize prior precision parame- ters, P0, equivalent to weight-decay, per layer of neural networks (Immer et al., 2021a; Antor´an et al., 2022a) and learn invariances from data (van der Wilk et al., 2018; Immer et al., 2022b). Learning invariances requires differentiating the NTK or GGN and therefore acts as a realistic example of gradient-based optimization for general hyperparame- ters, which should be the long-term goal of Bayesian model selection for neural networks. In the following, we first discuss the illustrative example used throughout the theoretical development. Further, we compare our estimators to the full dataset KFAC-Laplace approximation, which previously provided the best results for hyperparameter optimization with Laplace approxima- tions (Immer et al., 2021a; Daxberger et al., 2021; Immer et al., 2022b). In this case, we find that estimators based on subsets of data are significantly more efficient and can perform on par even when the dataset is large. Lastly, we benchmark the two most scalable estimators on TinyIma- genet, a setting, where full dataset Laplace approximations failed due to computational costs (Mlodozeniec et al., 2023). 5Code: github.com/AlexImmer/ntk-marglik 5.1. Tightness of Bounds, Performance, and Runtime Throughout the theoretical development, we illustrate the tightness of the derived lower bounds to the linearized Laplace log marginal likelihood (cf. Figure 3, 4, 5, and 6). We use a small 3-layer convolutional network with linear output that has P ≈ 16 000parameters on a random subset of 1000 MNIST (LeCun & Cortes, 2010) digits, which al- lows analytical computation of both the full GGN and NTK . For the illustration using the rotational invariance, we addi- tionally rotate digits by a random angle θ ∼ U[−π, π]. We compare the prior precision and rotational invariance hyper- parameters with the log marginal likelihood estimate for the same trained neural network. In App. C.1, we additionally show the bound and corresponding test performance when optimizing it during training in both settings. Parametric estimators. Figure 3 indicates that block- diagonal and KFAC variant attain almost the same marginal likelihood as a full GGN and have the same test performance. It also shows that the diagonal approximation can fail catas- trophically due to its high slack in the bound (cf. Theorem 1) as it is the most refined partitioning P′ of parameter indices. Stochastic estimators. Figure 4 illustrates the proposed subset-of-data marginal likelihood estimators showing that already a small fraction of the input-output pairs leads to tight lower bounds. However, too small subset sizes can lead to failure when optimizing them, similar to the di- agonal approximation. The doubly lower bound, which allows using structured parametric estimators on subsets of data (cf. Corollary 3.1), is displayed in Figure 5 and is similarly tight. Further, handling the C = 10 outputs in- dependently for NTK -based estimators, which corresponds to a partitioning by class, almost performs as well as a full NTK in Figure 6 and is often significantly cheaper. 7Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 test log likelihood original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 translated CIFAR-10 Figure 7.Proposed stochastic marginal likelihood estimators usingNTK and KFAC with a fixed batch size of400 and class-wise partitioning perform well on subsets and modifications of CIFAR-10 with a ResNet. Our estimators can take more stochastic gradient steps than the full KFAC estimator at a faster runtime and thus greatly improve over it in terms of test log likelihood. This suggests that using many stochastic gradients can be more effective for hyperparameter learning than taking few exact gradients. More results in App. C.4. Pareto-efficiency. Figure 1 shows the obtained marginal likelihood estimates versus the runtime of a single estimate (and its gradient w.r.t. hyperparameters) indicating a Pareto- frontier between the two. Previously, only the full-batch estimators (black markers) were known, of which onlyKFAC is both tractable and performant. Our derived estimators greatly increase the design space and provide many Pareto- optimal estimators, especially for lower runtime budgets. Partitioning by class label. In Sec. 4.3, we hypothesized that grouping input-output sets by label information could improve bounds due to higher intra-class correlation than inter-class anti-correlation. Across all runs, this did not make a significant difference for learning invariances. How- ever, for optimizing solely the prior precision (cf. App. C.1) it improves the bound and test log likelihood by 10% on average. This suggests that improved partitioning can help. 5.2. Benchmark of Proposed Estimators We compare the proposed marginal likelihood estimators to the KFAC-Laplace, the state-of-the-art among Laplace ap- proximations for hyperparameter optimization (Immer et al., 2021a; Daxberger et al., 2021; Antor´an et al., 2022a). In the first setting, we optimize the layer-wise prior precision on a Wide ResNet 16-4 (Zagoruyko & Komodakis, 2016) on CIFAR-10 (Krizhevsky, 2009) and CIFAR-100 (Krizhevsky et al.). In the second setting, we additionally optimize learn- able invariances, similar to data augmentation, where the Laplace-based method by Immer et al. (2022b) is extremely costly since it requires more than one pass through the dataset per gradient. The results in Table 1 suggest that stochastic marginal likelihood estimators are particularly useful for learning invariances, where they accelerate the runtime up to 25-fold at similar performance. In addition to our stochastic estimators based on subsets of data, i.e., NTK - 150-1 and KFAC-150-1 with a batch size of 150 on single outputs, we show a full-dataset stochastic KFAC estimator that obtains a bound by sampling single random outputs (cf. remark in App. A). 5.3. Behavior with Varying Dataset Size Since larger datasets theoretically lead to looser bounds when using constant subsets of data, we investigate how this impacts performance in Figure 7. In particular, we follow Immer et al. (2022b) and learn invariances present in transformed versions of CIFAR-10 (Krizhevsky, 2009) on random subsets ranging from 1 000to all 50 000data points. We follow prior work by considering distribution over affine invariances (Benton et al., 2020; van der Ouderaa & van der Wilk, 2021) detailed in App. C.4. We evaluate the test log likelihood of KFAC and NTK -based estimators on subsets of 400 data points and single independent outputs, KFAC-400-1 and NTK -400-1. We compare the approach with the method by Immer et al. (2022b), KFAC-full (equal to KFAC-N-C). The cheaper marginal likelihood estimates and gradients allow us to do more hyperparameter updates per epoch given equivalent computational constraints. Compared to KFAC-full, we can therefore increase the number of hyper- parameter steps for KFAC-400-1 and NTK -400-1 from 10 to 100, while still reducing overall training time. Figure 7 shows that our estimators perform on par or better than the full KFAC variant in terms of test log likelihood, especially, but not only, for small dataset sizes. We hypothesize for invariance learning that the ability to do more gradient up- dates is more beneficial than having tighter lower bounds. Further details on invariance learning, experimental details, and results can be found in App. C.4. 5.4. Scaling to Larger Datasets and Models For invariance learning, the results on CIFAR-100 in Ta- ble 1 already indicated that KFAC-N-C is not scalable enough to enable gradient-based hyperparameter optimiza- tion. Mlodozeniec et al. (2023) also found that it is not pos- sible to run it on the even larger TinyImagenet dataset (Le & Yang, 2015), which has N = 100 000data points and C = 200classes, using a ResNet-50 with roughly 23 million parameters. Using the two fastest estimators for invariance learning, we show in Table 2 that our subset-of-data estima- tors enable optimizing hyperparameters in this setting. 8Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels h Estimator log likelihood accuracy [%] prior BASELINE -4.42 ± 0.08 45.6 ± 0.3 NTK -400-1 -4.16 ± 0.25 49.8 ± 0.2 KFAC-400-1 -2.17 ± 0.02 53.0 ± 0.1 invariance AUGERINO - 41.1 ± 0.2 PARTITIONED - 48.6 ± 0.0 NTK -70-1 -3.15 ± 0.08 56.3 ± 0.1 KFAC-60-1 -2.02 ± 0.10 58.4 ± 0.3 Table 2.Hyperparameter learning on TinyImagenet with ResNet- 50 in comparison to Augerino (Benton et al., 2020) and neural network partitioning taken from Mlodozeniec et al. (2023). While previous Laplace approximations are intractable in this setting, KFAC with a subset size of 60 data points on single outputs excels. We compare to the results provided by Mlodozeniec et al. (2023) and use the same architecture but with Fixup (Zhang et al., 2019b) instead of normalization layers, which would be incompatible with weight decay (Antor´an et al., 2022b). Optimizing layer-wise prior precisions can greatly improve over the baseline with default settings and no data aug- mentation. Further, our estimators improve over invariance learning using neural network partitioning (Mlodozeniec et al., 2023) and Augerino (Benton et al., 2020). 5.5. Practical Considerations In our experiments, the proposed estimators using subset-of- data lower bounds using a single output often perform best, in particular using the parametric KFAC variant. Although, the NTK -based variant often yields a tighter bound, KFAC- based bounds seem particularly well-suited for marginal- likelihood optimization and are therefore preferable in prac- tice. As illustrated through invariance learning, our stochas- tic estimators are efficient for general differentiable hyperpa- rameter optimization, which is an exciting future direction. 6. Related Work Marginal-likelihood optimization, also referred to as em- pirical Bayes, is the de-facto standard for hyperparame- ter optimization of Gaussian process models (Rasmussen & Williams, 2006) without validation data, and was used in the early days of Bayesian neural networks (MacKay, 1992b; Foresee & Hagan, 1997). Use in modern larger neural networks dwindled, with Blundell et al. (2015) re- porting failure using mean-field variational approximations, although the approach continued to work in Deep Gaussian Processes (Damianou & Lawrence, 2013; Dutordoir et al., 2020), and newer work demonstrates the feasibility in mod- ern neural networks (Ober & Aitchison, 2021; Immer et al., 2021a). Antor ´an et al. (2022c) recently proposed an in- teresting alternative based on sampling, which works for prior precisions but not general hyperparameters. In the context of deep learning, the marginal likelihood has so far been used to select regularization strength, invariances, architectures, and representations. Our derived estimators further extend the family of Laplace approximations and provide alternatives to their parametric approximations. The partitioning of the kernel in our estimators is similar to the Bayesian committee machine (Tresp, 2000; Deisenroth & Ng, 2015), which distributes Gaussian process inference into smaller kernels but does not give a valid lower bound on the marginal likelihood. For a detailed discussion on the benefits, but also issues, of the marginal likelihood, we refer to Gelman et al. (1995, Sec. 7). NTK and GGN Jacot et al. (2018) introduced the neural tangent kernel to characterize training dynamics of neural networks under squared loss. The training dynamics are even available in a closed-form when considering infinite width. However, computing kernels in this case is not triv- ial for common architectures (Novak et al., 2019). Our estimators use the NTK of neural networks at finite width, sometimes referred to as empirical NTK (Novak et al., 2022). Interestingly, the NTK and Gauss-Newton approximation to the Hessian are dual to each other as shown by (Khan et al., 2019), and Immer et al. (2021b) for general likelihoods. Our work builds on this duality and derives novel estimators from the NTK viewpoint that allow stochastic estimation. The NTK is also used to estimate generalization without the marginal likelihood, e.g., in the context of neural architec- ture search (Park et al., 2020; Chen et al., 2021). Further, the linearized Laplace with NTK been used to improve posterior predictives (Deng et al., 2022; Kim et al., 2023). 7. Conclusion In this paper, we have derived stochastic estimators for the linearized Laplace approximation to the marginal likelihood that are suitable objectives for stochastic-gradient based op- timization of hyperparameters. Our estimators are derived from a functional view of the Laplace using the neural tan- gent kernel and allow to trade off estimation accuracy and speed. Our experiments show that the (mini-)batch estima- tors perform on par with previous full-batch estimators but are many times faster. This suggests that they could be use- ful for learning more complex hyperparameters with SGD. Future research could further find ways to make bounds tighter, for example, by improving partitioning of the NTK . Further, it could be interesting to apply the fast estimators to large-scale Bayesian linear models. Acknowledgements A.I. gratefully acknowledges funding by the Max Planck ETH Center for Learning Systems (CLS). The authors thank the reviewers for their constructive feedback and comments, in particular R2, who suggested Figures 4, 5, and 6. 9Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels References Antor´an, J., Barbano, R., Leuschner, J., Hern ´andez- Lobato, J. M., and Jin, B. A probabilistic deep im- age prior for computational tomography. arXiv preprint arXiv:2203.00479, 2022a. Antor´an, J., Janz, D., Allingham, J. U., Daxberger, E., Bar- bano, R. R., Nalisnick, E., and Hern´andez-Lobato, J. M. Adapting the linearised laplace model evidence for mod- ern deep learning. In International Conference on Ma- chine Learning, pp. 796–821. PMLR, 2022b. Antor´an, J., Padhy, S., Barbano, R., Nalisnick, E., Janz, D., and Hern´andez-Lobato, J. M. Sampling-based inference for large linear models, with application to linearised laplace. arXiv preprint arXiv:2210.04994, 2022c. Benton, G., Finzi, M., Izmailov, P., and Wilson, A. G. Learning invariances in neural networks. arXiv preprint arXiv:2010.11882, 2020. Bishop, C. M. Pattern recognition and machine learning. Information Science and Statistics. Springer, 2006. Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia- tional inference: A review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017. Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural networks. In Proceed- ings of the 32nd International Conference on Machine Learning, pp. 1613–1622, 2015. Botev, A. and Martens, J. KFAC-JAX, 2022. URL http: //github.com/deepmind/kfac-jax. Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In International Con- ference on Machine Learning, International Convention Centre, Sydney, Australia, 2017. PMLR. Chen, W., Gong, X., and Wang, Z. Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. arXiv preprint arXiv:2102.11535, 2021. Cohen, T. and Welling, M. Group equivariant convolutional networks. In International conference on machine learn- ing, pp. 2990–2999. PMLR, 2016. Damianou, A. and Lawrence, N. D. Deep Gaussian pro- cesses. In International Conference on Artificial Intelli- gence and Statistics, volume 31, pp. 207–215, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR. Dangel, F., Kunstner, F., and Hennig, P. Backpack: Packing more into backprop. In Proceedings of 7th International Conference on Learning Representations, 2019. Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux-effortless bayesian deep learning. Advances in Neural Informa- tion Processing Systems, 34, 2021. Deisenroth, M. and Ng, J. W. Distributed gaussian processes. In International Conference on Machine Learning , pp. 1481–1490. PMLR, 2015. Deng, Z., Zhou, F., and Zhu, J. Accelerated linearized laplace approximation for bayesian deep learning. In Ad- vances in Neural Information Processing Systems, 2022. Dutordoir, V ., van der Wilk, M., Artemev, A., and Hensman, J. Bayesian image classification with deep convolutional gaussian processes. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS), Aug 2020. Fischer, E. ¨Uber den hadamardschen determinantensatz. Arch. Math. U. Phys. (3), 13:32–40, 1908. Foresee, F. D. and Hagan, M. T. Gauss-newton approxima- tion to bayesian learning. In International Conference on Neural Networks (ICNN’97), volume 3, pp. 1930–1935. IEEE, 1997. Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. Bayesian data analysis. Chapman and Hall/CRC, 1995. Hataya, R. and Nakayama, H. Gradient-based hyperparame- ter optimization without validation data for learning fom limited labels. 2021. Horn, R. A. and Johnson, C. R. Matrix analysis. Cambridge university press, 2012. Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.). Auto- mated Machine Learning - Methods, Systems, Challenges. Springer, 2019. Immer, A., Bauer, M., Fortuin, V ., R¨atsch, G., and Emtiyaz, K. M. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pp. 4563–4573. PMLR, 2021a. Immer, A., Korzepa, M., and Bauer, M. Improving pre- dictions of bayesian neural nets via local linearization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pp. 703–711, 2021b. Immer, A., Torroba Hennigen, L., Fortuin, V ., and Cotterell, R. Probing as quantifying inductive bias. In Proceed- ings of the 60th Annual Meeting of the Association for Computational Linguistics, pp. 1839–1851, 2022a. Immer, A., van der Ouderaa, T. F., Ratsch, G., Fortuin, V ., and van der Wilk, M. Invariance learning in deep neu- ral networks with differentiable laplace approximations. 10Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels In Advances in Neural Information Processing Systems, 2022b. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571–8580, 2018. Khan, M. E. E., Immer, A., Abedi, E., and Korzepa, M. Approximate inference turns deep networks into gaussian processes. In Advances in Neural Information Processing Systems, pp. 3088–3098, 2019. Kim, S., Park, S., Kim, K., and Yang, E. Scale-invariant bayesian neural networks with connectivity tangent ker- nel. 2023. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009. Krizhevsky, A., Nair, V ., and Hinton, G. Cifar-100 (canadian institute for advanced research). URL http://www. cs.toronto.edu/˜kriz/cifar.html. Le, Y . and Yang, X. Tiny imagenet visual recognition chal- lenge. CS 231N, 7(7):3, 2015. LeCun, Y . and Cortes, C. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/, 2010. URL http: //yann.lecun.com/exdb/mnist/. Lorraine, J., Vicol, P., and Duvenaud, D. Optimizing mil- lions of hyperparameters by implicit differentiation. In International Conference on Artificial Intelligence and Statistics, pp. 1540–1552. PMLR, 2020. MacKay, D. J. Bayesian model comparison and backprop nets. In Advances in neural information processing sys- tems, pp. 839–846, 1992a. MacKay, D. J. The evidence framework applied to classi- fication networks. Neural computation, 4(5):720–736, 1992b. MacKay, D. J. Probable networks and plausible predic- tions—a review of practical bayesian methods for super- vised neural networks. Network: computation in neural systems, 6(3):469–505, 1995. MacKay, D. J. Information theory, inference and learning algorithms. Cambridge university press, 2003. Martens, J. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1–76, 2020. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In Interna- tional conference on machine learning, pp. 2408–2417, 2015. Mlodozeniec, B., Reisser, M., and Louizos, C. Hyperpa- rameter optimization through neural network partitioning. arXiv preprint arXiv:2304.14766, 2023. Murphy, K. P.Machine learning: a probabilistic perspective. MIT press, 2012. Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl- Dickstein, J., and Schoenholz, S. S. Neural tangents: Fast and easy infinite neural networks in python. arXiv preprint arXiv:1912.02803, 2019. Novak, R., Sohl-Dickstein, J., and Schoenholz, S. S. Fast finite width neural tangent kernel. In International Con- ference on Machine Learning, pp. 17018–17044. PMLR, 2022. Ober, S. W. and Aitchison, L. Global inducing point varia- tional posteriors for bayesian neural networks and deep gaussian processes. In International Conference on Ma- chine Learning, pp. 8248–8259. PMLR, 2021. Ober, S. W., Rasmussen, C. E., and van der Wilk, M. The promises and pitfalls of deep kernel learning. In Uncer- tainty in Artificial Intelligence (UAI) , volume 161, pp. 1206–1216. PMLR, 2021. Osawa, K. Automatic Second-Order Differentiation Li- brary (ASDL), 2021. URL http://github.com/ kazukiosawa/asdl. Osawa, K., Tsuji, Y ., Ueno, Y ., Naruse, A., Yokota, R., and Matsuoka, S. Large-scale distributed second-order opti- mization using kronecker-factored approximate curvature for deep convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12359–12367, 2019. Park, D. S., Lee, J., Peng, D., Cao, Y ., and Sohl-Dickstein, J. Towards nngp-guided neural architecture search. arXiv preprint arXiv:2011.06006, 2020. Rasmussen, C. E. and Ghahramani, Z. Occam’s razor. In Advances in neural information processing systems, pp. 294–300, 2001. Rasmussen, C. E. and Williams, C. K. Gaussian processes for machine learning. MIT press Cambridge, MA, 2006. 11Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Ritter, H., Botev, A., and Barber, D. A scalable laplace approximation for neural networks. In International Con- ference on Learning Representations, 2018. Schw¨obel, P., Jørgensen, M., Ober, S. W., and van der Wilk, M. Last layer marginal likelihood for invariance learn- ing. In Proceedings of the Twenty Fifth International Conference on Artificial Intelligence and Statistics, 2022. Titsias, M. Variational learning of inducing variables in sparse gaussian processes. In International Conference on Artificial Intelligence and Statistics , volume 5, pp. 567–574. PMLR, 16–18 Apr 2009. Tresp, V . A bayesian committee machine.Neural computa- tion, 12(11):2719–2741, 2000. van der Ouderaa, T. F. and van der Wilk, M. Learning invariant weights in neural networks. In Workshop in Un- certainty & Robustness in Deep Learning, ICML, 2021. van der Wilk, M., Bauer, M., John, S., and Hensman, J. Learning invariances using the marginal likelihood. In Advances in Neural Information Processing Systems, pp. 9938–9948, 2018. Wang, R., Walters, R., and Yu, R. Approximately equivari- ant networks for imperfectly symmetric dynamics. In In- ternational Conference on Machine Learning, pp. 23078– 23091. PMLR, 2022. Zagoruyko, S. and Komodakis, N. Wide residual networks. In BMVC. BMV A Press, 2016. Zhang, G., Wang, C., Xu, B., and Grosse, R. B. Three mechanisms of weight decay regularization. In ICLR (Poster). OpenReview.net, 2019a. Zhang, H., Dauphin, Y . N., and Ma, T. Fixup initialization: Residual learning without normalization. In International Conference on Learning Representations, 2019b. 12Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels A. Theoretical Results and Proofs The following Theorem and corollary are required to bound the parametric and NTK -based marginal likelihood approxi- mations as they allow to bound the determinant, or the log-determinant, respectively by ignoring off-diagonal blocks of a matrix. Theorem 4 (Fischer’s inequality (1908)). For a positive definite matrix M, as defined hereafter, it holds that det M = det \u0014A B BT C \u0015 ≤ det \u0014A 0 0 C \u0015 = det(A ⊕ C) = detA det C. (15) This Theorem is from Fischer (1908). We refer to Horn & Johnson (2012) for a proof. We immediately have the following useful corollary negative log-determinants, which show up in the Laplace approximation to the log marginal likelihood. The result simply follows from the fact that the log is monotonically increasing. Corollary 4.1. For a positive definite matrix M as defined in Theorem 4, the following inequality holds −log detM = −log det \u0014A B B C \u0015 ≥ −log detA − log detC. (16) For our bounds, a more general partitioned form of M is required, which corresponds to repeated bounding with Theorem 4 through partitions. In particular, we deal with a matrix M ∈ RM×M and use indices 1 through M, i.e., [M] ={1, . . . , M} that, for example, allow to denote the mth diagonal element as Mm,m. Further, we introduced a notation based on index sets in Sec. 2 that allows to denote blocks or off-diagonal elements of M. In particular, let A ⊂[M] and A′ ⊂ [M] disjoint subsets, A ∩ A′ = ∅ of the indices denoting dimensions in M, then we write MA,A = MA ∈ R|A|×|A| for the square block matrix with entries Mi,j such that i ∈ Aand j ∈ A. In line with this, off-diagonal blocks are MA,A′ ∈ R|A|×|A′|. Partitioning the dimension indices [M] of M into two disjoint subsets A and A′, we have three blocks, MA, MA′ , and MA,A′ . Since we can re-order the matrix to conform to such blocks, because it relies on simultaneous permutation of rows and columns, we can apply Theorem 4 to M and have det M = det \u0014 MA MA,A′ MA′,A MA′ \u0015 ≤ det \u0014MA 0 0 M A′ \u0015 = det(MA ⊕ MA′ ) = detMA + detMA′ . (17) Note that the first equality here only holds for the determinant and not for its matrix argument since the partitioning by indices permutes the rows and columns simultaneously, i.e., an even amount of times. With this, we have the following Lemma Lemma 1. For any partitioning P = {Ps}S s=1 of dimension indices [M] of the positive definite matrix M, we have det M = det   MP1 MP1,P2 ··· MP1,PS MP2,P1 MP2,P2 ··· ... ... ... ... ... MPS,P1 ··· ··· MPS   ≤ det(MP1 ⊕ ··· ⊕MPS ) = SY s=1 det MPs, (18) and further refining the partitioning P into P′, such that each element in P′ is a subset of P′ is a futher upper bound. Proof. To obtain this bound, we iteratively apply Theorem 4. In particular, let Rs be the complement of the first s subsets, i.e., Rs = {P1, . . . ,Ps}∁, for example, R1 = P∁ 1 and RS = ∅.6 The complement (·)∁ here is with respect to the full set of indices [M]. Using Theorem 4 S times on these complementing sets, we have det M = det \u0014 MP1 MP1,R1 MR1,P1 MR1 \u0015 ≤ det MP1 det MR1 = detMP1 det \u0014 MP2 MP2,R2 MR2,P2 MR2 \u0015 ≤ det MP1 det MP2 det MR2 = ··· ≤ SY s=1 det MPs. This proves the main statement of the Lemma. By refining the partition, we simply extend the upper bounds since a refinement splits up each element in P into at least one subset, which again enables application of Theorem 4. 6The order 1, . . . , Sis arbitrary. 13Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Theorem 1 (Parametric structured lower bound). For any partitioning P of the parameter indices {1, .., P}, the block- diagonal approximation of HP to H results in a lower bound to the LA log marginal likelihood (Equation 5), i.e., log q˜w(D|h) ≥ log p(D, ˜w|h) − 1 2 log |HP + P0| + c, (9) where c = P 2 log 2π. Further, a refinement7 of the partition P to P′ will result in a lower bound with even more slack. Proof. Since the log-likelihood and log-prior terms are identical, we only need to inspect the relationship between−log |H+ P0| and −log |HP + P0|. Due to the assumption that P0 is diagonal, it can be added to the individual blocks ofHP since a diagonal is the most refined partition possible on the indices {1, . . . , P}. 8 The lower bound on the negative log determinant and the increased slack when refining then follows from Lemma 1, which upper bounds the product of determinants, and thus lower bounds the negative log determinant (Corollary 4.1). Theorem 2 (Data subset lower bound). For any partitioning B of inputs and outputs I, the corresponding block-diagonal NTK approximation KB to K results in a lower bound to the linearized Laplace marginal likelihood, i.e., log q˜w(D|h) ≥ log p(D, ˜w|h) − 1 2 log |KB + I||P0| + c ∝ log p(D, ˜w|h) − 1 2 log |P0| −1 2 PM m=1 log |KBm + I|, (11) where c = P 2 log 2π is left out in the second line indicated by the proportionality. Further, a refinement of partitionB to B′ will result in a lower bound with more slack. Proof. As shown by Immer et al. (2021a) and in Sec. 2, the linearized Laplace approximation can be written in NTK -form using the matrix determinant Lemma giving log q˜w(D|h) = logp(D, ˜w|h) − 1 2 log |J˜wP−1 0 JT ˜wΛ˜w + I||P0| + P 2 log 2π = logq(D, ˜w|h) − 1 2 log |P0| + P 2 log 2π − 1 2 log |K + I| ≥ log q(D, ˜w|h) − 1 2 log |P0| + P 2 log 2π − 1 2 log |KB + I| = logq(D, ˜w|h) − 1 2 log |P0| + P 2 log 2π − 1 2 PM m=1 log |KBm + I|, where we first re-order and use our definition of the scaled NTK (cf. Sec. 2). Then, we use the lower bound according to Lemma 1 further giving us the statement that refinement leads to more slack. We again note that adding a diagonal, in this case I to K, can simply be absorbed into the block-matrices and extracted afterwards when applying it. In particular, define ˆK = K + I, apply the bound, and we obtain ˆKBm = KBm + I for all m and thus ˆKB = KB + I. Theorem 3 (Parametric data subset bound). The NTK -based lower bound in Equation 11 to the linearized Laplace marginal likelihood is equivalent to a parametric variant, log p(D, ˜w|h)−1 2 log |P0|− 1 2 PM m=1 log |KBm+I| (13) =log p(D, ˜w|h)+ M-1 2 log |P0|-1 2 PM m=1 log |HBm+P0|, where HBm = JT BmΛBmJBm is estimated on a data subset. Proof. The idea is to apply the matrix determinant Lemma to move from the subset of data NTK bound back to a parametric variant. This gives us a log determinant that depends on the GGN defined on subsets of input-output pairs. We subtract the log joint, log p(D, ˜w|h), and add 1 2 log |P0| from to both sides of Equation 13 and multiply by 2 to abbreviate the following equations. We then have −PM m=1 log |KBm + I| = −PM m=1 log |JBmP−1 0 JT BmΛBm + I| = M log |P0| −PM m=1 log |JBmP−1 0 JT BmΛBm + I||P0| = M log |P0| −PM m=1 log |JT BmΛBmJBm + P0| = M log |P0| −PM m=1 log |HBm + P0|, 7A refinement P′ of P is a partition such that for all P′ s ∈ P′ there exists Ps ∈ Psuch that P′ s ⊆ Ps. 8In case, P0 would not be diagonal and its structure would not correspond to a refinement of P, one would have to treat it jointly with H and apply the bound to ˆH def = H + P0 and then ˆHP. 14Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels where JBm corresponds to the Jacobians only for the input-output pairs in Bm and ΛBm to the log-likelihood Hessian for these outputs (cf. Sec. 2.3). subtracting and adding the removed quantities again concludes the proof. Remark. The form of Equation 13, in particular HBm is reminiscent of a GGN approximation but just on a subset of input-output pairs. In particular, it depends on the subset structure chosen for the NTK bound, and could be a class-wise GGN or simply on a random subset of data. Subsets of data do not change the shape of the parametric estimator, i.e., it still requires the log determinant of a P × P matrix but computing that matrix can be greatly sped up: apart from the requirement to compute Jacobians for all N data points, a major issue with the GGN approximation is that its cost scales linearly with the outputs C as well (Botev et al., 2017). Therefore, it is often approximated using sampling via equality to the Fisher information (Martens & Grosse, 2015) for exponential family likelihoods of natural form (Martens, 2020). By partitioning the input-output pairs [NC ] into C partitions, we can compute the GGN for a single output and use it as a proper lower-bound to the LA marginal likelihood at the same cost as the Fisher, which does not lead a lower bound of the LA due to the required sampling approximation. Further, we can apply our parametric bound in Theorem 1 to use mixed bounds on subsets of data and subsets of parameter groups enabling, for example, a lower bound to the LA marginal likelihood using KFAC on a subset of data. Corollary 3.1 (Parametric doubly lower bound). For any partitioning B of input-output pairs I and P of parameter indices {1, .., P}, respectively, the following lower bound to the linearized Laplace marginal likelihood holds: log q˜w(D|h) ≥log p(D, ˜w|h) +M−1 2 log |P0| (14) − 1 2 PM m=1 log |HBm,P + P0| + P 2 log 2π, where HBm,P is a block-structured GGN approximation on subsets of input-output pairs. Proof. This corollary simply follows from applying the parametric lower bound, Theorem 1, to the parametric estimator on a subset of data in Theorem 3. B. Computational Considerations and Complexity The computational complexity of the proposed estimators as well as the GGN , NTK , and their corresponding approximations like KFAC greatly depend on the model architecture. For simplicity, we assume a neural network with P parameters in L linear hidden layers with each width of D, inputs x ∈ RD of the same dimensionality, with output dimensionality C, i.e., we have P = DC + P l∈[L] D2, where DC are the parameters of the linear output layer. Further, we have N data points. The complexity of automatically differentiating the log determinant of the GGN or NTK is equivalent to computing the log determinant of these matrices defined in Equation 7. Computing the log determinant of the GGN first requires computing the GGN itself, which is O(NP 2C) for computing the sum of N Jacobian outer products, where each Jacobian is C × P. The log determinant is then additionally O(P3). In comparison, for naive computation of the NTK log determinant, we compute the NC × NC kernel in O(N2C2P) and its determinant in O(N3C3). The complexity for KFAC-GGN is O(NLD2C) for summing N outer products for L layers and its determinant O(D3) for each layer giving O(LD3). We have the following computational and complexities for computing and differentiating the log determinant using automatic differentiation: GGN ∈ O(NP 2C + P3) NTK ∈ O(N2C2P + N3C3) KFAC-GGN ∈ O(NLD2C + LD3). (19) While these are for naive estimators that can be improved for certain architectures, e.g., see Novak et al. (2022) for faster NTK computations, these complexities describe the worst-case setting. Our proposed lower bounds based on subsets of inputs and outputs can greatly improve these complexities. In particular, we can replace N by a subset size M ≪ N and use output-wise bounds to obtain C = 1. In our experiments, the fastest and still performant methods are NTK -M-1 and KFAC-M-1, which have a computational and memory complexity for estimation and automatic differentiation of O(M2P + M3) and O(MLD2 + LD3), respectively. Depending on the architecture, the NTK estimator can further be faster than O(M2P). Overall, the proposed methods attain an acceleration at least linear in the number of outputs C and data points N. 15Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels C. Additional Experimental Results and Details We provide additional experimental details and results complementing those in the main text here. For our implemen- tation, we modify and extend the asdl library (Osawa, 2021) that offers fast computation of KFAC and NTK , as well as laplace-torch (Daxberger et al., 2021) for the marginal likelihood approximations. The code is available at https://github.com/AlexImmer/ntk-marglik. C.1. Tightness of Bounds, Performance, and Runtime The experiments illustrating the bounds in the main text described in Sec. 5.1 are conducted on randomly chosen 1000 MNIST subsets for three random seeds and further images are fully rotated at random, i.e., up to ±π. We then use the LILA method proposed by Immer et al. (2022b) to try to learn these underlying invariances, which is essential to generalize well on this task since standard convolutional and fully connected layers are not rotationally invariant. We use a simple convolutional network with three layers, max pooling, and a linear classification head that totals roughly 16 000parameters so that we can compute and differentiate the full GGN and NTK . We use 30 augmentation samples to average the outputs and optimize the network parameters, invariance parameters, and prior parameters with Adam (Kingma & Ba, 2015). For network parameters we use a learning rate of 10−3 and decay it to 10−9 using cosine decay and use a batch size of 250. The invariance and prior learning hyperparameters follow the settings of Immer et al. (2022b): 10 epochs burnin and then update hyperparameters every epoch with learning rates 0.1 and 0.05 for prior precision and invariance parameters, respectively. Both are decayed by a factor of 10 using cosine decay. We use an optimized network to assess the bounds in Figures 4 to 6. After convergence, we apply the different bounds and assess them over the entire grid of rotational invarianceη ∈ [0, π]. Here, we additionally present results for hyperparameter optimization using the same bounds in Figure 8 and Figure 9. The experiments are run on an internal compute cluster with different NVIDIA GPUs. The timing experiments are run on a single A100 sequentially to ensure comparability. In addition to the invariance learning experiments, we compare the bounds for varying values of a scalar prior precision in all illustrative figures in the main text. Here, we show results optimizing the layerwise prior precision in Figure 10, 11, and 13 again on 1000 randomly sampled MNIST digits per seed but without rotating them. When only learning prior precision parameters and not invariances, a slack in the bound translates more directly into a slack in the test performance as can be seen in the figures. However, it is worth noting that the NTK and KFAC estimators are significantly cheaper on large batches of data for just optimizing the prior precision, in comparison to learning invariances. In Figure 13, we see that stochastic NTK -based estimators are Pareto-optimal in many cases as it was the case for invariance learning. We also find that small batch sizes do not lead to problems at larger scale (cf. Sec. 5.2) where they perform on par with full dataset marginal likelihood estimators. In Figure 12, we show the effect of class-wise partitioning of batches on the bound in comparison to the random partition. We find that the bound can become tighter for both prior precision and invariance learning but incurs a slightly higher variance as indicated by the standard error displayed. 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood full blockdiag kron diag 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 8.Parametric and NTK lower bound when optimizing invariances on random subsets of size 1000 from rotated MNIST (LeCun & Cortes, 2010). Only small subset sizes and the diagonal approximation fail to learn the invariance and result in good test log likelihood. This figure corresponds to Figure 3 and Figure 4 in the main text, which show the bounds constructed at step500 with parameters and hyperparameters trained by the “full” variant. 16Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood 10 20 50 100 250 500 1000 indices |Bm| Figure 9.Doubly parametric subset-of-data bound on the left with block-diagonal approximation and class-wiseNTK -based approximation. These figures correspond to Figure 5 and Figure 6 in the main text but use the bounds for optimization of invariances during training. In line with the bounds, already small subset sizes suffice to pick up the invariance in rotated MNIST and achieve good test log likelihood. 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood full blockdiag kron diag 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 10.Parametric and NTK lower bound when optimizing prior precision and not invariances on random subsets of size 1000 from MNIST (LeCun & Cortes, 2010). In contrast to invariance learning, lower bounds result in more reduction in test performance for the NTK -based bounds that use stochastic estimates. This figure corresponds to Figure 3 and Figure 4 in the main text. 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood 10 20 50 100 250 500 1000 indices |Bm| Figure 11.Doubly parametric subset-of-data bound with block-diagonal approximation (left) and class-wise NTK -based approximation (right) for prior precision optimization corresponding to Figure 5 and Figure 6 in the main text. 10− 2 100 102 prior precision − 2 − 4 − 6 log q˜w (D|h) 0 π/2 π rotational invariance − 2 − 4 − 6 100 200 500 1000 2500 5000 10000 indices|Bm| 10− 2 100 102 prior precision − 2 − 4 − 6 log q˜w (D|h) 0 π/2 π rotational invariance − 2 − 4 − 6 100 200 500 1000 2500 5000 10000 indices|Bm| Figure 12.Effect of partitioning data by class labels (right) in comparison to random partitioning (left; same figure as Figure 4) as proposed in Sec. 4.3. Using the class labels can slighty redue the slack of the bound but tends to incur a larger variance in the estimated bound as indicated by the shaded regions denoting one standard error. C.2. Additional Details for Estimator Benchmark on CIFAR Marginal likelihood estimation with the linearized Laplace in previous works was commonly applied to ResNets on CIFAR- 10 and CIFAR-100, where it gives remarkable performance improvements over unregularized networks in the case where we have no data augmentation or prior information. For example Immer et al. (2021a) and Daxberger et al. (2021) show that it improves the performance by 4% points over the baseline on CIFAR-10. Here, we reproduce this experiment and compare the performance of the previously used full-batch KFAC estimator, which produced the best results, to the proposed stochastic estimators. The results are given in Table 1 in the top block. Indeed, all estimators improve over the baseline that uses default settings of a Wide ResNet (Zagoruyko & Komodakis, 2016) but without data augmentation and Fixup initialization instead of normalization layers (Zhang et al., 2019b). We use the Wide ResNet 16-4 with Fixup (Zhang et al., 2019b), since a Gaussian prior conflicts with normalization layers (Zhang et al., 2019a; Antor ´an et al., 2022b). We optimize 17Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 10−2 10−1 100 Runtime (s) 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Negative Log Marginal Likelihood Full GGN NTK KFAC Diag GGN 10 20 50 100 250 500 1000 2500 5000 10000 batch size Figure 13.Pareto-frontier for prior precision learning runs on random subsets of 1000 MNIST digits with a small CNN. This figure corresponds to Figure 1 in the main text, which displays estimators for additional invariance learning, not only learning priors. Many of the NTK -based class-wise estimators are Pareto-optimal, i.e., the achieve the lowest marginal likelihood at a given runtime budget. the network for 300 epochs with a batch size of 128 using SGD with momentum of 0.9 and optimize the prior precision every 5 epochs after a burnin phase of 10 epochs for with 50 gradient steps using a learning rate of 0.1 and Adam (Kingma & Ba, 2015) for both CIFAR-10 and CIFAR-100. In the lower block of Table 1, we use our estimators in the context of learning invariance using laplace approximations (LILA , Immer et al., 2022b). We use the default hyperparameters of their largest-scale experiment, where they learn invariances with Wide ResNets using 20 augmentation samples. Their method uses full dataset KFAC for 200 epochs and takes more than 48 hours on of training on CIFAR-10 with an NVIDIA A100 GPU because they have to compute a preconditioner on the entire dataset for each hyperparameter gradient. On CIFAR-100, due to the scaling with output classes, it is therefore intractable as it would likely take weeks. The default settings from CIFAR-10 lead to out-of-memory errors. In contrast, our stochastic estimators can be directly evaluated on a small batch with a single output and thus improve the runtime by a factor 10 while maintaining the same performance, i.e., compare KFAC-150-1 with KFAC-N-C. Also our single-output KFAC bound already increases runtime by a factor of 5 and achieves the same performance. The choice of the batch size in our bounds is such that the GPU memory is utilized as much as possible. We therefore use NVIDIA A100 GPUs with 80GB memory to make the bounds as tight as possible and improve performance. To compare the runtimes, all methods use the same GPU type and runtimes are averaged over 3 or 5 seeds for invariance and prior learning, respectively. C.3. Details on TinyImagenet Benchmark using ResNet-50 We largely follow the setup of Mlodozeniec et al. (2023) in this experiment and use their results on Augerino (Benton et al., 2020) and their own network partitioning method for hyperparameter optimization. The key difference is that we use Fixup (Zhang et al., 2019b) instead of normalization layers in the ResNet-50 architecture. Like Mlodozeniec et al. (2023), we use 16 channels in the first layer and the standard growth factor of 2 per layer. The network has roughly 23 000 000 parameters and therefore poses a complex task for differentiable hyperparameter optimization. Even with 80GB of GPU memory, we can only fit 60 to 70 data points for differentiable KFAC or NTK computations. However, the performance is still sufficient to improve over the baselines. We optimize the network weights for 100 epochs and, as in the case for CIFAR, decay the learning rate with a cosine schedule from 0.1 to 10−6. The hyperparameters are optimized for 50 steps every epoch after 10 epochs of burn-in with a learning rate of 0.01 decayed to 0.001 with a cosine schedule. The learning rate for prior parameters is constant at 0.1. For only prior precision optimization, we use the default setting by Immer et al. (2021a) and update the prior parameters for 50 steps every 5 epochs after 10 epochs of burn-in. 18Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels C.4. Additional and Detailed Results for Invariance Learning on CIFAR-10 Subsets. It is well known that soft or hard symmetry constraints are beneficial to machine learning methods and neural networks in particular, equivariant symmetries of (group-)convolutional layers being a canonical example (Cohen & Welling, 2016). Although explicitly embedding symmetry in architectures can be very effective, they need to be known and specified in advance and can not be adapted. The right symmetry can be difficult to choose and misspecified symmetries have the risk of restricting the capacity of a model to fit data (Wang et al., 2022). Symmetry discovery methods aim to automatically learn symmetries from available training data, which is a difficult task because symmetries provide constraints and are therefore not necessarily favorable in terms of typical training losses. In invariance learning, we parameterise a learnable distribution g(x; η) over possible transformations on inputs, defined by invariance parameters η. By averaging outcomes of a regular non-invariant neural network f over inputs transformed by g, we obtain a predictor ˜f, which is (soft-) invariant to the transformations specified by η. We might take S Monte Carlo samples ϵ1, . . . ,ϵS iid ∼ p(ϵ) (van der Wilk et al., 2018; Benton et al., 2020) to obtain an unbiased estimate of the invariant function ˆf: ˆf(x; w, η) =Ep(x′|x,η)[f(x′; w)] =Ep(ϵ)[f(g(x, ϵ; η); w)] ≈ 1 S P sf(g(x, ϵs; η); w) , (20) where ˆf is differentiable in both model parameters w as well as invariance parameters η through the reparameterisation trick (Kingma & Welling, 2013). In our experiments, we consider a combination of uniform distributions on affine generator matrices resulting in a 6-dimensional invariance parameter η ∈ R6 that defines a density over the group of affine transformations, individually controlling x-translation, y-translation, rotation, x-scaling, y-scaling and shearing. Details on the used parameterisation can be found in Benton et al. (2020) and App. B of Immer et al. (2022b). Jointly learning model parameters w and invariance parameters η is not a trivial exercise, as typical maximum likelihood solutions will always select the least restrictive no invariance in order to fit training data best, given a sufficiently flexible model. To overcome this issue, some people have considered the use of validation data, such as cross-validation or more sophisticated approaches, which may require expensive retraining or involved outer loops. Alternatively, Augerino (Benton et al., 2020) learns invariance solely on training data, but relies on explicit regularisation in order to do so, which requires knowledge of the used invariance parameterisation and additional tuning. For a discussion on particular failure cases of this approach we refer to Sec. 2. and App. C. of Immer et al. (2022b), which proposes to use scalable marginal likelihood estimates to learn invariance parameters. The marginal likelihood offers a principled way to learn invariances through Bayesian model selection, balancing data fit and model complexity through an Occam’s razor effect. Unlike prior works that rely on the true marginal likelihood (van der Wilk et al., 2018) or variational lower bounds thereof (van der Wilk et al., 2018; van der Ouderaa & van der Wilk, 2021), modern differentiable Laplace approximations of the marginal likelihood can scale to large datasets and deep neural networks, such as ResNets. We consider this as a baseline, and denote it by KFAC-full as it uses the full dataset to estimate the marginal likelihood, which leads to a significant slowdown in training compared to classical MAP training. To improve estimates, we have derived lower bounds to the marginal likelihood that can be computed much more cheaply on subsets of the training data. This allows us to take more hyperparameter gradient steps per epoch at equivalent or faster training time. We hypothesize that taking more gradient steps can be beneficial, even though individual marginal likelihood estimates provide looser bounds. We empirically evaluate performance of our method in conjunction with invariance learning on subsets of the same transformed versions of CIFAR-10 as used in Immer et al. (2022b). The use of reduced dataset sizes is a common data efficiency benchmark for invariance learning methods (Schw¨obel et al., 2022). We measure performance for varying total dataset sizes where we estimate marginal likelihoods on fixed subsets of 400 datapoints and use independent outputs, KFAC-400-1 and NTK -400-1. We compare to KFAC-full which always uses the full set of available training data points for its estimates. We use a ResNet architecture with Fixup (Zhang et al., 2019b), to prevent Gaussian prior conflicts with normalization layers (Zhang et al., 2019a; Antor´an et al., 2022b), and take S=20 Monte Carlo of invariance transformations. We optimize the network for 200 epochs with a batch size of 128 using Adam (Kingma & Ba, 2015) with cosine annealed learning rates starting at 0.1 for parameters and prior variances and invariance parameter learning rates starting at 0.05 for KFAC-full and 0.01 for KFAC-400-1 and KFAC-400-1, which use more gradient steps. In the figures below, we show the marginal likelihood estimate (Figure 14), test log-likelihood (Figure 15) and test accuracy (Figure 16). We find that marginal likelihood estimates of KFAC-full are higher, which is not surprising since KFAC-400-1 and KFAC-400-1 use looser bounds. In terms of test performance, we find that using more approximate gradients can at the same time improve runtime as well as test accuracy and test log-likelihood. 19Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 1k 5k 10k 20k full subset size −3 −2 −1 0 q(D |h) original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size −3 −2 −1 0 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size −3 −2 −1 0 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size −3 −2 −1 0 translated CIFAR-10 Figure 14.Comparison of marginal likelihoods on different subsets of CIFAR-10 datasets. 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 test log likelihood original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 translated CIFAR-10 Figure 15.Comparison of test log-likelihood on different subsets of CIFAR-10 datasets. 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 test accuracy original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 translated CIFAR-10 Figure 16.Comparison of test accuracy on different subsets of CIFAR-10 datasets. 20",
      "meta_data": {
        "arxiv_id": "2306.03968v1",
        "authors": [
          "Alexander Immer",
          "Tycho F. A. van der Ouderaa",
          "Mark van der Wilk",
          "Gunnar Rätsch",
          "Bernhard Schölkopf"
        ],
        "published_date": "2023-06-06T19:02:57Z",
        "pdf_url": "https://arxiv.org/pdf/2306.03968v1.pdf",
        "github_url": "https://github.com/AlexImmer/ntk-marglik"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addresses the scalability limitations of gradient-based hyperparameter optimization in deep learning, particularly the high computational cost of estimating marginal likelihood gradients which typically require a full pass through the dataset. The paper introduces novel stochastic lower bounds to the linearized Laplace approximation of the marginal likelihood, which are amenable to stochastic-gradient-based optimization. These bounds allow for efficient, batch-based hyperparameter optimization, enabling a trade-off between estimation accuracy and computational complexity. The new estimators significantly accelerate gradient-based hyperparameter optimization, making it applicable to larger datasets and complex tasks like invariance learning, where previous full-batch methods were intractable.",
        "methodology": "The methodology is centered on deriving stochastic lower bounds for the linearized Laplace approximation of the marginal likelihood. This is achieved by leveraging the dual function-space form of the linearized Laplace, which can be estimated using the Neural Tangent Kernel (NTK). The key steps include: 1) Proving that existing structured parametric approximations (e.g., block-diagonal and diagonal Gauss-Newton, KFAC) are, in fact, lower bounds to the linearized Laplace marginal likelihood (Theorem 1). 2) Deriving novel data-subset lower bounds using the NTK form of the log-determinant, which allows partitioning input-output pairs into batches for unbiased stochastic estimation and gradients (Theorem 2). 3) Demonstrating the equivalence of the NTK-based data-subset bound to a parametric variant (Theorem 3), and combining this with the parametric structured bounds to create 'doubly lower bounds' for structured parametric approximations (like KFAC) on data subsets (Corollary 3.1). The paper also explores strategies for partitioning data (e.g., output-wise partitioning or grouping by labels) to improve bound tightness and efficiency.",
        "experimental_setup": "Experiments were conducted across various deep learning tasks and datasets to validate the proposed stochastic estimators: 1) **Illustrative Examples:** A small 3-layer convolutional neural network (approx. 16,000 parameters) was used on random subsets of 1,000 MNIST digits (both original and rotated) to optimize layer-wise prior precision (weight decay) and rotational invariance. 2) **CIFAR-10 and CIFAR-100 Classification:** A Wide ResNet 16-4 architecture with Fixup initialization (to prevent conflicts with Gaussian priors) was used to learn layer-wise prior precisions and affine invariances. Models were trained for 300 epochs using SGD with momentum and Adam for hyperparameters. 3) **Varying Dataset Sizes:** Invariance learning experiments were performed on transformed versions of CIFAR-10 using random subsets ranging from 1,000 to 50,000 data points with ResNets. 4) **Large-Scale Benchmark:** TinyImagenet (100,000 data points, 200 classes) with a ResNet-50 (approx. 23 million parameters) was used to test scalability for prior precision and invariance learning. Performance was evaluated using negative log marginal likelihood, test log likelihood, test accuracy, and runtime, benchmarking against full-batch KFAC-Laplace, Augerino, and neural network partitioning methods.",
        "limitations": "The paper acknowledges several limitations: 1) While the proposed methods significantly improve scalability, using excessively small subset sizes (batch sizes) for stochastic estimators can lead to optimization failures. 2) Theoretically, using constant subset sizes might lead to looser bounds on larger datasets, although experimental results suggest that more frequent gradient updates can compensate for this. 3) Although NTK-based variants often yield tighter theoretical bounds, KFAC-based bounds are sometimes more practically suited for marginal-likelihood optimization due to computational considerations. 4) Traditional full GGN and NTK computations are intractable for deep neural networks with many parameters and data points, necessitating the approximations developed. 5) Existing parametric bounds (Theorem 1) alone do not improve scalability with dataset size, only with parameter partitioning. 6) Structured approximations like KFAC can be non-trivial to compute or extend to custom architectures.",
        "future_research_directions": "Future research directions include: 1) Further improving the tightness of the derived bounds, particularly by developing more optimal partitioning strategies for the Neural Tangent Kernel, potentially by tracking (anti-)correlations between inputs, similar to inducing point optimization. 2) Expanding the application of these efficient stochastic estimators to a broader range of complex and general differentiable hyperparameter optimization problems. 3) Exploring the utility and effectiveness of these fast estimators in the context of large-scale Bayesian linear models.",
        "experimental_code": "# File Path: bound_grid.py\ndef get_marglik_loader(x, y, batch_size, augmenter, grouped_loader):\n    data_factor = len(x) / batch_size\n    DataLoaderCls = GroupedSubsetTensorDataLoader if grouped_loader else SubsetTensorDataLoader\n    marglik_loader = DataLoaderCls(x, y, transform=augmenter, subset_size=batch_size,\n                                   detach=False, data_factor=data_factor)\n    return marglik_loader\n\ndef main(setting, approximation, single_output, grouped_loader, stochastic):\n    # ... initial setup ...\n    X_train, y_train = dataset_to_tensors(train_dataset, subset_indices, device)\n    # ... augmenter setup ...\n\n    # ... initial marglik_optimization (MAP training, not the focus for stochastic bounds here) ...\n\n    ####### Assess bound at converged setting\n    backend_kwargs = dict(differentiable=False, kron_jac=False)\n    la_kwargs = dict(sod=True, single_output=single_output)\n    if approximation == 'kernel' and single_output:\n        la_kwargs['independent'] = True\n\n    if stochastic:\n        batch_sizes = [10, 20, 50, 100, 250, 500, 1000]\n    else:\n        # for parametric no sod bounds\n        batch_sizes = [1000]\n\n    # grid for prior precision or rotation factor\n    grid = np.logspace(-4, 4, 100) if setting == PRIOR else np.linspace(0, np.pi, 100)\n    result_frame = pd.DataFrame(index=batch_sizes, columns=grid)\n    result_frame_sem = pd.DataFrame(index=batch_sizes, columns=grid)\n    for batch_size in batch_sizes:\n        for hparam in grid:\n            set_seed(711)\n            # Create data subset loader for the current batch_size\n            marglik_loader = get_marglik_loader(X_train, y_train, batch_size, augmenter, grouped_loader)\n            marglik_loader = marglik_loader.detach() # Detach from training graph\n\n            # Set hyperparameter (prior precision or rotation factor) for current grid point\n            if setting == INVARIANCE:\n                augmenter.rot_factor.requires_grad = False\n                augmenter.rot_factor.data[2] = float(hparam) # Update rotation factor\n                prior_precision = la.prior_precision # Use converged prior precision\n            else: # setting == PRIOR\n                prior_precision = float(hparam) # Use grid search prior precision\n\n            margliks = list()\n            n_reps = int(subset_size / batch_size) # Number of data subsets to average over\n            for rep in range(n_reps):\n                # Select Laplace approximation class based on 'approximation'\n                if approximation == 'kernel':\n                    lap_cls = FunctionalLaplace\n                elif approximation == 'full':\n                    lap_cls = FullLaplace\n                elif approximation == 'blockdiag':\n                    lap_cls = BlockDiagLaplace\n                elif approximation == 'kron':\n                    lap_cls = KronLaplace\n                elif approximation == 'diag':\n                    lap_cls = DiagLaplace\n                \n                # Initialize Laplace approximation and fit to data subset\n                lap = lap_cls(model, 'classification', prior_precision=prior_precision,\n                              backend=backend, backend_kwargs=backend_kwargs, **la_kwargs)\n                lap.fit(marglik_loader) # Fit to a data subset\n                marglik = lap.log_marginal_likelihood().item() / subset_size # Compute log marginal likelihood\n                margliks.append(marglik)\n            result_frame.loc[batch_size, hparam] = np.mean(margliks)\n            result_frame_sem.loc[batch_size, hparam] = sem(margliks)\n            print(setting, batch_size, hparam, np.mean(margliks), np.nan_to_num(sem(margliks)))\n    # ... saving results ...\n\n# File Path: laplace/baselaplace.py\nclass FunctionalLaplace(BaseLaplace):\n    # ... __init__ ...\n    def _kernel_closure(self, X, y):\n        # Calls the backend's kernel computation based on configuration\n        if self.independent:\n            if self.single_output:\n                if self.single_output_iid:\n                    random_ix = torch.randint(self.n_outputs, (len(y),), device=X.device)\n                else:\n                    random_ix = torch.randint(self.n_outputs, ())\n                return self.backend.single_kernel(X, y, self.prior_precision, output_ix=random_ix)\n            else:\n                return self.backend.indep_kernel(X, y, self.prior_precision)\n        else:\n            return self.backend.kernel(X, y, self.prior_precision_diag, prec=self.prior_precision,\n                                       prec_structure=self.prior_structure)\n\n    def fit_batch(self, X, y, N):\n        # ... setup model/output_size ...\n        self.model.zero_grad()\n        X, y = X.to(self._device), y.to(self._device)\n        loss, H = self._kernel_closure(X, y) # H here is the computed kernel matrix\n        self.loss += loss\n        self.n_data_seen += len(y)\n        self.H.append(H) # Accumulate kernel matrices\n\n    @property\n    def log_det_ratio(self):\n        log_det_ratio = 0\n        for H_kernel in self.H:\n            if self.independent:\n                if self.single_output:\n                    # H_kernel n x n (for single output)\n                    log_det_ratio += self.n_outputs * torch.logdet(\n                        diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                    )\n                else:\n                    # H_kernel c x n x n (for independent outputs)\n                    log_det_ratio += torch.logdet(\n                        batch_diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                    ).sum()\n            else:\n                # H_kernel nc x nc (for full kernel)\n                log_det_ratio += torch.logdet(\n                    diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                )\n        return self.n_data / self.n_data_seen * log_det_ratio # Scale by data factor\n\n# File Path: laplace/curvature/asdl.py\nclass AsdlGGN(AsdlInterface, GGNInterface):\n    # ... __init__ ...\n    def kernel(self, x, y, prec_diag, **kwargs):\n        # Calls linear_network_kernel to compute the kernel matrix\n        f, K = linear_network_kernel(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,\n                                     differentiable=self.differentiable, kron_jac=self.kron_jac)\n        n, c = f.shape\n        K = K.transpose(1, 2).reshape(n*c, n*c) # Reshape to (N*C) x (N*C) for full kernel\n\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n    def indep_kernel(self, x, y, prec_diag):\n        # Calls linear_network_kernel_indep for independent outputs\n        f, K = linear_network_kernel_indep(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,\n                                           differentiable=self.differentiable, kron_jac=self.kron_jac)\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n    def single_kernel(self, x, y, prec_diag, output_ix):\n        # Calls linear_network_kernel_indep for a single output\n        f, K = linear_network_kernel_indep(\n            self._model, x, scale=1 / prec_diag, likelihood=self.likelihood, differentiable=self.differentiable, \n            kron_jac=self.kron_jac, single_output=output_ix\n        )\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n# File Path: dependencies/asdl/asdfghjkl/kernel.py\ndef linear_network_kernel(model, x, scale, likelihood='classification', \n                          differentiable=False, kron_jac=False):\n    # Computes J(x)^T L J(x) where L is the Hessian of the likelihood w.r.t. logits\n    operation_name = OP_BATCH_GRADS_KRON if kron_jac else OP_BATCH_GRADS\n    n_data = x.shape[0]\n    n_params = sum(p.numel() for p in model.parameters())\n\n    with extend(model, operation_name):\n        if x.requires_grad:\n            with disable_param_grad(model):\n                logits = model(x)\n        else:\n            logits = model(x)\n        if logits.ndim > 2: # handle augmented inputs by averaging\n            logits = logits.mean(dim=1)\n        n_data, c = logits.shape\n        j1 = logits.new_zeros(n_data, c, n_params)\n        for k in range(c): # Iterate over output dimensions to get individual Jacobians\n            model.zero_grad()\n            scalar = logits[:, k].sum()\n            if differentiable:\n                scalar.backward(retain_graph=True, create_graph=True)\n            else:\n                scalar.backward(retain_graph=(k < c - 1))\n            j_k = []\n            for module in model.modules():\n                operation = getattr(module, 'operation', None)\n                if operation is None:\n                    continue\n                batch_grads = operation.get_op_results()[operation_name]\n                for g in batch_grads.values():\n                    j_k.append(flatten_after_batch(g))\n            j_k = torch.cat(j_k, dim=1)\n            j1[:, k, :] = j_k\n\n    if likelihood == 'classification':\n        L = logits_hessian_cross_entropy(logits)\n        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale\n    elif likelihood == 'heteroscedastic_regression':\n        L = hessian_heteroscedastic_regression(logits)\n        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale\n    elif likelihood == 'regression':\n        j2 = j1 * scale\n    else:\n        raise ValueError('Invalid likelihood')\n    return logits, torch.einsum('ncp,mdp->nmcd', j1, j2)\n\n\ndef linear_network_kernel_indep(model, x, scale, likelihood='classification', differentiable=False, \n                                kron_jac=False, single_output=None):\n    # Similar to linear_network_kernel but for independent outputs or single output\n    n = x.shape[0]\n\n    module_list = [[module] * (2 if getattr(module, 'bias', None) is not None else 1)\n                   for module in model.modules() if hasattr(module, 'weight')]\n    module_list = [(m, loc) for sublist in module_list\n                   for m, loc in zip(sublist, ['weight', 'bias'])]\n    if len(scale) == 1:\n        scale = [scale] * len(module_list)\n    assert len(scale) == len(module_list), 'Scale should be either scalar or for each weight and bias.'\n    for (module, loc), scalem in zip(module_list, scale):\n        setattr(module, f'{loc}_scale', scalem)\n\n    op_name = OP_GRAM_HADAMARD if kron_jac else OP_GRAM_DIRECT\n    with extend(model, op_name):\n        _zero_kernel(model, n, n)\n        if x.requires_grad:\n            with disable_param_grad(model):\n                outputs = model(x)\n        else:\n            outputs = model(x)\n        if outputs.ndim > 2:\n            outputs = outputs.mean(dim=1)\n        n_classes = outputs.shape[-1]\n        if likelihood == 'classification':\n            if single_output is None:\n                L = logits_diag_hessian_cross_entropy(outputs)\n            else:\n                L = logits_single_hessian_cross_entropy(outputs, single_output)\n        elif likelihood == 'heteroscedastic_regression':\n            if single_output is None:\n                L = hessian_diag_heteroscedastic_regression(outputs)\n            else:\n                L = hessian_single_heteroscedastic_regression(outputs, single_output)\n        else:\n            assert likelihood == 'regression'\n        kernels = []\n        output_range = range(n_classes) if single_output is None else [n_classes]\n        for k in output_range:\n            model.zero_grad()\n            if single_output is None:\n                scalar = outputs[:, k].sum()\n            elif single_output.ndim == 0:\n                scalar = outputs[:, single_output].sum()\n            elif single_output.ndim == 1:\n                scalar = outputs.gather(1, single_output.unsqueeze(-1)).sum()\n            else:\n                raise ValueError('Invalid single_output')\n            scalar.backward(\n                retain_graph=differentiable or (k < n_classes - 1) or (single_output is not None),\n                create_graph=differentiable\n            )\n            if likelihood == 'regression':\n                kernels.append(model.kernel)\n            else:\n                kernels.append(model.kernel * (L if single_output is not None else L[:, k]))\n            _zero_kernel(model, n, n)\n        _clear_kernel(model)\n\n    for (module, loc), scale in zip(module_list, scale):\n        delattr(module, f'{loc}_scale')\n\n    return outputs, kernels[0] if single_output is not None else torch.stack(kernels)",
        "experimental_info": "The experiments focus on deriving and assessing stochastic lower bounds for the linearized Laplace approximation of the marginal likelihood. \n\n**Models:**\n- Primary model: `MiniNet` (a small convolutional neural network).\n- Other models supported: `MLP`, `LeNet`, `WideResNet`, `ResNet` (FixupResnet).\n\n**Datasets:**\n- Main datasets for bound assessment: `mnist` and `mnist_r180` (rotated MNIST).\n- Other supported datasets: `fmnist`, `cifar10`, `cifar100`, `tinyimagenet`, including their rotated, translated, or scaled variants.\n\n**Laplace Approximations and Backends:**\n- **NTK-based (Functional) Lower Bounds:** Implemented using `FunctionalLaplace` with `AsdlGGN` or `AugAsdlGGN` backends.\n- **Parametric Structured Lower Bounds:** Implemented using `FullLaplace`, `BlockDiagLaplace`, `KronLaplace`, `DiagLaplace` with `AsdlGGN` or `AugAsdlGGN` backends.\n- `AsdlEF` and `AugAsdlEF` backends are also available for Empirical Fisher approximations.\n\n**Experimental Settings for Stochastic Bounds (from `bound_grid.py` and `classification_image.py`):**\n- **Device:** 'cuda' (GPU).\n- **Subset Size (`subset_size`):** Total number of data points considered, can be `len(train_dataset)` or `1000` for specific evaluations. Used to determine `n_reps` for averaging stochastic estimates.\n- **Marginal Likelihood Batch Size (`marglik_batch_size`):** For stochastic bounds (`stochastic=True`), batch sizes are varied: `[10, 20, 50, 100, 250, 500, 1000]`. For non-stochastic (parametric) bounds, `batch_sizes = [1000]` is typically used.\n- **Data Subsetting:** `sod=True` (subset of data) is enabled in `la_kwargs` for all evaluated bounds.\n- **Output Partitioning:** `single_output=True` can be enabled for output-wise partitioning (which requires `independent=True` for `FunctionalLaplace`). `single_output_iid` controls whether single outputs are sampled iid or one per batch.\n- **Data Grouping:** `grouped_loader=True` uses `GroupedSubsetTensorDataLoader` (grouping by labels) for creating data subsets, otherwise `SubsetTensorDataLoader` (random subsets).\n- **Prior Precision / Invariance Parameter Grid:** A grid search is performed over the prior precision (for `PRIOR` setting, `logspace(-4, 4, 100)`) or the rotational invariance parameter of `AffineLayer2d` (for `INVARIANCE` setting, `linspace(0, pi, 100)`).\n- **Number of Repetitions (`n_reps`):** Stochastic estimates are averaged over `int(subset_size / batch_size)` repetitions for each batch size and hyperparameter grid point.\n\n**Optimization during initial training (MAP + hyperparameter search) from `marglik_optimization`:**\n- **Epochs (`n_epochs`):** `100` in `bound_grid.py`, `500` in `classification_image.py`.\n- **Learning Rates:** `lr = 1e-3`, `lr_hyp = 1e-1`, `lr_aug = 0.05` (for augmentation parameters).\n- **Learning Rate Minima:** `lr_min = 1e-4`, `lr_hyp_min = 1e-2`, `lr_aug_min = 0.005` for cosine annealing scheduling.\n- **Optimizers:** `Adam` (default for `MiniNet`), `SGD` (for other models, with special handling for Fixup parameters).\n- **Burn-in Epochs (`n_epochs_burnin`):** `10` epochs before marginal likelihood optimization starts.\n- **Hyperparameter Steps (`n_hypersteps`, `n_hypersteps_prior`):** `1` or `2` hyperparameter steps in `bound_grid.py` (can be `100` for stochastic gradients in `classification_image.py`).\n- **Marglik Frequency (`marglik_frequency`):`1` or `5` epochs before re-estimating marginal likelihood.\n- **Stochastic Gradients (`stochastic_grad`):** Set to `True` when using lower bound estimators.\n- **Kronecker Jacobians (`kron_jac`):** Enabled by default (`True`) for efficiency where applicable.\n- **Augmentation:** `AffineLayer2d` is used as an augmenter for `lila` method, and its parameters can be optimized.\n\n**Evaluation:**\n- The primary metric is the `log_marginal_likelihood`."
      }
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf",
        "github_url": "https://github.com/QB3/sparse-ho"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces an efficient implicit differentiation algorithm, without matrix inversion, tailored for hyperparameter optimization of non-smooth Lasso-type models. It addresses the challenge of setting regularization parameters for Lasso-type estimators, outperforming traditional grid-search and existing gradient-based methods which suffer from high memory consumption or numerical instability. Key contributions include showing that forward iterative differentiation of block coordinate descent (BCD) for Lasso-type problems converges linearly to the true gradient once the support is identified. The proposed algorithm (Algorithm 2) decouples the Jacobian computation from the regression coefficients, avoiding the solution of potentially ill-conditioned linear systems and leveraging solution sparsity for high-dimensional data. Experiments demonstrate superior performance on held-out error and Stein Unbiased Risk Estimator (SURE) criteria for both Lasso and weighted Lasso.",
        "methodology": "The hyperparameter optimization problem is framed as a bi-level optimization problem, where an outer loop optimizes a criterion (e.g., hold-out loss, SURE) w.r.t. hyperparameters (λ) and an inner loop computes Lasso-type regression coefficients (β̂(λ)). The core innovation lies in computing the weak Jacobian (∂β̂/∂λ) efficiently. Unlike standard implicit differentiation that relies on optimality conditions for smooth functions and requires solving a p×p linear system, this method leverages the fixed-point iteration property of proximal BCD algorithms for Lasso (which involve soft-thresholding). The authors derive sparse closed-form solutions for the Jacobian of Lasso and weighted Lasso (Proposition 1). The proposed algorithm, termed 'implicit forward iterative differentiation' (Algorithm 2), first computes the regression coefficients and identifies their support, then applies a modified forward differentiation recursion restricted to this support. This decouples computation, avoids large matrix inversions, and guarantees linear convergence of the Jacobian once the support is identified (Proposition 2). Hyperparameters are parametrized as e^λ to handle positivity constraints and scaling. For SURE, a weakly differentiable approximation based on Finite Differences Monte-Carlo (dof_FDMC) is used.",
        "experimental_setup": "The Python code for the proposed methods is open-sourced as 'sparse-ho', with critical BCD loops implemented using Numba. For fair comparison, all methods utilized the same vanilla BCD algorithm (Algorithm 5) for the inner optimization, stopping when the relative change in the cost function fell below a tolerance of 10^-5. Hypergradient-based methods employed a line-search strategy for optimization steps. Initialization for Lasso used λ_init = λ_max - log(10), while weighted Lasso used a pre-solved regularized problem for initialization. Competitors included: Implicit, Forward Iterative Differentiation (F. Iterdiff.), Grid-search, Random-search, Lattice Hyp., and Bayesian optimization. Backward Iterative Differentiation was largely excluded from main benchmarks due to its high computational cost. Experiments were conducted on two primary applications: 1) Held-out loss with Lasso and MCP estimators on real-world datasets: rcv1 (n=20k, p=20k), 20news (n=11k, p=130k), and finance (n=16k, p=1.6M). 2) SURE criterion with Lasso and weighted Lasso on simulated data, measuring normalized Mean Squared Error (MSE). Simulated data used n=100, varying p from 200 to 10,000, with 5 non-zero coefficients and SNR=3. A supplementary experiment explored performance under non-unique inner problem solutions using a specific toy example (n=100, p=10000).",
        "limitations": "The theoretical convergence guarantees for the Jacobian rely on the assumption of a unique solution for the inner Lasso optimization problem, although the algorithm is shown to still perform numerically in pathological cases of non-uniqueness. The current theory does not cover non-convex penalty functions like MCP, despite demonstrating proper numerical behavior in experiments. Traditional implicit differentiation and the baseline implicit method (Algorithm 1) can suffer from slow convergence or numerical instability when solving ill-conditioned linear systems, especially with large support sizes. Backward iterative differentiation is significantly slower and memory-intensive, particularly for problems with many hyperparameters. The overall hyperparameter optimization problem is generally non-convex, meaning convergence to a global minimum is not guaranteed and performance can be sensitive to initialization. The SURE criterion requires prior knowledge of the noise variance.",
        "future_research_directions": "Future work includes extending the theoretical framework to cover non-convex penalty functions, such as the Minimax Concave Penalty (MCP) and Elastic-Net formulations. The authors also suggest leveraging the two-step nature of the proposed algorithm (first solving for coefficients, then Jacobian) to integrate state-of-the-art Lasso solvers that use techniques like active sets or screening rules. Such advanced solvers often introduce discontinuities with respect to hyperparameters, which would pose significant challenges for single-step automatic differentiation approaches, but could potentially be accommodated by the proposed method.",
        "experimental_code": "class ImplicitForward():\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    get_only_jac.n_iter = i\n\n    return dbeta\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n    alpha = np.exp(log_alpha)\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None\n\nclass Lasso(BaseModel):\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        return alphas[mask] * np.sign(dense) @ jac\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y))\n            alpha_max /= X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        return np.clip(log_alpha, self.log_alpha_max - 12,\n                       self.log_alpha_max + np.log(0.9))\n\nclass WeightedLasso(BaseModel):\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alpha[j] * np.sign(beta[j]) / L[j]\n                ddual_var -= np.outer(X[:, j], (dbeta[j, :] - dbeta_old))\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \\\n                    (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz, :] -= np.outer(\n                    Xjs, (dbeta[j, :] - dbeta_old))\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        size_supp = mask.sum()\n        jac_t_v = np.zeros(size_supp)\n        jac_t_v = alphas[mask] * np.sign(dense) * jac\n        return jac_t_v\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y)) / X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        log_alpha = np.clip(log_alpha, self.log_alpha_max - 5,\n                            self.log_alpha_max + np.log(0.9))\n        return log_alpha\n\nclass FiniteDiffMonteCarloSure(BaseCriterion):\n    def __init__(self, sigma, finite_difference_step=None,\n                 random_state=42):\n        self.sigma = sigma\n        self.random_state = random_state\n        self.finite_difference_step = finite_difference_step\n        self.init_delta_epsilon = False\n\n        self.mask0 = None\n        self.dense0 = None\n        self.quantity_to_warm_start = None\n\n        self.mask02 = None\n        self.dense02 = None\n        self.quantity_to_warm_start2 = None\n\n        self.rmse = None\n\n    def _init_delta_epsilon(self, X):\n        if self.finite_difference_step:\n            self.epsilon = self.finite_difference_step\n        else:\n            self.epsilon = 2.0 * self.sigma / (X.shape[0]) ** 0.3\n        rng = check_random_state(self.random_state)\n        self.delta = rng.randn(X.shape[0])\n        self.init_delta_epsilon = True\n\n    def get_val_grad(\n            self, model, X, y, log_alpha, compute_beta_grad, max_iter=1000,\n            tol=1e-3, monitor=None):\n        if not self.init_delta_epsilon:\n            self._init_delta_epsilon(X)\n\n        def v(mask, dense):\n            X_m = X[:, mask]\n            return (2 * X_m.T @ (\n                    X_m @ dense - y -\n                    self.delta * self.sigma ** 2 / self.epsilon))\n\n        def v2(mask, dense):\n            return ((2 * self.sigma ** 2 *\n                     X[:, mask].T @ self.delta / self.epsilon))\n\n        mask, dense, jac_v, quantity_to_warm_start = compute_beta_grad(\n            X, y, log_alpha, model, v,\n            mask0=self.mask0, dense0=self.dense0,\n            quantity_to_warm_start=self.quantity_to_warm_start,\n            max_iter=max_iter, tol=tol, full_jac_v=True)\n        mask2, dense2, jac_v2, quantity_to_warm_start2 = compute_beta_grad(\n            X, y + self.epsilon * self.delta,\n            log_alpha, model, v2, mask0=self.mask02,\n            dense0=self.dense02,\n            quantity_to_warm_start=self.quantity_to_warm_start2,\n            max_iter=max_iter, tol=tol, full_jac_v=True)\n        val = self.get_val_outer(X, y, mask, dense, mask2, dense2)\n        self.mask0 = mask\n        self.dense0 = dense\n        self.quantity_to_warm_start = quantity_to_warm_start\n\n        self.mask02 = mask2\n        self.dense02 = dense2\n        self.quantity_to_warm_start2 = quantity_to_warm_start2\n\n        if jac_v is not None and jac_v2 is not None:\n            grad = jac_v + jac_v2\n        else:\n            grad = None\n        if monitor is not None:\n            monitor(val, grad, mask, dense, alpha=np.exp(log_alpha))\n\n        return val, grad",
        "experimental_info": "The hyperparameter optimization problem is framed as a bi-level optimization. The inner loop computes Lasso-type regression coefficients (or Elastic Net, Sparse Logistic Regression, Weighted Lasso, SVM, SVR, SimplexSVR coefficients) using proximal coordinate descent (BCD) algorithms. The inner solver can be `celer.Lasso`, `sklearn.linear_model.LogisticRegression`, `celer.ElasticNet`, or `lightning.classification.LinearSVC` for the respective models. Inner problem parameters include `max_iter` (e.g., 50-10000) and `tol` (e.g., 1e-3 to 1e-8), and `warm_start=True` is often used.\n\n\n\nThe core innovation is the efficient computation of the weak Jacobian (∂β̂/∂λ) using an 'implicit forward iterative differentiation' algorithm (`sparse_ho.ImplicitForward`). This algorithm first computes the regression coefficients and identifies their support, then applies a modified forward differentiation recursion restricted to this support. This decouples computation and avoids large matrix inversions. Key parameters for Jacobian computation are `tol_jac` (e.g., 1e-3 to 1e-8) and `n_iter_jac` (e.g., 100-1000).\n\n\n\nHyperparameters (λ) are parametrized as `log_alpha` internally, and converted to `alpha = np.exp(log_alpha)` for the models, to handle positivity constraints and scaling. Initial hyperparameter values (`alpha0`) are typically set relative to `alpha_max` (e.g., `alpha_max / 10` or `0.1 * alpha_max`), where `alpha_max` is derived from the data (e.g., `np.max(np.abs(X.T @ y)) / n_samples`). A range of hyperparameters from `alpha_min` (e.g., `alpha_max / 100` or `1e-4 * alpha_max`) to `alpha_max` is explored.\n\n\n\nThe outer loop optimizes a criterion such as held-out loss or SURE. Different optimizers are used for the outer problem:\n- `GradientDescent`: `n_outer` (number of outer iterations, e.g., 10-100), `step_size`, `p_grad_norm` (e.g., 1 to 1.9 for adaptive step size).\n- `LineSearch`: `n_outer` (e.g., 10-30), `tolerance_decrease` (e.g., 'constant', 'exponential') for inner solver tolerance.\n- `Adam`: `n_outer` (e.g., 10), `lr` (learning rate, e.g., 0.11), `beta_1`, `beta_2`.\n\n\n\nVarious criteria are employed:\n- `HeldOutMSE`, `HeldOutLogistic`, `HeldOutSmoothedHinge`: Evaluated on a validation set (`idx_val`) after training on `idx_train`.\n- `CrossVal`: Uses `sklearn.model_selection.KFold` (e.g., 5-fold) for splitting data.\n- `FiniteDiffMonteCarloSure`: A weakly differentiable approximation using Finite Differences Monte-Carlo (dof_FDMC). It requires a `sigma` (noise level) and an optional `finite_difference_step`; otherwise, it uses a power law heuristic (`2.0 * sigma / (X.shape[0]) ** 0.3`).\n\n\n\nExperiments are conducted on datasets such as `rcv1.binary`, `rcv1`, `simu` (synthetic data from `make_classification` or `make_correlated_data`), `mnist`, `usps`, `sector_scale`, `aloi`, and real MEG data. The `sparse_ho.utils.Monitor` class is used to track objective values, computation times, hyperparameter values, gradients, and accuracy metrics (`acc_vals`, `acc_tests`) throughout the optimization process."
      }
    },
    {
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "abstract": "Deep neural networks have seen great success in recent years; however,\ntraining a deep model is often challenging as its performance heavily depends\non the hyper-parameters used. In addition, finding the optimal hyper-parameter\nconfiguration, even with state-of-the-art (SOTA) hyper-parameter optimization\n(HPO) algorithms, can be time-consuming, requiring multiple training runs over\nthe entire dataset for different possible sets of hyper-parameters. Our central\ninsight is that using an informative subset of the dataset for model training\nruns involved in hyper-parameter optimization, allows us to find the optimal\nhyper-parameter configuration significantly faster. In this work, we propose\nAUTOMATA, a gradient-based subset selection framework for hyper-parameter\ntuning. We empirically evaluate the effectiveness of AUTOMATA in\nhyper-parameter tuning through several experiments on real-world datasets in\nthe text, vision, and tabular domains. Our experiments show that using\ngradient-based data subsets for hyper-parameter tuning achieves significantly\nfaster turnaround times and speedups of 3$\\times$-30$\\times$ while achieving\ncomparable performance to the hyper-parameters found using the entire dataset.",
      "full_text": "AUTOMATA : G RADIENT BASED DATA SUBSET SELECTION FOR COMPUTE -EFFICIENT HYPER -PARAMETER TUNING Krishnateja Killamsetty1, Guttu Sai Abhishek 2, Aakriti 2, Alexandre V . Evﬁmievski 3 Lucian Popa3, Ganesh Ramakrishnan 2, Rishabh Iyer 1 1 The University of Texas at Dallas 2Indian Institute of Technology Bombay, India 3 IBM Research {krishnateja.killamsetty, rishabh.iyer}@utdallas.edu {gsaiabhishek, aakriti, ganesh}@cse.iitb.ac.in {evfimi, lpopa}@us.ibm.com March 17, 2022 ABSTRACT Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, ﬁnding the optimal hyper-parameter conﬁguration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset for different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to ﬁnd the optimal hyper-parameter conﬁguration signiﬁcantly faster. In this work, we propose AUTOMATA , a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves signiﬁcantly faster turnaround times and speedups of 3×-30×while achieving comparable performance to the hyper-parameters found using the entire dataset. 1 Introduction In recent years, deep learning systems have found great success in a wide range of tasks, such as object recognition [14], speech recognition [16], and machine translation [1], making people’s lives easier on a daily basis. However, in the quest for near-human performance, more complex and deeper machine learning models trained on increasingly large datasets are being used at the expense of substantial computational costs. Furthermore, deep learning is associated with a signiﬁcantly large number of hyper-parameters such as the learning algorithm, batch size, learning rate, and model conﬁguration parameters (e.g., depth, number of hidden layers, etc.) that need to be tuned. Hence, running extensive hyper-parameter tuning and auto-ml pipelines is becoming increasingly necessary to achieve state-of-the-art models. However, tuning the hyper-parameters requires multiple training runs over the entire datasets (which are signiﬁcantly large nowadays), resulting in staggering compute costs, running times, and, more importantly, CO2 emissions. To give an idea of staggering compute costs, we consider an image classiﬁcation task on a relatively simple CIFAR-10 dataset where a single training run using a relatively simple model class of Residual Networks [15] on a V100 GPU takes around 6 hours. If we perform 1000 training runs (which is not uncommon today) naively using grid search for hyper-parameter tuning, it will take 6000 GPU hours. The resulting CO2 emissions would be between 640 to 1400 arXiv:2203.08212v1  [cs.LG]  15 Mar 2022A PREPRINT - MARCH 17, 2022 kg of CO2 emitted1, which is equivalent to 1600 to 3500 miles of car travel in the US. Similarly, the costs of training state-of-the-art NLP models and vision models on larger datasets like ImageNet are even more staggering [47]2. Figure 1: AUTOMATA ’s performance summary showing speedups, relative test errors, and tuning times on SST2, glue-SST2, CIFAR10, CIFAR100, and CONNECT-4 datasets. We observe that AUTOMATA achieves speedups(and similar energy savings) of 10x - 30x with around 2% performance loss using Hyperband as a sched- uler. Similarly, even when using a more efﬁcient ASHA sched- uler, AUTOMATA achieves a speedup of around 2x-3x with a per- formance loss of 0%-2%. Naive hyper-parameter tuning methods like grid search [ 4] often fail to scale up with the dimensionality of the search space and are computationally expensive. Hence, more efﬁcient and sophisticated Bayesian optimization methods [ 2, 18, 5, 45, 26] have dominated the ﬁeld of hyper-parameter optimization in recent years. Bayesian optimization methods aim to identify good hyper-parameter conﬁg- urations quickly by building a posterior distribution over the search space and by adaptively selecting conﬁgurations based on the proba- bility distribution. More recent methods [48, 49, 10, 26] try to speed up conﬁguration evaluations for efﬁcient hyper-parameter search; these approaches speed up the conﬁguration evaluation by adaptively allocating more resources to promising hyper-parameter conﬁgura- tions while eliminating poor ones quickly. Existing SOTA methods like SHA [19], Hyperband [ 31], ASHA [ 32] use aggressive early- stopping strategies to stop not-so-promising conﬁgurations quickly while allocating more resources to the promising ones. Generally, these resources can be the size of the training set, number of gra- dient descent iterations, training time, etc. Other approaches like [39, 28] try to quickly evaluate a conﬁguration’s performance on a large dataset by evaluating the training runs on small, random subsets; they empirically show that small data subsets could sufﬁce to estimate a conﬁguration’s quality. The past works [39, 28] show that very small data subsets can be effectively used to ﬁnd the best hyper-parameters quickly. However, all these approaches have naively used random training data subsets and did not place much focus on selecting informative subsets instead. Our central insight is that using small informative data subsets allows us to ﬁnd good hyper-parameter conﬁgurations more effectively than random data subsets. In this work, we study the application of the gradient-based subset selection approach for the task of hyper-parameter tuning and automatic machine learning. On that note, the use of gradient-based data subset selection approach in supervised learning setting was explored earlier in GRAD -MATCH [22] where the authors showed that training on gradient based data subsets allows the models to achieve comparable accuracy to full data training while being signiﬁcantly faster. In this work, we empirically study the advantage of using informative gradient-based subset selection algorithms for the hyper-parameter tuning task and study its accuracy when compared to using random subsets and a full dataset. So essentially, we use subsets of data to tune the hyper-parameters. Once we obtain the tuned hyper-parameters, we then train the model (with the obtained hyper-parameters) on the full dataset. The smaller the data subset we use, the more the speed up and energy savings (and hence the decrease in CO2 emissions). In light of all these insights, we propose AUTOMATA , an efﬁcient hyper-parameter tuning framework that combines existing hyper-parameter search and scheduling algorithms with intelligent subset selection. We further empirically show the effectiveness and efﬁciency of AUTOMATA for hyper-parameter tuning, when used with existing hyper-parameter search approaches (more speciﬁcally, TPE [2], Random search [42]), and hyper-parameter tuning schedulers (more speciﬁcally, Hyperband [31], ASHA [32]) on datasets spanning text, image, and tabular domains. 1.1 Related Work Hyper-parameter tuning and auto-ml approaches: A number of algorithms have been proposed for hyper-parameter tuning including grid search3, bayesian algorithms [3], random search [42], etc. Furthermore, a number of scalable toolkits and platforms for hyper-parameter tuning exist like Ray-tune [35]4, H2O automl [30], etc. See [44, 53] for a survey of current approaches and also tricks for hyper-parameter tuning for deep models. The biggest challenges of existing hyper-parameter tuning approaches are a) the large search space and high dimensionality of hyper-parameters and b) the increased training times of training models. Recent work [ 33] has proposed an efﬁcient approach for parallelizing hyper-parameter tuning using Asynchronous Successive Halving Algorithm (ASHA). AUTOMATA is complementary to such approaches and we show that our work can be be combined effectively with them. 1https://mlco2.github.io/impact/#compute 2https://tinyurl.com/a66fexc7 3https://tinyurl.com/3hb2hans 4https://docs.ray.io/en/master/tune/index.html 2A PREPRINT - MARCH 17, 2022 Configurations  comparison Choice of    Hyper-parameters Repeat until all possible sets of hyper- parameters are exhausted DSS based model training loop using the selected configuration DSS based model training loop using the selected configuration DSS based model training loop using the selected configuration Component-1  Hyper-parameter search algorithm Model's Validation set performance  Model's Validation set performance  Model's Validation set performance  Component-3  Hyper-parameter scheduler Initial  parameters     Mini-batch SGD on  for  epochs  Gradient based    DSS Mini-batch SGD on   for  epochs Gradient based    DSS Repeat   times    DSS based model training loop Promoted Configurations Component-2 Figure 2: Diagram of AUTOMATA , including hyper-parameter search, subset based conﬁguration evaluation (where models are trained on subsets of data), and hyper-parameter scheduler. Data Subset Selection: Several recent papers have used submodular functions 5 for data subset selection towards various applications like speech recognition [52, 51], machine translation [25] and computer vision [20]. Other common approaches for subset selection include the usage of coresets. Coresets are weighted subsets of the data, which approximate certain desirable characteristics of the full data (, e.g., the loss function) [11]. Coreset algorithms have been used for several problems including k-means and k-median clustering [13], SVMs [8] and Bayesian inference [6]. Recent coreset selection-based methods [37, 23, 22, 24] have shown great promise for efﬁcient and robust training of deep models. CRAIG [37] tries to select a coreset summary of the training data that estimate the full training gradient closely. Whereas GLISTER [ 23] poses the coreset selection problem as a discrete-continuous bilevel optimization problem that minimizes the validation set loss. Similarly, RETRIEVE [24] also uses a discrete bilevel coreset selection problem to select unlabeled data subsets for efﬁcient semi-supervised learning. Another approach GRAD -MATCH [22] selects coreset summary that approximately matches the full training loss gradient using orthogonal matching pursuit. 1.2 Contributions of the Work The contributions of our work can be summarized as follows: AUTOMATA Framework: We propose AUTOMATA a framework that combines intelligent subset selection with hyper-parameter search and scheduling algorithms to enable faster hyper-parameter tuning. To our knowledge, ours is the ﬁrst work that studies the role of intelligent data subset selection for hyper-parameter tuning. In particular, we seek to answer the following question: Is it possible to use small informative data subsets between 1% to 30% for faster conﬁguration evaluations in hyper-parameter tuning, thereby enabling faster tuning times while maintaining comparable accuracy to tuning hyper-parameters on the full dataset? Effectiveness of A UTOMATA : We empirically demonstrate the effectiveness of AUTOMATA framework used in conjunction with existing hyper-parameter search algorithms like TPE, Random Search, and hyper-parameter scheduling algorithms like Hyperband, ASHA through a set of extensive experiments on multiple real-world datasets. We give a summary of the speedup vs. relative performance achieved by AUTOMATA compared to full data training in Figure 1. More speciﬁcally, AUTOMATA achieves a speedup of 3x - 30x with minimal performance loss for hyper-parameter tuning. Further, in Section 3, we show that the gradient-based subset selection approach of AUTOMATA outperforms the previously considered random subset selection for hyper-parameter tuning. 5Let V = {1, 2, ··· , n}denote a ground set of items. A set function f : 2V →R is a submodular [ 12] if it satisﬁes the diminishing returns property: for subsets S ⊆T ⊆V and j ∈V \\T, f(j|S) ≜ f(S ∪j) −f(S) ≥f(j|T). 3A PREPRINT - MARCH 17, 2022 2 A UTOMATA Framework In this section, we present AUTOMATA , a hyper-parameter tuning framework, and discuss its different components shown in Figure 2. The AUTOMATA framework consists of three components: a hyper-parameter search algorithm that identiﬁes which conﬁguration sets need to be evaluated, a gradient-based subset selection algorithm for training and evaluating each conﬁguration efﬁciently, and a hyper-parameter scheduling algorithm that provides early stopping by eliminating the poor conﬁgurations quickly. With AUTOMATA framework, one can use any of the existing hyper- parameter search and hyper-parameter scheduling algorithms and still achieve signiﬁcant speedups with minimal performance degradation due to faster conﬁguration evaluation using gradient-based subset training. 2.1 Notation Denote by H, the set of conﬁgurations selected by the hyper-parameter search algorithm. Let D= {(xi,yi)}N i=1, denote the set of training examples, and V= {(xj,yj)}M j=1 the validation set. Let θi denote the classiﬁer model parameters trained using the conﬁguration i∈H. Let Si be the subset used for training the ith conﬁguration model θi and wi be its associated weight vector i.e., each data sample in the subset has an associated weight that is used for computing the weighted loss. We superscript the changing variables like model parameters θ, subset Swith the timestep tto denote their speciﬁc values at that timestep. Next, denote by Lj T(θi) =LT(xj,yj,θi), the training loss of the jth data sample in the dataset for ith classiﬁer model, and let LT(θi) =∑ k∈DLk T(θi) be the loss over the entire training set for ith conﬁguration model. Let, Lj T(S,θi) =∑ k∈XLT(xk,yk,θi) be the loss on a subset S⊆V of the training examples at timestep j. Let the validation loss be denoted by LV. 2.2 Component-1: Hyper-parameter Search Algorithm Given a hyper-parameter search space, hyper-parameter search algorithms provide a set of conﬁgurations that need to be evaluated. A naive way of performing the hyper-parameter search is Grid-Search, which deﬁnes the search space as a grid and exhaustively evaluates each grid conﬁguration. However, Grid-Search is a time-consuming process, meaning that thousands to millions of conﬁgurations would need to be evaluated if the hyper-parameter space is large. In order to ﬁnd optimal hyper-parameter settings quickly, Bayesian optimization-based hyper-parameter search algorithms have been developed. To investigate the effectiveness ofAUTOMATA across the spectrum of search algorithms, we used the Random Search method and the Bayesian optimization-based TPE method as representative hyper-parameter search algorithms. We provide more details on Random Search and TPE in Appendix D. 2.3 Component-2: Subset based Conﬁguration Evaluation Earlier, we discussed how a hyper-parameter search algorithm presents a set of potential hyper-parameter conﬁgurations that need to be evaluated when tuning hyper-parameters. Every time a conﬁguration needs to be evaluated, prior work trained the model on the entire dataset until the resource allocated by the hyper-parameter scheduler is exhausted. Rather than using the entire dataset for training, we propose using subsets of informative data selected based on gradients instead. As a result, given any hyper-parameter search algorithm, we can use the data subset selection to speed up each training epoch by a signiﬁcant factor (say 10x), thus improving the overall turnaround time of the hyper-parameter tuning. However, the critical advantage of AUTOMATA is that we can achieve speedups while still retaining the hyper-parameter tuning algorithm’s performance in ﬁnding the best hyper-parameters. The fundamental feature of AUTOMATA is that the subset selected by AUTOMATA changes adaptively over time, based on the classiﬁer model training. Thus, instead of selecting a common subset among all conﬁgurations,AUTOMATA selects the subset that best suits each conﬁguration. We give a detailed overview of the gradient-based subset selection process of AUTOMATA below. 2.3.1 Gradient Based Subset Selection (GSS) The key idea of gradient-based subset selection of AUTOMATA is to select a subset Sand its associated weight vector w such that the weighted subset loss gradient best approximates the entire training loss gradient. The subset selection of AUTOMATA for ith conﬁguration at time step tis as follows: wt i,St i = argmin wt i,St i:|St i|≤k,wt i≥0 ∥ ∑ l∈St i wt il∇θLl T(θt i) −∇θLT(θt i)∥+ λ wt i 2 (1) 4A PREPRINT - MARCH 17, 2022 The additional regularization term prevents assignment of very large weight values to data samples, thereby reducing the possibility of overﬁtting on a few data samples. A similar formulation for subset selection in the context of efﬁcient supervised learning was employed in a recent work called GRAD -MATCH [22]. The authors of the work [22] proved that the optimization problem given in Equation (1) is approximately submodular. Therefore, the above optimization problem can be solved using greedy algorithms with approximation guarantees [9, 36]. Similar to GRAD -MATCH, we use a greedy algorithm called orthogonal matching pursuit (OMP) to solve the above optimization problem. The goal of AUTOMATA is to accelerate the hyper-parameter tuning algorithm while preserving its original performance. Efﬁciency is, therefore, an essential factor that AUTOMATA considers even when selecting subsets. Due to this, we employ a faster per-batch subset selection introduced in the work [22] in our experiments, which is described in the following section. Per-Batch Subset Selection: Instead of selecting a subset of data points, one selects a subset of mini-batches by matching the weighted sum of mini-batch training gradients to the full training loss gradients. Therefore, one will have a subset of slected mini-batches and the associated mini-batch weights. One trains the model on the selected mini-batches by performing mini-batch gradient descent using the weighted mini-batch loss. Let us denote the batch size as B, and the total number of mini-batches as bN = N B, and the training set of mini-batches as DB. Let us denote the number of mini-batches that needs to be selected as bk = k B. Let us denote the subset of mini-batches that needs to be selected as SBi and denote the weights associated with mini-batches as wBi = {wBi1,wBi2 ···wBik}for the ith model conﬁguration. Let us denote the mini-batch gradients as ∇θLBi T (θi),··· ,∇θL BbN T (θi) be the mini-batch gradients for the ith model conﬁguration. Let us denote LB T(θi) =∑ i∈[1,bN] LBk T (θi) be the loss over the entire training set. The subset selection problem of mini-batches at time step tcan be written as follows: wt Bi,St Bi = argmin wt Bi,St Bi:|St Bi|≤bk,wt Bi≥0 ∥ ∑ l∈St Bi wt Bil∇θLBl T (θt i) −∇θLB T(θt i)∥+ λ wt Bi 2 (2) In the per-batch version, because the number of samples required for selection is bk is less than k, the number of greedy iterations required for data subset selection in OMP is reduced, resulting in a speedup of B×. A critical trade-off in using larger batch sizes is that in order to get better speedups, we must also sacriﬁce data subset selection performance. Therefore, it is recommended to use smaller batch sizes for subset selection to get a optimal trade-off between speedups and performance. In our experiments on Image datasets, we use a batch size of B = 20, and on text datasets, we use the batch size as a hyper-parameter with B ∈[16,32,64]. Apart from per-batch selection, we use model warm-starting to get more informative data subsets. Further, in our experiments, we use a regularization coefﬁcient of λ= 0. We give more details on warm-starting below. Warm-starting data selection: We warm-start each conﬁguration model by training on the entire training dataset for a few epochs similar to [22]. The warm-starting process enables the model to have informative loss gradients used for subset selection. To be more speciﬁc, the classiﬁer model is trained on the entire training data for Tw = κTk N epochs, where kis the coreset size, T is the total number of epochs, κis the fraction of warm start, and N is the size of the training dataset. We use a κvalue of 0 (i.e., no warm start) for experiments using Hyperband as scheduling algorithm, and a κvalue of 0.35 for experiments using ASHA. 2.4 Component-3: Hyper-parameter Scheduling Algorithm Hyper-parameter scheduling algorithms improve the overall efﬁciency of the hyper-parameter tuning by terminating some of the poor conﬁgurations runs early. In our experiments, we consider Hyperband [31], and ASHA [32], which are extensions of the Sequential Halving algorithm (SHA) [19] that uses aggressive early stopping to terminate poor conﬁguration runs and allocates an increasingly exponential amount of resources to the better performing conﬁgurations. SHA starts with nnumber of initial conﬁgurations, each assigned with a minimum resource amount r. The SHA algorithm uses a reduction factor η to reduce the number of conﬁgurations in each round by selecting the top 1 η th fraction of conﬁgurations while also increasing the resources allocated to these conﬁgurations by ηtimes each round. We discuss Hyperband and ASHA and the issues within SHA that each of them addresses in more detail in Appendix E. Detailed pseudocode of the AUTOMATA algorithm is provided in Appendix C due to space constraints in the main paper. We use the popular deep learning framework [40] for implementation of AUTOMATA framework, Ray-tune[35] for hyper-parameter search and scheduling algorithms, and CORDS [21] for subset selection strategies. 5A PREPRINT - MARCH 17, 2022 Full Random Automata Craig a) SST5(Random,HB)  b) SST5(TPE,HB)  c) SST5(Random,ASHA)  d) SST5(TPE,ASHA) e) TREC6(Random,HB)  f) TREC6(TPE,HB)  g) TREC6(Random,ASHA)  h) TREC6(TPE,ASHA) i) CIFAR10(Random,HB)  j) CIFAR10(TPE,HB)  k) CIFAR10(Random,ASHA)  l) CIFAR10(TPE,ASHA) m) CIFAR100(Random,HB)  n) CIFAR100(TPE,HB)  o) CIFAR100(Random,ASHA)  p) CIFAR100(TPE,ASHA) q) CONNECT-4(Random,HB)  r) CONNECT-4(TPE,HB)  s) CONNECT-4(Rand,ASHA)  t) CONNECT-4(TPE,ASHA) Figure 3: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-t), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) SST5, (e-h) TREC6, (i-l) CIFAR10, (m-p) CIFAR100, and (q-t) CONNECT-4 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 6A PREPRINT - MARCH 17, 2022 3 Experiments In this section, we present the effectiveness and the efﬁciency of AUTOMATA framework for hyper-parameter tuning by evaluating AUTOMATA on datasets spanning text, image, and tabular domains. Further, to assess AUTOMATA ’s effectiveness across the spectrum of existing hyper-parameter search and scheduling algorithms, we conduct experiments using combinations of different search and scheduling algorithms. As discussed earlier, we employ Random Search [42], TPE [2] as representative hyper-parameter search algorithms, and Hyperband [ 31], ASHA [ 32] as representative hyper-parameter scheduling algorithms. However, we believe the takeaways would remain the same even with other approaches. We repeat each experiment ﬁve times on the text and tabular datasets, thrice on the image datasets, and report the mean accuracy and speedups in the plots. Below, we provide further details on datasets, baselines, models, and the hyper-parameter search space used for experiments. 3.1 Baselines Our experiments aim to demonstrate the consistency and efﬁciency of AUTOMATA more speciﬁcally, the effectiveness of AUTOMATA ’s gradient-based subset selection (GSS) for hyper-parameter tuning. As baselines, we replace the GSS subset selection strategy in AUTOMATA with different subset selection strategies, namely RANDOM (randomly sample a same sized subset as AUTOMATA from the training data), CRAIG [37] (a gradient-based subset selection proposed for efﬁcient supervised learning), and FULL (using the entire training data for model training during conﬁguration evaluation). For ease of notation, we refer to baselines by the names of corresponding subset selection strategies. Note that by CRAIG baseline, we mean the faster per-batch version of CRAIG [37] for subset selection shown [22] to be more efﬁcient than the original. In addition, for all methods, we do not use any warm-start for experiments with Hyperband and use a warm start of κ= 0.35 for experiments with ASHA. We give more details on the reason for using warm-start with ASHA and no warm-start with Hyperband in Appendix F.3. We perform experiments with different subset size fractions of 1%, 5%, 10%, and 30%. In our experiments, we compare our approach’s accuracy and efﬁciency (time/energy) with Full training, Per Batch CRAIG selection, and Random selection. 3.2 Datasets, Model Architecture, and Experimental Setup To demonstrate the effectiveness ofAUTOMATA for hyper-parameter tuning, we performed experiments on datasets spanning text, image and tabular domains. Text datasets include SST2 [46], SST5 [46], glue-SST2 [50], and TREC6 [34, 17]. Image datasets include CIFAR10 [27], CIFAR100 [27], and Street View House Numbers (SVHN) [38]. Tabular datasets include DNA, SATIMAGE, LETTER, and CONNECT-4 fromLIBSVM (a library for Support Vector Machines (SVMs)) [7]. We give more details on dataset sizes and splits in Appendix F.1. For the Text datasets, we use the LSTM model (from PyTorch) with trainable GloVe [41] embeddings of 300 dimension as input. For Image datasets, we use the ResNet18 [15] and ResNet50 [15] models. For Tabular datasets, we use a multi-layer perceptron with 2 hidden layers. Once the best hyper-parameter conﬁguration is found, we perform one more ﬁnal training of the model using the best conﬁguration on the entire dataset and report the achieved test accuracy. We use ﬁnal training for all methods except FULL since the models trained on small data subsets (especially with small subset fractions of 1%, 5%) during tuning do not achieve high test accuracies. We also include the ﬁnal training times while calculating the tuning times for a more fair comparison6 For text datasets, we train the LSTM model for 20 epochs while choosing subsets (except for FULL ) every 5 epochs. The hyper-parameter space includes learning rate, hidden size & number of layers of LSTM, batch size of training. Some experiments (with TPE as the search algorithm) use 27 conﬁgurations in the hyper-parameter space, while others use 54. More details on hyper-parameter search space for text datasets are given in Appendix F.2.1. For image datasets, we train the ResNet [15] model for 300 epochs while choosing subsets (except for FULL ) every 20 epochs. We use a Stochastic Gradient Descent (SGD) optimizer with momentum set to 0.9 and weight decay factor set to 0.0005. The hyper-parameter search space consists of a choice between the Momentum method and Nesterov Accelerated Gradient method, choice of learning rate scheduler and their corresponding parameters, and four different group-wise learning rates. We use 27 conﬁgurations in the hyper-parameter space for Image datasets. More details on hyper-parameter search space for image datasets are given in Appendix F.2.2. For tabular datasets, we train a multi-layer perceptron with 2 hidden layers for 200 epochs while choosing subsets every 10 epochs. The hyper-parameter search space consists of a choice between the SGD optimizer or Adam optimizer, choice of learning rate, choice of learning rate scheduler, the sizes of the two hidden layers and batch size for training. We use 27 conﬁgurations in the hyper-parameter space for Tabular datasets. More details on hyper-parameter search space for tabular datasets are provided in Appendix F.2.3. 6Note that with a 30% subset, ﬁnal training is not required as the models trained with 30% subsets achieve similar accuracy to full data training. However, for the sake of consistency, we useﬁnal training with 30% subsets as well. 7A PREPRINT - MARCH 17, 2022 3.3 Hyper-parameter Tuning Results Results comparing the accuracy vs. efﬁciency tradeoff of different subset selection strategies for hyper-parameter tuning are shown in Figure 3. Performance is compared for different sizes of subsets of training data: 1%, 5%, 10%, and 30% along with four possible combinations of search algorithm (Random or TPE) and scheduling algorithm (ASHA or Hyperband). Text datasets results: Sub-ﬁgures(3a, 3b, 3c, 3d) show the plots of relative test error vs. speed ups, both w.r.tfull data tuning for SST5 dataset with different combinations of search and scheduling methods. Similarly, in sub-ﬁgures(3e, 3f, 3g, 3h) we present the plots of relative test error vs. speed ups for TREC6 dataset. From the results, we observe that AUTOMATA achieves best speed up vs. accuracy tradeoff and consistently gives better performance even with small subset sizes unlike other baselines like RANDOM , CRAIG . In particular, AUTOMATA achieves a speedup of 9.8×and 7.35×with a performance loss of 2.8% and a performance gain of 0.9% respectively on the SST5 dataset with TPE and Hyperband. Additionally, AUTOMATA achieves a speedup of around 3.15×, 2.68×with a performance gain of 3.4%, 4.6% respectively for the TREC6 dataset with TPE and ASHA. Image datasets results: Sub-ﬁgures(3i, 3j, 3k, 3l) show the plots of relative test error vs. speed ups, both w.r.tfull data tuning for CIFAR10 dataset with different combinations of search and scheduling methods. Similarly, sub-ﬁgures (3m, 3n, 3o, 3p) show the plots of relative test error vs. speed ups on CIFAR100. The results show that AUTOMATA achieves the best speed up vs. accuracy tradeoff consistently compared to other baselines. More speciﬁcally, AUTOMATA achieves a speedup of around 15×, 8.7× with a performance loss of 0.65%, 0.14% respectively on the CIFAR10 dataset with Random and Hyperband. Further, AUTOMATA achieves a speedup of around 3.7×, 2.3×with a performance gain of 1%, 2% for CIFAR100 dataset with TPE and ASHA. Tabular datasets results: Sub-ﬁgures(3q, 3r, 3s, 3t) show the plots of relative test errorvs. speed ups for the CONNECT-4 dataset. AUTOMATA consistently achieved better speedup vs. accuracy tradeoff compared to other baselines on CONNECT-4 as well. Owing to space constraints, we provide additional results showing the accuracy vs. efﬁciency tradeoff on additional text, image, and tabular datasets in the Appendix F.4. It is important to note that AUTOMATA obtains better speedups when used for hyper-parameter tuning on larger datasets and larger models (in terms of parameters). Apart from the speedups achieved by AUTOMATA , we show in Appendix F.5 that it also achieves similar reductions of energy consumption and CO2 emissions, thereby making it more environmentally friendly. 4 Conclusion, Limitations, and Broader Impact We introduce AUTOMATA , an efﬁcient hyper-parameter tuning framework that uses intelligent subset selection for model training for faster conﬁguration evaluations. Further, we perform extensive experiments showing the effectiveness of AUTOMATA for Hyper-parameter tuning. In particular, it achieves speedups of around 10×- 15×using Hyperband as scheduler and speedups of around 3×even with a more efﬁcient ASHA scheduler. AUTOMATA signiﬁcantly decreases CO2 emissions by making hyper-parameter tuning fast and energy-efﬁcient, in turn reducing environmental impact of such hyper-parameter tuning on society at large. We hope that the AUTOMATA framework will popularize the trend of using subset selection for hyper-parameter tuning and encourage further research on efﬁcient subset selection approaches for faster hyper-parameter tuning, helping us move closer to the goal of Green AI [43]. One of the limitations of AUTOMATA is that in scenarios in which no performance loss is desired, we do not know the minimum subset size to improve speed and, therefore, rely on larger subset sizes such as 10%, 30%. In the future, we consider adaptively changing subset sizes based on model performance for each conﬁguration to remove the dependency on subset size. References [1] L. Barrault, O. Bojar, M. R. Costa-jussà, C. Federmann, M. Fishel, Y . Graham, B. Haddow, M. Huck, P. Koehn, S. Malmasi, C. Monz, M. Müller, S. Pal, M. Post, and M. Zampieri. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1–61, Florence, Italy, Aug. 2019. Association for Computational Linguistics. [2] J. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. [3] J. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In25th annual conference on neural information processing systems (NIPS 2011), volume 24. Neural Information Processing Systems Foundation, 2011. [4] J. Bergstra and Y . Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(10):281–305, 2012. [5] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th 8A PREPRINT - MARCH 17, 2022 International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 115–123, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. [6] T. Campbell and T. Broderick. Bayesian coreset construction via greedy iterative geodesic ascent. InInternational Conference on Machine Learning, pages 698–706, 2018. [7] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/ libsvm. [8] K. L. Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):1–30, 2010. [9] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection. arXiv preprint arXiv:1102.3975, 2011. [10] T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the 24th International Conference on Artiﬁcial Intelligence, IJCAI’15, page 3460–3468. AAAI Press, 2015. [11] D. Feldman. Core-sets: Updated survey. In Sampling Techniques for Supervised or Unsupervised Tasks, pages 23–44. Springer, 2020. [12] S. Fujishige. Submodular functions and optimization. Elsevier, 2005. [13] S. Har-Peled and S. Mazumdar. On coresets for k-means and k-median clustering. InProceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291–300, 2004. [14] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026–1034, 2015. [15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. [16] J. R. Hershey, S. J. Rennie, P. A. Olsen, and T. T. Kristjansson. Super-human multi-talker speech recognition: A graphical modeling approach. Comput. Speech Lang., 24(1):45–66, Jan. 2010. [17] E. Hovy, L. Gerber, U. Hermjakob, C.-Y . Lin, and D. Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. [18] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In C. A. C. Coello, editor,Learning and Intelligent Optimization, pages 507–523, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. [19] K. Jamieson and A. Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter optimization. In A. Gretton and C. C. Robert, editors, Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 240–248, Cadiz, Spain, 09–11 May 2016. PMLR. [20] V . Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor, and G. Ramakrishnan. Learning from less data: A uniﬁed data subset selection and active learning framework for computer vision. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1289–1299. IEEE, 2019. [21] K. Killamsetty, D. Bhat, G. Ramakrishnan, and R. Iyer. CORDS: COResets and Data Subset selection for Efﬁcient Learning, March 2021. [22] K. Killamsetty, D. Sivasubramanian, G. Ramakrishnan, A. De, and R. Iyer. Grad-match: Gradient matching based data subset selection for efﬁcient deep model training. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5464–5474. PMLR, 18–24 Jul 2021. [23] K. Killamsetty, D. Sivasubramanian, G. Ramakrishnan, and R. Iyer. Glister: Generalization based data subset selection for efﬁcient and robust learning. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 35(9):8110–8118, May 2021. [24] K. Killamsetty, X. Zhao, F. Chen, and R. K. Iyer. RETRIEVE: Coreset selection for efﬁcient and robust semi- supervised learning. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. W. Vaughan, editors,Advances in Neural Information Processing Systems, 2021. [25] K. Kirchhoff and J. Bilmes. Submodularity for data selection in machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131–141, 2014. 9A PREPRINT - MARCH 17, 2022 [26] A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter. Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets. In A. Singh and J. Zhu, editors, Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 528–536. PMLR, 20–22 Apr 2017. [27] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. [28] T. Krueger, D. Panknin, and M. Braun. Fast cross-validation via sequential testing. J. Mach. Learn. Res. , 16(1):1103–1155, jan 2015. [29] A. Lacoste, A. Luccioni, V . Schmidt, and T. Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. [30] E. LeDell and S. Poirier. H2o automl: Scalable automatic machine learning. In Proceedings of the AutoML Workshop at ICML, volume 2020, 2020. [31] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18(1):6765–6816, jan 2017. [32] L. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, J. Ben-tzur, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. In I. Dhillon, D. Papailiopoulos, and V . Sze, editors,Proceedings of Machine Learning and Systems, volume 2, pages 230–246, 2020. [33] L. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 2018. [34] X. Li and D. Roth. Learning question classiﬁers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [35] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, and I. Stoica. Tune: A research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118, 2018. [36] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization techniques, pages 234–243. Springer, 1978. [37] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efﬁcient training of machine learning models, 2020. [38] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng. Reading digits in natural images with unsupervised feature learning. 2011. [39] T. Nickson, M. A. Osborne, S. Reece, and S. J. Roberts. Automated machine learning on big data using stochastic algorithm tuning, 2014. [40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. [41] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [42] N. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Computational Biology, 5, 2009. [43] R. Schwartz, J. Dodge, N. Smith, and O. Etzioni. Green ai. Communications of the ACM, 63:54 – 63, 2020. [44] L. N. Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [45] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. M. A. Patwary, P. Prabhat, and R. P. Adams. Scalable bayesian optimization using deep neural networks. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 2171–2180. JMLR.org, 2015. [46] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, Oct. 2013. Association for Computational Linguistics. [47] E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019. 10A PREPRINT - MARCH 17, 2022 [48] K. Swersky, J. Snoek, and R. P. Adams. Multi-task bayesian optimization. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. [49] K. Swersky, J. Snoek, and R. P. Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. [50] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR. [51] K. Wei, Y . Liu, K. Kirchhoff, C. Bartels, and J. Bilmes. Submodular subset selection for large-scale speech training data. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3311–3315. IEEE, 2014. [52] K. Wei, Y . Liu, K. Kirchhoff, and J. Bilmes. Unsupervised submodular subset selection for speech data. In2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4107–4111. IEEE, 2014. [53] T. Yu and H. Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. 11A PREPRINT - MARCH 17, 2022 Supplementary Material A Code The code of AUTOMATA is available at the following link: https://github.com/decile-team/cords. B Licenses We release the code repository ofAUTOMATA with MIT license, and it is available for everybody to use freely. We use the popular deep learning framework [40] for implementation of AUTOMATA framework, Ray-tune[35] for hyper-parameter search and scheduling algorithms, and CORDS [21] for subset selection strategies. As far as the datasets are considered, we use SST2 [46], SST5 [46], glue-SST2 [50], TREC6 [34, 17], CIFAR10 [27], SVHN [38], CIFAR100 [27], and DNA, SATIMAGE, LETTER, CONNECT-4 fromLIBSVM (a library for Support Vector Machines (SVMs)) [7] datasets. CIFAR10, CIFAR100 datasets are released with an MIT license. SVHN dataset is released with a CC0:Public Domain license. Furthermore, all the datasets used in this work are publicly available. In addition, the datasets used do not contain any personally identiﬁable information. 12A PREPRINT - MARCH 17, 2022 C A UTOMATA Algorithm Pseudocode We give the pseudo code of AUTOMATA algorithm in Algorithm 1. Algorithm 1: AUTOMATA Algorithm Input: Hyper-parameter scheduler Algorithm: scheduler , Hyper-parameter search Algorithm: search , No. of conﬁguration evaluations: n, Hyper-parameter search space: H, Training dataset: D, Validation dataset: V, Total no of epochs: T, Epoch interval for subset selection: R, Size of the coreset: k, Reg. Coefﬁcient: λ, Learning rates: {αt}t=T−1 t=0 , Tolerance: ϵ Generate nconﬁgurations by calling the search algorithm H = {h1,h2,··· ,hn}= search (H,n) Randomly initialize each conﬁguration model parameters h1.θ= h2.θ= ··· = hn.θ= θ Set h1.t= h2.t= ··· = hn.t= 0; Set h1.eval= h2.eval= ··· = hn.eval= 0; Assign the initial resources(i.e., in our case training epochs) using the scheduler for all initialized conﬁgurations {hi.r}i=n i=1 = scheduler (H,T) repeat ***Evaluate all remaining conﬁgurations*** for each conﬁguration numbered iin Hdo ***Train conﬁguration hiusing informative data subsets for hi.repochs and evaluate on validation set*** hi.eval,hi.theta= subset-conﬁg-evaluation (D,V,hi.theta,hi.r,R,k,λ, {αt} t=hi.r t=0 ,ϵ) hi.t= hi.t+ hi.r ***Assign resources again based on evaluation performance*** {hi.r}i=n i=1 = scheduler (H,T) until until h1.r== 0 & h2.r== 0 & ··· hn.r== 0 ***Get the best performing hyper-parameters based on ﬁnal conﬁguration evaluations*** finalconfig = argmax hi.config [hi.eval]n i=1 ***Perform ﬁnal training using the best hyper-parameter conﬁgurations*** θfinal = ﬁnaltrain (θ,D,finalconfig,T ) return θfinal Algorithm 2: subset-conﬁg-evaluation Input: Training dataset: D, Validation dataset: V, Initial model parameters: θ0, Total no of epochs: T, Epoch interval for subset selection: R, Size of the coreset: k, Reg. Coefﬁcient: λ, Learning rates: {αt}t=T−1 t=0 , Tolerance: ϵ Set t= 0; Randomly initialize coreset S0 ⊆D : |S0|= k; repeat if (t%R== 0) ∧(t> 0) then St= OMP(D,θt,λ,α t,k,ϵ ) else St = St−1 Compute batches Db = ((xb,yb); b∈(1 ··· B)) from D Compute batches Stb = ((xb); b∈(1 ··· B)) from S *** Mini-batch SGD *** Set θt0 = θt for b= 1 to Bdo Compute mask mton Stbfrom current model parameters θt(b−1) θtb = θt(b−1) −αt∇θLS(Db,θt) −αtλt ∑ j∈Stb mjt∇θlu(xj,θt(b−1)) Set θt+1 = θtB t= t+ 1 until until t≥T *** Evaluate trained model on validation set *** eval= evaluate (θT,V) return eval,θT Algorithm 3: OMP Input: Training loss LT, current parameters: θ, regularization coefﬁcient: λ, subset size: k, tolerance: ϵ Initialize S= ∅ r←∇w(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 |)w=0 repeat e= argmaxj|rj| S←S∪{ e} w ←argminw(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2) r←∇w(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 |) until until |S|≤ kand ∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 ≥ϵ return S,w D More details on Hyper-parameter Search Algorithms We give a brief overview of few representative hyper-parameter search algorithms, such as TPE [ 2] and Random Search [42] which we used in our experiments. As discussed earlier, given a hyper-parameter search space, hyper- parameter search algorithms provide a set of conﬁgurations that need to be evaluated. A naive way of performing the 13A PREPRINT - MARCH 17, 2022 hyper-parameter search is Grid Search, which deﬁnes the search space as a grid and exhaustively evaluates each grid conﬁguration. However, Grid Search is a time-consuming process, meaning that thousands to millions of conﬁgurations would need to be evaluated if the hyper-parameter space is large. In order to ﬁnd optimal hyper-parameter settings quickly, Bayesian optimization-based hyper-parameter search algorithms have been developed. To investigate the effectiveness of AUTOMATA across the spectrum of search algorithms, we used the Random Search method and the Bayesian optimization-based TPE method (described below) as representative hyper-parameter search algorithms. D.1 Random Search In random search [42], hyper-parameter conﬁgurations are selected at random and evaluated to discover the optimal conﬁguration among those chosen. As well as being more efﬁcient than a grid search since it does not evaluate all possible conﬁgurations exhaustively, random search also reduces overﬁtting [4]. D.2 Tree Parzen Structured Estimator (TPE) TPE [2] is a sequential model-based optimization (SMBO) approach that sequentially constructs a probability model to approximate the performance of hyper-parameters based on historical conﬁguration evaluations and then subsequently uses the model to select new conﬁgurations. TPE models the likelihood function P(D|f) and the prior over the function space P(f) using the kernel density estimation. TPE algorithm sorts the collected observations by the function evaluation value, typically validation set performance, and divides them into two groups based on some quantile. The ﬁrst group x1 contains best-performing observations, and the second group x2 contains all other observations. Then TPE models two different densities i(x1) and g(x2) based on the observations from the respective groups using kernel density estimation. Finally, TPE selects the subset observations that need to be evaluated by sampling from the distribution that models the maximum expected improvement, i.e., E[i(x)/g(x)]. E More details on Hyper-parameter Scheduling Algorithms We give a brief overview of some representative hyper-parameter scheduling algorithms, such as HyperBand [31] and ASHA [32] which we used in our experiments. As discussed earlier, hyper-parameter scheduling algorithms improve the overall efﬁciency of the hyper-parameter tuning by terminating some of the poor conﬁgurations runs early. In our experiments, we consider Hyperband, and ASHA, which are extensions of the Sequential Halving algorithm (SHA) [19] that uses aggressive early stopping to terminate poor conﬁguration runs and allocates an increasingly exponential amount of resources to the better performing conﬁgurations. SHA starts with nnumber of initial conﬁgurations, each assigned with a minimum resource amount r. The SHA algorithm uses a reduction factor η to reduce the number of conﬁgurations each round by selecting the top 1 η th fraction of conﬁgurations while also increasing the resources allocated to these conﬁgurations by ηtimes each round. Following, we will discuss Hyperband and ASHA and the issues within SHA that each of them addresses. E.1 HyperBand One of the issues with SHA is that its performance largely depends on the initial number nof conﬁgurations. Hyper- band [31] addresses this issue by performing a grid search over various feasible values of n. Further, each value of nis associated with a minimum resource rallocated to all conﬁgurations before some are terminated; larger values of nare assigned smaller rand hence more aggressive early-stopping. On the whole, in Hyperband [31] for different values of nand r, the SHA algorithm is run until completion. E.2 ASHA: One of the other issues with SHA is that the algorithm is sequential and has to wait for all the processes (assigned with an equal amount of resources) at a particular bracket to be completed before choosing the conﬁgurations to be selected for subsequent runs. Hence, due to the sequential nature of SHA, some GPU/CPU resources (with no processes running) cannot be effectively utilized in the distributed training setting, thereby taking more time for tuning. By contrast, ASHA [32] is an asynchronous variant of SHA and addresses the sequential issue of SHA by promoting a conﬁguration to the next rung as long as there are GPU or CPU resources available. If no resources appear to be promotable, it randomly adds a new conﬁguration to the base rung. 14A PREPRINT - MARCH 17, 2022 F More Experimental Details and Additional Results We performed experiments on a mix of RTX 1080, RTX 2080, and V100 GPU servers containing 2-8 GPUs. To be fair in timing computation, we ran AUTOMATA and all other baselines for a particular setting on the same GPU server. F.1 Additional Datasets Details F.1.1 Text Datasets We performed experiments on SST2 [46], SST5 [46], glue-SST2 [50], and TREC6 [34, 17] text datasets. SST2 [46] and glue-SST2 [50] dataset classify the sentiment of the sentence (movie reviews) as negative or positive. Whereas SST5 [46] classify sentiment of sentence as negative, somewhat negative, neutral, somewhat positive or positive. TREC6 [34, 17] is a dataset for question classiﬁcation consisting of open-domain, fact-based questions divided into broad semantic categories(ABBR - Abbreviation, DESC - Description and abstract concepts, ENTY - Entities, HUM - Human beings, LOC - Locations, NYM - Numeric values). The train, text and validation splits for SST2 [ 46] and SST5 [46] are used from the source itself while the validation data for TREC6 [34, 17] is obtained using 10% of the train data. The train and validation data for glue-SST2 [50] is used from source itself. In Table 1, we summarize the number classes, and number of instances in each split in the text datasets. Dataset #Classes #Train #Validation #Test SST2 2 8544 1101 2210 SST5 5 8544 1101 2210 glue-SST2 2 63982 872 3367 TREC6 6 4907 545 500 Table 1: Number of classes, Number of instances in Train, Validation and Test split in Text datasets F.1.2 Vision Datasets We performed experiments on CIFAR10 [27], CIFAR100 [27], and SVHN [38] vision datasets. The CIFAR-10 [27] dataset contains 60,000 colored images of size 32×32 divided into ten classes, each with 6000 images. CIFAR100 [27] is also similar but that it has 600 images per class and 100 classes. Both CIFAR10 [ 27] and CIFAR100 [27] have 50,000 training samples and 10,000 test samples distributed equally across all classes. SVHN [38] is obtained from house numbers in Google Street View images and has 10 classes, one for each digit. The colored images of size 32×32 are centered around a single digit with some distracting characters on the side. SVHN [38] has 73,257 training digits, 26,032 testing digits. For all 3 datasets, 10% of the training data is used for validation. In Table 2, we summarize the number classes, and number of instances in each split in the image datasets. Dataset #Classes #Train #Validation #Test CIFAR10 10 45000 5000 10000 CIFAR100 100 45000 5000 10000 SVHN 10 65932 7325 26032 Table 2: Number of classes, Number of instances in Train, Validation and Test split in Image datasets F.1.3 Tabular Datasets We performed experiments on the following tabular datasetsdna, letter, connect-4, and satimage from LIBSVM (a library for Support Vector Machines (SVMs)) [7]. Name #Classes #Train #Validation #Test #Features dna 3 1,400 600 1,186 180 satimage 6 3,104 1,331 2,000 36 letter 26 10,500 4,500 5,000 16 connect_4 3 67,557 - - 126 Table 3: Number of classes, Number of instances in Train, Validation and Test split in Tabular datasets 15A PREPRINT - MARCH 17, 2022 A brief description of the tabular datasets can be found in Table 3. For datasets without explicit validation and test datasets, 10% and 20% samples from the training set are used as validation and test datasets, respectively. F.2 Additional Experimental Details For tuning with FULL datasets, the entire dataset is used to train the model during hyper-parameter tuning. But when the AUTOMATA (or CRAIG ) is used, only a fraction of the dataset is used to train various models during tuning. Similar is the case with Random subset selection approach but the subsets are chosen at RANDOM . Note that subset selection techniques used are adaptive in nature, which mean that they chose subset every few epochs for the model to train on for coming few epochs. F.2.1 Details of Text Experiments The hyper-parameter space for experiments on text datasets include learning rate, hidden size & number of layers of LSTM and batch size of training. Some experiments (with TPE search algorithm) where the best conﬁguration among 27 conﬁgurations are found, the hyper-parameter space is learning rate: [0.001,0.1], LSTM hidden size: {64,128,256}, batch size: {16,32,64}. While the rest of the experiments where the best conﬁguration among 54 conﬁgurations are found, the hyper-parameter space is learning rate: [0.001,0.1], LSTM hidden size: {64,128,256}, number of layers in LSTM: {1, 2}, batch size: {16,32,64}. F.2.2 Details of Image Experiments The hyper-parameter search space for tuning experiments on image datasets include a choice between Momentum method and Nesterov Accelerated Gradient method, choice of learning rate scheduler and their corresponding parameters, and four different group-wise learning rates, lr1 for layers of the ﬁrst group, lr2 for layers of intermediate groups, lr3 for layers of the last group of ResNet model, and lr4 for the ﬁnal fully connected layer. For learning rate scheduler, we change the learning rates during training using either a cosine annealing schedule or decay it linearly by γafter every 20 epochs. Best conﬁguration for most experiments is selected from 27 conﬁgurations where the hyper-parameter space is lr1: [0.001, 0.01], lr2: [0.001, 0.01], lr3: [0.001, 0.01], lr4: [0.001, 0.01], Nesterov: {True, False}, learning rate scheduler: {Cosine Annealing, Linear Decay}, γ: [0.05, 0.5]. F.2.3 Details of Tabular Experiments The hyper-parameter search space consists of a choice between the Stochastic Gradient Descent(SGD) optimizer or Adam optimizer, choice of learning rate lr, choice of learning rate scheduler, the sizes of the two hidden layers h1 and h2 and batch size for training. For learning rate scheduler, we either don’t use a learning rate scheduler or change the learning rates during training using a cosine annealing schedule or decay it linearly by 0.05 after every 20 epochs. Best conﬁguration for most experiments is selected from 27 conﬁgurations where the hyper-parameter space is lr: [0.001, 0.01], Optimizer: {Adam, SGD}, learning rate scheduler: {None, Cosine Annealing, Linear Decay}, h1: {150, 200, 250, 300}, h2: {150, 200, 250, 300} and batch size: {16,32,64}. F.3 Use of Warm-start for subset selection We use warm-starting with ASHA as a scheduler because the initial bracket occurs early (i.e., att= 1) with ASHA; this implies that some of the initial conﬁguration evaluations are discarded made just after training for one epoch. The training of such conﬁgurations on small data subsets may not be sufﬁcient to make a sound decision about better- performing conﬁgurations in these scenarios. As a solution, we use warm-starting with ASHA so that all conﬁgurations are trained on the entire data for an initial few epochs. With Hyperband, the brackets do not occur very early during training, so no warm-up is necessary. F.4 More Hyper-parameter Tuning Results We present more hyper-parameter tuning results of AUTOMATA on additional text, image, and tabular datasets in Figures 4,5,6. From the results, it is evident that AUTOMATA achieves best speedup vs. accuracy tradeoff in almost all of the cases. 16A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) SST2(Random,HB)  (b) SST2(TPE,HB)  (c) SST2(Random,ASHA)  (d) SST2(TPE,ASHA) (e) glue-SST2(Random,HB)  (f) glue-SST2(TPE,HB)  (g) glue-SST2(Random,ASHA)  (h) glue-SST2(TPE,ASHA) (i) SST5(Random,HB)  (j) SST5(TPE,HB)  (k) SST5(Random,ASHA)  (l) SST5(TPE,ASHA) (m) TREC6(Random,HB)  (n) TREC6(TPE,HB)  (o) TREC6(Random,ASHA)  (p) TREC6(TPE,ASHA) Figure 4: Tuning Results on Text Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-p), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) SST2, (e-h) glue-SST2, (i-l) SST5, (m-p) TREC6 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). F.5 CO2 Emissions and Energy Consumption Results Sub-ﬁgures 7a,7b,7c,7d shows the energy efﬁciency plot of AUTOMATA on CIFAR100 dataset for 1%, 5%, 10%, 30% subset fractions. For calculating the energy consumed by the GPU/CPU cores, we use pyJoules 7. From the plot, it is evident that AUTOMATA is more energy efﬁcient compared to the other baselines and full data tuning. Sub- ﬁgures 7e,7f,7g,7h shows the plot of relative error vs CO2 emissions efﬁciency, both w.r.t full training. CO2 emissions were estimated based on the total compute time using the Machine Learning Impact calculator presented in [29]. From the results, it is evident that AUTOMATA achieved the best energy vs. accuracy tradeoff and is environmentally friendly based on CO2 emissions compared to other baselines (including CRAIG and RANDOM ). 7https://pypi.org/project/pyJoules/. 17A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) CIFAR10(Random,HB)  (b) CIFAR10(TPE,HB)  (c) CIFAR10(Random,ASHA)  (d) CIFAR10(TPE,ASHA) (e) CIFAR100(Random,HB)  (f) CIFAR100(TPE,HB)  (g) CIFAR100(Random,ASHA)  (h) CIFAR100(TPE,ASHA) (i) SVHN(Random,HB)  (j) SVHN(TPE,HB)  (k) SVHN(Random,ASHA)  (l) SVHN(TPE,ASHA) Figure 5: Tuning Results on Image Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-l), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) CIFAR10, (e-h) CIFAR100, (i-l) SVHN datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 18A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) DNA(Random,HB)  (b) DNA(TPE,HB)  (c) DNA(Random,ASHA)  (d) DNA(TPE,ASHA) (e) SATIMAGE(Random,HB)  (f) SATIMAGE(TPE,HB)  (g) SATIMAGE(Random,ASHA)  (h) SATIMAGE(TPE,ASHA) (i) LETTER(Random,HB)  (j) LETTER(TPE,HB)  (k) LETTER(Random,ASHA)  (l) LETTER(TPE,ASHA) (m) CONNECT-4(Random,HB)  (n) CONNECT-4(TPE,HB)  (o) CONNECT-4(Random,ASHA)  (p) CONNECT-4(TPE,ASHA) Figure 6: Tuning Results on Tabular Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-p), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) DNA, (e-h) SATIMAGE, (i-l) LETTER, (m-p) CONNECT-4 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 19A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) CIFAR100(Random,HB)  (b) CIFAR100(TPE,HB)  (c) CIFAR100(Random,ASHA)  (d) CIFAR100(TPE,ASHA) (e) SVHN(Random,HB)  (f) SVHN(TPE,HB)  (g) SVHN(Random,ASHA)  (h) SVHN(TPE,ASHA) Figure 7: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-d), we present energy ratio vs. relative test error (in %), compared to Full data tuning for different methods on CIFAR100 dataset. In sub-ﬁgures (e-h), we present co2 emissions ratio vs. relative test error (in %), compared to Full data tuning for different methods on SVHN dataset. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. The scatter plots show that AUTOMATA achieves the best energy savings and CO2 reductions, thereby achieving the best efﬁciency vs. performance tradeoff in almost every case. (Bottom-right corner of each plot indicates the best efﬁciency vs. performance tradeoff region). 20",
      "meta_data": {
        "arxiv_id": "2203.08212v1",
        "authors": [
          "Krishnateja Killamsetty",
          "Guttu Sai Abhishek",
          "Aakriti",
          "Alexandre V. Evfimievski",
          "Lucian Popa",
          "Ganesh Ramakrishnan",
          "Rishabh Iyer"
        ],
        "published_date": "2022-03-15T19:25:01Z",
        "pdf_url": "https://arxiv.org/pdf/2203.08212v1.pdf",
        "github_url": "https://github.com/decile-team/cords"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of computationally expensive and time-consuming hyper-parameter tuning for deep neural networks. It proposes AUTOMATA, a gradient-based data subset selection framework for hyper-parameter tuning, arguing that informative data subsets can significantly accelerate the process. The main contributions include the AUTOMATA framework itself, which combines intelligent subset selection with existing HPO search and scheduling algorithms, and empirical demonstrations of its effectiveness. AUTOMATA achieves speedups of 3x-30x with comparable performance (0%-2% performance loss) compared to tuning on the entire dataset, and it significantly reduces energy consumption and CO2 emissions, outperforming random subset selection and CRAIG.",
        "methodology": "AUTOMATA integrates three core components: a hyper-parameter search algorithm (e.g., Random Search, TPE) to identify configurations, a gradient-based subset selection (GSS) algorithm for efficient configuration evaluation, and a hyper-parameter scheduling algorithm (e.g., Hyperband, ASHA) for early stopping. The GSS method selects a subset and associated weights such that the weighted subset loss gradient best approximates the entire training loss gradient, formulated as an optimization problem and solved using a greedy algorithm like Orthogonal Matching Pursuit (OMP). A 'per-batch' variant of GSS is used for efficiency, selecting subsets of mini-batches. Additionally, 'warm-starting' involves initial training on the entire dataset for a few epochs to obtain informative loss gradients, particularly when using the ASHA scheduler.",
        "experimental_setup": "The framework was evaluated on real-world datasets across text (SST2, SST5, glue-SST2, TREC6), image (CIFAR10, CIFAR100, SVHN), and tabular (DNA, SATIMAGE, LETTER, CONNECT-4 from LIBSVM) domains. Models used were LSTM for text, ResNet18/ResNet50 for image, and a multi-layer perceptron for tabular data. Baselines included RANDOM (random subset selection), CRAIG (another gradient-based subset selection), and FULL (using the entire dataset). Experiments were conducted with different combinations of hyper-parameter search (Random Search, TPE) and scheduling algorithms (Hyperband, ASHA). Subset sizes of 1%, 5%, 10%, and 30% were tested. Evaluation focused on speedup vs. relative test error, energy consumption, and CO2 emissions. Experiments were repeated five times for text/tabular datasets and three times for image datasets. Final training of the best hyper-parameter configuration was performed on the full dataset for all methods (except FULL), with training times included in total tuning times. PyTorch, Ray-tune, and CORDS were used for implementation.",
        "limitations": "One limitation is the uncertainty regarding the minimum subset size required to achieve desired speed improvements without performance loss, often necessitating the use of larger subset sizes like 10% or 30% when no performance degradation is acceptable.",
        "future_research_directions": "Future research directions include adaptively changing subset sizes based on model performance for each configuration, aiming to remove the dependency on a fixed subset size. The authors also hope that the AUTOMATA framework will popularize the trend of using subset selection for hyper-parameter tuning and encourage further research into efficient subset selection approaches to advance 'Green AI'.",
        "experimental_code": "from ray import tune\n\nconfig = dict(setting= \"hyperparamtuning\",\n\n# parameter for subset selection\n# all settings for subset selection will be fetched from here\nsubset_config = \"configs/SL/config_gradmatchpb_glove_sst2.py\",\n\n# parameters for hyper-parameter tuning\n# search space for hyper-parameter tuning\nspace = dict(learning_rate=tune.uniform(0.001, 0.1), \n        # optimizer= tune.choice(['sgd', 'adam']),\n        hidden_size = tune.choice([64, 128, 256]),\n        trn_batch_size= tune.choice([16, 32, 64]),\n        # num_layers = tune.choice([1, 2])\n        ),\n\n# tuning algorithm \nsearch_algo = \"TPE\",\n\n# number of hyper-parameter set to try\nnum_evals = 27,\n\n# metric to be optimized, for 'mean_loss' metric mode should be 'min'\nmetric = \"mean_accuracy\",\nmode = \"max\",\n\n# scheduler to be used (i.e ASHAScheduler)\n# scheduler terminates trials that perform poorly\n# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html\n# scheduler = 'hyperband',\nscheduler = 'ASHA',\n\n# where to store logs\nlog_dir = \"RayLogs/\",\n\n# resume hyper-parameter tuning from previous log\n# specify 'name' (i.e main_2021-03-09_18-33-56) below\nresume = False,\n\n# only required if you want to resume from previous checkpoint\n# it can also be specified if you don't want to resume\nname = None,\n\n# specify resources to be used per trial\n# i.e {'gpu':1, 'cpu':2}\n# resources = {'gpu':1, 'cpu':2},\nresources = {'gpu':0.5, 'cpu':1},\n\n# if True, trains model on Full dataset with the best parameter selected.\nfinal_train = True,\n\nfinal_train_type = 'full' # full, gmpb\n\n)",
        "experimental_info": "Overall Hyper-parameter Optimization (HPO) Configuration:\n- Setting: Hyperparameter tuning.\n- Subset selection configuration: Uses 'configs/SL/config_gradmatchpb_glove_sst2.py'.\n- Search space: \n  - `learning_rate`: Uniformly sampled between 0.001 and 0.1.\n  - `hidden_size`: Choices among 64, 128, 256.\n  - `trn_batch_size`: Choices among 16, 32, 64.\n- Tuning algorithm: TPE (Tree-structured Parzen Estimator).\n- Number of evaluations: 27.\n- Metric to be optimized: `mean_accuracy`.\n- Optimization mode: `max` (maximize).\n- Scheduler: ASHA (Asynchronous Successive Halving Algorithm).\n- Logging directory: 'RayLogs/'.\n- Resume from previous log: False.\n- Resources per trial: {'gpu': 0.5, 'cpu': 1}.\n- Final training: True (trains the model on the full dataset with the best parameters found).\n- Final training type: 'full'."
      }
    },
    {
      "title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization",
      "abstract": "In many real-world scenarios, decision makers seek to efficiently optimize\nmultiple competing objectives in a sample-efficient fashion. Multi-objective\nBayesian optimization (BO) is a common approach, but many of the\nbest-performing acquisition functions do not have known analytic gradients and\nsuffer from high computational overhead. We leverage recent advances in\nprogramming models and hardware acceleration for multi-objective BO using\nExpected Hypervolume Improvement (EHVI)---an algorithm notorious for its high\ncomputational complexity. We derive a novel formulation of q-Expected\nHypervolume Improvement (qEHVI), an acquisition function that extends EHVI to\nthe parallel, constrained evaluation setting. qEHVI is an exact computation of\nthe joint EHVI of q new candidate points (up to Monte-Carlo (MC) integration\nerror). Whereas previous EHVI formulations rely on gradient-free acquisition\noptimization or approximated gradients, we compute exact gradients of the MC\nestimator via auto-differentiation, thereby enabling efficient and effective\noptimization using first-order and quasi-second-order methods. Our empirical\nevaluation demonstrates that qEHVI is computationally tractable in many\npractical scenarios and outperforms state-of-the-art multi-objective BO\nalgorithms at a fraction of their wall time.",
      "full_text": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization Samuel Daulton Facebook sdaulton@fb.com Maximilian Balandat Facebook balandat@fb.com Eytan Bakshy Facebook ebakshy@fb.com Abstract In many real-world scenarios, decision makers seek to efﬁciently optimize multiple competing objectives in a sample-efﬁcient fashion. Multi-objective Bayesian opti- mization (BO) is a common approach, but many of the best-performing acquisition functions do not have known analytic gradients and suffer from high computational overhead. We leverage recent advances in programming models and hardware acceleration for multi-objective BO using Expected Hypervolume Improvement (EHVI )—an algorithm notorious for its high computational complexity. We derive a novel formulation of q-Expected Hypervolume Improvement (qEHVI ), an acqui- sition function that extends EHVI to the parallel, constrained evaluation setting. qEHVI is an exact computation of the joint EHVI of qnew candidate points (up to Monte-Carlo (MC) integration error). Whereas previous EHVI formulations rely on gradient-free acquisition optimization or approximated gradients, we compute exact gradients of the MC estimator via auto-differentiation, thereby enabling efﬁ- cient and effective optimization using ﬁrst-order and quasi-second-order methods. Our empirical evaluation demonstrates that qEHVI is computationally tractable in many practical scenarios and outperforms state-of-the-art multi-objective BO algorithms at a fraction of their wall time. 1 Introduction The problem of optimizing multiple competing objectives is ubiquitous in scientiﬁc and engineering applications. For example in automobile design, an automaker will want to maximize vehicle durability and occupant safety, while using lighter materials that afford increased fuel efﬁciency and lower manufacturing cost [44, 72]. Evaluating the crash safety of an automobile design experimentally is expensive due to both the manufacturing time and the destruction of a vehicle. In such a scenario, sample efﬁciency is paramount. For a different example, video streaming web services commonly use adaptive control policies to determine the bitrate as the stream progresses in real time [47]. A decision maker may wish to optimize the control policy to maximize the quality of the video stream, while minimizing the stall time. Policy evaluation typically requires using the suggested policy on segments of live trafﬁc, which is subject to opportunity costs. If long evaluation times are the limiting factor, multiple designs may be evaluated in parallel to signiﬁcantly decrease end-to-end optimization time. For example, an automaker could manufacture multiple vehicle designs in parallel or a web service could deploy several control policies to different segments of trafﬁc at the same time. 1.1 Background Multi-Objective Optimization: In this work, we address the problem of optimizing a vector-valued objective f(x) : Rd →RM with f(x) = ( f(1)(x),...,f (M)(x) ) over a bounded set X ⊂Rd. We consider the scenario in which the f(i) are expensive-to-evaluate black-box functions with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2006.05078v3  [stat.ML]  23 Oct 2020no known analytical expression, and no observed gradients. Multi-objective (MO) optimization problems typically do not have a single best solution; rather, the goal is to identify the set of Pareto optimal solutions such that any improvement in one objective means deteriorating another. Without loss of generality, we assume the goal is to maximize all objectives. We say a solution f(x) Pareto dominates another solution f(x′) if f(m)(x) ≥ f(m)(x′) ∀m = 1 ,...,M and there exists m′ ∈{1,...,M }such that f(m′)(x) > f(m′)(x′). We write f(x) ≻f(x′). Let P∗= {f(x) s.t. ∄ x′∈X : f(x′) ≻f(x)}and X∗= {x ∈X s.t. f(x) ∈P∗}denote the set of Pareto optimal solutions and Pareto optimal inputs, respectively. Provided with the Pareto set, decision-makers can select a solution with an objective trade-off according to their preferences. A common approach for solving MO problems is to use evolutionary algorithms (e.g. NSGA-II), which are robust multi-objective optimizers, but require a large number of function evaluations [14]. Bayesian optimization (BO) offers a far more sample-efﬁcient alternative [57]. Bayesian Optimization: BO [38] is an established method for optimizing expensive-to-evaluate black-box functions. BO relies on a probabilistic surrogate model, typically a Gaussian Process (GP) [55], to provide a posterior distribution P(f|D) over the true function values f given the observed data D= {(xi,yi)}n i=1. An acquisition function α : Xcand ↦→R employs the surrogate model to assign a utility value to a set of candidates Xcand = {xi}q i=1 to be evaluated on the true function. While the true f may be expensive-to-evaluate, the surrogate-based acquisition function is not, and can thus be efﬁciently optimized to yield a set of candidates Xcand to be evaluated on f. If gradients of α(Xcand) are available, gradient-based methods can be utilized. If not, gradients are either approximated (e.g. with ﬁnite differences) or gradient-free methods (e.g. DIRECT [ 37] or CMA-ES [32]) are used. 1.2 Limitations of current approaches In the single-objective (SO) setting, a large body of work focuses on practical extensions to BO for supporting parallel evaluation and outcome constraints [49, 30, 66, 25, 43]. Less attention has been given to such extensions in the MO setting. Moreover, the existing constrained and parallel MO BO options have limitations: 1) many rely on scalarizations to transform the MO problem into a SO one [40]; 2) many acquisition functions are computationally expensive to compute [52, 21, 6, 71]; 3) few have known analytical gradients or are differentiable [19, 62, 33]; 4) many rely on heuristics to extend sequential algorithms to the parallel setting [27, 62]. A natural acquisition function for MO BO is Expected Hypervolume Improvement (EHVI ). Max- imizing the hypervolume ( HV) has been shown to produce Pareto fronts with excellent cover- age [73, 12, 69]. However, there has been little work on EHVI in the parallel setting, and the work that has been done resorts to approximate methods [71, 28, 62]. A vast body of literature has focused on efﬁcient EHVI computation [34, 20, 67], but the time complexity for computing EHVI is exponential in the number of objectives—in part due the hypervolume indicator itself incurring a time complexity that scales super-polynomially with the number of objectives [68]. Our core insight is that by exploiting advances in auto-differentiation and highly parallelized hardware [51], we can make EHVI computations fast and practical. 1.3 Contributions In this work, we derive a novel formulation of the parallelq-Expected Hypervolume Improvement acquisition function (qEHVI ) that is exact up to Monte-Carlo (MC) integration error. We compute the exact gradient of the MC estimator of qEHVI using auto-differentiation, which allows us to employ efﬁcient and effective gradient-based optimization methods. Rather than using ﬁrst-order gradient methods, we instead leverage the sample average approximation (SAA) approach from [5] to use higher-order deterministic optimization methods, and we prove theoretical convergence guarantees under the SAA approach. Our formulation of qEHVI is embarrassingly parallel, and despite its computational cost would achieve constant time complexity given inﬁnite processing cores. We demonstrate that, using modern GPU hardware and computing exact gradients, optimizing qEHVI is faster than existing state-of-the art methods in many practical scenarios. Moreover, we extend qEHVI to support auxiliary outcome constraints, making it practical in many real-world scenarios. Lastly, we demonstrate how modern auto-differentiation can be used to compute exact gradients of analytic EHVI , which has never been done before for M >2 objectives. Our empirical evaluation 2shows that qEHVI outperforms state-of-the-art multi-objective BO algorithms while using only a fraction of their wall time. 2 Related Work Yang et al. [69] is the only previous work to consider exact gradients of EHVI, but the authors only derive an analytical gradient for the unconstrained two-objective, sequential optimization setting. All other works either do not optimize EHVI (e.g. they use it for pre-screening candidates [ 18]), optimize it with gradient-free methods [68], or using approximate gradients [62]. In contrast, we use exact gradients and demonstrate that optimizing EHVI using this gradient information is far more efﬁcient. There are many alternatives to EHVI for MO BO. For example, ParEGO [ 40] and TS-TCH [50] randomly scalarize the objectives and use Expected Improvement [38] and Thompson Sampling [61], respectively. SMS-EGO [53] uses HV in a UCB-based acquisition function and is more scalable than EHVI [54]. ParEGO and SMS-EGO have only been considered for the q= 1, unconstrained setting. Predictive entropy search for MO BO (PESMO) [ 33] has been shown to be another competitive alternative and has been extended to handle constraints [ 26] and parallel evaluations [ 27]. MO max-value entropy search (MO-MES) has been shown to achieve superior optimization performance and faster wall times than PESMO, but is limited to q= 1. Wilson et al. [65] empirically and theoretically show that sequential greedy selection of qcandidates achieves performance comparable to jointly optimizing qcandidates for many acquisition functions (including [63, 66]). The sequential greedy approach integrates over the posterior of the unobserved outcomes corresponding to the previously selected candidates in the q-batch. Sequential greedy optimization often yields better empirical results because the optimization problem has a lower dimension: din each step, rather than qdin the joint problem. Most prior works in the MO setting use a sequential greedy approximation or heuristics [62, 71, 28, 10], but impute the unobserved outcomes with the posterior mean rather than integrating over the posterior [30]. For many joint acquisition functions involving expectations, this shortcut sacriﬁces the theoretical error bound on the sequential greedy approximation because the exact joint acquisition function overx1,..., xi, 1 ≤i≤qrequires integration over the joint posterior P(f(x1),..., f(xq)|D) and is not computed for i> 1. Garrido-Merchán and Hernández-Lobato [27] and Wada and Hino [62] jointly optimize the qcandi- dates and, noting the difﬁculty of the optimization, both papers focus on deriving gradients to aid in the optimization. Wada and Hino [62] deﬁned the qEHVI acquisition function, but after ﬁnding it challenging to optimize qcandidates jointly (without exact gradients), the authors propose optimizing an alternative acquisition function instead of exactqEHVI . In contrast, our novel qEHVI formulation allows for gradient-based parallel and sequential greedy optimization, with proper integration over the posterior for the latter. Feliot et al. [22] and Abdolshah et al. [1] proposed extensions of EHVI to the constrained q = 1 setting, but neither considers the batch setting and both rely on gradient-free optimization. 3 Differentiable q-Expected Hypervolume Improvement In this section, we review HVI and EHVI computation by means of box decompositions, and explain our novel formulation for the parallel setting. Deﬁnition 1. Given a reference point r ∈RM , the hypervolume indicator (HV) of a ﬁnite approxi- mate Pareto set Pis the M-dimensional Lebesgue measure λM of the space dominated by Pand bounded from below byr: HV(P,r) = λM (⋃|P| i=1[r,yi] ) , where [r,yi] denotes the hyper-rectangle bounded by vertices r and yi. Deﬁnition 2. Given a Pareto set Pand reference point r, the hypervolume improvement (HVI) of a set of points Yis: HVI (Y,P,r) = HV(P∪Y ,r) −HV(P,r).1 EHVI is the expectation of HVI over the posterior P(f,D): αEHVI (Xcand) = E [ HVI (f(Xcand)) ] . In the sequential setting, and assuming the objectives are independent and modeled with independent 1In this work, we omit the arguments Pand rwhen referring to HVI for brevity. 3GPs, EHVI can be expressed in closed form [69]. In other settings, EHVI can be approximated with MC integration. Following previous work, we assume that the reference point is known and speciﬁed by the decision maker [69] (see Appendix E.1.1 for additional discussion). 3.1 A review of hypervolume improvement computation using box decompositions Deﬁnition 3. For a set of objective vectors {f(xi)}q i=1, a reference point r ∈RM , and a non- dominated set P, let ∆({f(xi)}q i=1,P,r) ⊂RM denote the set of points (i) are dominated by {f(xi)}q i=1, dominate r, and are not dominated by P. Given P,r, the HVI of a new point f(x) is the HV of the intersection of space dominated by P∪{ f(x)}and the non-dominated space. Figure 1b illustrates this for one new point f(x) for M = 2. The yellow region is ∆({f(x)},P,r) and the hypervolume improvement is the volume covered by ∆({f(x)},P,r). Since ∆({f(x)},P,r) is often a non-rectangular polytope, HVI is typically computed by partitioning the non-dominated space into disjoint axis-parallel rectangles [12, 68] (see Figure 1a) and using piece-wise integration [18]. Let {Sk}K k=1 be a partitioning the of non-dominated space into disjoint hyper-rectangles, where each Sk is deﬁned by a pair of lower and upper vertices lk ∈RM and uk ∈RM ∪{∞}. The high level idea is to sum the HV ofSk ∩∆({f(x)},P,r) over all Sk. For each hyper-rectangle Sk, the intersec- tion of Sk and ∆({f(x)},P,r) is a hyper-rectangle where the lower bound vertex islk and the upper bound vertex is the component-wise minimum ofuk and the new pointf(x): zk := min [ uk,f(x) ] . r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f (2)(x) f (1)(x) (a) f(x1) r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f(2)(x ) f(1)(x ) (b) f(x1) f(2)(x ) f(1)(x )r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f(x2)  (c) Figure 1: For M=2, (a) the dominated space (red) and the non-dominated space partitioned into disjoint boxes (white), (b) the HVI of one new point f(x), and (c) the HVI of two new points f(x1),f(x2). Hence, the HVI of a single outcome vector f(x) within Sk is given by HVI k ( f(x),lk,uk ) = λM ( Sk ∩∆({f(x)},P,r) ) = ∏M m=1 [ z(m) k −l(m) k ] +, where u(m) k ,l(m) k ,f(m)(x), and z(m) k denote the mth component of the corresponding vector and [·]+ denotes the min(·,0) operation. Summing over rectangles yields HVI ( f(x) ) = K∑ k=1 HVIk ( f(x),lk,uk ) = K∑ k=1 M∏ m=1 [ z(m) k −l(m) k ] + (1) 3.2 Computing q-Hypervolume Improvement via the Inclusion-Exclusion Principle Figure 1c illustrates the HVI in the q = 2 setting. Given q new points {f(xi)}q i=1, let Ai := ∆({f(xi)},P,r) for i= 1,...,q be the space dominated by f(xi) but not dominated by P, independently of the other q−1 points. Note that λM (Ai) = HVI (f(xi)). The union of the subsets Ai is the space dominated jointly by the qnew points: ⋃q i=1 Ai = ⋃q i=1 ∆({f(xi)},P,r), and the Lebesgue measure λM (⋃q i=1 Ai ) is the joint HVI from the qnew points. Since each subspace Ai is bounded, the restricted Lebesgue measure is ﬁnite and we may compute λM (⋃q i=1 Ai ) using the inclusion-exclusion principle [13, 59]: HVI ({f(xi)}q i=1) = λM ( q⋃ i=1 Ai ) = q∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤q λM ( Ai1 ∩···∩ Aij ) (2) 4Since {Sk}K k=1 is a disjoint partition, λM (Ai1 ∩···∩ Aij ) = ∑K k=1 λM (Sk ∩Ai1 ∩···∩ Aij ), we can compute λM (Ai1 ∩···∩ Aij ) in a piece-wise fashion across the Khyper-rectangles {Sk}K k=1 as the HV of the intersection of Ai1 ∩···∩ Aij with each hyper-rectangle Sk. The inclusion- exclusion principle has been proposed for computingHV (not HVI) [45], but it is rarely used because complexity scales exponentially with the number of elements. However, the inclusion-exclusion principle is practical for computing the joint HVI of qpoints since typically q <<|P|. This formulation has three advantages. First, while the new dominated space Ai can be a non-rectangular polytope, the intersection Ai ∩Sk is a rectangular polytope, which simpliﬁes computation of overlapping hypervolume. Second, the vertices deﬁning the hyper-rectangle Sk ∩Ai1 ∩···∩ Aij are easily derived. The lower bound is simply the lk lower bound of Sk, and the upper bound is the component-wise minimum zk,i1,...ij := min [ uk,f(xi1 ),..., f(xij ) ] . Third, computation can be across all intersections of subsets Ai1 ∩···∩ Aij for 1 ≤ij ≤... ≤ij ≤qand across all Khyper-rectangles can be performed in parallel. Explicitly, the HVI is computed as: HVI ({f(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj −l(m) k ] + (3) where Xj := {Xj ⊂ Xcand : |Xj| = j}is the superset of all subsets of Xcand of size j, and z(m) k,Xj := z(m) k,i1,...ij for Xj = {xi1 ,..., xij }. See Appendix A for further details of the derivation. 3.3 Computing Expected q-Hypervolume Improvement The above approach for computing HVI assumes that we know the true objective values f(Xcand) = {f(xi)}q i=1. In BO, we instead compute qEHVI as the expectation over the posterior model posterior: αqEHVI (Xcand) = E [ HVI (f(Xcand)) ] = ∫ ∞ −∞ HVI(f(Xcand))df. (4) Since no known analytical form is known [ 70] for q > 1 (or in the case of correlated out- comes), we estimate (4) using MC integration with samples from the joint posterior {ft(xi)}q i=1 ∼ P ( f(x1),..., f(xq)|D ) ,t = 1,...N . Let z(m) k,Xj,t := min [ uk,minx′∈Xj ft(x′) ] . Then, ˆαN qEHVI (Xcand) = 1 N N∑ t=1 HVI(ft(Xcand)) = 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + (5) Provided that {Sk}K k=1 is an exact partitioning, (5) is an exact computation of qEHVI up to the MC estimation error, which scales as 1/ √ N when using iidMC samples regardless of the dimension of the search space [18]. In practice, we use randomized quasi MC methods [8] to reduce the variance and empirically observe low estimation error (see Figure 5a in the Appendix for a comparison of analytic EHVI and (quasi-)MC-based qEHVI). qEHVI requires computing the volume of 2q −1 hyper-rectangles (the number of subsets of q) for each of Khyper-rectangles and N MC samples. Given posterior samples, the time complexity on a single-threaded machine is: T1 = O(MNK(2q −1)). In the two-objective case, K = |P|+ 1, but K is super-polynomial in M [68]. The number of boxes required for a decomposition of the non-dominated space is unknown for M ≥4 [68]. qEHVI is agnostic to the partitioning algorithm used, and in F.4, we demonstrate using qEHVI in higher-dimensional objective spaces using an approximate box decomposition algorithm [11]. Despite the daunting workload, the critical work path—the time complexity of the smallest non-parallelizable unit—is constant: T∞= O(1).2 On highly-threaded many-core hardware (e.g. GPUs), our formulation achieves tractable wall times in many practical scenarios: as is shown in Figure 11 in the Appendix, the computation time is nearly constant with increasing quntil an inﬂection point at which the workload saturates the available cores. For additional discussion of both time and memory complexity of qEHVI see Appendix A.4. 3.4 Outcome Constraints Our proposed qEHVI acquisition function is easily extended to constraints on auxiliary outcomes. We consider the scenario where we receive observations of M objectives f(x) ∈RM and V constraints 2As evident from (5), the critical path consists of 3 multiplications and 5 summations. 5c(v) ∈RV , all of which are assumed to be “black-box”. We assume w.l.o.g. that c(v) is feasible iff c(v) ≥0. In the constrained optimization setting, we aim to identify the feasible Pareto set: Pfeas = {f(x) s.t. c(x) ≥0, ∄ x′ : c(x′) ≥0, f(x′) ≻f(x)}. The natural improvement measure in the constrained setting is feasible HVI, which we deﬁne for a single candidate point x as HVI C(f(x),c(x)) := HVI [f(x)] ·1 [c(x) ≥0]. Taking expectations, the constrained expected HV can be seen to be the HV weighted by the probability of feasibility. In Appendix A.3, we detail how performing feasibility-weighting on the sample-level allows us to include such auxiliary outcome constraints into our MC formulation in a straightforward way. 4 Optimizing q-Expected Hypervolume Improvement 4.1 Differentiability While an analytic formula for the gradient of EHVI exists for the M = 2 objective case in the unconstrained, sequential ( q = 1 ) setting, no such formula is known in 1) the case of M > 2 objectives, 2) the constrained setting, and 3) for q >1. Leveraging the re-parameterization trick [39, 64] and auto-differentiation, we are able to automatically compute exact gradients of the MC- estimator qEHVI in all of the above settings, as well as the gradient of analytic EHVI for M ≥2 (see Figure 5b in the Appendix for a comparison of the exact gradients of EHVI and the sample average gradients of qEHVI for M = 3).3,4 4.2 Optimization via Sample Average Approximation We show in Appendix C that if mean and covariance function of the GP are sufﬁciently regular, the gradient of the MC estimator (5) is an unbiased estimate of the gradient of the exact acquisition function (4). To maximize qEHVI , we could therefore directly apply stochastic optimization methods, as has previously been done for single-outcome acquisition functions [ 64, 66]. Instead, we opt to use the sample average approximation (SAA) approach from Balandat et al. [5], which allows us to employ deterministic, higher-order optimizers to achieve faster convergence rates. Informally (see Appendix C for the formal statement), if ˆx∗ N ∈arg maxx∈X ˆαN qEHVI (x), we can show under some regularity conditions that, as N →∞, (i) ˆαN qEHVI (ˆx∗ N ) →maxx∈XαqEHVI (x) a.s., and (ii) dist ( ˆx∗ N ,arg maxx∈XαqEHVI (x) ) →0 a.s.. These results hold for any covariance function satisfying the regularity conditions, including such ones that model correlation between outcomes. In particular, our results do not require the outputs to be modeled by independent GPs. Figure 2a demonstrates the importance of using exact gradients for efﬁciently and effectively op- timizing EHVI and qEHVI by comparing the following optimization methods: L-BFGS-B with exact gradients, L-BFGS-B with gradients approximated via ﬁnite differences, and CMA-ES (without gradients). The cumulative time spent optimizing the acquisition function is an order of magnitude less when using exact gradients rather than approximate gradients or zeroth order methods. 4.3 Sequential Greedy and Joint Batch Optimization Jointly optimizing qcandidates increases in difﬁculty with qbecause the problem dimension is dq. An alternative is to sequentially and greedily select candidates and condition the acquisition function on the previously selected pending points when selecting the next point [65]. Using a submodularity argument similar to that in Wilson et al. [64], the sequential greedy approximation of qEHVI enjoys regret of no more than 1 e α∗ qEHVI , where α∗ qEHVI is the optima of αqEHVI [23] (see Appendix B). Although sequential greedy approaches have been considered for many acquisition functions [65], no previous work has proposed a proper sequential greedy approach (with integration over the posterior) for parallel EHVI , as this would require computing the Pareto front under each sample ft from the joint posterior before computing the hypervolume improvement. These operations would be computationally expensive for even modest N and non-differentiable. qEHVI avoids determining the Pareto set for each sample by using inclusion-exclusion principle to compute the joint HVI over the pending points x1,..., xi−1 and the new candidate xi for each MC sample. Figure 2b empirically 3Technically,min and max are only sub-differentiable, but are known to be well-behaved [64]. In our MC setting with GP posteriors, qEHVI is differentiable w.p. 1 if xcontains no repeated points. 4For the constrained case, we replace the indicator with a differentiable sigmoid approximation. 60 5000 10000 15000 20000 Average/uni00A0Cumulative/uni00A0Acquisition/uni00A0Optimization/uni00A0Wall/uni00A0Time/uni00A0(s) 0.40 0.35 0.30 0.25 0.20 0.15 0.10 log/uni00A0HV/uni00A0difference EHVI/uni00A0/uni00AD/uni00A0Exact/uni00A0Gradient EHVI/uni00A0/uni00AD/uni00A0Approx./uni00A0Gradient EHVI/uni00A0/uni00AD/uni00A0Gradient/uni00A0Free qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Exact/uni00A0Gradient qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Approx./uni00A0Gradient qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Gradient/uni00A0Free (a) 0 20 40 60 80 100 Batch/uni00A0Iteration 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0difference qEHVI/uni00A0Joint/uni00A0q=2 qEHVI/uni00A0Joint/uni00A0q=4 qEHVI/uni00A0Joint/uni00A0q=8 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=2 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=4 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=8 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=2 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=4 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=8 qEHVI/uni00A0q=1 (b) Figure 2: (a) A comparison of EHVI and qEHVI (q = 2) optimized with L-BFGS-B using exact gradients, L-BFGS-B using gradients approximated using ﬁnite differences, and CMA-ES, a gradient- free method. (b) A comparison of joint optimization, sequential greedy optimization with proper integration at the pending points, and sequential greedy using the posterior mean. Both plots show optimization performance on a DTLZ2 problem (d= 6,M = 2) with a budget of 100 evaluations (plus the initial quasi-random design). We report means and 2 standard errors across 20 trials. demonstrates the improved optimization performance from properly integrating over the unobserved outcomes rather than using the posterior mean or jointly optimizing the qcandidates. 5 Benchmarks We empirically evaluate qEHVI on synthetic and real world optimization problems. We compare qEHVI 5 against existing state-of-the-art methods including SMS-EGO6, PESMO6, TS-TCH5, and analytic EHVI [68] with gradients5. Additionally, we compare against a novel extension of ParEGO [40] that supports parallel evaluation and constraints (neither of which have been done before to our knowledge); we call this method qPAREGO5. Additionally, we include a quasi-random baseline that selects candidates from a scrambled Sobol sequence. See Appendix E.1 for details on all baseline algorithms. Synthetic Benchmarks We evaluate optimization performance on four benchmark problems in terms of log hypervolume difference, which is deﬁned as the difference between the hypervolume of the true (feasible) Pareto front and the hypervolume of the approximate (feasible) Pareto front based on the observed data; in the case that the true Pareto front is unknown (or not easily approximated), we evaluate the hypervolume indicator. All references points and search spaces are provided in Appendix E.2. For synthetic problems, we consider the Branin-Currin problem ( d = 2,M = 2, convex Pareto front) [6] and the C2-DTLZ2 (d= 12,M = 2,V = 1, concave Pareto front), which is a standard constrained benchmark from the MO literature [16] (see Appendix F.1 for additional synthetic benchmarks). Real-World Benchmarks Structural Optimization in Automobile Safety Design (VEHICLE SAFETY ): Vehicle crash safety is an important consideration in the structural design of automobiles. A lightweight car is preferable because of its potentially lower manufacturing cost and better fuel economy, but lighter material can fare worse than sturdier alternatives in a collision, potentially leading to increased vehicle damage and more severe injury to the vehicle occupants [72]. We consider the problem designing the thickness of 5 reinforced parts of the frontal frame of a vehicle that considerably affect crash safety. The goal is to minimize: 1) the mass of the vehicle; 2) the collision acceleration in a full frontal crash—a proxy for bio-mechanical trauma to the vehicle occupants from the acceleration; and 3) the toe-board 5Acquisition functions are available as part of the open-source library BoTorch [ 5]. Code is available at https://github.com/pytorch/botorch. 6We leverage existing implementations from the Spearmint library. The code is available at https:// github.com/HIPS/Spearmint/tree/PESM. 70 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol qEHVI qParEGO (b) 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (c) 0 20 40 60 80 100 Function/uni00A0Evaluations 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (d) Figure 3: Sequential optimization performance on (a) on the Branin-Currin problem (q= 1), (b) the C2-DTLZ2 problem, (c) the vehicle crash safety problem (q= 1), and (d) the ABR control problem (q= 1). We report the means and 2 standard errors across 20 trials. intrusion—a measure of the most extreme mechanical damage to the vehicle in an off-frontal collision [44]. For this problem, we optimize the surrogate from Tanabe and Ishibuchi [60]. Policy Optimization for Adaptive Bitrate Control(ABR ): Many web services adapt video playback quality adaptively based on the receiver’s network bandwith to maintain steady, high quality stream with minimal stalls and buffer periods [47]. Previous works have proposed controllers with different scalarized objective functions [46], but in many cases, engineers may prefer to learn the set of optimal trade-offs between their metrics of interest, rather than specifying a scalarized objective in advance. In this problem, we decompose the objective function proposed in Mao et al. [46] into its constituent metrics and optimize 4 parameters of an ABR control policy on the Park simulator [48] to maximize video quality (bitrate) and minimize stall time. See Appendix E.2 for details. 5.1 Results Figure 3 shows thatqEHVI outperforms all baselines in terms of sequential optimization performance on all evaluated problems. Table 1 shows that qEHVI achieves wall times that are an order of magnitude smaller than those of PESMO on a CPU in sequential optimization, and maintains competitive wall times even relative toqPAREGO (which has a signiﬁcantly smaller workload) for large q on a GPU. TS-TCH has by far the fastest wall time, but this comes at the cost of inferior optimization performance. Figure 4 illustrates optimization performance of parallel acquisition functions for varying batch sizes. Increasing the level of parallelism leads to faster convergence for all algorithms (Figure 4a). In contrast with other algorithms, qEHVI ’s sample complexity does not deteriorate substantially when high levels of parallelism are used (Figure 4b). 8Table 1: Acquisition Optimization wall time in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and a GPU (Tesla V100-SXM2-16GB). We report the mean and 2 standard errors across 20 trials. NA indicates that the algorithm does not support constraints. CPU BRANIN CURRIN C2DTLZ2 ABR V EHICLE SAFETY PESMO ( q=1) 249.16 (±19.35) NA 214.16 (±18.38) 492 .64 (±58.98) SMS-EGO ( q=1) 146.1 (±8.57) NA 89.54 (±5.79) 115 .11 (±8.21) TS-TCH ( q=1) 2.82 (±0.03) NA 17.22 (±0.04) 47 .46 (±0.05) qPAREGO ( q=1) 1.56 (±0.16) 4 .01 (±0.77) 7 .47 (±0.67) 1 .74 (±0.27) EHVI ( q=1) 3.04 (±0.16) NA 2.48 (±0.19) 15 .18 (±2.24) qEHVI ( q=1) 3.63 (±0.23) 5 .4 (±1.18) 6 .15 (±0.71) 67 .54 (±10.45) GPU BRANIN CURRIN C2DTLZ2 ABR V EHICLE SAFETY TS-TCH ( q=1) 0.07 (±0.00) NA 0.16 (±0.00) 0 .32 (±0.0) TS-TCH ( q=2) 0.07 (±0.00) NA 0.15 (±0.00) 0 .34 (±0.01) TS-TCH ( q=4) 0.09 (±0.01) NA 0.15 (±0.00) 0 .31 (±0.01) TS-TCH ( q=8) 0.08 (±0.00) NA 0.16 (±0.00) 0 .34 (±0.01) qPAREGO ( q=1) 3.2 (±0.37) 3 .85 (±0.91) 9 .64 (±0.96) 3 .44 (±0.51) qPAREGO ( q=2) 7.12 (±0.81) 12 .1 (±2.77) 21 .19 (±1.53) 7 .32 (±0.97) qPAREGO ( q=4) 15.34 (±1.69) 39 .71 (±7.40) 35 .46 (±2.32) 17 .2 (±2.29) qPAREGO ( q=8) 32.11 (±4.14) 99 .58 (±15.20) 72 .52 (±5.04) 39 .72 (±7.13) EHVI ( q=1) 4.53 (±0.23) NA 6.82 (±0.55) 8 .95 (±0.64) qEHVI ( q=1) 5.98 (±0.28) 3 .36 (±0.94) 7 .71 (±0.67) 10 .43 (±0.64) qEHVI ( q=2) 11.37 (±0.56) 21 .56 (±3.45) 18 .32 (±1.48) 17 .67 (±1.54) qEHVI ( q=4) 25.29 (±1.51) 89 .18 (±10.86) 44 .44 (±3.53) 54 .25 (±4.17) qEHVI ( q=8) 102.46 (±9.22) 215 .74 (±15.85) 100 .64 (±7.22) 255 .72 (±23.73) 0 20 40 60 80 100 Batch/uni00A0Iteration 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) Figure 4: Parallel optimization performance on the ABR problem with varying batch sizes (q) by (a) batch BO iterations and (b) function evaluations. 6 Discussion We present a practical and efﬁcient acquisition function, qEHVI , for parallel, constrained multi- objective Bayesian optimization. Leveraging differentiable programming, modern parallel hardware, and the Sample Average Approximation, we efﬁciently optimizeqEHVI via quasi second-order meth- ods and provide theoretical convergence guarantees for our approach. Empirically, we demonstrate that our method out-performs state-of-the-art multi-objective Bayesian optimization methods. One limitation of our approach is that it currently assumes noiseless observations, which, to our knowledge, is the case with all formulations of EHVI . Integrating over the uncertainty around the previous observations [43] by using MC samples over the new candidates and the training points, one may be able to account for the noise.Another limitation of qEHVI is that its scalability is limited the partitioning algorithm, precluding its use in high-dimensional objective spaces. More scalable partitioning algorithms, either approximate algorithms (e.g. the algorithm proposed by Couckuyt et al. [11], which we examine brieﬂy in Appendix F.4) or more efﬁcient exact algorithms that result in fewer disjoint hyper-rectangles (e.g. [41, 17, 69]), will improve the scalability and computation time of of qEHVI . We hope this work encourages researchers to consider more improvements from applying modern computational paradigms and tooling to Bayesian optimization. 97 Statement of Broader Impact Optimizing a single outcome commonly comes at the expense of other secondary outcomes. In some cases, decision makers may be able to form a scalarization of their objectives in advance, but in the researcher’s experience, formulating such trade-offs in advance is difﬁcult for most. Improvements to the optimization performance and practicality of multi-objective Bayesian optimization have the potential to allow decision makers to better understand and make more informed decisions across multiple trade-offs. We expect these directions to be particularly important as Bayesian optimization is increasingly used for applications such as recommender systems [42], where auxiliary goals such as fairness must be accounted for. Of course, at the end of the day, exactly what objectives decision makers choose to optimize, and how they balance those trade-offs (and whether that is done in equitable fashion) is up to the individuals themselves. Acknowledgments We would like to thank Daniel Jiang for helpful discussions around our theoretical results. References [1] M. Abdolshah, A. Shilton, S. Rana, S. Gupta, and S. Venkatesh. Expected hypervolume improvement with constraints. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 3238–3243, 2018. [2] Arash Asadpour, Hamid Nazerzadeh, and Amin Saberi. Stochastic submodular maximization. In Christos Papadimitriou and Shuzhong Zhang, editors, Internet and Network Economics. Springer Berlin Heidelberg, 2008. [3] R. Astudillo and P. Frazier. Bayesian optimization of composite functions. Forthcoming, in Proceedings of the 35th International Conference on Machine Learning, 2019. [4] Anne Auger, Johannes Bader, Dimo Brockhoff, and Eckart Zitzler. Theory of the hypervolume indicator: Optimal mu-distributions and the choice of the reference point. In Proceedings of the Tenth ACM SIGEVO Workshop on Foundations of Genetic Algorithms, FOGA ’09, page 87–102, New York, NY , USA, 2009. Association for Computing Machinery. [5] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efﬁcient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. [6] Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-objective bayesian optimization. In Advances in Neural Information Processing Systems 32, 2019. [7] Eric Bradford, Artur Schweidtmann, and Alexei Lapkin. Efﬁcient multiobjective optimization employing gaussian processes, spectral sampling and a genetic algorithm. Journal of Global Optimization, 71, 02 2018. doi: 10.1007/s10898-018-0609-2. [8] Russel E Caﬂisch. Monte carlo and quasi-monte carlo methods. Acta numerica, 7:1–49, 1998. [9] Mauro Cerasoli and Aniello Fedullo. The inclusion-exclusion principle. Journal of Interdisciplinary Mathematics, 5(2):127–141, 2002. [10] Anirban Chaudhuri, Raphael Haftka, Peter Ifju, Kelvin Chang, Christopher Tyler, and Tony Schmitz. Experimental ﬂapping wing optimization and uncertainty quantiﬁcation using limited samples. Structural and Multidisciplinary Optimization, 51, 11 2014. doi: 10.1007/s00158-014-1184-x. [11] I. Couckuyt, D. Deschrijver, and T. Dhaene. Towards efﬁcient multiobjective optimization: Multiobjective statistical criterions. In 2012 IEEE Congress on Evolutionary Computation, pages 1–8, 2012. [12] Ivo Couckuyt, Dirk Deschrijver, and Tom Dhaene. Fast calculation of multiobjective probability of improvement and expected improvement criteria for pareto optimization. J. of Global Optimization, 60(3): 575–594, November 2014. [13] Daniel A. da Silva. Proprietades geraes. J. de l’Ecole Polytechnique, cah. 30. I, 1854. 10[14] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182–197, 2002. [15] Kalyan Deb, L. Thiele, Marco Laumanns, and Eckart Zitzler. Scalable multi-objective optimization test problems. volume 1, pages 825–830, 06 2002. ISBN 0-7803-7282-4. doi: 10.1109/CEC.2002.1007032. [16] Kalyanmoy Deb. Constrained Multi-objective Evolutionary Algorithm, pages 85–118. Springer Interna- tional Publishing, Cham, 2019. [17] Kerstin Dächert, Kathrin Klamroth, Renaud Lacour, and Daniel Vanderpooten. Efﬁcient computation of the search region in multi-objective optimization. European Journal of Operational Research, 260(3):841 – 855, 2017. [18] M. T. M. Emmerich, K. C. Giannakoglou, and B. Naujoks. Single- and multiobjective evolutionary opti- mization assisted by gaussian random ﬁeld metamodels. IEEE Transactions on Evolutionary Computation, 10(4):421–439, 2006. [19] M. T. M. Emmerich, A. H. Deutz, and J. W. Klinkenberg. Hypervolume-based expected improvement: Monotonicity properties and exact computation. In 2011 IEEE Congress of Evolutionary Computation (CEC), pages 2147–2154, 2011. [20] Michael Emmerich, Kaifeng Yang, André Deutz, Hao Wang, and Carlos M. Fonseca. A Multicriteria Generalization of Bayesian Global Optimization, pages 229–242. Springer International Publishing, 2016. [21] Michael T. M. Emmerich and Carlos M. Fonseca. Computing hypervolume contributions in low dimensions: Asymptotically optimal algorithm and complexity results. In Ricardo H. C. Takahashi, Kalyanmoy Deb, Elizabeth F. Wanner, and Salvatore Greco, editors, Evolutionary Multi-Criterion Optimization , pages 121–135, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. [22] Paul Feliot, Julien Bect, and Emmanuel Vazquez. A bayesian approach to constrained single- and multi- objective optimization. Journal of Global Optimization, 67(1-2):97–133, Apr 2016. ISSN 1573-2916. doi: 10.1007/s10898-016-0427-3. URL http://dx.doi.org/10.1007/s10898-016-0427-3 . [23] M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey. An analysis of approximations for maximizing submodular set functions—II , pages 73–87. Springer Berlin Heidelberg, Berlin, Heidelberg, 1978. [24] Tobias Friedrich and Frank Neumann. Maximizing submodular functions under matroid constraints by multi-objective evolutionary algorithms. In Thomas Bartz-Beielstein, Jürgen Branke, Bogdan Filipiˇc, and Jim Smith, editors, Parallel Problem Solving from Nature – PPSN XIII , pages 922–931, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10762-2. [25] Jacob Gardner, Matt Kusner, Zhixiang, Kilian Weinberger, and John Cunningham. Bayesian optimization with inequality constraints. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 937–945, Beijing, China, 22–24 Jun 2014. PMLR. [26] Eduardo C Garrido-Merchán and Daniel Hernández-Lobato. Predictive entropy search for multi-objective bayesian optimization with constraints. Neurocomputing, 361:50–68, 2019. [27] Eduardo C Garrido-Merchán and Daniel Hernández-Lobato. Parallel predictive entropy search for multi- objective bayesian optimization with constraints, 2020. [28] David Gaudrie, Rodolphe Le Riche, Victor Picheny, Benoît Enaux, and Vincent Herbert. Targeting solutions in bayesian multi-objective optimization: sequential and batch versions. Annals of Mathematics and Artiﬁcial Intelligence, 88(1-3):187–212, Aug 2019. ISSN 1573-7470. doi: 10.1007/s10472-019-09644-8. URL http://dx.doi.org/10.1007/s10472-019-09644-8 . [29] Michael A. Gelbart, Jasper Snoek, and Ryan P. Adams. Bayesian optimization with unknown constraints. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence, UAI, 2014. [30] David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro. Kriging Is Well-Suited to Parallelize Optimization, pages 131–162. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010. [31] P. Glasserman. Performance continuity and differentiability in monte carlo optimization. In 1988 Winter Simulation Conference Proceedings, pages 518–524, 1988. [32] Nikolaus Hansen. The CMA Evolution Strategy: A Comparing Review, volume 192, pages 75–102. 06 2007. doi: 10.1007/3-540-32494-1_4. 11[33] Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Amar Shah, and Ryan P. Adams. Predictive entropy search for multi-objective bayesian optimization, 2015. [34] Iris Hupkens, Andre Deutz, Kaifeng Yang, and Michael Emmerich. Faster exact algorithms for computing expected hypervolume improvement. In Antonio Gaspar-Cunha, Carlos Henggeler Antunes, and Car- los Coello Coello, editors, Evolutionary Multi-Criterion Optimization, pages 65–79. Springer International Publishing, 2015. [35] Hisao Ishibuchi, Naoya Akedo, and Yusuke Nojima. A many-objective test problem for visually examining diversity maintenance behavior in a decision space. In Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation , GECCO ’11, page 649–656, New York, NY , USA, 2011. Association for Computing Machinery. ISBN 9781450305570. doi: 10.1145/2001576.2001666. URL https://doi.org/10.1145/2001576.2001666. [36] Hisao Ishibuchi, Ryo Imada, Yu Setoguchi, and Yusuke Nojima. How to specify a reference point in hypervolume calculation for fair performance comparison. Evol. Comput., 26(3):411–440, September 2018. [37] Donald Jones, C. Perttunen, and B. Stuckman. Lipschitzian optimisation without the lipschitz constant. Journal of Optimization Theory and Applications, 79:157–181, 01 1993. doi: 10.1007/BF00941892. [38] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global Optimization, 13:455–492, 1998. [39] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv e-prints , page arXiv:1312.6114, Dec 2013. [40] J. Knowles. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):50–66, 2006. [41] Renaud Lacour, Kathrin Klamroth, and Carlos M. Fonseca. A box decomposition algorithm to compute the hypervolume indicator. Computers & Operations Research, 79:347 – 360, 2017. [42] Benjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via online-ofﬂine experimen- tation. Journal of Machine Learning Research, 20(145):1–30, 2019. URL http://jmlr.org/papers/ v20/18-225.html. [43] Benjamin Letham, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. Constrained bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 06 2019. doi: 10.1214/18-BA1110. [44] Xingtao Liao, Qing Li, Xujing Yang, Weigang Zhang, and Wei Li. Multiobjective optimization for crash safety design of vehicles using stepwise regression model. Structural and Multidisciplinary Optimization, 35:561–569, 06 2008. doi: 10.1007/s00158-007-0163-x. [45] Edgar Manoatl Lopez, Luis Miguel Antonio, and Carlos A. Coello Coello. A gpu-based algorithm for a faster hypervolume contribution computation. In António Gaspar-Cunha, Carlos Henggeler Antunes, and Carlos Coello Coello, editors, Evolutionary Multi-Criterion Optimization, pages 80–94. Springer International Publishing, 2015. [46] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. Neural adaptive video streaming with pensieve. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, SIGCOMM ’17, page 197–210, New York, NY , USA, 2017. Association for Computing Machinery. ISBN 9781450346535. doi: 10.1145/3098822.3098843. URL https://doi.org/10.1145/3098822.3098843. [47] Hongzi Mao, Shannon Chen, Drew Dimmery, Shaun Singh, Drew Blaisdell, Yuandong Tian, Mohammad Alizadeh, and Eytan Bakshy. Real-world video adaptation with reinforcement learning. 2019. [48] Hongzi Mao, Parimarjan Negi, Akshay Narayan, Hanrui Wang, Jiacheng Yang, Haonan Wang, Ryan Marcus, Ravichandra Addanki, Mehrdad Khani Shirkoohi, Songtao He, Vikram Nathan, Frank Cangialosi, Shaileshh Bojja Venkatakrishnan, Wei-Hung Weng, Shu-Wen Han, Tim Kraska, and Mohammad Alizadeh. Park: An open platform for learning-augmented computer systems. In NeurIPS, 2019. [49] Sébastien Marmin, Clément Chevalier, and David Ginsbourger. Differentiating the multipoint expected improvement for optimal batch design. In Panos Pardalos, Mario Pavone, Giovanni Maria Farinella, and Vincenzo Cutello, editors, Machine Learning, Optimization, and Big Data , pages 37–48, Cham, 2015. Springer International Publishing. [50] B. Paria, K. Kandasamy, and B. Póczos. A Flexible Multi-Objective Bayesian Optimization Approach using Random Scalarizations. ArXiv e-prints, May 2018. 12[51] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. 2017. [52] Victor Picheny. Multiobjective optimization using gaussian process emulators via stepwise uncertainty reduction. Statistics and Computing, 25, 10 2013. doi: 10.1007/s11222-014-9477-x. [53] Wolfgang Ponweiser, Tobias Wagner, Dirk Biermann, and Markus Vincze. Multiobjective optimization on a limited budget of evaluations using model-assisted s-metric selection. In Günter Rudolph, Thomas Jansen, Nicola Beume, Simon Lucas, and Carlo Poloni, editors, Parallel Problem Solving from Nature – PPSN X, pages 784–794, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. [54] Alma A. M. Rahat, Richard M. Everson, and Jonathan E. Fieldsend. Alternative inﬁll strategies for expensive multi-objective optimisation. In Proceedings of the Genetic and Evolutionary Computation Con- ference, GECCO ’17, page 873–880, New York, NY , USA, 2017. Association for Computing Machinery. ISBN 9781450349208. [55] Carl Edward Rasmussen. Gaussian Processes in Machine Learning , pages 63–71. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. [56] Jerry Segercrantz. Inclusion-exclusion and characteristic functions.Mathematics Magazine, 71(3):216–218, 1998. ISSN 0025570X, 19300980. URL http://www.jstor.org/stable/2691209. [57] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016. [58] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, page 1015–1022, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077. [59] J. Sylvester. Note sur la théorème de legendre. Comptes Rendus Acad. Sci., 96:463–465, 1883. [60] Ryoji Tanabe and Hisao Ishibuchi. An easy-to-use real-world multi-objective optimization problem suite. Applied Soft Computing, 89:106078, 2020. ISSN 1568-4946. doi: https://doi.org/10.1016/j.asoc.2020. 106078. [61] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. [62] Takashi Wada and Hideitsu Hino. Bayesian optimization for multi-objective optimization and multi-point search, 2019. [63] Jialei Wang, Scott C. Clark, Eric Liu, and Peter I. Frazier. Parallel bayesian global optimization of expensive functions, 2016. [64] J. T. Wilson, R. Moriconi, F. Hutter, and M. P. Deisenroth. The reparameterization trick for acquisition functions. ArXiv e-prints, December 2017. [65] James Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for bayesian optimization. In Advances in Neural Information Processing Systems 31, pages 9905–9916. 2018. [66] Jian Wu and Peter I. Frazier. The parallel knowledge gradient method for batch bayesian optimization. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, page 3134–3142, Red Hook, NY , USA, 2016. Curran Associates Inc. ISBN 9781510838819. [67] Kaifeng Yang, Michael Emmerich, André Deutz, and Carlos M. Fonseca. Computing 3-d expected hypervolume improvement and related integrals in asymptotically optimal time. In 9th International Conference on Evolutionary Multi-Criterion Optimization - Volume 10173, EMO 2017, page 685–700, Berlin, Heidelberg, 2017. Springer-Verlag. [68] Kaifeng Yang, Michael Emmerich, André H. Deutz, and Thomas Bäck. Efﬁcient computation of expected hypervolume improvement using box decomposition algorithms. CoRR, abs/1904.12672, 2019. [69] Kaifeng Yang, Michael Emmerich, André Deutz, and Thomas Bäck. Multi-objective bayesian global optimization using expected hypervolume improvement gradient. Swarm and Evolutionary Computation, 44:945 – 956, 2019. ISSN 2210-6502. doi: https://doi.org/10.1016/j.swevo.2018.10.007. URL http: //www.sciencedirect.com/science/article/pii/S2210650217307861. 13[70] Kaifeng Yang, Pramudita Palar, Michael Emmerich, Koji Shimoyama, and Thomas Bäck. A multi- point mechanism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. pages 656–663, 07 2019. doi: 10.1145/3321707.3321784. [71] Kaifeng Yang, Pramudita Satria Palar, Michael Emmerich, Koji Shimoyama, and Thomas Bäck. A multi-point mechanism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’19, page 656–663, New York, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450361118. doi: 10.1145/3321707.3321784. URL https://doi.org/10.1145/3321707.3321784. [72] R. J. Yang, N. Wang, C. H. Tho, J. P. Bobineau, and B. P. Wang. Metamodeling Development for Vehicle Frontal Impact Simulation. Journal of Mechanical Design, 127(5):1014–1020, 01 2005. [73] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V . G. da Fonseca. Performance assessment of multiobjective optimizers: an analysis and review. IEEE Transactions on Evolutionary Computation, 7(2): 117–132, 2003. 14Appendix to: Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization A Derivation of q-Expected Hypervolume Improvement A.1 Hypervolume Improvement via the Inclusion-Exclusion Principle The hypervolume improvement of f(x) within the hyper-rectangle Sk is the volume of Sk ∩∆({f(x)},P,r) and is given by: HVIk ( f(x),lk,uk ) = λM ( Sk ∩∆({f(x)},P,r) ) = M∏ m=1 [ z(m) k −l(m) k ] +, where u(m) k ,l(m) k ,f(m)(x), and z(m) k denote the mth component of the corresponding vector and [·]+ denotes the min(·,0) operation. Summing over all Sk gives the total hypervolume improvement: HVI ( f(x) ) = K∑ k=1 HVIk ( f(x),lk,uk ) = K∑ k=1 λM ( Sk ∩∆({f(x)},P,r) ) = K∑ k=1 M∏ m=1 [ z(m) k −l(m) k ] +. We can extend the HVI computation to the q >1 case using the inclusion-exclusion principle. Principle 1. The inclusion-exclusion principle[13, 59, 9] Given a ﬁnite measure space (B,A,µ) and a ﬁnite sequence of potentially empty or overlapping sets {Ai}i = 1n where Ai ∈A and µ(B) <∞, then, λM ( p⋃ i=1 Ai ) = p∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤p λM ( Ai1 ∩...∩Aij ) In the context of computing the joint HVI of q new points{f(xi)}q i=1, each subset Ai for i = 1,...,q is the set of points contained in ∆({f(xi)},P,r) — independently of the other q−1 points. λM (Ai) is the hypervolume improvement from the new point f(xi): λM (Ai) = HVI(f(xi)). The union of these subsets is the set of points in the new space dominated by the qnew points: ⋃q i=1 Ai = ⋃q i=1 ∆({f(xi)},P,r). The hypervolume of ⋃q i=1 ∆({f(xi)},P,r) is the hypervolume improvement from the qnew points: HVI({f(xi)}q i=1) = λM ( q⋃ i=1 Ai ) = q∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤q λM ( Ai1 ∩···∩ Aij ) To compute λM (Ai1 ∩···∩ Aij ), we partition the space covered by Ai1 ∩···∩ Aij across the K hyper- rectangles {Sk}K k=1 and compute the hypervolume of the overlapping space of Ai1 ∩···∩ Aij with each Sk independently. Since {Sk}K k=1 is a disjoint partition, summing overKgives the hypervolume ofAi1 ∩···∩ Aij : λM ( Ai1 ∩···∩ Aij ) = K∑ k=1 λM ( Sk ∩Ai1 ∩···∩ Aij ) This has two advantages. First, the new dominated space Ai can be a non-rectangular polytope, but the intersection Ai ∩Sk is a rectangular polytope, which simpliﬁes computation of overlapping hypervolume. 15Second, the vertices deﬁning the hyper-rectangle encapsulated by Sk ∩Ai1 ∩···∩ Aij are easily derived. The lower bound is simply the lk lower bound of Sk and the upper bound is the component-wise minimum zk,i1,...ij = min [ uk,f(xi1 ),..., f(xij ) ] . Importantly, this is computationally tractable because this speciﬁc approach enables parallelizing computation across all intersections of subsets Ai1 ∩···∩ Aij for 1 ≤ij ≤... ≤ij ≤qand across all Khyper-rectangles. Explicitly, the HVI is computed as: HVI({f(xi)}q i=1) = λM ( p⋃ i=1 Ai ) = q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Ai1 ∩···∩ Aij ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Sk ∩Ai1 ∩···∩ Aij ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Sk ∩∆({f(xi1 )},P,r) ∩... ∩∆({f(xij )},P,r) ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1 M∏ m=1 [ z(m) k,i1,...ij −l(m) k ] + = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj −l(m) k ] + where Xj is the superset all subsets of Xcand of size j: Xj = {Xj ⊂Xcand : |Xj|= j}and z(m) k,Xj = z(m) k,i1,...ij for Xj = {xi1 ,..., xij }. A.2 Computing Expected Hypervolume Improvement The above approach for computing HVI assumes we know the true objective values {f(xi)}q i=1. Since we do not know the true function values {f(xi)}q i=1, we compute qEHVI as the expectation over the GP posterior. αqEHVI = E [ HVI({f(xi)}q i=1) ] = ∫ RM HVI({f(xi)}q i=1)df (6) In the sequential setting and under the assumption of independent outcomes, qEHVI is simply EHVI and can be expressed in closed form [ 69]. However when q > 1, there is no known analytical formulation [70]. Instead, we estimate the expectation in (6) using MC integration with samples from the joint posterior P ( f(x1),..., f(xq)|D): αqEHVI = E [ HVI({f(xi)}q i=1) ] ≈ 1 N N∑ t=1 HVI({ft(xi)}q i=1) (7) = 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + (8) where {ft(xi)}q i=1 ∼P ( f(x1),..., f(xq)|X,Y ) is the tth sample from the joint posterior over Xcand and z(m) k,Xj,t = min [ uk,minx′∈Xj ft(x′) ] . A.3 Supporting Outcome Constraints Recall that we deﬁned the constrained hypervolume improvement as HVI C (f(x),c(x)) = HVI[f(x)] ·1 [c(x) ≥0]. (9) For q= 1 and assuming independence of the objectives and the constraints, the expected HVI C is the product of the expected HVI and the probability of feasibility (the expectation of 1 [c(x) ≥0]) [22]. However, requiring objectives and constraints to be independent is unnecessary when estimating the expectation with MC integration using samples from the joint posterior. 16In the parallel setting, if all constraints are satisﬁed for all q candidates Xcand = {xi}q i=1, HVI C is simply HVI . If a subset V⊂X cand,V̸= ∅ of the candidates violate at least one of the constraints, then the feasible HVI is the HVI of the set of feasible candidates: HVI C (Xcand) = HVI(Xcand \\V). That is, the hypervolume contribution (i.e. the marginal HVI) of an infeasible point is zero. In our formulation, HVI can be computed by multiplying (5) with an additional factor ∏ x′∈Xj ∏V v=1 1 [c(v)(x′) ≥0]: HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] . (10) The additional factor ∏ x′∈Xj ∏V v=1 1 [c(v)(xa) ≥0] indicates whether all constraints are satisﬁed for all candidates in a given subset Xj. Thus HVI C can be computed in the same fashion as HVI , but with the additional step of setting the HV of all subsets containing x′to zero if x′violates any constraint. We can now again perform MC integration as in (5) to compute the expected constrained hypervolume improvement. In this formulation, the marginal hypervolume improvement from a candidate is weighted by the probability that the candidate is feasible. The marginal hypervolume improvements are highly dependent on the outcomes of the other candidates. Importantly, the MC-based approach enables us to properly estimate the marginal hypervolume improvements across candidates by sampling from the joint posterior. Note that while the expected constrained hypervolume E [ HVI C ({f(xi),c(xi)}q i=1) ] is differentiable, we may not differentiate inside the expectation (hence we cannot expect simply differentiating (10) on the sample-level to provide proper gradients). We therefore replace the indicator with a sigmoid function with temperature parameter ϵ, which provides a differentiable relaxation 1 [c(v)(x′) ≥0] ≈s(c(v)(x′); ϵ) := 1 1 + exp(−c(v)(x′)/ϵ) (11) that becomes exact in the limit ϵ↘0. As in the unconstrained parallel scenario, there is no known analytical expression for the expected feasible hypervolume improvement. Therefore, we again use MC integration to approximate the expectation: αqEHVI C (x) = E [ HVI C ({f(xi),c(xi)}q i=1) ] (12a) ≈ 1 N N∑ t=1 HVI C ({ft(xi),ct(xi)}q i=1) (12b) ≈ 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 s(c(v)(x′); ϵ) ] (12c) A.3.1 Inclusion Exclusion principle for HVI C Equation (10) holds when the indicator function because HVI C is equivalent to HVI with the subset of feasible points. However, the sigmoid approximation can result in non-zero error. The error function ε: 2Xcand →R can be expressed as ε(X) = ∏ x′∈X V∏ v=1 1 [c(x′) >0] − ∏ x′∈X V∏ v=1 s(c(x′),ϵ) The error function gives a value to each to each element of 2Xcand . Weight functions have been studied in conjunction with the inclusion-exclusion principle [56], but under the assumption of that the weight of a set is the sum of the weights of its elements: w(A) = ∑ a∈A w(a). In our case, the weight function of a set Ais the product the weights of its elements. There, it is not obvious whether the inclusion-exclusion principle will hold in this case. Theorem 1. Given a feasible Pareto frontPfeas, a partitioning {(lk,uk}K k=1 of the objective space RM that is not dominated by the Pfeas, then for a set of points Xcand with objective values f(Xcand) and constraint values c(Xcand), HVI C (f(Xcand),c(Xcand),P,r) = HVI(f′(Xcand),P′,r′) where f′(Xcand) is the set of objective-constraint vectors for each candidate point f′(x) ∈RM+V , P′is the set of vectors [f(1)(x),...,f (M)(x),0V ] ∈RM+V , and r′= [r(1),...,r (M),0V ] ∈RM+V . Proof. Recall equation 10, HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] . 17Note that the constraint product ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] = V∏ v=1 ∏ x′∈Xj 1 [c(v)(x′) ≥0] = V∏ v=1 min x′∈Xj 1 [c(v)(x′) ≥0] = V∏ v=1 min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] = V∏ v=1 [ min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] −0 ] . (13) For v = 1 ,...,V , k = 1 ,...K, let l(M+v) k = 0 and u(M+v) k = 1 . Then, substituting into the following expression from Equation 13 gives min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] = min [ u(M+v) k , min x′∈Xj 1 [c(v)(x′) ≥0] ] Recall from Section 4, that zis deﬁned as: zk := min [ uk,f(x) ] . The high-level idea is that if we consider the indicator of the slack constraints 1 [c(v)(x′) ≥0] as objectives, then the above expression is consistent with the deﬁnition of zat the beginning of section 4. For v= 1,...,V , z(M+v) k,Xj = min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] Thus, ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] = V∏ v=1 [ min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] −0 ] = V∏ v=1 [ z(M+v) k,Xj −l(M+v) k ] + Returning to the HVI C equation, we have HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) M+V∏ v=M+1 [ z(v) k,Xj −l(M+v) k ] + ] = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [M+V∏ m=1 [ z(m) k,Xj −l(m) k ] + ] (14) Now consider the case when a sigmoid approximation 1 [c(v)(x′) ≥0] ≈s(c(v)(x′); ϵ) is used. The only change to Equation 14 is that z(m) k,Xj ≈ˆz(m) k,Xj = min [ u(M+v) k , min x′∈Xj S[c(v)(x′),ϵ] ] . If S[c(v)(x′),ϵ] = 1 [c(v)(x′) ≥0] for all v,x′, then HVI is computed exactly without approximation error. If S[c(v)(x′),ϵ]1 [c(v)(x′) ≥0] for any v,x′, then there is approximation error: the hypervolume improvement from all subsets containing x′is proportional to ∏V v=1 minx′∈X s(c(x′),ϵ). Since the constraint outcomes are directly considered as components in the hypervolume computation, the inclusion-exclusion principle incorporates the approximate indicator properly. 18A.4 Complexity Recall from Section 3.3 that, given posterior samples, the time complexity on a single-threaded machine is T1 = O(MNK(2q −1)). The space complexity required for maximum parallelism is also is T1 (ignoring the space required by the models), which does limit scalability to larger M and q, but difﬁculty scaling to large M is a known limitaiton of EHVI [69]. To reduce memory load, rectangles could be materialized and processed in chunks at the cost of additional runtime. In addition, our implementation of qEHVI uses the box decomposition algorithm from Couckuyt et al. [11], but we emphasize qEHVI is agnostic to the choice of partitioning algorithm and using a more efﬁcient partitioning algorithm (e.g. [69, 17, 41]) may signiﬁcantly improve memory footprint on GPU and enable larger using qin many scenarios. B Error Bound on Sequential Greedy Approximation If the acquisition function L(Xcand) is a normalized, monotone, submodular set function (where submodular means that the increase in L(Xcand) is non-increasing as elements are added to Xcand and normalized means that L(∅) = 0), then the sequential greedy approximation of Lenjoys regret of no more than 1 e L∗, where L∗is the optima of L[23]. We have αqEHVI (Xcand) = L(Xcand) = Ef ( HVI [ f(Xcand) ]) . Since HVI is a submodular set function [24] and the expectation of a stochastic submodular function is also submodular [ 2], αqEHVI (Xcand) is also submodular and therefore its sequential greedy approximation enjoys regret of no more than 1 e α∗ qEHVI . Using the result from Wilson et al. [65], the MC-based approximation ˆαqEHVI (Xcand) = ∑N t=1 HVI [ ft(Xcand) ] also enjoys the same regret bound since HVI is a normalized submodular set function.7 C Convergence Results For the purpose of stating our convergence results, we recall some concepts and notation from Balandat et al. [5]. First, consider a sample {ft(x1)}q i=1 from the multi-output posterior of the GP surrogate model. Let x∈Rqd be the stacked set of candidates Xcand and let ft(x) := [ft(x1)T ,...,f t(xq)T ]T be the stacked set of corresponding objective vectors. It is well known that, using the reparameterization trick, we can write ft(x) = µ(x) + L(x)ϵt, (15) where µ: Rqd →RqM is the mean function of the multi-output GP, L(x) ∈RqM×qM is a root decomposition (typically the Cholesky decomposition) of the multi-output GP’s posterior covarianceΣ(x) ∈RqM×qM , and ϵt ∈RqM with ϵt ∼N(0,IqM ). For x∈X , consider the MC-approximation ˆαN qEHVI (x) from (5). Denote by ∇x ˆαN qEHVI (x) the gradient of ˆαN qEHVI (x), obtained by averaging the gradients on the sample-level: ∇x ˆαN qEHVI (x) := 1 N N∑ t=1 ∇xHVI({ft(xi)}q i=1) (16) Let α∗ qEHVI := max x∈XαqEHVI (x) denote the maximum of the true acquisition function qEHVI , and let X∗:= arg maxx∈XαqEHVI (x) denote the set of associated maximizers. Theorem 2. Suppose that Xis compact and thatfhas a Multi-Output Gaussian Process prior with continuously differentiable mean and covariance functions. If the base samples {ϵt}N t=1 are drawn i.i.d. from N(0,IqM ), and if ˆx∗ N ∈arg maxx∈X ˆαN qEHVI (x), then (1) αqEHVI (ˆx∗ N ) →α∗ qEHVI a.s. (2) dist(ˆx∗ N ,X∗) →0 a.s. In addition to the almost sure convergence in Theorem 2, deriving a result on the convergence rate of the optimizer, similar to the one obtained in [5], should be possible. We leave this to future work. Moreover, the results in Theorem 2 can also be extended to the situation in which the base samples are generated using a particular class of randomized QMC methods (see similar results in [5]). Proof. We consider the setting from Balandat et al. [5, Section D.5]. Let ϵ ∼N(0,IqM ), so that we can write the posterior over outcome mat xas the random variable f(m)(x,ϵ) = S{ij,m}(µ(x) + L(x)ϵ), where µ(x) 7As noted in Wilson et al. [65], submodularity technically requires the search space Xto be ﬁnite, whereas in BO, it will typically be inﬁnite. Wilson et al. [65] note that in similar scenarios, submodularity has been extended to inﬁnite sets X(e.g. Srinivas et al. [58]). 19and L(x) are the (vector-valued) posterior mean and the Cholesky factor of posterior covariance, respectively, and S{ij,m}is an appropriate selection matrix (in particular, ∥S{ij,m}∥∞≤1 for all ij and m). Let A(x,ϵ) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj (ϵ) −l(m) k ] + where z(m) k,Xj (ϵ) = min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] and Xj = {xi1 ,..., xij }. Following [5, Theorem 3], we need to show that there exists an integrable function ℓ: Rq×M ↦→R such that for almost every ϵand all x,y⊆X,x,y∈Rq×d, |A(x,ϵ) −A(y,ϵ)|≤ ℓ(ϵ)∥x−y∥. (17) Let us deﬁne ˜akmjXj (x,ϵ) := [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + . Linearity implies that it sufﬁces to show that this condition holds for ˜A(x,ϵ) := M∏ m=1 ˜akmjXj (x,ϵ) = M∏ m=1 [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + (18) for all k, j, and Xj. Observe that ˜akmjXj (x,ϵ) ≤ ⏐⏐⏐min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ⏐⏐⏐ ≤|l(m) k |+ ⏐⏐⏐min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ]⏐⏐⏐. Note that if u(m) k = ∞, then min[u(m) k ,f(x,ϵ)(m) i1 ,...f(m)(xij ,ϵ)] = min[f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)]. If u(m) k < ∞, then min[u(m) k ,f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)] < ⏐⏐min[f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)] ⏐⏐+ ⏐⏐u(m) k ⏐⏐. Let w(m) k = u(m) k if u(m) k <∞and 0 otherwise. Then ˜akmjXj (x,ϵ) ≤|l(m) k |+ |w(m) k |+ ⏐⏐min [ f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ]⏐⏐ ≤|l(m) k |+ |w(m) k |+ ∑ i1,...,ij ⏐⏐f(m)(xij ,ϵ) ⏐⏐. We therefore have that |˜akmjXj (x,ϵ)|≤| l(m) k |+ |w(m) k |+ |Xj| ( ∥µ(m)(x)∥+ ∥L(m)(x)∥∥ϵ∥ ) for all k,m,j,X j, where |Xj|denotes the cardinality of the set Xj. Under our assumptions (compactness of X, continuous differentiability of mean and covariance function), both µ(x) and L(x), as well as their respective gradients w.r.t. x, are uniformly bounded. In particular there exist C1,C2 <∞such that |˜akmjXj (x,ϵ)|≤ C1 + C2∥ϵ∥ for all k,m,j,X j. Dropping indices k,j,X j for simplicity, observe that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐= ⏐⏐˜a1(x,ϵ)˜a2(x,ϵ) −˜a1(y,ϵ)˜a2(y,ϵ) ⏐⏐ (19a) = ⏐⏐˜a1(x,ϵ) ( ˜a2(x,ϵ) −˜a2(y,ϵ) ) + ˜a2(y,ϵ) ( ˜a1(x,ϵ) −˜a1(y,ϵ) )⏐⏐ (19b) ≤|˜a1(x,ϵ)| ⏐⏐˜a2(x,ϵ) −˜a2(y,ϵ) ⏐⏐+ |˜a2(y,ϵ)| ⏐⏐˜a1(x,ϵ) −˜a1(y,ϵ) ⏐⏐. (19c) Furthermore, |˜akmjXj (x,ϵ) −˜akmjXj (y,ϵ)|≤ ∑ i1,...,ij ⏐⏐S{ij,m}(µ(x) + L(x)ϵ) −S{ij,m}(µ(y) + L(y)ϵ) ⏐⏐ ≤|Xj| ( ∥µ(x) −µ(y)∥+ ∥L(x) −L(y)∥∥ϵ∥ ) . Since µand Lhave uniformly bounded gradients, they are Lipschitz. Therefore, there exist C3,C4 <∞such that |˜akmjXj (x,ϵ) −˜akmjXj (y,ϵ)|≤ (C3 + C4∥ϵ∥)∥x−y∥ 20for all x,y,k,m,j,X j. Plugging this into (19) above, we ﬁnd that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐≤2 ( C1C3 + (C1C4 + C2C3)∥ϵ∥+ C2C4∥ϵ∥2 ) ∥x−y∥ for all x,yand ϵ. For M > 2 we generalize the idea from (19), making sure to telescope the respective expressions. It is not hard to see that with this, there exist C <∞such that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐≤C M∑ m=1 ∥ϵ∥m∥x−y∥ Letting ℓ(ϵ) := C∑M m=1 ∥ϵ∥m, we observe that ℓ(ϵ) is integrable (since all absolute moments exist for the Normal distribution). The result now follows from in Balandat et al. [5, Theorem 3]. Besides the above convergence result, we can also show that the sample average gradient of the MC approximation of qEHVI is an unbiased estimator of the true gradient of qEHVI: Proposition 1. Suppose that the GP mean and covariance function are continuously differentiable. Suppose further that the candidate set xhas no duplicates, and that the sample-level gradients ∇xHVI({ft(xi)}q i=1) are obtained using the reparameterization trick as in [5]. Then E [ ∇x ˆαN qEHVI (x) ] = ∇xαqEHVI (x), (20) that is, the averaged sample-level gradient is an unbiased estimate of the gradient of the true acquisition function. Proof. This proof follows the arguments Wang et al.[63, Theorem 1], which leverages Glasserman[31, Theorem 1]. We verify the conditions of Glasserman [31, Theorem 1] below. Using the arguments from [5], we know that, under the assumption of differentiable mean and covariance functions, the samples ft(x) are continuously differentiable w.r.t. x(since there are no duplicates, and thus the covariance Σ(x) is non-singular). Hence, Glasserman [31, A1] is satisﬁed. Furthermore, it is easy to see from(1) that HVI({f(xi)}q i=1) is a.s. continuous and is differentiable w.r.t. ft(x) on RM , except on the edges of the hyper-rectangle decomposition {Sk}K k=1 of the non-dominated space, which satisﬁes [31, A3]. The set of points deﬁned by the union of these edges clearly has measure zero under any non-degenerate (non-singular covariance) GP posterior on RM , so Glasserman [31, A4] holds. Therefore Glasserman [31, Lemma 2] holds, so HVI({f(xi)}q i=1) is a.s. piece-wise differentiable w.r.t. x. Lastly, we need to show that the result in Glasserman [31, Lemma 3] holds: E [ sup xci /∈˜D |A′(x,ϵ)| ] <∞. As in Wang et al.[63, Theorem 1], we ﬁx xexcept for xci where xci is the cth component of the ith point, We need to show that E [ supxci /∈˜D |A′(x,ϵ)| ] <∞. By linearity, it sufﬁces to show that E [ supxci /∈˜D |˜A′(x,ϵ)| ] <∞. We have E [ sup xci /∈˜D |˜A′(x,ϵ)| ] = E [ sup xci /∈˜D ⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐ ] . Consider the M = 2 case. We have ˜A(x,ϵ) = a1(x,ϵ)a2(x,ϵ), where am(x,ϵ) = [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + . The partial derivative of ˜A(x,ϵ) with respect to xci is ∂˜A(x,ϵ) ∂xci = ∂a1(x,ϵ) ∂xci a2(x,ϵ) + a1(x,ϵ)∂a2(x,ϵ) ∂xci , and therefore ⏐⏐⏐∂˜A(x,ϵ) ∂xci ⏐⏐⏐≤ ⏐⏐⏐∂a1(x,ϵ) ∂xci ⏐⏐⏐· ⏐⏐⏐a2(x,ϵ) ⏐⏐⏐+ ⏐⏐⏐a1(x,ϵ) ⏐⏐⏐· ⏐⏐⏐∂a2(x,ϵ) ∂xci ⏐⏐⏐ Since we are only concerned with xci /∈ ˜D, am(x,ϵ) = [ min [ f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(1) k ] + . 21As in the proof of Theorem 2, we write the posterior over outcome mat xas the random variable f(m)(x,ϵ) = S{ij,m}(µ(x) + L(x)ϵ), where ϵ ∼N(0,IqM ) and S{ij,m}is an appropriate selection matrix. With this, am(x,ϵ) = [ min [ S{i1,1} ( µ(x) + L(x)ϵ ) ,...,S {ij,1} ( µ(x) + L(x)ϵ )] −l(1) k ] + . Since the interval X is compact and the mean, covariance, and Cholesky factor of the covariance µ(x),C(x),L(x) are continuously differentiable, for all mwe have sup xci ⏐⏐⏐⏐ ∂µ(m)(xa) ∂xci ⏐⏐⏐⏐= µ∗,(m) a <∞, sup xci ⏐⏐⏐⏐ ∂L(m)(x) ∂xci ⏐⏐⏐⏐= L∗,(m) ca <∞. Let µ(m) ∗∗ = maxa µ∗,(m) a , L(m) ∗∗ = maxa,b L∗,(m) ab (x), where L(m) ab is the element at row a, column bin L(m), the Cholesky factor for outcome m. Let ϵ(m) ∈Rq denote the vector of i.i.d. N(0,1) samples corresponding to outcome m. Then we have⏐⏐⏐⏐ ∂ ∂xci [ [min [ S{i1,1} ( µ(x) + L(x)ϵ ) ,...,S {ij,1} ( µ(x) + L(x)ϵ )] −l(1) k ] + ⏐⏐⏐⏐ ≤ ⏐⏐⏐ [ µ(m) ∗∗ + L(m) ∗∗ ||ϵ(m)||1 −l(m) k ] + ⏐⏐⏐ ≤ ⏐⏐⏐µ(m) ∗∗ + L(m) ∗∗ ||ϵ(m)||1 ⏐⏐⏐+ ⏐⏐⏐l(m) k ⏐⏐⏐. Under our assumptions (compactness of X, continuous differentiability of mean and covariance function) both µ(x) and L(x), as well as their respective gradients, are uniformly bounded. In particular there exist C(m) 1 ,C(m) 2 <∞such that ⏐⏐S{a,m} ( µ(x) + L(x)ϵ ) −l(m) k ⏐⏐≤C(m) 1 + C(m) 2 ||ϵ(m)||1 for all a= i1,...,i j. Hence, ⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐≤ [⏐⏐⏐µ(1) ∗∗ + C(1) ∗∗||ϵ(1)||1 ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ][ C(2) 1 + C(2) 2 ||ϵ(2)||1 ] + [ C(1) 1 + C(1) 2 ||ϵ(1)||1 ][⏐⏐⏐µ(2) ∗∗ + C(2) ∗∗||ϵ(2)||1 ⏐⏐⏐+ ⏐⏐⏐l(2) k ⏐⏐⏐ ] Since ϵis absolutely integrable, E (⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐ ) <∞. Hence, E [ supxci /∈˜D |A′(x,ϵ)| ] <∞. This can be extended to M >2 in the same manner using the product rule to obtain E (∂˜A(x,ϵ) ∂xci ) ≤ M∑ m=1 ([⏐⏐⏐µ(m) ∗∗ + C(m) ∗∗ E[||ϵ(m)||1] ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ] M∏ n=1,n̸=m [ C(n) 1 + C(n) 2 E[||ϵ(n)||1] ]) ≤ M∑ m=1 ([⏐⏐⏐µ(m) ∗∗ + π 2 qC(m) ∗∗ ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ] M∏ n=1,n̸=m [ C(n) 1 + π 2 qC(n) 2 ] ]) . Hence, E [ supxci /∈˜D |A′(x,ϵ)| ] <∞for M ≥2 and Glasserman [31, Theorem 1] holds. 22D Monte-Carlo Approximation Figure 5b shows the gradient of analytic EHVI and the MC estimator qEHVI on slice of a 3-objective problem. Even using only N = 32 QMC samples, the average sample gradient has very low variance. Moreover, ﬁxing the base samples also greatly reduces the variance without introducing bias. 0.0 0.1EHVI MC,/uni00A0N=32 analytic qMC,/uni00A0N=32 analytic 0.00 0.25 0.50 0.75 1.00 0.0 0.1EHVI MC,/uni00A0N=32/uni00A0(fixed) analytic 0.00 0.25 0.50 0.75 1.00 qMC,/uni00A0N=32/uni00A0(fixed) analytic (a) A comparison of the analytic EHVI acquisition function and the MC-based qEHVI for q= 1. 0.1 0.0 0.1 /uni00A0EHVI MC,/uni00A0N=32 analytic qMC,/uni00A0N=32 analytic 0.00 0.25 0.50 0.75 1.00 0.1 0.0 0.1 /uni00A0EHVI MC,/uni00A0N=32/uni00A0(fixed) analytic 0.00 0.25 0.50 0.75 1.00 qMC,/uni00A0N=32/uni00A0(fixed) analytic (b) A comparison of the exact gradient of analytic EHVI and the exact sample average gradient of the MC-based qEHVI for q= 1. Figure 5: A comparison of (a) the analytic EHVI and MC-based qEHVI for q = 1 and (b) a comparison of the exact gradient ∇αEHVI of analytic EHVI and average sample gradient of the MC-estimator ∇ˆαqEHVI over a slice of the input space on a DTLZ2 problem (q= 1, M = 3, d= 6) [15]. x(0) is varied across 0 ≤λ≤1, while x(i) for 1,...D are held constant. In each of (a) and (b), the top row show qEHVI where the (quasi-)standard normal base samples are resampled for each value of x(0). The solid line is one sample average (across (q)MC samples) and the shaded area is the mean plus 2 standard errors across 50 repetitions. The bottom row uses the same base samples for evaluating each test point and the sample average for each of 50 repetitions is plotted. E Experiment Details E.1 Algorithms For TS-TCH, we draw a sample from the joint posterior over a discrete set of 1000dpoints sampled from a scrambled Sobol sequence. For PESMO, we follow [27] and use a Pareto set of size 10 for each sampled GP, which is optimized over a discrete set of 1000dpoints sampled from a scrambled Sobol sequence. The current 23Table 2: Reference points for all benchmark problems. Assuming minimization. In our benchmarks, equivalently maximize the negative objectives and multiply the reference points by -1. PROBLEM REFERENCE POINT BRANIN CURRIN (18.0, 6.0) DTLZ2 (1.1,..., 1.1) ∈RM ABR (-150.0, 3500.0, 5.1) VEHICLE CRASH SAFETY (1864.72022, 11.81993945, 0.2903999384) CONSTRAINED BRANIN CURRIN (90.0, 10.0) C2-DTLZ2 (1.1,..., 1.1) ∈RM Pareto front is approximated by optimizing the posterior means over a grid as is done in Garrido-Merchán and Hernández-Lobato [26, 27]. For SMS-EGO, we use the observed Pareto front. All acquisition functions are optimized with L-BFGS-B (with a maximum of 200 iterations); SMS-EGO [53] and PESMO [26] use gradients approximated by ﬁnite differences and all other methods use exact gradients. For all methods, each outcome is modeled with an independent Gaussian process with a Matern 5/2 ARD kernel. The methods implemented in Spearmint use a fully Bayesian treatment of the hyperparameters with 10 samples from posterior over the hyperparamters, and the methods implemented in BoTorch use maximum a posteriori estimates of the GP hyperparameters. All methods are initialized with 2(d+ 1)points from a scrambled Sobol sequence. qPAREGO and qEHVI use N = 128 QMC samples. E.1.1 Reference point speciﬁcation There is a large body of literature on the effects of reference point speciﬁcation [4, 35, 36]. The hypervolume indicator is sensitive to speciﬁed the reference point: a reference point that is far away from the Pareto front will favor extreme points, where as reference point that is close to the Pareto front gives more weight to less extreme points [36]. Sensitivity to the reference point is affects both the evaluation of different MO methods and the utility function for methods that rely HV. In practice, a decision maker may be able to specify a reference point that satisﬁes their preference with domain knowledge. If a reference point is provided by the decision maker, previous work has suggested heuristics for choosing reference points for use in an algorithm’s utility function [35, 53]. We follow previous work [69, 68] and assume that the reference point is known. We also considered (but did not use in our experiments) a dynamic reference point strategy where at each BO iteration, the reference point is selected to be a point slightly worse than the nadir (component-wise minimum) point of the current observed Pareto front for computing the acquisition function: r = ynadir −0.1 ·|ynadir| where ynadir = ( miny(1)∈D(1) y(1),..., miny(m)∈D(m) y(m)) . This reference point is used in SMS-EMOA in Ishibuchi et al. [35]), and we ﬁnd similar average performance (but higher variance) on problems to using a known reference point with continuous Pareto fronts. If the Pareto front is discontinuous, then it is possible not all sections of the Pareto front will be reached. E.1.2 qPAREGO Previous work has only considered unconstrained sequential optimization with ParEGO [40, 7] and ParEGO is often optimized with gradient-free methods [ 53]. To the best of our knowledge, qPAREGO is the ﬁrst to support parallel and constrained optimization. Moreover, we compute exact gradients via auto-differentiation for acquisition optimization. ParEGO is typically implemented by applying augmented Chebyshev scalarization and modeling the scalarized outcome [40]. However, recent work has shown that composite objectives offer improved optimization performance [3]. qPAREGO uses a MC-based Expected Improvement [38] acquisition function, where the objectives are modeled independently and the augmented Chebyshev scalarization [40] is applied to the posterior samples as a composite objective. This approach enables the use of sequential greedy optimization of qcandidates with proper integration over the posterior at the pending points. Importantly, the sequential greedy approach allows for using different random scalarization weights for selecting each of the q candidates. qPAREGO is extended to the constrained setting by weighting the EI by the probability of feasibility [25]. We estimate the probability of feasiblity using the posterior samples and approximate the indicator function with a sigmoid to maintain differentiablity as in constrained qEHVI . qPAREGO is trivially extended to the noisy setting using Noisy Expected Improvement [43, 5], but we use Expected Improvement in our experiments as all of the problems are noiseless. E.2 Benchmark Problems The details for the benchmark problems below assume minimization of all objectives. Table 2 provides the reference points used for all benchmark problems. 24Branin-Currin f(1)(x′ 1,x′ 2) = (x2 −5.1 4π2 x2 1 + 5 πx1 −r)2 + 10(1 − 1 8π) cos(x1) + 10 f(2)(x1,x2) = [ 1 −exp ( − 1 (2x2) )]2300x3 1 + 1900x2 1 + 2092x1 + 60 100x3 1 + 500x2 1 + 4x1 + 20 where x1,x2 ∈[0,1], x′ 1 = 15x1 −5, and x′ 2 = 15x2. The constrained Branin-Currin problem uses the following disk constraint from [29]: c(x′ 1,x′ 2) = 50 −(x′ 1 −2.5)2 −(x′ 2 −7.5)2) ≥0 DTLZ2 The objectives are given by [15]: f1(x) = (1 + g(xM )) cos (π 2 x1 ) ··· cos (π 2 xM−2 ) cos (π 2 xM−1 ) f2(x) = (1 + g(xM )) cos (π 2 x1 ) ··· cos (π 2 xM−2 ) sin (π 2 xM−1 ) f3(x) = (1 + g(xM )) cos (π 2 x1 ) ··· sin (π 2 xM−2 ) ... fM (x) = (1 + g(xM )) sin (π 2 x1 ) where g(x) = ∑ xi∈xM (xi −0.5)2,x∈[0,1]d,and xM represents the last d−M + 1 elements of x. The C2-DTLZ2 problem adds the following constraint [16]: c(x) = −min [ M min i=1 ( (fi(x) −1)2 + M∑ j=1,j=i (f2 j −r2) ) , ( M∑ i=1 ( (fi(x) − 1√ M )2 −r2))] ≥0 Vehicle Crash Safety The objectives are given by [60]: f1(x) = 1640.2823 + 2.3573285x1 + 2.3220035x2 + 4.5688768x3 + 7.7213633x4 + 4.4559504x5 f2(x) = 6.5856 + 1.15x1 −1.0427x2 + 0.9738x3 + 0.8364x4 −0.3695x1x4 + 0.0861x1x5 + 0.3628x2x4 + 0.1106x2 1 −0.3437x2 3 + 0.1764x2 4 f3(x) = −0.0551 + 0.0181x1 + 0.1024x2 + 0.0421x3 −0.0073x1x2 + 0.024x2x3 −0.0118x2x4 −0.0204x3x4 −0.008x3x5 −0.0241x2 2 + 0.0109x2 4 where x∈[1,3]5. Policy Optimization for Adaptive Bitrate Control The controller is given by: at = x0 ˆzbd,t + x2zbf,t + x3, where ˆzbd,t = ∑ ti<t zbd,ti exp(−x1ti) ∑ ti<t exp(−x1ti) is estimated bandwidth at time tusing an exponential moving average, zbf,t is the buffer occupancy at time t, and x0,...x3 are the parameters we seek to optimize. We evaluate each policy on a set of 400 videos, where the number of time steps (chunks) in each video stream trajectory depends on the size of the video. 25Table 3: Acquisition Optimization wall time in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and on a GPU (Tesla V100-SXM2-16GB). The mean and two standard errors are reported. NA indicates that the algorithm does not support constraints. CPU CONSTRAINED BRANIN CURRIN DTLZ2 PESMO ( q=1) NA 278.53 (±25.66) SMS-EGO ( q=1) NA 104.26 (±7.66) TS-TCH ( q=1) NA 52.55 (±0.06) qPAREGO ( q=1) 2.4 (±0.37) 4 .68 (±0.46) EHVI ( q=1) NA 3.58 (±0.28) qEHVI ( q=1) 5.69 (±0.43) 5 .95 (±0.45) GPU CONSTRAINED BRANIN CURRIN DTLZ2 TS-TCH ( q=1) NA 0.25 (±0.00) TS-TCH ( q=2) NA 0.27 (±0.00) TS-TCH ( q=4) NA 0.28 (±0.00) TS-TCH ( q=8) NA 0.32 (±0.01) qPAREGO ( q=1) 3.52 (±0.34) 9 .04 (±0.93) qPAREGO ( q=2) 6.0 (±0.56) 14 .23 (±1.55) qPAREGO ( q=4) 12.07 (±0.98) 40 .5 (±3.21) qPAREGO ( q=8) 33.1 (±3.32) 84 .15 (±6.9) EHVI ( q=1) NA 84.15 (±6.9) qEHVI ( q=1) 5.61 (±0.17) 10 .21 (±0.58) qEHVI ( q=2) 19.06 (±5.88) 17 .75 (±0.97) qEHVI ( q=4) 29.26 (±2.01) 40 .41 (±2.78) qEHVI ( q=8) 91.56 (±5.51) 106 .51 (±7.69) F Additional Empirical Results F.1 Additional Sequential Optimization Results We include results for an additional synthetic benchmark: the DTLZ2 problem from the MO literature [ 15] (d= 6,M = 2). Figure 6 shows that qEHVI outperforms all other baseline algorithms on the DTLZ2 in terms of sequential optimization performance with competitive wall times as shown in 3. 0 20 40 60 80 100 Function/uni00A0Evaluations 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO Figure 6: Optimization performance on the DTLZ2 synthetic function (d= 6,M = 2). F.2 Performance with Increasing Parallelism Figure 7 shows that that the performance of qEHVI performance does not degrade substantially, whereas performance does degrade for qPAREGO and TS-TCH on some benchmark problems. We include results for all problems in Section 5 and Appendix F.1 as well as a Constrained Branin-Currin problem (which is described in Appendix E.2). 260 20 40 60 80 100 Batch/uni00A0Iteration 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) VEHICLE SAFETY 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) VEHICLE SAFETY 0 20 40 60 80 100 Batch/uni00A0Iteration 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (c) C2DTLZ2 0 20 40 60 80 100 Function/uni00A0Evaluations 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (d) C2DTLZ2 0 20 40 60 80 100 Batch/uni00A0Iteration 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (e) BRANIN CURRIN 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (f) BRANIN CURRIN Figure 7: Optimization performance of parallel acquisition functions over batch BO iterations (left) and function evaluations (right) for benchmark problems in Section 5. 270 20 40 60 80 100 Batch/uni00A0Iteration 0.5 1.0 1.5 2.0 2.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) CONSTRAINED BRANIN CURRIN 0 20 40 60 80 100 Function/uni00A0Evaluations 0.5 1.0 1.5 2.0 2.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) CONSTRAINED BRANIN CURRIN 0 20 40 60 80 100 Batch/uni00A0Iteration 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (c) DTLZ2 ( M = 2,d = 6) 0 20 40 60 80 100 Function/uni00A0Evaluations 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (d) DTLZ2 ( M = 2,d = 6) Figure 8: Optimization performance of parallel acquisition functions over batch BO iterations (left) and function evaluations (right) for additional benchmark problems. 28F.3 Noisy Observations Although neither qEHVI nor any variant of expected hypervolume improvement (to our knowledge) directly account for noisy observations, noisy observations are a practical challenge. We empirically evaluate the performance of all algorithms on a Branin-Currin function where observations have additive, zero-mean, iid Gaussian noise; the unknown standard deviation of the noise is set to be 1% of the range of each objective. Fig 9 shows that qEHVI performs favorably in the presence of noise, besting all algorithms including Noisy qPAREGO (qNParego) (described in Appendix E.1.2), PESMO and TS-TCH, all of which account for noise. 0 20 40 60 80 100 Function/uni00A0Evaluations 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8log/uni00A0HV/uni00A0Difference Sobol EHVI qEHVI qParEGO qNParEGO TS/uni00ADTCH Figure 9: Sequential optimization performance on a noisy Branin-Currin problem. F.4 Approximate Box Decompositions EHVI becomes prohibitively computationally expensive in many scenarios with ≥4 objectives because of the wall time of partitioning the non-dominated space into disjoint rectangles [ 11]. Therefore, in addition to providing an exact binary partitioning algorithm, Couckuyt et al. [11] propose an approximation that terminates the partitioning algorithm when the new additional set of hyper-rectangles in the partitioning has a total hypervolume of less than a predetermined fraction ζof the hypervolume dominated by the Pareto front. While qEHVI is guaranteed to be exact when an exact partitioning of the non-dominated space is used, qEHVI is agnostic to the partitioning algorithm used and is compatible with more scalable approximate methods. We evaluate the performance ofqEHVI with approximation of various ﬁdelities ζon DTLZ2 problems with 3 and 4 objectives (with d = 6 ). ζ = 0 corresponds to an exact partitioning and the approximation is monotonically worse as ζincreases. Larger values of ζdegrade optimization performance (Figure 10), but can result in substantial speedups (Table 4). Even with coarser levels of approximation, qEHVI () performs better than qPAREGO with respect to log hypervolume difference, while achieving wall time improvements of 2-7x compared to exact qEHVI. 0 20 40 60 80 100 Function/uni00A0Evaluations 1.0 0.8 0.6 0.4 0.2 log/uni00A0HV/uni00A0Difference qEHVI/uni00A0( = 10 3) qEHVI/uni00A0( = 10 4) qEHVI/uni00A0( = 10 5) qEHVI/uni00A0( = 10 6) qEHVI/uni00A0(exact) qParEGO (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 0.7 0.6 0.5 0.4 0.3 0.2 0.1 log/uni00A0HV/uni00A0DifferenceqEHVI/uni00A0( = 10 3) qEHVI/uni00A0( = 10 4) qEHVI/uni00A0( = 10 5) qEHVI/uni00A0( = 10 6) qEHVI/uni00A0(exact) qParEGO (b) Figure 10: Optimization performance on DTLZ2 problems (d= 6) with approximate partitioning using various approximation levels ζfor (a) M = 3 objectives and (b) M = 4 objectives. 29CPU DTLZ2 ( M = 3) DTLZ2 ( M = 4) qPAREGO 5.86 (±0.51) 5 .6 (±0.53) qEHVI ( ζ = 10−3) 6.89 (±0.41) 9 .53 (±0.49) qEHVI ( ζ = 10−4) 9.83 (±0.9) 17 .47 (±1.2) qEHVI ( ζ = 10−5) 18.99 (±2.72) 60 .27 (±3.57) qEHVI ( ζ = 10−6) 37.9 (±7.47) 136 .15 (±12.88) qEHVI ( EXACT ) 45.52 (±9.83) 459 .33 (±77.95) Table 4: Acquisition function optimization wall time with approximate hypervolume computation, in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz). The mean and two standard errors are reported. F.5 Acquisition Computation Time Figure 11 show the acquisition computation time for different M and q. The inﬂection points corresponds to available processor cores becoming saturated. For large M an qon the GPU, memory becomes an issue, but we discuss ways of mitigating the issue in Appendix A.4. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 q 0 1 2 3 4 5 6Acquisition/uni00A0Computation/uni00A0Time/uni00A0(s) M=2/uni00A0(CPU) M=3/uni00A0(CPU) M=4/uni00A0(CPU) M=2/uni00A0(GPU) M=3/uni00A0(GPU) M=4/uni00A0(GPU) Figure 11: Acquisition computation time for different batch sizes qand numbers of objectives M (this excludes the time required to compute the acquisition function given box decomposition of the non-dominated space). This uses N = 512 MC samples, d= 6, |P|= 10, and 20 training points. CPU time was measured on 2x Intel Xeon E5-2680 v4 @ 2.40GHz and GPU time was measured on a Tesla V100-SXM2-16GB GPU using 64-bit ﬂoating point precision. The mean and 2 standard errors over 1000 trials are reported. 30",
      "meta_data": {
        "arxiv_id": "2006.05078v3",
        "authors": [
          "Samuel Daulton",
          "Maximilian Balandat",
          "Eytan Bakshy"
        ],
        "published_date": "2020-06-09T06:57:47Z",
        "venue": "Advances in Neural Information Processing Systems 33, 2020",
        "pdf_url": "https://arxiv.org/pdf/2006.05078v3.pdf",
        "github_url": "https://github.com/pytorch/botorch"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the problem of efficiently optimizing multiple competing objectives in parallel Bayesian Optimization, specifically tackling the high computational complexity and lack of analytic gradients for Expected Hypervolume Improvement (EHVI). Its main contributions are: introducing q-Expected Hypervolume Improvement (qEHVI), a novel, exact (up to MC error) acquisition function for parallel and constrained multi-objective BO; enabling efficient optimization by computing exact gradients of the Monte-Carlo estimator via auto-differentiation and leveraging the Sample Average Approximation (SAA) approach; demonstrating its computational tractability and superior performance (both in optimization outcome and wall time) over state-of-the-art methods on various synthetic and real-world benchmarks; and extending its applicability to scenarios with auxiliary outcome constraints and higher-dimensional objective spaces (M>2).",
        "methodology": "The methodology centers on a novel formulation of q-Expected Hypervolume Improvement (qEHVI). It leverages the inclusion-exclusion principle to compute joint HVI for q candidate points, partitioning the non-dominated space into disjoint hyper-rectangles. For Expected HVI, it employs Monte-Carlo (MC) integration over the Gaussian Process (GP) posterior, using randomized quasi MC for variance reduction. Key to its efficiency is the computation of exact gradients of this MC estimator via auto-differentiation and the re-parameterization trick, enabling the use of deterministic, higher-order optimizers through the Sample Average Approximation (SAA) approach, which provides theoretical convergence guarantees. The method is extended to handle auxiliary outcome constraints by incorporating feasibility-weighting on the sample level and using a differentiable sigmoid approximation for the indicator function. It supports both joint batch optimization and a proper sequential greedy approach that integrates over the posterior of unobserved outcomes.",
        "experimental_setup": "The experimental setup evaluates qEHVI against state-of-the-art multi-objective BO algorithms (SMS-EGO, PESMO, TS-TCH, analytic EHVI, and a novel qPAREGO extension) and a quasi-random baseline. Each outcome is modeled with an independent Gaussian Process with a Matern 5/2 ARD kernel, with hyperparameters estimated via MAP or Bayesian treatment. All methods are initialized with 2(d+1) Sobol sequence points. qEHVI and qPAREGO use N=128 Quasi-Monte Carlo samples. Acquisition functions are optimized using L-BFGS-B, with exact gradients for qEHVI and qPAREGO, and finite differences for SMS-EGO and PESMO. Performance is measured by log hypervolume difference or hypervolume indicator. Benchmarks include synthetic problems (Branin-Currin, C2-DTLZ2, DTLZ2 with 2-4 objectives, and noisy Branin-Currin) and real-world problems (Vehicle Crash Safety design and Adaptive Bitrate Control policy optimization). Experiments are conducted on both CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and GPU (Tesla V100-SXM2-16GB) hardware, with results reported as means and 2 standard errors over 20 trials. Approximate box decomposition with varying fidelity ζ is also tested for higher-dimensional objectives.",
        "limitations": "The main limitations of qEHVI are its current assumption of noiseless observations, which is a common constraint for EHVI formulations. Its scalability is also limited by the underlying partitioning algorithm's computational complexity, particularly in high-dimensional objective spaces (M ≥ 4), where the number of required boxes scales super-polynomially. Furthermore, for large M and q on GPUs, memory can become an issue.",
        "future_research_directions": "Future research directions include extending qEHVI to account for noisy observations by integrating uncertainty from previous observations. Significant improvements can be made by integrating more scalable and efficient partitioning algorithms (both exact and approximate) to enhance performance in high-dimensional objective spaces and optimize memory usage. Further theoretical work could focus on deriving convergence rates for the optimizer and extending existing convergence results to include specific randomized Quasi-Monte Carlo methods. Generally, the authors hope this work encourages the application of modern computational paradigms and tooling to advance Bayesian Optimization.",
        "experimental_code": "class qLogExpectedHypervolumeImprovement(MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin):_log: bool = Truedef __init__(self,model: Model,ref_point: list[float] | Tensor,partitioning: NondominatedPartitioning,sampler: MCSampler | None = None,objective: MCMultiOutputObjective | None = None,constraints: list[Callable[[Tensor], Tensor]] | None = None,X_pending: Tensor | None = None,eta: Tensor | float = 1e-2,fat: bool = True,tau_relu: float = TAU_RELU,tau_max: float = TAU_MAX,) -> None:r\"\"\"Parallel Log Expected Hypervolume Improvement supporting m>=2 outcomes.See [Ament2023logei]_ for details and the methodology behind the LogEI family ofacquisition function. Line-by-line differences to the original differentiableexpected hypervolume formulation of [Daulton2020qehvi]_ are described via inlinecomments in `forward`.Example:>>> model = SingleTaskGP(train_X, train_Y)>>> ref_point = [0.0, 0.0]>>> acq = qLogExpectedHypervolumeImprovement(model, ref_point, partitioning)>>> value = acq(test_X)Args:model: A fitted model.ref_point: A list or tensor with `m` elements representing the referencepoint (in the outcome space) w.r.t. to which compute the hypervolume.This is a reference point for the objective values (i.e. afterapplying`objective` to the samples).partitioning: A `NondominatedPartitioning` module that provides the non-dominated front and a partitioning of the non-dominated space in hyper-rectangles. If constraints are present, this partitioning must onlyinclude feasible points.sampler: The sampler used to draw base samples. If not given,a sampler is generated using `get_sampler`.objective: The MCMultiOutputObjective under which the samples are evaluated.Defaults to `IdentityMultiOutputObjective()`.constraints: A list of callables, each mapping a Tensor of dimension`sample_shape x batch-shape x q x m` to a Tensor of dimension`sample_shape x batch-shape x q`, where negative values implyfeasibility. The acquisition function will compute expected feasiblehypervolume.X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that havepoints that have been submitted for function evaluation but have not yetbeen evaluated. Concatenated into `X` upon forward call. Copied and setto have no gradient.eta: The temperature parameter for the sigmoid function used for thedifferentiable approximation of the constraints. In case of a float thesame eta is used for every constraint in constraints. In case of a`tensor the length of the tensor must match the number of providedconstraints. The i-th constraint is then estimated with the i-theta value.fat: Toggles the logarithmic / linear asymptotic behavior of the smoothapproximation to the ReLU and the maximum.tau_relu: Temperature parameter controlling the sharpness of theapproximation to the ReLU over the `q` candidate points. For furtherdetails, see the comments above the definition of `TAU_RELU`.tau_max: Temperature parameter controlling the sharpness of theapproximation to the `max` operator over the `q` candidate points.For further details, see the comments above the definition of `TAU_MAX`.\"\"\"if len(ref_point) != partitioning.num_outcomes:raise ValueError(\"The dimensionality of the reference point must match the number of \"f\"outcomes. Got ref_point with {len(ref_point)} elements, but expected \"f\"{partitioning.num_outcomes}.\")ref_point = torch.as_tensor(ref_point,dtype=partitioning.pareto_Y.dtype,device=partitioning.pareto_Y.device,)super().__init__(model=model,sampler=sampler,objective=objective,constraints=constraints,eta=eta,X_pending=X_pending,)self.register_buffer(\"ref_point\", ref_point)cell_bounds = partitioning.get_hypercell_bounds()self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])SubsetIndexCachingMixin.__init__(self)self.tau_relu = tau_reluself.tau_max = tau_maxself.fat = fatdef _compute_log_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:r\"\"\"Compute the expected (feasible) hypervolume improvement given MC samples.Args:samples: A `sample_shape x batch_shape x q' x m`-dim tensor of samples.X: A `batch_shape x q x d`-dim tensor of inputs.Returns:A `batch_shape x (model_batch_shape)`-dim tensor of expected hypervolumeimprovement for each batch.\"\"\"obj = self.objective(samples, X=X) # mc_samples x batch_shape x q x mq = obj.shape[-2]if self.constraints is not None:log_feas_weights = compute_smoothed_feasibility_indicator(constraints=self.constraints,samples=samples,eta=self.eta,log=True,fat=self.fat,)device = self.ref_point.deviceq_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)batch_shape = obj.shape[:-2] # mc_samples x batch_shape# areas tensor is `mc_samples x batch_shape x num_cells x 2`-dimlog_areas_per_segment = torch.full(size=(*batch_shape,self.cell_lower_bounds.shape[-2], # num_cells2, # for even and odd terms),fill_value=-torch.inf,dtype=obj.dtype,device=device,)cell_batch_ndim = self.cell_lower_bounds.ndim - 2# conditionally adding mc_samples dim if cell_batch_ndim > 0# adding ones to shape equal in number to to batch_shape_ndim - cell_batch_ndim# adding cell_bounds batch shape w/o 1st dimensionsample_batch_view_shape = torch.Size([batch_shape[0] if cell_batch_ndim > 0 else 1,*[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],*self.cell_lower_bounds.shape[1:-2],])view_shape = (*sample_batch_view_shape,self.cell_upper_bounds.shape[-2], # num_cells1, # adding for q_choose_i dimensionself.cell_upper_bounds.shape[-1], # num_objectives)for i in range(1, self.q_out + 1):q_choose_i = q_subset_indices[f\"q_choose_{i}\"] # q_choose_i x i# this tensor is mc_samples x batch_shape x i x q_choose_i x mobj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))obj_subsets = obj_subsets.view(obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]) # mc_samples x batch_shape x q_choose_i x i x m# NOTE: the order of operations in non-log _compute_qehvi is 3), 1), 2).# since 3) moved above 1), _log_improvement adds another Tensor dimension# that keeps track of num_cells.# 1) computes log smoothed improvement over the cell lower bounds.# mc_samples x batch_shape x num_cells x q_choose_i x i x mlog_improvement_i = self._log_improvement(obj_subsets, view_shape)# 2) take the minimum log improvement over all i subsets.# since all hyperrectangles share one vertex, the opposite vertex of the# overlap is given by the component-wise minimum.# negative of maximum of negative log_improvement is approximation to min.log_improvement_i = self._smooth_min(log_improvement_i,dim=-2,) # mc_samples x batch_shape x num_cells x q_choose_i x m# 3) compute the log lengths of the cells' sides.# mc_samples x batch_shape x num_cells x q_choose_i x mlog_lengths_i = self._log_cell_lengths(log_improvement_i, view_shape)# 4) take product over hyperrectangle side lengths to compute area (m-dim).# after, log_areas_i is mc_samples x batch_shape x num_cellslog_areas_i = log_lengths_i.sum(dim=-1) # areas_i = lengths_i.prod(dim=-1)# 5) if constraints are present, apply a differentiable approximation of# the indicator function.if self.constraints is not None:log_feas_subsets = log_feas_weights.index_select(dim=-1, index=q_choose_i.view(-1)).view(log_feas_weights.shape[:-1] + q_choose_i.shape)log_areas_i = log_areas_i + log_feas_subsets.unsqueeze(-3).sum(dim=-1)# 6) sum over all subsets of size i, i.e. reduce over q_choose_i-dim# after, log_areas_i is mc_samples x batch_shape x num_cellslog_areas_i = logsumexp(log_areas_i, dim=-1) # areas_i.sum(dim=-1)# 7) Using the inclusion-exclusion principle, set the sign to be positive# for subsets of odd sizes and negative for subsets of even sign# in non-log space: areas_per_segment += (-1) ** (i + 1) * areas_i,# but here in log space, we need to keep track of sign:log_areas_per_segment[..., i % 2] = logplusexp(log_areas_per_segment[..., i % 2],log_areas_i,)# 8) subtract even from odd log area termslog_areas_per_segment = logdiffexp(log_a=log_areas_per_segment[..., 0], log_b=log_areas_per_segment[..., 1])# 9) sum over segments (n_cells-dim) and average over MC samplesreturn logmeanexp(logsumexp(log_areas_per_segment, dim=-1), dim=0)def _log_improvement(self, obj_subsets: Tensor, view_shape: tuple | torch.Size) -> Tensor:# smooth out the clamp and take the log (previous step 3)# subtract cell lower bounds, clamp min at zero, but first# make obj_subsets broadcastable with cell bounds:# mc_samples x batch_shape x (num_cells = 1) x q_choose_i x i x mobj_subsets = obj_subsets.unsqueeze(-4)# making cell bounds broadcastable with obj_subsets:# (mc_samples = 1) x (batch_shape = 1) x num_cells x 1 x (i = 1) x mcell_lower_bounds = self.cell_lower_bounds.view(view_shape).unsqueeze(-3)Z = obj_subsets - cell_lower_boundslog_Zi = self._log_smooth_relu(Z)return log_Zi # mc_samples x batch_shape x num_cells x q_choose_i x i x mdef _log_cell_lengths(self, log_improvement_i: Tensor, view_shape: tuple | torch.Size) -> Tensor:cell_upper_bounds = self.cell_upper_bounds.clamp_max(1e10 if log_improvement_i.dtype == torch.double else 1e8) # num_cells x num_objectives# add batch-dim to compute area for each segment (pseudo-pareto-vertex)# (mc_samples = 1) x (batch_shape = 1) x n_cells x (q_choose_i = 1) x mlog_cell_lengths = ((cell_upper_bounds - self.cell_lower_bounds).log().view(view_shape))# mc_samples x batch_shape x num_cells x q_choose_i x mreturn self._smooth_minimum(log_improvement_i,log_cell_lengths,)def _log_smooth_relu(self, X: Tensor) -> Tensor:f = log_fatplus if self.fat else log_softplusreturn f(X, tau=self.tau_relu)def _smooth_min(self, X: Tensor, dim: int, keepdim: bool = False) -> Tensor:f = fatmin if self.fat else smooth_aminreturn f(X, tau=self.tau_max, dim=dim)def _smooth_minimum(self, X: Tensor, Y: Tensor) -> Tensor:XY = torch.stack(torch.broadcast_tensors(X, Y), dim=-1)return self._smooth_min(XY, dim=-1, keepdim=False)@concatenate_pending_points@t_batch_mode_transform()@average_over_ensemble_modelsdef forward(self, X: Tensor) -> Tensor:posterior = self.model.posterior(X)samples = self.get_posterior_samples(posterior)return self._compute_log_qehvi(samples=samples, X=X)class qExpectedHypervolumeImprovement(MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin):def __init__(self,model: Model,ref_point: list[float] | Tensor,partitioning: NondominatedPartitioning,sampler: MCSampler | None = None,objective: MCMultiOutputObjective | None = None,constraints: list[Callable[[Tensor], Tensor]] | None = None,X_pending: Tensor | None = None,eta: Tensor | float = 1e-3,fat: bool = False,) -> None:r\"\"\"q-Expected Hypervolume Improvement supporting m>=2 outcomes.See [Daulton2020qehvi]_ for details.Example:>>> model = SingleTaskGP(train_X, train_Y)>>> ref_point = [0.0, 0.0]>>> qEHVI = qExpectedHypervolumeImprovement(model, ref_point, partitioning)>>> qehvi = qEHVI(test_X)Args:model: A fitted model.ref_point: A list or tensor with `m` elements representing the referencepoint (in the outcome space) w.r.t. to which compute the hypervolume.This is a reference point for the objective values (i.e. afterapplying`objective` to the samples).partitioning: A `NondominatedPartitioning` module that provides the non-dominated front and a partitioning of the non-dominated space in hyper-rectangles. If constraints are present, this partitioning must onlyinclude feasible points.sampler: The sampler used to draw base samples. If not given,a sampler is generated using `get_sampler`.objective: The MCMultiOutputObjective under which the samples are evaluated.Defaults to `IdentityMCMultiOutputObjective()`.constraints: A list of callables, each mapping a Tensor of dimension`sample_shape x batch-shape x q x m` to a Tensor of dimension`sample_shape x batch-shape x q`, where negative values implyfeasibility. The acquisition function will compute expected feasiblehypervolume.X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that havepoints that have been submitted for function evaluation but have not yetbeen evaluated. Concatenated into `X` upon forward call. Copied and setto have no gradient.eta: The temperature parameter for the sigmoid function used for thedifferentiable approximation of the constraints. In case of a float thesame eta is used for every constraint in constraints. In case of a`tensor the length of the tensor must match the number of providedconstraints. The i-th constraint is then estimated with the i-theta value.fat: A Boolean flag indicating whether to use the heavy-tailed approximationof the constraint indicator.\"\"\"legacy_ei_numerics_warning(legacy_name=type(self).__name__)if len(ref_point) != partitioning.num_outcomes:raise ValueError(\"The length of the reference point must match the number of outcomes. \"f\"Got ref_point with {len(ref_point)} elements, but expected \"f\"{partitioning.num_outcomes}.\")ref_point = torch.as_tensor(ref_point,dtype=partitioning.pareto_Y.dtype,device=partitioning.pareto_Y.device,)super().__init__(model=model,sampler=sampler,objective=objective,constraints=constraints,eta=eta,X_pending=X_pending,)self.register_buffer(\"ref_point\", ref_point)cell_bounds = partitioning.get_hypercell_bounds()self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])SubsetIndexCachingMixin.__init__(self)self.fat = fatdef _compute_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:r\"\"\"Compute the expected (feasible) hypervolume improvement given MC samples.Args:samples: A `n_samples x batch_shape x q' x m`-dim tensor of samples.X: A `batch_shape x q x d`-dim tensor of inputs.Returns:A `batch_shape x (model_batch_shape)`-dim tensor of expected hypervolumeimprovement for each batch.\"\"\"obj = self.objective(samples, X=X)q = obj.shape[-2]if self.constraints is not None:feas_weights = compute_smoothed_feasibility_indicator(constraints=self.constraints,samples=samples,eta=self.eta,fat=self.fat,) # `sample_shape x batch-shape x q`device = self.ref_point.deviceq_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)batch_shape = obj.shape[:-2]# this is n_samples x input_batch_shape xareas_per_segment = torch.zeros(*batch_shape,self.cell_lower_bounds.shape[-2],dtype=obj.dtype,device=device,)cell_batch_ndim = self.cell_lower_bounds.ndim - 2sample_batch_view_shape = torch.Size([batch_shape[0] if cell_batch_ndim > 0 else 1,*[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],*self.cell_lower_bounds.shape[1:-2],])view_shape = (*sample_batch_view_shape,self.cell_upper_bounds.shape[-2],1,self.cell_upper_bounds.shape[-1],)for i in range(1, self.q_out + 1):q_choose_i = q_subset_indices[f\"q_choose_{i}\"]# this tensor is mc_samples x batch_shape x i x q_choose_i x mobj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))obj_subsets = obj_subsets.view(obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:])# since all hyperrectangles share one vertex, the opposite vertex of the# overlap is given by the component-wise minimum.# take the minimum in each subsetoverlap_vertices = obj_subsets.min(dim=-2).values# add batch-dim to compute area for each segment (pseudo-pareto-vertex)# this tensor is mc_samples x batch_shape x num_cells x q_choose_i x moverlap_vertices = torch.min(overlap_vertices.unsqueeze(-3), self.cell_upper_bounds.view(view_shape))# subtract cell lower bounds, clamp min at zerolengths_i = (overlap_vertices - self.cell_lower_bounds.view(view_shape)).clamp_min(0.0)# take product over hyperrectangle side lengths to compute area# sum over all subsets of size iareas_i = lengths_i.prod(dim=-1)# if constraints are present, apply a differentiable approximation of# the indicator functionif self.constraints is not None:feas_subsets = feas_weights.index_select(dim=-1, index=q_choose_i.view(-1)).view(feas_weights.shape[:-1] + q_choose_i.shape)areas_i = areas_i * feas_subsets.unsqueeze(-3).prod(dim=-1)areas_i = areas_i.sum(dim=-1)# Using the inclusion-exclusion principle, set the sign to be positive# for subsets of odd sizes and negative for subsets of even signareas_per_segment += (-1) ** (i + 1) * areas_i# sum over segments and average over MC samplesreturn areas_per_segment.sum(dim=-1).mean(dim=0)@concatenate_pending_points@t_batch_mode_transform()@average_over_ensemble_modelsdef forward(self, X: Tensor) -> Tensor:posterior = self.model.posterior(X)samples = self.get_posterior_samples(posterior)return self._compute_qehvi(samples=samples, X=X)class qLogNoisyExpectedHypervolumeImprovement(NoisyExpectedHypervolumeMixin,qLogExpectedHypervolumeImprovement,):_log: bool = Truedef __init__(self,model: Model,ref_point: list[float] | Tensor,X_baseline: Tensor,sampler: MCSampler | None = None,objective: MCMultiOutputObjective | None = None,constraints: list[Callable[[Tensor], Tensor]] | None = None,X_pending: Tensor | None = None,eta: Tensor | float = 1e-3,prune_baseline: bool = False,alpha: float = 0.0,cache_pending: bool = True,max_iep: int = 0,incremental_nehvi: bool = True,cache_root: bool = True,tau_relu: float = TAU_RELU,tau_max: float = 1e-3, # TAU_MAX,fat: bool = True,marginalize_dim: int | None = None,) -> None:r\"\"\"q-Log Noisy Expected Hypervolume Improvement supporting m>=2 outcomes.Based on the differentiable hypervolume formulation of [Daulton2021nehvi]_.Example:>>> model = SingleTaskGP(train_X, train_Y)>>> ref_point = [0.0, 0.0]>>> qNEHVI = qNoisyExpectedHypervolumeImprovement(model, ref_point, train_X)>>> qnehvi = qNEHVI(test_X)Args:model: A fitted model.ref_point: A list or tensor with `m` elements representing the referencepoint (in the outcome space) w.r.t. to which compute the hypervolume.This is a reference point for the objective values (i.e. afterapplying `objective` to the samples).X_baseline: A `r x d`-dim Tensor of `r` design points that have alreadybeen observed. These points are considered as potential approximatepareto-optimal design points.sampler: The sampler used to draw base samples. If not given,a sampler is generated using `get_sampler`.Note: a pareto front is created for each mc sample, which can becomputationally intensive for `m` > 2.objective: The MCMultiOutputObjective under which the samples areevaluated. Defaults to `IdentityMultiOutputObjective()`.constraints: A list of callables, each mapping a Tensor of dimension`sample_shape x batch-shape x q x m` to a Tensor of dimension`sample_shape x batch-shape x q`, where negative values implyfeasibility. The acquisition function will compute expected feasiblehypervolume.X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points thathave points that have been submitted for function evaluation, buthave not yet been evaluated.eta: The temperature parameter for the sigmoid function used for thedifferentiable approximation of the constraints. In case of a float thesame `eta` is used for every constraint in constraints. In case of a`tensor the length of the tensor must match the number of providedconstraints. The i-th constraint is then estimated with the i-th`eta` value. For more details, on this parameter, see the docs of`compute_smoothed_feasibility_indicator`.fat: A Boolean flag indicating whether to use the heavy-tailed approximationof the constraint indicator.prune_baseline: If True, remove points in `X_baseline` that arehighly unlikely to be the pareto optimal and better than the`reference point. This can significantly improve computation time andis generally recommended. In order to customize pruning parameters,instead manually call `prune_inferior_points_multi_objective` on`X_baseline` before instantiating the acquisition function.alpha: The hyperparameter controlling the approximate non-dominatedpartitioning. The default value of 0.0 means an exact partitioningis used. As the number of objectives `m` increases, consider increasingthis parameter in order to limit computational complexity.cache_pending: A boolean indicating whether to use cached boxdecompositions (CBD) for handling pending points. This isgenerally recommended.max_iep: The maximum number of pending points before the boxdecompositions will be recomputed.incremental_nehvi: A boolean indicating whether to compute theincremental NEHVI from the `i`th point where `i=1, ..., q`under sequential greedy optimization, or the full qNEHVI over`q` points.cache_root: A boolean indicating whether to cache the rootdecomposition over `X_baseline` and use low-rank updates.marginalize_dim: A batch dimension that should be marginalized. For example,this is useful when using a batched fully Bayesian model.\"\"\"MultiObjectiveMCAcquisitionFunction.__init__(self,model=model,sampler=sampler,objective=objective,constraints=constraints,eta=eta,)SubsetIndexCachingMixin.__init__(self)NoisyExpectedHypervolumeMixin.__init__(self,model=model,ref_point=ref_point,X_baseline=X_baseline,sampler=self.sampler,objective=self.objective,constraints=self.constraints,X_pending=X_pending,prune_baseline=prune_baseline,alpha=alpha,cache_pending=cache_pending,max_iep=max_iep,incremental_nehvi=incremental_nehvi,cache_root=cache_root,marginalize_dim=marginalize_dim,)# parameters that are used by qLogEHVIself.tau_relu = tau_reluself.tau_max = tau_maxself.fat = fat@t_batch_mode_transform()@average_over_ensemble_modelsdef forward(self, X: Tensor) -> Tensor:# Get samples from the posterior, and manually concatenate pending points that# have not yet been cached. Shared with qNEHVI.samples, X = self._compute_posterior_samples_and_concat_pending(X)# Add previous nehvi from pending points.return self._compute_log_qehvi(samples=samples, X=X) + self._prev_nehvi",
        "experimental_info": "The methodology centers on a novel formulation of q-Expected Hypervolume Improvement (qEHVI), which leverages the inclusion-exclusion principle to compute joint HVI for `q` candidate points. Monte-Carlo (MC) integration over the Gaussian Process (GP) posterior is employed, utilizing randomized quasi-Monte Carlo (SobolQMCNormalSampler) for variance reduction. Exact gradients of this MC estimator are computed via auto-differentiation and the re-parameterization trick, enabling higher-order optimizers through the Sample Average Approximation (SAA) approach, which provides theoretical convergence guarantees.The method is extended to handle auxiliary outcome constraints by incorporating feasibility-weighting on the sample level and using a differentiable sigmoid approximation for the indicator function. It supports both joint batch optimization and a sequential greedy approach (controlled by `incremental_nehvi` in noisy variants) that integrates over the posterior of unobserved outcomes.### Specific Experimental Settings and Parameters:- **MC Samples**: The default number of MC samples (`mc_samples`) for acquisition function evaluation is 128 (e.g., in `get_acquisition_function`), though `MCSamplerMixin` has a `_default_sample_shape` of 512.- **Quasi-Monte Carlo (QMC)**: QMC sampling (`qmc=True` by default in `get_acquisition_function`) using `SobolQMCNormalSampler` is employed for variance reduction.- **Reference Point (`ref_point`)**: Used in multi-objective acquisition functions to define the hypervolume region of interest.- **Partitioning (`partitioning`)**: `NondominatedPartitioning` or `FastNondominatedPartitioning` (if `alpha=0`) is used to decompose the non-dominated space. `alpha` (default 0 for <=4 objectives, 10^-3 for >=5, 10^-2 for >=6 objectives in `get_default_partitioning_alpha`) controls the approximation level of the partitioning.- **Constraints Handling**: `constraints` (a list of callables), `eta` (temperature parameter for sigmoid approximation, default 1e-3), and `fat` (boolean for heavy-tailed approximation of constraint indicator, default `True` for LogEI family) are used for auxiliary outcome constraints.- **Log-Space Numerics (`LogEI` family)**: For improved numerical stability, `qLogExpectedHypervolumeImprovement` and `qLogNoisyExpectedHypervolumeImprovement` utilize specific temperature parameters: `tau_relu` (default 1e-6 for smooth ReLU approximation) and `tau_max` (default 1e-2 for smooth maximum operator over `q` candidates, though 1e-3 for noisy variants) and the `fat` parameter. Helper functions `log_fatplus`, `log_softplus`, `logmeanexp`, `logdiffexp`, `logplusexp`, `logsumexp`, `fatmin`, `smooth_amin` are used for robust log-space computations.- **Noisy Baselines (`qNEHVI`, `qLogNEHVI`)**: These variants use `X_baseline` (observed points), `prune_baseline` (default `False`, can remove inferior points from baseline for efficiency), `cache_root` (default `True`, uses low-rank Cholesky updates), and `marginalize_dim` (for fully Bayesian models). `cache_pending` (default `True`) and `max_iep` are also settings for handling pending points.- **Sequential Greedy Optimization**: `incremental_nehvi` (default `True` in `qLogNoisyExpectedHypervolumeImprovement`) is a boolean flag to compute incremental NEHVI for sequential greedy candidate selection.- **General Numerical Settings (from `botorch/__init__.py`)**: `linear_operator.settings` are adjusted: `_fast_covar_root_decomposition`, `_fast_log_prob`, `_fast_solves` are set to `False`; `cholesky_max_tries` to 6; `max_cholesky_size` to 4096; and `gpytorch.settings.max_eager_kernel_size` to 4096. These aim to improve robustness of GP computations in a BO-loop. Constant `CLAMP_LB` (e.g. 1e-8 for float32) is used for numerical stability."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Partitioned Neural Networks (PNs), a novel and efficient method for hyperparameter optimization in deep learning. It addresses the challenges of traditional hyperparameter tuning, which relies on multiple training runs and validation sets, making it computationally expensive and problematic in low-data regimes or Federated Learning (FL) settings. PNs are inspired by marginal likelihood and optimize hyperparameters during a single training run without needing a validation set. The key findings include demonstrating PNs' ability to identify correct models in toy tasks, learn effective data augmentations for improved generalization and data efficiency (outperforming baselines like Augerino and Differentiable Laplace on various datasets), optimize complex feature extractors, and successfully tune dropout rates. In FL, PNs significantly reduce communication overhead while achieving better model performance compared to FedAvg and FedAvg + Augerino.",
        "methodology": "The method partitions the training data into C shards (D1,...,DC) and the neural network parameters (w) into C partitions (w1,...,wC). C subnetworks are defined, where the k-th subnetwork w(k)s consists of the first k parameter partitions (w1,...,wk) and sets the remaining parameters (w_k+1,...,w_C) to default values (e.g., initialization values or zero). Each partition wk is optimized using data from D1:k (i.e., data shards 1 to k) by computing gradients through its corresponding subnetwork w(k)s. Hyperparameters (ψ) are optimized using an 'out-of-training-sample' loss, which measures the loss of subnetwork w(k-1)s on data from the unseen chunk Dk. This objective is a lower-bound approximation to the marginal likelihood. The optimization involves interleaving stochastic gradient updates for parameter partitions and hyperparameters. The primary partitioning scheme used is random weight partitioning, where a fixed proportion of weights in each layer is randomly assigned to a partition. For FL, clients are assigned to non-overlapping chunks, and each client belonging to chunk k optimizes partitions wk:C through subnetworks w(k:C)s, computing gradients for both model parameters and hyperparameters.",
        "experimental_setup": "Experiments were conducted on various tasks: input selection (toy task with 30 features, 15 informative) using a fully connected MLP; learning affine data augmentations on MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet) using CNN, fixupResNet-8/14, and ResNet-50 architectures; and Federated Learning (FL) with non-i.i.d. splits of MNIST, rotMNIST, CIFAR10, and rotCIFAR10 using CNN and ResNet-9 with GroupNorm. Baselines included standard training, Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, and a multi-network ensemble. For FL, FedAvg and FedAvg + Augerino were used as baselines. Validation methods involved comparing the LML objective, test accuracy, and test log-likelihood. Ablation studies investigated performance in low-data regimes, comparison to traditional train/validation splits with fine-tuning, and sensitivity to various partitioning schemes (number of chunks, data/parameter proportions). Metrics included test accuracy, average log-likelihood, and communication cost reduction for FL.",
        "limitations": "The proposed method has a few limitations. It inherently requires an additional forward-backward pass to update hyperparameters, leading to increased computational costs, although these are significantly less than existing marginal likelihood-based alternatives. Empirically, partitioned networks generally require more training iterations to converge. The partitioning of the network can inherently constrain its capacity, potentially leading to a slight loss in performance compared to a full network ideally tuned with optimal hyperparameters. Furthermore, the partitioning strategy itself (number of chunks, relative data proportions, and relative parameter proportions) introduces an additional hyperparameter that might require tuning for optimal performance, though experiments suggest reasonable robustness to these choices.",
        "future_research_directions": "Potential future research directions include exploring dynamic partitioning of network parameters during training, investigating methods to alleviate the performance loss due to constrained network capacity (e.g., by adjusting training rounds or increasing initial network capacity), developing alternative partitioning schemes for reduced overhead (such as node partitioning that allows for simultaneous updates), and exploring strategies for hyperparameter updates like interleaving them at less frequent intervals or accumulating gradients from different chunks for lower variance estimates. In the federated learning context, further exploration of different sequential updating schemes for client-side parameter updates is also suggested."
      }
    },
    {
      "title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing",
      "abstract": "Tuning hyperparameters is a crucial but arduous part of the machine learning\npipeline. Hyperparameter optimization is even more challenging in federated\nlearning, where models are learned over a distributed network of heterogeneous\ndevices; here, the need to keep data on device and perform local training makes\nit difficult to efficiently train and evaluate configurations. In this work, we\ninvestigate the problem of federated hyperparameter tuning. We first identify\nkey challenges and show how standard approaches may be adapted to form\nbaselines for the federated setting. Then, by making a novel connection to the\nneural architecture search technique of weight-sharing, we introduce a new\nmethod, FedEx, to accelerate federated hyperparameter tuning that is applicable\nto widely-used federated optimization methods such as FedAvg and recent\nvariants. Theoretically, we show that a FedEx variant correctly tunes the\non-device learning rate in the setting of online convex optimization across\ndevices. Empirically, we show that FedEx can outperform natural baselines for\nfederated hyperparameter tuning by several percentage points on the\nShakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using\nthe same training budget.",
      "full_text": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing Mikhail Khodak, Renbo Tu, Tian Li Carnegie Mellon University {khodak,renbo,tianli}@cmu.edu Liam Li Hewlett Packard Enterprise me@liamcli.com Maria-Florina Balcan, Virginia Smith Carnegie Mellon University ninamf@cs.cmu.edu,smithv@cmu.edu Ameet Talwalkar Carnegie Mellon University & Hewlett Packard Enterprise talwalkar@cmu.edu Abstract Tuning hyperparameters is a crucial but arduous part of the machine learning pipeline. Hyperparameter optimization is even more challenging in federated learn- ing, where models are learned over a distributed network of heterogeneous devices; here, the need to keep data on device and perform local training makes it difﬁcult to efﬁciently train and evaluate conﬁgurations. In this work, we investigate the prob- lem of federated hyperparameter tuning. We ﬁrst identify key challenges and show how standard approaches may be adapted to form baselines for the federated setting. Then, by making a novel connection to the neural architecture search technique of weight-sharing, we introduce a new method, FedEx, to accelerate federated hyperparameter tuning that is applicable to widely-used federated optimization methods such as FedAvg and recent variants. Theoretically, we show that a FedEx variant correctly tunes the on-device learning rate in the setting of online convex optimization across devices. Empirically, we show that FedEx can outperform natural baselines for federated hyperparameter tuning by several percentage points on the Shakespeare, FEMNIST, and CIFAR-10 benchmarks—obtaining higher accuracy using the same training budget. 1 Introduction Federated learning (FL) is a popular distributed computational setting where training is performed locally or privately [ 33, 39] and where hyperparameter tuning has been identiﬁed as a critical problem [20]. Although general hyperparameter optimization has been the subject of intense study [4, 18, 29], several unique aspects of the federated setting make tuning hyperparameters especially challenging. However, to the best of our knowledge there has been no dedicated study on the speciﬁc challenges and solutions in federated hyperparameter tuning. In this work, we ﬁrst formalize the problem of hyperparameter optimization in FL, introducing the following three key challenges: 1. Federated validation data: In federated networks, as the validation data is split across devices, the entire dataset is not available at any one time; instead a central server is given access to some number of devices at each communication round, for one or at most a few runs of local training and validation. Thus, because the standard measure of complexity in FL is the number of communication rounds, computing validation metrics exactly dramatically increases the cost. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.04502v2  [cs.LG]  4 Nov 2021Figure 1: FedEx can be applied to any local training-based FL method, e.g. FedAvg, by interleaving standard updates to model weights (computed by aggregating results of local training) with exponen- tiated gradient updates to hyperparameters (computed by aggregating results of local validation). 2. Extreme resource limitations: FL applications often involve training using devices with very limited computational and communication capabilities. Furthermore, many require the use of privacy techniques such as differential privacy that limit the number times user data can be accessed. Thus we cannot depend on being able to run many different conﬁgurations to completion. 3. Evaluating personalization: Finally, even with non-federated data, applying common hyperpa- rameter optimization methods to standard personalized FL approaches (such as ﬁnetuning) can be costly because evaluation may require performing many additional training steps locally. With these challenges1 in mind, we propose reasonable baselines for federated hyperparameter tuning by showing how to adapt standard non-federated algorithms. We further study the challenge of noisy validation signal due to federation, and show that simple state-estimation-based ﬁxes do not help. Our formalization and analysis of this problem leads us to develop FedEx, a method that exploits a novel connection between hyperparameter tuning in FL and the weight-sharing technique widely used in neural architecture search (NAS) [5, 37, 43]. In particular, we observe that weight-sharing is a natural way of addressing the three challenges above for federated hyperparameter tuning, as it incorporates noisy validation signal, simultaneously tunes and trains the model, and evaluates personalization as part of training rather than as a costly separate step. Although standard weight- sharing only handles architectural hyperparameters such as the choice of layer or activation, and not critical settings such as those of local stochastic gradient descent (SGD), we develop a formulation that allows us to tune most of these as well via the relationship between local-training and ﬁne-tuning- based personalization. This make FedEx a general hyperparameter tuning algorithm applicable to many local training-based FL methods, e.g. FedAvg [39], FedProx [34], and SCAFFOLD [21]. In Section 4, we next conduct a theoretical study of FedEx in a simple setting: tuning the client step-size. Using the ARUBA framework for analyzing meta-learning [22], we show that a variant of FedEx correctly tunes the on-device step-size to minimize client-averaged regret by adapting to the intrinsic similarity between client data. We improve the convergence rate compared to some past meta-learning theory [22, 28] while not depending on knowing the (usually unknown) task-similarity. Finally, in Section 5, we instantiate our baselines and FedEx to tune hyperparameters of FedAvg, FedProx, and Reptile, evaluating on three standard FL benchmarks: Shakespeare, FEMNIST, and CIFAR-10 [6, 39]. While our baselines already obtain performance similar to past hand-tuning, FedEx further surpasses them in most settings examined, including by 2-3% on Shakespeare. Related Work To the best of our knowledge, we are the ﬁrst to systematically analyze the formu- lation and challenges of hyperparameter optimization in the federated setting. Several papers have explored limited aspects of hyperparameter tuning in FL [8, 26, 41], focusing on a small number of hyperparameters (e.g. the step-size and sometimes one or two more) in less general settings (studying small-scale problems or assuming server-side validation data). In contrast our methods are able to tune a wide range of hyperparameters in realistic federated networks. Some papers also discussed the challenges of ﬁnding good conﬁgurations while studying other aspects of federated training [ 44]. We argue that it is critical to properly address the challenges of federated hyperparameter optimization in practical settings, as we discuss in detail in Section 2. Methodologically, our approach draws on the fact that local training-based methods such as FedAvg can be viewed as optimizing a surrogate objective for personalization [22], and more broadly leverages the similarity of the personalized FL setup and initialization-based meta-learning [ 7, 12, 19, 28]. While FedEx’s formulation and guarantees use this relationship, the method itself is general-purpose 1A further challenge we do not address is that of the time-dependency of federated evaluation, c.f. [10]. 2and applicable to federated training of a single global model. Many recent papers address FL personalization more directly [15, 32, 38, 47, 51]. This connection and our use of NAS techniques also makes research connecting NAS and meta-learning relevant [11, 36], but unlike these methods we focus on tuning non-architectural parameters. In fact, we believe our work is the ﬁrst to apply weight- sharing to regular hyperparameter search. Furthermore, meta-learning does not have the data-access and computational restrictions of FL, where such methods using the DARTS mixture relaxation [37] are less practical. Instead, FedEx employs the lower-overhead stochastic relaxation [9, 31], and its exponentiated update is similar to the recently proposed GAEA approach for NAS [30]. Running NAS itself in federated settings has also been studied [14, 17, 50]; while our focus is on non-architectural hyperparameters, in-principle our algorithms can also be used for federated NAS. Theoretically, our work makes use of the average regret-upper-bound analysis (ARUBA) frame- work [22] to derive guarantees for learning the initialization, i.e. the global model, while simul- taneously tuning the step-size of the local algorithm. The step-size of gradient-based algorithms has also been tuned on its own in the settings of data-driven algorithm design [16] and of statistical learning-to-learn [49]. 2 Federated Hyperparameter Optimization In this section we formalize the problem of hyperparameter optimization for FL and discuss the con- nection of its personalized variant to meta-learning. We also reviewFedAvg [39], a common federated optimization method, and present a reasonable baseline approach for tuning its hyperparameters. Global and Personalized FL In FL we are concerned with optimizing over a network of heteroge- neous clients i= 1,...,n , each with training, validation, and testing sets Ti, Vi, and Ei, respectively. We use LS(w) to denote the average loss over a dataset Sof some w-parameterized ML model, for w ∈Rd some real vector. For hyperparameter optimization, we assume a class of algorithms Alga hyperparameterized by a∈A that use federated access to training sets Ti to output some element of Rd. Here by “federated access\" we mean that each iteration corresponds to a communication round at which Alga has access to a batch of Bclients2 that can do local training and validation. Speciﬁcally, we assume Alga can be described by two subroutines with hyperparameters encoded by b∈B and c∈C, so that a= (b,c) and A= B×C . Here cencodes settings of a local training algorithm Locc that take a training set S and initialization w ∈Rd as input and outputs a model Locc(S,w) ∈Rd, while b sets those of an aggregation Aggb that takes the initialization w and outputs of Locc as input and returns a model parameter. For example, in standard FedAvg, Locc is T steps of gradient descent with step-size η and Aggb takes a weighted average of the outputs of Locc across clients; here c = (η,T ) and b = (). As detailed in the appendix, many FL methods can be decomposed this way, including well-known ones such as FedAvg [39], FedProx [34], SCAFFOLD [21], and Reptile [42] as well as more recent methods [1, 2, 32]. Our analysis and our proposed FedEx algorithm will thus apply to all of them, up to an assumption detailed next. Starting from this decomposition, the global hyperparameter optimization problem can be written as min a∈A n∑ i=1 |Vi|LVi(Alga({Tj}n j=1)) (1) In many cases we are also interested in obtaining a device-speciﬁc local model, where we take a model trained on all clients and ﬁnetune it on each individual client before evaluating. A key assumption we make is that the ﬁnetuning algorithm will be the same as the local training algorithm Locc used by Alga. This assumption can be justiﬁed by recent work in meta-learning that shows that algorithms that aggregate the outputs of local SGD can be viewed as optimizing for personalization using local SGD [22]. Then, in the personalized setting, the tuning objective becomes min a=(b,c)∈A n∑ i=1 |Vi|LVi(Locc(Ti,Alga({Tj}n j=1)) (2) Our approach will focus on the setting where the hyperparameters cof local training make up a signiﬁcant portion of all hyperparameters a= (b,c); by considering the personalization objective we will be able to treat such hyperparameters as architectural and thus apply weight-sharing. 3Algorithm 1: Successive halving algorithm (SHA) ap- plied to personalized FL. For the non-personalized ob- jective (1), replace LVti(wi) by LVti(wa). For random search (RS) with N samples, set η= N and R= 1. Input: distribution Dover hyperparameters A, elimination rate η∈N, elimination rounds τ0 = 0,τ1,...,τ R sample set of ηR hyperparameters H ∼D[ηR] initialize a model wa ∈Rd for each a∈H for elimination round r∈[R] do for setting a= (b,c) ∈H do for comm. round t= τr−1 + 1,...,τ r do for client i= 1,...,B do send wa,c to client wi ←Locc(Tti,wa) send wi,LVti(wi) to server wa ←Aggb(wa,{wi}B i=1) sa ←∑B i=1 |Vti|LVti(wi)/∑B i=1 |Vti| H ←{a∈H : sa ≤1 η-quantile({sa : a∈H})} Output: remaining a∈H and associated model wa Figure 2: Tuning FL with SHA but mak- ing elimination decisions based on vali- dation estimates using different discount factors. On both FEMNIST (top) and CI- FAR (bottom) using more of the validation data does not improve upon just using the most recent round’s validation error. Tuning FL Methods: Challenges and Baselines In the non-federated setting, the objective (1) is amenable to regular hyperparameter optimization methods; for example, a random search approach would repeatedly sample a setting afrom some distribution over A, run Alga to completion, and eval- uate the objective, saving the best setting and output [4]. With a reasonable distribution and enough samples this is guaranteed to converge and can be accelerated using early stopping methods [29], in which Alga is not always run to completion if the desired objective is poor at intermediate stages, or by adapting the sampling distribution using the results of previous objective evaluations [48]. As mentioned in the introduction, applying such methods to FL is inherently challenging due to 1. Federated validation data: Separating data across devices means we cannot immediately get a good estimate of the model’s validation performance, as we only have access to a possibly small batch of devices at a time. This means that decisions such as which models to ﬂag for early stopping will be noisy and may not fully incorporate all the available validation signal. 2. Extreme resource limitations: As FL algorithms can take a very long time to run in-practice due to the weakness and spotty availability of devices, we often cannot afford to conduct many training runs to evaluate different conﬁgurations. This issue is made more salient in cases where we use privacy techniques that only allow a limited number of accesses to the data of any individual user. 3. Evaluating personalization: While personalization is important in FL due to client heterogeneity, checking the performance of the current model on the personalization objective(2) is computation- ally intensive because computing may require running local training multiple times. In particular, while regular validation losses require computing one forward pass per data point, personalized losses require several forward-backward passes, making it many times more expensive if this loss is needed to make a tuning decision such as eliminating a conﬁguration from consideration. Despite these challenges, we can still devise sensible baselines for tuning hyperparameters in FL, most straightforward of which is to use a regular hyperparameter method but use validation data from a single round as a noisy surrogate for the full validation objective. Speciﬁcally, one can use random search (RS)—repeatedly evaluate random conﬁgurations—and a simple generalization called successive halving (SHA), in which we sample a set of conﬁgurations and partially run all of them for some number of communication rounds before eliminating all but the best 1 η fraction, repeating until only one conﬁguration remains. Note both are equivalent to a “bracket” in Hyperband [29] and their adaptation to FL is detailed in Algorithm 1. 2For simplicity the number of clients per round is ﬁxed, but all methods can be easily generalized to varying B. 4As shown in Section 5, SHA performs reasonably well on the benchmarks we consider. However, by using validation data from one round it may make noisy elimination decisions, early-stopping potentially good conﬁgurations because of a difﬁcult set of clients on a particular round. Here the problem is one of insufﬁcient utilization of the validation data to estimate model performance. A reasonable approach to use more is to try some type of state-estimation: using the performance from previous rounds to improve the noisy measurement of the current one. For example, instead of using only the most recent round for elimination decisions we can use a weighted sum of the performances at all past rounds. To investigate this, we study a power decay weighting, where a round is discounted by some constant factor for each time step it is in the past. We consider factors 0.0 (taking the most recent performance only, as before), 0.5, and 1.0 (taking the average). However, in Figure 2 we show that incorporating more validation data this way than is used by Algorithm 1 by default does not signiﬁcantly affect results. Thus we may need a better algorithm to use more of the validation signal, most of which is discarded by using the most recent round’s performance. We next proposeFedEx, a new method that does so by using validation on each round to update a client hyperparameters distribution used to sample conﬁgurations to send to devices. Thus it alleviates issue (1) above by updating at each step, not waiting for an elimination round as in RS or SHA. By simultaneously training the model and tuning (client) hyperparameters, it also moves towards a fully single-shot procedure in which we only train once (we must still run multiple times due to server hyperparameters), which would solve issue (2). Finally, FedEx addresses issue (3) by using local training to both update the model and to estimate personalized validation loss, thus not spending extra computation on this more expensive objective. 3 Weight-Sharing for Federated Learning We now present FedEx, a way to tune local FL hyperparameters. This section contains the general algorithm and its connection to weight-sharing; we instantiate it on several FL methods in Section 5. Weight-Sharing for Architecture Search We ﬁrst review the weight-sharing approach in NAS, which for a set Cof network conﬁgurations is often posed as the bilevel optimization min c∈C Lvalid(w,c) s.t. w ∈arg min u∈Rd Ltrain(u,c) (3) where Ltrain,Lvalid evaluate a single conﬁguration with the given weights. If, as in NAS, all hyperpa- rameters are architectural, then they are effectively themselves trainable model parameters [30], so we could instead consider solving the following “single-level\" empirical risk minimization (ERM): min c∈C,w∈Rd L(w,c) = min c∈C,w∈Rd Ltrain(w,c) + Lvalid(w,c) (4) Solving this instead of the bilevel problem (3) has been proposed in several recent papers [27, 30]. Early approaches to solving either formulation of NAS were costly due to the need for full or partial training of many architectures in a very large search space. The weight-sharing paradigm [43] reduces the problem to that of training a single architecture, a “supernet\" containing all architectures in the search space C. A straightforward way of constructing a supernet is via a “stochastic relaxation\" where the loss is an expectation w.r.t. sampling cfrom some distribution over C[9]. Then the shared weights can be updated using SGD by ﬁrst sampling an architecture cand using an unbiased estimate of ∇wL(w,c) to update w. The distribution over Cmay itself be adapted or stay ﬁxed. We focus on the former case, adapting some θ-parameterized distribution Dθ; this yields the stochastic relaxation objective min θ∈Θ,w∈Rd Ec∼DθL(w,c) (5) Since architectural hyperparameters are often discrete decisions, e.g. a choice of which of a ﬁxed number of operations to use, a natural choice of Dθ is as a product of categorical distributions over simplices. In this case, any discretization of an optimum θof the relaxed objective (5) whose support is in the support of θ will be an optimum of the original objective (4). A natural update scheme here is exponentiated gradient [ 25], where each successive θ is proportional to θ⊙exp(−η˜∇), ηis a step-size, and ˜∇an unbiased estimate of ∇θEc∼DθL(w,c) that can be computed using the re-parameterization trick [45]. By alternating this exponentiated update with the standard SGD update to w discussed earlier we obtain a simple block-stochastic minimization scheme that is guaranteed to converge, under certain conditions, to the ERM objective, and also performs well in practice [30]. 5The FedEx Method To obtain FedEx from weight-sharing we restrict to the case of tuning only the hyperparameters cof local training Locc.3 Our goal then is just to ﬁnd the best initialization w ∈Rd and local hyperparameters c∈C, i.e. we replace the personalized objective (2) by min c∈C,w∈Rd n∑ i=1 |Vi|LVi(Locc(Ti,w)) (6) Note Alga outputs an element of Rd, so this new objective is upper-bounded by the original (2), i.e. any solution will be at least as good for the original objective. Note also that for ﬁxed cthis is equivalent to the classic train-validation split objective for meta-learning withLoccas the base-learner. More importantly for us, it is also in the form of the r.h.s. of the weight-sharing objective (4), i.e. it is a single-level function of w and c. We thus apply a NAS-like stochastic relaxation: min θ∈Θ,w∈Rd n∑ i=1 |Vi|Ec∈DθLVi(Locc(Ti,w)) (7) In NAS we would now set the distribution to be a product of categorical distributions over different architectures, thus making θan element of a product of simplices and making the optimum of the original objective (6) equivalent to the optimum of the relaxed objective(7) as an extreme point of the simplex. Unlike in NAS, FL hyperparameters such as the learning rate are not extreme points of a simplex and so it is less clear what parameterized distribution Dθ to use. Nevertheless, we ﬁnd that crudely imposing a categorical distribution over k> 1 random samples from some distribution (e.g. uniform) over Cand updating θusing exponentiated gradient over the resulting k-simplex works well. We alternate this with updating w ∈Rd, which in a NAS algorithm involves an SGD update using an unbiased estimate of the gradient at the current w and θ. We call this alternating method for solving (7) FedEx and describe it for a general Alga consisting of sub-routines Aggb and Locc in Algorithm 2; recall from Section 2 that many FL methods can be decomposed this way, so our approach is widely applicable. FedEx has a minimal overhead, consisting only of the last four lines of the outer loop updating θ. Thus, as with weight-sharing, FedEx can be viewed as reducing the complexity of tuning local hyperparameters to that of training a single model. Each update to θrequires a step-size ηt and an approximation ˜∇of the gradient w.r.t. θ; for the latter we obtain an estimate ˜∇j of each gradient entry via the reparameterization trick, whose variance we reduce by subtracting a baseline λt. How we set ηt and λt is detailed in the Appendix. To see how FedEx is approximately optimizing the relaxed objective (7), we can consider the case where Alga is Reptile [42], which was designed to optimize some approximation of (6) for ﬁxed c, or equivalently the relaxed objective for an atomic distributionDθ. The theoretical literature on meta-learning [22, 23] shows that Reptile can be interpreted as optimizing a surrogate objective minimizing the squared distance between w and the optimum of each task i, with the latter being replaced by the last iterate in practice. It is also shown that the surrogate objective is useful for personalization in the online convex setting. 4 As opposed to this past work, FedEx makes two gradient updates in the outer loop, on two disjoint sets of variables: the ﬁrst is the sub-routine Aggb of Alga that aggregates the outputs of local training and is using the gradient of the surrogate objective, since the derivative of the squared distance is the difference between the initialization w and the parameter at the last iterate of Locc; the second is the exponentiated gradient update that is directly using an unbiased estimate of the derivative of the second objective w.r.t. the distribution parametersθ. Thus, roughly speaking FedEx runs simultaneous stochastic gradient descent on the relaxed objective (7), although for the variables w we are using a ﬁrst-order surrogate. In the theoretical portion of this work we employ this interpretation to show the approach works for tuning the step-size of online gradient descent in the online convex optimizations setting. Wrapping FedEx We can view FedEx as an algorithm of the form tuned by Algorithm 1 that implements federated training of a supernet parameter (w,θ), with the local training routine Loc including a step for sampling c∼Dθ and the server aggregation routine including an exponentiated update of θ. Thus we can wrap FedEx in Algorithm 1, which we ﬁnd useful for a variety of reasons: • The wrapper can tune the settings of bfor the aggregation step Aggb, which FedEx cannot. • FedEx itself has a few hyperparameters, e.g. how to set the baseline λt, which can be tuned. 3We will use some wrapper algorithm to tune the hyperparameters b of Aggb. 4Formally they study a sequence of upper bounds and not a surrogate objective, as their focus is online learning. 6Algorithm 2: FedEx Input: conﬁgurations c1,...,c k ∈C, setting bfor Aggb, schemes for setting step-size ηt and baseline λt, total number of steps τ ≥1 initialize θ1 = 1k/kand shared weights w1 ∈Rd for comm. round t= 1,...,τ do for client i= 1,...,B do send wt,θt to client sample cti ∼Dθt wti ←Loccti(Tti,wt) send wti,cti,LVti(wti) to server wt+1 ←Aggb(w,{wti}B i=1) ˜∇j ← ∑B i=1 |Vti|(LVti(wti)−λt)1cti=cj θt[j] ∑B i=1 |Vti| ∀j θt+1 ←θt ⊙exp(−ηt˜∇) θt+1 ←θt+1/∥θt+1∥1 Output: model w, hyperparameter distribution θ Figure 3: Comparison of the range of perfor- mance values attained using different pertur- bation settings. Although the range is much smaller for ϵ= 0.1 than for ϵ= 1.0 (the lat- ter is the entire space), it still covers a large (roughly 10-20%) range of different perfor- mance levels on both FEMNIST (left) and CIFAR (right). • By running multiple seeds and potentially using early stopping, we can run FedEx using more aggressive steps-sizes and the wrapper will discard cases where this leads to poor results. • We can directly compare FedEx to a regular hyperparameter optimization scheme run over the original algorithm, e.g. FedAvg, by using the same scheme to both wrap FedEx and tune FedAvg. • Using the wrapper allows us to determine the conﬁgurations c1,...,c k given to Algorithm 2 using a local perturbation scheme (detailed next) while still exploring the entire hyperparameter space. Local Perturbation It remains to specify how to select the conﬁgurations c1,...,c k ∈C to pass to Algorithm 2. While the simplest approach is to draw fromUnifk(C), we ﬁnd that this leads to unstable behavior if the conﬁgurations are too distinct from each other. To interpolate between sampling ci independently and setting them to be identical (which would just be equivalent to the baseline algorithm), we use a simple local perturbation method in which c1 is sampled from Unif(C) and c2,...,c k are sampled uniformly from a local neighborhood of C. For continuous hyperparameters (e.g. step-size, dropout) drawn from an interval [a,b] ⊂R the local neighborhood is [c±(b−a)ε] for some ε≥0, i.e. a scaled ε-ball; for discrete hyperparameters (e.g. batch-size, epochs) drawn from a set {a,...,b }⊂ Z, the local neighborhood is similarly {c−⌊(b−a)ε⌋,...,c + ⌈(b−a)ε⌉}; in our experiments we set ε= 0.1, which works well, but run ablation studies varying these values in the appendix showing that a wide range of them leads to improvement. Note that while local perturbation does limit the size of the search space explored by each instance of FedEx, as shown in Figure 3 the difference in performance between different conﬁgurations in the same ball is still substantial. Limitations of FedEx While FedEx is applicable to many important FL algorithms, those that cannot be decomposed into local ﬁne-tuning and aggregation should instead be tuned by one of our baselines, e.g. SHA. FedEx is also limited in that it is forced to rely on such algorithms as wrappers for tuning its own hyperparameters and certain FL hyperparameters such as server learning rate. 4 Theoretical Analysis for Tuning the Step-Size in an Online Setting As noted in Section 3, FedEx can be viewed as alternating minimization, with a gradient step on a surrogate personalization loss and an exponentiated gradient update of the conﬁguration distribution θ. We make this formal and prove guarantees for a simple variant of FedEx in the setting where the server has one client per round, to which the server sends an initialization to solve an online convex optimization (OCO) problem using online gradient descent (OGD) on a sequence of m adversarial convex losses (i.e. one SGD epoch in the stochastic case). Note we use “client” and “task” interchangeably, as the goal is a meta-learning (personalization) result. The performance measure here is task-averaged regret, which takes the average over τ clients of the regret they incur on its loss: 7¯Rτ = 1 τ τ∑ t=1 m∑ i=1 ℓt,i(wt,i) −ℓt,i(w∗ t) (8) Here ℓt,i is the ith loss of client t, wt,i the parameter chosen on its ith round from a compact parameter space W, and w∗ t ∈arg minw∈W ∑m i=1 ℓt,i(w) the task optimum. In this setting, the Average Regret-Upper-Bound Analysis (ARUBA) framework [22] can be used to show guarantees for a Reptile (i.e. FedEx with a server step-size) variant in which at each round the initialization is updated as wt+1 ←(1 −αt)wt + αtw∗ t for server step-size αt = 1/t. Observe that the only difference between this update and FedEx’s is that the task-toptimum w∗ t is used rather than the last iterate of OGD on that task. Speciﬁcally they bound task-averaged regret by ¯Rτ ≤ ˜O ( 1 4√τ + V )√m for V2 = min w∈W 1 τ τ∑ t=1 ∥w −w∗ t∥2 2 (9) Here V—the average deviation of the optimal actions w∗ t across tasks—is a measure of task-similarity: V is small when the tasks (clients) have similar data and thus can be solved by similar parameters inW but large when their data is different and so the optimum parameters to use are very different. Thus the bound in (9) shows that as the server (meta-learning) sees more and more clients (tasks), their regret on each decays with rate 1/4√τ to depend only on the task-similarity, which is hopefully small if the client data is similar enough that transfer learning makes sense, in particular ifV ≪diam(W). Since single-task regret has lower bound Ω(D√m), achieving asymptotic regret V√mthus demonstrates successful learning of a useful initialization in Wthat can be used for personalization. Note that such bounds can also be converted to obtain guarantees in the statistical meta-learning setting as well [22]. A drawback of past results using the ARUBA framework is that they either assume the task-similarity V is known in order to set the client step-size [28] or they employ an OCO method to learn the local step-size that cannot be applied to other potential algorithmic hyperparameters [22]. In contrast, we prove results for using bandit exponentiated gradient to tune the client step-size, which is precisely the FedEx update. In particular, Theorem 4.1 shows that by using a discretization of potential client step-sizes as the conﬁgurations in Algorithms 2 we can obtain the following task-averaged regret: Theorem 4.1. Let W⊂ Rd be convex and compact with diameter D = diam(W) and let ℓt,i be a sequence of mτ b-bounded convex losses— mfor each of τ tasks—with Lipschitz constant ≤G. We assume that the adversary is oblivious within-task. Suppose we run Algorithm 2 with B = 1, conﬁgurations cj = D Gj√m for each j = 1,...,k determining the local step-size of single-epoch SGD (OGD), wti = w∗ t, regret∑m i=1 ℓt,i(wt,i)−ℓt,i(wt) used in place ofLVti(wti), and λt = 0 ∀t∈[τ]. Then if ηt = 1 mb √ log k kτ ∀t ∈[τ], k 3 2 = DG b √ τ 2m, and Aggb(w,w∗ t) = (1 −αt)w + αtw∗ t for αt = 1/t∀t∈[τ] we have (taking expectations over sampling from Dθt) E ¯Rτ ≤ ˜O ( 3 √ m/τ + V )√m (10) The proof of this result, given in the supplement, follows the ARUBA framework of using meta OCO algorithm to optimize the initialization-dependent upper bound on the regret of OGD; in addition we bound errors to the bandit setting and discretization of the step-sizes. Theorem 4.1 demonstrates that FedEx is a sensible algorithm for tuning the step-size in the meta-learning setting where each task is an OCO problem, with the average regret across tasks (clients) converging to depend only on the task-similarity V, which we hope is small in the setting where personalization is useful. As we can see by comparing to the bound in (9), besides holding for a more generally-applicable algorithm our bound also improves the dependence on τ, albeit at the cost of an additional m 1 3 factor. Note that that the sublinear term can be replaced by 1/√τ in the full-information setting, i.e. where required the client to try SGD with each conﬁguration cj at each round to obtain regret for all of them. 8Table 1: Final test error obtained when tuning using a standard hyperparameter tuning algorithm (SHA or RS) alone, or when using it for server (aggregation) hyperparameters while FedEx tunes client (on-device training) hyperparameters. The target model is the one used to compute on-device validation error by the wrapper method, as well as the one used to compute test error after tuning. Note that this table reports the ﬁnal error results corresponding to the online evaluations reported in Figure 4, which measure performance as more of the computational budget is expended. Wrapper Target Tuning Shakespeare FEMNIST CIFAR-10 method model method i.i.d. non-i.i.d. i.i.d. non-i.i.d. i.i.d. global RS (server & client)60.32±10.03 64.36±14.19 22.81±4.56 22.98±3.41 30.46±9.44 Random + FedEx(client) 53.94±9.13 57.70±17.57 20.96±4.77 22.30±3.66 34.83±14.74 Search person- RS (server & client) 61.10±9.32 61 .71±9.08 17.45±2.82 17.77±2.63 34.89±10.56 (RS) alized + FedEx(client) 54.90±9.97 56.48±13.60 16.31±3.77 15.93±3.06 39.13±15.13 global SHA (server & client)47.38±3.40 46 .79±3.51 18.64±1.68 20.30±1.66 21.62±2.51 Successive + FedEx(client) 44.52±1.68 45.24±3.31 19.22±2.05 19.43±1.45 20.82±1.37 Halving person- SHA (server & client)46.77±3.61 48 .04±3.72 14.79±1.55 14.78±1.31 24.81±6.13 (SHA) alized + FedEx(client) 46.08±2.57 45 .89±3.76 14.97±1.31 14.76±1.70 21.77±2.83 5 Empirical Results In our experiments, we instantiate FedEx on the problem of tuning FedAvg, FedProx, and Reptile; the ﬁrst is the most popular algorithm for federated training, the second is an extension designed for heterogeneous devices, and the last is a compatible meta-learning method used for learning initializations for personalization. At communication round tthese algorithms use the aggregation Aggb(w,{wi}B i=1) = (1 −αt)w + αt ∑B i=1 |Tti| B∑ i=1 |Tti|wi (11) for some learning rate αt >0 that can vary through time; in the case of FedAvg we have αt = 1 ∀t. The local training sub-routine Locc is SGD with hyperparameters cover some objective deﬁned by the training data Tti, which can also depend on c. For example, to include FedProx we include in c an additional local hyperparameter for the proximal term compared with that of FedAvg. We tune several hyperparameters of both aggregation and local training; for the former we tune the server learning rate schedule and momentum, found to be helpful for personalization [19]; for the latter we tune the learning rate, momentum, weight-decay, the number of local epochs, the batch-size, dropout, and proximal regularization. Please see the supplementary material for the exact hyperparameter space considered. While we mainly evaluate FedEx in cross-device federated settings, which is generally more difﬁcult than cross-silo in terms of hyperparameter optimization, FedEx can be naturally applied to cross-silo settings, where the challenges of heterogeneity, privacy requirements, and personalization remain. Because our baseline is running Algorithm 1, a standard hyperparameter tuning algorithm, to tune all hyperparameters, and because we need to also wrap FedEx in such an algorithm for the reasons described in Section 3, our empirical results will test the following question: doesFedEx, wrapped by random search (RS) or a successive halving algorithm (SHA), do better than RS or SHA run with the same settings directly? Here “better” will mean both the ﬁnal test accuracy obtained and the online evaluation setting, which tests how well hyperparameter optimization is doing at intermediate phases. Furthermore, we also investigate whether FedEx can improve upon the wrapper alone even when targeting a good global and not personalized model, i.e. when elimination decisions are made using the average global validation loss. We run Algorithm 1 on the personalized objective and use RS and SHA with elimination rate η= 3, the latter following Hyperband [29]. To both wrappers we allocate the same (problem-dependent) tuning budget. To obtain the elimination rounds in Algorithm 1 for SHA, we set the number of eliminations to R= 3, ﬁx a total communication round budget, and ﬁx a maximum number of rounds to be allocated to any conﬁguration a; as detailed in the Appendix, this allows us to determine T1,...,T R so as to use up as much of the budget as possible. We evaluate the performance ofFedEx on three datasets (Shakespeare, FEMNIST, and CIFAR-10) on both vision and language tasks. We consider the following two different partitions of data: 1. Each device holds i.i.d. data. While overall data across the entire network can be non-i.i.d., we randomly shufﬂe local data within each device before splitting into train, validation, and test sets. 9Figure 4: Online evaluation of FedEx on the Shakespeare next-character prediction dataset (left), the FEMNIST image classiﬁcation dataset (middle), and the CIFAR-10 image classiﬁcation dataset (right) in the fully non-i.i.d. setting (except CIFAR-10). We report global model performance on the top and personalized performance on the bottom. All evaluations are run for three trials. 2. Each device holds non-i.i.d. data. In Shakespeare, each device is an actor and the local data is split according to the temporal position in the play; in FEMNIST, each device is the digit writer and the local data is split randomly; in CIFAR-10, we do not consider a non-i.i.d. setting. For Shakespeare and FEMNIST we use 80% of the data for training and 10% each for validation and testing. In CIFAR-10 we hold out 10K examples from the usual training/testing split for validation. The backbone models used for Shakespeare and CIFAR-10 follow from the FedAvg evaluation [39] and use 4K communications rounds (at most 800 round for each arm), while that of FEMNIST follows from LEAF [6] and uses 2K communication rounds (at most 200 for each arm). Table 1 presents our main results, displaying the ﬁnal test error of the target model after tuning using either a wrapper algorithm alone or its combination with FedEx. The evaluation shows that using FedEx on the client parameters is either equally or more effective in most cases; in particular, a FedEx-modiﬁed method performs best everywhere except i.i.d. FEMNIST, where it is very close. Furthermore, FedEx frequently improves upon the wrapper algorithm by 2 or more percentage points. We further present online evaluation results in Figure 4, where we display the test error of FedEx wrapped with SHA compared to SHA alone as a function of communication rounds. Here we see that for most of training FedEx is either around the same or better then the alternative, except at the beginning; the former is to be expected since the randomness of FedEx leads to less certain updates at initialization. Nevertheless FedEx is usually better than the SHA baseline by the halfway point. 6 Conclusion In this paper we study the problem of hyperparameter optimization in FL, starting with identifying the key challenges and proposing reasonable baselines that adapts standard approaches to the federated setting. We further make a novel connection to the weight-sharing paradigm from NAS—to our knowledge the ﬁrst instance of this being used for regular (non-architectural) hyperparameters— and use it to introduce FedEx. This simple, low-overhead algorithm for accelerating the tuning of hyperparameters in federated learning can be theoretically shown to successfully tune the step-size for multi-task OCO problems and effectively tunes FedAvg, FedProx, and Reptile on standard benchmarks. The scope of application of FedEx is very broad, including tuning actual architectural hyperparameters rather than just settings of local SGD, i.e. doing federated NAS, and tuning initialization-based meta-learning algorithms such as Reptile and MAML. Lastly, any work on FL comes with privacy and fairness risks due its frequent use of sensitive data; thus any application of our work must consider tools being developed by the community for mitigating such issues [35, 40]. 10Acknowledgments This material is based on work supported by the National Science Foundation under grants CCF- 1535967, CCF-1910321, IIS-1618714, IIS-1901403, SES-1919453, IIS-1705121, IIS-1838017, IIS-2046613 and IIS-2112471; the Defense Advanced Research Projects Agency under cooperative agreements HR00112020003 and FA875017C0141; an AWS Machine Learning Research Award; an Amazon Research Award; a Bloomberg Research Grant; a Microsoft Research Faculty Fellowship; an Amazon Web Services Award; a Facebook Faculty Research Award; funding from Booz Allen Hamilton Inc.; a Block Center Grant; and a Two Sigma Fellowship Award. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of any of these funding agencies. References [1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N. What- mough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In Proceedings of the 9th International Conference on Learning Representations, 2021. [2] Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated learning via posterior averaging: A new perspective and practical algorithms. In Proceedings of the 9th International Conference on Learning Representations, 2021. [3] Peter L. Bartlett, Elad Hazan, and Alexander Rakhlin. Adaptive online gradient descent. In Advances in Neural Information Processing Systems, 2008. [4] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281–305, 2012. [5] Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In Proceedings of the 7th International Conference on Learning Representations, 2019. [6] Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcný, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings. arXiv, 2018. [7] Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning for recom- mendation. arXiv, 2018. [8] Zhongxiang Dai, Kian Hsiang Low, and Patrick Jaillet. Federated bayesian optimization via thompson sampling. In Advances in Neural Information Processing Systems, 2020. [9] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four GPU hours. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. [10] Hubert Eichner, Tomer Koren, H. Brendan McMahan, Nathan Srebro, and Kunal Talwar. Semi- cyclic stochastic gradient descent. In Proceedings of the 36th International Conference on Machine Learning, 2019. [11] Thomas Elsken, Benedikt Stafﬂer, Jan Hendrik Metzen, and Frank Hutter. Meta-learning of neural architectures for few-shot learning. arXiv, 2019. [12] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning approach. In Advances in Neural Information Processing Systems, 2020. [13] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, 2017. [14] Anubhav Garg, Amit Kumar Saha, and Debo Dutta. Direct federated neural architecture search. arXiv, 2020. [15] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efﬁcient framework for clustered federated learning. arXiv, 2020. 11[16] Rishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm selection. SIAM Journal on Computing, 46(3):992–1017, 2017. [17] Chaoyang He, Murali Annavaram, and Salman Avestimehr. Towards non-i.i.d. and invisible data with FedNAS: Federated deep learning via neural architecture search. arXiv, 2020. [18] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In Proceedings of the International Conference on Learning and Intelligent Optimization, 2011. [19] Yihan Jiang, Jakub Koneˇcný, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. arXiv, 2019. [20] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Ar- jun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone ˇcný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. arXiv, 2019. [21] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In Proceedings of the 37th International Conference on Machine Learning, 2020. [22] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta- learning methods. In Advances in Neural Information Processing Systems, 2019. [23] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Provable guarantees for gradient- based meta-learning. In Proceedings of the 36th International Conference on Machine Learning, 2019. [24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. InProceedings of the 3rd International Conference on Learning Representations, 2015. [25] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132:1–63, 1997. [26] Antti Koskela and Antti Honkela. Learning rate adaptation for federated and differentially private learning. arXiv, 2018. [27] Guilin Li, Xing Zhang, Zitong Wang, Zhenguo Li, and Tong Zhang. StacNAS: Towards stable and consistent differentiable neural architecture search. arXiv, 2019. [28] Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private meta-learning. In Proceedings of the 8th International Conference on Learning Representations, 2020. [29] Liam Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy- perband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18(185):1–52, 2018. [30] Liam Li, Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Geometry-aware gradient algorithms for neural architecture search. In Proceedings of the 9th International Conference on Learning Representations, 2021. [31] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 12[32] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. arXiv, 2020. [33] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Chal- lenges, methods, and future directions. IEEE Signal Processing Magazine, 37, 2020. [34] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Proceedings of the Conference on Machine Learning and Systems, 2020. [35] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in feder- ated learning. In Proceedings of the 6th International Conference on Learning Representations, 2020. [36] Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, and Shenghua Gao. Towards fast adaptation of neural architectures with meta-learning. In Proceedings of the 8th International Conference on Learning Representations, 2020. [37] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In Proceedings of the 7th International Conference on Learning Representations, 2019. [38] Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv, 2020. [39] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artiﬁcal Intelligence and Statistics, 2017. [40] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In Proceedings of the 6th International Conference on Learning Representations, 2018. [41] Hesham Mostafa. Robust federated learning through representation matching and adaptive hyper-parameters. arXiv, 2019. [42] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv, 2018. [43] Hieu Pham, Melody Y . Guan, Barret Zoph, Quoc V . Le, and Jeff Dean. Efﬁcient neural architecture search via parameter sharing. In Proceedings of the 35th International Conference on Machine Learning, 2018. [44] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcný, Sanjiv Kumar, and H. Brendan McMahan. Adaptive federated optimization. In Proceedings of the 9th International Conference on Learning Representations, 2021. [45] Reuven Y . Rubinstein and Alexander Shapiro.Discrete Event Systems: Sensitivity Analysis and Stochastic Optimization by the Score Function Method. John Wiley & Sons, Inc., 1993. [46] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107—-194, 2011. [47] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learning. In Advances in Neural Information Processing Systems, 2017. [48] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, 2012. [49] Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step size using a learning-to-learn approach. In Proceedings of the 38th International Conference on Machine Learning, 2021. [50] Mengwei Xu, Yuxin Zhao, Kaigui Bian, Gang Huang, Qiaozhu Mei, and Xuanzhe Liu. Federated neural architecture search. arXiv, 2020. [51] Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation. arXiv, 2020. 13A Proof of Theorem 4.1 Proof. Let γt ∼Dθt be the step-size chosen at time t. Then we have that τE ¯Rτ = τ∑ t=1 Eγt m∑ i=1 ℓt,i ( w(wt,γt) t,i ) − m∑ i=1 ℓt,i(w∗ t) = τ∑ t=1 k∑ j=1 θt[j] m∑ i=1 ℓt,i ( w(wt,cj) t,i ) − m∑ i=1 ℓt,i(w∗ t) ≤log k η + ηkτm2b2 + min j∈[k] τ∑ t=1 m∑ i=1 ℓt,i ( w(wt,cj) t,i ) −min w∈W m∑ i=1 ℓt,i(w∗ t) ≤2mb √ τklog k+ min j∈[k] τ∑ t=1 1 2cj ∥wt −w∗ t∥2 2 + cjmG2 ≤2mb √ τklog k+ min j∈[k] D2(1 + logτ) 2cj + (V2 2cj + cjmG2 ) τ ≤2mb √ τklog k+ D2(1 + logτ) + V2τ 2γ∗ + γ∗mG2τ + min j∈[k] (1 cj − 1 γ∗ )D2(1 + logτ) + V2τ 2 + (cj −γ∗)mG2τ ≤2mb √ τklog k+ 4D √ τ + τlog τ 2 + ( 2V + D k ) Gτ √m 2 = mb √ 2τlog τ + 4D √ τ + τlog τ 2 + (DG+ 2GVτ ) √m 2 where the second line uses linearity of expectations over γt ∼Dθt, the third substitutes the bandit regret of EG [ 46, Corollary 4.2], the fourth substitutes η = 1 mb √ log k τk and the regret of OGD [46, Corollary 2.7], the ﬁfth substitutes the regret guarantee of Adaptive OGD over functions 1 2 ∥wt−w∗ t∥2 2 [3, Theorem 2.1] with step-size αt = 1/tand the deﬁnition of V, the sixth substitutes the best discretized step-size cj for the optimal γ∗ ∈ ( 0, D G √ 2m ] , and the seventh substitutes V 2G √ 2m + D 2G √ 1+log τ 2mτ for γ∗and arg minj:cj≥γ∗ for arg minjcj. Setting k 3 2 = DG b √ τ 2m and dividing both sides by τ yields the result. 14B Decomposing Federated Optimization Methods As detailed in Section 2 our analysis and use of FedEx to tune local training hyperparameters depends on a formulation that decomposes FL methods into two subroutines: a local training routine Locc(S,w) with hyperparameters cover data S and starting from initialization w and an aggregation routine Aggbwith hyperparameters b. In this section we discuss how a variety of federated optimization methods, including several of the best-known, can be decomposed in this manner. This enables the application of FedEx to tune their hyperparameters. B.1 FedAvg [39] The best-known FL method, FedAvg runs SGD on each client in a batch starting from a shared initialization and then updates to the average of the last iterate of the clients, often weighted by the number of data points each client has. The decomposition here is: Locc Local SGD (or another gradient-based algorithm, e.g. Adam [24]), with cbeing the standard hyperparameters such as step-size, momentum, weight-decay, etc. Aggb Weighted averaging, with no hyperparameters in b. B.2 FedProx [34] FedProx has the same decomposition as FedAvg except local SGD is replaced by a proximal version that regularizes the routine to be closer to the initialization, adding another hyperparameter to c governing the strength of this regularization. B.3 Reptile [42] A well-known meta-learning algorithm, Reptile has the same decomposition as FedAvg except the averaged aggregation is replaced by a convex combination of the initialization and the average of the last iterates, as in Equation 11. This adds another hyperparameter to bgoverning the tradeoff between the two. B.4 SCAFFOLD [21] SCAFFOLD comes in two variants, both of which compute and aggregate control variates in parallel to the model weights. The decomposition here is: Locc Local SGD starting from a weight initialization with a control variate, which can be merged to form the local training initialization. The hyperparameters in care the same as in FedAvg. Aggb Weighted averaging of both the initialization and the control variates, with the same hyper- parameters as Reptile. B.5 FedDyn [1] In addition to a FedAvg/FedProx/Reptile-like training routine, this algorithm maintains a regu- larizer on each device that affects the local training routine. While this statefulness cannot strictly be subsumed in our decomposition, since it does not introduce any additional hyperparameters the remaining hyperparameters can be tuned in the same manner as we do forFedAvg/FedProx/Reptile. In order to choose between using FedDyn or not, one can introduce a binary hyperparameter to c specifying whether or not Locc uses that term in the objective it optimizes or not, allowing it also to be tuned via FedEx. B.6 FedPA [2] This algorithm replaces local SGD in FedAvg by a local Markov-chain Monte Carlo (MCMC) routine starting from the initialization given by aggregating the previous round’s MCMC routines. The decomposition is then just a replacement of local SGD and its associated hyperparameters by local MCMC and its hyperparameters, with the aggregation routine remaining the same. 15B.7 Ditto [32] Although it depends on what solver is used for the local solver and aggregation routines, in the simplest formulation, the local optimization of personalized models involves an additional regular- ization hyperparameter. While the updating rule of Ditto is different from that of FedProx, the hyperparameters can be decomposed and tuned in a similar manner. B.8 MAML [13] A well-known meta-learning algorithm, MAML takes one or more full-batch gradient descent (GD) steps locally and updates the global model using a second-order gradient using validation data. The decomposition here is : Locc Local SGD starting from a weight initialization. The hyperparameters in care the same as in FedAvg. The algorithm also returns second-order information required to compute the meta-gradient. Aggb Meta-gradient computation, summation, and updating using a standard descent method like Adam [24]. The hyperparameters in bare the hyperparameters of the latter. C FedEx Details C.1 Stochastic Gradient used by FedEx Below is a simple calculation showing that the stochastic gradient used to update the categorical architecture distribution of FedEx is an unbiased approximation of the true gradient w.r.t. its parameters. ∇θjEcij|θLVti(wi) = ∇θjEcij|θ(LVti(wi) −λ) = Ecij|θ ( (LVti(wi) −λ)∇θj log Pθ(cij) ) = Ecij|θ ( (LVti( ˆwk) −λ)∇θj log n∏ i=1 Pθ(cij = cj) ) = Ecij|θ ( (LVti(wi) −λ) n∑ i=1 ∇θj log Pθ(cij = cj) ) = Ecij|θ ((LVti(wi) −λ)1cij=cj θj ) Note that this use of the reparameterization trick has some similarity with a recent RL approach to tune the local step-size and number of epochs [41]; however, FedEx can be rigorously formulated as an optimization over the personalization objective, has provable guarantees in a simple setting, uses a different conﬁguration distribution that leads to our exponentiated update, and crucially for practical deployment does not depend on obtaining aggregate reward signal on each round. 16C.2 FedEx wrapped with SHA For completeness, we present the pseudo code of wrapping FedEx with SHA in Algorithm 3 below. Algorithm 3: FedEx wrapped with SHA Input: distribution Dover hyperparameters A, elimination rate η∈N, elimination rounds τ0 = 0,τ1,...,τ R sample set of ηR hyperparameters H ∼D[ηR] initialize a model wa ∈Rd for each a∈H for elimination round r∈[R] do for setting a= (b,c) ∈H do sa,wa,θa ←FedEx (wa,b,c,θ a,τr+1 −τr) H ←{a∈H : sa ≤1 η-quantile({sa : a∈H})} Output: remaining a∈H and associated model wa FedEx (w,b, {c1,...,c k},θ,τ ≥1): initialize θ1 ←θ initialize shared weights w1 ←w for comm. round t= 1,...,τ do for client i= 1,...,B do send wt,θt to client sample cti ∼Dθt wti ←Loccti(Tti,wt) send wti,cti,LVti(wti) to server wt+1 ←Aggb(w,{wti}B i=1) set step size ηt and baseline λt ˜∇j ← ∑B i=1 |Vti|(LVti(wti)−λt)1cti=cj θt[j] ∑B i=1 |Vti| ∀j θt+1 ←θt ⊙exp(−ηt˜∇) θt+1 ←θt+1/∥θt+1∥1 s←∑B i=1 |Vti|LVti/∑B i=1 |Vti| Return s, model w, hyperparameter distribution θ C.3 Hyperparameters of FedEx We tune the computation of the baseline λt, which we set to λt = 1∑ s<tγt−s ∑ s<t γt−s ∑B i=1 |Vti| B∑ i=1 LVti(wi) for discount factor γ ∈[0,1]. As discussed in Section 3, the local perturbation factor is set to ε= 0.1. 27 conﬁgurations are used in each arm for SHA and RS. The number of conﬁguration used per arm of FedEx (i.e. the dimensionality of θ) is the same (27). 17D Experimental Details Code implementing FedEx is available at https://github.com/mkhodak/fedex. The code auto- matically downloads CIFAR-10 data, while Shakespeare and FEMNIST data is made available by the LEAF repository: https://github.com/TalwalkarLab/leaf. D.1 Settings of the Baseline/Wrapper Algorithm We use the same settings of Algorithm 1 for both tuning FedAvg and for wrapping FedEx. Given an elimination rate η, number of elimination rounds R, resource budget B, and maximum rounds per arm M, we assign T1,...,T R s.t. Ti −Ti−1 = T −M ηn+1−1 η−1 −n−1 (recall T0 = 0) and assign any remaining resources to maximize resource use. All remaining details were noted in Section 5. D.2 Hyperparameters of FedAvg/FedProx/Reptile Server hyperparameters (learning rate αt = γt): log10 lr : Unif[ −1,1] momentum : Unif[0 ,0.9] log10(1 −γ) : Unif[ −4,−2] Local training hyperparameters (note we only use 1 epoch for Shakespeare to conserve computation): log10(lr) : Unif[ −4,0] momentum : Unif[0 .0,1.0] log10(weight-decay) : Unif[ −5,−1] epoch : Unif {1,2,3,4,5} log2(batch) : Unif {3,4,5,6,7} dropout : Unif[0 ,0.5] E Conﬁdence Intervals Table 2: Final test error obtained when tuning using a standard hyperparameter tuning algorithm (SHA or RS) alone, or when using it for server (aggregation) hyperparameters while FedEx tunes client (on-device training) hyperparameters. The target model is the one used to compute on-device validation error by the wrapper method, as well as the one used to compute test error after tuning. The conﬁdence intervals displayed are 90% Student-t conﬁdence intervals for the mean estimates from Table 1, with 5 independent trials for Shakespeare, 10 for FEMNIST, 10 for RS on CIFAR, and 6 for SHA on CIFAR. Wrapper Target Tuning Shakespeare FEMNIST CIFAR-10 method model method i.i.d. non-i.i.d. i.i.d. non-i.i.d. i.i.d. global RS (server & client)60.32±9.56 64.36±13.53 22.81±2.64 22.98±1.98 30.46±5.47 Random + FedEx(client) 53.94±8.70 57.70±16.75 20.96±2.77 22.30±2.12 34.83±8.54 Search person- RS (server & client) 61.10±8.89 61.71±8.66 17.45±1.63 17.77±1.52 34.89±6.12 (RS) alized + FedEx(client) 54.90±9.50 56.48±12.97 16.31±2.19 15.93±1.77 39.13±8.77 global SHA (server & client)47.38±3.24 46.79±3.35 18.64±0.97 20.30±0.96 21.62±1.45 Successive + FedEx(client) 44.52±1.60 45.24±3.16 19.22±1.19 19.43±0.84 20.82±0.79 Halving person- SHA (server & client)46.77±3.44 48.04±3.54 14.79±0.90 14.78±0.75 24.81±3.55 (SHA) alized + FedEx(client) 46.08±2.45 45.89±3.58 14.97±0.76 14.76±0.99 21.77±1.64 18Figure 5: Comparison of different εsettings for the local perturbation component of FedEx from Section 3. Figure 6: Comparison of step-size schedules for ηt in FedEx. In practice we chose the ‘aggres- sive’ schedule, which exhibits faster convergence to favorable conﬁgurations. F Ablation Studies We now discuss two design choices of FedEx and how they affect performance of the algorithm. First, the choice of the local perturbation ε = 0.1 discussed in Section 3; we choose this setting due to its consistent performance across several settings. In Figure 5 we plot the performance of FedEx on CIFAR-10 between ε= 0.0 (no FedEx, i.e. SHA only) and ε= 1.0 (full FedEx, i.e. client conﬁgurations are chosen independently) and show that while the use of a nonzero εis important, performance at fairly low values of εis roughly similar. We further investigated the setting of the step-size ηt for the exponentiated gradient update in FedEx. We examine three different approaches: a constant rate of ηt = √2 logk, an ‘adaptive’ schedule of ηt = √2 logk/ √∑ s≤t∥˜∇s∥2∞, and an ‘aggressive’ schedule ofηt = √2 logk/∥˜∇t∥∞. Here ˜∇t is the stochastic gradient w.r.t. θcomputed in Algorithm 2 at step tand the form of the step-size is derived from standard settings for exponentiated gradient in online learning [46]. We found that the ‘aggressive’ schedule works best in practice, as shown in Figure 6. A key issue with using the ‘constant’ and ‘adaptive’ approaches is that they continue to assign high probability to several conﬁgurations late in the tuning process; this slows down training of the shared weights. One could consider a tradeoff between allowingFedEx to run longer than while keeping the total budget constant, but for simplicity we chose the more effective ‘aggressive’ schedule. 19",
      "meta_data": {
        "arxiv_id": "2106.04502v2",
        "authors": [
          "Mikhail Khodak",
          "Renbo Tu",
          "Tian Li",
          "Liam Li",
          "Maria-Florina Balcan",
          "Virginia Smith",
          "Ameet Talwalkar"
        ],
        "published_date": "2021-06-08T16:42:37Z",
        "pdf_url": "https://arxiv.org/pdf/2106.04502v2.pdf",
        "github_url": "https://github.com/mkhodak/fedex"
      },
      "llm_extracted_info": {
        "main_contributions": "This research systematically investigates the problem of federated hyperparameter tuning, identifying key challenges such as federated validation data, extreme resource limitations, and evaluating personalization. It introduces FedEx, a novel method that accelerates federated hyperparameter tuning by making a new connection to the weight-sharing technique from neural architecture search (NAS). FedEx is applicable to widely-used federated optimization methods like FedAvg and its variants. Theoretically, a FedEx variant is shown to correctly tune on-device learning rates in online convex optimization. Empirically, FedEx outperforms natural baselines on Shakespeare, FEMNIST, and CIFAR-10 benchmarks, achieving higher accuracy with the same training budget. This is the first systematic analysis of hyperparameter optimization in the federated setting and the first application of weight-sharing to regular (non-architectural) hyperparameters.",
        "methodology": "The study formalizes hyperparameter optimization in federated learning (FL) and adapts standard non-federated algorithms (Random Search, Successive Halving) to form baselines. The core method, FedEx, exploits a connection between FL hyperparameter tuning and NAS weight-sharing. It frames the personalized tuning objective as a single-level empirical risk minimization and applies a stochastic relaxation. FedEx alternates between a standard SGD-like update for model weights and an exponentiated gradient update for a categorical distribution over local hyperparameters. It is applicable to FL methods decomposable into local training (Locc) and aggregation (Aggb). A 'local perturbation' scheme is used for selecting initial hyperparameter configurations to ensure stability. For theoretical analysis, the ARUBA (Average Regret-Upper-Bound Analysis) framework is used to prove guarantees for a FedEx variant tuning client step-sizes in an online convex optimization setting.",
        "experimental_setup": "Experiments were conducted on three standard FL benchmarks: Shakespeare (next-character prediction), FEMNIST (image classification), and CIFAR-10 (image classification). Data partitions included both i.i.d. and non-i.i.d. settings for Shakespeare and FEMNIST, while only i.i.d. was used for CIFAR-10. Data splits were 80% training, 10% validation, and 10% testing for Shakespeare and FEMNIST, and 10K examples from the training/testing split for CIFAR-10 validation. Backbone models from FedAvg evaluation (Shakespeare, CIFAR-10) and LEAF (FEMNIST) were used, with 4K and 2K communication rounds respectively. FedEx was instantiated on FedAvg, FedProx, and Reptile, tuning both server hyperparameters (learning rate schedule, momentum) and client hyperparameters (learning rate, momentum, weight-decay, local epochs, batch-size, dropout, proximal regularization). Baselines included Random Search (RS) and Successive Halving Algorithm (SHA) with an elimination rate of η=3. Performance was evaluated using final test error and online evaluation (test error over communication rounds). Multiple independent trials (5 for Shakespeare, 10 for FEMNIST, 10 for RS on CIFAR, 6 for SHA on CIFAR) were conducted for robust results.",
        "limitations": "FedEx is primarily applicable to FL algorithms that can be decomposed into local fine-tuning and aggregation, implying that other types of FL algorithms may still require tuning via baselines. Furthermore, FedEx itself relies on wrapper algorithms like SHA or RS to tune its own internal hyperparameters (e.g., baseline calculation λt) and certain server-side FL hyperparameters (e.g., server learning rate). The local perturbation scheme used by FedEx, while improving stability, intrinsically limits the size of the search space explored by each instance of the algorithm. The theoretical analysis for tuning the step-size in an online convex optimization setting, while providing guarantees, incurs an additional m^(1/3) factor compared to existing bounds. Practical observations showed that 'constant' and 'adaptive' step-size schedules for FedEx could assign high probability to multiple configurations late in the tuning process, potentially slowing down training of shared weights.",
        "future_research_directions": "The authors suggest several future research directions. These include extending FedEx to tune actual architectural hyperparameters, effectively performing federated neural architecture search (NAS). Another avenue is to apply FedEx to tune initialization-based meta-learning algorithms such as MAML. The paper also highlights the general importance of considering privacy and fairness risks in FL, suggesting that any application of their work must integrate community-developed tools for mitigating these issues. Additionally, further work could explore the time-dependency of federated evaluation, a challenge not fully addressed in this paper. Investigating trade-offs for FedEx step-size schedules, such as allowing FedEx to run longer while keeping the total budget constant, is also a potential area for exploration.",
        "experimental_code": "def discounted_mean(trace, factor=1.0):weight = factor ** np.flip(np.arange(len(trace)), axis=0)return np.inner(trace, weight) / weight.sum()class FedEx:    '''runs hyperparameter optimization given a federated learning server'''    def entropy(self):        entropy = 0.0        for probs in product(*(theta[theta>0.0] for theta in self._theta)):            prob = np.prod(probs)            entropy -= prob * np.log(prob)        return entropy    def mle(self):        return np.prod([theta.max() for theta in self._theta])    def __init__(                 self,                  server,                  configs,                  eta0='auto',                  sched='auto',                  cutoff=0.0,                  baseline=0.0,                  diff=False,                 ):        '''        Args:            server: Object that implements two methods, 'communication_round' and 'full_evaluation'                    taking as input a single argument, 'get_config', itself a function that takes                     no inputs and outputs an element of the provided list 'configs'.                     - 'communication_round' samples a batch of clients, assigns a config to each                     using 'get_config', and runs local training using that config. It then                     aggregates the local models to to take a training step and returns three lists                     or arrays: a list of each client's validation error before local training, a                     list of each client's validation error after local training, and a list of each                     client's weight (e.g. size of its validation set).                     - 'full_evaluation' assigns a config to each client using 'get_config' and runs                    local training using that config. It then returns three lists or arrays: a list                    of each client's test error before local training, a list of each client's test                    error after local training, and a list of each client's weight (e.g. size of                     its test set).            configs: list of configs used for local training and testing by 'server'                      OR dict of (string, list) pairs denoting a grid of configs            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))            sched: learning rate schedule for exponentiated gradient:                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)                    - 'aggressive': uses eta0 / gradient l-infinity norm                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds                    - 'constant': uses eta0                    - 'scale': uses sched * sqrt(2 * log(len(configs)))            cutoff: entropy level below which to stop updating the config probability and use MLE            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean            diff: if True uses performance difference; otherwise uses absolute performance        '''        self._server = server        self._configs = configs        self._grid = [] if type(configs) == list else sorted(configs.keys())        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]        self._sched = sched        self._cutoff = cutoff        self._baseline = baseline        self._diff = diff        self._z = [np.full(size, -np.log(size)) for size in sizes]        self._theta = [np.exp(z) for z in self._z]        self._store = [0.0 for _ in sizes]        self._stopped = False        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}    def stop(self):        self._stopped = True    def sample(self, mle=False, _index=[]):        '''samples from configs using current probability vector'''        if mle or self._stopped:            if self._grid:                return {param: self._configs[param][theta.argmax()]                         for theta, param in zip(self._theta, self._grid)}            return self._configs[self._theta[0].argmax()]        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])        if self._grid:            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}        return self._configs[_index[-1][0]]    def settings(self):        '''returns FedEx input settings'''        output = {'configs': deepcopy(self._configs)}        output['eta0'], output['sched'] = self._eta0, self._sched        output['cutoff'], output['baseline'] = self._cutoff, self._baseline         if self._trace['refine']:            output['theta'] = self.theta()        return output    def step(self):        '''takes exponentiated gradient step (calls 'communication_round' once)'''        index = []        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))                before, after = np.array(before), np.array(after)        weight = np.array(weight, dtype=np.float64) / sum(weight)        if self._trace['refine']:            trace = self.trace('refine')            if self._diff:                trace -= self.trace('global')            baseline = discounted_mean(trace, self._baseline)        else:            baseline = 0.0        self._trace['global'].append(np.inner(before, weight))        self._trace['refine'].append(np.inner(after, weight))        if not index:            self._trace['entropy'].append(0.0)            self._trace['mle'].append(1.0)            return        for i, (z, theta) in enumerate(zip(self._z, self._theta)):            grad = np.zeros(len(z))            for idx, s, w in zip(index, after-before if self._diff else after, weight):                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]            if self._sched == 'adaptive':                self._store[i] += norm(grad, float('inf')) ** 2                denom = np.sqrt(self._store[i])            elif self._sched == 'aggressive':                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))            elif self._sched == 'auto':                self._store[i] += 1.0                denom = np.sqrt(self._store[i])            elif self._sched == 'constant':                denom = 1.0            elif self._sched == 'scale':                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')            else:                raise NotImplementedError            eta = self._eta0[i] / denom            z -= eta * grad            z -= logsumexp(z)            self._theta[i] = np.exp(z)        self._trace['entropy'].append(self.entropy())        self._trace['mle'].append(self.mle())        if self._trace['entropy'][-1] < self._cutoff:            self.stop()    def test(self, mle=False):        '''evaluates found config (calls 'full_evaluation' once)        Args:            mle: use MLE config instead of sampling        Returns:            output of 'full_evaluation'        '''        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))        return {'global': np.inner(before, weight) / weight.sum(),                'refine': np.inner(after, weight) / weight.sum()}    def theta(self):        '''returns copy of config probability vector'''        return deepcopy(self._theta)    def trace(self, key):        '''returns trace of one of three tracked quantities        Args:            key: 'entropy', 'global', or 'refine'        Returns:            numpy vector with length equal to number of calls to 'step'        '''        return np.array(self._trace[key])class Server:    '''object for federated training implementing methods required by FedEx'''    def _set_test_state(self):        state = (np.random.get_state(), torch.get_rng_state(), torch.cuda.get_rng_state())        if self._state is None:            self._state = state        else:            np.random.set_state(self._state[0])            torch.set_rng_state(self._state[1])            torch.cuda.set_rng_state(self._state[2])        return state    def _reset_state(self, state):        np.random.set_state(state[0])        torch.set_rng_state(state[1])        torch.cuda.set_rng_state(state[2])    def __init__(                 self,                  model,                  clients,                  train,                  test,                  lr=1.0,                  momentum=0.0,                  step=1,                  gamma=1.0,                  batch=10,                  state=None,                 ):        '''        Args:            model: PyTorch model            clients: list of clients, each a function that takes one or more strings 'train',                     'val', 'test' and returns, as one tuple, input and output tensors for each            train: method that takes as argument a PyTorch model, an input tensor, an output                   tensor, and optional kwargs and returns the same PyTorch model            test: method that takes as argument a PyTorch model, an input tensor, and an output                  tensor and returns the model's error            lr: server learning rate            momentum: server momentum            step: server learning rate decay interval            gamma: server learning rate decay factor            batch: number of clients to sample per communication round            state: np.random, torch, torch.cuda random state tuple; if None uses current states        '''        self._model = model        self._clients = clients        self._train = train        self._test = test        self._opt = optim.SGD(self._model.parameters(), lr=lr, momentum=momentum)        self._sched = optim.lr_scheduler.StepLR(self._opt, step, gamma=gamma)        self._batch = batch        self._state = state        self._reset_state(self._set_test_state())    def communication_round(self, get_config):        '''runs one step of local training and model aggregation        Args:            get_config: returns kwargs for 'train' as a dict        Returns:            np.array objects for global val error, local val error, and val size of each client        '''        self._model.cuda()        before, after, weight = [np.zeros(self._batch) for _ in range(3)]        total = 0.0        for i in range(self._batch):            Xtrain, Ytrain, Xval, Yval = random.choice(self._clients)('train', 'val')            before[i] = self._test(self._model, Xval, Yval)            model = self._train(deepcopy(self._model), Xtrain, Ytrain, **get_config())            after[i] = self._test(model, Xval, Yval)            weight[i] = len(Yval)            total += len(Ytrain)            if i:                for agg, param in zip(aggregate.parameters(), model.parameters()):                    agg.data += len(Ytrain) * param.data            else:                for param in model.parameters():                    param.data *= len(Ytrain)                aggregate = model        self._opt.zero_grad()        for agg, param in zip(aggregate.parameters(), self._model.parameters()):            param.grad = param.data - agg / total        self._opt.step()        self._opt.zero_grad()        self._sched.step()        self._model.cpu()        return before, after, weight    def full_evaluation(self, get_config):        '''evaluates personalization on each client        Args:            get_config: returns kwargs for 'train' as a dict        Returns:            np.array objects for global test error, local test error, and test size of each client        '''        state = self._set_test_state()        self._model.cuda()        before, after, weight = [np.zeros(len(self._clients)) for _ in range(3)]        for i, client in enumerate(self._clients):            Xtrain, Ytrain, Xtest, Ytest = client('train', 'test')            before[i] = self._test(self._model, Xtest, Ytest)            after[i] = self._test(self._train(deepcopy(self._model),                                               Xtrain, Ytrain, **get_config()),                                   Xtest, Ytest)            weight[i] = len(Ytest)            print('\\r\\tEvaluated client', frac(i+1, len(self._clients)),                  '    global error:', round(np.inner(before, weight) / weight.sum(), 4),                  '    refine error:', round(np.inner(after, weight) / weight.sum(), 4),                   end=32*' ')        self._model.cpu()        self._reset_state(state)        return before, after, weightdef wrapped_fedex(                  get_server,                  get_client,                  num_configs=1,                  prod=False,                  stepsize_init='auto',                  stepsize_sched='aggressive',                  cutoff=1E-4,                  baseline_discount=-1.0,                  diff=False,                  mle=False,                  logdir=None,                  val_discount=0.0,                  last_stop=False,                  eval_global=False,                  **kwargs,                  ):    '''evaluates FedEx wrapped with successive elimination algorithm;       uses FedAvg when num_configs = 1 and prod = False    Args:        get_server: function that takes no input and returns an object that can be passed as the                     first argument to FedEx.__init__, e.g. a Server object        get_client: function that takes no input and returns a dict of local training configs, a                    list of which is passed as the second argument to 'FedEx.__init__'; can also                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'        num_configs: determines number of configs in the list passed to 'FedEx.__init__':                     - >0: use this value directly                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]                     - =-1: use the number of arms given by the wrapper                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':                           - >0.0: use this value directly                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]        diff: passed to 'diff' kwarg of 'FedEx.__init__'        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'        logdir: passed to 'logdir' kwarg of 'successive_elimination'        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'        kwargs: passed to 'get_schedule'    Returns:        FedEx object    '''    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')    if num_configs < -1:        samples = lambda n: random.randint(1, -num_configs)    elif num_configs == -1:        samples = lambda n: n    elif num_configs == 0:        samples = lambda n: random.randint(1, n)    else:        samples = lambda n: num_configs    if baseline_discount < 0.0:        baseline = lambda: random.uniform(0.0, -baseline_discount)    else:        baseline = lambda: baseline_discount    def sampler(n):        for _ in range(n):            yield FedEx(                        get_server(),                         get_client() if prod else get_client(samples(n)),                        eta0=stepsize_init,                         sched=stepsize_sched,                         cutoff=cutoff,                         baseline=baseline(),                        diff=diff,                        )    return successive_elimination(                                  sampler,                                   ['refine', 'global'],                                   logdir=logdir,                                   val_discount=val_discount,                                  elim_rate=elim_rate,                                   elim_sched=elim_sched,                                   eval_sched=eval_sched,                                  traces=['entropy', 'mle', 'global', 'refine'],                                   last_round='stop' if last_stop else None,                                  mle=mle,                                  eval_global=eval_global,                                  )import argparseimport jsonimport osimport pdbimport pickleimport randomimport reimport stringimport mathfrom copy import deepcopyfrom collections import defaultdictfrom glob import globimport numpy as npimport torch; torch.backends.cudnn.benchmark = Truefrom torch import nnfrom torch import optimfrom hyper import wrapped_fedexfrom hyper import Serverimport torch.nn.functional as Fimport torchvision.datasets as datasetsimport torchvision.transforms as transforms CLIENT = lambda: {'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),'momentum': np.random.uniform(low=0.0,high=1.0),'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),'epochs': np.random.choice(np.arange(1, 6)),'batch': 2 ** np.random.choice(np.arange(3, 8)),'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),'dropout': np.random.uniform(low=0.0, high=0.5),}class CNN(nn.Module):    def __init__(self):        super(CNN, self).__init__()        self.conv1 = nn.Sequential(                                   nn.Conv2d(3, 32, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.conv2 = nn.Sequential(                                   nn.Conv2d(32, 64, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.conv3 = nn.Sequential(                                   nn.Conv2d(64, 64, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.dropout = nn.Dropout(0.0)        self.fc = nn.Sequential(                                nn.Linear(1024, 64),                                nn.ReLU(),                                )        self.clf = nn.Linear(64, 10)    def forward(self, x):        x = self.conv1(x)        x = self.conv2(x)        x = self.conv3(x)        x = self.fc(self.dropout(x.flatten(1)))        return self.clf(self.dropout(x))def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):    if not mu:        return criterion    mu *= 0.5    model0 = [param.data.clone() for param in model.parameters()]    def objective(*args, **kwargs):        prox = sum((param-param0).pow(2).sum()                   for param, param0 in zip(model.parameters(), model0))        return criterion(*args, **kwargs) + mu * prox    return objectivedef train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):    optimizer = optim.SGD(model.parameters(), **kwargs)    criterion = get_prox(model, mu=mu)    model.dropout.p = dropout    model.train()    m = len(Y)    for e in range(epochs):        randperm = torch.randperm(m)        X, Y = X[randperm], Y[randperm]        for i in range(0, m, batch):            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]            pred = model(Xbatch)            loss = criterion(pred, Ybatch)            optimizer.zero_grad()            loss.backward()            optimizer.step()    model.eval()    return modeldef get_client(n_clients=1):    if args.lr_only:        return [SIMPLE_CLIENT()]    initial_client = CLIENT()    client_arr = [initial_client]    eps = args.eps    for i in range(n_clients-1):        other_client = deepcopy(initial_client)                log_lr = np.log10(other_client['lr'])        other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)                other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)                log_wd = np.log10(other_client['weight_decay'])        other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)                epochs_range = math.ceil(eps * 4)        other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)        log_batch = int(np.log2(other_client['batch']))        batch_range = math.ceil(eps * 4)        other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)                log_mu = np.log10(other_client['mu'])        other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)                other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)        client_arr.append(other_client)    return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr",
        "experimental_info": "Datasets: CIFAR-10, FEMNIST, Shakespeare.Specific data settings include: CIFAR-10: 500 clients, 0.2 validation proportion. FEMNIST: 0.1 validation proportion, 0.1 test proportion. Shakespeare: sequence length 80, 0.1 validation proportion, 0.1 test proportion. Both FEMNIST and Shakespeare can be configured for IID client data.Model Architectures: For CIFAR-10 and FEMNIST, a CNN is used with 3 convolutional layers (32, 64, 64 filters respectively) followed by a fully connected layer (1024 to 64 units) and a classification layer (64 to 10 for CIFAR-10, 62 for FEMNIST). For Shakespeare, a CharLSTM model is used with configurable hidden size (default 256) and number of layers (default 2), an embedding layer, and a linear output layer.Hyperparameter Search Spaces (Randomly Sampled):Server-side (from SERVER lambda):    - 'lr': 10.0 ** U(-1.0, 1.0) (learning rate)    - 'momentum': Choice([0.0, 0.9])    - 'step': 1 (learning rate decay interval)    - 'gamma': 1.0 - 10.0 ** U(-4.0, -2.0) (learning rate decay factor)Client-side (from CLIENT lambda, for local training):    - 'lr': 10.0 ** U(-4.0, 0.0) (learning rate)    - 'momentum': U(0.0, 1.0)    - 'weight_decay': 10.0 ** U(-5.0, -1.0)    - 'epochs': Choice(arange(1, 6))    - 'batch': 2 ** Choice(arange(3, 8))    - 'mu': 10.0 ** U(-5.0, 0.0) (proximal term coefficient)    - 'dropout': U(0.0, 0.5)FedEx-specific Settings (controlled by command-line arguments):    - '--configs' (default: 1): Number of hyperparameter configurations to optimize over with FedEx. Can be >0 (direct value), =0 (random from 1 to number of arms), =-1 (number of arms given by wrapper), or < -1 (random from 1 to abs(num_configs)). When `--configs`=1 and `--uniform`/`--random` are false, it behaves like FedAvg.    - '--lr_only' (action='store_true'): Tune only learning rate as a hyperparameter.    - '--eps' (default: 0.0 for CIFAR/FEMNIST, 0.1 for Shakespeare): Multiplicative perturbation to client config; eps=0.0 effectively means FedAvg (no perturbation).    - '--uniform' (action='store_true'): Run FedEx over a product set of single-parameter uniform grids.    - '--random' (action='store_true'): Run FedEx over a product set of single-parameter random grids.    - '--eta0' (default: 0.0): FedEx initial step size; if 0.0, uses FedEx default ('auto' which is sqrt(2*log(size))).    - '--sched' (default: 'aggressive'): FedEx step size schedule ('adaptive', 'aggressive', 'auto', 'constant', 'scale').    - '--cutoff' (default: 0.0): Stop updating FedEx config distribution if entropy falls below this value.    - '--baseline' (default: -1.0): How FedEx computes the baseline. If >=-1.0,<0.0, samples discount factor from [0.0, abs(args.baseline)). If =0.0, uses most recent value. If >0.0,<1.0, uses geometrically discounted mean. If =1.0, uses mean of all values.    - '--diff' (action='store_true'): Use difference between refine and global error as FedEx objective.    - '--stop' (action='store_true'): Stop updating FedEx config distribution after the last elimination round.Wrapper Algorithm Settings:    - '--rounds' (default: 800 for CIFAR/Shakespeare, 50 for FEMNIST): Maximum number of communication rounds (max_resources).    - '--total' (default: 4000 for CIFAR/Shakespeare, 450 for FEMNIST): Total number of communication rounds (total_resources).    - '--rate' (default: 3): Multiplicative elimination rate.    - '--elim' (default: 0): Number of elimination rounds; if 0, runs random search.    - '--eval' (default: 1): Number of evaluation rounds.    - '--discount' (default: 0.0): Discount factor for computing the validation score of an arm.Evaluation Settings:    - '--mle' (action='store_true'): Use MLE config at test time.    - '--loss' (action='store_true'): Use loss instead of error as evaluation metric.    - '--eval_global' (action='store_true'): Use global error as elimination metric instead of refine error.Other settings:    - BATCH (constant in code): 100, used for test function.    - Server batch size (args.batch, default: 10): Number of clients to sample per communication round.    - '--seed' (default: 0): Random seed."
      }
    },
    {
      "title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization is crucial for obtaining peak performance of\nmachine learning models. The standard protocol evaluates various hyperparameter\nconfigurations using a resampling estimate of the generalization error to guide\noptimization and select a final hyperparameter configuration. Without much\nevidence, paired resampling splits, i.e., either a fixed train-validation split\nor a fixed cross-validation scheme, are often recommended. We show that,\nsurprisingly, reshuffling the splits for every configuration often improves the\nfinal model's generalization performance on unseen data. Our theoretical\nanalysis explains how reshuffling affects the asymptotic behavior of the\nvalidation loss surface and provides a bound on the expected regret in the\nlimiting regime. This bound connects the potential benefits of reshuffling to\nthe signal and noise characteristics of the underlying optimization problem. We\nconfirm our theoretical results in a controlled simulation study and\ndemonstrate the practical usefulness of reshuffling in a large-scale, realistic\nhyperparameter optimization experiment. While reshuffling leads to test\nperformances that are competitive with using fixed splits, it drastically\nimproves results for a single train-validation holdout protocol and can often\nmake holdout become competitive with standard CV while being computationally\ncheaper.",
      "full_text": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization Thomas Nagler∗ Lennart Schneider∗ Bernd Bischl Matthias Feurer t.nagler@lmu.de Department of Statistics, LMU Munich Munich Center for Machine Learning (MCML) Abstract Hyperparameter optimization is crucial for obtaining peak performance of ma- chine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide opti- mization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross- validation scheme, are often recommended. We show that, surprisingly, reshuffling the splits for every configuration often improves the final model’s generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simula- tion study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper. 1 Introduction Hyperparameters have been shown to strongly influence the performance of machine learning models (van Rijn & Hutter, 2018; Probst et al., 2019). The primary goal of hyperparameter optimization (HPO; also called tuning) is the identification and selection of a hyperparameter configuration (HPC) that minimizes the estimated generalization error (Feurer & Hutter, 2019; Bischl et al., 2023). Typically, this task is challenged by the absence of a closed-form mathematical description of the objective function, the unavailability of an analytic gradient, and the large cost to evaluate HPCs, categorizing HPO as a noisy, black-box optimization problem. An HPC is evaluated via resampling, such as a holdout split or M-fold cross-validation (CV), during tuning. These resampling splits are usually constructed in a fixed and instantiated manner, i.e., the same training and validation splits are used for the internal evaluation of all configurations. On the one hand, this is an intuitive approach, as it should facilitate a fair comparison between HPCs and reduce the variance in the comparison.1 On the other hand, such a fixing of train and validation splits might steer the optimization, especially after a substantial budget of evaluations, towards favoring HPCs ∗Equal contribution. 1This approach likely originates from the concept of paired statistical tests and the resulting variance reduction, but in our literature search we did not find any references discussing this in the context of HPO. For example, when comparing the performance of two classifiers on one dataset, paired tests are commonly 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2405.15393v2  [stat.ML]  7 Nov 2024which are specifically tailored to the chosen splits. Such and related effects, where we \"overoptimize\" the validation performance without effective reward in improved generalization performance have been sometimes dubbed \"overtuning\" or \"oversearching\". For a more detailed discussion of this topic, including related work, see Section 5 and Appendix B. The practice of reshuffling resampling splits during HPO is generally neither discussed in the scientific literature nor HPO software tools.2 To the best of our knowledge, only Lévesque (2018) investigated reshuffling train-validation splits for every new HPC. For both holdout and M-fold CV using reshuffled resampling splits resulted in, on average, slightly lower generalization error when used in combination with Bayesian optimization (BO, Garnett, 2023) or CMA-ES (Hansen & Ostermeier, 2001) as HPO algorithms. Additionally, reshuffling was used by a solution to the NeurIPS 2006 performance prediction challenge to estimate the final generalization performance (Guyon et al., 2006). Recently, in the context of evolutionary optimization, reshuffling was applied after every generation (Larcher & Barbosa, 2022). In this paper, we systematically examine the effect of reshuffling on HPO performance. Our contribu- tions can be summarized as follows: 1. We show theoretically that reshuffling resampling splits during HPO can result in finding a configuration with better overall generalization performance, especially when the loss surface is rather flat and its estimate is noisy (Section 2). 2. We confirm these theoretical insights through controlled simulation studies (Section 3). 3. We demonstrate in realistic HPO benchmark experiments that reshuffling splits can lead to a real-world improvement of HPO (Section 4). Especially in the case of reshuffled holdout, we find that the final generalization performance is often on par with 5-fold CV under a wide range of settings. We discuss results, limitations, and avenues for future research in Section 5. 2 Theoretical Analysis 2.1 Problem Statement and Setup Machine learning (ML) aims to fit a model to data, so that it generalizes well to new observations of the same distribution. Let D = {Zi}n i=1 be the observed dataset consisting of i.i.d. random variables from a distribution P, i.e., in the supervised setting Zi = ( Xi, Yi).3,4 Formally, an inducer g configured by an HPC λ ∈ Λ maps a dataset D to a model from our hypothesis space h = gλ(D) ∈ H. During HPO, we want to find a HPC that minimizes the expected generalization error, i.e., find λ∗ = arg min λ∈Λ µ(λ), where µ(λ) = E[ℓ(Z, gλ(D))], where ℓ(Z, h) is the loss of model h on a fresh observation Z. In practice, there is usually a limited computational budget for each HPO run, so we assume that there is only a finite number of distinct HPCs Λ = {λ1, . . . ,λJ} to be evaluated, which also simplifies the subsequent analysis. Naturally, we cannot optimize the generalization error directly, but only an estimate of it. To do so, a resampling is constructed. For every HPC λj, draw M random sets I1,j, . . . ,IM,j ⊂ {1, . . . , n} of validation indices with nvalid = ⌈αn⌉ instances each. The random index draws are assumed to be independent of the observed data. The data is then split accordingly into pairs Vm,j = {Zi}i∈Im,j , Tm,j = {Zi}i/∈Im,j of disjoint validation and training sets. Define the validation loss on the m-th fold L(Vm,j, gλj (Tm,j)) = 1 nvalid X i∈Im,j ℓ(Zi, gλj (Tm,j)), employed that implicitly assume that differences between the performance of classifiers on a given CV fold are comparable (Dietterich, 1998; Nadeau & Bengio, 1999, 2003; Demšar, 2006). 2In Appendix B, we present an overview of how resampling is addressed in tutorials and examples of standard HPO libraries and software. We conclude that usually fixed splits are used or recommended. 3Throughout, we use bold letters to indicate (fixed and random) vectors. 4We provide a notation table for symbols used in the main paper in Table 2 in the appendix. 2and the M-fold validation loss as bµ(λj) = 1 M MX m=1 L(Vm,j, gλj (Tm,j)). Since µ is unknown, we minimize bλ = arg minλ∈Λ bµ(λ), hoping that µ(bλ) will also be small. Typically, the same splits are used for every HPC, so Im,j = Im for all j = 1, . . . , Jand m = 1, . . . , M. In the following, we investigate how reshuffling train-validation splits (i.e., Im,j ̸= Im,j′ for j ̸= j′) affects the HPO problem. 2.2 How Reshuffling Affects the Loss Surface We first investigate how different validation and reshuffling strategies affect the empirical loss surface bµ. In particular, we derive the limiting distribution of the sequence √n(bµ(λj) − µ(λj))J j=1. This limiting regime will not only reveal the effect of reshuffling on the loss surface, but also give us a tractable setting to study HPO performance. Theorem 2.1. Under regularity conditions stated in Appendix C.1, it holds √n (bµ(λj) − µ(λj))J j=1 → N(0, Σ) in distribution, where Σi,j = τi,j,M K(λi, λj), τ i,j,M = lim n→∞ 1 nM2α2 nX s=1 MX m=1 MX m′=1 Pr(s ∈ Im,i ∩ Im′,j), and K(λi, λj) = lim n→∞ Cov[¯ℓn(Z′, λi), ¯ℓn(Z′, λj)], ¯ℓn(z, λ) = E[ℓ(z, gλ(T ))] − E[ℓ(Z, gλ(T ))], where the expectation is taken over a training set T of size n and two fresh samples Z, Z′ from the same distribution. The regularity conditions are rather mild and discussed further in Appendix C.1. The kernel K reflects the (co-)variability of the losses caused by validation samples. The contribution of training samples only has a higher-order effect. The validation scheme enters the distribution through the quantities τi,j,M . In what follows, we compute explicit expressions for some popular examples. The following list provides formal definitions for the index sets Im,j. (i) (holdout) Let M = 1 and I1,j = I1 for all j = 1, . . . , J, and some size-⌈αn⌉ index set I1. (ii) (reshuffled holdout) Let M = 1 and I1,1, . . . ,I1,J be independently drawn from the uniform distribution over all size-⌈αn⌉ subsets from {1, . . . , n}. (iii) (M-fold CV) Let α = 1/M and I1, . . . ,IM be a disjoint partition of {1, . . . , n}, and Im,j = Im for all j = 1, . . . , J. (iv) (reshuffled M-fold CV) Let α = 1/M and (I1,j, . . . ,IM,j ), j= 1, . . . , J, be independently drawn from the uniform distribution over disjoint partitions of {1, . . . , n}. (v) (M-fold holdout) LetIm, m= 1, . . . , M, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n} and set Im,j = Im for all m = 1, . . . , M, j= 1, . . . , J. (vi) (reshuffled M-fold holdout) Let Im,j, m= 1, . . . , M, j= 1, . . . , J, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n}. The value of τi,j,M for each example is computed explicitly in Appendix E. In all these examples, we in fact have τi,j,M = \u001aσ2, i = j τ2σ2, i ̸= j. , (1) for some method-dependent parameters σ, τshown in Table 1. The parameter σ2 captures any increase in variance caused by omitting an observation from the validation sets. The parameter τ quantifies a potential decrease in correlation in the loss surface due to reshuffling. More precisely, 3Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details. Method σ2 τ2 holdout (HO) 1/α 1 reshuffled HO 1/α α M-fold CV 1 1 reshuffled M-fold CV 1 1 M-fold HO (subsampling / Monte Carlo CV) 1 + (1− α)/Mα 1 reshuffled M-fold HO 1 + (1− α)/Mα 1/(1 + (1− α)/Mα) the observed losses bµ(λi), bµ(λj) at distinct HPCs λi ̸= λj become less correlated when τ is small. Generally, an increase in variance leads to worse generalization performance. The effect of a correlation decrease is less obvious and is studied in detail in the following section. We make the following observations about the differences between methods in Table 1: • M-fold CV incurs no increase in variance (σ2 = 1) and — because every HPC uses the same folds — no decrease in correlation. Interestingly, the correlation does not even decrease when reshuffling the folds. In any case, all samples are used exactly once as validation and training instance. At least asymptotically, this leads to the same behavior, and reshuffling should have almost no effect on M-fold CV . • The two (1-fold) holdout methods bear the same 1/α increase in variance. This is caused by only using a fraction α of the data as validation samples. Reshuffled holdout also decreases the correlation parameter τ2. In fact, if HPCs λi ̸= λj are evaluated on largely distinct samples, the validation losses bµ(λi) and bµ(λj) become almost independent. • M-fold holdout also increases the variance, because some samples may still be omitted from validation sets. This increase is much smaller for large M. Accordingly, the correlation is also decreased by less in the reshuffled variant. 2.3 How Reshuffling Affects HPO Performance In practice, we are mainly interested in the performance of a model trained with the optimal HPC bλ. To simplify the analysis, we explore this in the large-sample regime derived in the previous section. Assume bµ(λj) = µ(λj) + ϵ(λj) (2) where ϵ(λ) is a zero-mean Gaussian process with covariance kernel Cov(ϵ(λ), ϵ(λ′)) = \u001aK(λ, λ) if λ = λ′, τ2K(λ, λ′) else. (3) Let Λ ⊆ {λ ∈ Rd : ∥λ∥ ≤1} with |Λ| = J <∞ be the set of hyperparameters. Theorem 2.2 ahead gives a bound on the expected regretE[µ(bλ)−µ(λ∗)]. It depends on several quantities characterizing the difficulty of the HPO problem. The constant κ = sup ∥λ∥,∥λ′∥≤1 |K(λ, λ) − K(λ, λ′)| K(λ, λ)∥λ − λ′∥2 . can be interpreted as a measure of correlation of the process ϵ. In particular, Corr(ϵ(λ), ϵ(λ′)) ≥ 1 − κ∥λ − λ′∥2. The constant is small when ϵ is strongly correlated, and large otherwise. Further, define η as the minimal number such that any η-ball contained in {∥λ∥ ≤1} contains at least one element of Λ. It measures how densely the set of candidate HPCsΛ covers set of all possible HPCs. If Λ is a deterministic uniform grid, we have about η ≈ J−1/d. Similarly, Lemma D.1 in the Appendix shows that η ≲ J−1/2d when randomly sampling HPCs. Finally, the constant m = sup λ∈Λ |µ(λ) − µ(λ∗)| ∥λ − λ∗∥2 , 4−2 −1 0 1 2 3 0.00 0.25 0.50 0.75 1.00 λ Loss Surface T ype T rue Empirical ( τ = 1 ) Empirical ( τ = 0.3 ) (a) High signal-to-noise ratio −2 0 2 0.00 0.25 0.50 0.75 1.00 λ Loss Surface T ype T rue Empirical ( τ = 1 ) Empirical ( τ = 0.3 ) (b) Low signal-to-noise ratio Figure 1: Example of reshuffled empirical loss yielding a worse (left) and better (right) minimizer. measures the local curvature at the minimum of the loss surface µ. Finding an HPC λ close to the theoretical optimum λ∗ is easier when the minimum is more pronounced (large m). On the other hand, the regret µ(λ) − µ(λ∗) is also punishing mistakes more quickly. Defining log(x)+ = max{0, log(x)}, we can now state our main result. Theorem 2.2. Let bµ follow the Gaussian process model(2). Suppose κ <∞, 0 < σ2 ≤ Var[ϵ(λ)] ≤ σ2 < ∞ for all λ ∈ Λ, and m >0. Then E[µ(bλ) − µ(λ∗)] ≤ σ √ d[8 + B(τ) − A(τ)]. where B(τ) = 48 hp 1 − τ2 p log J + τ p 1 + log(3κ)+ i , A (τ) = p 1 − τ2(σ/σ) s log \u0012 σ 2mη2 \u0013 + . The numeric constants result from several simplifications in a worst-case analysis, which lowers their practical relevance. A qualitative analysis of the bound is still insightful. The bound is increasing in σ and d, indicating that the HPO problem is harder when there is a lot of noise or there are many parameters to tune. The terms B(τ) and A(τ) have conceptual interpretations: • The term B(τ) quantifies how likely it is to pick a bad bλ because of bad luck: a λ far away from λ∗ had such a small ϵ(λ) that it outweighs the increase in µ. Such events are more likely when the process ϵ is weakly correlated. Accordingly, B(τ) is decreasing in τ and increasing in κ. • The term A(τ) quantifies how likely it is to pick a good bλ by luck: a λ close to λ∗ had such a small ϵ(λ) that it overshoots all the other fluctuations. Also such events are more likely when the process ϵ is weakly correlated. Accordingly, the term A(τ) is decreasing in τ. The B, as stated, is unbounded, but a closer inspection of the proof shows that it is upper bounded by √log J. This bound is attained only in the unrealistic scenario when the validation losses are essentially uncorrelated across all HPCs. The term A is bounded from below by zero, which is also the worst case because the term enters our regret bound with a negative sign. Both A and B are decreasing in the reshuffling parameter τ. There are two regimes. If σ/2mη2 ≤ e, then A(τ) = 0 and reshuffling cannot lead to an improvement of the bound. The term σ/mη2 can be interpreted as noise-to-signal ratio (relative to the grid density). If the signal is much stronger than the noise, the HPO problem is so easy that reshuffling will not help. This situation is illustrated in Figure 1a. If on the other hand σ/mη2 > e, the terms A(τ) and B(τ) enter the bound with opposing signs. This creates tension: reshuffling between HPCs increases B(τ), which is countered by a decrease in A(τ). So which scenarios favor reshuffling? When the process ϵ is strongly correlated, κ is small and reshuffling (decreasing τ) incurs a high cost in B(τ). This is intuitive: When there is strong 5correlation, the validation loss surface bµ is essentially just a vertical shift of µ. Finding the optimal λ is then almost as easy as if we would know µ, and decorrelating the surface through reshuffling would make it unnecessarily hard. When ϵ is less correlated (κ large) however, reshuffling does not hurt the term B(τ) as much, but we can reap all the benefits of increasing A(τ). Here, the effect of reshuffling can be interpreted as hedging against the catastrophic case where all bµ(λ) close to the optimal λ∗ are simultaneously dominated by a region of bad hyperparameters. This is illustrated in Figure 1b. 3 Simulation Study To test our theoretical understanding of the potential benefits of reshuffling resampling splits during HPO, we conduct a simulation study. This study helps us explore the effects of reshuffling in a controlled setting. 3.1 Design We construct a univariate quadratic loss surface function µ : Λ ⊂ R 7→ R, λ→ m(λ − 0.5)2/2 which we want to minimize. The global minimum is given at µ(0.5) = 0 . Combined with a kernel for the noise process ϵ as in Equation (3), this allows us to simulate an objective as ob- served during HPO by sampling bµ(λ) = µ(λ) + ϵ(λ). We use a squared exponential kernel K(λ, λ′) = σ2 K exp (−κ(λ − λ′)2/2) that is plugged into the covariance kernel of the noise process ϵ in Equation (3). The parameters m and κ in our simulation setup correspond exactly to the curva- ture and correlation constants from the previous sections. Recall that Theorem 2.2 states that the effect of reshuffling strongly depends on the curvature m of the loss surface µ (a larger m implies a stronger curvature) and the constant κ as a measure of correlation of the noise ϵ (a larger κ implies weaker correlation). Combined with the possibility to vary τ in the covariance kernel of ϵ, we can systematically investigate how curvature of the loss surface, correlation of the noise and the extent of reshuffling affect optimization performance. In each simulation run, we simulate the observed objective ˆµ(λ), identify the minimizer ˆλ = arg minλ∈Λ ˆµ(λ), and calculate its true risk, µ(ˆλ). We repeat this process 10000 times for various combinations of τ, m, and κ. 3.2 Results Figure 2 visualizes the true risk of the configuration ˆλ that minimizes the observed objective. We observe that for a loss surface with low curvature (i.e., m ≤ 2), reshuffling is beneficial (lower values of τ resulting in a better true risk of the configuration that optimizes the observed objective) as long as the noise process is not too correlated (i.e., κ ≥ 1). As soon as the noise process is more strongly correlated, even flat valleys of the true risk µ remain clearly visible in the observed risk bµ, and reshuffling starts to hurt the optimization performance. Moving to scenarios of high curvature, the general relationship of m and κ remains the same, but reshuffling starts to hurt optimization performance already with weaker correlation in the noise. In summary, the simulations show that in cases of low curvature of the loss surface, reshuffling (reducing τ) tends to improve the true risk of the optimized configuration, especially when the loss surface is flat (small m) and the noise is not strongly correlated (i.e., κ is large). This exactly confirms our theoretical predictions from the previous section. 4 Benchmark Experiments In this section, we present benchmark experiments of real-world HPO problems where we investigate the effect of reshuffling resampling splits during HPO. First, we discuss the experimental setup. Second, we present results for HPO using random search (Bergstra & Bengio, 2012). Third, we also show the effect of reshuffling when applied in BO using HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). Recall that our theoretical insight suggests that 1) reshuffling might be beneficial during HPO and 2) holdout should be affected the most by reshuffling and other resamplings should only be affected to a lesser extent. 6m: 20 κ: 0.04 m: 20 κ: 1 m: 20 κ: 4 m: 20 κ: 100 m: 10 κ: 0.04 m: 10 κ: 1 m: 10 κ: 4 m: 10 κ: 100 m: 2 κ: 0.04 m: 2 κ: 1 m: 2 κ: 4 m: 2 κ: 100 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.070 0.075 0.080 0.085 0.090 0.175 0.200 0.225 0.250 0.275 0.21 0.24 0.27 0.30 0.08 0.10 0.12 0.18 0.19 0.20 0.100 0.125 0.150 0.175 0.200 0.07 0.08 0.09 0.10 0.11 0.12 0.08 0.12 0.16 0.05 0.10 0.15 0.20 0.02 0.04 0.06 0.00 0.05 0.10 0.15 0.00 0.05 0.10 0.15 0.20 τ μ(λ^) Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength κ of the noise (a larger κ implying weaker correlation), and extent of reshuffling τ (lower τ increasing reshuffling). A τ of 1 indicates no reshuffling. Error bars represent standard errors. 4.1 Experimental Setup As benchmark tasks, we use a set of standard HPO problems defined on small- to medium-sized tabular datasets for binary classification. We suspect the effect of the resampling variant used and whether the resampling is reshuffled to be larger for smaller datasets, where the variance of the validation loss estimator is naturally higher. Furthermore, from a practical perspective, this also ensures computational feasibility given the large number of HPO runs in our experiments. We systematically vary the learning algorithm, optimized performance metric, resampling method, whether the resampling is reshuffled, and the size of the dataset used for training and validation during HPO. Below, we outline the general experimental design and refer to Appendix F for details. We used a subset of the datasets defined by the AutoML benchmark (Gijsbers et al., 2024), treating these as data generating processes (DGPs; Hothorn et al., 2005). We only considered datasets with less than 100 features to reduce the required computation time and required the number of observations to be between 10000 and 1000000; for further details see Appendix F.1. Our aim was to robustly measure the generalization performance when varying the size n, which, as defined in Section 2 denotes the size of the combined data for model selection, so one training and validation set combined. First, we sampled 5000 data points per dataset for robust assessment of the generalization error; these points are not used during HPO in any way. Then, from the remaining points we sampled tasks with n ∈ {500, 1000, 5000}. We selected CatBoost (Prokhorenkova et al., 2018) and XGBoost (Chen & Guestrin, 2016) for their state-of-the-art performance on tabular data (Grinsztajn et al., 2022; Borisov et al., 2022; McElfresh et al., 2023; Kohli et al., 2024). Additionally, we included an Elastic Net (Zou & Hastie, 2005) to represent a linear baseline with a smaller search space and a funnel-shaped MLP (Zimmer et al., 2021) as a cost-effective neural network baseline. We provide details regarding training pipelines and search spaces in Appendix F.2. We conduct a random search with500 HPC evaluations for every resampling strategy we described in Table 1, for both fixed and reshuffled splits. We always use 80/20 train-validation splits for holdout 7500 1000 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.730 −0.725 −0.720 −0.715 −0.70 −0.69 −0.68 −0.67 −0.66 −0.68 −0.67 −0.66 −0.65 −0.64 −0.63 No. HPC Evaluations Mean T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. and 5-fold CVs, so that training set size (and negative estimation bias) are the same. Anytime test performance of an HPO run is assessed by re-training the current incumbent (i.e. the best HPC until the current HPO iteration based on validation performance) on all available train and validation data and evaluating its performance on the outer test set. Note we do this for scientific evaluation in this experiment; obviously, this is not possible in practice. Using random search allows us to record various metrics and afterwards simulate optimizing for different ones, specifically, we recorded accuracy, area under the ROC curve (ROC AUC) and logloss. We also investigated the effect of reshuffling on two state-of-the-art BO variants (Eggensperger et al., 2021; Turner et al., 2021), namely HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). The experimental design was the same as for random search, except for the budget, which we reduced from 500 HPCs to 250 HPCs, and only optimized ROC AUC. 4.2 Experimental Results In the following, we focus on the results obtained using ROC AUC. We present aggregated results over different tasks, learning algorithms and replications to get a general understanding of the effects. Unaggregated results and results involving accuracy and logloss can be found in Appendix G. Results of Reshuffling Different Resamplings For each resampling (holdout, 5-fold holdout, 5-fold CV , and 5x 5-fold CV), we empirically analyze the effect of reshuffling train and validation splits during HPO. In Figure 3 we exemplarily show how test performance develops over the course of an HPO run on a single task for different resamplings (with and without reshuffling). Naturally, test performance does not necessarily increase in a monotonic fashion, and especially holdout without reshuffling tends to be unstable. Its reshuffled version results in substantially better test performance. Next, we look at the relative improvement (compared to standard 5-fold CV , which we consider our baseline) with respect to test ROC AUC performance of the incumbent over time in Figure 4, i.e., the difference in test performance of the incumbent between standard 5-fold CV and a different resampling protocol; hence a positive difference tells us how much better in test error we are, if we would have chosen the other protocol instead 5-fold CV . We observe that reshuffling generally results in equal or better performance compared to the same resampling protocol without reshuffling. For 5-fold holdout and especially 5-fold CV and 5x 5-fold CV , reshuffling has a smaller effect on relative test performance improvement, as expected. Holdout is affected the most by reshuffling and results in substantially better relative test performance compared to standard holdout. We also observe that an HPO protocol based on reshuffled holdout results in similar final test performance as standard 5-fold CV while overall being substantially cheaper due to requiring less model fits per HPC evaluation. In Appendix G.2, we further provide an ablation study on the number of folds when using M-fold holdout, where we observed that – in line with our theory – the more folds are used, the less reshuffling affects M-fold holdout. 8500 1000 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.50 −0.25 0.00 0.25 −1.2 −0.8 −0.4 0.0 0.4 −1.0 −0.5 0.0 0.5 No. HPC Evaluations Mean T est Improvement Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 4: Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. However, this general trend can vary for certain combinations of classifier and performance metric, see Appendix G. Especially for logloss, we observed that reshuffling rarely is beneficial; see the discussion in Section 5. Finally, the different resamplings generally behave as expected. The more we are willing to invest compute resources into a more intensive resampling like 5-fold CV or 5x 5-fold CV , the better the generalization performance of the final incumbent. Results for BO and Reshuffling Figure 5 shows that, generally HEBO and SMAC3 outperform random search with respect to generalization performance (i.e., comparing HEBO and SMAC3 to random search under standard holdout, or comparing under reshuffled holdout). More interestingly, HEBO, SMAC3 and random search all strongly benefit from reshuffling. Moreover, the performance gap between HEBO and random search but also SMAC3 and random search narrows when the resampling is reshuffled, which is an interesting finding of its own: As soon as we are concerned with generalization performance of HPO and not only investigate validation performance during optimization, the choice of optimizer might have less impact on final generalization performance compared to other choices such as whether the resampling is reshuffled during HPO or not. We present results for BO and reshuffling for different resamplings in Appendix G. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 5: Average improvement (compared to random search on standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learning algorithms and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 5 Discussion In the previous sections, we have shown theoretically and empirically that reshuffling can enhance generalization performance of HPO. The main purpose of this article is to draw attention to this 9surprising fact about a technique that is simple but rarely discussed. Our work goes beyond a preliminary experimental study on reshuffling (Lévesque, 2018), in that we also study the effect of reshuffling on random search, multiple metrics and learning algorithms, and most importantly, for the first time, we provide a theoretical analysis that explains why reshuffling can be beneficial. Limitations To unveil the mechanisms underlying the reshuffling procedures, our theoretical analysis relies on an asymptotic approximation of the empirical loss surface. This allows us to operate on Gaussian loss surfaces, which exhibit convenient concentration and anti-concentration properties required in our proof. The latter are lacking for general distributions, which explains our asymptotic approach. The analysis was further facilitated by a loss stability assumption regarding the learning algorithms that is generally rather mild; see the discussion in Bayle et al. (2020). However, it typically fails for highly sensitive losses, which has practical consequences. In fact, Figure 9 in Appendix G shows that reshuffling usually hurts generalization for the logloss and small sample sizes. It is still an open question whether this problem can be fixed by less naive implementations of the technique. Another limitation is our focus on generalization after search through a fixed, finite set of candidates. This largely ignores the dynamic nature of many HPO algorithms, which would greatly complicate our analysis. Finally, our experiments are limited in that we restricted ourselves to tabular data and binary classification and we avoided extremely small or large datasets. Relation to Overfitting The fact that generalization performance can decrease during HPO (or computational model selection in general) is sometimes known as oversearching, overtuning, or overfitting to the validation set (Quinlan & Cameron-Jones, 1995; Escalante et al., 2009; Koch et al., 2010; Igel, 2012; Bischl et al., 2023), but has arguably not been studied very thoroughly. Given recent theoretical (Feldman et al., 2019) and empirical (Purucker & Beel, 2023) findings, we expect less overtuning on multi-class datasets, making it interesting to see how reshuffling would affect the generalization performance. Several works suggest strategies to counteract this effect. First, LOOCVCV proposes a conservative choice of incumbents (Ng, 1997) at the cost of leave-one-out analysis or an additional hyperparameter. Second, it is possible to use an extra selection set (Igel, 2012; Lévesque, 2018; Mohr et al., 2018) at the cost of reduced training data, which was found to lead to reduced overall performance (Lévesque, 2018). Third, by using early stopping one can stop hyperparameter optimization before the generaliza- tion performance degrades again. This was so far demonstrated to be able to save compute budget at only marginally reduced performance, but also requires either a sensitivity hyperparameter or correct estimation of the variance of the generalization estimate and was only developed for cross-validation so far (Makarova et al., 2022). Reshuffling itself is orthogonal to these proposals and a combination with the above-mentioned methods might result in further improvements. Outlook Generally, the related literature detects overfitting to the validation set either visually (Ng, 1997) or by measuring it (Koch et al., 2010; Igel, 2012; Fabris & Freitas, 2019). Developing a unified formal definition of the above-mentioned terms and thoroughly analyzing the effect of decreased generalization performance after many HPO iterations and how it relates to our measurements of the validation performance is an important direction for future work. We further found, both theoretically and experimentally, that investing more resources when evaluating each HPC can result in better final HPO performance. To reduce the computational burden on HPO again, we suggest further investigating the use of adaptive CV techniques, as proposed by Auto- WEKA (Thornton et al., 2013) or under the name Lazy Paired Hyperparameter Tuning (Zheng & Bilenko, 2013). Designing more advanced HPO algorithms exploiting the reshuffling effect should be a promising avenue for further research. Acknowledgments and Disclosure of Funding We thank Martin Binder and Florian Karl for helpful discussions. Lennart Schneider is supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy through the Center for Analytics - Data - Applications (ADACenter) within the framework of BAYERN DIGITAL II (20-3410-2-9-8). Lennart Schneider acknowledges funding from the LMU Mentoring Program of the Faculty of Mathematics, Informatics and Statistics. 10References Arlot, S. and Celisse, A. A survey of cross-validation procedures for model selection. Statistics Surveys, 4:40 – 79, 2010. B Austern, M. and Zhou, W. Asymptotics of cross-validation. arXiv:2001.11111 [math.ST], 2020. C.1 Awad, N., Mallik, N., and Hutter, F. DEHB: Evolutionary hyberband for scalable, robust and efficient Hyperparameter Optimization. In Zhou, Z. (ed.), Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI’21), pp. 2147–2153, 2021. B Bayle, P., Bayle, A., Janson, L., and Mackey, L. Cross-validation confidence intervals for test error. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.-F., and Lin, H. (eds.),Proceedings of the 33rd International Conference on Advances in Neural Information Processing Systems (NeurIPS’20), pp. 16339–16350. Curran Associates, 2020. 5, C.1, C.1, C.1 Bergman, E., Purucker, L., and Hutter, F. Don’t waste your time: Early stopping cross-validation. In Eggensperger, K., Garnett, R., Vanschoren, J., Lindauer, M., and Gardner, J. (eds.),Proceedings of the Third International Conference on Automated Machine Learning, volume 256 of Proceedings of Machine Learning Research, pp. 9/1–31. PMLR, 2024. B Bergstra, J. and Bengio, Y . Random search for hyper-parameter optimization.Journal of Machine Learning Research, 13:281–305, 2012. 4, B Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A., Deng, D., and Lindauer, M. Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, pp. e1484, 2023. 1, 5, B Blum, A., Kalai, A., and Langford, J. Beating the hold-out: Bounds for k-fold and progressive cross-validation. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT ’99, pp. 203–208, 1999. B Borisov, V ., Leemann, T., Seßler, K., Haug, J., Pawelczyk, M., and Kasneci, G. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, pp. 1–21, 2022. 4.1 Bouckaert, Remcoand Frank, E. Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms. In Dai, H., Srikant, R., and Zhang, C. (eds.), Advances in Knowledge Discovery and Data Mining, pp. 3–12. Springer, 2004. B Bousquet, O. and Zhivotovskiy, N. Fast classification rates without standard margin assumptions. Information and Inference: A Journal of the IMA, 10(4):1389–1421, 2021. C.1 Bouthillier, X., Delaunay, P., Bronzi, M., Trofimov, A., Nichyporuk, B., Szeto, J., Sepahvand, N. M., Raff, E., Madan, K., V oleti, V ., Kahou, S. E., Michalski, V ., Arbel, T., Pal, C., Varoquaux, G., and Vincent, P. Accounting for variance in machine learning benchmarks. In Smola, A., Dimakis, A., and Stoica, I. (eds.), Proceedings of Machine Learning and Systems 3, volume 3, pp. 747–769, 2021. B Buczak, P., Groll, A., Pauly, M., Rehof, J., and Horn, D. Using sequential statistical tests for efficient hyperparameter tuning. AStA Advances in Statistical Analysis, 108(2):441–460, 2024. B Cawley, G. and Talbot, N. On Overfitting in Model Selection and Subsequent Selection Bias in Performance Evaluation. Journal of Machine Learning Research, 11:2079–2107, 2010. B Chen, T. and Guestrin, C. XGBoost: A scalable tree boosting system. In Krishnapuram, B., Shah, M., Smola, A., Aggarwal, C., Shen, D., and Rastogi, R. (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’16) , pp. 785–794. ACM Press, 2016. 4.1 Cowen-Rivers, A., Lyu, W., Tutunov, R., Wang, Z., Grosnit, A., Griffiths, R., Maraval, A., Jianye, H., Wang, J., Peters, J., and Ammar, H. HEBO: Pushing the limits of sample-efficient hyper-parameter optimisation. Journal of Artificial Intelligence Research, 74:1269–1349, 2022. 4, 4.1, B 11Demšar, J. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30, 2006. 1 Dietterich, T. G. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895–1923, 1998. 1 Dunias, Z., Van Calster, B., Timmerman, D., Boulesteix, A.-L., and van Smeden, M. A comparison of hyperparameter tuning procedures for clinical prediction models: A simulation study. Statistics in Medicine, 43(6):1119–1134, 2024. B Eggensperger, K., Lindauer, M., Hoos, H., Hutter, F., and Leyton-Brown, K. Efficient benchmarking of algorithm configurators via model-based surrogates. Machine Learning, 107(1):15–41, 2018. 5 Eggensperger, K., Lindauer, M., and Hutter, F. Pitfalls and best practices in algorithm configuration. Journal of Artificial Intelligence Research, pp. 861–893, 2019. B Eggensperger, K., Müller, P., Mallik, N., Feurer, M., Sass, R., Klein, A., Awad, N., Lindauer, M., and Hutter, F. HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO. In Vanschoren & Yeung (2021). 4.1, B Escalante, H., Montes, M., and Sucar, E. Particle Swarm Model Selection. Journal of Machine Learning Research, 10:405–440, 2009. 5 Fabris, F. and Freitas, A. Analysing the overfit of the auto-sklearn automated machine learning tool. In Nicosia, G., Pardalos, P., Umeton, R., Giuffrida, G., and Sciacca, V . (eds.), Machine Learning, Optimization, and Data Science, volume 11943 of Lecture Notes in Computer Science, pp. 508–520, 2019. 5 Falkner, S., Klein, A., and Hutter, F. BOHB: Robust and efficient Hyperparameter Optimization at scale. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning (ICML’18), volume 80, pp. 1437–1446. Proceedings of Machine Learning Research, 2018. B Feldman, V ., Frostig, R., and Hardt, M. The advantages of multiple classes for reducing overfitting from test set reuse. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th Interna- tional Conference on Machine Learning (ICML’19), volume 97, pp. 1892–1900. Proceedings of Machine Learning Research, 2019. 5 Feurer, M. and Hutter, F. Hyperparameter Optimization. In Hutter et al. (2019), chapter 1, pp. 3 – 38. Available for free at http://automl.org/book. 1, B Feurer, M., Eggensperger, K., Falkner, S., Lindauer, M., and Hutter, F. Auto-Sklearn 2.0: Hands-free automl via meta-learning. Journal of Machine Learning Research, 23(261):1–61, 2022. B Garnett, R. Bayesian Optimization. Cambridge University Press, 2023. 1, B Gijsbers, P., Bueno, M., Coors, S., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. AMLB: an automl benchmark. Journal of Machine Learning Research, 25(101):1–65, 2024. 4.1 Giné, E. and Nickl, R. Mathematical Foundations of Infinite-Dimensional Statistical Models, vol- ume 40. Cambridge University Press, 2016. C.2 Grinsztajn, L., Oyallon, E., and Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, pp. 507–520, 2022. 4.1 Guyon, I., Alamdari, A., Dror, G., and Buhmann, J. Performance prediction challenge. In The 2006 IEEE International Joint Conference on Neural Network Proceedings, 2006. 1 Guyon, I., Saffari, A., Dror, G., and Cawley, G. Model selection: Beyond the Bayesian/Frequentist divide. Journal of Machine Learning Research, 11:61–87, 2010. B 12Guyon, I., Bennett, K., Cawley, G., Escalante, H. J., Escalera, S., Ho, T. K., Macià, N., Ray, B., Saeed, M., Statnikov, A., and Viegas, E. Design of the 2015 ChaLearn AutoML challenge. In 2015 International Joint Conference on Neural Networks (IJCNN’15), pp. 1–8. International Neural Network Society and IEEE Computational Intelligence Society, IEEE, 2015. B Guyon, I., Sun-Hosoya, L., Boullé, M., Escalante, H., Escalera, S., Liu, Z., Jajetic, D., Ray, B., Saeed, M., Sebag, M., Statnikov, A., Tu, W., and Viegas, E. Analysis of the AutoML Challenge Series 2015-2018. In Hutter et al. (2019), chapter 10, pp. 177–219. Available for free at http: //automl.org/book. B Guyon, I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (eds.). Proceedings of the First International Conference on Automated Machine Learning, 2022. Proceedings of Machine Learning Research. 5 Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evolutionary C., 9(2):159–195, 2001. 1 Hothorn, T., Leisch, F., Zeileis, A., and Hornik, K. The design and analysis of benchmark experiments. Journal of Computational and Graphical Statistics, 14(3):675–699, 2005. 4.1, F.1 Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.).Automated Machine Learning: Methods, Systems, Challenges. Springer, 2019. Available for free at http://automl.org/book. 5 Igel, C. A note on generalization loss when evolving adaptive pattern recognition systems. IEEE Transactions on Evolutionary Computation, 17(3):345–352, 2012. 5, 5 Jamieson, K. and Talwalkar, A. Non-stochastic best arm identification and Hyperparameter Op- timization. In Gretton, A. and Robert, C. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS’16) , volume 51. Proceedings of Machine Learning Research, 2016. B Kadra, A., Janowski, M., Wistuba, M., and Grabocka, J. Scaling laws for hyperparameter optimization. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 47527–47553, 2023. B Kallenberg, O. Foundations of modern probability, volume 2. Springer, 1997. D Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Singh, A. and Zhu, J. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS’17), volume 54. Proceedings of Machine Learning Research, 2017. B Koch, P., Konen, W., Flasch, O., and Bartz-Beielstein, T. Optimizing support vector machines for stormwater prediction. Technical Report TR10-2-007, Technische Universität Dortmund, 2010. Proceedings of Workshop on Experimental Methods for the Assessment of Computational Systems joint to PPSN2010. 5, 5 Kohli, R., Feurer, M., Bischl, B., Eggensperger, K., and Hutter, F. Towards quantifying the effect of datasets for benchmarking: A look at tabular machine learning. In Data-centric Machine Learning (DMLR) workshop at the International Conference on Learning Representations (ICLR), 2024. 4.1 Lang, M., Kotthaus, H., Marwedel, P., Weihs, C., Rahnenführer, J., and Bischl, B. Automatic model selection for high-dimensional survival analysis. Journal of Statistical Computation and Simulation, 85:62–76, 2015. B Larcher, C. and Barbosa, H. Evaluating models with dynamic sampling holdout in auto-ml. SN Computer Science, 3(506), 2022. 1 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to Hyperparameter Optimization. Journal of Machine Learning Research, 18(185):1–52, 2018. B Lindauer, M., Eggensperger, K., Feurer, M., Biedenkapp, A., Deng, D., Benjamins, C., Ruhkopf, T., Sass, R., and Hutter, F. SMAC3: A versatile bayesian optimization package for Hyperparameter Optimization. Journal of Machine Learning Research, 23(54):1–9, 2022. 4, 4.1, B 13Loshchilov, I. and Hutter, F. CMA-ES for Hyperparameter Optimization of deep neural networks. In International Conference on Learning Representations Workshop track, 2016. Published online: iclr.cc. B Lévesque, J. Bayesian Hyperparameter Optimization: Overfitting, Ensembles and Conditional Spaces. PhD thesis, Université Laval, 2018. 1, 5, 5 Makarova, A., Shen, H., Perrone, V ., Klein, A., Faddoul, J., Krause, A., Seeger, M., and Archambeau, C. Automatic termination for hyperparameter optimization. In Guyon et al. (2022). 5 Mallik, N., Bergman, E., Hvarfner, C., Stoll, D., Janowski, M., Lindauer, M., Nardi, L., and Hutter, F. PriorBand: Practical hyperparameter optimization in the age of deep learning. In Oh et al. (2023). B McElfresh, D., Khandagale, S., Valverde, J., Prasad C., V ., Ramakrishnan, G., Goldblum, M., and White, C. When do neural nets outperform boosted trees on tabular data? In Oh et al. (2023), pp. 76336–76369. 4.1, F.2 Mohr, F., Wever, M., and Hüllermeier, E. ML-Plan: Automated machine learning via hierarchical planning. Machine Learning, 107(8-10):1495–1515, 2018. 5, B Molinaro, A., Simon, R., and Pfeiffer, R. Prediction error estimation: A comparison of resampling methods. Bioinformatics, 21(15):3301–3307, 2005. B Nadeau, C. and Bengio, Y . Inference for the generalization error. In Solla, S., Leen, T., and Müller, K. (eds.), Proceedings of the 13th International Conference on Advances in Neural Information Processing Systems (NeurIPS’99). The MIT Press, 1999. 1 Nadeau, C. and Bengio, Y . Inference for the generalization error.Machine Learning, 52:239–281, 2003. 1 Ng, A. Preventing “overfitting”’ of cross-validation data. In Fisher, D. H. (ed.), Proceedings of the Fourteenth International Conference on Machine Learning (ICML’97), pp. 245–253. Morgan Kaufmann Publishers, 1997. 5, 5 Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.). Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS’23), 2023. Curran Associates. 5 Pfisterer, F., Schneider, L., Moosbauer, J., Binder, M., and Bischl, B. YAHPO Gym – an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization. In Guyon et al. (2022). B, 5 Pineda Arango, S., Jomaa, H., Wistuba, M., and Grabocka, J. HPO-B: A large-scale reproducible benchmark for black-box HPO based on OpenML. In Vanschoren & Yeung (2021). B, 5 Probst, P., Boulesteix, A., and Bischl, B. Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53):1–32, 2019. 1 Prokhorenkova, L., Gusev, G., V orobev, A., Dorogush, A., and Gulin, A. Catboost: Unbiased boosting with categorical features. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS’18), pp. 6639–6649. Curran Associates, 2018. 4.1 Purucker, L. and Beel, J. CMA-ES for post hoc ensembling in automl: A great success and salvageable failure. In Faust, A., Garnett, R., White, C., Hutter, F., and Gardner, J. R. (eds.),Proceedings of the Second International Conference on Automated Machine Learning, volume 224 of Proceedings of Machine Learning Research, pp. 1/1–23. PMLR, 2023. 5 Quinlan, J. and Cameron-Jones, R. Oversearching and layered search in empirical learning. In Proceedings of the 14th International Joint Conference on Artificial Intelligence , volume 2 of IJCAI’95, pp. 1019–1024, 1995. 5 14Rao, R., Fung, G., and Rosales, R. On the dangers of cross-validation. an experimental evaluation. In Proceedings of the 2008 SIAM International Conference on Data Mining (SDM), pp. 588–596, 2008. B Salinas, D., Seeger, M., Klein, A., Perrone, V ., Wistuba, M., and Archambeau, C. Syne Tune: A library for large scale hyperparameter tuning and reproducible research. In Guyon et al. (2022), pp. 16–1. B Schaffer, C. Selecting a classification method by cross-validation. Machine Learning Journal, 13: 135–143, 1993. B Swersky, K., Snoek, J., and Adams, R. Freeze-thaw Bayesian optimization. arXiv:1406.3896 [stats.ML], 2014. B Talagrand, M. The generic chaining: upper and lower bounds of stochastic processes . Springer Science & Business Media, 2005. C.2 Thornton, C., Hutter, F., Hoos, H., and Leyton-Brown, K. Auto-WEKA: Combined selection and Hyperparameter Optimization of classification algorithms. In Dhillon, I., Koren, Y ., Ghani, R., Senator, T., Bradley, P., Parekh, R., He, J., Grossman, R., and Uthurusamy, R. (eds.), The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’13), pp. 847–855. ACM Press, 2013. 5, B Turner, R., Eriksson, D., McCourt, M., Kiili, J., Laaksonen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the Black-Box Optimization Challenge 2020. In Escalante, H. and Hofmann, K. (eds.),Proceedings of the Neural Information Processing Systems Track Competition and Demonstration, pp. 3–26. Curran Associates, 2021. 4.1 van der Vaart, A. Asymptotic statistics, volume 3. Cambridge university press, 2000. C.1 van Erven, T., Grünwald, P., Mehta, N., Reid, M., and Williamson, R. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16(54):1793–1861, 2015. C.1 van Rijn, J. and Hutter, F. Hyperparameter importance across datasets. In Guo, Y . and Farooq, F. (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’18), pp. 2367–2376. ACM Press, 2018. 1 Vanschoren, J. and Yeung, S. (eds.).Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2021. Curran Associates. 5 Vanschoren, J., van Rijn, J., Bischl, B., and Torgo, L. OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2):49–60, 2014. 4 Wainer, J. and Cawley, G. Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters. Journal of Machine Learning Research, 18:1–35, 2017. B Wainwright, M. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019. C.2 Wistuba, M., Schilling, N., and Schmidt-Thieme, L. Scalable Gaussian process-based transfer surrogates for Hyperparameter Optimization. Machine Learning, 107(1):43–78, 2018. G.1 Wu, J., Toscano-Palmerin, S., Frazier, P., and Wilson, A. Practical multi-fidelity Bayesian optimization for hyperparameter tuning. In Peters, J. and Sontag, D. (eds.), Proceedings of The 36th Uncertainty in Artificial Intelligence Conference (UAI’20), pp. 788–798. PMLR, 2020. B Zheng, A. and Bilenko, M. Lazy paired hyper-parameter tuning. In Rossi, F. (ed.), Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI’13), pp. 1924–1931, 2013. 5, B Zimmer, L., Lindauer, M., and Hutter, F. Auto-Pytorch: Multi-fidelity metalearning for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43:3079–3090, 2021. 4.1, F.2 Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301–320, 2005. 4.1 15A Notation Table 2: Notation table. We discuss all symbols used in the main paper. Xi Random vector, describing the features Yi Random variable, describing the target Zi = (Xi, Yi) Data point D = {Zi}n i=1 Dataset consisting of iid random variables n Number of observations g Inducer/ML algorithm h Model, created by the inducer via h = gλ(D) λ Hyperparameter configuration Λ Finite set of all hyperparameter configurations J |Λ|, i.e., the number of hyperparameter configurations gλj Hyperparameterized inducer µ(λ) Expected loss of a hyperparameterized inducer on the distribution of a dataset ℓ(Z, h) Loss of a model h on a fresh observation Z M Number of folds in M-fold cross-validation α Percentage of samples to be used for validation I1,j, . . . ,IM,j ⊂ {1, . . . , n} M sets of validation indices, to be used for evaluating λj Vm,j Validation data for fold m and configuration λj Tm,j Training data for fold m and configuration λj L(Vm,j, gλj (Tm,j)) Validation loss for fold m and configuration λj bµ(λj) M-fold validation loss σ2 Increase in variance of validation loss caused by resampling τ2 Decrease in correlation among validation losses caused by reshuffling τi,j,M Resampling-related component of validation loss covariance K(·, ·) Kernel capturing the covariance of the pointwise losses between two HPCs ϵ(λj) Zero-mean Gaussian process, see Equation (2) d Number of hyperparameters κ Curvature constant of covariance kernel η Density of hyperparameter set Λ m Local curvature at the minimum of the loss surface µ σ Lower bound on the noise level B(τ) Part of the regret bound penalizing reshuffling A(τ) Part of the regret bound rewarding reshuffling B Extended Related Work Due to the black box nature of the HPO problem (Feurer & Hutter, 2019; Bischl et al., 2023), gradient free, zeroth-order optimization algorithms such as BO (Garnett, 2023), Evolutionary Strate- gies (Loshchilov & Hutter, 2016) or a simple random search (Bergstra & Bengio, 2012) have become standard optimization algorithms to tackle vanilla HPO problems. In the last decade, most research on HPO has been concerned with constructing new algorithms that excel at finding configurations with a low estimated generalization error. Examples include BO variants such as as HEBO (Cowen-Rivers et al., 2022) or SMAC3 (Lindauer et al., 2022). Another direction of HPO research has been concerned with speeding up the HPO process to allow more efficient spending of compute resources. Multifidelity HPO, for example, turns the black box optimization problem into a gray box one by making use of lower fidelity approximations to the target function, i.e., using fewer numbers of epochs or subsets of the data for cheap low-fidelity evaluations that approximate the costly high-fidelity evaluation. Examples include bandit-based budget allocation algorithms such as Successive Halving (Jamieson & Talwalkar, 2016), Hyperband (Li et al., 2018) and their extensions that use non-random search mechanisms (Falkner et al., 2018; Awad et al., 2021; Mallik et al., 2023) or algorithms making use of multi-fidelity information in the context of BO (Swersky et al., 2014; Klein et al., 2017; Wu et al., 2020; Kadra et al., 2023). Several works address the problem of speeding up cross-validation techniques and use techniques that could be described as grey box optimization techniques. Besides the ones mentioned in the main paper (Thornton et al., 2013; Zheng & Bilenko, 2013), it is possible to employ racing techniques for model selection in machine learning as demonstrated by Lang et al. (2015), and there has been a recent interest in methods that adapt the cost of running full cross-validation procedures (Bergman et al., 2024; Buczak et al., 2024). When addressing the problem of HPO, we must acknowledge an inherent mismatch between the explicit objective we optimize – namely, the estimated generalization performance of a model – and the actual implicit optimization goal, which is to identify a configuration that yields the best 16generalization performance on new, unseen data. Typically, evaluations and comparisons of different HPO algorithms focus exclusively on the final best validation performance (i.e., the objective that is directly optimized), even though an unbiased estimate of performance on an external unseen test set might be available. While this approach is logical for assessing the efficacy of an optimization algorithm based on the metric it seeks to improve, relying solely on finding an optimal validation configuration is beneficial only if there is reason to assume a strong correlation between the optimized validation performance and true generalization ability on new, unseen test data. This discrepancy can be found deeply within the HPO community, where the evaluation of HPO algorithms on standard benchmark libraries is usually done solely with respect to the validation performance (Eggensperger et al., 2021; Pineda Arango et al., 2021; Salinas et al., 2022; Pfisterer et al., 2022).5 This relationship between validation performance (i.e., the estimated generalization error derived from resampling) and true generalization performance (e.g., assessed through an outer holdout test set or additional resampling) of an optimal validation configuration found during HPO remains a largely unexplored area of research. In general, little research has focused on the selection of resampling types, let alone the automated selection of resampling types (Guyon et al., 2010; Feurer et al., 2022). While we usually expect that a more intensive resampling will reduce the variance of the estimated generalization error and thereby improve the (rank) correlation between optimized validation and unbiased outer test performance within HPCs, this benefit is naturally offset by a higher computational expense. Overall, there is little research on which resampling method to use in practice for model selection, and we only know of a study for support vector machines (Wainer & Cawley, 2017), a simulation study for clinical prediction models (Dunias et al., 2024), a study on feature selection (Molinaro et al., 2005) and a study on fast CV (Bergman et al., 2024). In addition, ML-Plan (Mohr et al., 2018) proposed a two-stage procedure. In a first stage (search), the tool uses planning on hierarchical task networks to find promising machine learning pipelines on 70% of the training data. In a second step (selection), it uses 100% of the training data and retrains the most promising candidates from the search step. Finally, it uses a combination of the internal generalization error estimation that was used during search and the 0.75 percentile of the generalization error estimation from the selection step to make a more unbiased selection of the final model. The paper found that this improves performance over using only regular cross-validation for search and selection. The general consensus, that is in agreement with our findings, is that CV or repeated CV generally leads to better generalization performance. In addition, while there are theoretical works that compare the accuracy of estimating the generalization error of holdout and CV (Blum et al., 1999), our goals is to correctly identify a single solution, which generalizes well, see the excellent survey by Arlot & Celisse (2010) for a discussion on this topic. Bouthillier et al. (2021) studied the sources of variance in machine learning experiments, and find that the split into training and test data has the largest impact. Consequently, they suggest to reshuffle the data prior to splitting it into the training, which is then used for HPO, and the test set. We followed their suggestion when designing our experiments and draw a new test sample for every replication, see Section 4.1 and Appendix F. This dependence on the exact split was further already discussed in the context of how much the outcome of a statistical test on results of machine learning experiments depended on the exact train-test split (Bouckaert, 2004). Finally, the first warning against comparing too many hypothesis using cross-validation was raised by Schaffer (1993), and in addition to the works discussed in Section 5 in the main paper, also picked up by Rao et al. (2008); Cawley & Talbot (2010). Moreover, the problem of finding a correct \"upper objective\" in a bilevel optimization problem has been noted (Guyon et al., 2010, 2015, 2019). Also, in the related field of algorithm configuration the problem has been identified (Eggensperger et al., 2019). B.1 Current Treatment of Resamplings in HPO Libraries and Software In Table 3, we provide a brief summary of how resampling is handled in popular HPO libraries and software.6 For each library, we checked whether the core functionality, examples, or tutorials mention 5We admit that these benchmark libraries implement efficient benchmarking methods such as surro- gate (Eggensperger et al., 2018; Pfisterer et al., 2022) or tabular benchmarks (Pineda Arango et al., 2021). It would be possible to adapt them to return the test performance, however, changes in the HPO evaluation protocol, such as the one we propose, would not be feasible. 6This summary is not exhaustive but reflects the general consensus observed in widely-used software. 17the possibility of reshuffling the resampling during HPO or if the resampling is considered fixed. If reshuffling is used in an example, mentioned, or if core functionality uses it, we mark it with a ✓. If it is unclear or inconsistent across examples and core functionality, we mark it with a ?. Otherwise, we use a ✗. Our conclusion is that the concept of reshuffling resampling generally receives little attention. Table 3: Exemplary Treatment of Resamplings in HPO Libraries and Software Software Reshuffled? Reference(s) sklearn ✗ GridSearchCV1/ RandomizedSearchCV2 HEBO ✗ sklearn_tuner3 optuna ? Inconsistency between examples 4,5,6 bayesian-optimization ✗ sklearn Example7,8 ax ✗ CNN Example9 spearmint ✗ No official HPO Examples scikit-optimize ✗ BO for GBT Example7,10 SMAC3 ✗ SVM Example7,11 dragonfly ✗ Tree Based Ensemble Example12 aws sagemaker ✗ Blog Post13 raytune ? Inconsistency between examples 14, 15 hyperopt(-sklearn) ? Cost Function Logic 16 ✗: no reshuffling, ?: both reshuffling and no reshuffling or unclear, ✓: reshuffling 1 https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1263 2 https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1644 3 https://github.com/huawei-noah/HEBO/blob/b60f41aa862b4c5148e31ab4981890da6d41f2b1/HEBO/hebo/sklearn_t uner.py#L73 4 https://github.com/optuna/optuna-integration/blob/15e6b0ec6d9a0d7f572ad387be8478c56257bef7/optuna_in tegration/sklearn/sklearn.py#L223 here sklearn’s cross_validate is used which by default does not reshuffle the resampling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_validation.py#L186 5 https://github.com/optuna/optuna-examples/blob/dd56b9692e6d1f4fa839332edbcdd93fd48c16d8/pytorch/py torch_simple.py#L79 here, data loaders for train and valid are instantiated within the objective of the trial but the data within the loaders is fixed 6 https://github.com/optuna/optuna-examples/blob/dd56b9692e6d1f4fa839332edbcdd93fd48c16d8/xgboost/xgbo ost_simple.py#L22 here, the train validation split is performed within the objective of the trial and no seed is set which results in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/s klearn/model_selection/_split.py#L2597 7 functionality relies on sklearn’s cross_val_score which by default does not reshuffle the resampling https://github.com/sciki t-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/model_selection/_validati on.py#L631 8 https://github.com/bayesian-optimization/BayesianOptimization/blob/c7e5c3926944fc6011ae7ace29f7b5ed0f 9c983b/examples/sklearn_example.py#L32 9 https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/tutorials/tune_cnn_serv ice.ipynb#L39 and https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/ax/util s/tutorials/cnn_utils.py#L154 10 https://github.com/scikit-optimize/scikit-optimize/blob/a2369ddbc332d16d8ff173b12404b03fea472492/ex amples/hyperparameter-optimization.py#L82C21-L82C36 11 https://github.com/automl/SMAC3/blob/9aaa8e94a5b3a9657737a87b903ee96c683cc42c/examples/1_basics/2_sv m_cv.py#L63 12 https://github.com/dragonfly/dragonfly/blob/3eef7d30bcc2e56f2221a624bd8ec7f933f81e40/examples/tree_r eg/skltree.py#L111 13 https://aws.amazon.com/blogs/architecture/field-notes-build-a-cross-validation-machine-learning-mod el-pipeline-at-scale-with-amazon-sagemaker/ 14 https://github.com/ray-project/ray/blob/3f5aa5c4642eeb12447d9de5dce22085512312f3/doc/source/tune/exa mples/tune-pytorch-cifar.ipynb#L120 here, data loaders for train and valid are instantiated within the objective but the data within the loaders are fixed 15 https://github.com/ray-project/ray/blob/3f5aa5c4642eeb12447d9de5dce22085512312f3/doc/source/tune/exa mples/tune-xgboost.ipynb#L335 here, the train validation split is performed within the objective and no seed is set which results in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3 /sklearn/model_selection/_split.py#L2597 16 https://github.com/hyperopt/hyperopt-sklearn/blob/4bc286479677a0bfd2178dac4546ea268b3f3b77/hpsklearn /estimator/_cost_fn.py#L144 dependence on random seed which by default is not set and there is no discussion of reshuffling and behavior is somewhat unclear 18C Proofs of the Main Results C.1 Proof of Theorem 2.1 We impose stability assumptions on the learning algorithm similar to Bayle et al. (2020); Austern & Zhou (2020). Let Z, Z1, . . . ,Zn, Z′ 1, be iid random variables. Define T = {Zi}n i=1, and T ′ as T but with Zn replaced by the independent copy Z′ n. Define eℓn(z, λ) = ℓ(z, gλ(T )) − E[ℓ(Z, gλ(T )) | T], assume that each gλ(T ) is invariant to the ordering in T , ℓ is bounded, and max λ∈Λ E{[eℓ(Z, gλ(T )) − eℓ(Z, gλ(T ′))]2} = o(1/n). (4) This loss stability assumption is rather mild, see Bayle et al. (2020) for an extensive discussion. Further, define the risk R(g) = E[ℓ(Z, g)] and assume that for every λ ∈ Λ, there is a prediction rule g∗ λ such that max λ∈Λ E[|R(gλ(T )) − R(g∗ λ)|] = o(1/√n). (5) This assumption requires gλ(T ) to converge to some fixed prediction rule sufficiently fast and serves as a reasonable working condition for our purposes. It is satisfied, for example, when ℓ is the square loss and gλ is an empirical risk minimizer over a hypothesis class Gλ with finite VC-dimension. For further examples, see, e.g., Bousquet & Zhivotovskiy (2021), van Erven et al. (2015), and references therein. The assumption could be relaxed, but this would lead to a more complicated limiting distribution but with the same essential interpretation. Theorem C.1. Under assumptions (4) and (5), it holds √n (bµ(λj) − µ(λj))J j=1 →d N(0, Σ), where Σj,j′ = τi,j,M lim n→∞ Cov[¯ℓn(Z, λj), ¯ℓn(Z, λj′ )], τj,j′,M = lim n→∞ 1 nM2α2 nX i=1 MX m=1 MX m′=1 Pr(i ∈ Im,j ∩ Im′,j′ ). Proof. Define eµ(λj) = 1 M MX m=1 E[L(Vm,j, gλj (Tm,j)) | Tm,j]. By the triangle inequality (first and second step), Jensen’s inequality (third step), and (5) (last step), E[|eµ(λj) − µ(λj)|] ≤ max 1≤m≤M E \u0002\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, gλj (Tm,j))] \f\f\u0003 ≤ max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, g∗ λj )] \f\f\f i + max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j))] − E[L(Vm,j, g∗ λj )] \f\f\f i ≤ 2 max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, g∗ λj )] \f\f\f i = 2 max 1≤m≤M E h\f\f\fR(gλj (Tm,j)) − R(g∗ λj ) \f\f\f i = o(1/√n). Next, assumption (4) together with Theorem 2 and Proposition 3 of Bayle et al. (2020) yield √n (bµ(λj) − eµ(λj)) − 1 M MX m=1 1 α√n X i∈Im,j ¯ℓn(Zi, λj) →p 0. 19Now rewrite 1 Mα√n MX m=1 X i∈Im,j ¯ℓn(Zi, λj) = 1 Mα√n nX i=1 MX m=1 1(i ∈ Im,j)¯ℓn(Zi, λj) | {z } :=ξ(j) i,n . The sequence (ξi,n)n i=1 = (ξ(j) i,n, . . . , ξ(j) i,n)n i=1 is a triangular array of independent, centered, and bounded random vectors. Because 1(Zi ∈ Vm,j) and Zi are independent, it holds Cov(ξ(j) i,n, ξ(j′) i,n ) = MX m=1 MX m′=1 E[1(i ∈ Im,j ∩ Im′,j′ )]E[¯ℓn(Zi, λj)¯ℓn(Zi, λj′ )], so lim n→∞ Cov \" 1 Mα√n nX i=1 ξ(j) i,n, 1 Mα√n nX i=1 ξ(j′) i,n # = lim n→∞ 1 nM2α2 nX i=1 Cov h ξ(j) i,n, ξ(j′) i,n i = Σj,j′ . Now the result follows from Lindeberg’s central limit theorem for triangular arrays (e.g., van der Vaart, 2000, Proposition 2.27). C.2 Proof of Theorem 2.2 We want to bound the probability thatµ(ˆλ) −µ(λ∗) is large. For some δ >0, define the set of ‘good’ hyperparameters Λδ = {λj : µ(λj) − µ(λ∗) ≤ δ}. Now Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 = Pr \u0010 bλ /∈ Λδ \u0011 = Pr \u0012 min λ/∈Λδ bµ(λ) < min λ∈Λδ bµ(λ) \u0013 ≤ Pr \u0012 min λ/∈Λδ bµ(λ) < min λ∈Λδ/2 bµ(λ) \u0013 = Pr \u0012 min λ/∈Λδ µ(λ) + ϵ(λ) < min λ∈Λδ/2 µ(λ) + ϵ(λ) \u0013 ≤ Pr \u0012 δ + min λ/∈Λδ ϵ(λ) < δ/2 + min λ∈Λδ/2 ϵ(λ) \u0013 = Pr \u0012 min λ/∈Λδ ϵ(λ) − min λ∈Λδ/2 ϵ(λ) < −δ/2 \u0013 = Pr \u0012 max λ/∈Λδ ϵ(λ) − max λ∈Λδ/2 ϵ(λ) > δ/2 \u0013 . (ϵ d = −ϵ) There is a tension between the two maxima. The more λ’s there are in Λδ/2 and the less they are correlated, the more likely it is to find one ϵ(λ) that is large. This makes the probability small. However, the less ϵ is correlated, the larger is maxλ/∈Λδ ϵ(λ), making the probability large. To formalize this, use the Gaussian concentration inequality (Talagrand, 2005, Lemma 2.1.3): Pr \u0012 max λ/∈Λδ ϵ(λ) − max λ∈Λδ/2 ϵ(λ) > δ/2 \u0013 ≤ Pr \u0012 2 \f\f\f\fmax λ∈Λ ϵ(λ) − E \u0014 max λ∈Λ ϵ(λ) \u0015\f\f\f\f > δ/2 − E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 + E \u0014 max λ/∈Λδ ϵ(λ) \u0015\u0013 ≤ 2 exp ( − \u0000 δ/2 − E \u0002 maxλ∈Λδ/2 ϵ(λ) \u0003 + E[maxλ/∈Λδ ϵ(λ)] \u00012 8σ2 ) , provided δ/2−E \u0002 maxλ∈Λδ/2 ϵ(λ) \u0003 +E[maxλ/∈Λδ ϵ(λ)] ≥ 0. We bound the two maxima separately. 20Lower Bound for Maximum over the Good Set Recall the definition of m right before Theorem 2.2 and observe Λδ/2 = {λ: µ(λ) − µ(λ∗) ≤ δ/2} ⊃ {λ: m∥λ − λ∗∥2 ≤ δ/2} = {λ: ∥λ − λ∗∥ ≤(δ/2m)1/2} = B(λ∗, (δ/2m)1/2). Pack the ball B(λ∗, (δ/2m)1/2) with smaller balls with radius η. We can always construct such a packing with at least (δ/2mη2)d/2 elements. By assumption, each small ball contains at least one element of Λ. Pick one element from each small ball and collect them into the set Λ′ δ/2. By construction, |Λ′ δ/2| ≥(δ/2mη2)d/2 and min λ̸=λ′∈Λ′ δ/2| ∥λ − λ′∥ ≥η. Sudakov’s minoration principle (e.g., Wainwright, 2019, Theorem 5.30) gives E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 ≥ 1 2 q log |Λ′ δ/2| min {λ̸=λ′}∩Λ′ δ/2 p Var[ϵ(λ) − ϵ(λ′)] ≥ 1 2 q log |Λ′ δ/2| min ∥λ−λ′∥≥η p Var[ϵ(λ) − ϵ(λ′)]. In general, Var[ϵ(λ) − ϵ(λ′)] = K(λ, λ) + K(λ′, λ′) − 2τ2K(λ, λ′) = (1 − τ2)[K(λ, λ) + K(λ′, λ′)] + τ2[K(λ, λ) − K(λ, λ′)] + τ2[K(λ′, λ′) − K(λ, λ′)] ≥ 2σ2(1 − τ2). Hence, we have min ∥λ−λ′∥≥η Var[ϵ(λ) − ϵ(λ′)] ≥ 2σ2(1 − τ2), which implies E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 ≥ 1 2σ √ d p 1 − τ2 p log(δ/2mη2) =: σ √ dA(τ, δ)/2. Upper Bound for Maximum over the Bad Set Dudley’s entropy bound (e.g., Giné & Nickl, 2016, Theorem 2.3.6) gives E \u0014 max λ/∈Λδ ϵ(λ) \u0015 ≤ 12 Z ∞ 0 p log N(s)ds, where N(s) is the minimum number of points λ1, . . . ,λN(s) such that sup λ∈Λ min 1≤k≤N(s) p Var[ϵ(λ) − ϵ(λk)] ≤ s. Note that sup λ,λ′∈Λ p Var[ϵ(λ) − ϵ(λ′)] ≤ 2σ, so N(s) = 1 for all s ≥ 2σ. For s2 ≤ 4σ2(1 − τ2), we can use the trivial bound N(s) ≤ J. For s2 > 4σ2(1 − τ2), cover Λ with ℓ2-balls of size (s/2στκ ). We can do this with less than N(s) ≤ (6σκ/s)d ∨ 1 such balls. Let λ1, . . . ,λN be the centers of these balls. In general, it holds Var[ϵ(λ) − ϵ(λ′)] = K(λ, λ) + K(λ′, λ′) − 2τ2K(λ, λ′) = (1 − τ2)[K(λ, λ) + K(λ′, λ′)] + τ2[K(λ, λ) − K(λ, λ′)] + τ2[K(λ′, λ′) − K(λ, λ′)] ≤ 2(1 − τ2)σ2 + 2τ2σ2κ2∥λ − λ′∥2. 21For s2 > 4σ2(1 − τ2), we thus have sup λ∈Λ min 1≤k≤N(s) Var[ϵ(λ) − ϵ(λk)] ≤ sup ∥λ−λ′∥2≤(s/2τσκ)2 Var[ϵ(λ) − ϵ(λ′)] ≤ 2(1 − τ2)σ2 + 2τ2σ2κ2(s/2τσκ )2 ≤ s2, as desired. Now decompose the integral Z ∞ 0 p log N(s)ds = Z 2σ √ 1−τ2 0 p log N(s)ds + Z 2σ 2σ √ 1−τ2 p log N(s)ds ≤ 2σ √ d p 1 − τ2 p log J + Z 2σ 2σ √ 1−τ2 p log N(s)ds. For the second term, compute Z 2σ σ √ 1−τ2 p log N(s)ds ≤ √ d Z 2σ 2σ √ 1−τ2 p log(6σκ/s)+ ds = σ √ d Z 2 2 √ 1−τ2 p log(6κ/s)+ ds ≤ σ √ d \u0012Z 2 0 log(6κ/s)+ ds \u00131/2 \u0010 2(1 − p 1 − τ2) \u00111/2 = σ √ d p 2 + 2 log(3κ)+ \u0010 2(1 − p 1 − τ2) \u00111/2 = 2σ √ d p 1 + log(3κ)+ τ (1 + √ 1 − τ2)1/2 ≤ 2σ √ dτ p 1 + log(3κ)+. We have shown that E \u0014 max λ/∈Λδ ϵ(λ) \u0015 ≤ 24σ √ d hp 1 − τ2 p log J + τ p 1 + log(3κ)+ i =: σ √ dB(τ)/4. Integrating Probabilities Summarizing the two previous steps, we have Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 ≤ 2 exp    − \u0010 δ − σ √ d[B(τ) − A(τ, δ)] \u00112 36σ2    , provided t ≥ σ √ d[B(τ) − A(τ, δ)]. Now for any s ≥ 0 and t ≥ 2es2 mη2, it holds A(τ, s) ≥ (σ/σ) p 1 − τ2s =: A(τ)s. In particular, if t ≥ 2es2 mη2 + σ √ d[B(τ) − A(τ)s] =: C, we have Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 ≤ 4 exp    − \u0010 δ − σ √ d[B(τ) − A(τ)s] \u00112 36σ2    . 22Integrating the probability gives E[µ(bλ) − µ(λ∗)] = Z ∞ 0 Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ = Z C 0 Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ + Z ∞ C Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ ≤ C + Z ∞ C exp    − \u0010 δ − σ √ d[B(τ) − A(τ)s] \u00112 36σ2    dδ ≤ C + √ 36σ = 2es2 mη2 + σ √ d[B(τ) − A(τ)s] + 6σ. Simplifying The bound can be optimized with respect to s, but the solution involves the Lambert W-function, which has no analytical expression. Instead choose s for simplicity as s = s log \u0012 σ 2mη2 \u0013 + . which gives E[µ(bλ) − µ(λ∗)] ≤ σ √ d \" 8 + B(τ) − A(τ) s log \u0012 σ 2mη2 \u0013# . D Additional Results on the Density of Random HPC Grids Lemma D.1. Suppose that the J elements in Λ are drawn independently from a continuous density p with c := min∥λ∥≤1 p(λ) > 0. Then with probability at least 1 − δ, η ≲ \u0010p log(1/δ)/J \u00111/d , and with probability 1, η ≲ \u0010p log(J)/J \u00111/d , for all J sufficiently large. Proof. We want to bound the probability that there is a λ such that |B(λ, η) ∩ Λ| = 0. In what follows λ is silently understood to have norm bounded by 1. Let eλ1, . . . ,eλN the centers of η/2-balls covering {∥λ∥ ≤1}, for which we may assume N ≤ (6/η)d. For eλk the closest center to λ, it holds ∥λ′ − λ∥ ≤ ∥λ′ − eλk∥ + ∥eλk − λ∥ ≤ ∥λ′ − eλk∥ + η/2, so ∥λ′ − eλk∥ ≤η/2 implies ∥λ′ − λ∥ ≤η. We thus have Pr(∃λ: |B(λ, η) ∩ Λ| = 0) = Pr   inf λ JX i=1 1{∥λi − λ∥ ≤η} ≤0 ! ≤ Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! . 23Further Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! = Pr   max 1≤k≤N JX i=1 −1{∥λi − eλk∥ ≤η/2} ≥0 ! ≤ Pr   max 1≤k≤N JX i=1 E h 1{∥λi − eλk∥ ≤η/2} i − 1{∥λi − eλk∥ ≤η/2} ≥J inf λ E[1{∥λi − λ∥ ≤η/2}] ! . It holds E[1{∥λi − λ∥ ≤η/2}] = Pr (∥λi − λ∥ ≤η/2) = Z ∥λ′−λ∥≤η/2 p(λ′)dλ′ ≥ c vol(B(0, η/2)) = cvd(η/2)d, where vd = vol(B(0, 1)). Now the union bound and Hoeffding’s inequality give Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≤ N exp \u0012 −Jc2v2 d(η/2)2d 2 \u0013 ≤ (6/η)d exp \u0012 −Jc2v2 d(η/2)2d 2 \u0013 . Choosing η = 2 \u0012q 2 log(3d √ Jcvd/δ)/ √ Jcvd \u00131/d gives Pr(∃λ: |B(λ, η) ∩ Λ| = 0) ≤ δ/ q 2 log(3d √ Jcvd), which is bounded by δ when √ J ≥ e1/2/3dcvd. Further, setting η = 2( p 6 log(J)/ √ Jcvd)1/d gives Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≲ J−5/2, so that ∞X J=1 Pr   min 1≤j≤J min 1≤k≤N jX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≤ ∞X J=1 J Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≲ ∞X J=1 1 J3/2 < ∞. Now the Borel-Cantelli lemma (e.g., Kallenberg, 1997, Theorem 4.18) implies that, with probability 1, |B(λ, η) ∩ Λ| ≥1, for all J sufficiently large. 24E Selected Validation Schemes E.1 Definition of Index Sets Recall: (i) (holdout) Let M = 1 and I1,j = I1 for all j = 1, . . . , J, and some size-⌈αn⌉ index set I1. (ii) (reshuffled holdout) Let M = 1 and I1,1, . . . ,I1,J be independently drawn from the uniform distribution over all size-⌈αn⌉ subsets from {1, . . . , n}. (iii) (M-fold CV) Let α = 1/M and I1, . . . ,IM be a disjoint partition of {1, . . . , n}, and Im,j = Im for all j = 1, . . . , J. (iv) (reshuffled M-fold CV) Let α = 1/M and (I1,j, . . . ,IM,j ), j= 1, . . . , J, be independently drawn from the uniform distribution over disjoint partitions of {1, . . . , n}. (v) (M-fold holdout) LetIm, m= 1, . . . , M, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n} and set Im,j = Im for all m = 1, . . . , M, j= 1, . . . , J. (vi) (reshuffled M-fold holdout) Let Im,j, m= 1, . . . , M, j= 1, . . . , J, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n}. E.2 Derivation of Reshuffling Parameters in Limiting Distribution Recall τi,j,M = 1 nM2α2 nX s=1 MX m=1 MX m′=1 Pr(s ∈ Im,i ∩ Im′,j). For all schemes in the proposition, the probabilities are independent of the index s, so the average over s = 1, . . . , ncan be omitted. We now verify the constants σ, τfrom Table 1. (i) It holds Pr(s ∈ I1,i ∩ I1,j) = Pr(s ∈ I1) = α. Hence, τi,j,1 = 1/α = 1/α × 1 = σ2 × τ2. (ii) (reshuffled holdout) This is a special case of part (vi) with M = 1. (iii) ( M-fold CV) It holds Pr(s ∈ Im,i ∩ Im′,j) = Pr(s ∈ Im ∩ Im′ ) = \u001a1/M, m = m′, 0, m ̸= m′. Only M probabilities in the double sum are non-zero, whence τi,j,M = 1 M2α2 × M/M = 1/α2M2 = 1 × 1 = σ2 × τ2, where we used α = 1/M. (iv) (reshuffled M-fold CV) It holds Pr(s ∈ Im,i ∩ Im′,j) =    1/M, m = m′, i= j 0, m ̸= m′, i= j 1/M2, m = m′, i̸= j 1/M2, m ̸= m′, i̸= j. For i = j, only M probabilities in the double sum are non-zero. Also using α = 1/M, we get τi,j,M = 1 M2α2 × M × 1/M = 1 = σ2. For i ̸= j, τi,j,M = 1 M2α2 × M2 × 1/M2 = 1 × 1 = σ2 × τ2. 25(v) ( M-fold holdout) It holds Pr(s ∈ Im,i ∩ Im′,j) = Pr(s ∈ Im ∩ Im′ ) = \u001aα, m = m′, α2, else. This gives τi,j,M = 1 M2α2 × [M × α + (M − 1)M × α2] = [1/αM + (M − 1)/M] × 1 = σ2 × τ2. for all i, j. (vi) (reshuffled M-fold holdout) It holds Pr(s ∈ Im,i ∩ Im′,j) = \u001aα, m = m′, i= j α2, else. For i = j, this gives τi,j,M = 1 M2α2 × [M × α + (M − 1)M × α2] = 1/αM + (M − 1)/M. For i ̸= j, τi,j,M = 1 M2α2 × (M2 × α2) = 1. This implies that (1) holds with σ2 = 1/Mα + (M − 1)/M, τ2 = 1/(1/Mα + (M − 1)/M). Remark E.1. Although not technically covered by Theorem 2.1, performing independent bootstraps for each λj correspond to reshuffled n-fold holdout with α = 1 /n. Accordingly, σ ≈ √ 2 and τ ≈ p 1/2. F Details Regarding Benchmark Experiments F.1 Datasets We list all datasets used in the benchmark experiments in Table 4. Table 4: List of datasets used in benchmark experiments. All information can be found on OpenML (Vanschoren et al., 2014). OpenML Dataset ID Dataset Name Size ( n × p) 23517 numerai28.6 96320 × 21 1169 airlines 539383 × 7 41147 albert 425240 × 78 4135 Amazon_employee_access 32769 × 9 1461 bank-marketing 45211 × 16 1590 adult 48842 × 14 41150 MiniBooNE 130064 × 50 41162 kick 72983 × 32 42733 Click_prediction_small 39948 × 11 42742 porto-seguro 595212 × 57 Note that datasets serve as data generating processes (DGPs; Hothorn et al., 2005). As we are mostly concerned with the actual generalization performance of the final best HPC found during HPO based on validation performance we rely on a comparably large held out test set that is not used during HPO. We therefore use 5000 data points sampled from a DGP as an outer test set. To further be able to measure the generalization performance robustly for varying data sizes available during HPO, we construct concrete tasks based on the DGPs by sampling subsets of (train_valid; n) size 500, 1000 and 5000 from the DGPs. This results in 30 tasks in total (10 DGPS × 3 train_valid sizes). For more details and the concrete implementation of this procedure, see Appendix F.3. We also collected another 5000 data points as an external validation set, but did not use it. Therefore, we had to tighten the restriction to 10000 data points mentioned in the main paper to 15000 data points as the lower bound on data points. To allow for stronger variation over different replications, we decided to use 20000 as the final lower bound. 26F.2 Learning Algorithms Here we briefly present training pipeline details and search spaces of the learning algorithms used in our benchmark experiments. The funnel-shaped MLP is based on sklearn’s MLP Classifier and is constructed in the following way: The hidden layer size for each layer is determined by num_layers and max_units. We start with max_units and half the number of units for every subsequent layer to create a funnel. max_batch_size is the largest power of 2 that is smaller than the number of training samples available. We use ReLU as activation function and train the network optimizing logloss as a loss function via SGD using a constant learning rate and Nesterov momentum for 100 epochs. Table 5 lists the search space (inspired from Zimmer et al. (2021)) used during HPO. The Elastic Net is based on sklearn’s Logistic Regression Classifier. We train it for a maximum of 1000 iterations using the \"saga\" solver. Table 6 lists the search space used during HPO. The XGBoost and CatBoost search spaces are listed in Table 7 and Table 8, both inspired from their search spaces used in McElfresh et al. (2023). For both the Elastic Net and Funnel MLP, missing values are imputed in the preprocessing pipeline (mean imputation for numerical features and adding a new level for categorical features). Categorical features are target encoded in a cross-validated manner using a 5-fold CV . Features are then scaled to zero mean and unit variance via a standard scaler. For XGBoost, we impute missing values for categorical features (adding a new level) and target encode them in a cross-validated manner using a 5-fold CV . For CatBoost, no preprocessing is performed. XGBoost and CatBoost models are trained for 2000 iterations and stop early if the validation loss (using the default internal loss function used during training, i.e., logloss) does not improve over a horizon of 20 iterations. For retraining the best configuration on the whole train and validation data, the number of boosting iterations is set to the number of iterations used to find the best validation performance prior to the stopping mechanism taking action.7 F.3 Exact Implementation In the following, we outline the exact implementation of performing one HPO run for a given learning algorithm on a concrete task (dataset × train_valid size) and a given resampling. We release all code to replicate benchmark results and reproduce our analyses via https://github.com/slds-l mu/paper_2024_reshuffling. For a given replication (in total 10): 1. We sample (without replacement) train_valid size (500, 1000 or 5000 points) and test size (always 5000) points from the DGP (i.e. a concrete dataset in Table 4). These are shared for every learning algorithm (i.e. all learning algorithms are evaluated on the same data). 2. A given HPC is evaluated in the following way: • The resampling operates on the train validation 8 set of size train_valid. • The learning algorithm is configured by the HPC. • The learning algorithm is trained on training splits and evaluated on validation splits according to the resampling strategy. In case reshuffling is turned on, the training and validation splits are recreated for every HPO. We compute the Accuracy, ROC AUC and logloss when using a random search and compute ROC AUC when using HEBO or SMAC3 and average performance over all folds for resamplings involving multiple folds. • For each HPC we then always re-train the model on all train_valid data being available and evaluate the model on the held-out test set to compute an outer estimate of generalization performance for each HPC (regardless of whether it is the incumbent for a given iteration or not). 7For CV and repeated holdout we take the average number of boosting iterations over the models trained on the different folds. 8With train validation we refer to all data being available during HPO which is then further split by a resampling into train and validation sets. 27Table 5: Search Space for Funnel-Shaped MLP Classifier. Parameter Type Range Log num_layers Int. 1 to 5 No max_units Int. 64, 128, 256, 512 No learning_rate Num. 1 × 10−4 to 1 × 10−1 Yes batch_size Int. 16, 32, ..., max_batch_size No momentum Num. 0.1 to 0.99 No alpha Num. 1 × 10−6 to 1 × 10−1 Yes Table 6: Search Space for Elastic Net Classifier. Parameter Type Range Log C Num. 1 × 10−6 to 1 × 104 Yes l1_ratio Num. 0.0 to 1.0 No Table 7: Search Space for XGBoost Classifier. Parameter Type Range Log max_depth Int. 2 to 12 Yes alpha Num. 1 × 10−8 to 1.0 Yes lambda Num. 1 × 10−8 to 1.0 Yes eta Num. 0.01 to 0.3 Yes Table 8: Search Space for CatBoost Classifier. Parameter Type Range Log learning_rate Num. 0.01 to 0.3 Yes depth Int. 2 to 12 Yes l2_leaf_reg Num. 0.5 to 30 Yes 3. We evaluate 500 HPCs when using random search and 250 HPC when using HEBO or SMAC3 (SMAC4HPO facade). As resamplings, we use holdout with a 80/20 train-validation split and 5 folds for CV , so that the holdout strategy is just one fold of the CV and the fraction of data points being used for training and respectively validation are the same across different resampling strategies. 5-fold holdout simply repeats the holdout procedure five times and 5x 5-fold CV repeats the 5-fold CV five times. Each of the four resamplings can be reshuffled or not (standard). As mentioned above, the test set is only varied for each of the 10 replica (repetitions with different seeds), but consistent for different tasks (i.e. the different learning algorithms are evaluated on the same test set, similarly, also the different dataset subsets all share the same test set). This allows for fair comparisons of different resamplings on a concrete problem (i.e. a given dataset, train_valid size and learning algorithm). Additionally, for the random search, the 500 HPCs evaluated for a given learning algorithm are also fixed over different dataset and train_valid size combinations. This is done to allow for an isolation of the effect, the concrete resampling (and whether it is reshuffled or not) has on generalization performance, reducing noise arising due to different HPCs. Learning algorithms themselves are not explicitly seeded to allow for variation during model training over different replications. Resamplings and partitioning of data are always performed in a stratified manner with respect to the target variable. For the random search, we only ran (standard and reshuffled) holdout and (standard and reshuffled) 5x 5-fold CV experiments (because we can simulate 5-fold CV and 5-fold holdout experiments based 28on the results obtained from the 5x 5-fold CV (by only considering the first repeat or the first fold for each of the five repeats).9 For running HEBO or SMAC3, each resampling (standard and reshuffled for holdout, 5-fold holdout, 5-fold CV , 5x 5-fold CV) has to be actually run due to the adaptive nature of BO. For the random search experiments, this results in 10 (DGPs)× 3 (train_valid sizes) × 4 (learning algorithms) × 2 (holdout or 5x 5-fold CV) × 2 (standard or reshuffled) × 10 (replications) = 4800 HPO runs,10 each involving the evaluation of 500 HPCs and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data) or 26 (for 5x 5-fold CV; due to retraining on train validation data) model fits. In summary, the random search experiments involve the evaluation of 2.4 Million HPCs with in total 33.6 Million model fits. Similarly, for the HEBO and SMAC3 experiments, this each results in 10 (DGPs)× 3 (train_valid sizes) × 4 (learning algorithms) × 4 (holdout, 5-fold CV , 5x 5-fold CV or 5-fold holdout) × 2 (standard or reshuffled) × 10 (replications) = 9600 HPO runs 11, each involving the evaluation of 250 HPCs and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data), 6 (for 5-fold CV or 5-fold holdout; due to retraining on train validation data) or 26 (for 5x 5-fold CV; due to retraining on train validation data) model fits. In summary, the HEBO and SMAC3 experiments each involve the evaluation of 2.4 Million HPCs with in total 24 Million model fits. F.4 Compute Resources We estimate our total compute time for the random search, HEBO and SMAC3 experiments to be roughly 11.86 CPU years. Benchmark experiments were run on an internal HPC cluster equipped with a mix of Intel Xeon E5-2670, Intel Xeon E5-2683 and Intel Xeon Gold 6330 instances. Jobs were scheduled to use a single CPU core and were allowed to use up to 16GB RAM. Total emissions are estimated to be an equivalent of roughly 6508.67 kg CO2. G Additional Benchmark Results Visualizations G.1 Main Experiments In this section, we provide additional visualizations of the results of our benchmark experiments. Figure 6 illustrates the trade-off between the final number of model fits required by different resam- plings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. We can see that the reshuffled holdout on average comes close to the final test performance of the overall more expensive 5-fold CV . Below, we give an overview of the different types of additional analyses and visualizations we provide. Normalized metrics, i.e., normalized validation or test performance refer to the measure being scaled to [0, 1] based on the empirical observed minimum and maximum values obtained on the raw results level (ADTM; see Wistuba et al., 2018). More concretely, for each scenario consisting of a learning algorithm that is run on a given task (dataset × train_valid size) given a certain performance metric, the performance values (validation or test) for all resamplings and optimizers are normalized on the replication level to [0, 1] by subtracting the empirical best value and dividing by the range of performance values. Therefore a normalized performance value of 0 is best and 1 is worst. Note that we additionally provide further aggregated results on the learning algorithm level and raw results of validation and test performance via https://github.com/slds-lmu/paper_2024_reshuffl ing. • Random search – Normalized validation performance in Figure 7. 9We even could have simulated the vanilla holdout from the 5x 5-fold CV experiments by choosing an arbitrary fold and repeat but choose not to do so, to have some sanity checks regarding our implementation by being able to compare the \"true\" holdout with a the simulated holdout. 10Note that we do not have to take the 3 different metrics into account because random search allows us to simulate runs for different metric post hoc. 11Note that HEBO and SMAC3 were only run for ROC AUC as the performance metric. 29500 1000 5000 300 1000 3000 10000 300 1000 3000 10000 300 1000 3000 10000 0.20 0.25 0.30 0.35 0.40 0.25 0.30 0.35 0.40 0.45 0.30 0.35 0.40 0.45 No. Final Model Fits Mean Normalized T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. – Normalized test performance in Figure 8. – Improvement in test performance over 5-fold CV in Figure 9. – Rank w.r.t. test performance in Figure 10. • HEBO and SMAC3 vs. random search holdout – Normalized validation performance in Figure 11. – Normalized test performance in Figure 12. – Improvement in test performance over standard holdout in Figure 13. – Rank w.r.t. test performance in Figure 14. • HEBO and SMAC3 vs. random search 5-fold holdout – Normalized validation performance in Figure 15. – Normalized test performance in Figure 16. – Improvement in test performance over standard 5-fold holdout in Figure 17. – Rank w.r.t. test performance in Figure 18. • HEBO and SMAC3 vs. random search 5-fold CV – Normalized validation performance in Figure 19. – Normalized test performance in Figure 20. – Improvement in test performance over 5-fold CV in Figure 21. – Rank w.r.t. test performance in Figure 22. • HEBO and SMAC3 vs. random search 5x 5-fold CV – Normalized validation performance in Figure 23. – Normalized test performance in Figure 24. – Improvement in test performance over 5x 5-fold CV in Figure 25. – Rank w.r.t. test performance in Figure 26. 30Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 No. HPC Evaluations Mean Normalized Validation Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 7: Random search. Average normalized performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.3 0.4 0.5 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.3 0.4 0.25 0.30 0.35 0.40 0.45 0.50 0.1 0.2 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 8: Random search. Average normalized test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 31Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −0.50 −0.25 0.00 0.25 −0.3 −0.2 −0.1 0.0 0.1 −0.4 −0.2 0.0 −1.2 −0.8 −0.4 0.0 0.4 −1.5 −1.0 −0.5 0.0 −0.75 −0.50 −0.25 0.00 0.25 −1.0 −0.5 0.0 0.5 −2.0 −1.5 −1.0 −0.5 0.0 No. HPC Evaluations Mean T est Improvement Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 9: Random search. Average improvement (compared to standard 5-fold CV) with respect to test performance of the incumbent over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 4.0 4.5 5.0 5.5 3.5 4.0 4.5 5.0 5.5 6.0 4 5 4.0 4.5 5.0 5.5 3.5 4.0 4.5 5.0 5.5 6.0 4 5 6 4.0 4.5 5.0 4.0 4.5 5.0 5.5 4 5 6 No. HPC Evaluations Mean Rank (T est Performance) Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 10: Random search. Average ranks (lower is better) with respect to test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 32500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 11: HEBO and SMAC3 vs. random search for holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 12: HEBO and SMAC3 vs. random search for holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 13: HEBO and SMAC3 vs. random search for holdout. Average improvement (compared to standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 33500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.25 3.50 3.75 4.00 3.00 3.25 3.50 3.75 4.00 3.00 3.25 3.50 3.75 4.00 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 14: HEBO and SMAC3 vs. random search for holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.4 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 15: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.25 0.35 0.45 0.55 0.3 0.4 0.5 0.25 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 16: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 34500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 17: HEBO and SMAC3 vs. random search for 5-fold holdout. Average improvement (compared to standard 5-fold holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.00 3.25 3.50 3.75 4.00 3.25 3.50 3.75 3.25 3.50 3.75 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 18: HEBO and SMAC3 vs. random search for 5-fold holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.4 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 19: HEBO and SMAC3 vs. random search for 5-fold CV . Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 35500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 20: HEBO and SMAC3 vs. random search for 5-fold CV . Average normalized test performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 21: HEBO and SMAC3 vs. random search for 5-fold CV . Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.25 3.50 3.75 3.2 3.4 3.6 3.8 3.3 3.5 3.7 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 22: HEBO and SMAC3 vs. random search for 5-fold CV . Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 36500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 23: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 24: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average normalized test performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 −0.4 0.0 0.4 0.8 1.2 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 25: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average improvement (compared to standard 5x 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 37500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.00 3.25 3.50 3.75 3.25 3.50 3.75 4.00 3.2 3.4 3.6 3.8 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 26: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 38G.2 Ablation on M-fold holdout Based on the 5x 5-fold CV results we further simulated different M-fold holdout resamplings (standard and reshuffled) by taking M repeats from the first fold of the 5x 5-fold CV . This allows us to get an understanding of the effect more folds have on M-fold holdout, especially in the context of reshuffling. Regarding normalized validation performance we observe that more folds generally result in a less optimistically biased validation performance (see Figure 27). Looking at normalized test performance (Figure 28) we observe the general trend that more folds result in better test performance – which is expected. Reshuffling generally results in better test performance compared to the standard resampling (with the exception of logloss where especially in the case of a single holdout, reshuffling can hurt generalization performance). This effect is smaller, the more folds are used, which is in line with our theoretical results presented in Table 1. Looking at improvement compared to standard 5-fold holdout with respect to test performance and ranks with respect to test performance, we observe that often reshuffled 2-fold holdout results that are highly competitive with standard 3, 4 or 5-fold holdout. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.0 0.2 0.4 0.6 0.8 0.25 0.50 0.75 No. HPC Evaluations Mean Normalized Validation Performance Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 27: Random search. Average normalized validation performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 39Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.25 0.30 0.35 0.40 0.45 0.50 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.25 0.30 0.35 0.40 0.45 0.30 0.35 0.40 0.45 0.50 0.1 0.2 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 28: Random search. Average normalized test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −0.6 −0.4 −0.2 0.0 0.2 −0.3 −0.2 −0.1 0.0 0.1 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −1.0 −0.5 0.0 −1.5 −1.0 −0.5 0.0 −0.50 −0.25 0.00 0.25 0.50 −1.0 −0.5 0.0 −2.0 −1.5 −1.0 −0.5 0.0 No. HPC Evaluations Mean T est Improvement Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 29: Random search. Average improvement (compared to standard 5-fold holdout) with respect to test performance of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 40Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 5.0 5.5 6.0 6.5 5.0 5.5 6.0 6.5 7.0 4.5 5.0 5.5 6.0 6.5 5.0 5.5 6.0 6.5 4.5 5.0 5.5 6.0 6.5 7.0 5 6 7 4.8 5.2 5.6 6.0 5.0 5.5 6.0 6.5 5 6 7 No. HPC Evaluations Mean Rank (T est Performance) Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 30: Random search. Average ranks (lower is better) with respect to test performance of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 41NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We outline our three main contributions in the introduction (Section 1). We do not discuss generalization in the introduction, but rather in the discussion in Section 5. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper provides an analysis of reshuffling data in the context of estimating the generalization error for hyperparameter optimization. Our theoretical analysis explains why reshuffling works, and we experimentally verify the theoretical analysis. We discuss the limitations of our work in Section 5. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs 42Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Full assumptions and proofs for our main results (Theorem 2.1 and Theo- rem 2.2) are given in Appendix C.1 and Appendix C.2, respectively. Derivations for the parameters in Table 1 are provided in Appendix E. The additional results for the grid density are stated and proven directly in Appendix D. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 43(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Regarding datasets, we rely on OpenML.org. We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu /paper_2024_reshuffling. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] 44Justification: We report the standard error in every analysis. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details in Appendix F.4. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work provides a study on reshuffling data when estimating the generaliza- tion error in hyperparameter tuning. Therefore, our work is applicable wherever standard machine learning is applicable, and we do not see any ethical concerns in our method. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts 45Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper conducts fundamental research that is not tied to particular applica- tions, let alone deployment. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper conducts fundamental research that is not tied to particular applica- tions, let alone deployment. The paper does not develop models that have a high risk for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] 46Justification: We used datasets from OpenML.org and reference the dataset pages. Further information of the datasets, including their licenses, are available at OpenML.org. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide code as a new asset and describe how we make our code available in Point 5 of the NeurIPS Paper Checklist. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does neither involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 47Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does neither involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 48",
      "meta_data": {
        "arxiv_id": "2405.15393v2",
        "authors": [
          "Thomas Nagler",
          "Lennart Schneider",
          "Bernd Bischl",
          "Matthias Feurer"
        ],
        "published_date": "2024-05-24T09:48:18Z",
        "pdf_url": "https://arxiv.org/pdf/2405.15393v2.pdf",
        "github_url": "https://github.com/slds-lmu/paper_2024_reshuffling"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates that reshuffling resampling splits for every hyperparameter configuration (HPC) often improves the final model's generalization performance on unseen data. Theoretically, it shows that reshuffling can lead to finding configurations with better overall generalization, especially when the loss surface is flat and noisy, explaining how it affects the asymptotic behavior of the validation loss surface and providing a bound on expected regret. Empirically, controlled simulation studies confirm these theoretical insights, and large-scale, realistic hyperparameter optimization (HPO) experiments demonstrate that reshuffling leads to competitive test performances, drastically improving results for a single train-validation holdout protocol, making it competitive with standard cross-validation while being computationally cheaper.",
        "methodology": "The methodology comprises theoretical analysis, simulation studies, and benchmark experiments. The theoretical analysis investigates the effect of reshuffling on the empirical loss surface by deriving the limiting distribution of the validation loss, modeling it as a zero-mean Gaussian process and providing a bound on the expected regret. The simulation study uses a univariate quadratic loss surface with a squared exponential kernel for the noise process to systematically vary curvature, noise correlation, and reshuffling extent. Benchmark experiments utilize random search and state-of-the-art Bayesian Optimization (BO) variants (HEBO, SMAC3) to evaluate different resampling strategies (holdout, M-fold CV, M-fold holdout, 5x 5-fold CV) with and without reshuffling.",
        "experimental_setup": "The theoretical analysis relies on assumptions of loss stability and prediction rule convergence. The simulation study constructs a univariate quadratic loss surface and a squared exponential kernel for the noise process, varying parameters like curvature (m), noise correlation (κ), and reshuffling extent (τ), repeating each run 10,000 times. Benchmark experiments use a subset of small- to medium-sized tabular datasets from the AutoML benchmark (10 DGPs with 10,000-1,000,000 observations). Data is split into an outer test set (5,000 points) and varying train-validation sizes (500, 1000, 5000 points). Learning algorithms include CatBoost, XGBoost, Elastic Net, and a Funnel-shaped MLP, with detailed search spaces provided in the appendix. HPO algorithms are Random Search (500 HPCs) and BO (HEBO, SMAC3, 250 HPCs). Resampling methods are 80/20 holdout, 5-fold CV, 5-fold holdout, and 5x 5-fold CV, each tested with fixed and reshuffled splits. Performance metrics are Accuracy, ROC AUC, and Logloss (for Random Search) and ROC AUC (for BO). Anytime test performance is assessed by retraining the incumbent on all train/validation data and evaluating on the outer test set, averaged over 10 replications. Total compute resources were estimated at ~11.86 CPU years with ~6508.67 kg CO2 emissions.",
        "limitations": "The theoretical analysis relies on an asymptotic approximation of the empirical loss surface, operating on Gaussian loss surfaces with convenient concentration properties. It also assumes loss stability of learning algorithms, which may fail for highly sensitive losses like logloss, empirically observed to sometimes hurt generalization for small sample sizes. The study focuses on generalization after searching through a fixed, finite set of candidates, thus largely ignoring the dynamic nature of many HPO algorithms. Experimental evaluation is limited to tabular data and binary classification, avoiding extremely small or large datasets. The paper notes that for logloss, reshuffling rarely proved beneficial and could even harm generalization for small sample sizes.",
        "future_research_directions": "Future research should focus on developing a unified formal definition for 'oversearching,' 'overtuning,' or 'overfitting to the validation set,' and thoroughly analyzing its relation to validation performance measurements. Investigating the use of adaptive cross-validation techniques (e.g., Auto-WEKA, Lazy Paired Hyperparameter Tuning) to reduce computational burden on HPO while maintaining or improving performance is also suggested. Designing more advanced HPO algorithms that actively exploit the reshuffling effect is a promising avenue. Additionally, exploring combinations of reshuffling with existing strategies to counteract overfitting (e.g., LOOCVCV, extra selection sets, early stopping) could lead to further improvements. The impact of reshuffling on multi-class datasets and developing less naive implementations to address issues with highly sensitive losses like logloss are also areas for further study.",
        "experimental_code": "File Path: simulations/simulate.py\nContent:\nfrom functools import partial\n\nimport numpy as np\nimport torch\n\ndef simulate_gp(x: torch.Tensor, mu: callable, cov: callable) -> torch.Tensor:\n    \"\"\"\n    Simulate a Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param mu: Function to compute the mean of the Gaussian process.\n    :param cov: Function to compute the covariance matrix of the Gaussian process.\n\n    :return: A tensor of shape (n, d) representing the simulated Gaussian process.\n    \"\"\"\n    sigma_2 = 1e-5\n    n = x.shape[0]\n    d = x.shape[1]\n    mu_x = mu(x)\n    K_x = cov(x, x)\n    K_x += sigma_2 * torch.eye(n)\n    eigenvalues, eigenvectors = torch.linalg.eigh(K_x)\n    positive_eigenvalues = torch.clamp(eigenvalues, min=0)\n    sqrt_K_x = eigenvectors @ torch.diag(positive_eigenvalues.sqrt()) @ eigenvectors.T\n    z = torch.normal(0, 1, (n, d))\n    return mu_x + sqrt_K_x @ z\n\n\ndef mu_factory(x: torch.Tensor, alpha: float) -> torch.Tensor:\n    \"\"\"\n    The mean function of the Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param alpha: The alpha parameter of the mean function.\n\n    :return: A tensor of shape (n, 1) representing the mean of the Gaussian process at each point.\n    \"\"\"\n    values = alpha * (x - 0.5).pow(2)\n    return values\n\n\ndef cov_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Vectorized computation of the covariance matrix of the Gaussian process.\n\n    :param x1: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param x2: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param lengthscale: The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A tensor of shape (n, n) representing the covariance matrix of the Gaussian process.\n    \"\"\"\n    sq_dist = torch.sum((x1[:, None, :] - x2[None, :, :]) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K\n    return K\n\n\ndef kernel_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Computation of the covariance between two points.\n\n    :param x1: A tensor of shape (1, d) where d is the dimensionality.\n    :param x2: A tensor of shape (1, d) where d is the dimensionality.\n    :param lengthscale:  The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A scalar tensor representing the covariance between the two points.\n    \"\"\"\n    sq_dist = torch.sum((x1 - x2) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        if x1 == x2:\n            K = (tau**2) * K\n    return K\n\n\nif __name__ == \"__main__\":\n    import argparse\n    import os\n\n    import pandas as pd\n    from tqdm import tqdm\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--alpha\", type=float, default=1)\n    parser.add_argument(\"--lengthscale\", type=float, default=0.5)\n    args = parser.parse_args()\n\n    np.random.seed(42)\n    torch.manual_seed(42)\n\n    results_path = \"results\"\n    d = 1\n    alpha = args.alpha\n    lengthscale = args.lengthscale\n    n_replicates = 10000\n\n    x = torch.linspace(0, 1, 101).reshape(-1, d)\n    mu = partial(mu_factory, alpha=alpha)\n    kernel = partial(kernel_factory, lengthscale=lengthscale)\n    cov = partial(cov_factory, lengthscale=lengthscale)\n    taus = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n    taus.sort()\n    results = {tau: {\"y\": [], \"y_shuffled\": [], \"y_diff\": []} for tau in taus}\n    for tau in taus:\n        y_mu_y_list = []\n        y_mu_y_shuffled_list = []\n        y_diff_list = []\n        for _ in tqdm(range(n_replicates)):\n            y = simulate_gp(x, mu, cov)\n            y_shuffled = simulate_gp(x, mu, partial(cov, shuffled=True, tau=tau))\n            y_mu_y = mu(x[y.argmin(dim=0)])\n            y_mu_y_shuffled = mu(x[y_shuffled.argmin(dim=0)])\n            y_mu_y_list.append(y_mu_y)\n            y_mu_y_shuffled_list.append(y_mu_y_shuffled)\n            y_diff_list.append(y_mu_y - y_mu_y_shuffled)\n        results[tau][\"y\"] = torch.tensor(y_mu_y_list)\n        results[tau][\"y_shuffled\"] = torch.tensor(y_mu_y_shuffled_list)\n        results[tau][\"y_diff\"] = torch.tensor(y_diff_list)\n\n    results_df = pd.DataFrame(results[taus[0]])\n    results_df[\"replicate\"] = range(n_replicates)\n    results_df[\"tau\"] = taus[0]\n    for tau in taus[1:]:\n        results_df_tmp = pd.DataFrame(results[tau])\n        results_df_tmp[\"replicate\"] = range(n_replicates)\n        results_df_tmp[\"tau\"] = tau\n        results_df = pd.concat([results_df, results_df_tmp], axis=0)\n    results_df[\"alpha\"] = alpha\n    results_df[\"lengthscale\"] = lengthscale\n    results_df[\"scenario\"] = str(alpha) + \"_\" + str(lengthscale)\n    results_df = results_df.reset_index(drop=True)\n    results_df.to_csv(\n        os.path.join(\n            results_path,\n            f\"results_alpha_{alpha}_lengthscale_{lengthscale}.csv\",\n        ),\n        index=False,\n    )\nFile Path: analyze/result_analyzer.py\nContent:\n    def calculate_curvature(self) -> None:\n        \"\"\"\n        Fit a GP on observed values and calculate some curvature metrics at the empirical optimum.\n        \"\"\"\n        for metric in self.params[\"metrics\"]:\n            dat = self.results_raw[metric]\n            relevant_columns_valid = [\n                column for column in dat.columns if \"params_\" in column\n            ] + [\"valid\"]\n            dat_valid = dat.loc[:, relevant_columns_valid]\n            dat_valid.rename(columns={\"valid\": \"y\"}, inplace=True)\n            X = dat_valid.drop(columns=[\"y\"])\n            y = dat_valid[\"y\"].values.reshape(-1, 1)\n            X.rename(\n                columns={column: column.replace(\"params_\", \"\") for column in X.columns},\n                inplace=True,\n            )\n            if self.params[\"classifier\"] == \"logreg\":\n                classifier = LogReg(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            elif self.params[\"classifier\"] == \"funnel_mlp\":\n                classifier = FunnelMLP(self.params[\"seed\"])\n                n_train_samples = int(\n                    0.8 * self.params[\"train_valid_size\"]\n                )  # Holdout 80/20 or 5-fold CV variants\n                space = classifier.get_hebo_search_space(\n                    n_train_samples=n_train_samples\n                )\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub)\n                    for name in space.paras\n                    if not space.paras[name].is_categorical\n                ]\n            elif self.params[\"classifier\"] == \"xgboost\":\n                classifier = XGBoost(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            elif self.params[\"classifier\"] == \"catboost\":\n                classifier = CatBoost(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            # https://github.com/huawei-noah/HEBO/blob/c1c7d72b996a7d11eb2b86e25f21a174b0cc7bd4/HEBO/hebo/optimizers/hebo.py#L117\n            X, Xe = space.transform(X)\n            model_config = {\n                \"lr\": 0.01,\n                \"num_epochs\": 100,\n                \"verbose\": False,\n                \"noise_lb\": 8e-4,\n                \"pred_likeli\": False,\n            }\n            if space.num_categorical > 0:\n                model_config[\"num_uniqs\"] = [\n                    len(space.paras[name].categories) for name in space.enum_names\n                ]\n            # try:\n            #    if y.min() <= 0:\n            #        y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))\n            #    else:\n            #        y = torch.FloatTensor(power_transform(y / y.std(), method='box-cox'))\n            #        if y.std() < 0.5:\n            #            y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))\n            #    if y.std() < 0.5:\n            #        raise RuntimeError('Power transformation failed')\n            #    model = get_model(\"gp\", space.num_numeric, space.num_categorical, 1, **model_config)\n            #    model.fit(X, Xe, y)\n            # except:\n            y = torch.FloatTensor(y).clone()\n            model = get_model(\n                \"gp\", space.num_numeric, space.num_categorical, 1, **model_config\n            )\n            model.fit(X, Xe, y)\n\n            empirical_argmin = model.predict(X, Xe)[0].argmin()\n            X_argmin = X[empirical_argmin, :].unsqueeze(0)\n            Xe_argmin = Xe[empirical_argmin, :].unsqueeze(0)\n\n            def posterior_mean_wrapper(x, model, Xe_argmin):\n                x_tensor = torch.FloatTensor(x).unsqueeze(0).requires_grad_(True)\n                return model.predict(x_tensor, Xe_argmin)[0][0, 0].detach().numpy()\n\n            x0 = X[empirical_argmin, :].numpy()\n            result = opt.minimize(\n                posterior_mean_wrapper,\n                x0,\n                args=(model, Xe_argmin),\n                bounds=bounds,\n                method=\"Nelder-Mead\",\n            )\n\n            x_optimal = result.x\n            hessian_function = numdifftools.Hessian(posterior_mean_wrapper)\n            hessian_optimal = hessian_function(x_optimal, model, Xe_argmin)\n\n            def make_psd(matrix):\n                eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n                already_is_psd = np.all(eigenvalues >= 0)\n                eigenvalues[eigenvalues < 0] = 0\n                return (\n                    already_is_psd,\n                    eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T,\n                )\n\n            already_is_psd, hessian_optimal = make_psd(hessian_optimal)\n\n            det_hessian = np.linalg.det(hessian_optimal)\n            trace_hessian = np.trace(hessian_optimal)\n            eigenvalues_hessian = np.linalg.eigvals(hessian_optimal)\n            smallest_eigenvalue_hessian = np.min(eigenvalues_hessian)\n            biggest_eigenvalue_hessian = np.max(eigenvalues_hessian)\n\n            curvature_pd = pd.DataFrame(\n                {\n                    \"det_hessian\": [det_hessian],\n                    \"trace_hessian\": [trace_hessian],\n                    \"smallest_eigenvalue_hessian\": [smallest_eigenvalue_hessian],\n                    \"biggest_eigenvalue_hessian\": [biggest_eigenvalue_hessian],\n                    \"already_is_psd\": [already_is_psd],\n                    \"gp_noise\": [\n                        model.noise.item()\n                    ],  # homoscedastic observation variance (noise) from the GP\n                    \"seed\": [self.seed],\n                    \"classifier\": [self.params[\"classifier\"]],\n                    \"data_id\": [self.params[\"data_id\"]],\n                    \"train_valid_size\": [self.params[\"train_valid_size\"]],\n                    \"resampling\": [self.params[\"resampling\"]],\n                    \"metric\": [metric],\n                }\n            )\n            self.curvature.update({metric: curvature_pd})\nFile Path: reshufflebench/learner/learner_random_cv.py\nContent:\nclass LearnerRandomCV(LearnerRandom):\n    # ... (init and prepare_resampling methods)\n\n    def objective(self, trial: Trial) -> float:\n        \"\"\"\n        Objective function for the optimization.\n        \"\"\"\n        # construct classifier pipeline\n        self.classifier.construct_pipeline(\n            trial,\n            refit=False,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n            n_train_samples=self.train_size,\n        )\n\n        if self.reshuffle:\n            self.cv = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits = []\n            for cv in self.cv:\n                self.cv_splits.append(\n                    list(cv.split(self.x_valid_train, self.y_valid_train))\n                )\n\n            # partition add_valid_use data into n_splits folds and repeat it n_repeats times\n            self.cv_add_valid = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits_add_valid = []\n            for cv in self.cv_add_valid:\n                self.cv_splits_add_valid.append(\n                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))\n                )\n\n        cv_splits_hist_train_tmp = []\n        cv_splits_hist_valid_tmp = []\n        cv_splits_add_valid_hist_valid_tmp = []\n        y_train_hist_tmp = []\n        y_valid_hist_tmp = []\n        y_add_valid_hist_tmp = []\n\n        # for each repeat and each fold fit the classifier and predict on the train, valid, test and add_valid set and compute the metrics\n        predictions = dict(\n            [(data, []) for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]]\n        )\n        predictions_proba = dict(\n            [(data, []) for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]]\n        )\n        cv_metrics_train = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_valid = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_add_valid = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_test = dict([(metric, []) for metric in self.metrics])\n\n        for repeat in range(self.n_repeats):\n            for fold in range(self.n_splits):\n                train_index, valid_index = self.cv_splits[repeat][fold]\n                cv_splits_hist_train_tmp.append(train_index)\n                cv_splits_hist_valid_tmp.append(valid_index)\n                x_train = self.x_valid_train.iloc[train_index]\n                x_valid = self.x_valid_train.iloc[valid_index]\n                y_train = self.y_valid_train[train_index]\n                y_valid = self.y_valid_train[valid_index]\n                y_train_hist_tmp.append(y_train)\n                y_valid_hist_tmp.append(y_valid)\n\n                add_valid_index = self.cv_splits_add_valid[repeat][fold][1]\n                cv_splits_add_valid_hist_valid_tmp.append(\n                    np.concatenate([valid_index, add_valid_index])\n                )\n                x_add_valid, y_add_valid = construct_x_and_y_add_valid(\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    x_add_valid=self.x_add_valid_use.iloc[add_valid_index],\n                    y_add_valid=self.y_add_valid_use[add_valid_index],\n                )\n                y_add_valid_hist_tmp.append(y_add_valid)\n\n                self.classifier.fit(\n                    trial=trial,\n                    x_train=x_train,\n                    y_train=y_train,\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    cat_features=self.cat_features,\n                )\n                for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]:\n                    if data == \"x_test\":\n                        predictions[data].append(\n                            self.classifier.predict(getattr(self, data))\n                        )\n                        predictions_proba[data].append(\n                            self.classifier.predict_proba(getattr(self, data))\n                        )\n                    else:\n                        predictions[data].append(self.classifier.predict(eval(data)))\n                        predictions_proba[data].append(\n                            self.classifier.predict_proba(eval(data))\n                        )\n                for metric in self.metrics:\n                    cv_metrics_train[metric].append(\n                        compute_metric(\n                            y_train,\n                            y_pred=predictions[\"x_train\"][-1],\n                            y_pred_proba=predictions_proba[\"x_train\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_valid[metric].append(\n                        compute_metric(\n                            y_valid,\n                            y_pred=predictions[\"x_valid\"][-1],\n                            y_pred_proba=predictions_proba[\"x_valid\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_add_valid[metric].append(\n                        compute_metric(\n                            y_add_valid,\n                            y_pred=predictions[\"x_add_valid\"][-1],\n                            y_pred_proba=predictions_proba[\"x_add_valid\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_test[metric].append(\n                        compute_metric(\n                            self.y_test,\n                            y_pred=predictions[\"x_test\"][-1],\n                            y_pred_proba=predictions_proba[\"x_test\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n\n        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)\n        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)\n        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)\n        self.y_train_hist.append(y_train_hist_tmp)\n        self.y_valid_hist.append(y_valid_hist_tmp)\n        self.y_add_valid_hist.append(y_add_valid_hist_tmp)\n        self.y_pred_train_proba_hist.append(predictions_proba[\"x_train\"])\n        self.y_pred_valid_proba_hist.append(predictions_proba[\"x_valid\"])\n        self.y_pred_add_valid_proba_hist.append(predictions_proba[\"x_add_valid\"])\n        self.y_pred_test_proba_hist.append(predictions_proba[\"x_test\"])\n\n        for cv_metric in self.cv_metric_to_metric.keys():\n            metric = self.cv_metric_to_metric[cv_metric]\n            trial.set_user_attr(\n                f\"{cv_metric}_train\",\n                json.dumps(cv_metrics_train[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_valid\",\n                json.dumps(cv_metrics_valid[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_add_valid\",\n                json.dumps(cv_metrics_add_valid[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_test\",\n                json.dumps(cv_metrics_test[metric], cls=NumpyArrayEncoder),\n            )\n\n        # compute the mean of the metrics over the folds and repeats\n        metrics_train = {}\n        metrics_valid = {}\n        metrics_add_valid = {}\n        metrics_test = {}\n\n        for metric in self.metrics:\n            metrics_train[metric] = np.mean(cv_metrics_train[metric])\n            metrics_valid[metric] = np.mean(cv_metrics_valid[metric])\n            metrics_add_valid[metric] = np.mean(cv_metrics_add_valid[metric])\n            metrics_test[metric] = np.mean(cv_metrics_test[metric])\n            trial.set_user_attr(f\"{metric}_train\", metrics_train[metric])\n            trial.set_user_attr(f\"{metric}_valid\", metrics_valid[metric])\n            trial.set_user_attr(f\"{metric}_add_valid\", metrics_add_valid[metric])\n            trial.set_user_attr(f\"{metric}_test\", metrics_test[metric])\n\n        # compute the metrics on the test set also in ensemble style\n        metrics_test_ensemble = {}\n        predictions_proba_test_ensemble = np.mean(predictions_proba[\"x_test\"], axis=0)\n        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)\n        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums\n        check_y_predict_proba(predictions_proba_test_ensemble)\n        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)\n        for metric in self.metrics:\n            metrics_test_ensemble[metric] = compute_metric(\n                self.y_test,\n                y_pred=predictions_test_ensemble,\n                y_pred_proba=predictions_proba_test_ensemble,\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(\n                f\"{metric}_test_ensemble\", metrics_test_ensemble[metric]\n            )\n\n        if self.bootstrap_test:\n            # bootstrap the ensemble style test performance\n            for metric in self.metrics:\n                metric_test_bootstrap = bootstrap_test_performance(\n                    y_test=self.y_test,\n                    y_pred=predictions_test_ensemble,\n                    y_pred_proba=predictions_proba_test_ensemble,\n                    metric=metric,\n                    labels=self.labels,\n                    multiclass=self.multiclass,\n                    seed=self.seed,\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_ensemble_bootstrap\", metric_test_bootstrap\n                )\n                average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(\n                    metric_test_bootstrap\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_ensemble_bootstrap_average\",\n                    average_metric_test_bootstrap,\n                )\n\n        # refit on the train_valid set\n        self.classifier.construct_pipeline(\n            trial,\n            refit=True,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n        )\n        self.classifier.fit(\n            trial=trial,\n            x_train=self.x_valid_train,\n            y_train=self.y_valid_train,\n            cat_features=self.cat_features,\n        )\n\n        # predict on the train_valid set and compute the metrics\n        predictions[\"x_valid_train\"] = self.classifier.predict(self.x_valid_train)\n        predictions_proba[\"x_valid_train\"] = self.classifier.predict_proba(\n            self.x_valid_train\n        )\n        metrics_valid_train = {}\n        for metric in self.metrics:\n            metrics_valid_train[metric] = compute_metric(\n                self.y_valid_train,\n                y_pred=predictions[\"x_valid_train\"],\n                y_pred_proba=predictions_proba[\"x_valid_train\"],\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(f\"{metric}_valid_train\", metrics_valid_train[metric])\n\n        self.y_pred_valid_train_proba_hist.append(predictions_proba[\"x_valid_train\"])\n\n        # predict on the test set and compute the metrics\n        predictions[\"x_test_retrained\"] = self.classifier.predict(self.x_test)\n        predictions_proba[\"x_test_retrained\"] = self.classifier.predict_proba(\n            self.x_test\n        )\n        metrics_test_retrained = {}\n        for metric in self.metrics:\n            metrics_test_retrained[metric] = compute_metric(\n                self.y_test,\n                y_pred=predictions[\"x_test_retrained\"],\n                y_pred_proba=predictions_proba[\"x_test_retrained\"],\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(\n                f\"{metric}_test_retrained\", metrics_test_retrained[metric]\n            )\n\n        if self.bootstrap_test:\n            # bootstrap the retrained test performance\n            for metric in self.metrics:\n                metric_test_retrained_bootstrap = bootstrap_test_performance(\n                    y_test=self.y_test,\n                    y_pred=predictions[\"x_test_retrained\"],\n                    y_pred_proba=predictions_proba[\"x_test_retrained\"],\n                    metric=metric,\n                    labels=self.labels,\n                    multiclass=self.multiclass,\n                    seed=self.seed,\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_retrained_bootstrap\",\n                    metric_test_retrained_bootstrap,\n                )\n                average_metric_test_retrained_bootstrap = sum(\n                    metric_test_retrained_bootstrap\n                ) / len(metric_test_retrained_bootstrap)\n                trial.set_user_attr(\n                    f\"{metric}_test_retrained_bootstrap_average\",\n                    average_metric_test_retrained_bootstrap,\n                )\n\n        self.y_pred_test_proba_retrained_hist.append(\n            predictions_proba[\"x_test_retrained\"]\n        )\n\n        self.classifier.reset()\n\n        # return the validation accuracy (mean over folds and repeats)\n        return metrics_valid[\"accuracy\"]\nFile Path: main.py\nContent:\nif __name__ == \"__main__\":\n    import argparse\n\n    from reshufflebench.algorithms import (\n        CatBoost,\n        Featureless,\n        FunnelMLP,\n        LogReg,\n        TabPFN,\n        XGBoost,\n        XGBoostLarge,\n    )\n    from reshufflebench.learner import (\n        LearnerHeboCV,\n        LearnerHeboHoldout,\n        LearnerHeboRepeatedHoldout,\n        LearnerRandomCV,\n        LearnerRandomHoldout,\n        LearnerRandomRepeatedHoldout,\n        LearnerSmacCV,\n        LearnerSmacHoldout,\n        LearnerSmacRepeatedHoldout,\n    )\n    from reshufflebench.utils import str2bool\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--classifier\",\n        type=str,\n        default=\"catboost\",\n        choices=[\n            \"catboost\",\n            \"funnel_mlp\",\n            \"logreg\",\n            \"tabpfn\",\n            \"xgboost\",\n            \"xgboost_large\",\n            \"featureless\",\n        ],\n    )\n    parser.add_argument(\"--default\", type=str2bool, default=False)\n    parser.add_argument(\n        \"--optimizer\",\n        type=str,\n        default=\"random\",\n        choices=[\"random\", \"hebo\", \"smac\"],\n    )\n    parser.add_argument(\n        \"--data_id\",\n        type=int,\n        default=11111,\n        choices=[\n            23517,\n            1169,\n            41147,\n            4135,\n            1461,\n            1590,\n            41150,\n            41162,\n            42733,\n            42742,\n            99999,\n            11111,\n        ],\n    )\n    parser.add_argument(\n        \"--valid_type\",\n        type=str,\n        default=\"holdout\",\n        choices=[\"cv\", \"holdout\", \"repeatedholdout\"],\n    )\n    parser.add_argument(\n        \"--train_valid_size\",\n        type=int,\n        default=500,\n        choices=[500, 1000, 5000],\n    )\n    parser.add_argument(\"--reshuffle\", type=str2bool, default=True)\n    parser.add_argument(\"--n_splits\", type=int, default=5, choices=[5])\n    # n_repeats = 1 or 5 for cv, 1 for holdout and 5 for repeatedholdout\n    parser.add_argument(\"--n_repeats\", type=int, default=1, choices=[1, 5])\n    parser.add_argument(\"--valid_frac\", type=float, default=0.2, choices=[0.2])\n    parser.add_argument(\"--test_size\", type=int, default=5000)\n    parser.add_argument(\"--add_valid_size\", type=int, default=5000)\n    parser.add_argument(\"--n_trials\", type=int, default=10)\n    parser.add_argument(\"--seed\", type=int, default=42)\n\n    args = parser.parse_args()\n\n    classifiers = {\n        \"catboost\": CatBoost(seed=args.seed, default=args.default),\n        \"funnel_mlp\": FunnelMLP(seed=args.seed, default=args.default),\n        \"logreg\": LogReg(seed=args.seed, default=args.default),\n        \"tabpfn\": TabPFN(seed=args.seed),\n        \"xgboost\": XGBoost(seed=args.seed, default=args.default),\n        \"xgboost_large\": XGBoostLarge(seed=args.seed, default=args.default),\n        \"featureless\": Featureless(seed=args.seed),\n    }\n    classifier = classifiers[args.classifier]\n\n    if args.n_trials > 500:\n        raise ValueError(\n            \"n_trials must be <= 500 - or you must adjust seeds in codebase\"\n        )\n\n    if args.optimizer == \"random\":\n        if args.valid_type == \"cv\":\n            learner = LearnerRandomCV(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerRandomHoldout(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerRandomRepeatedHoldout(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    elif args.optimizer == \"hebo\":\n        if args.valid_type == \"cv\":\n            learner = LearnerHeboCV(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerHeboHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerHeboRepeatedHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    elif args.optimizer == \"smac\":\n        if args.valid_type == \"cv\":\n            learner = LearnerSmacCV(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerSmacHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerSmacRepeatedHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    else:\n        raise ValueError(\"Invalid optimizer\")\n    learner.run()\n",
        "experimental_info": "Simulation Study:\n- `d` (dimensionality of each point): 1\n- `alpha` (mean function parameter for quadratic loss surface): [0.5, 1, 5, 10]\n- `lengthscale` (covariance function parameter for squared exponential kernel): [0.1, 0.5, 1, 5]\n- `n_replicates`: 10000\n- `taus` (parameter for shuffled covariance function, representing reshuffling extent/noise correlation): [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\nBenchmark Experiments:\n- Optimizers: Random Search, HEBO (Bayesian Optimization), SMAC3 (Bayesian Optimization)\n- Classifiers (methods evaluated): CatBoost, FunnelMLP, LogReg (Logistic Regression), TabPFN, XGBoost, XGBoostLarge, Featureless (baseline)\n- Resampling Strategies (`valid_type`):\n    - Holdout: `valid_type=\"holdout\"`, `valid_frac=0.2`\n    - M-fold CV: `valid_type=\"cv\"`, `n_splits=5`, `n_repeats=1`\n    - M-fold Holdout (repeated holdout / Monte Carlo CV): `valid_type=\"repeatedholdout\"`, `valid_frac=0.2`, `n_repeats=5`\n    - 5x 5-fold CV: `valid_type=\"cv\"`, `n_splits=5`, `n_repeats=5`\n- Reshuffling (`reshuffle`): True, False\n- Number of trials (`n_trials`):\n    - 250 for HEBO and SMAC\n    - 500 for Random Search (unless `classifier` is 'tabpfn' or 'default', then 1 trial)\n- Seeds: 42 to 51 (range(42, 52))\n- Data IDs (OpenML datasets): [23517, 1169, 41147, 4135, 1461, 1590, 41150, 41162, 42733, 42742] (plus synthetic 99999, 11111 for testing)\n- Training/Validation set sizes (`train_valid_size`): [500, 1000, 5000]\n- Test set size (`test_size`): 5000\n- Additional validation set size (`add_valid_size`): 5000\n- Metrics:\n    - For Random Search: Accuracy, Balanced Accuracy, Logloss, AUC (Area Under the Curve)\n    - For HEBO and SMAC: AUC\n"
      }
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize ``adaptive''\nhyperparameter optimization methods such as Gaussian process-based\noptimization, which select the next candidate based on information gathered\nfrom previous outputs. This substantial contrast between private and\nnon-private hyperparameter optimization underscores a critical concern. In our\npaper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private\nhyperparameter optimization, aiming to bridge the gap between private and\nnon-private hyperparameter optimization. To accomplish this, we provide a\ncomprehensive differential privacy analysis of our framework. Furthermore, we\nempirically demonstrate the effectiveness of DP-HyPO on a diverse set of\nreal-world datasets.",
      "full_text": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wang∗ Sheng Gao† Huanyu Zhang‡ Weijie J. Su§ Milan Shen¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize “adaptive” hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for “adaptive” private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. ∗Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. †Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. ‡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. §Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. ¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedℓ2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forε >0, and0 ≤ δ <1, we consider a (randomized) algorithmM : Zn → Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (ε, δ)-DP if for any neighboring dataset D, D′ ∈ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) ∈ E] ⩽ eε · P \u0002 M \u0000 D′\u0001 ∈ E \u0003 + δ. Here, ε and δ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms Mλ : Zn → Y, whereλ ∈ Λ represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setΛ may be any infinite set, e.g., the cross product of the learning rateη and clipping normR in DP-SGD. We require that the set Λ is a measure space with an associated measureµ. Common choices forµ include the counting measure or Lebesgue measure. We make a mild assumption thatµ(Λ) < ∞. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their “performance measure”, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterλ, the return value ofMλ is (x, q) ∼ Mλ(D), wherex represents the combination of the model parameters and the hyperparameter λ, andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3ε, 0)-DP as long as each training run of a hyperparameter was (ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainΛ. In practice, this setting is unrealistic since we want to “adapt” the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionπ at each iteration that reflects accumulated information. Let Q(D, π) be the procedure that randomly draws a hyperparameterλ from the distribution1 π ∈ D(Λ) , and then returns the output fromMλ(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asπ(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset D′ is denoted asπ′(j). We now present the DP-HyPO framework, denoted asA(D, π(0), T , C, c), in Framework 1. The algorithm takes a prior distributionπ(0) ∈ D(Λ) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, π(0), T , C, c) Initialize π(0), a prior distribution overΛ. Initialize the result setA = {} Draw T ∼ T for j = 0 to T − 1 do (x, q) ∼ Q(D, π(j)) A = A ∪ {(x, q)} Update π(j+1) based onA according to any adaptive algorithm such that for allλ ∈ Λ, c ≤ π(j+1)(λ) π(0)(λ) ≤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forπ(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c ≤ π(j+1)(λ) π(0)(λ) ≤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Λ) represents the space of probability densities onΛ. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of Rényi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(ε, δ)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(θ, γ) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(θ, γ) for simplicity. We also assume for simplicity that the prior distributionπ(0) is a uniform distribution overΛ. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞), γ ∈ (0, 1), and 0 < c≤ C. Suppose for allMλ : Zn → Yover λ ∈ Λ, the base algorithms satisfy(α, ε)-RDP and(ˆα, ˆε)-RDP for someε, ˆε ≥ 0, α∈ (1, ∞), and ˆα ∈ [1, ∞). Then the DP-HyPO algorithmA(D, π(0), NegBin(θ, γ), C, c) satisfies (α, ε′)-RDP where ε′ = ε + (1 +θ) · \u0012 1 − 1 ˆα \u0013 ˆε + \u0012 α α − 1 + 1 +θ \u0013 log C c + (1 + θ) · log(1/γ) ˆα + log E[T] α − 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the Rényi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the Rényi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(ε, 0)-DP is equivalent to (∞, ε)-RDP [29]. When bothα and ˆα tend towards infinity, we easily obtain the following theorem in terms of(ε, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞) and γ ∈ (0, 1). If all the base algorithmsMλ satisfies (ε, 0)-DP, then the DP-HyPO algorithm A(D, π(0), NegBin(θ, γ), C, c) satisfies \u0000 (2 + θ) \u0000 ε + log C c \u0001 , 0 \u0001 -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerε improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionπ. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, θ, γ,Λ) Let π = Unif({1, ...,|Λ|}), andA = {} Draw T ∼ NegBin(θ, γ) for j = 0 to T − 1 do (x, q) ∼ Q(D, π) A = A ∪ {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Λ is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setθ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityπ(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingπ(j) based on the history. It is crucial to note that this distributionπ(j) can be significantly different from the distribution π′(j) if we were given a different input datasetD′. Therefore, we require the probability mass/density function to satisfy c µ(Λ) ≤ π(j)(λ) ≤ C µ(Λ) for allλ ∈ Λ to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityπ(j). When sampling from the spaceΛ, we perform a projection from π(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f ∈ ΛR+ : ess supf ≤ C µ(Λ), ess inff ≥ c µ(Λ), R α∈Λ f(α)dα = 1}. For such a space to be non-empty, we require thatc ≤ 1 ≤ C, where µ is the measure onΛ. This condition is well-defined as we assumeµ(Λ) < ∞. To privatizeπ(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f ∥f − π(j)∥2, s.t. f ∈ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(π(j)). Theoretically, problem(3.1) allows the hyperparameter space Λ to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeΛ to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, θ, γ, τ, β,Λ, C, c) Initialize π(0) = Unif(Λ), andA = {} Draw T ∼ NegBin(θ, γ) for t = 0 to T − 1 do Truncate the density of currentπ(t) to be bounded into the range of[c, C] by projecting to SC,c. ˜π(t) = PSC,c(π(t)). Sample (x, q) ∼ Q(D, ˜π(j)), and updateA = A ∪ {(x, q)} Update mean estimation and variance estimation of the Gaussian processµλ, σ2 λ, and get the score assλ = µλ + τσλ. Update true (untruncated) posteriorπ(t+1) with softmax, byπ(t+1)(λ) = exp(β·sλ)R λ′∈Λ exp(β·s′ λ). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a “score” to each hyperparameter λ, known as the estimated upper confidence bound (UCB). The weight factorτ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterβ as the inverse temperature. A higher value ofβ signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as “GP”) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9“Uniform”, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetε = 153 and δ = 1e − 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The ε values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anε as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with ε = 15, δ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with ε = 12, δ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateη and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (µλ) and standard error (σλ) of accuracy for each hyperparameter combinationλ. When the algorithm (GP or Uniform) visits a specificλ, our oracle returns a noisy scoreq(λ) drawn from a normal distribution of N(µλ, σλ). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofε = 12 and δ = 1e − 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofγ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofγ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. [5] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre Csiszár and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474–1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381–390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3–33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785–1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507–523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376–1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358–1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367–377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765–6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58–65. PMLR, 2016. [29] Ilya Mironov. Rényi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263–275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806–7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised Lectures, pages 63–71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148–175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347–450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208–12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define Rényi divergence as follows. Definition A.1(Rényi Divergences). Let P and Q be probability distributions on a common space Ω. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE ⊂ Ω, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(P∥Q) := E X←P \u0014 log \u0012P(X) Q(X) \u0013\u0015 = Z Ω P(x) log \u0012P(x) Q(x) \u0013 dx. The max divergence fromP to Q is defined as D∞(P∥Q) := sup \u001a log \u0012P(E) Q(E) \u0013 : P(E) > 0 \u001b . For α ∈ (1, ∞), the Rényi divergence fromP to Q of orderα is defined as Dα(P∥Q) := 1 α − 1 log   E X←P \"\u0012P(X) Q(X) \u0013α−1#! = 1 α − 1 log \u0012 E X←Q \u0014\u0012P(X) Q(X) \u0013α\u0015\u0013 = 1 α − 1 log \u0012Z Q P(x)αQ(x)1−αdx \u0013 . We now present the definition of Rényi DP (RDP) in [29]. Definition A.2(Rényi Differential Privacy). A randomized algorithmM : Xn → Yis (α, ε)-Rényi differentially private if, for all neighbouring pairs of inputsD, D′ ∈ Xn, Dα (M(x)∥M (x′)) ≤ ε. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 ≤ j ≤ T, and neighboring datasetD and D′, we define the following notations for anyy = (x, q) ∈ Y, the totally ordered range set. Pj(y) = P˜y∼Q(D,π(j))(˜y = y) and P′ j(y) = P˜y∼Q(D′,π′(j))(˜y = y) Pj(≤ y) = P˜y∼Q(D,π(j))(˜y ≤ y) and P′ j(≤ y) = P˜y∼Q(D′,π′(j))(˜y ≤ y) Pj(< y) = P˜y∼Q(D,π(j))(˜y < y) and P′ j(< y) = P˜y∼Q(D′,π′(j))(˜y < y). By these definitions, we havePj(≤ y) = Pj(< y) + Pj(y), andP′ j(≤ y) = P′ j(< y) + P′ j(y). And additionally, we have Pj(y) P′ j(y) = R λ∈Λ P(Mλ(D) = y)π(j)(λ)dλR λ∈Λ P(Mλ(D′) = y)π′(j)(λ)dλ ≤ sup λ∈Λ P(Mλ(D) = y)π(j)(λ) P(Mλ(D′) = y)π′(j)(λ) ≤ C c · sup λ∈Λ P(Mλ(D) = y) P(Mλ(D′) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatπ(j) has bounded density betweenc and C. Similarly, we have Pj(≤ y) P′ j(≤ y) ≤ C c · sup λ∈Λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y), (A.2) and Pj(< y) P′ j(< y) ≤ C c · sup λ∈Λ P(Mλ(D) < y) P(Mλ(D′) < y). (A.3) Note thatD and D′ are neighboring datasets, andMλ satisfies some DP guarantees. So the ratio P(Mλ(D)∈E) P(Mλ(D′)∈E) for any eventE can be bounded. For simplicity, we define the inner product of a distribution π with the vector M(D) = (P(Mλ(D) = y) : λ ∈ Λ) as π · M(D) := Z λ∈Λ P(Mλ(D) = y)π(λ)dλ. (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f ∈ ΛR+ : ess supf ≤ C, ess inff ≥ c, R α∈Λ f(α)dα = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) = y)π(j)(λ)dλ = π+ · M(D), (A.5) where π+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define P′−(y) for D′ as given by P′−(y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) = y) · π′(j)(λ)dλ = π′− · M. (A.6) Similarly, we can defineP′+(y) and P−(y) accordingly. From the definition, we know that P−(y) ≤ Pj(y) ≤ P+(y) and P′−(y) ≤ P′ j(y) ≤ P′+(y). (A.7) We also have P+(y) P′−(y) = π∗ · M(D) π′− · M(D′) ≤ sup λ P(Mλ(D) = y) P(Mλ(D′) = y) · C c . (A.8) It is similar to define P+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) < y) 18P−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) < y). Following the exact same proof, we have P−(≤ y) ≤ Pj(≤ y) ≤ P+(≤ y) and P′−(≤ y) ≤ P′ j(≤ y) ≤ P′+(≤ y) (A.9) P−(< y) ≤ Pj(< y) ≤ P+(< y) and P′−(< y) ≤ P′ j(< y) ≤ P′+(< y) (A.10) P+(≤ y) P′−(≤ y) ≤ sup λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y) · C c and P+(< y) P′−(< y) ≤ sup λ P(Mλ(D) < y) P(Mλ(D′) < y) · C c . (A.11) It is also straightforward to verify from the definition that P+(≤ y) = P+(< y) + P+(y) and P′+(≤ y) = P′+(< y) + P′+(y) (A.12) P+ − (≤ y) = P−(< y) + P−(y) and P′−(≤ y) = P′−(< y) + P′−(y). (A.13) Lemma A.3.Suppose ifaλ, bλ are non-negative andcλ, c′ λ are positive for allλ. Then we have P λ aλcλP λ bλc′ λ ≤ P λ aλP λ bλ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X λ aλcλ X λ′ b′ λ ≤ X λ aλ X λ′ b′ λc′ λ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. (A.14) For each termaλb′ λ, its coefficient on the left hand side of(A.14) is cλ, but its coefficient on the right hand side of(A.14) is c′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f. Since we always havec′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f ≥ cλ, andaλb′ λ ≥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The Rényi divergence betweenP+ and P− is be bounded as follows: Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 Proof of Lemma A.4.We write that e(α−1)Dα(P+∥P′−) = X y∈Y P+(y)α · P′−(y)1−α = X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 (A.15) Here, π+ and π′− are defined in(A.5) and (A.6), so they are essentiallyπ+ y and π′− y as they depend on the value ofy. Therefore, we need to “remove” this dependence ony to leverage the RDP guarantees for each base algorithmMλ. We accomplish this task by bridging viaπ, the uniform 19density onΛ (that isπ(λ) = π(λ′) for anyλ, λ′ ∈ Λ). Specifically, we defineaλ = π(λ)P(Mλ(D) = y), bλ = π(λ)P(Mλ(D′) = y), cλ = π+ y (λ) π(λ) , andc′ λ = π′− y (λ) π(λ) . We see that sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)/π(λ) π′−y (λ′)/π(λ′) \f\f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)) π′−y (λ′) \f\f\f\f\f ≤ C/c, (A.16) since π is the uniform, andπ+ y and π′− y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 = X y∈Y \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 \u0010P λ π(λ)P(Mλ(D′) = y) · π′−(λ) π(λ) \u0011α−1 = X y∈Y (P λ aλ · cλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ · c′ λ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ)α−1 = X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ · cλ) (P λ bλ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ) · supλ cλ (P λ bλ)α−1 ≤ X y∈Y \u0012C c \u0013α−1 (P λ aλ)α−1 (P λ aλ) · \u0000C c \u0001 (P λ bλ)α−1 = X y∈Y \u0012C c \u0013α · (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 The first inequality is due to Lemma A.3, the second inequality is becauseaλ are non-negative, and the last inequality is because of(A.16) and the fact that bothπ+(λ) and π(λ) are defined inSC,c, and thus their ratio is upper bounded byC c for anyλ. Now we only need to prove that for any fixed distributionπ that doesn’t depend on valuey, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ sup λ∈Λ e(α−1)Dα(Mλ(D)∥Mλ(D′)). (A.17) With this result, we immediately know the result holds for uniform distributionπ as a special case. To prove this result, we first observe that the functionf(u, v) = uαv1−α is a convex function. This 20is because the Hessian off is \u0012α(α − 1)uα−2v1−α −α(α − 1)uα−1v−α −α(α − 1)uα−1v−α α(α − 1)uαv−α−1 \u0013 , which is easy to see to be positive semi-definite. And now, consider any distributionπ, denote u(λ) = P(Mλ(D) = y) and v(λ) = P(Mλ(D′) = y) by Jensen’s inequality, we have f( X λ π(λ)u(λ), X λ π(λ)v(λ)) ≤ X λ π(λ)f(u(λ), v(λ)). By adding the summation overy on both side of the above inequality, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ X y∈Y X λ π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = X λ X y∈Y π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 ≤ sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 . The first equality is due to Fubini’s theorem. And the second inequality is straight forward as one observe π(λ) only depends onλ. This concludes the proof as we know that e(α−1)Dα(P+∥P′−) ≤ \u0012C c \u0013α sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′) or equivalently, Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix α >1. LetT be a random variable supported onN≥0. Letf : [0, 1] → R be the probability generating function ofK, that is,f(x) = P∞ k=0 P[T = k]xk. Let Mλ and M′ λ be the base algorithm forλ ∈ Λ on Y on D and D′ respectively. Define A1 := A(D, π(0), T , C, c), andA2 := A(D′, π(0), T , C, c). Then Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 , where applying the same postprocessing to the bounding probabilitiesP+ and P′− gives probabilitiesq and q′ respectively. This means that, there exist a function setg : Y →[0, 1] such thatq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = ∞X k=1 P(T = k)[ kY j=1 Pj(≤ y) − kY i=1 Pj(< y)] = ∞X k=1 P(T = k)[ kX i=1 Pi(y) i−1Y j=1 Pj(< y) · kY j=i+1 Pj(≤ y)] ≤ ∞X k=1 P(T = k)[ kX i=1 P+(y) i−1Y j=1 P+(< y) · kY j=i+1 P+(≤ y)] = ∞X k=1 P(T = k)[ kX i=1 P+(y) · P+(< y)i−1 · P+(≤ y)k−i] = ∞X k=1 P(T = k)[P+(≤ y)k − P+(< y)k] = f(P+(≤ y)) − f(P+(< y)) = P+(y) · E X←Uniform([P+(<y),P+(≤y)]) [f′(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) ≥ ∞X k=1 P(T = k)[P′−(≤ y)k − P′−(< y)k] = P′−(y) · E X←Uniform([P′−(<y),P′−(≤y)]) [f′(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(α−1)Dα(A1∥A2) = X y∈Y A1(y)α · A2(y)1−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] \u0002 f′(X) \u0003α · E X′←[P′−(<y),P′−(≤y)] \u0002 f′ \u0000 X′\u0001\u00031−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′)) · max y∈Y E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any α ∈ R, the functionh : (0, ∞)2 → (0, ∞) given byh(u, v) = uα · v1−α is convex. Therefore, E[U]αE[V ]1−α = h(E[(U, V)]) ≤ E[h(U, V)] = E \u0002 Uα · V 1−α\u0003 all positive random variables(U, V). Note that X and X′ are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatX−P+(<y) P+(y) = X′−P′−(<y) P′−(y) . In particular, this implies that, for eachy ∈ Y, there exists somet ∈ [0, 1] such that E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ f′(P+(< y)+t·P+(y))α ·f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α Therefore, we have Dα (A1∥A2) ≤sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log  max y∈Y t∈[0,1] f′(P+(< y) + t · P+(y))α · f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α  . To prove the result, we simply fixy∗ ∈ Yand t∗ ∈ [0, 1] achieving the maximum above and define g(y) :=    1 if y < y∗ t∗ if y = y∗ 0 if y > y∗ The result directly follows by settingq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E T∼NegBin(θ,γ) \u0002 xT \u0003 = ((1−(1−γ)x)−θ−1 γ−θ−1 if θ ̸= 0 log(1−1−γ)x) log(γ) if θ = 0 Therefore, f′(x) = (1 − (1 − γ)x)−θ−1 · (θ·(1−γ) γ−θ−1 if θ ̸= 0 1−γ log(1/γ) if θ = 0 = (1 − (1 − γ)x)−θ−1 · γθ+1 · E[T]. By Lemma A.5, for appropriate valuesq, q′ ∈ [0, 1] and for allα >1 and all ˆα >1, we have Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · (1 − (1 − γ)q)−α(θ+1) · \u0000 1 − (1 − γ)q′\u0001−(1−α)(θ+1)\u0011 = ε + α α − 1 log C c 23+ 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 (γ + (1 − γ)(1 − q))1−ˆα · \u0000 γ + (1 − γ) \u0000 1 − q′\u0001\u0001ˆα\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here, we letˆαν = (α − 1)(1 + θ) and (1 − ˆα)ν + u = −α(θ + 1)) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1)Dˆα(P+∥P−)\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here,1 − q and 1 − q′ are postprocessings of someP+ and P′− respectively ande(ˆα−1)Dˆα(·∥·) is convex) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (By Lemma A.4) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · γu \u0011 (Here γ ≤ γ + (1 − γ)(1 − q) and u ≤ 0) = ε + α α − 1 log C c + ν α − 1 log \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011 + 1 α − 1 log \u0010 γθ+1 · E[T] · γu \u0011 = ε + α α − 1 log C c + ν α − 1 \u0012 (ˆα − 1) sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + ˆα log C c + log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011\u0011 + 1 α − 1 log \u0010 γu+θ+1 · E[T] \u0011 = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011 + log(E[T]) α − 1 + 1 + θ ˆα log(1/γ) (Here we haveν = (α − 1)(1 + θ) ˆα and u = −(1 + θ) \u0012α − 1 ˆα + 1 \u0013 ) = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ − 1 + e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)−ˆα log C c \u0013 + log(E[T]) α − 1 ≤ ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 ˆε + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ \u0013 + log(E[T]) α − 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let γ ∈ (0, 1) and θ ∈ (−1, ∞). Define a distribution NegBin(θ, γ) on N+ as follows: 24• If θ ̸= 0 and T is drawn from NegBin(θ, γ), then ∀k ∈ N P [T = k] = (1 − γ)k γ−θ − 1 · k−1Y ℓ=0 \u0012ℓ + θ ℓ + 1 \u0013 and E[T] = θ·(1−γ) γ·(1−γθ). Note that whenθ = 1, it reduces to the geometric distribution with parameter γ. • If θ = 0 and T is drawn from NegBin(0, γ), then P[T = k] = (1 − γ)k k · log(1/γ) and E[T] = 1/γ−1 log(1/γ). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofℓ2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantν: min f ∥f − π(j)∥2 + νKL(π(j), f) (C.1) s.t. f ∈ SC,c. When ν = 0, we recover the originalℓ2 projection. Moreover, it’s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterν approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onΛ. However, whenΛ represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Λ and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeπ(0) follows a uniform distribution over the parameter spaceΛ for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toπ(0) as SC,c(π(0)) = {f ∈ ΛR+ : ess supf/π(0) ≤ C, ess inff/π(0) ≥ c, Z α∈Λ f(α)dα = 1, f≪ π(0)}. When π(0) = 1 µ(λ), we recover the original definition ofSC,c. Note that heref ≪ π(0) means thatf is absolute continuous with respect to the prior distributionπ(0) and this ensures thatSC,c(π(0)) is non-empty. Note that such condition is automatically satisfied whenπ(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, π(j), into the spaceSC,c(π(0)), we consider the following functional programming problem: min f ∥f − π(j)∥2 s.t. f ∈ SC,c(π(0)), which is a direct generalization of Equation (3.1). As before,SC,c(π(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onΛ. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.71 for GP andσ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setτ to 0.1 andβ to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onη in the range[0.0001, 10] with a multiplicative factor of 3√ 10, resulting in16 observations forη. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.65 for GP andσ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, τ = 0.1, andβ = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onη in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forη. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forσ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for σ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forσ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterλ, our oracle returns a noisy scoreq(λ) drawn from a normal distributionN(µλ, σλ). Figure 3 displays a heatmap that presents the mean (µλ) and standard error (σλ) structure of the loss over these two hyperparameters, providing insights into the landscape’s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Λ can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the “honest HPO” setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(ε, δ)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of Rényi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3ε, 0)-DP as long as each training run of a hyperparameter was(ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30",
      "meta_data": {
        "arxiv_id": "2306.05734v2",
        "authors": [
          "Hua Wang",
          "Sheng Gao",
          "Huanyu Zhang",
          "Weijie J. Su",
          "Milan Shen"
        ],
        "published_date": "2023-06-09T07:55:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05734v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces DP-HyPO, a pioneering adaptive private hyperparameter optimization (HPO) framework that bridges the gap between non-private and private HPO. DP-HyPO enables the flexible use of non-DP adaptive HPO methods, such as Gaussian process-based optimization, for enhanced efficiency while avoiding the substantial privacy costs typically associated with composition. The framework provides sharp differential privacy (DP) guarantees by utilizing the Rényi DP framework, offering a strict generalization of prior uniform sampling results without stability assumptions. Empirically, the Gaussian process-based DP-HyPO algorithm is shown to outperform its uniform counterpart across various practical scenarios, allowing practitioners to integrate any non-private adaptive HPO method and manage privacy budget allocation for adaptivity.",
        "methodology": "DP-HyPO is an iterative framework that starts with a prior distribution over the hyperparameter space. For a random number of repetitions (e.g., drawn from a truncated negative binomial distribution), a hyperparameter is sampled from an adaptively updated distribution. A key mechanism is a projection technique that privatizes non-private HPO algorithms: it maintains an adaptive sampling distribution (posterior) that is projected onto a convex space (SC,c) of essentially bounded density functions before sampling. This ensures that the ratio of the posterior density to the prior density stays within bounds [c, C], controlling privacy leakage. The framework is instantiated using Gaussian processes (GPs), where GPs build a surrogate model for performance measures, generate scores (e.g., estimated upper confidence bound), and these scores are converted into a sampling distribution via a softmax function before being projected to meet DP constraints. The privacy guarantees are derived using Rényi Differential Privacy (RDP) analysis.",
        "experimental_setup": "The DP-HyPO framework, specifically its Gaussian process (GP) instantiation, is compared against a Uniform DP-HyPO method (a non-adaptive baseline from prior work) in two privacy configurations: white-box and black-box settings. In the white-box setting, experiments were conducted on the MNIST and CIFAR-10 datasets, training standard CNNs with DP-SGD. For MNIST, a semi-real simulation was performed by caching mean accuracies for discretized hyperparameters and adding Gaussian noise upon sampling. For CIFAR-10, hyperparameter landscapes were generated using BoTorch. Hyperparameters optimized included learning rate and clipping norm across predefined log-spaced and linear-spaced grids. In the black-box setting, a real-world Federated Learning (FL) task on a proprietary dataset was used, where optimal learning rates for the central server (AdaGrad) and users (SGD) were determined using a landscape generated by BoTorch. Performance was evaluated based on accuracy (MNIST, CIFAR-10) or loss (FL) of the output hyperparameter, aggregated over geometric distributions for the total number of runs.",
        "limitations": "The framework requires that the adaptive sampling distributions, when updated, must have their densities bounded by specific constants (c and C) relative to the prior distribution, which necessitates modifications to standard non-private HPO methods. Practically, the continuous hyperparameter space must be discretized to make the convex functional projection computationally feasible, serving as an approximation. The empirical evaluations on MNIST and CIFAR-10, due to computational resource constraints, relied on a semi-real simulation or pre-generated hyperparameter landscapes, rather than full end-to-end training for every hyperparameter combination. Additionally, the paper notes that the hyperparameter landscape of MNIST is relatively uncomplicated, which might limit the observed benefits of adaptive algorithms compared to more complex datasets.",
        "future_research_directions": "Future work could involve exploring alternative, more practically favorable HPO specifications by leveraging advanced HPO methods to further improve empirical performance. Another direction is to establish theoretical utility guarantees for the general DP-HyPO framework or for specific configurations within it, potentially by adapting proof methodologies from existing literature like Theorem 3.3 in [26]."
      }
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?\",\n    \"Methods\": \"We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.\",\n    \"Experimental Setup\": \"Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.\",\n    \"Experimental Code\": \"# pseudo-code\\nfor cfg in scheduler.sample():\\n    model = build_model(cfg)\\n    data = next(train_loader)            # one mini-batch\\n    loss  = forward_loss(model, data)\\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\\n    with torch.no_grad():               # one hyper step\\n        for p,g in zip(cfg.continuous_params(), grads):\\n            p -= eta_h * g\\n    scheduler.launch(cfg)               # continue as usual\",\n    \"Expected Result\": \"Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.\",\n    \"Expected Conclusion\": \"A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort.\"\n}",
    "experimental_design": {
      "experiment_strategy": "Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)\n\n1. Core Hypotheses to Validate\n   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.\n   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.\n   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.\n   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.\n   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).\n\n2. Experiment Families (all experiments draw from one common pool of settings below)\n   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).\n   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.\n   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).\n   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.\n   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).\n\n3. Comparison Axes for Every Experiment\n   • Baseline Scheduler (vanilla).\n   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).\n   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).\n   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.\n\n4. Metrics & Evaluation Protocol\n   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):\n      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).\n      – Best final validation/test score after fixed budget.\n      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.\n   Secondary diagnostics:\n      – AUC of best-score-vs-time curve (overall sample efficiency).\n      – Seed-wise variance of T@τ and final score.\n      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).\n   Qualitative/visual:\n      – Survival plots of promoted configurations per rung.\n      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.\n   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.\n\n5. Success Criteria (must hold in ≥75 % of experiment tuples)\n   • ≥15 % median reduction in T@τ with p<0.05.\n   • ≤0.2 % relative drop (or improvement) in best final score.\n   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.\n   • Std-dev(T@τ) not inflated by more than 10 %.\n\n6. Multi-Perspective Validation Plan\n   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.\n   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.\n   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.\n   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.\n   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.\n   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.\n\n7. Experimental Logistics\n   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).\n   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.\n   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.\n\n8. Risk Mitigation & Contingency\n   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).\n   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.\n\nThis unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.",
      "experiments": [
        {
          "experiment_id": "exp-1",
          "run_variations": [
            "asha-baseline",
            "asha-random-warm",
            "asha-ohgw-1step",
            "asha-ohgw-3step"
          ],
          "description": "Objective / Hypothesis: Quantify the raw efficiency gains of One-Shot Hyper-Gradient Warm-Starts (OHGW) for classical image-classification hyper-parameter optimisation and disentangle the effect of merely perturbing the hyper-parameters from using the true hyper-gradient.\n\nDomain & Task: Vision – CIFAR-10 image classification.\n\nModels: ResNet-20 (main), ResNet-50 (sanity-check on a deeper network – evaluated only for the best scheduler variant after selection).\n\nSchedulers under test: ASHA (official implementation in Ray-Tune v2.6).\n\nSearch space (5-dim continuous): log10-learning-rate, log10-weight-decay, momentum, RandAugment-magnitude, label-smoothing.\n\nRun variations:\n• asha-baseline – vanilla ASHA.\n• asha-random-warm – perform one random Gaussian step (σ=0.01) in ψ instead of a hyper-gradient step (control).* \n• asha-ohgw-1step – proposed method (η_h=1e-3, one step).\n• asha-ohgw-3step – same but apply three successive hyper-gradient steps (ablation for diminishing returns).\n\nDatasets & Pre-processing: CIFAR-10, standard train/val/test split = 45k/5k/10k.  Per-image Z-score normalisation, RandAugment (m searchable), Cutout(16), label-smoothing (ε searchable).  Validation set is stratified.\n\nTraining loop: 200 epochs (budget for full training), Cosine LR, batch size 128, SGD-M.  Weight initialisation He-normal.  Mixed precision (AMP).  Early-stopping OFF (to reveal full curve).\n\nEvaluation metrics:\nPrimary – T@93%: wall-clock hours & GPU hours needed to first reach 93% val accuracy.\nSecondary – best test accuracy after 200 epochs; AUC(score-vs-time); overhead-FLOPs ratio; peak VRAM; seed-wise variance.\n\nProtocol: 32 independent HPO replicates (seed-paired across variations).  Each replicate is allowed 12 A100-GPU hours (4 GPUs×3 h) exactly.  Statistics: median±IQR, Wilcoxon signed-rank (α=0.05) on paired T@93%.\n\nHyper-parameter sensitivity: inside asha-ohgw-1step, sweep η_h ∈ {1e-4, 3e-4, 1e-3, 3e-3} on an extra 8 seeds; fit LOESS to quantify curvature.\n\nRobustness: inject 10% symmetric label noise on a duplicate run; compare % speed-up retained.\n\nCompute efficiency: PyTorch-Profiler to record FLOPs; NVML + psutil for VRAM/RAM; timestamps logged by Lightning Callbacks.\n\nExample code fragment (within Ray trial):\n```python\n# one-shot HG warm-start\nloss = model(train_mb)                    # forward\nhyper_grads = torch.autograd.grad(loss, cfg.continuous())\nwith torch.no_grad():\n    for p, g in zip(cfg.continuous(), hyper_grads):\n        p -= 1e-3 * g                     # η_h\n# hand control back to ASHA\ntrain_full(cfg)\n```\n\nComputational budget on cluster: 8 concurrent trials * 4 GPUs each = 32 GPUs (< half of node).  Wall-clock expected 18h incl. repeats.\n\nSuccess criteria: ≥15% lower median T@93% for asha-ohgw-1step than asha-baseline with p<0.05 and ≤0.2% drop in final test accuracy.\n\nFootnote *: random-warm uses identical σ distribution as the empirical magnitude of a hyper-gradient step, ensuring fair energy injection.",
          "github_repository_info": {
            "github_owner": "auto-res2",
            "repository_name": "experiment_matsuzawa_251002",
            "branch_name": "research-0-retry-4"
          },
          "code": {
            "train_py": "",
            "evaluate_py": "",
            "preprocess_py": "",
            "model_py": "",
            "main_py": "",
            "pyproject_toml": "",
            "smoke_test_yaml": "",
            "full_experiment_yaml": ""
          },
          "results": {
            "result": "",
            "error": "",
            "image_file_name_list": [
              "accuracy_asha-baseline.pdf",
              "accuracy_asha-ohgw-1step.pdf",
              "accuracy_asha-ohgw-3step.pdf",
              "accuracy_asha-random-warm.pdf",
              "accuracy_comparison.pdf",
              "accuracy_trajectories.pdf",
              "training_loss_asha-baseline.pdf",
              "training_loss_asha-ohgw-1step.pdf",
              "training_loss_asha-ohgw-3step.pdf",
              "training_loss_asha-random-warm.pdf"
            ]
          }
        },
        {
          "experiment_id": "exp-2",
          "run_variations": [
            "pasha-baseline",
            "pasha-ohgw-vision-tuned-eta",
            "pasha-ohgw-low-eta",
            "pasha-ohgw-noisy-data"
          ],
          "description": "Objective / Hypothesis: Test zero-shot generalisation and robustness of OHGW in a different modality (language modelling) and under distribution shift.\n\nDomain & Task: NLP – word-level language modelling on WikiText-103, with an additional OOD evaluation on WikiText-103-v1 (headlines first 5% shuffled – simulates domain drift).\n\nModel: GPT2-small (124 M params, HuggingFace implementation) trained from scratch with BPE vocab 50 k.\n\nScheduler: PASHA (2023) – gradient-aware variation of Hyperband; ours wraps PASHA in the same way.\n\nSearch space (6 dims): log10-learning-rate, log10-weight-decay, attention-dropout, residual-dropout, label-smoothing, warmup-steps (continuous proxy by scaling factor).\n\nRun variations:\n• pasha-baseline – vanilla PASHA.\n• pasha-ohgw-vision-tuned-eta – OHGW with η_h=1e-3 exactly copied from CIFAR experiment (tests transfer without retuning).\n• pasha-ohgw-low-eta – OHGW with η_h=3e-4 (sensitivity extremum).\n• pasha-ohgw-noisy-data – OHGW with η_h=1e-3 while 15% of training tokens are randomly replaced (robustness to noise).\n\nDataset processing: SentencePiece BPE (shared).  Sequence length 1024 tokens; dynamic batching up to 2M tokens/GPU.  Train/val/test split 238M / 8M / 8M tokens.  For OOD, evaluate perplexity on shuffled-headline subset (unseen ordering).\n\nTraining loop & budget: 50 training epochs (~250 k updates), AdamW, cosine LR.  PASHA minimum resource per config = 2 epochs, rungs ×2.  Total compute budget per replicate 16 GPU×hours (8 GPUs ×2 h).  24 replicates.\n\nEvaluation metrics:\nPrimary – T@ppl=30 on validation set (wall-clock & GPU-h).\nSecondary – best validation perplexity, best OOD perplexity, FLOPs overhead, peak VRAM, std-dev across seeds.\nCalibration metric – ECE (expected calibration error) on top-k probabilities (k=10).\n\nData splitting: Configs use 90/10 split of train for inner-val to avoid test leakage; final report on held-out test.\n\nHyper-parameter analysis: log sweep of η_h done offline on 5 seeds; fit cubic spline to measure optimal region width (reports in appendix).\n\nRobustness procedures:\n1. Data noise: see pasha-ohgw-noisy-data.\n2. Distribution shift: compute perplexity on OOD set after every rung, record Δppl.\n3. Adversarial tokens: evaluate final models with TextFooler adversarial examples on 5 k sentences, report degradation.\n\nCompute profiling: same toolkit as exp-1; additionally use PyTorch-CUDA-Profiler for kernel-level warm-start cost.\n\nExample trial wrapper:\n```python\nloss = lm_model(input_ids, labels=input_ids).loss\nhg = torch.autograd.grad(loss, cfg.continuous())\nwith torch.no_grad():\n    for p,g in zip(cfg.continuous(), hg):\n        p -= eta_h * g    # no extra unroll\npasha_scheduler.launch(cfg)\n```\n\nStatistical testing: Paired Wilcoxon on T@30 with Bonferroni correction (m=3 comparisons).  Significance if p<0.016.\n\nSuccess criteria: vision-tuned η_h version achieves ≥15% speed-up vs. baseline on T@30 with ≤0.5 ppl regression on final test; effect still ≥10% under noisy data.  Extra compute overhead ≤5% of total FLOPs.",
          "github_repository_info": {
            "github_owner": "auto-res2",
            "repository_name": "experiment_matsuzawa_251002",
            "branch_name": "research-0-retry-4"
          },
          "code": {
            "train_py": "",
            "evaluate_py": "",
            "preprocess_py": "",
            "model_py": "",
            "main_py": "",
            "pyproject_toml": "",
            "smoke_test_yaml": "",
            "full_experiment_yaml": ""
          },
          "results": {
            "result": "=== [PHASE 2/2] Full experiment start Sat Oct  4 03:59:44 AM UTC 2025 ===\nRunning 4 experiment variations defined in config/full_experiment.yaml\n\n========== Starting run: pasha-baseline ==========\n{\"run_id\": \"pasha-baseline\", \"epoch\": 1, \"train_loss\": 3.1942589678103968, \"train_ppl\": 24.392091668343667, \"val_loss\": 3.0007877913748082, \"val_ppl\": 20.10136637026642}\n{\"run_id\": \"pasha-baseline\", \"epoch\": 2, \"train_loss\": 3.038589286499437, \"train_ppl\": 20.875772718343853, \"val_loss\": 2.9517171234510746, \"val_ppl\": 19.138789192638537}\n{\"run_id\": \"pasha-baseline\", \"epoch\": 3, \"train_loss\": 2.969218556822731, \"train_ppl\": 19.476693718304873, \"val_loss\": 2.922179020292037, \"val_ppl\": 18.581733351568122}\n{\"run_id\": \"pasha-baseline\", \"epoch\": 4, \"train_loss\": 2.925080865034609, \"train_ppl\": 18.635732968161122, \"val_loss\": 2.9072656166504034, \"val_ppl\": 18.306672605693414}\n",
            "error": "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "image_file_name_list": []
          }
        }
      ],
      "expected_models": [
        "ResNet-20",
        "ResNet-50",
        "GPT2-small"
      ],
      "expected_datasets": [
        "CIFAR-10",
        "CIFAR-100",
        "WikiText-103"
      ],
      "external_resources": {
        "hugging_face": {
          "models": [
            {
              "id": "timm/resnet200d.ra2_in1k",
              "author": "timm",
              "sha": "1c92d12387fd99d4985a457a55d68f05e9d1bc53",
              "created_at": "2023-04-05T18:40:03+00:00",
              "last_modified": "2025-01-21T21:42:27+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 516,
              "likes": 1,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1512.03385",
                "arxiv:1812.01187",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet200d.ra2_in1k\n\nA ResNet-D image classification model.\n\nThis model features:\n * ReLU activations\n * 3-layer stem of 3x3 convolutions with pooling\n * 2x2 average pool + 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * RandAugment `RA2` recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as `B` recipe in [ResNet Strikes Back](https://arxiv.org/abs/2110.00476).\n * RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging\n * Step (exponential decay w/ staircase) LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 64.7\n  - GMACs: 20.0\n  - Activations (M): 43.1\n  - Image size: train = 256 x 256, test = 320 x 320\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n  - Bag of Tricks for Image Classification with Convolutional Neural Networks: https://arxiv.org/abs/1812.01187\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet200d.ra2_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet200d.ra2_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 128, 128])\n    #  torch.Size([1, 256, 64, 64])\n    #  torch.Size([1, 512, 32, 32])\n    #  torch.Size([1, 1024, 16, 16])\n    #  torch.Size([1, 2048, 8, 8])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet200d.ra2_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 8, 8) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n```bibtex\n@article{He2018BagOT,\n  title={Bag of Tricks for Image Classification with Convolutional Neural Networks},\n  author={Tong He and Zhi Zhang and Hang Zhang and Zhongyue Zhang and Junyuan Xie and Mu Li},\n  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2018},\n  pages={558-567}\n}\n```\n",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet200d.ra2_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet200d.ra2_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 128, 128])\n    #  torch.Size([1, 256, 64, 64])\n    #  torch.Size([1, 512, 32, 32])\n    #  torch.Size([1, 1024, 16, 16])\n    #  torch.Size([1, 2048, 8, 8])\n\n    print(o.shape)\n\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet200d.ra2_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 8, 8) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            },
            {
              "id": "timm/resnetrs200.tf_in1k",
              "author": "timm",
              "sha": "97ab648651cbf3a4593b5e9bb5f0247be2494573",
              "created_at": "2023-04-05T18:48:02+00:00",
              "last_modified": "2025-01-21T21:43:06+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 144,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2103.07579",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnetrs200.tf_in1k\n\nA ResNetRS-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k by paper authors in Tensorflow.\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 93.2\n  - GMACs: 20.2\n  - Activations (M): 43.4\n  - Image size: train = 256 x 256, test = 320 x 320\n- **Papers:**\n  - Revisiting ResNets: Improved Training and Scaling Strategies: https://arxiv.org/abs/2103.07579\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/tensorflow/tpu/tree/master/models/official/resnet\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnetrs200.tf_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetrs200.tf_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 128, 128])\n    #  torch.Size([1, 256, 64, 64])\n    #  torch.Size([1, 512, 32, 32])\n    #  torch.Size([1, 1024, 16, 16])\n    #  torch.Size([1, 2048, 8, 8])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetrs200.tf_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 8, 8) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@article{bello2021revisiting,\n  title={Revisiting ResNets: Improved Training and Scaling Strategies},\n  author={Irwan Bello and William Fedus and Xianzhi Du and Ekin D. Cubuk and Aravind Srinivas and Tsung-Yi Lin and Jonathon Shlens and Barret Zoph},\n  journal={arXiv preprint arXiv:2103.07579},\n  year={2021}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnetrs200.tf_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetrs200.tf_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 128, 128])\n    #  torch.Size([1, 256, 64, 64])\n    #  torch.Size([1, 512, 32, 32])\n    #  torch.Size([1, 1024, 16, 16])\n    #  torch.Size([1, 2048, 8, 8])\n\n    print(o.shape)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetrs200.tf_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 8, 8) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            },
            {
              "id": "BVRA/resnet18.in1k_ft_df20_299",
              "author": "BVRA",
              "sha": "0e0d519fea85d6a1d0ce218d8c1d4c58447593dc",
              "created_at": "2024-01-06T02:48:14+00:00",
              "last_modified": "2024-07-30T12:50:54+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 15,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "config.yaml"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "cc-by-nc-4.0",
                "language": [],
                "library_name": "DanishFungi",
                "tags": [
                  "image-classification",
                  "ecology",
                  "fungi",
                  "FGVC"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "DanishFungi",
                "pytorch",
                "image-classification",
                "ecology",
                "fungi",
                "FGVC",
                "arxiv:1512.03385",
                "license:cc-by-nc-4.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "DanishFungi",
              "readme": "\n---\ntags:\n- image-classification\n- ecology\n- fungi\n- FGVC\nlibrary_name: DanishFungi\nlicense: cc-by-nc-4.0\n---\n# Model card for BVRA/resnet18.in1k_ft_df20_299\n\n## Model Details\n- **Model Type:** Danish Fungi Classification \n- **Model Stats:**\n  - Params (M): 12.0M\n  - Image size: 299 x 299\n- **Papers:**\n  - **Original:** Deep Residual Learning for Image Recognition --> https://arxiv.org/pdf/1512.03385\n  - **Train Dataset:** DF20 --> https://github.com/BohemianVRA/DanishFungiDataset/\n\n## Model Usage\n### Image Embeddings\n```python\nimport timm\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom urllib.request import urlopen\nmodel = timm.create_model(\"hf-hub:BVRA/resnet18.in1k_ft_df20_299\", pretrained=True)\nmodel = model.eval()\ntrain_transforms = T.Compose([T.Resize((299, 299)), \n                              T.ToTensor(), \n                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \nimg = Image.open(PATH_TO_YOUR_IMAGE)\noutput = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# output is a (1, num_features) shaped tensor\n```\n\n## Citation \n```bibtex\n@InProceedings{Picek_2022_WACV,\n    author    = {Picek, Lukas and Sulc, Milan and Matas, Jiri and Jeppesen, Thomas S. and Heilmann-Clausen, Jacob and L{\u0007e}ss{\\o}e, Thomas and Fr{\\o}slev, Tobias},\n    title     = {Danish Fungi 2020 - Not Just Another Image Recognition Dataset},\n    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},\n    month     = {January},\n    year      = {2022},\n    pages     = {1525-1535}\n}\n\n@article{picek2022automatic,\n  title={Automatic Fungi Recognition: Deep Learning Meets Mycology},\n  author={Picek, Lukas and Sulc, Milan and Matas, Jiri and Heilmann-Clausen, Jacob and Jeppesen, Thomas S and Lind, Emil},\n  journal={Sensors},\n  volume={22},\n  number={2},\n  pages={633},\n  year={2022},\n  publisher={Multidisciplinary Digital Publishing Institute}\n}\n```",
              "extracted_code": "import timm\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom urllib.request import urlopen\nmodel = timm.create_model(\"hf-hub:BVRA/resnet18.in1k_ft_df20_299\", pretrained=True)\nmodel = model.eval()\ntrain_transforms = T.Compose([T.Resize((299, 299)), \n                              T.ToTensor(), \n                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \nimg = Image.open(PATH_TO_YOUR_IMAGE)\noutput = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor"
            },
            {
              "id": "BVRA/resnet50.in1k_ft_df20_299",
              "author": "BVRA",
              "sha": "e69147397d55f0c0d5149468f61a0315e9a6d0a8",
              "created_at": "2024-01-11T12:22:49+00:00",
              "last_modified": "2024-07-30T12:49:57+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 10,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "config.yaml"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "cc-by-nc-4.0",
                "language": [],
                "library_name": "DanishFungi",
                "tags": [
                  "image-classification",
                  "ecology",
                  "fungi",
                  "FGVC"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "DanishFungi",
                "pytorch",
                "image-classification",
                "ecology",
                "fungi",
                "FGVC",
                "arxiv:1512.03385",
                "license:cc-by-nc-4.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "DanishFungi",
              "readme": "\n---\ntags:\n- image-classification\n- ecology\n- fungi\n- FGVC\nlibrary_name: DanishFungi\nlicense: cc-by-nc-4.0\n---\n# Model card for BVRA/resnet50.in1k_ft_df20_299\n\n## Model Details\n- **Model Type:** Danish Fungi Classification \n- **Model Stats:**\n  - Params (M): 26.8M\n  - Image size: 299 x 299\n- **Papers:**\n  - **Original:** Deep Residual Learning for Image Recognition --> https://arxiv.org/pdf/1512.03385\n  - **Train Dataset:** DF20 --> https://github.com/BohemianVRA/DanishFungiDataset/\n\n## Model Usage\n### Image Embeddings\n```python\nimport timm\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom urllib.request import urlopen\nmodel = timm.create_model(\"hf-hub:BVRA/resnet50.in1k_ft_df20_299\", pretrained=True)\nmodel = model.eval()\ntrain_transforms = T.Compose([T.Resize((299, 299)), \n                              T.ToTensor(), \n                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \nimg = Image.open(PATH_TO_YOUR_IMAGE)\noutput = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# output is a (1, num_features) shaped tensor\n```\n\n## Citation \n```bibtex\n@InProceedings{Picek_2022_WACV,\n    author    = {Picek, Lukas and Sulc, Milan and Matas, Jiri and Jeppesen, Thomas S. and Heilmann-Clausen, Jacob and L{\u0007e}ss{\\o}e, Thomas and Fr{\\o}slev, Tobias},\n    title     = {Danish Fungi 2020 - Not Just Another Image Recognition Dataset},\n    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},\n    month     = {January},\n    year      = {2022},\n    pages     = {1525-1535}\n}\n\n@article{picek2022automatic,\n  title={Automatic Fungi Recognition: Deep Learning Meets Mycology},\n  author={Picek, Lukas and Sulc, Milan and Matas, Jiri and Heilmann-Clausen, Jacob and Jeppesen, Thomas S and Lind, Emil},\n  journal={Sensors},\n  volume={22},\n  number={2},\n  pages={633},\n  year={2022},\n  publisher={Multidisciplinary Digital Publishing Institute}\n}\n```",
              "extracted_code": "import timm\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom urllib.request import urlopen\nmodel = timm.create_model(\"hf-hub:BVRA/resnet50.in1k_ft_df20_299\", pretrained=True)\nmodel = model.eval()\ntrain_transforms = T.Compose([T.Resize((299, 299)), \n                              T.ToTensor(), \n                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \nimg = Image.open(PATH_TO_YOUR_IMAGE)\noutput = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# output is a (1, num_features) shaped tensor\n"
            },
            {
              "id": "timm/resnet50.a1_in1k",
              "author": "timm",
              "sha": "767268603ca0cb0bfe326fa87277f19c419566ef",
              "created_at": "2023-04-05T18:07:45+00:00",
              "last_modified": "2025-07-11T18:17:03+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 3499821,
              "likes": 39,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet50.a1_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A1` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 25.6\n  - GMACs: 4.1\n  - Activations (M): 11.1\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    print(o.shape)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            },
            {
              "id": "timm/wide_resnet50_2.racm_in1k",
              "author": "timm",
              "sha": "30f73aceaaa1911830a9795b83ab1908dba18719",
              "created_at": "2023-04-05T20:38:52+00:00",
              "last_modified": "2025-01-21T21:45:51+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 289328,
              "likes": 2,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1605.07146",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for wide_resnet50_2.racm_in1k\n\nA Wide-ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * RandAugment `RACM` recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as `B` recipe in [ResNet Strikes Back](https://arxiv.org/abs/2110.00476).\n * RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging\n * Step (exponential decay w/ staircase) LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 68.9\n  - GMACs: 11.4\n  - Activations (M): 14.4\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Wide Residual Networks: https://arxiv.org/abs/1605.07146\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('wide_resnet50_2.racm_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'wide_resnet50_2.racm_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'wide_resnet50_2.racm_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{DBLP:journals/corr/ZagoruykoK16,\n  author    = {Sergey Zagoruyko and\n               Nikos Komodakis},\n  title     = {Wide Residual Networks},\n  journal   = {CoRR},\n  volume    = {abs/1605.07146},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1605.07146},\n  archivePrefix = {arXiv},\n  eprint    = {1605.07146},\n  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('wide_resnet50_2.racm_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'wide_resnet50_2.racm_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'wide_resnet50_2.racm_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            },
            {
              "id": "timm/resnet50.ram_in1k",
              "author": "timm",
              "sha": "6e2446dfff2a12859ab4c1b2dc5cb3d95fccab8f",
              "created_at": "2023-04-05T18:14:08+00:00",
              "last_modified": "2025-01-21T21:40:39+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 218417,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet50.ram_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * AugMix (with RandAugment) recipe\n * SGD (w/ Nesterov) optimizer and JSD (Jensen–Shannon divergence) loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 25.6\n  - GMACs: 4.1\n  - Activations (M): 11.1\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.ram_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.ram_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            },
            {
              "id": "microsoft/resnet-50",
              "author": "microsoft",
              "sha": "34c2154c194f829b11125337b98c8f5f9965ff19",
              "created_at": "2022-03-16T15:42:43+00:00",
              "last_modified": "2024-02-13T21:24:05+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 163115,
              "likes": 440,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "flax_model.msgpack"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "preprocessor_config.json"
                },
                {
                  "rfilename": "pytorch_model.bin"
                },
                {
                  "rfilename": "tf_model.h5"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "tags": [
                  "vision",
                  "image-classification"
                ],
                "datasets": [
                  "imagenet-1k"
                ],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "transformers",
                "pytorch",
                "tf",
                "jax",
                "safetensors",
                "resnet",
                "image-classification",
                "vision",
                "dataset:imagenet-1k",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "autotrain_compatible",
                "endpoints_compatible",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "transformers",
              "readme": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n---\n\n# ResNet-50 v1.5\n\nResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by He et al. \n\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.\n\nThis is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (\\~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=resnet) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/resnet).\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{he2016deep,\n  title={Deep residual learning for image recognition},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={770--778},\n  year={2016}\n}\n```\n",
              "extracted_code": "from transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])"
            },
            {
              "id": "timm/resnetv2_50x1_bit.goog_in21k",
              "author": "timm",
              "sha": "28dc27a546380665e285a3aeb0ee1e0b4ea45097",
              "created_at": "2023-03-22T20:57:10+00:00",
              "last_modified": "2025-01-21T21:33:02+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 103683,
              "likes": 5,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [
                  "imagenet-21k"
                ],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "dataset:imagenet-21k",
                "arxiv:1912.11370",
                "arxiv:1603.05027",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\ndatasets:\n- imagenet-21k\n---\n# Model card for resnetv2_50x1_bit.goog_in21k\n\nA ResNet-V2-BiT (Big Transfer w/ pre-activation ResNet) image classification model. Trained on ImageNet-21k by paper authors.\n\nThis model uses:\n* Group Normalization (GN) in combination with Weight Standardization (WS) instead of Batch Normalization (BN)..\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 68.3\n  - GMACs: 4.3\n  - Activations (M): 11.1\n  - Image size: 224 x 224\n- **Papers:**\n  - Big Transfer (BiT): General Visual Representation Learning: https://arxiv.org/abs/1912.11370\n  - Identity Mappings in Deep Residual Networks: https://arxiv.org/abs/1603.05027\n- **Dataset:** ImageNet-21k\n- **Original:** https://github.com/google-research/big_transfer\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnetv2_50x1_bit.goog_in21k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetv2_50x1_bit.goog_in21k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetv2_50x1_bit.goog_in21k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n## Citation\n```bibtex\n@inproceedings{Kolesnikov2019BigT,\n  title={Big Transfer (BiT): General Visual Representation Learning},\n  author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},\n  booktitle={European Conference on Computer Vision},\n  year={2019}\n}\n```\n```bibtex\n@article{He2016,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Identity Mappings in Deep Residual Networks},\n  journal = {arXiv preprint arXiv:1603.05027},\n  year = {2016}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnetv2_50x1_bit.goog_in21k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\n\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetv2_50x1_bit.goog_in21k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n\n\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnetv2_50x1_bit.goog_in21k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            },
            {
              "id": "stanford-crfm/alias-gpt2-small-x21",
              "author": "stanford-crfm",
              "sha": "ef2e2aa6e81351c5d01ea11ec5a6a0f631c72180",
              "created_at": "2022-03-02T23:29:05+00:00",
              "last_modified": "2022-12-03T00:33:39+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 10652,
              "likes": 4,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "global_step400000/mp_rank_00_model_states.pt"
                },
                {
                  "rfilename": "global_step400000/zero_pp_rank_0_mp_rank_00optim_states.pt"
                },
                {
                  "rfilename": "global_step400000/zero_pp_rank_1_mp_rank_00optim_states.pt"
                },
                {
                  "rfilename": "global_step400000/zero_pp_rank_2_mp_rank_00optim_states.pt"
                },
                {
                  "rfilename": "global_step400000/zero_pp_rank_3_mp_rank_00optim_states.pt"
                },
                {
                  "rfilename": "global_step400000/zero_pp_rank_4_mp_rank_00optim_states.pt"
                },
                {
                  "rfilename": "global_step400000/zero_pp_rank_5_mp_rank_00optim_states.pt"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "tags": [
                  "gpt2",
                  "text-generation"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "transformers",
                "pytorch",
                "gpt2",
                "text-generation",
                "arxiv:1910.09700",
                "license:apache-2.0",
                "autotrain_compatible",
                "text-generation-inference",
                "endpoints_compatible",
                "region:us"
              ],
              "pipeline_tag": "text-generation",
              "library_name": "transformers",
              "readme": "---\nlicense: apache-2.0\ntags:\n- gpt2\n- text-generation\n---\n\n# Model Card for alias-gpt2-small-x21 \n \n# Model Details\n \n## Model Description\n \nMore information needed\n \n- **Developed by:** Stanford CRFM\n- **Shared by [Optional]:** Stanford CRFM\n\n- **Model type:** Text Generation\n- **Language(s) (NLP):** More information needed\n- **License:** Apache 2.0\n- **Parent Model:** [GPT-2](https://huggingface.co/gpt2?text=My+name+is+Thomas+and+my+main)\n- **Resources for more information:** \n    - [GitHub Repo](https://github.com/stanford-crfm/mistral)\n\n\n\n# Uses\n \n\n## Direct Use\nThis model can be used for the task of Text Generation.\n \n## Downstream Use [Optional]\n \nMore information needed.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n\n\n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n \n## Training Data\n \nMore information needed \n \n## Training Procedure\n\n \n### Preprocessing\n \nMore information needed \n\n\n \n### Speeds, Sizes, Times\n \nMore information needed \n\n\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nMore information needed\n \n### Factors\nMore information needed\n \n### Metrics\n \nMore information needed\n \n \n## Results \n \nMore information needed\n\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \n \nMore information needed\n \n### Software\n \nMore information needed.\n \n# Citation\n\n \n**BibTeX:**\n \nMore information needed\n\n\n\n\n**APA:**\n\nMore information needed\n  \n# Glossary [optional]\n \nMore information needed\n\n# More Information [optional]\nMore information needed\n\n# Model Card Authors [optional]\n \nStanford CRFM  in collaboration with Ezi Ozoani and the Hugging Face team\n\n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"stanford-crfm/alias-gpt2-small-x21\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"stanford-crfm/alias-gpt2-small-x21\")\n ```\n</details>\n",
              "extracted_code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"stanford-crfm/alias-gpt2-small-x21\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"stanford-crfm/alias-gpt2-small-x21\")"
            }
          ],
          "datasets": [
            {
              "id": "uoft-cs/cifar10",
              "author": "uoft-cs",
              "sha": "0b2714987fa478483af9968de7c934580d0bb9a2",
              "created_at": "2022-03-02T23:29:22+00:00",
              "last_modified": "2024-01-04T06:53:11+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 71114,
              "likes": 85,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "plain_text/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "plain_text/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "license": [
                  "unknown"
                ],
                "language": [
                  "en"
                ],
                "tags": [],
                "datasets": [],
                "task_categories": [
                  "image-classification"
                ],
                "size_categories": [
                  "10K<n<100K"
                ],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "task_categories:image-classification",
                "annotations_creators:crowdsourced",
                "language_creators:found",
                "multilinguality:monolingual",
                "source_datasets:extended|other-80-Million-Tiny-Images",
                "language:en",
                "license:unknown",
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-10\npretty_name: Cifar10\ndataset_info:\n  config_name: plain_text\n  features:\n  - name: img\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': airplane\n          '1': automobile\n          '2': bird\n          '3': cat\n          '4': deer\n          '5': dog\n          '6': frog\n          '7': horse\n          '8': ship\n          '9': truck\n  splits:\n  - name: train\n    num_bytes: 113648310.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22731580.0\n    num_examples: 10000\n  download_size: 143646105\n  dataset_size: 136379890.0\nconfigs:\n- config_name: plain_text\n  data_files:\n  - split: train\n    path: plain_text/train-*\n  - split: test\n    path: plain_text/test-*\n  default: true\n---\n\n# Dataset Card for CIFAR-10\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://www.cs.toronto.edu/~kriz/cifar.html\n- **Repository:** \n- **Paper:** Learning Multiple Layers of Features from Tiny Images by Alex Krizhevsky\n- **Leaderboard:**\n- **Point of Contact:**\n\n### Dataset Summary\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 10 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-10).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x201FA6EE748>,\n  'label': 0\n}\n```\n\n### Data Fields\n\n- img: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`\n- label: 0-9 with the following correspondence\n         0 airplane\n         1 automobile\n         2 bird\n         3 cat\n         4 deer\n         5 dog\n         6 frog\n         7 horse\n         8 ship\n         9 truck\n\n### Data Splits\n\nTrain and Test\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed]\n\n#### Who are the source language producers?\n\n[More Information Needed]\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed]\n\n#### Who are the annotators?\n\n[More Information Needed]\n\n### Personal and Sensitive Information\n\n[More Information Needed]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed]\n\n### Discussion of Biases\n\n[More Information Needed]\n\n### Other Known Limitations\n\n[More Information Needed]\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed]\n\n### Licensing Information\n\n[More Information Needed]\n\n### Citation Information\n\n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@czabo](https://github.com/czabo) for adding this dataset.",
              "extracted_code": ""
            },
            {
              "id": "uoft-cs/cifar100",
              "author": "uoft-cs",
              "sha": "aadb3af77e9048adbea6b47c21a81e47dd092ae5",
              "created_at": "2022-03-02T23:29:22+00:00",
              "last_modified": "2024-01-04T06:57:47+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 12540,
              "likes": 53,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "cifar100/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "cifar100/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "license": [
                  "unknown"
                ],
                "language": [
                  "en"
                ],
                "tags": [],
                "datasets": [],
                "task_categories": [
                  "image-classification"
                ],
                "size_categories": [
                  "10K<n<100K"
                ],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "task_categories:image-classification",
                "annotations_creators:crowdsourced",
                "language_creators:found",
                "multilinguality:monolingual",
                "source_datasets:extended|other-80-Million-Tiny-Images",
                "language:en",
                "license:unknown",
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-100\npretty_name: Cifar100\ndataset_info:\n  config_name: cifar100\n  features:\n  - name: img\n    dtype: image\n  - name: fine_label\n    dtype:\n      class_label:\n        names:\n          '0': apple\n          '1': aquarium_fish\n          '2': baby\n          '3': bear\n          '4': beaver\n          '5': bed\n          '6': bee\n          '7': beetle\n          '8': bicycle\n          '9': bottle\n          '10': bowl\n          '11': boy\n          '12': bridge\n          '13': bus\n          '14': butterfly\n          '15': camel\n          '16': can\n          '17': castle\n          '18': caterpillar\n          '19': cattle\n          '20': chair\n          '21': chimpanzee\n          '22': clock\n          '23': cloud\n          '24': cockroach\n          '25': couch\n          '26': cra\n          '27': crocodile\n          '28': cup\n          '29': dinosaur\n          '30': dolphin\n          '31': elephant\n          '32': flatfish\n          '33': forest\n          '34': fox\n          '35': girl\n          '36': hamster\n          '37': house\n          '38': kangaroo\n          '39': keyboard\n          '40': lamp\n          '41': lawn_mower\n          '42': leopard\n          '43': lion\n          '44': lizard\n          '45': lobster\n          '46': man\n          '47': maple_tree\n          '48': motorcycle\n          '49': mountain\n          '50': mouse\n          '51': mushroom\n          '52': oak_tree\n          '53': orange\n          '54': orchid\n          '55': otter\n          '56': palm_tree\n          '57': pear\n          '58': pickup_truck\n          '59': pine_tree\n          '60': plain\n          '61': plate\n          '62': poppy\n          '63': porcupine\n          '64': possum\n          '65': rabbit\n          '66': raccoon\n          '67': ray\n          '68': road\n          '69': rocket\n          '70': rose\n          '71': sea\n          '72': seal\n          '73': shark\n          '74': shrew\n          '75': skunk\n          '76': skyscraper\n          '77': snail\n          '78': snake\n          '79': spider\n          '80': squirrel\n          '81': streetcar\n          '82': sunflower\n          '83': sweet_pepper\n          '84': table\n          '85': tank\n          '86': telephone\n          '87': television\n          '88': tiger\n          '89': tractor\n          '90': train\n          '91': trout\n          '92': tulip\n          '93': turtle\n          '94': wardrobe\n          '95': whale\n          '96': willow_tree\n          '97': wolf\n          '98': woman\n          '99': worm\n  - name: coarse_label\n    dtype:\n      class_label:\n        names:\n          '0': aquatic_mammals\n          '1': fish\n          '2': flowers\n          '3': food_containers\n          '4': fruit_and_vegetables\n          '5': household_electrical_devices\n          '6': household_furniture\n          '7': insects\n          '8': large_carnivores\n          '9': large_man-made_outdoor_things\n          '10': large_natural_outdoor_scenes\n          '11': large_omnivores_and_herbivores\n          '12': medium_mammals\n          '13': non-insect_invertebrates\n          '14': people\n          '15': reptiles\n          '16': small_mammals\n          '17': trees\n          '18': vehicles_1\n          '19': vehicles_2\n  splits:\n  - name: train\n    num_bytes: 112545106.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22564261.0\n    num_examples: 10000\n  download_size: 142291368\n  dataset_size: 135109367.0\nconfigs:\n- config_name: cifar100\n  data_files:\n  - split: train\n    path: cifar100/train-*\n  - split: test\n    path: cifar100/test-*\n  default: true\n---\n \n# Dataset Card for CIFAR-100\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** [CIFAR Datasets](https://www.cs.toronto.edu/~kriz/cifar.html)\n- **Repository:** \n- **Paper:** [Paper](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)\n- **Leaderboard:**\n- **Point of Contact:**\n \n### Dataset Summary\n \nThe CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images\nper class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses. \nThere are two labels per image - fine label (actual class) and coarse label (superclass).\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 100 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-100).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x2767F58E080>, 'fine_label': 19,\n  'coarse_label': 11\n}\n```\n\n### Data Fields\n\n- `img`: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`\n- `fine_label`: an `int` classification label with the following mapping:\n\n  `0`: apple\n\n  `1`: aquarium_fish\n\n  `2`: baby\n\n  `3`: bear\n\n  `4`: beaver\n  \n  `5`: bed\n\n  `6`: bee\n\n  `7`: beetle\n\n  `8`: bicycle\n\n  `9`: bottle\n\n  `10`: bowl\n\n  `11`: boy\n\n  `12`: bridge\n\n  `13`: bus\n\n  `14`: butterfly\n\n  `15`: camel\n\n  `16`: can\n\n  `17`: castle\n\n  `18`: caterpillar\n\n  `19`: cattle\n\n  `20`: chair\n\n  `21`: chimpanzee\n\n  `22`: clock\n\n  `23`: cloud\n\n  `24`: cockroach\n\n  `25`: couch\n\n  `26`: cra\n\n  `27`: crocodile\n\n  `28`: cup\n\n  `29`: dinosaur\n\n  `30`: dolphin\n\n  `31`: elephant\n\n  `32`: flatfish\n\n  `33`: forest\n\n  `34`: fox\n\n  `35`: girl\n\n  `36`: hamster\n\n  `37`: house\n\n  `38`: kangaroo\n\n  `39`: keyboard\n\n  `40`: lamp\n\n  `41`: lawn_mower\n\n  `42`: leopard\n\n  `43`: lion\n\n  `44`: lizard\n\n  `45`: lobster\n\n  `46`: man\n\n  `47`: maple_tree\n\n  `48`: motorcycle\n\n  `49`: mountain\n\n  `50`: mouse\n\n  `51`: mushroom\n\n  `52`: oak_tree\n\n  `53`: orange\n\n  `54`: orchid\n\n  `55`: otter\n\n  `56`: palm_tree\n\n  `57`: pear\n\n  `58`: pickup_truck\n\n  `59`: pine_tree\n\n  `60`: plain\n\n  `61`: plate\n\n  `62`: poppy\n\n  `63`: porcupine\n\n  `64`: possum\n\n  `65`: rabbit\n\n  `66`: raccoon\n\n  `67`: ray\n\n  `68`: road\n\n  `69`: rocket\n\n  `70`: rose\n\n  `71`: sea\n\n  `72`: seal\n\n  `73`: shark\n\n  `74`: shrew\n\n  `75`: skunk\n\n  `76`: skyscraper\n\n  `77`: snail\n\n  `78`: snake\n\n  `79`: spider\n\n  `80`: squirrel\n\n  `81`: streetcar\n\n  `82`: sunflower\n\n  `83`: sweet_pepper\n\n  `84`: table\n\n  `85`: tank\n\n  `86`: telephone\n\n  `87`: television\n\n  `88`: tiger\n\n  `89`: tractor\n\n  `90`: train\n\n  `91`: trout\n\n  `92`: tulip\n\n  `93`: turtle\n\n  `94`: wardrobe\n\n  `95`: whale\n\n  `96`: willow_tree\n\n  `97`: wolf\n\n  `98`: woman\n\n  `99`: worm\n\n- `coarse_label`: an `int` coarse classification label with following mapping:\n\n  `0`: aquatic_mammals\n\n  `1`: fish\n\n  `2`: flowers\n\n  `3`: food_containers\n\n  `4`: fruit_and_vegetables\n\n  `5`: household_electrical_devices\n\n  `6`: household_furniture\n\n  `7`: insects\n\n  `8`: large_carnivores\n\n  `9`: large_man-made_outdoor_things\n\n  `10`: large_natural_outdoor_scenes\n\n  `11`: large_omnivores_and_herbivores\n\n  `12`: medium_mammals\n\n  `13`: non-insect_invertebrates\n\n  `14`: people\n\n  `15`: reptiles\n\n  `16`: small_mammals\n\n  `17`: trees\n\n  `18`: vehicles_1\n\n  `19`: vehicles_2\n\n\n \n### Data Splits\n \n|   name   |train|test|\n|----------|----:|---------:|\n|cifar100|50000|     10000|\n \n## Dataset Creation\n \n### Curation Rationale\n \n[More Information Needed]\n \n### Source Data\n \n#### Initial Data Collection and Normalization\n \n[More Information Needed]\n \n#### Who are the source language producers?\n \n[More Information Needed]\n \n### Annotations\n \n#### Annotation process\n \n[More Information Needed]\n \n#### Who are the annotators?\n \n[More Information Needed]\n \n### Personal and Sensitive Information\n \n[More Information Needed]\n \n## Considerations for Using the Data\n \n### Social Impact of Dataset\n \n[More Information Needed]\n \n### Discussion of Biases\n \n[More Information Needed]\n \n### Other Known Limitations\n \n[More Information Needed]\n \n## Additional Information\n \n### Dataset Curators\n \n[More Information Needed]\n \n### Licensing Information\n \n[More Information Needed]\n \n### Citation Information\n \n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@gchhablani](https://github.com/gchablani) for adding this dataset.",
              "extracted_code": ""
            },
            {
              "id": "tanganke/cifar100",
              "author": "tanganke",
              "sha": "7de7416777eb0bca0194ab35ac1c1b297f1d6b26",
              "created_at": "2024-09-23T13:10:03+00:00",
              "last_modified": "2024-09-23T13:10:15+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 1996,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "data/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "data/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\ndataset_info:\n  features:\n  - name: image\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': apple\n          '1': aquarium_fish\n          '2': baby\n          '3': bear\n          '4': beaver\n          '5': bed\n          '6': bee\n          '7': beetle\n          '8': bicycle\n          '9': bottle\n          '10': bowl\n          '11': boy\n          '12': bridge\n          '13': bus\n          '14': butterfly\n          '15': camel\n          '16': can\n          '17': castle\n          '18': caterpillar\n          '19': cattle\n          '20': chair\n          '21': chimpanzee\n          '22': clock\n          '23': cloud\n          '24': cockroach\n          '25': couch\n          '26': cra\n          '27': crocodile\n          '28': cup\n          '29': dinosaur\n          '30': dolphin\n          '31': elephant\n          '32': flatfish\n          '33': forest\n          '34': fox\n          '35': girl\n          '36': hamster\n          '37': house\n          '38': kangaroo\n          '39': keyboard\n          '40': lamp\n          '41': lawn_mower\n          '42': leopard\n          '43': lion\n          '44': lizard\n          '45': lobster\n          '46': man\n          '47': maple_tree\n          '48': motorcycle\n          '49': mountain\n          '50': mouse\n          '51': mushroom\n          '52': oak_tree\n          '53': orange\n          '54': orchid\n          '55': otter\n          '56': palm_tree\n          '57': pear\n          '58': pickup_truck\n          '59': pine_tree\n          '60': plain\n          '61': plate\n          '62': poppy\n          '63': porcupine\n          '64': possum\n          '65': rabbit\n          '66': raccoon\n          '67': ray\n          '68': road\n          '69': rocket\n          '70': rose\n          '71': sea\n          '72': seal\n          '73': shark\n          '74': shrew\n          '75': skunk\n          '76': skyscraper\n          '77': snail\n          '78': snake\n          '79': spider\n          '80': squirrel\n          '81': streetcar\n          '82': sunflower\n          '83': sweet_pepper\n          '84': table\n          '85': tank\n          '86': telephone\n          '87': television\n          '88': tiger\n          '89': tractor\n          '90': train\n          '91': trout\n          '92': tulip\n          '93': turtle\n          '94': wardrobe\n          '95': whale\n          '96': willow_tree\n          '97': wolf\n          '98': woman\n          '99': worm\n  splits:\n  - name: train\n    num_bytes: 112145106.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22484261.0\n    num_examples: 10000\n  download_size: 142006370\n  dataset_size: 134629367.0\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: test\n    path: data/test-*\n---\n",
              "extracted_code": ""
            },
            {
              "id": "tanganke/cifar10",
              "author": "tanganke",
              "sha": "97b332c7b2981f9decda4ba9abac76ef18bb0636",
              "created_at": "2024-09-23T13:11:52+00:00",
              "last_modified": "2024-09-23T13:12:03+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 1636,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "data/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "data/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\ndataset_info:\n  features:\n  - name: image\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': airplane\n          '1': automobile\n          '2': bird\n          '3': cat\n          '4': deer\n          '5': dog\n          '6': frog\n          '7': horse\n          '8': ship\n          '9': truck\n  splits:\n  - name: train\n    num_bytes: 113648310.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22731580.0\n    num_examples: 10000\n  download_size: 143650937\n  dataset_size: 136379890.0\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: test\n    path: data/test-*\n---\n",
              "extracted_code": ""
            },
            {
              "id": "uoft-cs/cifar100",
              "author": "uoft-cs",
              "sha": "aadb3af77e9048adbea6b47c21a81e47dd092ae5",
              "created_at": "2022-03-02T23:29:22+00:00",
              "last_modified": "2024-01-04T06:57:47+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 12540,
              "likes": 53,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "cifar100/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "cifar100/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "license": [
                  "unknown"
                ],
                "language": [
                  "en"
                ],
                "tags": [],
                "datasets": [],
                "task_categories": [
                  "image-classification"
                ],
                "size_categories": [
                  "10K<n<100K"
                ],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "task_categories:image-classification",
                "annotations_creators:crowdsourced",
                "language_creators:found",
                "multilinguality:monolingual",
                "source_datasets:extended|other-80-Million-Tiny-Images",
                "language:en",
                "license:unknown",
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-100\npretty_name: Cifar100\ndataset_info:\n  config_name: cifar100\n  features:\n  - name: img\n    dtype: image\n  - name: fine_label\n    dtype:\n      class_label:\n        names:\n          '0': apple\n          '1': aquarium_fish\n          '2': baby\n          '3': bear\n          '4': beaver\n          '5': bed\n          '6': bee\n          '7': beetle\n          '8': bicycle\n          '9': bottle\n          '10': bowl\n          '11': boy\n          '12': bridge\n          '13': bus\n          '14': butterfly\n          '15': camel\n          '16': can\n          '17': castle\n          '18': caterpillar\n          '19': cattle\n          '20': chair\n          '21': chimpanzee\n          '22': clock\n          '23': cloud\n          '24': cockroach\n          '25': couch\n          '26': cra\n          '27': crocodile\n          '28': cup\n          '29': dinosaur\n          '30': dolphin\n          '31': elephant\n          '32': flatfish\n          '33': forest\n          '34': fox\n          '35': girl\n          '36': hamster\n          '37': house\n          '38': kangaroo\n          '39': keyboard\n          '40': lamp\n          '41': lawn_mower\n          '42': leopard\n          '43': lion\n          '44': lizard\n          '45': lobster\n          '46': man\n          '47': maple_tree\n          '48': motorcycle\n          '49': mountain\n          '50': mouse\n          '51': mushroom\n          '52': oak_tree\n          '53': orange\n          '54': orchid\n          '55': otter\n          '56': palm_tree\n          '57': pear\n          '58': pickup_truck\n          '59': pine_tree\n          '60': plain\n          '61': plate\n          '62': poppy\n          '63': porcupine\n          '64': possum\n          '65': rabbit\n          '66': raccoon\n          '67': ray\n          '68': road\n          '69': rocket\n          '70': rose\n          '71': sea\n          '72': seal\n          '73': shark\n          '74': shrew\n          '75': skunk\n          '76': skyscraper\n          '77': snail\n          '78': snake\n          '79': spider\n          '80': squirrel\n          '81': streetcar\n          '82': sunflower\n          '83': sweet_pepper\n          '84': table\n          '85': tank\n          '86': telephone\n          '87': television\n          '88': tiger\n          '89': tractor\n          '90': train\n          '91': trout\n          '92': tulip\n          '93': turtle\n          '94': wardrobe\n          '95': whale\n          '96': willow_tree\n          '97': wolf\n          '98': woman\n          '99': worm\n  - name: coarse_label\n    dtype:\n      class_label:\n        names:\n          '0': aquatic_mammals\n          '1': fish\n          '2': flowers\n          '3': food_containers\n          '4': fruit_and_vegetables\n          '5': household_electrical_devices\n          '6': household_furniture\n          '7': insects\n          '8': large_carnivores\n          '9': large_man-made_outdoor_things\n          '10': large_natural_outdoor_scenes\n          '11': large_omnivores_and_herbivores\n          '12': medium_mammals\n          '13': non-insect_invertebrates\n          '14': people\n          '15': reptiles\n          '16': small_mammals\n          '17': trees\n          '18': vehicles_1\n          '19': vehicles_2\n  splits:\n  - name: train\n    num_bytes: 112545106.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22564261.0\n    num_examples: 10000\n  download_size: 142291368\n  dataset_size: 135109367.0\nconfigs:\n- config_name: cifar100\n  data_files:\n  - split: train\n    path: cifar100/train-*\n  - split: test\n    path: cifar100/test-*\n  default: true\n---\n \n# Dataset Card for CIFAR-100\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** [CIFAR Datasets](https://www.cs.toronto.edu/~kriz/cifar.html)\n- **Repository:** \n- **Paper:** [Paper](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)\n- **Leaderboard:**\n- **Point of Contact:**\n \n### Dataset Summary\n \nThe CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images\nper class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses. \nThere are two labels per image - fine label (actual class) and coarse label (superclass).\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 100 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-100).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x2767F58E080>, 'fine_label': 19,\n  'coarse_label': 11\n}\n```\n\n### Data Fields\n\n- `img`: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`\n- `fine_label`: an `int` classification label with the following mapping:\n\n  `0`: apple\n\n  `1`: aquarium_fish\n\n  `2`: baby\n\n  `3`: bear\n\n  `4`: beaver\n  \n  `5`: bed\n\n  `6`: bee\n\n  `7`: beetle\n\n  `8`: bicycle\n\n  `9`: bottle\n\n  `10`: bowl\n\n  `11`: boy\n\n  `12`: bridge\n\n  `13`: bus\n\n  `14`: butterfly\n\n  `15`: camel\n\n  `16`: can\n\n  `17`: castle\n\n  `18`: caterpillar\n\n  `19`: cattle\n\n  `20`: chair\n\n  `21`: chimpanzee\n\n  `22`: clock\n\n  `23`: cloud\n\n  `24`: cockroach\n\n  `25`: couch\n\n  `26`: cra\n\n  `27`: crocodile\n\n  `28`: cup\n\n  `29`: dinosaur\n\n  `30`: dolphin\n\n  `31`: elephant\n\n  `32`: flatfish\n\n  `33`: forest\n\n  `34`: fox\n\n  `35`: girl\n\n  `36`: hamster\n\n  `37`: house\n\n  `38`: kangaroo\n\n  `39`: keyboard\n\n  `40`: lamp\n\n  `41`: lawn_mower\n\n  `42`: leopard\n\n  `43`: lion\n\n  `44`: lizard\n\n  `45`: lobster\n\n  `46`: man\n\n  `47`: maple_tree\n\n  `48`: motorcycle\n\n  `49`: mountain\n\n  `50`: mouse\n\n  `51`: mushroom\n\n  `52`: oak_tree\n\n  `53`: orange\n\n  `54`: orchid\n\n  `55`: otter\n\n  `56`: palm_tree\n\n  `57`: pear\n\n  `58`: pickup_truck\n\n  `59`: pine_tree\n\n  `60`: plain\n\n  `61`: plate\n\n  `62`: poppy\n\n  `63`: porcupine\n\n  `64`: possum\n\n  `65`: rabbit\n\n  `66`: raccoon\n\n  `67`: ray\n\n  `68`: road\n\n  `69`: rocket\n\n  `70`: rose\n\n  `71`: sea\n\n  `72`: seal\n\n  `73`: shark\n\n  `74`: shrew\n\n  `75`: skunk\n\n  `76`: skyscraper\n\n  `77`: snail\n\n  `78`: snake\n\n  `79`: spider\n\n  `80`: squirrel\n\n  `81`: streetcar\n\n  `82`: sunflower\n\n  `83`: sweet_pepper\n\n  `84`: table\n\n  `85`: tank\n\n  `86`: telephone\n\n  `87`: television\n\n  `88`: tiger\n\n  `89`: tractor\n\n  `90`: train\n\n  `91`: trout\n\n  `92`: tulip\n\n  `93`: turtle\n\n  `94`: wardrobe\n\n  `95`: whale\n\n  `96`: willow_tree\n\n  `97`: wolf\n\n  `98`: woman\n\n  `99`: worm\n\n- `coarse_label`: an `int` coarse classification label with following mapping:\n\n  `0`: aquatic_mammals\n\n  `1`: fish\n\n  `2`: flowers\n\n  `3`: food_containers\n\n  `4`: fruit_and_vegetables\n\n  `5`: household_electrical_devices\n\n  `6`: household_furniture\n\n  `7`: insects\n\n  `8`: large_carnivores\n\n  `9`: large_man-made_outdoor_things\n\n  `10`: large_natural_outdoor_scenes\n\n  `11`: large_omnivores_and_herbivores\n\n  `12`: medium_mammals\n\n  `13`: non-insect_invertebrates\n\n  `14`: people\n\n  `15`: reptiles\n\n  `16`: small_mammals\n\n  `17`: trees\n\n  `18`: vehicles_1\n\n  `19`: vehicles_2\n\n\n \n### Data Splits\n \n|   name   |train|test|\n|----------|----:|---------:|\n|cifar100|50000|     10000|\n \n## Dataset Creation\n \n### Curation Rationale\n \n[More Information Needed]\n \n### Source Data\n \n#### Initial Data Collection and Normalization\n \n[More Information Needed]\n \n#### Who are the source language producers?\n \n[More Information Needed]\n \n### Annotations\n \n#### Annotation process\n \n[More Information Needed]\n \n#### Who are the annotators?\n \n[More Information Needed]\n \n### Personal and Sensitive Information\n \n[More Information Needed]\n \n## Considerations for Using the Data\n \n### Social Impact of Dataset\n \n[More Information Needed]\n \n### Discussion of Biases\n \n[More Information Needed]\n \n### Other Known Limitations\n \n[More Information Needed]\n \n## Additional Information\n \n### Dataset Curators\n \n[More Information Needed]\n \n### Licensing Information\n \n[More Information Needed]\n \n### Citation Information\n \n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@gchhablani](https://github.com/gchablani) for adding this dataset.",
              "extracted_code": ""
            },
            {
              "id": "tanganke/cifar100",
              "author": "tanganke",
              "sha": "7de7416777eb0bca0194ab35ac1c1b297f1d6b26",
              "created_at": "2024-09-23T13:10:03+00:00",
              "last_modified": "2024-09-23T13:10:15+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 1996,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "data/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "data/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\ndataset_info:\n  features:\n  - name: image\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': apple\n          '1': aquarium_fish\n          '2': baby\n          '3': bear\n          '4': beaver\n          '5': bed\n          '6': bee\n          '7': beetle\n          '8': bicycle\n          '9': bottle\n          '10': bowl\n          '11': boy\n          '12': bridge\n          '13': bus\n          '14': butterfly\n          '15': camel\n          '16': can\n          '17': castle\n          '18': caterpillar\n          '19': cattle\n          '20': chair\n          '21': chimpanzee\n          '22': clock\n          '23': cloud\n          '24': cockroach\n          '25': couch\n          '26': cra\n          '27': crocodile\n          '28': cup\n          '29': dinosaur\n          '30': dolphin\n          '31': elephant\n          '32': flatfish\n          '33': forest\n          '34': fox\n          '35': girl\n          '36': hamster\n          '37': house\n          '38': kangaroo\n          '39': keyboard\n          '40': lamp\n          '41': lawn_mower\n          '42': leopard\n          '43': lion\n          '44': lizard\n          '45': lobster\n          '46': man\n          '47': maple_tree\n          '48': motorcycle\n          '49': mountain\n          '50': mouse\n          '51': mushroom\n          '52': oak_tree\n          '53': orange\n          '54': orchid\n          '55': otter\n          '56': palm_tree\n          '57': pear\n          '58': pickup_truck\n          '59': pine_tree\n          '60': plain\n          '61': plate\n          '62': poppy\n          '63': porcupine\n          '64': possum\n          '65': rabbit\n          '66': raccoon\n          '67': ray\n          '68': road\n          '69': rocket\n          '70': rose\n          '71': sea\n          '72': seal\n          '73': shark\n          '74': shrew\n          '75': skunk\n          '76': skyscraper\n          '77': snail\n          '78': snake\n          '79': spider\n          '80': squirrel\n          '81': streetcar\n          '82': sunflower\n          '83': sweet_pepper\n          '84': table\n          '85': tank\n          '86': telephone\n          '87': television\n          '88': tiger\n          '89': tractor\n          '90': train\n          '91': trout\n          '92': tulip\n          '93': turtle\n          '94': wardrobe\n          '95': whale\n          '96': willow_tree\n          '97': wolf\n          '98': woman\n          '99': worm\n  splits:\n  - name: train\n    num_bytes: 112145106.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22484261.0\n    num_examples: 10000\n  download_size: 142006370\n  dataset_size: 134629367.0\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: test\n    path: data/test-*\n---\n",
              "extracted_code": ""
            },
            {
              "id": "yehzw/wikitext-103",
              "author": "yehzw",
              "sha": "b05eb821ccd6b3633dd62a5bd32c1a218d0cc44b",
              "created_at": "2024-08-08T11:56:11+00:00",
              "last_modified": "2024-08-08T12:37:29+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 301,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "clean/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "clean/train-00000-of-00002.parquet"
                },
                {
                  "rfilename": "clean/train-00001-of-00002.parquet"
                },
                {
                  "rfilename": "clean/validation-00000-of-00001.parquet"
                },
                {
                  "rfilename": "raw/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "raw/train-00000-of-00002.parquet"
                },
                {
                  "rfilename": "raw/train-00001-of-00002.parquet"
                },
                {
                  "rfilename": "raw/validation-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "size_categories:1M<n<10M",
                "format:parquet",
                "modality:text",
                "library:datasets",
                "library:dask",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\ndataset_info:\n- config_name: clean\n  features:\n  - name: text\n    sequence: string\n  splits:\n  - name: train\n    num_bytes: 830910180\n    num_examples: 3805842\n  - name: validation\n    num_bytes: 1760396\n    num_examples: 8139\n  - name: test\n    num_bytes: 1979602\n    num_examples: 9421\n  download_size: 311802693\n  dataset_size: 834650178\n- config_name: raw\n  features:\n  - name: text\n    dtype: string\n  splits:\n  - name: train\n    num_bytes: 519638274\n    num_examples: 29567\n  - name: validation\n    num_bytes: 1103823\n    num_examples: 60\n  - name: test\n    num_bytes: 1241003\n    num_examples: 62\n  download_size: 301810546\n  dataset_size: 521983100\nconfigs:\n- config_name: clean\n  data_files:\n  - split: train\n    path: clean/train-*\n  - split: validation\n    path: clean/validation-*\n  - split: test\n    path: clean/test-*\n- config_name: raw\n  data_files:\n  - split: train\n    path: raw/train-*\n  - split: validation\n    path: raw/validation-*\n  - split: test\n    path: raw/test-*\n---\n",
              "extracted_code": ""
            },
            {
              "id": "vish26/wikitext-103-v1-cleaned",
              "author": "vish26",
              "sha": "a338a0c2ec0a947b7cf5b53a480fae950a8923e2",
              "created_at": "2025-09-18T13:09:24+00:00",
              "last_modified": "2025-09-18T13:15:26+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 104,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "data/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "data/train-00000-of-00002.parquet"
                },
                {
                  "rfilename": "data/train-00001-of-00002.parquet"
                },
                {
                  "rfilename": "data/validation-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "size_categories:1M<n<10M",
                "format:parquet",
                "modality:text",
                "library:datasets",
                "library:dask",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\ndataset_info:\n  features:\n  - name: text\n    dtype: string\n  splits:\n  - name: test\n    num_bytes: 1286902\n    num_examples: 4358\n  - name: train\n    num_bytes: 541646398\n    num_examples: 1801350\n  - name: validation\n    num_bytes: 1147368\n    num_examples: 3760\n  download_size: 304529656\n  dataset_size: 544080668\nconfigs:\n- config_name: default\n  data_files:\n  - split: test\n    path: data/test-*\n  - split: train\n    path: data/train-*\n  - split: validation\n    path: data/validation-*\n---\n",
              "extracted_code": ""
            }
          ]
        }
      },
      "base_code": {
        "train_py": "\"\"\"src/train.py\nCore training script for a single experimental run.\nImplements end-to-end training, validation, checkpointing, figure generation and\nmetrics logging.  ALL dataset / model specialisation must be supplied via the\nconfiguration handed in by main.py.\n\nUsage (called only by main.py):\n    python -m src.train \\\n        --config-file <path/to/config.yaml> \\\n        --results-dir <path/to/results_dir>\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport yaml\n\nfrom . import preprocess as pp  # type: ignore\nfrom . import model as mdl  # type: ignore\n\n# ----------------------------- Utility functions ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n    \"\"\"Compute top-1 accuracy for classification.\"\"\"\n    pred_label = pred.argmax(dim=1)\n    correct = (pred_label == target).sum().item()\n    return correct / target.size(0)\n\n\n# --------------------------------- Training --------------------------------- #\n\ndef train_one_epoch(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n) -> tuple[float, float]:\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        running_acc += accuracy(outputs.detach(), targets.detach()) * inputs.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = running_acc / len(dataloader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef validate(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    device: torch.device,\n) -> tuple[float, float]:\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n            running_acc += accuracy(outputs, targets) * inputs.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = running_acc / len(dataloader.dataset)\n    return epoch_loss, epoch_acc\n\n\n# --------------------------------- Figures ---------------------------------- #\n\ndef save_training_curves(\n    metrics: list[dict[str, float]],\n    run_id: str,\n    results_dir: str,\n) -> None:\n    images_dir = Path(results_dir) / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    epochs = [m[\"epoch\"] for m in metrics]\n    train_losses = [m[\"train_loss\"] for m in metrics]\n    val_losses = [m[\"val_loss\"] for m in metrics]\n    train_accs = [m[\"train_acc\"] for m in metrics]\n    val_accs = [m[\"val_acc\"] for m in metrics]\n\n    sns.set_style(\"whitegrid\")\n\n    # Loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, train_losses, label=\"train_loss\")\n    plt.plot(epochs, val_losses, label=\"val_loss\")\n    plt.scatter(epochs[-1], val_losses[-1], color=\"red\")\n    plt.text(\n        epochs[-1],\n        val_losses[-1],\n        f\"{val_losses[-1]:.3f}\",\n        fontsize=8,\n        verticalalignment=\"bottom\",\n    )\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.title(f\"Training/Validation Loss ({run_id})\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Accuracy curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, train_accs, label=\"train_acc\")\n    plt.plot(epochs, val_accs, label=\"val_acc\")\n    plt.scatter(epochs[-1], val_accs[-1], color=\"red\")\n    plt.text(\n        epochs[-1],\n        val_accs[-1],\n        f\"{val_accs[-1]*100:.2f}%\",\n        fontsize=8,\n        verticalalignment=\"bottom\",\n    )\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"accuracy\")\n    plt.title(f\"Training/Validation Accuracy ({run_id})\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"accuracy_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# --------------------------------- Main ------------------------------------- #\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config-file\", required=True, type=str, help=\"Path to YAML config for this run\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Root directory for all experiment outputs\")\n    args = parser.parse_args()\n\n    # ---------------------------------------------------------------------\n    # Configuration & directories\n    # ---------------------------------------------------------------------\n    with open(args.config_file, \"r\", encoding=\"utf-8\") as f:\n        cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    run_id: str = cfg[\"run_id\"]\n    run_dir = Path(args.results_dir) / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    checkpoints_dir = run_dir / \"checkpoints\"\n    checkpoints_dir.mkdir(exist_ok=True)\n\n    # Persist a copy of the resolved configuration for reproducibility\n    with open(run_dir / \"config.yaml\", \"w\", encoding=\"utf-8\") as f_cfg_out:\n        yaml.safe_dump(cfg, f_cfg_out)\n\n    seed = int(cfg.get(\"seed\", 42))\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # Data pipeline\n    # ---------------------------------------------------------------------\n    train_loader = pp.get_dataloader(cfg, split=\"train\")\n    val_loader = pp.get_dataloader(cfg, split=\"val\")\n\n    # ---------------------------------------------------------------------\n    # Model & optimisation\n    # ---------------------------------------------------------------------\n    model = mdl.create_model(cfg).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    optim_cfg = cfg.get(\"optimizer\", {})\n    opt_type = optim_cfg.get(\"type\", \"SGD\").upper()\n    lr = float(optim_cfg.get(\"lr\", 0.01))\n    momentum = float(optim_cfg.get(\"momentum\", 0.9))\n    weight_decay = float(optim_cfg.get(\"weight_decay\", 0.0))\n\n    if opt_type == \"ADAM\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:  # Default to SGD\n        optimizer = optim.SGD(\n            model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n        )\n\n    epochs: int = int(cfg.get(\"training\", {}).get(\"epochs\", 10))\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    epoch_metrics: list[dict[str, float]] = []\n    best_val_acc = -1.0\n    start_time = time.time()\n\n    for epoch in range(1, epochs + 1):\n        train_loss, train_acc = train_one_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n        # Book-keeping\n        metric_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }\n        epoch_metrics.append(metric_dict)\n\n        # Checkpointing\n        ckpt_path = checkpoints_dir / f\"epoch_{epoch}.pt\"\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            },\n            ckpt_path,\n        )\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), run_dir / \"best_model.pt\")\n\n        # Progress feedback (the CI system captures this)\n        print(json.dumps({\"run_id\": run_id, \"epoch\": epoch, **metric_dict}), flush=True)\n\n    elapsed = time.time() - start_time\n\n    # Final serialization of metrics\n    final_metrics = epoch_metrics[-1]\n    results_summary: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"final_metrics\": final_metrics,\n        \"epoch_metrics\": epoch_metrics,\n        \"training_time_sec\": elapsed,\n    }\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as f_res:\n        json.dump(results_summary, f_res, indent=2)\n\n    # Generate publication-ready figures\n    save_training_curves(epoch_metrics, run_id, args.results_dir)\n\n    # Print the summary to stdout last – required by evaluation harness\n    print(json.dumps(results_summary), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all experimental runs stored in `results_dir/*/results.json`,\ncomputes summary statistics and generates cross-run comparison figures.\n\nUsage:\n    python -m src.evaluate --results-dir <path/to/experiments>\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ----------------------------- Helper utilities ----------------------------- #\n\ndef load_run_results(results_path: str) -> Dict[str, Any]:\n    with open(results_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\n\n# --------------------------------- Metrics ---------------------------------- #\n\ndef aggregate_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n    summary = {\n        \"num_runs\": len(results),\n        \"runs\": {r[\"run_id\"]: r[\"final_metrics\"] for r in results},\n    }\n    # Identify best run by validation accuracy\n    best_run = max(results, key=lambda r: r[\"final_metrics\"][\"val_acc\"])\n    summary[\"best_run_id\"] = best_run[\"run_id\"]\n    summary[\"best_val_acc\"] = best_run[\"final_metrics\"][\"val_acc\"]\n    return summary\n\n\n# -------------------------------- Figures ----------------------------------- #\n\ndef save_comparison_figures(results: List[Dict[str, Any]], results_dir: str) -> None:\n    images_dir = Path(results_dir) / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Bar chart of final validation accuracy\n    run_ids = [r[\"run_id\"] for r in results]\n    val_accs = [r[\"final_metrics\"][\"val_acc\"] for r in results]\n\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=run_ids, y=val_accs, palette=\"viridis\")\n    for idx, val in enumerate(val_accs):\n        plt.text(idx, val + 0.005, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.ylabel(\"validation accuracy\")\n    plt.title(\"Final Validation Accuracy Across Runs\")\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy_comparison.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Line graph of validation accuracy over epochs\n    plt.figure(figsize=(6, 4))\n    for r in results:\n        epochs = [m[\"epoch\"] for m in r[\"epoch_metrics\"]]\n        val_accs_epoch = [m[\"val_acc\"] for m in r[\"epoch_metrics\"]]\n        plt.plot(epochs, val_accs_epoch, label=r[\"run_id\"])\n        plt.scatter(epochs[-1], val_accs_epoch[-1])\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"validation accuracy\")\n    plt.title(\"Validation Accuracy Trajectories\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy_trajectories.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# --------------------------------- Main ------------------------------------- #\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare experimental runs\")\n    parser.add_argument(\"--results-dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    # --------------------------------------------------------------------------------\n    # Load results.json from every run directory\n    # --------------------------------------------------------------------------------\n    result_files = glob(os.path.join(args.results_dir, \"*\", \"results.json\"))\n    if len(result_files) == 0:\n        raise FileNotFoundError(f\"No results.json files found under {args.results_dir}\")\n\n    results = [load_run_results(p) for p in sorted(result_files)]\n\n    # Aggregate and summarise\n    summary = aggregate_metrics(results)\n\n    # Generate figures\n    save_comparison_figures(results, args.results_dir)\n\n    # Final JSON summary to stdout\n    print(json.dumps(summary, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "preprocess_py": "\"\"\"src/preprocess.py\nCommon data loading and preprocessing pipeline with *dataset placeholders*.\nOnly the generic mechanics are fully implemented here.  Any concrete dataset\nintegration must be supplied during the dataset-specific derivation step.\n\nCurrent supported placeholder dataset:\n    RANDOM_PLACEHOLDER – synthetic data used for smoke tests.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Tuple, Any\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------------- Placeholders ------------------------------ #\n\nclass RandomClassificationDataset(Dataset):\n    \"\"\"Synthetic dataset that produces random tensors for classification.\n\n    This is *only* intended for smoke tests; real experiments must provide\n    a concrete dataset loader by replacing the placeholder in config.\n    \"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, ...], num_classes: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        x = torch.randn(self.input_shape)\n        y = torch.randint(0, self.num_classes, (1,)).long().squeeze()\n        return x, y\n\n\n# -------------------------- Dataloader factory ------------------------------ #\n\ndef _create_placeholder_dataset(cfg: dict, split: str):\n    ds_cfg = cfg[\"dataset\"]\n    name = ds_cfg[\"name\"].upper()\n\n    if name == \"RANDOM_PLACEHOLDER\":\n        n_train = int(ds_cfg.get(\"num_samples\", 1024))\n        n_val = max(1, math.ceil(n_train * 0.2))\n        if split == \"train\":\n            dataset = RandomClassificationDataset(\n                num_samples=n_train,\n                input_shape=tuple(ds_cfg.get(\"input_shape\", [1, 28, 28])),\n                num_classes=int(ds_cfg.get(\"num_classes\", 10)),\n            )\n        else:\n            dataset = RandomClassificationDataset(\n                num_samples=n_val,\n                input_shape=tuple(ds_cfg.get(\"input_shape\", [1, 28, 28])),\n                num_classes=int(ds_cfg.get(\"num_classes\", 10)),\n            )\n        return dataset\n\n    # ---------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    # ---------------------------------------------------------------------\n    raise NotImplementedError(\n        f\"Dataset '{name}' not implemented in common core – must be provided in specialising step.\"\n    )\n\n\ndef get_dataloader(cfg: dict, split: str = \"train\") -> DataLoader:\n    \"\"\"Return a PyTorch DataLoader for requested split.\n\n    Parameters\n    ----------\n    cfg : dict\n        Run-level configuration dictionary.\n    split : str\n        One of {\"train\", \"val\"}.\n    \"\"\"\n    assert split in {\"train\", \"val\"}, \"split must be 'train' or 'val'\"\n    ds = _create_placeholder_dataset(cfg, split)\n\n    batch_size = int(cfg.get(\"training\", {}).get(\"batch_size\", 32))\n    shuffle = split == \"train\"\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n",
        "model_py": "\"\"\"src/model.py\nModel architecture registry.  Contains *baseline* implementation that works for\nany generic classification smoke test.  Real experiments will replace or extend\nthese models.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# ----------------------------- Base classifier ------------------------------ #\n\nclass BaseClassifier(nn.Module):\n    \"\"\"Very small CNN suitable for 1×28×28 images (MNIST-like).\"\"\"\n\n    def __init__(self, in_channels: int = 1, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 8, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# ------------------------------- Registry ----------------------------------- #\n\n_MODEL_REGISTRY: Dict[str, nn.Module] = {\n    \"BASE_CLASSIFIER\": BaseClassifier,\n    # ---------------------------------------------------------------------\n    # PLACEHOLDER: Additional models will be registered here in later phase\n    # ---------------------------------------------------------------------\n}\n\n\ndef create_model(cfg: dict) -> nn.Module:\n    mdl_cfg = cfg.get(\"model\", {})\n    name = mdl_cfg.get(\"name\", \"BASE_CLASSIFIER\").upper()\n    if name not in _MODEL_REGISTRY:\n        raise NotImplementedError(\n            f\"Model '{name}' unknown to common core – must be provided in specialising step.\"\n        )\n    kwargs = mdl_cfg.get(\"kwargs\", {})\n    return _MODEL_REGISTRY[name](**kwargs)\n\n\ndef save_model(model: nn.Module, path: str) -> None:\n    torch.save(model.state_dict(), path)\n\n\ndef load_model(model_class_name: str, path: str, device: torch.device) -> nn.Module:\n    model_class_name = model_class_name.upper()\n    if model_class_name not in _MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model class '{model_class_name}' for loading\")\n    model = _MODEL_REGISTRY[model_class_name]()\n    state_dict = torch.load(path, map_location=device)\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n    return model\n",
        "main_py": "\"\"\"src/main.py\nCentral orchestrator that sequentially executes all run variations defined in\n`smoke_test.yaml` or `full_experiment.yaml`, captures their output & error\nstreams, and finally triggers evaluation.\n\nCLI:\n    python -m src.main --smoke-test --results-dir <path>\n    python -m src.main --full-experiment --results-dir <path>\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport yaml\n\n# ------------------------- Subprocess streaming utils ----------------------- #\n\ndef _stream(pipe, tee_files):\n    \"\"\"Read lines from a pipe and write them to multiple file-like objects.\"\"\"\n    with pipe:\n        for line in iter(pipe.readline, b\"\"):\n            decoded = line.decode()\n            for f in tee_files:\n                f.write(decoded)\n                f.flush()\n            # Always mirror to main process' stdout / stderr\n            if tee_files[0].name.endswith(\"stdout.log\"):\n                sys.stdout.write(decoded)\n                sys.stdout.flush()\n            else:\n                sys.stderr.write(decoded)\n                sys.stderr.flush()\n\n\ndef _run_subprocess(cmd: list[str], stdout_log: Path, stderr_log: Path) -> int:\n    \"\"\"Launch subprocess, tee stdout/err to provided log files *and* console.\"\"\"\n    stdout_log.parent.mkdir(parents=True, exist_ok=True)\n    stderr_log.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(stdout_log, \"w\", encoding=\"utf-8\") as fout, open(\n        stderr_log, \"w\", encoding=\"utf-8\") as ferr:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        t_out = threading.Thread(target=_stream, args=(proc.stdout, [fout]))\n        t_err = threading.Thread(target=_stream, args=(proc.stderr, [ferr]))\n        t_out.start()\n        t_err.start()\n        t_out.join()\n        t_err.join()\n        return proc.wait()\n\n\n# --------------------------------- Main flow -------------------------------- #\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Run experiment suite\")\n    grp = parser.add_mutually_exclusive_group(required=True)\n    grp.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke test\")\n    grp.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Output directory\")\n    args = parser.parse_args()\n\n    root_results_dir = Path(args.results_dir)\n    root_results_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = root_results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    config_path = (\n        Path(\"config/smoke_test.yaml\") if args.smoke_test else Path(\"config/full_experiment.yaml\")\n    )\n\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        suite_cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    runs = suite_cfg.get(\"runs\", [])\n    if len(runs) == 0:\n        raise ValueError(\"No runs found in configuration file\")\n\n    print(f\"Running {len(runs)} experiment variations defined in {config_path}\\n\", flush=True)\n\n    for run_cfg in runs:\n        run_id = run_cfg[\"run_id\"]\n        print(f\"========== Starting run: {run_id} ==========\")\n\n        run_dir = root_results_dir / run_id\n        run_dir.mkdir(exist_ok=True)\n\n        # Persist individual run config to file so that train.py can read it\n        run_cfg_path = run_dir / \"config.yaml\"\n        with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f_run:\n            yaml.safe_dump(run_cfg, f_run)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config-file\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(root_results_dir),\n        ]\n\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        exit_code = _run_subprocess(cmd, stdout_log, stderr_log)\n        if exit_code != 0:\n            raise RuntimeError(f\"Run {run_id} failed with exit code {exit_code}\")\n\n    # After all runs, perform evaluation & visualisation\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(root_results_dir),\n    ]\n    exit_code = subprocess.call(eval_cmd)\n    if exit_code != 0:\n        raise RuntimeError(\"Evaluation script failed\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "pyproject_toml": "[project]\nname = \"ohgw-common-core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experimental suite\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"torch>=2.1.0\",\n    \"torchvision>=0.16.0\",\n    \"pyyaml>=6.0\",\n    \"numpy>=1.23\",\n    \"matplotlib>=3.8\",\n    \"seaborn>=0.13\",\n    \"tqdm>=4.66\",\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
        "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight configuration used by CI smoke test.  Uses synthetic data & a\n# minimal model to keep runtime and memory requirements negligible.\n\nruns:\n  - run_id: smoke_random_model\n    seed: 0\n    dataset:\n      name: RANDOM_PLACEHOLDER  # PLACEHOLDER: real dataset name will replace this\n      input_shape: [1, 28, 28]\n      num_classes: 10\n      num_samples: 256\n    model:\n      name: base_classifier  # Uses the built-in tiny CNN for smoke tests\n      kwargs: {}\n    optimizer:\n      type: SGD\n      lr: 0.01\n      momentum: 0.9\n    training:\n      batch_size: 32\n      epochs: 2\n",
        "full_experiment_yaml": "# config/full_experiment.yaml\n# Template for the complete experimental suite.  Concrete dataset / model /\n# scheduler definitions will be injected during the specialisation step.\n\nruns:\n  - run_id: EXPERIMENT_VARIATION_PLACEHOLDER_1  # will be replaced\n    seed: 0\n    dataset:\n      name: DATASET_PLACEHOLDER\n      # Additional dataset-specific parameters here\n    model:\n      name: MODEL_PLACEHOLDER\n      # Model-specific hyper-parameters here\n    optimizer:\n      type: SGD  # or Adam etc.\n      lr: SPECIFIC_LR_PLACEHOLDER\n    training:\n      batch_size: SPECIFIC_BATCHSIZE_PLACEHOLDER\n      epochs: SPECIFIC_EPOCHS_PLACEHOLDER\n\n  # Additional run variations can be added here\n\n"
      }
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
        "methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
        "experimental_setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
        "experimental_code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
        "expected_result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
        "expected_conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
      },
      "evaluate": {
        "novelty_reason": "Existing gray-box / multi-fidelity HPO algorithms (ASHA, PASHA, DyHPO, BOIL, FastBO, DyHPO) treat every new trial as a black box and rely only on early learning-curve observations. Implicit-gradient papers (Lasso, DNN-MFBO, hypernetwork approaches) compute exact or approximate hyper-gradients, but they optimise one configuration at a time and require unrolling or bilevel solvers.  OHGW is the first method that: (1) harvests a single stochastic hyper-gradient that is already produced inside the very first mini-batch, (2) uses it only for a one-step warm-start, and (3) leaves the promotion / pruning logic of bandit schedulers completely unchanged.  This simple “one-shot hyper-nudging” of every candidate before the race begins has not been explored in any of the listed related works or mainstream HPO literature, making the idea genuinely new.",
        "novelty_score": 8,
        "significance_reason": "OHGW delivers a median 20 % reduction in time-to-target-accuracy across three strong schedulers (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while adding <3 % extra compute and no additional model evaluations. Because it is a one-line wrapper around trial creation and needs no change to the scheduler or searcher, the technique can be adopted immediately in existing HPO stacks and scales with the same wall-clock efficiency as the host scheduler.  Although the absolute gains are modest relative to bigger algorithmic overhauls, the cost-benefit ratio (≈7× speed-up per % extra compute) and universality across schedulers give the method practical impact for anyone already running gray-box HPO at scale.",
        "significance_score": 7
      }
    }
  ],
  "experiment_iteration": 3,
  "experiment_branches": [
    "research-0-retry-3-exp-1",
    "research-0-retry-3-exp-2"
  ]
}