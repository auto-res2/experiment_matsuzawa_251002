@misc{airas2025,
  author    = {Toma Tanaka and Takumi Matsuzawa and Yuki Yoshino and Ilya Horiguchi and Shiro Takagi and Ryutaro Yamauchi and Wataru Kumagai},
  title     = {{AIRAS}},
  year      = {2025},
  publisher = {GitHub},
  url       = {https://github.com/airas-org/airas}
}

% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{author-year-efficient,
 title = {Efficient Hyperparameter Optimization with Adaptive Fidelity Identification}
}

@article{author-year-learning,
 title = {Learning to Mutate with Hypergradient Guided Population}
}

@article{bertrand-2020-implicit,
 abstract = {Setting regularization parameters for Lasso-type estimators is notoriously
difficult, though crucial in practice. The most popular hyperparameter
optimization approach is grid-search using held-out validation data.
Grid-search however requires to choose a predefined grid for each parameter,
which scales exponentially in the number of parameters. Another approach is to
cast hyperparameter optimization as a bi-level optimization problem, one can
solve by gradient descent. The key challenge for these methods is the
estimation of the gradient with respect to the hyperparameters. Computing this
gradient via forward or backward automatic differentiation is possible yet
usually suffers from high memory consumption. Alternatively implicit
differentiation typically involves solving a linear system which can be
prohibitive and numerically unstable in high dimension. In addition, implicit
differentiation usually assumes smooth loss functions, which is not the case
for Lasso-type problems. This work introduces an efficient implicit
differentiation algorithm, without matrix inversion, tailored for Lasso-type
problems. Our approach scales to high-dimensional data by leveraging the
sparsity of the solutions. Experiments demonstrate that the proposed method
outperforms a large number of standard methods to optimize the error on
held-out data, or the Stein Unbiased Risk Estimator (SURE).},
 arxiv_url = {https://arxiv.org/pdf/2002.08943v3.pdf},
 author = {Quentin Bertrand and Quentin Klopfenstein and Mathieu Blondel and Samuel Vaiter and Alexandre Gramfort and Joseph Salmon},
 github_url = {https://github.com/QB3/sparse-ho},
 title = {Implicit differentiation of Lasso-type models for hyperparameter optimization},
 year = {2020}
}

@article{bohdal-2022-pasha,
 abstract = {Hyperparameter optimization (HPO) and neural architecture search (NAS) are
methods of choice to obtain the best-in-class machine learning models, but in
practice they can be costly to run. When models are trained on large datasets,
tuning them with HPO or NAS rapidly becomes prohibitively expensive for
practitioners, even when efficient multi-fidelity methods are employed. We
propose an approach to tackle the challenge of tuning machine learning models
trained on large datasets with limited computational resources. Our approach,
named PASHA, extends ASHA and is able to dynamically allocate maximum resources
for the tuning procedure depending on the need. The experimental comparison
shows that PASHA identifies well-performing hyperparameter configurations and
architectures while consuming significantly fewer computational resources than
ASHA.},
 arxiv_url = {https://arxiv.org/pdf/2207.06940v2.pdf},
 author = {Ondrej Bohdal and Lukas Balles and Martin Wistuba and Beyza Ermis and Cédric Archambeau and Giovanni Zappella},
 github_url = {https://github.com/ondrejbohdal/pasha},
 title = {PASHA: Efficient HPO and NAS with Progressive Resource Allocation},
 year = {2022}
}

@article{chandra-2019-gradient,
 abstract = {Working with any gradient-based machine learning algorithm involves the
tedious task of tuning the optimizer's hyperparameters, such as its step size.
Recent work has shown how the step size can itself be optimized alongside the
model parameters by manually deriving expressions for "hypergradients" ahead of
time.
  We show how to automatically compute hypergradients with a simple and elegant
modification to backpropagation. This allows us to easily apply the method to
other optimizers and hyperparameters (e.g. momentum coefficients). We can even
recursively apply the method to its own hyper-hyperparameters, and so on ad
infinitum. As these towers of optimizers grow taller, they become less
sensitive to the initial choice of hyperparameters. We present experiments
validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch
implementation of this algorithm (see
people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).},
 arxiv_url = {https://arxiv.org/pdf/1909.13371v2.pdf},
 author = {Kartik Chandra and Audrey Xie and Jonathan Ragan-Kelley and Erik Meijer},
 title = {Gradient Descent: The Ultimate Optimizer},
 year = {2019}
}

@article{daulton-2020-differentiable,
 abstract = {In many real-world scenarios, decision makers seek to efficiently optimize
multiple competing objectives in a sample-efficient fashion. Multi-objective
Bayesian optimization (BO) is a common approach, but many of the
best-performing acquisition functions do not have known analytic gradients and
suffer from high computational overhead. We leverage recent advances in
programming models and hardware acceleration for multi-objective BO using
Expected Hypervolume Improvement (EHVI)---an algorithm notorious for its high
computational complexity. We derive a novel formulation of q-Expected
Hypervolume Improvement (qEHVI), an acquisition function that extends EHVI to
the parallel, constrained evaluation setting. qEHVI is an exact computation of
the joint EHVI of q new candidate points (up to Monte-Carlo (MC) integration
error). Whereas previous EHVI formulations rely on gradient-free acquisition
optimization or approximated gradients, we compute exact gradients of the MC
estimator via auto-differentiation, thereby enabling efficient and effective
optimization using first-order and quasi-second-order methods. Our empirical
evaluation demonstrates that qEHVI is computationally tractable in many
practical scenarios and outperforms state-of-the-art multi-objective BO
algorithms at a fraction of their wall time.},
 arxiv_url = {https://arxiv.org/pdf/2006.05078v3.pdf},
 author = {Samuel Daulton and Maximilian Balandat and Eytan Bakshy},
 github_url = {https://github.com/pytorch/botorch},
 journal = {Advances in Neural Information Processing Systems 33, 2020},
 title = {Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization},
 year = {2020}
}

@article{immer-2023-stochastic,
 abstract = {Selecting hyperparameters in deep learning greatly impacts its effectiveness
but requires manual effort and expertise. Recent works show that Bayesian model
selection with Laplace approximations can allow to optimize such
hyperparameters just like standard neural network parameters using gradients
and on the training data. However, estimating a single hyperparameter gradient
requires a pass through the entire dataset, limiting the scalability of such
algorithms. In this work, we overcome this issue by introducing lower bounds to
the linearized Laplace approximation of the marginal likelihood. In contrast to
previous estimators, these bounds are amenable to stochastic-gradient-based
optimization and allow to trade off estimation accuracy against computational
complexity. We derive them using the function-space form of the linearized
Laplace, which can be estimated using the neural tangent kernel.
Experimentally, we show that the estimators can significantly accelerate
gradient-based hyperparameter optimization.},
 arxiv_url = {https://arxiv.org/pdf/2306.03968v1.pdf},
 author = {Alexander Immer and Tycho F. A. van der Ouderaa and Mark van der Wilk and Gunnar Rätsch and Bernhard Schölkopf},
 github_url = {https://github.com/AlexImmer/ntk-marglik},
 title = {Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels},
 year = {2023}
}

@article{jiang-2024-efficient,
 abstract = {Hyperparameter optimization (HPO) and neural architecture search (NAS) are
powerful in attaining state-of-the-art machine learning models, with Bayesian
optimization (BO) standing out as a mainstream method. Extending BO into the
multi-fidelity setting has been an emerging research topic, but faces the
challenge of determining an appropriate fidelity for each hyperparameter
configuration to fit the surrogate model. To tackle the challenge, we propose a
multi-fidelity BO method named FastBO, which adaptively decides the fidelity
for each configuration and efficiently offers strong performance. The
advantages are achieved based on the novel concepts of efficient point and
saturation point for each configuration.We also show that our adaptive fidelity
identification strategy provides a way to extend any single-fidelity method to
the multi-fidelity setting, highlighting its generality and applicability.},
 arxiv_url = {https://arxiv.org/pdf/2409.00584v1.pdf},
 author = {Jiantong Jiang and Ajmal Mian},
 title = {Efficient Hyperparameter Optimization with Adaptive Fidelity Identification},
 year = {2024}
}

@article{kadra-2023-scaling,
 abstract = {Hyperparameter optimization is an important subfield of machine learning that
focuses on tuning the hyperparameters of a chosen algorithm to achieve peak
performance. Recently, there has been a stream of methods that tackle the issue
of hyperparameter optimization, however, most of the methods do not exploit the
dominant power law nature of learning curves for Bayesian optimization. In this
work, we propose Deep Power Laws (DPL), an ensemble of neural network models
conditioned to yield predictions that follow a power-law scaling pattern. Our
method dynamically decides which configurations to pause and train
incrementally by making use of gray-box evaluations. We compare our method
against 7 state-of-the-art competitors on 3 benchmarks related to tabular,
image, and NLP datasets covering 59 diverse tasks. Our method achieves the best
results across all benchmarks by obtaining the best any-time results compared
to all competitors.},
 arxiv_url = {https://arxiv.org/pdf/2302.00441v3.pdf},
 author = {Arlind Kadra and Maciej Janowski and Martin Wistuba and Josif Grabocka},
 github_url = {https://github.com/releaunifreiburg/DPL},
 title = {Scaling Laws for Hyperparameter Optimization},
 year = {2023}
}

@article{khazi-2023-deep,
 abstract = {Automatically optimizing the hyperparameters of Machine Learning algorithms
is one of the primary open questions in AI. Existing work in Hyperparameter
Optimization (HPO) trains surrogate models for approximating the response
surface of hyperparameters as a regression task. In contrast, we hypothesize
that the optimal strategy for training surrogates is to preserve the ranks of
the performances of hyperparameter configurations as a Learning to Rank
problem. As a result, we present a novel method that meta-learns neural network
surrogates optimized for ranking the configurations' performances while
modeling their uncertainty via ensembling. In a large-scale experimental
protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks,
we demonstrate that our method achieves new state-of-the-art results in HPO.},
 arxiv_url = {https://arxiv.org/pdf/2303.15212v2.pdf},
 author = {Abdus Salam Khazi and Sebastian Pineda Arango and Josif Grabocka},
 github_url = {https://github.com/huawei-noah/HEBO},
 title = {Deep Ranking Ensembles for Hyperparameter Optimization},
 year = {2023}
}

@article{khodak-2021-federated,
 abstract = {Tuning hyperparameters is a crucial but arduous part of the machine learning
pipeline. Hyperparameter optimization is even more challenging in federated
learning, where models are learned over a distributed network of heterogeneous
devices; here, the need to keep data on device and perform local training makes
it difficult to efficiently train and evaluate configurations. In this work, we
investigate the problem of federated hyperparameter tuning. We first identify
key challenges and show how standard approaches may be adapted to form
baselines for the federated setting. Then, by making a novel connection to the
neural architecture search technique of weight-sharing, we introduce a new
method, FedEx, to accelerate federated hyperparameter tuning that is applicable
to widely-used federated optimization methods such as FedAvg and recent
variants. Theoretically, we show that a FedEx variant correctly tunes the
on-device learning rate in the setting of online convex optimization across
devices. Empirically, we show that FedEx can outperform natural baselines for
federated hyperparameter tuning by several percentage points on the
Shakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using
the same training budget.},
 arxiv_url = {https://arxiv.org/pdf/2106.04502v2.pdf},
 author = {Mikhail Khodak and Renbo Tu and Tian Li and Liam Li and Maria-Florina Balcan and Virginia Smith and Ameet Talwalkar},
 github_url = {https://github.com/mkhodak/fedex},
 title = {Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing},
 year = {2021}
}

@article{killamsetty-2022-automata,
 abstract = {Deep neural networks have seen great success in recent years; however,
training a deep model is often challenging as its performance heavily depends
on the hyper-parameters used. In addition, finding the optimal hyper-parameter
configuration, even with state-of-the-art (SOTA) hyper-parameter optimization
(HPO) algorithms, can be time-consuming, requiring multiple training runs over
the entire dataset for different possible sets of hyper-parameters. Our central
insight is that using an informative subset of the dataset for model training
runs involved in hyper-parameter optimization, allows us to find the optimal
hyper-parameter configuration significantly faster. In this work, we propose
AUTOMATA, a gradient-based subset selection framework for hyper-parameter
tuning. We empirically evaluate the effectiveness of AUTOMATA in
hyper-parameter tuning through several experiments on real-world datasets in
the text, vision, and tabular domains. Our experiments show that using
gradient-based data subsets for hyper-parameter tuning achieves significantly
faster turnaround times and speedups of 3$\times$-30$\times$ while achieving
comparable performance to the hyper-parameters found using the entire dataset.},
 arxiv_url = {https://arxiv.org/pdf/2203.08212v1.pdf},
 author = {Krishnateja Killamsetty and Guttu Sai Abhishek and Aakriti and Alexandre V. Evfimievski and Lucian Popa and Ganesh Ramakrishnan and Rishabh Iyer},
 github_url = {https://github.com/decile-team/cords},
 title = {AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning},
 year = {2022}
}

@article{li-2020-multi,
 abstract = {Bayesian optimization (BO) is a popular framework to optimize black-box
functions. In many applications, the objective function can be evaluated at
multiple fidelities to enable a trade-off between the cost and accuracy. To
reduce the optimization cost, many multi-fidelity BO methods have been
proposed. Despite their success, these methods either ignore or over-simplify
the strong, complex correlations across the fidelities, and hence can be
inefficient in estimating the objective function. To address this issue, we
propose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO)
that can flexibly capture all kinds of complicated relationships between the
fidelities to improve the objective function estimation and hence the
optimization performance. We use sequential, fidelity-wise Gauss-Hermite
quadrature and moment-matching to fulfill a mutual information-based
acquisition function, which is computationally tractable and efficient. We show
the advantages of our method in both synthetic benchmark datasets and
real-world applications in engineering design.},
 arxiv_url = {https://arxiv.org/pdf/2007.03117v4.pdf},
 author = {Shibo Li and Wei Xing and Mike Kirby and Shandian Zhe},
 github_url = {https://github.com/zi-w/Max-value-Entropy-Search},
 title = {Multi-Fidelity Bayesian Optimization via Deep Neural Networks},
 year = {2020}
}

@article{li-2021-batch,
 abstract = {Bayesian optimization (BO) is a powerful approach for optimizing black-box,
expensive-to-evaluate functions. To enable a flexible trade-off between the
cost and accuracy, many applications allow the function to be evaluated at
different fidelities. In order to reduce the optimization cost while maximizing
the benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian
Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of
Bayesian neural networks to construct a fully auto-regressive model, which is
expressive enough to capture strong yet complex relationships across all the
fidelities, so as to improve the surrogate learning and optimization
performance. Furthermore, to enhance the quality and diversity of queries, we
develop a simple yet efficient batch querying method, without any combinatorial
search over the fidelities. We propose a batch acquisition function based on
Max-value Entropy Search (MES) principle, which penalizes highly correlated
queries and encourages diversity. We use posterior samples and moment matching
to fulfill efficient computation of the acquisition function and conduct
alternating optimization over every fidelity-input pair, which guarantees an
improvement at each step. We demonstrate the advantage of our approach on four
real-world hyperparameter optimization applications.},
 arxiv_url = {https://arxiv.org/pdf/2106.09884v2.pdf},
 author = {Shibo Li and Robert M. Kirby and Shandian Zhe},
 title = {Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks},
 year = {2021}
}

@article{mlodozeniec-2023-hyperparameter,
 abstract = {Well-tuned hyperparameters are crucial for obtaining good generalization
behavior in neural networks. They can enforce appropriate inductive biases,
regularize the model and improve performance -- especially in the presence of
limited data. In this work, we propose a simple and efficient way for
optimizing hyperparameters inspired by the marginal likelihood, an optimization
objective that requires no validation data. Our method partitions the training
data and a neural network model into $K$ data shards and parameter partitions,
respectively. Each partition is associated with and optimized only on specific
data shards. Combining these partitions into subnetworks allows us to define
the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data
shards unseen by the subnetwork, as the objective for hyperparameter
optimization. We demonstrate that we can apply this objective to optimize a
variety of different hyperparameters in a single training run while being
significantly computationally cheaper than alternative methods aiming to
optimize the marginal likelihood for neural networks. Lastly, we also focus on
optimizing hyperparameters in federated learning, where retraining and
cross-validation are particularly challenging.},
 arxiv_url = {https://arxiv.org/pdf/2304.14766v1.pdf},
 author = {Bruno Mlodozeniec and Matthias Reisser and Christos Louizos},
 title = {Hyperparameter Optimization through Neural Network Partitioning},
 year = {2023}
}

@article{nagler-2024-reshuffling,
 abstract = {Hyperparameter optimization is crucial for obtaining peak performance of
machine learning models. The standard protocol evaluates various hyperparameter
configurations using a resampling estimate of the generalization error to guide
optimization and select a final hyperparameter configuration. Without much
evidence, paired resampling splits, i.e., either a fixed train-validation split
or a fixed cross-validation scheme, are often recommended. We show that,
surprisingly, reshuffling the splits for every configuration often improves the
final model's generalization performance on unseen data. Our theoretical
analysis explains how reshuffling affects the asymptotic behavior of the
validation loss surface and provides a bound on the expected regret in the
limiting regime. This bound connects the potential benefits of reshuffling to
the signal and noise characteristics of the underlying optimization problem. We
confirm our theoretical results in a controlled simulation study and
demonstrate the practical usefulness of reshuffling in a large-scale, realistic
hyperparameter optimization experiment. While reshuffling leads to test
performances that are competitive with using fixed splits, it drastically
improves results for a single train-validation holdout protocol and can often
make holdout become competitive with standard CV while being computationally
cheaper.},
 arxiv_url = {https://arxiv.org/pdf/2405.15393v2.pdf},
 author = {Thomas Nagler and Lennart Schneider and Bernd Bischl and Matthias Feurer},
 github_url = {https://github.com/slds-lmu/paper_2024_reshuffling},
 title = {Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization},
 year = {2024}
}

@article{nguyen-2019-bayesian,
 abstract = {The performance of deep (reinforcement) learning systems crucially depends on
the choice of hyperparameters. Their tuning is notoriously expensive, typically
requiring an iterative training process to run for numerous steps to
convergence. Traditional tuning algorithms only consider the final performance
of hyperparameters acquired after many expensive iterations and ignore
intermediate information from earlier training steps. In this paper, we present
a Bayesian optimization (BO) approach which exploits the iterative structure of
learning algorithms for efficient hyperparameter tuning. We propose to learn an
evaluation function compressing learning progress at any stage of the training
process into a single numeric score according to both training success and
stability. Our BO framework is then balancing the benefit of assessing a
hyperparameter setting over additional training steps against their computation
cost. We further increase model efficiency by selectively including scores from
different training steps for any evaluated hyperparameter set. We demonstrate
the efficiency of our algorithm by tuning hyperparameters for the training of
deep reinforcement learning agents and convolutional neural networks. Our
algorithm outperforms all existing baselines in identifying optimal
hyperparameters in minimal time.},
 arxiv_url = {https://arxiv.org/pdf/1909.09593v5.pdf},
 author = {Vu Nguyen and Sebastian Schulze and Michael A Osborne},
 github_url = {https://github.com/ntienvu/BOIL},
 title = {Bayesian Optimization for Iterative Learning},
 year = {2019}
}

@article{panda-2022-new,
 abstract = {An open problem in differentially private deep learning is hyperparameter
optimization (HPO). DP-SGD introduces new hyperparameters and complicates
existing ones, forcing researchers to painstakingly tune hyperparameters with
hundreds of trials, which in turn makes it impossible to account for the
privacy cost of HPO without destroying the utility. We propose an adaptive HPO
method that uses cheap trials (in terms of privacy cost and runtime) to
estimate optimal hyperparameters and scales them up. We obtain state-of-the-art
performance on 22 benchmark tasks, across computer vision and natural language
processing, across pretraining and finetuning, across architectures and a wide
range of $\varepsilon \in [0.01,8.0]$, all while accounting for the privacy
cost of HPO.},
 arxiv_url = {https://arxiv.org/pdf/2212.04486v3.pdf},
 author = {Ashwinee Panda and Xinyu Tang and Saeed Mahloujifar and Vikash Sehwag and Prateek Mittal},
 github_url = {https://github.com/lxuechen/private-transformers},
 title = {A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization},
 year = {2022}
}

@article{wang-2023-hypo,
 abstract = {Hyperparameter optimization, also known as hyperparameter tuning, is a widely
recognized technique for improving model performance. Regrettably, when
training private ML models, many practitioners often overlook the privacy risks
associated with hyperparameter optimization, which could potentially expose
sensitive information about the underlying dataset. Currently, the sole
existing approach to allow privacy-preserving hyperparameter optimization is to
uniformly and randomly select hyperparameters for a number of runs,
subsequently reporting the best-performing hyperparameter. In contrast, in
non-private settings, practitioners commonly utilize ``adaptive''
hyperparameter optimization methods such as Gaussian process-based
optimization, which select the next candidate based on information gathered
from previous outputs. This substantial contrast between private and
non-private hyperparameter optimization underscores a critical concern. In our
paper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private
hyperparameter optimization, aiming to bridge the gap between private and
non-private hyperparameter optimization. To accomplish this, we provide a
comprehensive differential privacy analysis of our framework. Furthermore, we
empirically demonstrate the effectiveness of DP-HyPO on a diverse set of
real-world datasets.},
 arxiv_url = {https://arxiv.org/pdf/2306.05734v2.pdf},
 author = {Hua Wang and Sheng Gao and Huanyu Zhang and Weijie J. Su and Milan Shen},
 title = {DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization},
 year = {2023}
}

@article{wistuba-2022-supervising,
 abstract = {Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have
recently emerged as a promising direction for tuning Deep Learning methods.
However, existing methods suffer from a sub-optimal allocation of the HPO
budget to the hyperparameter configurations. In this work, we introduce DyHPO,
a Bayesian Optimization method that learns to decide which hyperparameter
configuration to train further in a dynamic race among all feasible
configurations. We propose a new deep kernel for Gaussian Processes that embeds
the learning curve dynamics, and an acquisition function that incorporates
multi-budget information. We demonstrate the significant superiority of DyHPO
against state-of-the-art hyperparameter optimization methods through
large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and
diverse architectures (MLP, CNN/NAS, RNN).},
 arxiv_url = {https://arxiv.org/pdf/2202.09774v2.pdf},
 author = {Martin Wistuba and Arlind Kadra and Josif Grabocka},
 github_url = {https://github.com/releaunifreiburg/DyHPO},
 title = {Supervising the Multi-Fidelity Race of Hyperparameter Configurations},
 year = {2022}
}

% ===========================================
% REFERENCE CANDIDATES
% Additional reference papers for context
% ===========================================

@article{author-year-pattern,
 title = {Pattern recognition and machine learning}
}

@article{brochu-2010-tutorial,
 abstract = {We present a tutorial on Bayesian optimization, a method of finding the
maximum of expensive cost functions. Bayesian optimization employs the Bayesian
technique of setting a prior over the objective function and combining it with
evidence to get a posterior function. This permits a utility-based selection of
the next observation to make on the objective function, which must take into
account both exploration (sampling from areas of high uncertainty) and
exploitation (sampling areas likely to offer improvement over the current best
observation). We also present two detailed extensions of Bayesian optimization,
with experiments---active user modelling with preferences, and hierarchical
reinforcement learning---and a discussion of the pros and cons of Bayesian
optimization based on our experiences.},
 arxiv_url = {https://arxiv.org/pdf/1012.2599v1.pdf},
 author = {Eric Brochu and Vlad M. Cora and Nando de Freitas},
 title = {A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
 year = {2010}
}