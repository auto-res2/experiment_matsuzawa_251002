\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bohdal-2022-pasha}
\citation{wistuba-2022-supervising}
\citation{chandra-2019-gradient}
\citation{bertrand-2020-implicit}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{khazi-2023-deep}
\citation{jiang-2024-efficient}
\citation{panda-2022-new,khodak-2021-federated}
\citation{bohdal-2022-pasha}
\citation{wistuba-2022-supervising}
\citation{nguyen-2019-bayesian}
\citation{kadra-2023-scaling}
\citation{khazi-2023-deep}
\citation{daulton-2020-differentiable}
\citation{chandra-2019-gradient}
\citation{bertrand-2020-implicit}
\citation{immer-2023-stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Contributions}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\citation{killamsetty-2022-automata}
\citation{jiang-2024-efficient}
\citation{li-2020-multi,li-2021-batch}
\citation{khodak-2021-federated}
\citation{panda-2022-new,wang-2023-hypo}
\citation{chandra-2019-gradient}
\citation{bertrand-2020-implicit,immer-2023-stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Warm-start procedure}{3}{subsection.4.1}\protected@file@percent }
\citation{bertrand-2020-implicit}
\citation{nguyen-2019-bayesian}
\citation{jiang-2024-efficient}
\citation{khazi-2023-deep}
\citation{bohdal-2022-pasha,wistuba-2022-supervising}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Implementation details and design choices}{4}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Pseudocode for the warm-start wrapper}{4}{subsection.4.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces One-Shot Hyper-Gradient Warm-Start (OHGW)}}{4}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Relation to existing methods}{4}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Vision: CIFAR-10 with ASHA}{5}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Language: WikiText-103 with PASHA}{5}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Figures}{5}{subsection.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Validation accuracy over time for ASHA baseline; higher values indicate better performance.}}{5}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Validation accuracy over time for ASHA + OHGW (one step); higher is better.}}{6}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Validation accuracy for ASHA + OHGW (three steps); higher is better.}}{6}{figure.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Validation accuracy for ASHA with random warm-start; higher is better.}}{6}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Accuracy comparison across all ASHA variants; higher is better.}}{7}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Accuracy trajectories across 32 seeds; higher is better.}}{7}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Training loss over time for ASHA baseline; lower values indicate better performance.}}{7}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Training loss for ASHA + OHGW (one step); lower is better.}}{8}{figure.caption.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training loss for ASHA + OHGW (three steps); lower is better.}}{8}{figure.caption.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Training loss for ASHA random warm-start; lower is better.}}{8}{figure.caption.11}\protected@file@percent }
\citation{jiang-2024-efficient,khazi-2023-deep}
\citation{panda-2022-new,khodak-2021-federated}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{bertrand-2020-implicit}{{1}{2020}{{Bertrand et~al.}}{{Bertrand, Klopfenstein, Blondel, Vaiter, Gramfort, and Salmon}}}
\bibcite{bohdal-2022-pasha}{{2}{2022}{{Bohdal et~al.}}{{Bohdal, Balles, Wistuba, Ermis, Archambeau, and Zappella}}}
\bibcite{chandra-2019-gradient}{{3}{2019}{{Chandra et~al.}}{{Chandra, Xie, Ragan-Kelley, and Meijer}}}
\bibcite{daulton-2020-differentiable}{{4}{2020}{{Daulton et~al.}}{{Daulton, Balandat, and Bakshy}}}
\bibcite{immer-2023-stochastic}{{5}{2023}{{Immer et~al.}}{{Immer, van~der Ouderaa, van~der Wilk, Rätsch, and Schölkopf}}}
\bibcite{jiang-2024-efficient}{{6}{2024}{{Jiang and Mian}}{{}}}
\bibcite{kadra-2023-scaling}{{7}{2023}{{Kadra et~al.}}{{Kadra, Janowski, Wistuba, and Grabocka}}}
\bibcite{khazi-2023-deep}{{8}{2023}{{Khazi et~al.}}{{Khazi, Arango, and Grabocka}}}
\bibcite{khodak-2021-federated}{{9}{2021}{{Khodak et~al.}}{{Khodak, Tu, Li, Li, Balcan, Smith, and Talwalkar}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Ablations and diagnostics}{9}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Aggregate outcome}{9}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{9}{section.7}\protected@file@percent }
\bibcite{killamsetty-2022-automata}{{10}{2022}{{Killamsetty et~al.}}{{Killamsetty, Abhishek, Aakriti, Evfimievski, Popa, Ramakrishnan, and Iyer}}}
\bibcite{li-2020-multi}{{11}{2020}{{Li et~al.}}{{Li, Xing, Kirby, and Zhe}}}
\bibcite{li-2021-batch}{{12}{2021}{{Li et~al.}}{{Li, Kirby, and Zhe}}}
\bibcite{nguyen-2019-bayesian}{{13}{2019}{{Nguyen et~al.}}{{Nguyen, Schulze, and Osborne}}}
\bibcite{panda-2022-new}{{14}{2022}{{Panda et~al.}}{{Panda, Tang, Mahloujifar, Sehwag, and Mittal}}}
\bibcite{wang-2023-hypo}{{15}{2023}{{Wang et~al.}}{{Wang, Gao, Zhang, Su, and Shen}}}
\bibcite{wistuba-2022-supervising}{{16}{2022}{{Wistuba et~al.}}{{Wistuba, Kadra, and Grabocka}}}
\gdef \@abspage@last{10}
