Warning 2 in paper.tex line 49: Non-breaking space (`~') should have been used.
Hyperparameter optimisation (HPO) is indispensable for obtaining robust performance in modern machine-learning systems, yet even the most popular grey-box schedulers squander a sizable fraction of their budget on clearly sub-optimal configurations. Successive-Halving variants such as Hyperband, ASHA and PASHA prune weak contenders early by evaluating them on progressively larger budgets \cite{bohdal-2022-pasha}. Grey-box Bayesian schemes like DyHPO refine this idea through learning-curve modelling and dynamic promotion rules \cite{wistuba-2022-supervising}. Despite these advances, almost all schedulers regard the training process itself as opaque: internal gradients that are already computed for parameter updates are ignored during the search.  
                                                                                                                                                                                                                                                                                                                                                                                                     ^
Warning 2 in paper.tex line 49: Non-breaking space (`~') should have been used.
Hyperparameter optimisation (HPO) is indispensable for obtaining robust performance in modern machine-learning systems, yet even the most popular grey-box schedulers squander a sizable fraction of their budget on clearly sub-optimal configurations. Successive-Halving variants such as Hyperband, ASHA and PASHA prune weak contenders early by evaluating them on progressively larger budgets \cite{bohdal-2022-pasha}. Grey-box Bayesian schemes like DyHPO refine this idea through learning-curve modelling and dynamic promotion rules \cite{wistuba-2022-supervising}. Despite these advances, almost all schedulers regard the training process itself as opaque: internal gradients that are already computed for parameter updates are ignored during the search.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
Warning 2 in paper.tex line 51: Non-breaking space (`~') should have been used.
Hyper-gradient methods have shown that gradients with respect to hyperparameters can be extracted cheaply via automatic differentiation \cite{chandra-2019-gradient} or implicit differentiation techniques that avoid expensive unrolling \cite{bertrand-2020-implicit}. Unfortunately these approaches typically assume full control over the optimisation routine and therefore clash with production HPO systems whose scheduling logic is complex and battle-tested. The open question, then, is how to inject very cheap but noisy hyper-gradient information into existing bandit-style frameworks without having to rewrite their core.  
                                                                                                                                       ^
Warning 2 in paper.tex line 51: Non-breaking space (`~') should have been used.
Hyper-gradient methods have shown that gradients with respect to hyperparameters can be extracted cheaply via automatic differentiation \cite{chandra-2019-gradient} or implicit differentiation techniques that avoid expensive unrolling \cite{bertrand-2020-implicit}. Unfortunately these approaches typically assume full control over the optimisation routine and therefore clash with production HPO systems whose scheduling logic is complex and battle-tested. The open question, then, is how to inject very cheap but noisy hyper-gradient information into existing bandit-style frameworks without having to rewrite their core.  
                                                                                                                                                                                                                                          ^
Warning 8 in paper.tex line 55: Wrong length of dash may have been used.
Two practical challenges arise. First, a gradient measured on a single mini-batch is extremely noisy, so the step must be sufficiently small to prevent biasing the search or harming exploration. Second, adoption hinges on a minimal engineering footprint - ideally a few lines of code that do not depend on the internals of the scheduler. OHGW meets both constraints: the extra cost is one forward and one backward pass per trial (\(<4\%\) FLOPs in our experiments) and integration is a five-line wrapper around trial creation.  
                                                                                                                                                                                                                                                              ^
Warning 8 in paper.tex line 57: Wrong length of dash may have been used.
We validate OHGW in two contrasting settings - vision (CIFAR-10, ResNet-20, ASHA) and language modelling (WikiText-103, GPT2-small, PASHA) - using 56 paired random seeds and equal GPU budgets. Metrics include time-to-target quality, best final score, compute overhead, variance, and hyperparameter distribution shift. OHGW consistently shortens time-to-target by about twenty percent while preserving ultimate performance and introducing negligible bias. Ablations confirm that gradient directionality, not random perturbation, drives the gain, and that repeating the warm-start step gives only marginal additional savings.  
                                             ^
Warning 8 in paper.tex line 57: Wrong length of dash may have been used.
We validate OHGW in two contrasting settings - vision (CIFAR-10, ResNet-20, ASHA) and language modelling (WikiText-103, GPT2-small, PASHA) - using 56 paired random seeds and equal GPU budgets. Metrics include time-to-target quality, best final score, compute overhead, variance, and hyperparameter distribution shift. OHGW consistently shortens time-to-target by about twenty percent while preserving ultimate performance and introducing negligible bias. Ablations confirm that gradient directionality, not random perturbation, drives the gain, and that repeating the warm-start step gives only marginal additional savings.  
                                                                                                                                           ^
Warning 2 in paper.tex line 67: Non-breaking space (`~') should have been used.
Looking forward, we plan to extend OHGW to mixed discrete-continuous spaces, integrate warm-start signals into surrogate-based selection \cite{khazi-2023-deep} and adaptive-fidelity frameworks \cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \cite{panda-2022-new,khodak-2021-federated}.  
                                                                                                                                        ^
Warning 2 in paper.tex line 67: Non-breaking space (`~') should have been used.
Looking forward, we plan to extend OHGW to mixed discrete-continuous spaces, integrate warm-start signals into surrogate-based selection \cite{khazi-2023-deep} and adaptive-fidelity frameworks \cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \cite{panda-2022-new,khodak-2021-federated}.  
                                                                                                                                                                                                ^
Warning 2 in paper.tex line 67: Non-breaking space (`~') should have been used.
Looking forward, we plan to extend OHGW to mixed discrete-continuous spaces, integrate warm-start signals into surrogate-based selection \cite{khazi-2023-deep} and adaptive-fidelity frameworks \cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \cite{panda-2022-new,khodak-2021-federated}.  
                                                                                                                                                                                                                                                                                                                                                   ^
Warning 2 in paper.tex line 71: Non-breaking space (`~') should have been used.
Successive-Halving, Hyperband and ASHA progressively allocate resources; PASHA adds an adaptive cap on maximum fidelity \cite{bohdal-2022-pasha}. DyHPO supervises the race among configurations with a deep-kernel Gaussian Process that embeds learning-curve dynamics \cite{wistuba-2022-supervising}. All these methods leverage intermediate metrics yet still initialise every configuration blindly. OHGW is complementary: it keeps the scheduling logic intact and instead improves the starting point of each trial.  
                                                                                                                       ^
Warning 2 in paper.tex line 71: Non-breaking space (`~') should have been used.
Successive-Halving, Hyperband and ASHA progressively allocate resources; PASHA adds an adaptive cap on maximum fidelity \cite{bohdal-2022-pasha}. DyHPO supervises the race among configurations with a deep-kernel Gaussian Process that embeds learning-curve dynamics \cite{wistuba-2022-supervising}. All these methods leverage intermediate metrics yet still initialise every configuration blindly. OHGW is complementary: it keeps the scheduling logic intact and instead improves the starting point of each trial.  
                                                                                                                                                                                                                                                                        ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                     ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                                                                                                                                      ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                                                                                                                                                                                                                                            ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                                                                                                                                                                                                                                                                                                                                                                  ^
Warning 2 in paper.tex line 77: Non-breaking space (`~') should have been used.
Early work showed how to compute hyper-gradients by augmenting backpropagation \cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.  
                                                                              ^
Warning 2 in paper.tex line 77: Non-breaking space (`~') should have been used.
Early work showed how to compute hyper-gradients by augmenting backpropagation \cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.  
                                                                                                                                                                    ^
Warning 2 in paper.tex line 77: Non-breaking space (`~') should have been used.
Early work showed how to compute hyper-gradients by augmenting backpropagation \cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.  
                                                                                                                                                                                                                                                                ^
Warning 2 in paper.tex line 80: Non-breaking space (`~') should have been used.
AUTOMATA speeds up HPO by selecting informative data subsets \cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.  
                                                            ^
Warning 2 in paper.tex line 80: Non-breaking space (`~') should have been used.
AUTOMATA speeds up HPO by selecting informative data subsets \cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.  
                                                                                                                                                     ^
Warning 2 in paper.tex line 80: Non-breaking space (`~') should have been used.
AUTOMATA speeds up HPO by selecting informative data subsets \cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.  
                                                                                                                                                                                                                                           ^
Warning 2 in paper.tex line 83: Non-breaking space (`~') should have been used.
Federated HPO faces communication bottlenecks \cite{khodak-2021-federated}; differentially-private HPO must account for privacy budgets \cite{panda-2022-new,wang-2023-hypo}. OHGW's one-shot nature and tiny overhead make it attractive in such resource-sensitive regimes.  
                                             ^
Warning 2 in paper.tex line 83: Non-breaking space (`~') should have been used.
Federated HPO faces communication bottlenecks \cite{khodak-2021-federated}; differentially-private HPO must account for privacy budgets \cite{panda-2022-new,wang-2023-hypo}. OHGW's one-shot nature and tiny overhead make it attractive in such resource-sensitive regimes.  
                                                                                                                                       ^
Warning 2 in paper.tex line 92: Non-breaking space (`~') should have been used.
Deep-learning frameworks already compute \(\partial L/\partial \theta\); obtaining \(\partial L/\partial \psi\) requires little additional work as long as \(\psi\) influences the forward computation \cite{chandra-2019-gradient}. Although these hyper-gradients are noisy when estimated on a single mini-batch, they still indicate how the loss would change if \(\psi\) were perturbed.  
                                                                                                                                                                                                      ^
Warning 13 in paper.tex line 95: Intersentence spacing (`\@') should perhaps be used.
We aim to inject this cheap signal into existing schedulers without touching their allocation policies. Constraints are: overhead \(\leq 5\%\) FLOPs and \(\leq 10\%\) VRAM; zero changes to promotion logic; ability to operate in mixed search spaces (only continuous \(\psi\) are updated); preservation of exploration diversity. Prior art typically computes hyper-gradients throughout training, unrolls optimisation steps, or solves auxiliary linear systems \cite{bertrand-2020-implicit,immer-2023-stochastic}. OHGW avoids all of these by taking exactly one hyper-step before heavy training begins.  
                                                                                                                                                                           ^
Warning 2 in paper.tex line 95: Non-breaking space (`~') should have been used.
We aim to inject this cheap signal into existing schedulers without touching their allocation policies. Constraints are: overhead \(\leq 5\%\) FLOPs and \(\leq 10\%\) VRAM; zero changes to promotion logic; ability to operate in mixed search spaces (only continuous \(\psi\) are updated); preservation of exploration diversity. Prior art typically computes hyper-gradients throughout training, unrolls optimisation steps, or solves auxiliary linear systems \cite{bertrand-2020-implicit,immer-2023-stochastic}. OHGW avoids all of these by taking exactly one hyper-step before heavy training begins.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
Warning 13 in paper.tex line 111: Intersentence spacing (`\@') should perhaps be used.
Differentiable hyperparameters are wrapped as tensors that influence the forward computation (e.g., learning rate scales the optimiser update, label smoothing alters target distributions). A small \(\eta_h\) prevents excessive bias; we sweep \(\eta_h\in\{10^{-4}, 3\cdot10^{-4}, 10^{-3}, 3\cdot10^{-3}\}\) in the experiments. Because only one extra backward pass is added, empirical overhead stays below 4\% FLOPs and 1\% VRAM.  
                                                                                                                                                                                                                                                                                                                                                                                                                                          ^
Warning 1 in paper.tex line 116: Command terminated with space.
\State \textbf{Input:} scheduler producing configurations \(x=(\theta_0,\psi)\); hyper-step size \(\eta_h\)  
      ^
Warning 1 in paper.tex line 118: Command terminated with space.
  \State \(x \leftarrow\) scheduler.sample()  
        ^
Warning 36 in paper.tex line 118: You should put a space in front of parenthesis.
  \State \(x \leftarrow\) scheduler.sample()  
                                          ^
Warning 1 in paper.tex line 119: Command terminated with space.
  \State model \(\leftarrow\) BuildModel\((x)\)  
        ^
Warning 1 in paper.tex line 120: Command terminated with space.
  \State batch \(\leftarrow\) NextMiniBatch\((\text{train data})\)  
        ^
Warning 1 in paper.tex line 121: Command terminated with space.
  \State \(\ell \leftarrow L(\theta_0, \psi; \text{batch})\)  
        ^
Warning 1 in paper.tex line 122: Command terminated with space.
  \State \(g_{\psi} \leftarrow \operatorname{autograd\_grad}(\ell, \psi)\)  
        ^
Warning 1 in paper.tex line 123: Command terminated with space.
  \State disable further gradient tracking  
        ^
Warning 1 in paper.tex line 125: Command terminated with space.
    \State \(p \leftarrow p - \eta_h\, g\)  
          ^
Warning 1 in paper.tex line 126: Command terminated with space.
  \EndFor  
         ^
Warning 1 in paper.tex line 127: Command terminated with space.
  \State scheduler.launch\((x)\) \Comment{Train and evaluate with unmodified policy}  
        ^
Warning 1 in paper.tex line 128: Command terminated with space.
\EndWhile  
         ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                           ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                                                                                                          ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                                                                                                                                                                                        ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                                                                                                                                                                                                                                                                           ^
Warning 2 in paper.tex line 140: Non-breaking space (`~') should have been used.
We employ the public implementations of ASHA, PASHA and DyHPO \cite{bohdal-2022-pasha,wistuba-2022-supervising} unmodified. Variants suffixed "+OHGW" wrap trial creation with the warm-start procedure described above.  
                                                             ^
Warning 18 in paper.tex line 140: Use either `` or '' as an alternative to `"'.
We employ the public implementations of ASHA, PASHA and DyHPO \cite{bohdal-2022-pasha,wistuba-2022-supervising} unmodified. Variants suffixed "+OHGW" wrap trial creation with the warm-start procedure described above.  
                                                                                                                                              ^
Warning 18 in paper.tex line 140: Use either `` or '' as an alternative to `"'.
We employ the public implementations of ASHA, PASHA and DyHPO \cite{bohdal-2022-pasha,wistuba-2022-supervising} unmodified. Variants suffixed "+OHGW" wrap trial creation with the warm-start procedure described above.  
                                                                                                                                                    ^
Warning 13 in paper.tex line 166: Intersentence spacing (`\@') should perhaps be used.
Baseline reaches 93\% validation accuracy in \(11.4\,\text{h} \pm 1.1\). Random warm-start improves this marginally to \(11.2\,\text{h} \pm 1.0\) (\(-1.8\%\)). OHGW (one step) lowers time-to-target to \(9.1\,\text{h} \pm 1.0\) (\(-20.2\%\), \(p = 3.1 \times 10^{-6}\)). Three steps reduce time further to \(8.9\,\text{h} \pm 1.3\) (\(-21.9\%\)) but raise overhead to 6\% FLOPs. Final test accuracy is \(94.73\% \pm 0.12\) (baseline) versus \(94.81\% \pm 0.10\) (OHGW), difference not significant. Warm-start overhead is 2.7\% FLOPs and \(<0.1\%\) VRAM.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
Warning 13 in paper.tex line 229: Intersentence spacing (`\@') should perhaps be used.
Baseline reaches validation perplexity 30 in \(6.9\,\text{h} \pm 0.8\). OHGW with \(\eta_h = 10^{-3}\) needs \(5.6\,\text{h} \pm 0.7\) (\(-18.8\%\), \(p = 7.5 \times 10^{-5}\)). Lowering \(\eta_h\) to \(3\cdot10^{-4}\) produces \(5.8\,\text{h}\) (\(-16.3\%\)). Under 15\% token noise OHGW still gains 11.6\%. Final validation perplexity improves slightly from \(24.8 \pm 0.3\) to \(24.6 \pm 0.3\); out-of-domain perplexity drops from 32.1 to 31.7. Overhead is 3.4\% FLOPs and 1.2\% VRAM.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
Warning 8 in paper.tex line 238: Wrong length of dash may have been used.
Across 56 paired seeds, OHGW reduces median time-to-target by 19.5\%, preserves or slightly improves final task performance, incurs \(<4\%\) extra compute, and does not inflate variance - meeting all pre-registered success criteria.  
                                                                                                                                                                                          ^
Warning 2 in paper.tex line 243: Non-breaking space (`~') should have been used.
Practitioners can adopt OHGW via a five-line wrapper, immediately reclaiming a significant share of wasted GPU hours in existing HPO pipelines. Future work will extend the idea to mixed discrete-continuous spaces, integrate warm-start signals into surrogate-based candidate selection and adaptive-fidelity frameworks \cite{jiang-2024-efficient,khazi-2023-deep}, and explore privacy-aware or federated settings where the one-shot, low-overhead characteristic of OHGW is particularly advantageous \cite{panda-2022-new,khodak-2021-federated}. By showing that even a noisy, single-batch hyper-gradient can materially accelerate grey-box optimisation, this work opens the door to deeper synergies between internal training-loop signals and external scheduling strategies.  
                                                                                                                                                                                                                                                                                                                            ^
Warning 2 in paper.tex line 243: Non-breaking space (`~') should have been used.
Practitioners can adopt OHGW via a five-line wrapper, immediately reclaiming a significant share of wasted GPU hours in existing HPO pipelines. Future work will extend the idea to mixed discrete-continuous spaces, integrate warm-start signals into surrogate-based candidate selection and adaptive-fidelity frameworks \cite{jiang-2024-efficient,khazi-2023-deep}, and explore privacy-aware or federated settings where the one-shot, low-overhead characteristic of OHGW is particularly advantageous \cite{panda-2022-new,khodak-2021-federated}. By showing that even a noisy, single-batch hyper-gradient can materially accelerate grey-box optimisation, this work opens the door to deeper synergies between internal training-loop signals and external scheduling strategies.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
