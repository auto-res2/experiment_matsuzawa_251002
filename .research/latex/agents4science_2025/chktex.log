Warning 2 in paper.tex line 49: Non-breaking space (`~') should have been used.
Hyperparameter optimisation (HPO) is indispensable for obtaining robust performance in modern machine-learning systems, yet even the most popular grey-box schedulers squander a sizable fraction of their budget on clearly sub-optimal configurations. Successive-Halving variants such as Hyperband, ASHA and PASHA prune weak contenders early by evaluating them on progressively larger budgets \cite{bohdal-2022-pasha}. Grey-box Bayesian schemes like DyHPO refine this idea through learning-curve modelling and dynamic promotion rules \cite{wistuba-2022-supervising}. Despite these advances, almost all schedulers regard the training process itself as opaque: internal gradients that are already computed for parameter updates are ignored during the search.  
                                                                                                                                                                                                                                                                                                                                                                                                     ^
Warning 2 in paper.tex line 49: Non-breaking space (`~') should have been used.
Hyperparameter optimisation (HPO) is indispensable for obtaining robust performance in modern machine-learning systems, yet even the most popular grey-box schedulers squander a sizable fraction of their budget on clearly sub-optimal configurations. Successive-Halving variants such as Hyperband, ASHA and PASHA prune weak contenders early by evaluating them on progressively larger budgets \cite{bohdal-2022-pasha}. Grey-box Bayesian schemes like DyHPO refine this idea through learning-curve modelling and dynamic promotion rules \cite{wistuba-2022-supervising}. Despite these advances, almost all schedulers regard the training process itself as opaque: internal gradients that are already computed for parameter updates are ignored during the search.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
Warning 2 in paper.tex line 51: Non-breaking space (`~') should have been used.
Hyper-gradient methods have shown that gradients with respect to hyperparameters can be extracted cheaply via automatic differentiation \cite{chandra-2019-gradient} or implicit differentiation techniques that avoid expensive unrolling \cite{bertrand-2020-implicit}. Unfortunately these approaches typically assume full control over the optimisation routine and therefore clash with production HPO systems whose scheduling logic is complex and battle-tested. The open question, then, is how to inject very cheap but noisy hyper-gradient information into existing bandit-style frameworks without having to rewrite their core.  
                                                                                                                                       ^
Warning 2 in paper.tex line 51: Non-breaking space (`~') should have been used.
Hyper-gradient methods have shown that gradients with respect to hyperparameters can be extracted cheaply via automatic differentiation \cite{chandra-2019-gradient} or implicit differentiation techniques that avoid expensive unrolling \cite{bertrand-2020-implicit}. Unfortunately these approaches typically assume full control over the optimisation routine and therefore clash with production HPO systems whose scheduling logic is complex and battle-tested. The open question, then, is how to inject very cheap but noisy hyper-gradient information into existing bandit-style frameworks without having to rewrite their core.  
                                                                                                                                                                                                                                          ^
Warning 2 in paper.tex line 67: Non-breaking space (`~') should have been used.
Looking forward, we plan to extend OHGW to mixed discretecontinuous spaces, integrate warm-start signals into surrogate-based selection \cite{khazi-2023-deep} and adaptive-fidelity frameworks \cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \cite{panda-2022-new,khodak-2021-federated}.  
                                                                                                                                        ^
Warning 2 in paper.tex line 67: Non-breaking space (`~') should have been used.
Looking forward, we plan to extend OHGW to mixed discretecontinuous spaces, integrate warm-start signals into surrogate-based selection \cite{khazi-2023-deep} and adaptive-fidelity frameworks \cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \cite{panda-2022-new,khodak-2021-federated}.  
                                                                                                                                                                                                ^
Warning 2 in paper.tex line 67: Non-breaking space (`~') should have been used.
Looking forward, we plan to extend OHGW to mixed discretecontinuous spaces, integrate warm-start signals into surrogate-based selection \cite{khazi-2023-deep} and adaptive-fidelity frameworks \cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \cite{panda-2022-new,khodak-2021-federated}.  
                                                                                                                                                                                                                                                                                                                                                   ^
Warning 2 in paper.tex line 71: Non-breaking space (`~') should have been used.
Successive-Halving, Hyperband and ASHA progressively allocate resources; PASHA adds an adaptive cap on maximum fidelity \cite{bohdal-2022-pasha}. DyHPO supervises the race among configurations with a deep-kernel Gaussian Process that embeds learning-curve dynamics \cite{wistuba-2022-supervising}. All these methods leverage intermediate metrics yet still initialise every configuration blindly. OHGW is complementary: it keeps the scheduling logic intact and instead improves the starting point of each trial.  
                                                                                                                       ^
Warning 2 in paper.tex line 71: Non-breaking space (`~') should have been used.
Successive-Halving, Hyperband and ASHA progressively allocate resources; PASHA adds an adaptive cap on maximum fidelity \cite{bohdal-2022-pasha}. DyHPO supervises the race among configurations with a deep-kernel Gaussian Process that embeds learning-curve dynamics \cite{wistuba-2022-supervising}. All these methods leverage intermediate metrics yet still initialise every configuration blindly. OHGW is complementary: it keeps the scheduling logic intact and instead improves the starting point of each trial.  
                                                                                                                                                                                                                                                                        ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                     ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                                                                                                                                      ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                                                                                                                                                                                                                                            ^
Warning 2 in paper.tex line 74: Non-breaking space (`~') should have been used.
BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  
                                                                                                                                                                                                                                                                                                                                                                                                                  ^
Warning 2 in paper.tex line 77: Non-breaking space (`~') should have been used.
Early work showed how to compute hyper-gradients by augmenting backpropagation \cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.  
                                                                              ^
Warning 2 in paper.tex line 77: Non-breaking space (`~') should have been used.
Early work showed how to compute hyper-gradients by augmenting backpropagation \cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.  
                                                                                                                                                                    ^
Warning 2 in paper.tex line 77: Non-breaking space (`~') should have been used.
Early work showed how to compute hyper-gradients by augmenting backpropagation \cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.  
                                                                                                                                                                                                                                                                ^
Warning 2 in paper.tex line 80: Non-breaking space (`~') should have been used.
AUTOMATA speeds up HPO by selecting informative data subsets \cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.  
                                                            ^
Warning 2 in paper.tex line 80: Non-breaking space (`~') should have been used.
AUTOMATA speeds up HPO by selecting informative data subsets \cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.  
                                                                                                                                                     ^
Warning 2 in paper.tex line 80: Non-breaking space (`~') should have been used.
AUTOMATA speeds up HPO by selecting informative data subsets \cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.  
                                                                                                                                                                                                                                           ^
Warning 2 in paper.tex line 83: Non-breaking space (`~') should have been used.
Federated HPO faces communication bottlenecks \cite{khodak-2021-federated}; differentially-private HPO must account for privacy budgets \cite{panda-2022-new,wang-2023-hypo}. OHGWs one-shot nature and tiny overhead make it attractive in such resource-sensitive regimes.  
                                             ^
Warning 2 in paper.tex line 83: Non-breaking space (`~') should have been used.
Federated HPO faces communication bottlenecks \cite{khodak-2021-federated}; differentially-private HPO must account for privacy budgets \cite{panda-2022-new,wang-2023-hypo}. OHGWs one-shot nature and tiny overhead make it attractive in such resource-sensitive regimes.  
                                                                                                                                       ^
Warning 2 in paper.tex line 92: Non-breaking space (`~') should have been used.
Deep-learning frameworks already compute \(\tfrac{\partial L}{\partial \theta}\); obtaining \(\tfrac{\partial L}{\partial \psi}\) requires little additional work as long as \(\psi\) influences the forward computation \cite{chandra-2019-gradient}. Although these hyper-gradients are noisy when estimated on a single mini-batch, they still indicate how the loss would change if \(\psi\) were perturbed.  
                                                                                                                                                                                                                        ^
Warning 13 in paper.tex line 95: Intersentence spacing (`\@') should perhaps be used.
We aim to inject this cheap signal into existing schedulers without touching their allocation policies. Constraints are: overhead \(\leq 5 \%\) FLOPs and \(\leq 10 \%\) VRAM; zero changes to promotion logic; ability to operate in mixed search spaces (only continuous \(\psi\) are updated); preservation of exploration diversity. Prior art typically computes hyper-gradients throughout training, unrolls optimisation steps, or solves auxiliary linear systems \cite{bertrand-2020-implicit,immer-2023-stochastic}. OHGW avoids all of these by taking exactly one hyper-step before heavy training begins.  
                                                                                                                                                                             ^
Warning 2 in paper.tex line 95: Non-breaking space (`~') should have been used.
We aim to inject this cheap signal into existing schedulers without touching their allocation policies. Constraints are: overhead \(\leq 5 \%\) FLOPs and \(\leq 10 \%\) VRAM; zero changes to promotion logic; ability to operate in mixed search spaces (only continuous \(\psi\) are updated); preservation of exploration diversity. Prior art typically computes hyper-gradients throughout training, unrolls optimisation steps, or solves auxiliary linear systems \cite{bertrand-2020-implicit,immer-2023-stochastic}. OHGW avoids all of these by taking exactly one hyper-step before heavy training begins.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
Warning 13 in paper.tex line 113: Intersentence spacing (`\@') should perhaps be used.
Differentiable hyperparameters are wrapped as tensors that influence the forward computation (e.g., learning rate scales the optimiser update, label smoothing alters target distributions). A small \(\eta_h\) prevents excessive bias; we sweep \(\eta_h\in\{10^{-4}, 3\cdot10^{-4}, 10^{-3}, 3\cdot10^{-3}\}\) in the results. Because only one extra backward pass is added, empirical overhead stays below four percent FLOPs and one percent VRAM.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
Warning 1 in paper.tex line 119: Command terminated with space.
    \State \textbf{Input:} Scheduler producing configurations \(x=(\theta_0,\psi)\); hyper-step size \(\eta_h\); training data loader  
          ^
Warning 1 in paper.tex line 121: Command terminated with space.
        \State \(x\gets\) scheduler.sample()  
              ^
Warning 36 in paper.tex line 121: You should put a space in front of parenthesis.
        \State \(x\gets\) scheduler.sample()  
                                          ^
Warning 1 in paper.tex line 122: Command terminated with space.
        \State model \(\gets\) build\_model with initial parameters \(\theta_0\)  
              ^
Warning 1 in paper.tex line 123: Command terminated with space.
        \State data \(\gets\) next mini-batch from loader  
              ^
Warning 1 in paper.tex line 124: Command terminated with space.
        \State Compute loss: \(\ell \gets L(\theta_0, \psi; \text{data})\)  
              ^
Warning 1 in paper.tex line 125: Command terminated with space.
        \State Compute hyper-gradient: \(g_{\psi} \gets \nabla_{\psi} \, \ell\) via autograd  
              ^
Warning 1 in paper.tex line 126: Command terminated with space.
        \State Update hyperparameters: \(\psi \gets \psi - \eta_h \, g_{\psi}\)  
              ^
Warning 1 in paper.tex line 127: Command terminated with space.
        \State Launch unmodified training of \(x'=(\theta_0,\psi)\) under the scheduler (budgets, promotion, and stopping unchanged)  
              ^
Warning 1 in paper.tex line 128: Command terminated with space.
    \EndWhile  
             ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                           ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                                                                                                          ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                                                                                                                                                                                        ^
Warning 2 in paper.tex line 133: Non-breaking space (`~') should have been used.
OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.  
                                                                                                                                                                                                                                                                                                                                                           ^
Warning 2 in paper.tex line 140: Non-breaking space (`~') should have been used.
We employ the public implementations of ASHA, PASHA and DyHPO \cite{bohdal-2022-pasha,wistuba-2022-supervising} unmodified. Variants suffixed ``+OHGW'' wrap trial creation with the procedure described above.  
                                                             ^
Warning 8 in paper.tex line 165: Wrong length of dash may have been used.
\subsection{Vision - CIFAR-10 + ASHA}  
                   ^
Warning 13 in paper.tex line 166: Intersentence spacing (`\@') should perhaps be used.
Baseline reaches 93 \% validation accuracy in \(11.4\,\mathrm{h} \pm 1.1\). Random warm-start improves this marginally to \(11.2\,\mathrm{h} \pm 1.0\) (\(-1.8 \%\)). OHGW (one step) lowers time-to-target to \(9.1\,\mathrm{h} \pm 1.0\) (\(-20.2 \%\), \(p = 3.1 \times 10^{-6}\)). Three steps reduce time further to \(8.9\,\mathrm{h} \pm 1.3\) (\(-21.9 \%\)) but raise overhead to 6 \% FLOPs. Final test accuracy is \(94.73 \% \pm 0.12\) (baseline) versus \(94.81 \% \pm 0.10\) (OHGW), difference not significant. Warm-start overhead is 2.7 \% FLOPs and \(<0.1 \%\) VRAM.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
Warning 8 in paper.tex line 168: Wrong length of dash may have been used.
\subsection{Language - WikiText-103 + PASHA}  
                     ^
Warning 13 in paper.tex line 169: Intersentence spacing (`\@') should perhaps be used.
Baseline reaches validation perplexity 30 in \(6.9\,\mathrm{h} \pm 0.8\). OHGW with \(\eta_h = 10^{-3}\) needs \(5.6\,\mathrm{h} \pm 0.7\) (\(-18.8 \%\), \(p = 7.5 \times 10^{-5}\)). Lowering \(\eta_h\) to \(3\cdot10^{-4}\) produces \(5.8\,\mathrm{h}\) (\(-16.3 \%\)). Under 15 \% token noise OHGW still gains 11.6 \%. Final validation perplexity improves slightly from \(24.8 \pm 0.3\) to \(24.6 \pm 0.3\); out-of-domain perplexity drops from 32.1 to 31.7. Overhead is 3.4 \% FLOPs and 1.2 \% VRAM.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
Warning 2 in paper.tex line 244: Non-breaking space (`~') should have been used.
Practitioners can adopt OHGW via a five-line wrapper, immediately reclaiming a significant share of wasted GPU hours in existing HPO pipelines. Future work will extend the idea to mixed discretecontinuous spaces, integrate warm-start signals into surrogate-based candidate selection and adaptive-fidelity frameworks \cite{jiang-2024-efficient,khazi-2023-deep}, and explore privacy-aware or federated settings where the one-shot, low-overhead characteristic of OHGW is particularly advantageous \cite{panda-2022-new,khodak-2021-federated}. By showing that even a noisy, single-batch hyper-gradient can materially accelerate grey-box optimisation, this work opens the door to deeper synergies between internal training-loop signals and external scheduling strategies.  
                                                                                                                                                                                                                                                                                                                            ^
Warning 2 in paper.tex line 244: Non-breaking space (`~') should have been used.
Practitioners can adopt OHGW via a five-line wrapper, immediately reclaiming a significant share of wasted GPU hours in existing HPO pipelines. Future work will extend the idea to mixed discretecontinuous spaces, integrate warm-start signals into surrogate-based candidate selection and adaptive-fidelity frameworks \cite{jiang-2024-efficient,khazi-2023-deep}, and explore privacy-aware or federated settings where the one-shot, low-overhead characteristic of OHGW is particularly advantageous \cite{panda-2022-new,khodak-2021-federated}. By showing that even a noisy, single-batch hyper-gradient can materially accelerate grey-box optimisation, this work opens the door to deeper synergies between internal training-loop signals and external scheduling strategies.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
