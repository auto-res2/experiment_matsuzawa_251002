
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Base Code (Reference Foundation)
{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # For non-interactive backends\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIGURE_PARAMS = {\n    \"pdf.fonttype\": 42,\n    \"ps.fonttype\": 42,\n    \"font.size\": 12,\n    \"legend.fontsize\": 10,\n}\nplt.rcParams.update(FIGURE_PARAMS)\n\n\ndef load_results(results_dir: Path) -\u003e List[Dict[str, Any]]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef plot_best_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=\"best_val_accuracy\", data=df)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.ylim(0, 1)\n    # Annotate bars\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"best_val_accuracy\"] + 0.01, f\"{row[\u0027best_val_accuracy\u0027]*100:.1f}%\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(out_path / \"accuracy.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef plot_loss_curves(all_results: List[Dict[str, Any]], out_path: Path):\n    for res in all_results:\n        run_id = res[\"run_id\"]\n        epochs = [m[\"epoch\"] for m in res[\"epoch_metrics\"]]\n        train_losses = [m[\"train_loss\"] for m in res[\"epoch_metrics\"]]\n        val_losses = [m[\"val_loss\"] for m in res[\"epoch_metrics\"]]\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, train_losses, label=\"Train loss\")\n        plt.plot(epochs, val_losses, label=\"Val loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"Loss Curve - {run_id}\")\n        # Annotate final values\n        plt.text(epochs[-1], val_losses[-1], f\"{val_losses[-1]:.3f}\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(out_path / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n        plt.close()\n\n\ndef main(results_dir: str):\n    results_path = Path(results_dir)\n    all_results = load_results(results_path)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json found in {results_dir}\")\n\n    # Convert to pandas DataFrame for easy handling\n    df = pd.DataFrame([\n        {\n            \"run_id\": r[\"run_id\"],\n            \"best_val_accuracy\": r[\"best_val_accuracy\"],\n            \"time_to_threshold\": r.get(\"time_to_threshold\"),\n            \"final_test_accuracy\": r.get(\"final_test_accuracy\"),\n        }\n        for r in all_results\n    ])\n\n    # Print numerical comparison to stdout\n    comparison = df.to_dict(orient=\"records\")\n    print(json.dumps({\"comparison\": comparison}, indent=2))\n\n    # Create figure output directory\n    figs_dir = results_path / \"figures\"\n    figs_dir.mkdir(exist_ok=True)\n\n    # Plot best accuracy bar chart\n    plot_best_accuracy(df, figs_dir)\n\n    # Plot loss curves per run\n    plot_loss_curves(all_results, figs_dir)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory containing all run subdirs.\")\n    args = parser.parse_args()\n    main(args.results_dir)\n", "full_experiment_yaml": "description: \"FULL EXPERIMENT CONFIG \u2013 TO BE POPULATED WITH REAL DATASETS AND MODELS IN DERIVED STEP\"\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER_1  # PLACEHOLDER: replace with actual identifier\n    seed: 0\n    dataset: DATASET_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    training: SPECIFIC_CONFIG_PLACEHOLDER\n    evaluation:\n      threshold: SPECIFIC_CONFIG_PLACEHOLDER\n  # Additional experiments can be added here\n", "main_py": "import argparse\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport yaml\nimport json\nimport datetime\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run subprocess while teeing its stdout / stderr to files and console.\"\"\"\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1)\n        assert process.stdout is not None and process.stderr is not None\n        # Stream stdout\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if not stdout_line and not stderr_line and process.poll() is not None:\n                break\n            if stdout_line:\n                sys.stdout.buffer.write(stdout_line)\n                out_f.write(stdout_line)\n                out_f.flush()\n            if stderr_line:\n                sys.stderr.buffer.write(stderr_line)\n                err_f.write(stderr_line)\n                err_f.flush()\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n\ndef launch_train(run_cfg: Dict[str, Any], results_root: Path, smoke_test: bool):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save run-specific config so that train.py can read it.\n    cfg_path = run_dir / \"config.yaml\"\n    with cfg_path.open(\"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # Build subprocess command\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--config\",\n        str(cfg_path),\n        \"--results-dir\",\n        str(run_dir),\n    ]\n    if smoke_test:\n        cmd.append(\"--smoke-test\")\n\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    tee_subprocess(cmd, stdout_path, stderr_path)\n\n\ndef run_all(cfg_file: str, results_dir: str, smoke_test: bool):\n    with open(cfg_file) as f:\n        exp_cfg = yaml.safe_load(f)\n\n    if \"experiments\" not in exp_cfg:\n        raise KeyError(\"Config YAML must contain \u0027experiments\u0027 list.\")\n\n    results_root = Path(results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    description = exp_cfg.get(\"description\", \"No description provided.\")\n    print(\n        json.dumps(\n            {\n                \"timestamp\": datetime.datetime.now().isoformat(),\n                \"experiment_description\": description,\n            },\n            indent=2,\n        )\n    )\n\n    for run_cfg in exp_cfg[\"experiments\"]:\n        print(json.dumps({\"status\": \"starting\", \"run_id\": run_cfg[\"run_id\"]}))\n        launch_train(run_cfg, results_root, smoke_test)\n        print(json.dumps({\"status\": \"finished\", \"run_id\": run_cfg[\"run_id\"]}))\n\n    # After all runs, launch evaluation\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    tee_subprocess(eval_cmd, results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for OHGW experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke test config.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment config.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where results will be stored.\")\n    args = parser.parse_args()\n\n    root_dir = Path(__file__).resolve().parent.parent  # project root\n    if args.smoke_test:\n        cfg_file = root_dir / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_file = root_dir / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_file), args.results_dir, args.smoke_test)\n", "model_py": "\"\"\"Model architectures and registry.\"\"\"\n\nfrom typing import Dict, Any\nimport torch.nn as nn\nimport torch\n\n\nclass MLP(nn.Module):\n    \"\"\"Simple Multi-Layer Perceptron for tabular or flattened image data.\"\"\"\n\n    def __init__(self, input_dim: int, num_classes: int, hidden_dims=(256, 128)):\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for h in hidden_dims:\n            layers += [nn.Linear(prev_dim, h), nn.ReLU(inplace=True)]\n            prev_dim = h\n        layers.append(nn.Linear(prev_dim, num_classes))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if x.dim() \u003e 2:\n            x = torch.flatten(x, 1)\n        return self.net(x)\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Minimal CNN suitable for 32\u00d732 images \u2013 serves as baseline.\n    This keeps dependencies minimal yet provides a working example.\"\"\"\n\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# Registry mapping model_type string to constructor\n_MODEL_REGISTRY = {\n    \"MLP\": MLP,\n    \"SimpleCNN\": SimpleCNN,\n    \"MODEL_PLACEHOLDER\": None,  # PLACEHOLDER: replaced in derivative experiments\n}\n\n\ndef get_model(model_cfg: Dict[str, Any], *, num_classes: int, input_shape):\n    \"\"\"Return instantiated nn.Module based on config.\"\"\"\n    model_type = model_cfg[\"type\"]\n    if model_type == \"MLP\":\n        if isinstance(input_shape, (tuple, list)):\n            flat_dim = int(torch.prod(torch.tensor(input_shape)))\n        else:\n            flat_dim = input_shape\n        return MLP(flat_dim, num_classes)\n    elif model_type == \"SimpleCNN\":\n        return SimpleCNN(num_classes)\n    elif model_type == \"MODEL_PLACEHOLDER\":\n        # PLACEHOLDER: Replace with specific model logic in derived experiments\n        raise NotImplementedError(\"MODEL_PLACEHOLDER must be replaced in the derived experiment step.\")\n    else:\n        raise ValueError(f\"Unknown model type {model_type}\")\n", "preprocess_py": "\"\"\"Common data loading \u0026 preprocessing with dataset placeholders.\"\"\"\n\nfrom typing import Tuple, Any, Dict\nfrom pathlib import Path\n\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, random_split\n\n\n# PLACEHOLDER: Additional dataset-specific imports can be added here.\n\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any], *, batch_size: int, num_workers: int, smoke_test: bool)\n    -\u003e Tuple[DataLoader, DataLoader, DataLoader, int, Any]:\n    \"\"\"Return train/val/test DataLoaders, num_classes and input_shape.\n\n    This function implements full data pipeline. Dataset-specific logic is handled\n    by branching on dataset_cfg[\"type\"]. Add new branches when introducing new\n    datasets in derived experiments.\n\n    Returns:\n        train_loader, val_loader, test_loader, num_classes, input_shape\n    \"\"\"\n    dataset_type = dataset_cfg[\"type\"]\n    data_root = Path(dataset_cfg.get(\"root\", \"./data\"))\n\n    if dataset_type == \"FakeData\":  # Generic vision dummy data for smoke tests\n        num_classes = dataset_cfg.get(\"num_classes\", 10)\n        image_size = dataset_cfg.get(\"image_size\", (3, 32, 32))\n        transform = transforms.ToTensor()\n        full_dataset = datasets.FakeData(\n            size=dataset_cfg.get(\"size\", 2000),\n            image_size=image_size,\n            num_classes=num_classes,\n            transform=transform,\n        )\n        val_size = int(0.2 * len(full_dataset))\n        test_size = int(0.1 * len(full_dataset))\n        train_size = len(full_dataset) - val_size - test_size\n        train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n        input_shape = image_size\n\n    elif dataset_type == \"DATASET_PLACEHOLDER\":\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic in derived experiments\n        raise NotImplementedError(\"Dataset type DATASET_PLACEHOLDER must be implemented in derived experiment.\")\n\n    else:\n        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n\n    if smoke_test:\n        # Use small subset for speed\n        train_set = torch.utils.data.Subset(train_set, range(min(256, len(train_set))))\n        val_set = torch.utils.data.Subset(val_set, range(min(256, len(val_set))))\n        test_set = torch.utils.data.Subset(test_set, range(min(256, len(test_set))))\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, test_loader, num_classes, input_shape\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\n\n[project]\nname = \"ohgw-experiments\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n    \"torch\u003e=2.0\",\n    \"torchvision\u003e=0.15\",\n    \"tqdm\",\n    \"pyyaml\",\n    \"pandas\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"scikit-learn\",\n]\n", "smoke_test_yaml": "description: \"Smoke test for CI \u2013 uses FakeData and minimal epochs.\"\nexperiments:\n  - run_id: smoke_simplecnn\n    seed: 0\n    dataset:\n      type: FakeData  # PLACEHOLDER will be replaced with real dataset in full experiments\n      size: 500\n      num_classes: 10\n    model:\n      type: SimpleCNN  # baseline model for dummy data\n    training:\n      batch_size: 32\n      epochs: 2\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    evaluation:\n      threshold: 0.2  # arbitrary low threshold to satisfy metric\n", "train_py": "import argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\ndef set_seed(seed: int):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    import random\n    import numpy as np\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy(output: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Compute top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        preds = output.argmax(dim=1)\n        correct = preds.eq(target).sum().item()\n    return correct / target.size(0)\n\n\ndef train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * x.size(0)\n        running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            output = model(x)\n            loss = criterion(output, y)\n            running_loss += loss.item() * x.size(0)\n            running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef run_experiment(cfg: Dict[str, Any], results_dir: Path, smoke_test: bool):\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 0)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = cfg[\"training\"].get(\"batch_size\", 128)\n    num_workers = cfg[\"training\"].get(\"num_workers\", 4)\n    max_epochs = cfg[\"training\"].get(\"epochs\", 20)\n    threshold = cfg[\"evaluation\"].get(\"threshold\")  # Optional\n\n    # Adjust for smoke test.\n    if smoke_test:\n        max_epochs = min(2, max_epochs)\n        batch_size = min(32, batch_size)\n\n    train_loader, val_loader, test_loader, num_classes, input_shape = get_dataloaders(\n        cfg[\"dataset\"], batch_size=batch_size, num_workers=num_workers, smoke_test=smoke_test\n    )\n\n    model = get_model(cfg[\"model\"], num_classes=num_classes, input_shape=input_shape).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer_cfg = cfg[\"training\"].get(\"optimizer\", {\"name\": \"SGD\", \"lr\": 0.1, \"momentum\": 0.9})\n    if optimizer_cfg[\"name\"].lower() == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=optimizer_cfg[\"lr\"], momentum=optimizer_cfg.get(\"momentum\", 0))\n    elif optimizer_cfg[\"name\"].lower() == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=optimizer_cfg[\"lr\"])\n    else:\n        raise ValueError(f\"Unsupported optimizer {optimizer_cfg[\u0027name\u0027]}\")\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": cfg,\n        \"epoch_metrics\": [],\n        \"best_val_accuracy\": 0.0,\n        \"best_epoch\": 0,\n        \"time_to_threshold\": None,\n    }\n\n    start_time = time.time()\n    for epoch in range(1, max_epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        elapsed = time.time() - start_time\n\n        results[\"epoch_metrics\"].append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n                \"elapsed_sec\": elapsed,\n            }\n        )\n\n        if val_acc \u003e results[\"best_val_accuracy\"]:\n            results[\"best_val_accuracy\"] = val_acc\n            results[\"best_epoch\"] = epoch\n            # Save checkpoint\n            torch.save(model.state_dict(), results_dir / \"best_model.pt\")\n\n        if threshold is not None and results[\"time_to_threshold\"] is None and val_acc \u003e= threshold:\n            results[\"time_to_threshold\"] = elapsed / 3600  # hours\n\n    # Final test evaluation\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n    results[\"final_test_accuracy\"] = test_acc\n    results[\"final_test_loss\"] = test_loss\n\n    # Persist results\n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Print JSON to stdout for GitHub Actions to capture\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run-specific YAML config file.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store outputs.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test.\")\n    args = parser.parse_args()\n\n    import yaml\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    run_dir = Path(args.results_dir)\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    run_experiment(cfg, run_dir, args.smoke_test)\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-2
- Description: Objective / Hypothesis: Test zero-shot generalisation and robustness of OHGW in a different modality (language modelling) and under distribution shift.

Domain & Task: NLP – word-level language modelling on WikiText-103, with an additional OOD evaluation on WikiText-103-v1 (headlines first 5% shuffled – simulates domain drift).

Model: GPT2-small (124 M params, HuggingFace implementation) trained from scratch with BPE vocab 50 k.

Scheduler: PASHA (2023) – gradient-aware variation of Hyperband; ours wraps PASHA in the same way.

Search space (6 dims): log10-learning-rate, log10-weight-decay, attention-dropout, residual-dropout, label-smoothing, warmup-steps (continuous proxy by scaling factor).

Run variations:
• pasha-baseline – vanilla PASHA.
• pasha-ohgw-vision-tuned-eta – OHGW with η_h=1e-3 exactly copied from CIFAR experiment (tests transfer without retuning).
• pasha-ohgw-low-eta – OHGW with η_h=3e-4 (sensitivity extremum).
• pasha-ohgw-noisy-data – OHGW with η_h=1e-3 while 15% of training tokens are randomly replaced (robustness to noise).

Dataset processing: SentencePiece BPE (shared).  Sequence length 1024 tokens; dynamic batching up to 2M tokens/GPU.  Train/val/test split 238M / 8M / 8M tokens.  For OOD, evaluate perplexity on shuffled-headline subset (unseen ordering).

Training loop & budget: 50 training epochs (~250 k updates), AdamW, cosine LR.  PASHA minimum resource per config = 2 epochs, rungs ×2.  Total compute budget per replicate 16 GPU×hours (8 GPUs ×2 h).  24 replicates.

Evaluation metrics:
Primary – T@ppl=30 on validation set (wall-clock & GPU-h).
Secondary – best validation perplexity, best OOD perplexity, FLOPs overhead, peak VRAM, std-dev across seeds.
Calibration metric – ECE (expected calibration error) on top-k probabilities (k=10).

Data splitting: Configs use 90/10 split of train for inner-val to avoid test leakage; final report on held-out test.

Hyper-parameter analysis: log sweep of η_h done offline on 5 seeds; fit cubic spline to measure optimal region width (reports in appendix).

Robustness procedures:
1. Data noise: see pasha-ohgw-noisy-data.
2. Distribution shift: compute perplexity on OOD set after every rung, record Δppl.
3. Adversarial tokens: evaluate final models with TextFooler adversarial examples on 5 k sentences, report degradation.

Compute profiling: same toolkit as exp-1; additionally use PyTorch-CUDA-Profiler for kernel-level warm-start cost.

Example trial wrapper:
```python
loss = lm_model(input_ids, labels=input_ids).loss
hg = torch.autograd.grad(loss, cfg.continuous())
with torch.no_grad():
    for p,g in zip(cfg.continuous(), hg):
        p -= eta_h * g    # no extra unroll
pasha_scheduler.launch(cfg)
```

Statistical testing: Paired Wilcoxon on T@30 with Bonferroni correction (m=3 comparisons).  Significance if p<0.016.

Success criteria: vision-tuned η_h version achieves ≥15% speed-up vs. baseline on T@30 with ≤0.5 ppl regression on final test; effect still ≥10% under noisy data.  Extra compute overhead ≤5% of total FLOPs.
- Run Variations: ['pasha-baseline', 'pasha-ohgw-vision-tuned-eta', 'pasha-ohgw-low-eta', 'pasha-ohgw-noisy-data']

# Derived Experiment Code (To be validated)

{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # For non-interactive backends\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIGURE_PARAMS = {\n    \"pdf.fonttype\": 42,\n    \"ps.fonttype\": 42,\n    \"font.size\": 12,\n    \"legend.fontsize\": 10,\n}\nplt.rcParams.update(FIGURE_PARAMS)\n\n\ndef load_results(results_dir: Path) -\u003e List[Dict[str, Any]]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef plot_best_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=\"best_val_accuracy\", data=df)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.ylim(0, 1)\n    # Annotate bars\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"best_val_accuracy\"] + 0.01, f\"{row[\u0027best_val_accuracy\u0027]*100:.1f}%\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(out_path / \"accuracy.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef plot_loss_curves(all_results: List[Dict[str, Any]], out_path: Path):\n    for res in all_results:\n        run_id = res[\"run_id\"]\n        epochs = [m[\"epoch\"] for m in res[\"epoch_metrics\"]]\n        train_losses = [m[\"train_loss\"] for m in res[\"epoch_metrics\"]]\n        val_losses = [m[\"val_loss\"] for m in res[\"epoch_metrics\"]]\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, train_losses, label=\"Train loss\")\n        plt.plot(epochs, val_losses, label=\"Val loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"Loss Curve - {run_id}\")\n        # Annotate final values\n        plt.text(epochs[-1], val_losses[-1], f\"{val_losses[-1]:.3f}\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(out_path / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n        plt.close()\n\n\ndef main(results_dir: str):\n    results_path = Path(results_dir)\n    all_results = load_results(results_path)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json found in {results_dir}\")\n\n    # Convert to pandas DataFrame for easy handling\n    df = pd.DataFrame([\n        {\n            \"run_id\": r[\"run_id\"],\n            \"best_val_accuracy\": r[\"best_val_accuracy\"],\n            \"time_to_threshold\": r.get(\"time_to_threshold\"),\n            \"final_test_accuracy\": r.get(\"final_test_accuracy\"),\n        }\n        for r in all_results\n    ])\n\n    # Print numerical comparison to stdout\n    comparison = df.to_dict(orient=\"records\")\n    print(json.dumps({\"comparison\": comparison}, indent=2))\n\n    # Create figure output directory\n    figs_dir = results_path / \"figures\"\n    figs_dir.mkdir(exist_ok=True)\n\n    # Plot best accuracy bar chart\n    plot_best_accuracy(df, figs_dir)\n\n    # Plot loss curves per run\n    plot_loss_curves(all_results, figs_dir)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory containing all run subdirs.\")\n    args = parser.parse_args()\n    main(args.results_dir)\n", "full_experiment_yaml": "description: \"Experiment exp-2: WikiText-103 language modelling with PASHA baseline and OHGW variants\"\nexperiments:\n  - run_id: pasha-baseline\n    seed: 0\n    dataset:\n      type: WikiText103\n      hf_name: \"yehzw/wikitext-103\"\n      seq_length: 1024\n      tokenizer_name: \"gpt2\"\n    model:\n      type: GPT2SmallLM\n    training:\n      batch_size: 2  # fits 12GB GPU with seq_len 1024\n      epochs: 50\n      optimizer:\n        name: AdamW\n        lr: 3e-4\n        weight_decay: 0.01\n    evaluation:\n      metric: perplexity\n      threshold: 30  # T@ppl=30\n\n  - run_id: pasha-ohgw-vision-tuned-eta\n    seed: 0\n    dataset:\n      type: WikiText103\n      hf_name: \"yehzw/wikitext-103\"\n      seq_length: 1024\n      tokenizer_name: \"gpt2\"\n    model:\n      type: GPT2SmallLM\n    training:\n      batch_size: 2\n      epochs: 50\n      optimizer:\n        name: AdamW\n        lr: 3e-4\n        weight_decay: 0.01\n    ohgw:\n      eta_h: 1e-3\n    evaluation:\n      metric: perplexity\n      threshold: 30\n\n  - run_id: pasha-ohgw-low-eta\n    seed: 0\n    dataset:\n      type: WikiText103\n      hf_name: \"yehzw/wikitext-103\"\n      seq_length: 1024\n      tokenizer_name: \"gpt2\"\n    model:\n      type: GPT2SmallLM\n    training:\n      batch_size: 2\n      epochs: 50\n      optimizer:\n        name: AdamW\n        lr: 3e-4\n        weight_decay: 0.01\n    ohgw:\n      eta_h: 3e-4\n    evaluation:\n      metric: perplexity\n      threshold: 30\n\n  - run_id: pasha-ohgw-noisy-data\n    seed: 0\n    dataset:\n      type: WikiText103\n      hf_name: \"yehzw/wikitext-103\"\n      seq_length: 1024\n      tokenizer_name: \"gpt2\"\n      noise_rate: 0.15  # 15% token replacement for robustness test\n    model:\n      type: GPT2SmallLM\n    training:\n      batch_size: 2\n      epochs: 50\n      optimizer:\n        name: AdamW\n        lr: 3e-4\n        weight_decay: 0.01\n    ohgw:\n      eta_h: 1e-3\n    evaluation:\n      metric: perplexity\n      threshold: 30\n", "main_py": "import argparse\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport yaml\nimport json\nimport datetime\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run subprocess while teeing its stdout / stderr to files and console.\"\"\"\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1)\n        assert process.stdout is not None and process.stderr is not None\n        # Stream stdout\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if not stdout_line and not stderr_line and process.poll() is not None:\n                break\n            if stdout_line:\n                sys.stdout.buffer.write(stdout_line)\n                out_f.write(stdout_line)\n                out_f.flush()\n            if stderr_line:\n                sys.stderr.buffer.write(stderr_line)\n                err_f.write(stderr_line)\n                err_f.flush()\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n\ndef launch_train(run_cfg: Dict[str, Any], results_root: Path, smoke_test: bool):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save run-specific config so that train.py can read it.\n    cfg_path = run_dir / \"config.yaml\"\n    with cfg_path.open(\"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # Build subprocess command\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--config\",\n        str(cfg_path),\n        \"--results-dir\",\n        str(run_dir),\n    ]\n    if smoke_test:\n        cmd.append(\"--smoke-test\")\n\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    tee_subprocess(cmd, stdout_path, stderr_path)\n\n\ndef run_all(cfg_file: str, results_dir: str, smoke_test: bool):\n    with open(cfg_file) as f:\n        exp_cfg = yaml.safe_load(f)\n\n    if \"experiments\" not in exp_cfg:\n        raise KeyError(\"Config YAML must contain \u0027experiments\u0027 list.\")\n\n    results_root = Path(results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    description = exp_cfg.get(\"description\", \"No description provided.\")\n    print(\n        json.dumps(\n            {\n                \"timestamp\": datetime.datetime.now().isoformat(),\n                \"experiment_description\": description,\n            },\n            indent=2,\n        )\n    )\n\n    for run_cfg in exp_cfg[\"experiments\"]:\n        print(json.dumps({\"status\": \"starting\", \"run_id\": run_cfg[\"run_id\"]}))\n        launch_train(run_cfg, results_root, smoke_test)\n        print(json.dumps({\"status\": \"finished\", \"run_id\": run_cfg[\"run_id\"]}))\n\n    # After all runs, launch evaluation\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    tee_subprocess(eval_cmd, results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for OHGW experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke test config.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment config.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where results will be stored.\")\n    args = parser.parse_args()\n\n    root_dir = Path(__file__).resolve().parent.parent  # project root\n    if args.smoke_test:\n        cfg_file = root_dir / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_file = root_dir / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_file), args.results_dir, args.smoke_test)\n", "model_py": "\"\"\"Model architectures and registry \u2013 extended with GPT2SmallLM.\"\"\"\n\nfrom typing import Dict, Any\nimport torch.nn as nn\nimport torch\n\n# Transformers import\nfrom transformers import GPT2Config, GPT2LMHeadModel\n\n\nclass MLP(nn.Module):\n    \"\"\"Simple Multi-Layer Perceptron for tabular or flattened image data.\"\"\"\n\n    def __init__(self, input_dim: int, num_classes: int, hidden_dims=(256, 128)):\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for h in hidden_dims:\n            layers += [nn.Linear(prev_dim, h), nn.ReLU(inplace=True)]\n            prev_dim = h\n        layers.append(nn.Linear(prev_dim, num_classes))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if x.dim() \u003e 2:\n            x = torch.flatten(x, 1)\n        return self.net(x)\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Minimal CNN suitable for 32\u00d732 images \u2013 serves as baseline.\"\"\"\n\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\nclass GPT2SmallLM(nn.Module):\n    \"\"\"GPT-2 small (124M) language model initialised from scratch.\"\"\"\n\n    def __init__(self, vocab_size: int, seq_len: int = 1024):\n        super().__init__()\n        config = GPT2Config(\n            vocab_size=vocab_size,\n            n_positions=seq_len,\n            n_ctx=seq_len,\n            n_embd=768,\n            n_layer=12,\n            n_head=12,\n        )\n        self.model = GPT2LMHeadModel(config)\n\n    def forward(self, input_ids):\n        out = self.model(input_ids=input_ids, use_cache=False)\n        return out.logits  # (B, T, V)\n\n\n# Registry mapping model_type string to constructor\n_MODEL_REGISTRY = {\n    \"MLP\": MLP,\n    \"SimpleCNN\": SimpleCNN,\n    \"GPT2SmallLM\": GPT2SmallLM,\n}\n\n\ndef get_model(model_cfg: Dict[str, Any], *, num_classes: int, input_shape):\n    \"\"\"Return instantiated nn.Module based on config.\"\"\"\n    model_type = model_cfg[\"type\"]\n    if model_type == \"MLP\":\n        if isinstance(input_shape, (tuple, list)):\n            flat_dim = int(torch.prod(torch.tensor(input_shape)))\n        else:\n            flat_dim = input_shape\n        return MLP(flat_dim, num_classes)\n    elif model_type == \"SimpleCNN\":\n        return SimpleCNN(num_classes)\n    elif model_type == \"GPT2SmallLM\":\n        seq_len = input_shape[0] if isinstance(input_shape, (tuple, list)) else 1024\n        return GPT2SmallLM(vocab_size=num_classes, seq_len=seq_len)\n    else:\n        raise ValueError(f\"Unknown model type {model_type}\")\n", "preprocess_py": "\"\"\"Common data loading \u0026 preprocessing with concrete dataset implementations.\"\"\"\n\nfrom typing import Tuple, Any, Dict\nfrom pathlib import Path\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nfrom torchvision import transforms, datasets\n\n# NLP imports\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n\nclass LMDataset(Dataset):\n    \"\"\"Lightweight wrapper around a HuggingFace token dataset for language modelling.\"\"\"\n\n    def __init__(\n        self,\n        hf_dataset,\n        seq_length: int,\n        vocab_size: int,\n        noise_rate: float = 0.0,\n    ):\n        self.data = hf_dataset\n        self.seq_length = seq_length\n        self.vocab_size = vocab_size\n        self.noise_rate = noise_rate\n\n    def __len__(self):\n        return len(self.data)\n\n    def _apply_noise(self, tokens: torch.Tensor) -\u003e torch.Tensor:\n        if self.noise_rate \u003c= 0:\n            return tokens\n        # Randomly replace tokens with random vocab id with probability noise_rate\n        mask = torch.rand_like(tokens.float()) \u003c self.noise_rate\n        random_tokens = torch.randint(0, self.vocab_size, size=tokens.shape, dtype=torch.long)\n        return torch.where(mask, random_tokens, tokens)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        input_ids = item[\"input_ids\"].clone()  # tensor\n        input_ids = self._apply_noise(input_ids)\n        return input_ids, input_ids  # labels identical; trainer will shift\n\n\n# -----------------------------------------------------------------------------\n# Main entry point\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(\n    dataset_cfg: Dict[str, Any],\n    *,\n    batch_size: int,\n    num_workers: int,\n    smoke_test: bool,\n) -\u003e Tuple[DataLoader, DataLoader, DataLoader, int, Any]:\n    \"\"\"Return train/val/test DataLoaders, num_classes (or vocab size) and input_shape.\"\"\"\n\n    dataset_type = dataset_cfg[\"type\"]\n    data_root = Path(dataset_cfg.get(\"root\", \"./data\"))\n\n    if dataset_type == \"FakeData\":  # Vision dummy data for CI\n        num_classes = dataset_cfg.get(\"num_classes\", 10)\n        image_size = dataset_cfg.get(\"image_size\", (3, 32, 32))\n        transform = transforms.ToTensor()\n        full_dataset = datasets.FakeData(\n            size=dataset_cfg.get(\"size\", 2000),\n            image_size=image_size,\n            num_classes=num_classes,\n            transform=transform,\n        )\n        val_size = int(0.2 * len(full_dataset))\n        test_size = int(0.1 * len(full_dataset))\n        train_size = len(full_dataset) - val_size - test_size\n        train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n        input_shape = image_size\n\n    elif dataset_type == \"WikiText103\":\n        # --------------------------------------------------------------\n        # WikiText-103 language modelling dataset\n        # --------------------------------------------------------------\n        hf_name = dataset_cfg.get(\"hf_name\", \"yehzw/wikitext-103\")\n        seq_length = dataset_cfg.get(\"seq_length\", 1024)\n        tokenizer_name = dataset_cfg.get(\"tokenizer_name\", \"gpt2\")\n        noise_rate = dataset_cfg.get(\"noise_rate\", 0.0)\n\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n\n        def _tokenize(example):\n            return tokenizer(example[\"text\"])\n\n        # Load splits\n        raw_datasets = load_dataset(hf_name)\n        # Tokenise\n        tokenized = raw_datasets.map(\n            _tokenize,\n            batched=True,\n            remove_columns=[\"text\"],\n            num_proc=2,\n            desc=\"Tokenising\",\n        )\n\n        # Group into fixed-length chunks\n        def _group(examples):\n            concatenated = sum(examples[\"input_ids\"], [])\n            total_length = len(concatenated)\n            if total_length \u003e= seq_length:\n                total_length = (total_length // seq_length) * seq_length\n            result = {\n                \"input_ids\": [\n                    concatenated[i : i + seq_length]\n                    for i in range(0, total_length, seq_length)\n                ]\n            }\n            return result\n\n        lm_datasets = tokenized.map(\n            _group,\n            batched=True,\n            num_proc=2,\n            desc=\"Grouping\",\n        )\n        # Set PyTorch format\n        lm_datasets.set_format(type=\"torch\", columns=[\"input_ids\"])\n\n        vocab_size = tokenizer.vocab_size\n        train_set = LMDataset(lm_datasets[\"train\"], seq_length, vocab_size, noise_rate)\n        val_set = LMDataset(lm_datasets[\"validation\"], seq_length, vocab_size, 0.0)\n        test_set = LMDataset(lm_datasets[\"test\"], seq_length, vocab_size, 0.0)\n        num_classes = vocab_size\n        input_shape = (seq_length,)\n\n    else:\n        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n\n    # Smoke-test sub-sampling\n    if smoke_test:\n        sub_len = min(256, len(train_set))\n        train_set = torch.utils.data.Subset(train_set, range(sub_len))\n        val_set = torch.utils.data.Subset(val_set, range(min(128, len(val_set))))\n        test_set = torch.utils.data.Subset(test_set, range(min(128, len(test_set))))\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    test_loader = DataLoader(\n        test_set,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_loader, val_loader, test_loader, num_classes, input_shape\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\n\n[project]\nname = \"ohgw-experiments\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n    \"torch\u003e=2.0\",\n    \"torchvision\u003e=0.15\",\n    \"tqdm\",\n    \"pyyaml\",\n    \"pandas\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"scikit-learn\",\n    \"datasets\u003e=2.14\",\n    \"transformers\u003e=4.32\",\n    \"sentencepiece\",\n]\n", "smoke_test_yaml": "description: \"Smoke test for CI \u2013 lightweight variants of all runs\"\nexperiments:\n  - run_id: pasha-baseline\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n    model:\n      type: SimpleCNN\n    training:\n      batch_size: 32\n      epochs: 2\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    evaluation:\n      threshold: 0.2\n  - run_id: pasha-ohgw-vision-tuned-eta\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n    model:\n      type: SimpleCNN\n    training:\n      batch_size: 32\n      epochs: 2\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    ohgw:\n      eta_h: 1e-3\n    evaluation:\n      threshold: 0.2\n  - run_id: pasha-ohgw-low-eta\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n    model:\n      type: SimpleCNN\n    training:\n      batch_size: 32\n      epochs: 2\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    ohgw:\n      eta_h: 3e-4\n    evaluation:\n      threshold: 0.2\n  - run_id: pasha-ohgw-noisy-data\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n    model:\n      type: SimpleCNN\n    training:\n      batch_size: 32\n      epochs: 2\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    ohgw:\n      eta_h: 1e-3\n    evaluation:\n      threshold: 0.2\n", "train_py": "import argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\ndef set_seed(seed: int):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    import random\n    import numpy as np\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy(output: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Compute accuracy.\n    For image classification (output dim == 2) computes top-1 accuracy per sample.\n    For language modelling (output dim == 3) computes token-level accuracy ignoring\n    positions whose target is -100 (masked).\n    \"\"\"\n    with torch.no_grad():\n        if output.dim() == 2:  # (B, C)\n            preds = output.argmax(dim=1)\n            correct = preds.eq(target).sum().item()\n            total = target.size(0)\n        elif output.dim() == 3:  # (B, T, V)\n            preds = output.argmax(dim=-1)\n            mask = target != -100  # ignore padded / masked positions\n            correct = preds.eq(target).masked_select(mask).sum().item()\n            total = mask.sum().item()\n        else:\n            raise ValueError(\"Unsupported output dimension for accuracy computation\")\n    return correct / max(total, 1)\n\n\ndef _compute_loss(criterion, output: torch.Tensor, target: torch.Tensor):\n    \"\"\"Generic loss computation that supports both CLS and LM.\"\"\"\n    # Classification (B, C) vs Language model (B, T, V)\n    if output.dim() == 2:\n        return criterion(output, target)\n    elif output.dim() == 3:\n        # Shift for next-token prediction\n        vocab_size = output.size(-1)\n        # Predict token t+1 given tokens up to t (GPT-style)\n        pred = output[:, :-1, :].contiguous().view(-1, vocab_size)\n        gold = target[:, 1:].contiguous().view(-1)\n        return criterion(pred, gold)\n    else:\n        raise ValueError(\"Unsupported output dimension for loss computation\")\n\n\ndef train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    total_samples = 0\n    for batch in loader:\n        if isinstance(batch, (list, tuple)) and len(batch) == 2:\n            x, y = batch\n        else:  # Dict coming from HF datasets \u2192 {\u0027input_ids\u0027: tensor, \u0027labels\u0027: tensor}\n            x, y = batch[\"input_ids\"], batch[\"labels\"] if \"labels\" in batch else batch[\"input_ids\"]\n        x, y = x.to(device), y.to(device)\n\n        optimizer.zero_grad()\n        output = model(x)\n        loss = _compute_loss(criterion, output, y)\n        loss.backward()\n        optimizer.step()\n\n        bs = x.size(0)\n        running_loss += loss.item() * bs\n        running_acc += accuracy(output, y) * bs\n        total_samples += bs\n    epoch_loss = running_loss / total_samples\n    epoch_acc = running_acc / total_samples\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    total_samples = 0\n    with torch.no_grad():\n        for batch in loader:\n            if isinstance(batch, (list, tuple)) and len(batch) == 2:\n                x, y = batch\n            else:\n                x, y = batch[\"input_ids\"], batch[\"labels\"] if \"labels\" in batch else batch[\"input_ids\"]\n            x, y = x.to(device), y.to(device)\n            output = model(x)\n            loss = _compute_loss(criterion, output, y)\n            bs = x.size(0)\n            running_loss += loss.item() * bs\n            running_acc += accuracy(output, y) * bs\n            total_samples += bs\n    epoch_loss = running_loss / total_samples\n    epoch_acc = running_acc / total_samples\n    return epoch_loss, epoch_acc\n\n\ndef run_experiment(cfg: Dict[str, Any], results_dir: Path, smoke_test: bool):\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 0)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = cfg[\"training\"].get(\"batch_size\", 2)\n    num_workers = cfg[\"training\"].get(\"num_workers\", 4)\n    max_epochs = cfg[\"training\"].get(\"epochs\", 2)\n    threshold = cfg[\"evaluation\"].get(\"threshold\")  # Optional\n\n    # Adjust for smoke test.\n    if smoke_test:\n        max_epochs = min(2, max_epochs)\n        batch_size = min(4, batch_size)\n\n    train_loader, val_loader, test_loader, num_classes, input_shape = get_dataloaders(\n        cfg[\"dataset\"], batch_size=batch_size, num_workers=num_workers, smoke_test=smoke_test\n    )\n\n    model = get_model(cfg[\"model\"], num_classes=num_classes, input_shape=input_shape).to(device)\n\n    # Criterion: use ignore_index for language modelling\n    if cfg[\"model\"][\"type\"].startswith(\"GPT2\"):\n        criterion = nn.CrossEntropyLoss(ignore_index=-100)\n    else:\n        criterion = nn.CrossEntropyLoss()\n\n    optimizer_cfg = cfg[\"training\"].get(\"optimizer\", {\"name\": \"AdamW\", \"lr\": 3e-4})\n    opt_name = optimizer_cfg[\"name\"].lower()\n    if opt_name == \"sgd\":\n        optimizer = optim.SGD(\n            model.parameters(), lr=optimizer_cfg[\"lr\"], momentum=optimizer_cfg.get(\"momentum\", 0)\n        )\n    elif opt_name == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=optimizer_cfg[\"lr\"])\n    elif opt_name == \"adamw\":\n        optimizer = optim.AdamW(\n            model.parameters(), lr=optimizer_cfg[\"lr\"], weight_decay=optimizer_cfg.get(\"weight_decay\", 0)\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer {optimizer_cfg[\u0027name\u0027]}\")\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": cfg,\n        \"epoch_metrics\": [],\n        \"best_val_accuracy\": 0.0,\n        \"best_epoch\": 0,\n        \"time_to_threshold\": None,\n    }\n\n    start_time = time.time()\n    for epoch in range(1, max_epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        elapsed = time.time() - start_time\n\n        results[\"epoch_metrics\"].append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n                \"elapsed_sec\": elapsed,\n            }\n        )\n\n        if val_acc \u003e results[\"best_val_accuracy\"]:\n            results[\"best_val_accuracy\"] = val_acc\n            results[\"best_epoch\"] = epoch\n            # Save checkpoint\n            torch.save(model.state_dict(), results_dir / \"best_model.pt\")\n\n        if threshold is not None and results[\"time_to_threshold\"] is None:\n            # For LM threshold might be perplexity.\n            if cfg[\"evaluation\"].get(\"metric\", \"accuracy\") == \"perplexity\":\n                if torch.exp(torch.tensor(val_loss)).item() \u003c= threshold:\n                    results[\"time_to_threshold\"] = elapsed / 3600\n            else:\n                if val_acc \u003e= threshold:\n                    results[\"time_to_threshold\"] = elapsed / 3600  # hours\n\n    # Final test evaluation\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n    results[\"final_test_accuracy\"] = test_acc\n    results[\"final_test_loss\"] = test_loss\n\n    # Persist results\n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Print JSON to stdout for GitHub Actions to capture\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run-specific YAML config file.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store outputs.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test.\")\n    args = parser.parse_args()\n\n    import yaml\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    run_dir = Path(args.results_dir)\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    run_experiment(cfg, run_dir, args.smoke_test)\n"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "OHGW functionality that differentiates the run variations is not implemented anywhere in the derived source. Fields under `ohgw:` in the YAML files are never read and no hyper-gradient warm-start step is executed; thus all four run variations behave identically, violating criterion 2 (complete placeholder replacement & variation implementation)."
}
