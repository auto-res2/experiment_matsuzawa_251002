
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"After all individual runs are finished, this script consolidates their\nnumeric results and generates comparison figures (.pdf).\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom src.model import BaseModel  # pylint: disable=import-error\n\n\nFIGURE_STYLE = {\n    \"figure.dpi\": 300,\n    \"font.size\": 12,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n}\n\n\nfor k, v in FIGURE_STYLE.items():\n    plt.rcParams[k] = v  # type: ignore[arg-type]\n\ndef load_results(run_dir: Path) -\u003e Dict:\n    with open(run_dir / \"results.json\", \"r\", encoding=\"utf-8\") as fp:\n        return json.load(fp)\n\n\ndef make_accuracy_bar(results: List[Dict], save_dir: Path):\n    run_ids = [r[\"run_id\"] for r in results]\n    accs = [r[\"best_val_acc\"] for r in results]\n\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=run_ids, y=accs, palette=\"viridis\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n\n    # Annotate bars\n    for i, a in enumerate(accs):\n        plt.text(i, a + 0.001, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n\n    plt.ylim(0, max(accs) * 1.1)\n    plt.tight_layout()\n\n    fname = save_dir / \"accuracy.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return str(fname.name)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate \u0026 compare experimental variations\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory produced by main.py\")\n    args = parser.parse_args()\n\n    results_root = Path(args.results_dir)\n    evaluation_dir = results_root / \"evaluation\"\n    evaluation_dir.mkdir(parents=True, exist_ok=True)\n\n    # Gather all result files (exclude evaluation dir itself)\n    run_dirs = [p for p in results_root.iterdir() if p.is_dir() and p.name != \"evaluation\"]\n    run_results: List[Dict] = []\n    for d in run_dirs:\n        try:\n            run_results.append(load_results(d))\n        except FileNotFoundError:\n            print(f\"[WARN] results.json not found in {d}. Skipping.\")\n\n    if not run_results:\n        print(\"No run results found; evaluation aborted.\")\n        return\n\n    # Generate figures -------------------------------------------------\n    figures: List[str] = []\n    figures.append(make_accuracy_bar(run_results, evaluation_dir))\n\n    # Structured comparison output ------------------------------------\n    summary = {\n        \"num_runs\": len(run_results),\n        \"runs\": [{\"run_id\": r[\"run_id\"], \"best_val_acc\": r[\"best_val_acc\"]} for r in run_results],\n        \"figures\": figures,\n    }\n\n    with open(evaluation_dir / \"comparison.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(summary, fp, indent=2)\n\n    # Print JSON to stdout so that calling process can capture aggregated info\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# Template for the *real* experiments. Concrete values will be filled in\n# during the derive_specific step. Only placeholder structure is provided here.\n\nexperiments:\n  # ------------------------------------------------------------------\n  # PLACEHOLDER: Will be replaced with actual experiment variations.\n  # Copy / paste the block below and fill \u003c...\u003e as needed.\n  # ------------------------------------------------------------------\n  # - run_id: \u003cunique_name\u003e\n  #   dataset: DATASET_PLACEHOLDER\n  #   model: MODEL_PLACEHOLDER\n  #   training:\n  #     epochs: 100\n  #     batch_size: 128\n  #     lr: 0.1\n  #     device: cuda\n\n# Optional global defaults can also be placed at the root level, e.g. seed.\nseed: 0\n# Additional scheduler / search-space / HPO settings will be injected later.\n\n# END OF TEMPLATE\n", "main_py": "\"\"\"Main orchestrator: reads config YAML, sequentially launches train.py for each\nvariation, then triggers evaluate.py. All stdout/stderr from subprocesses are\ntee-redirected into log files *and* the main process\u0027 stdout/stderr.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\nimport yaml  # type: ignore\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef tee_stream(stream, log_file_path: Path):\n    \"\"\"Forward bytes from a stream to both sys.stdout/sys.stderr and a log file.\"\"\"\n\n    log_file = open(log_file_path, \"wb\")\n\n    while True:\n        chunk = stream.readline()\n        if not chunk:\n            break\n        log_file.write(chunk)\n        log_file.flush()\n        # Decode for console display (fallback to utf-8 ignore)\n        try:\n            sys.stdout.buffer.write(chunk)  # pylint: disable=no-member\n            sys.stdout.flush()\n        except Exception:  # pylint: disable=broad-except\n            # If writing raw bytes fails (e.g. non-UTF-8), try decode-ignore\n            sys.stdout.write(chunk.decode(errors=\"ignore\"))\n            sys.stdout.flush()\n\n    log_file.close()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Launch a subprocess and tee its stdout/stderr to log files.\"\"\"\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)  # noqa: S603, S607\n    # Stream stdout\n    tee_stream(proc.stdout, stdout_path)  # type: ignore[arg-type]\n    tee_stream(proc.stderr, stderr_path)  # type: ignore[arg-type]\n    proc.wait()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess {\u0027 \u0027.join(cmd)} failed with code {proc.returncode}\")\n\n\ndef prepare_run_cfg(base_cfg: Dict, run: Dict, run_dir: Path) -\u003e Path:\n    \"\"\"Merge base \u0026 run-specific config, dump to YAML file inside run_dir.\"\"\"\n    merged = {**base_cfg, **run}\n    cfg_path = run_dir / \"config.yaml\"\n    with open(cfg_path, \"w\", encoding=\"utf-8\") as fp:\n        yaml.safe_dump(merged, fp)\n    return cfg_path\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OHGW experiment launcher (foundation)\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store all outputs\")\n    args = parser.parse_args()\n\n    root_results = Path(args.results_dir).expanduser().resolve()\n    root_results.mkdir(parents=True, exist_ok=True)\n\n    cfg_file = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_file, \"r\", encoding=\"utf-8\") as fp:\n        full_cfg: Dict = yaml.safe_load(fp)\n\n    global_cfg = {k: v for k, v in full_cfg.items() if k != \"experiments\"}\n    experiments: List[Dict] = full_cfg.get(\"experiments\", [])\n\n    if not experiments:\n        print(\"No experiments defined in the configuration file.\")\n        sys.exit(1)\n\n    # ------------------------------------------------------------------\n    # Launch each experiment sequentially\n    # ------------------------------------------------------------------\n    for run in experiments:\n        run_id = run[\"run_id\"]\n        run_dir = root_results / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        cfg_path = prepare_run_cfg(global_cfg, run, run_dir)\n\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(cfg_path), \"--results-dir\", str(run_dir)]\n        print(f\"\\n[MAIN] Launching run \u0027{run_id}\u0027 ...\")\n        run_subprocess(cmd, stdout_path, stderr_path)\n        print(f\"[MAIN] Run \u0027{run_id}\u0027 finished successfully.\")\n\n    # ------------------------------------------------------------------\n    # After all runs complete, launch evaluation\n    # ------------------------------------------------------------------\n    evaluation_dir = root_results / \"evaluation\"\n    evaluation_dir.mkdir(exist_ok=True)\n\n    stdout_path = evaluation_dir / \"stdout.log\"\n    stderr_path = evaluation_dir / \"stderr.log\"\n\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(root_results)]\n    print(\"\\n[MAIN] Launching evaluation ...\")\n    run_subprocess(eval_cmd, stdout_path, stderr_path)\n    print(\"[MAIN] Evaluation finished successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"Model architectures and persistence utilities.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, Dict\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\"BaseModel\", \"DummyClassifier\", \"build_model\"]\n\n\nclass BaseModel(nn.Module):\n    \"\"\"Base class providing save / load convenience wrappers.\"\"\"\n\n    def save_checkpoint(self, path, config: Dict[str, Any] | None = None, metadata: Dict[str, Any] | None = None):\n        ckpt = {\n            \"state_dict\": self.state_dict(),\n            \"config\": config or {},\n            \"metadata\": metadata or {},\n            \"class_name\": self.__class__.__name__,\n        }\n        torch.save(ckpt, path)\n\n    @classmethod\n    def load_checkpoint(cls, path, device: str | torch.device = \"cpu\") -\u003e tuple[\"BaseModel\", Dict]:\n        ckpt = torch.load(path, map_location=device)\n        class_name = ckpt.get(\"class_name\", \"DummyClassifier\")\n        if class_name != cls.__name__:\n            raise ValueError(\n                f\"Checkpoint was saved from class \u0027{class_name}\u0027 but you are loading with \u0027{cls.__name__}\u0027. \"\n                \"Call the correct subclass\u0027s load_checkpoint instead.\"\n            )\n        init_args = ckpt.get(\"config\", {}).get(\"init_args\", {})\n        model = cls(**init_args)  # type: ignore[arg-type]\n        model.load_state_dict(ckpt[\"state_dict\"])\n        return model, ckpt.get(\"metadata\", {})\n\n\nclass DummyClassifier(BaseModel):\n    \"\"\"A tiny linear classifier for smoke-test purposes.\"\"\"\n\n    def __init__(self, input_dim: int = 10, num_classes: int = 2):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x):  # type: ignore[override]\n        return self.fc(x)\n\n\ndef build_model(cfg: Dict):  # noqa: D401  (simple factory)\n    \"\"\"Factory function building a model instance from config.\n\n    Model-specific hyper-parameters should live under cfg[\u0027model_init\u0027].\n    \"\"\"\n\n    model_name: str = cfg.get(\"model\", \"dummy\")\n\n    # Baseline dummy model --------------------------------------------------\n    if model_name == \"dummy\":\n        init_args = cfg.get(\"model_init\", {})\n        return DummyClassifier(**init_args)\n\n    # ----------------------------------------------------------------------\n    # PLACEHOLDER: Add real model architectures here.\n    # ----------------------------------------------------------------------\n    raise ValueError(f\"Unknown model type: {model_name}\")", "preprocess_py": "\"\"\"Common preprocessing pipeline WITH placeholders for dataset-specific logic.\nThe module guarantees that *some* DataLoader is returned even for smoke tests by\nfalling back to a tiny synthetic dataset when an unknown dataset is requested.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n__all__ = [\"get_dataloader\"]\n\n\nclass DummyDataset(Dataset):\n    \"\"\"A 10-dimensional random classification task for smoke tests.\"\"\"\n\n    def __init__(self, n_samples: int = 256, input_dim: int = 10, num_classes: int = 2):\n        self.x = torch.randn(n_samples, input_dim)\n        self.y = torch.randint(0, num_classes, (n_samples,))\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n    def __len__(self):\n        return len(self.x)\n\n\n# ---------------------------------------------------------------------\n# Public API\n# ---------------------------------------------------------------------\n\ndef get_dataloader(cfg: Dict, split: str = \"train\") -\u003e DataLoader:\n    \"\"\"Return a PyTorch DataLoader for the requested split.\n\n    Parameters\n    ----------\n    cfg : dict\n        Full run configuration dictionary. Keys *dataset* and *training* are\n        inspected.\n    split : str\n        One of {\"train\", \"val\", \"test\"}.\n    \"\"\"\n\n    dataset_name = cfg.get(\"dataset\", \"dummy\")\n    batch_size = cfg[\"training\"].get(\"batch_size\", 32)\n\n    if dataset_name == \"dummy\":\n        base_ds = DummyDataset()\n        # 80/20 split for train/val\n        n_val = int(0.2 * len(base_ds))\n        n_train = len(base_ds) - n_val\n        train_ds, val_ds = random_split(base_ds, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n        chosen_ds = train_ds if split == \"train\" else val_ds\n        return DataLoader(chosen_ds, batch_size=batch_size, shuffle=(split == \"train\"))\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with real dataset loading logic.\n    # Insert your dataset-specific code for \u003cdataset_name\u003e below this line.\n    # ------------------------------------------------------------------\n    raise NotImplementedError(\n        f\"Dataset \u0027{dataset_name}\u0027 not yet implemented in preprocess.get_dataloader. \"\n        \"Add dataset-specific logic in the derive_specific step.\"\n    )", "pyproject_toml": "[project]\nname = \"ohgw_experiment_framework\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nreadme = \"README.md\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\ndependencies = [\n  \"torch\u003e=2.0\",\n  \"pyyaml\u003e=6.0\",\n  \"numpy\u003e=1.23\",\n  \"matplotlib\u003e=3.8\",\n  \"seaborn\u003e=0.13\",\n  \"tqdm\u003e=4.66\"\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "smoke_test_yaml": "# Lightweight sanity check configuration. Uses a dummy dataset \u0026 model so the\n# entire pipeline runs within seconds on CPU.\n\nexperiments:\n  - run_id: dummy_run\n    dataset: dummy               # implemented in preprocess.py\n    model: dummy                 # implemented in model.py\n    training:\n      epochs: 2\n      batch_size: 32\n      lr: 0.01\n      device: cpu                # Force CPU to avoid GPU requirement in CI\n\n# You may add global defaults here that apply to every run, e.g. a seed.\nseed: 0\n", "train_py": "import argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports\nfrom src.preprocess import get_dataloader  # pylint: disable=import-error\nfrom src.model import build_model  # pylint: disable=import-error\n\n\ndef describe_experiment(cfg: Dict) -\u003e str:\n    \"\"\"Return a human-readable, multi-line experiment description.\"\"\"\n    lines: List[str] = []\n    lines.append(\"=\" * 80)\n    lines.append(f\"RUN ID            : {cfg[\u0027run_id\u0027]}\")\n    lines.append(f\"DATASET           : {cfg[\u0027dataset\u0027]}\")\n    lines.append(f\"MODEL             : {cfg[\u0027model\u0027]}\")\n    lines.append(f\"EPOCHS            : {cfg[\u0027training\u0027][\u0027epochs\u0027]}\")\n    lines.append(f\"BATCH SIZE        : {cfg[\u0027training\u0027][\u0027batch_size\u0027]}\")\n    lines.append(f\"LEARNING RATE     : {cfg[\u0027training\u0027][\u0027lr\u0027]}\")\n    lines.append(\"=\" * 80)\n    return \"\\n\".join(lines)\n\n\ndef accuracy(pred, target):\n    return (pred.argmax(dim=1) == target).float().mean().item()\n\n\ndef train_one_epoch(model, loader, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = F.cross_entropy(logits, y)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * x.size(0)\n        running_acc += (logits.argmax(1) == y).float().sum().item()\n    n = len(loader.dataset)\n    return running_loss / n, running_acc / n\n\n\ndef evaluate(model, loader, device):\n    model.eval()\n    loss_sum, acc_sum = 0.0, 0.0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            loss = F.cross_entropy(logits, y)\n            loss_sum += loss.item() * x.size(0)\n            acc_sum += (logits.argmax(1) == y).float().sum().item()\n    n = len(loader.dataset)\n    return loss_sum / n, acc_sum / n\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run a single experimental variation\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store outputs of this run\")\n    args = parser.parse_args()\n\n    # Import here to avoid heavyweight deps for ultra-lightweight unit tests where torch is patched out.\n    import yaml  # pylint: disable=import-error\n\n    cfg: Dict = yaml.safe_load(Path(args.config).read_text())\n\n    run_id = cfg[\"run_id\"]\n\n    # ------------------------------------------------------------------\n    # I/O preparation\n    # ------------------------------------------------------------------\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n    (results_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Experiment description\n    # ------------------------------------------------------------------\n    print(describe_experiment(cfg))\n\n    # ------------------------------------------------------------------\n    # Device \u0026 seed handling\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg[\"training\"].get(\"device\", \"cuda\") == \"cuda\" else \"cpu\")\n    torch.manual_seed(cfg.get(\"seed\", 0))\n    np.random.seed(cfg.get(\"seed\", 0))\n\n    # ------------------------------------------------------------------\n    # Data pipeline (placeholders handled inside get_dataloader)\n    # ------------------------------------------------------------------\n    train_loader: DataLoader = get_dataloader(cfg, split=\"train\")\n    val_loader: DataLoader = get_dataloader(cfg, split=\"val\")\n\n    # ------------------------------------------------------------------\n    # Model, optimiser, scheduler (simple SGD, extendable via cfg)\n    # ------------------------------------------------------------------\n    model = build_model(cfg).to(device)\n    optimiser = torch.optim.SGD(model.parameters(), lr=cfg[\"training\"][\"lr\"])\n\n    best_val_acc = -1.0\n    history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    for epoch in range(1, cfg[\"training\"][\"epochs\"] + 1):\n        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimiser, device)\n        val_loss, val_acc = evaluate(model, val_loader, device)\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(tr_loss)\n        history[\"train_acc\"].append(tr_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        # Structured per-epoch log line\n        per_epoch_log = {\n            \"run_id\": run_id,\n            \"epoch\": epoch,\n            \"train_loss\": tr_loss,\n            \"train_acc\": tr_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }\n        print(json.dumps(per_epoch_log))\n\n        # Save best checkpoint\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            ckpt_path = results_dir / \"checkpoints\" / \"best.pt\"\n            model.save_checkpoint(\n                ckpt_path,\n                config={\"init_args\": cfg.get(\"model_init\", {})},\n                metadata={\"val_acc\": val_acc, \"epoch\": epoch},\n            )\n\n    # ------------------------------------------------------------------\n    # Final metrics summary\n    # ------------------------------------------------------------------\n    final_results = {\n        \"run_id\": run_id,\n        \"best_val_acc\": best_val_acc,\n        \"num_epochs\": cfg[\"training\"][\"epochs\"],\n        \"history\": history,\n    }\n\n    # Persist metrics\n    with open(results_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(final_results, fp, indent=2)\n\n    # Persist history as numpy for fast bulk access\n    np.save(results_dir / \"history.npy\", history)\n\n    # Final structured output\n    print(json.dumps(final_results))\n\n\nif __name__ == \"__main__\":\n    main()"}

# Current Experiment (To be validated)
- Experiment ID: exp-2
- Description: Objective / Hypothesis: Test zero-shot generalisation and robustness of OHGW in a different modality (language modelling) and under distribution shift.

Domain & Task: NLP – word-level language modelling on WikiText-103, with an additional OOD evaluation on WikiText-103-v1 (headlines first 5% shuffled – simulates domain drift).

Model: GPT2-small (124 M params, HuggingFace implementation) trained from scratch with BPE vocab 50 k.

Scheduler: PASHA (2023) – gradient-aware variation of Hyperband; ours wraps PASHA in the same way.

Search space (6 dims): log10-learning-rate, log10-weight-decay, attention-dropout, residual-dropout, label-smoothing, warmup-steps (continuous proxy by scaling factor).

Run variations:
• pasha-baseline – vanilla PASHA.
• pasha-ohgw-vision-tuned-eta – OHGW with η_h=1e-3 exactly copied from CIFAR experiment (tests transfer without retuning).
• pasha-ohgw-low-eta – OHGW with η_h=3e-4 (sensitivity extremum).
• pasha-ohgw-noisy-data – OHGW with η_h=1e-3 while 15% of training tokens are randomly replaced (robustness to noise).

Dataset processing: SentencePiece BPE (shared).  Sequence length 1024 tokens; dynamic batching up to 2M tokens/GPU.  Train/val/test split 238M / 8M / 8M tokens.  For OOD, evaluate perplexity on shuffled-headline subset (unseen ordering).

Training loop & budget: 50 training epochs (~250 k updates), AdamW, cosine LR.  PASHA minimum resource per config = 2 epochs, rungs ×2.  Total compute budget per replicate 16 GPU×hours (8 GPUs ×2 h).  24 replicates.

Evaluation metrics:
Primary – T@ppl=30 on validation set (wall-clock & GPU-h).
Secondary – best validation perplexity, best OOD perplexity, FLOPs overhead, peak VRAM, std-dev across seeds.
Calibration metric – ECE (expected calibration error) on top-k probabilities (k=10).

Data splitting: Configs use 90/10 split of train for inner-val to avoid test leakage; final report on held-out test.

Hyper-parameter analysis: log sweep of η_h done offline on 5 seeds; fit cubic spline to measure optimal region width (reports in appendix).

Robustness procedures:
1. Data noise: see pasha-ohgw-noisy-data.
2. Distribution shift: compute perplexity on OOD set after every rung, record Δppl.
3. Adversarial tokens: evaluate final models with TextFooler adversarial examples on 5 k sentences, report degradation.

Compute profiling: same toolkit as exp-1; additionally use PyTorch-CUDA-Profiler for kernel-level warm-start cost.

Example trial wrapper:
```python
loss = lm_model(input_ids, labels=input_ids).loss
hg = torch.autograd.grad(loss, cfg.continuous())
with torch.no_grad():
    for p,g in zip(cfg.continuous(), hg):
        p -= eta_h * g    # no extra unroll
pasha_scheduler.launch(cfg)
```

Statistical testing: Paired Wilcoxon on T@30 with Bonferroni correction (m=3 comparisons).  Significance if p<0.016.

Success criteria: vision-tuned η_h version achieves ≥15% speed-up vs. baseline on T@30 with ≤0.5 ppl regression on final test; effect still ≥10% under noisy data.  Extra compute overhead ≤5% of total FLOPs.
- Run Variations: ['pasha-baseline', 'pasha-ohgw-vision-tuned-eta', 'pasha-ohgw-low-eta', 'pasha-ohgw-noisy-data']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"After all individual runs are finished, this script consolidates their\nnumeric results and generates comparison figures (.pdf).\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom src.model import BaseModel  # pylint: disable=import-error\n\n\nFIGURE_STYLE = {\n    \"figure.dpi\": 300,\n    \"font.size\": 12,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n}\n\n\nfor k, v in FIGURE_STYLE.items():\n    plt.rcParams[k] = v  # type: ignore[arg-type]\n\ndef load_results(run_dir: Path) -\u003e Dict:\n    with open(run_dir / \"results.json\", \"r\", encoding=\"utf-8\") as fp:\n        return json.load(fp)\n\n\ndef make_accuracy_bar(results: List[Dict], save_dir: Path):\n    run_ids = [r[\"run_id\"] for r in results]\n    accs = [r[\"best_val_acc\"] for r in results]\n\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=run_ids, y=accs, palette=\"viridis\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n\n    # Annotate bars\n    for i, a in enumerate(accs):\n        plt.text(i, a + 0.001, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n\n    plt.ylim(0, max(accs) * 1.1)\n    plt.tight_layout()\n\n    fname = save_dir / \"accuracy.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return str(fname.name)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate \u0026 compare experimental variations\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory produced by main.py\")\n    args = parser.parse_args()\n\n    results_root = Path(args.results_dir)\n    evaluation_dir = results_root / \"evaluation\"\n    evaluation_dir.mkdir(parents=True, exist_ok=True)\n\n    # Gather all result files (exclude evaluation dir itself)\n    run_dirs = [p for p in results_root.iterdir() if p.is_dir() and p.name != \"evaluation\"]\n    run_results: List[Dict] = []\n    for d in run_dirs:\n        try:\n            run_results.append(load_results(d))\n        except FileNotFoundError:\n            print(f\"[WARN] results.json not found in {d}. Skipping.\")\n\n    if not run_results:\n        print(\"No run results found; evaluation aborted.\")\n        return\n\n    # Generate figures -------------------------------------------------\n    figures: List[str] = []\n    figures.append(make_accuracy_bar(run_results, evaluation_dir))\n\n    # Structured comparison output ------------------------------------\n    summary = {\n        \"num_runs\": len(run_results),\n        \"runs\": [{\"run_id\": r[\"run_id\"], \"best_val_acc\": r[\"best_val_acc\"]} for r in run_results],\n        \"figures\": figures,\n    }\n\n    with open(evaluation_dir / \"comparison.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(summary, fp, indent=2)\n\n    # Print JSON to stdout so that calling process can capture aggregated info\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# Full experimental set-up for *exp-2* (WikiText-103 language modelling) \u2013 all\n# four run variations are defined below.  Only parameters that differ across\n# runs are overridden locally; everything else inherits the global defaults\n# declared at the bottom.\n\nexperiments:\n  # ------------------------------------------------------------------\n  - run_id: pasha-baseline\n    dataset: wikitext103\n    model: gpt2-small\n    scheduler: pasha\n    training:\n      epochs: 50                # \u2248250k updates at bs=2M tokens/GPU\n      batch_size: 4             # tiny batches \u2013 actual PASHA launcher will do gradient-accum\n      lr: 5e-4                  # will be overwritten by HPO during PASHA search\n      device: cuda\n    dataset_params:\n      seq_length: 1024\n      max_tokens: 2000000       # 2M tokens for sanity \u2013 full corpus used in production\n\n  # ------------------------------------------------------------------\n  - run_id: pasha-ohgw-vision-tuned-eta\n    dataset: wikitext103\n    model: gpt2-small\n    scheduler: pasha\n    ohgw_eta: 1e-3              # transferred from CIFAR experiment\n    training:\n      epochs: 50\n      batch_size: 4\n      lr: 5e-4\n      device: cuda\n    dataset_params:\n      seq_length: 1024\n      max_tokens: 2000000\n\n  # ------------------------------------------------------------------\n  - run_id: pasha-ohgw-low-eta\n    dataset: wikitext103\n    model: gpt2-small\n    scheduler: pasha\n    ohgw_eta: 3e-4\n    training:\n      epochs: 50\n      batch_size: 4\n      lr: 5e-4\n      device: cuda\n    dataset_params:\n      seq_length: 1024\n      max_tokens: 2000000\n\n  # ------------------------------------------------------------------\n  - run_id: pasha-ohgw-noisy-data\n    dataset: wikitext103\n    model: gpt2-small\n    scheduler: pasha\n    data_noise: 0.15            # 15 % token replacement noise on *train* split\n    ohgw_eta: 1e-3\n    training:\n      epochs: 50\n      batch_size: 4\n      lr: 5e-4\n      device: cuda\n    dataset_params:\n      seq_length: 1024\n      max_tokens: 2000000\n\n# --------------------------------------------------------------------\n# Global defaults shared by every run unless explicitly overridden above\n# --------------------------------------------------------------------\nseed: 0           # ensures determinism for dataset splits / weight init etc.\n\n# The following keys are *place-holders* for the PASHA hyper-parameter search.\n# They are read by the external scheduler harness, not by the local scripts.\nsearch_space:\n  log10_learning_rate: [-6, -3]\n  log10_weight_decay:  [-6, -2]\n  attention_dropout:    [0.0, 0.3]\n  residual_dropout:     [0.0, 0.3]\n  label_smoothing:      [0.0, 0.2]\n  warmup_steps_factor:  [0.01, 0.2]\n\npasha:\n  min_resource_epochs: 2\n  reduction_factor:    2\n\n# END OF CONFIG\n", "main_py": "\"\"\"Main orchestrator: reads config YAML, sequentially launches train.py for each\nvariation, then triggers evaluate.py. All stdout/stderr from subprocesses are\ntee-redirected into log files *and* the main process\u0027 stdout/stderr.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\nimport yaml  # type: ignore\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef tee_stream(stream, log_file_path: Path):\n    \"\"\"Forward bytes from a stream to both sys.stdout/sys.stderr and a log file.\"\"\"\n\n    log_file = open(log_file_path, \"wb\")\n\n    while True:\n        chunk = stream.readline()\n        if not chunk:\n            break\n        log_file.write(chunk)\n        log_file.flush()\n        # Decode for console display (fallback to utf-8 ignore)\n        try:\n            sys.stdout.buffer.write(chunk)  # pylint: disable=no-member\n            sys.stdout.flush()\n        except Exception:  # pylint: disable=broad-except\n            # If writing raw bytes fails (e.g. non-UTF-8), try decode-ignore\n            sys.stdout.write(chunk.decode(errors=\"ignore\"))\n            sys.stdout.flush()\n\n    log_file.close()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Launch a subprocess and tee its stdout/stderr to log files.\"\"\"\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)  # noqa: S603, S607\n    # Stream stdout\n    tee_stream(proc.stdout, stdout_path)  # type: ignore[arg-type]\n    tee_stream(proc.stderr, stderr_path)  # type: ignore[arg-type]\n    proc.wait()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess {\u0027 \u0027.join(cmd)} failed with code {proc.returncode}\")\n\n\ndef prepare_run_cfg(base_cfg: Dict, run: Dict, run_dir: Path) -\u003e Path:\n    \"\"\"Merge base \u0026 run-specific config, dump to YAML file inside run_dir.\"\"\"\n    merged = {**base_cfg, **run}\n    cfg_path = run_dir / \"config.yaml\"\n    with open(cfg_path, \"w\", encoding=\"utf-8\") as fp:\n        yaml.safe_dump(merged, fp)\n    return cfg_path\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OHGW experiment launcher (foundation)\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store all outputs\")\n    args = parser.parse_args()\n\n    root_results = Path(args.results_dir).expanduser().resolve()\n    root_results.mkdir(parents=True, exist_ok=True)\n\n    cfg_file = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_file, \"r\", encoding=\"utf-8\") as fp:\n        full_cfg: Dict = yaml.safe_load(fp)\n\n    global_cfg = {k: v for k, v in full_cfg.items() if k != \"experiments\"}\n    experiments: List[Dict] = full_cfg.get(\"experiments\", [])\n\n    if not experiments:\n        print(\"No experiments defined in the configuration file.\")\n        sys.exit(1)\n\n    # ------------------------------------------------------------------\n    # Launch each experiment sequentially\n    # ------------------------------------------------------------------\n    for run in experiments:\n        run_id = run[\"run_id\"]\n        run_dir = root_results / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        cfg_path = prepare_run_cfg(global_cfg, run, run_dir)\n\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(cfg_path), \"--results-dir\", str(run_dir)]\n        print(f\"\\n[MAIN] Launching run \u0027{run_id}\u0027 ...\")\n        run_subprocess(cmd, stdout_path, stderr_path)\n        print(f\"[MAIN] Run \u0027{run_id}\u0027 finished successfully.\")\n\n    # ------------------------------------------------------------------\n    # After all runs complete, launch evaluation\n    # ------------------------------------------------------------------\n    evaluation_dir = root_results / \"evaluation\"\n    evaluation_dir.mkdir(exist_ok=True)\n\n    stdout_path = evaluation_dir / \"stdout.log\"\n    stderr_path = evaluation_dir / \"stderr.log\"\n\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(root_results)]\n    print(\"\\n[MAIN] Launching evaluation ...\")\n    run_subprocess(eval_cmd, stdout_path, stderr_path)\n    print(\"[MAIN] Evaluation finished successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"Model architectures and registry for the OHGW experiments.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM  # type: ignore\n\n__all__ = [\"BaseModel\", \"DummyClassifier\", \"GPT2LastTokenClassifier\", \"build_model\"]\n\n################################################################################\n# Generic base model \u2013 adds (de)serialization helpers\n################################################################################\n\nclass BaseModel(nn.Module):\n    \"\"\"Common mix-in that provides `save_checkpoint` / `load_checkpoint`.\"\"\"\n\n    def save_checkpoint(self, path, config: Dict[str, Any] | None = None, metadata: Dict[str, Any] | None = None):\n        ckpt = {\n            \"state_dict\": self.state_dict(),\n            \"config\": config or {},\n            \"metadata\": metadata or {},\n            \"class_name\": self.__class__.__name__,\n        }\n        torch.save(ckpt, path)\n\n    # ------------------------------------------------------------------\n    @classmethod\n    def load_checkpoint(cls, path, device: str | torch.device = \"cpu\") -\u003e Tuple[\"BaseModel\", Dict]:\n        ckpt = torch.load(path, map_location=device)\n        class_name = ckpt.get(\"class_name\", cls.__name__)\n        if class_name != cls.__name__:\n            raise ValueError(\n                f\"Checkpoint class mismatch: stored as \u0027{class_name}\u0027, requested \u0027{cls.__name__}\u0027.\"\n            )\n        init_args = ckpt.get(\"config\", {}).get(\"init_args\", {})\n        model = cls(**init_args)  # type: ignore[arg-type]\n        model.load_state_dict(ckpt[\"state_dict\"])\n        return model, ckpt.get(\"metadata\", {})\n\n################################################################################\n# 1) Dummy linear classifier (used in smoke tests)\n################################################################################\n\nclass DummyClassifier(BaseModel):\n    \"\"\"A single fully-connected layer \u2013 suffices for CI pipeline integrity test.\"\"\"\n\n    def __init__(self, input_dim: int = 10, num_classes: int = 2):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, num_classes)\n\n    # ------------------------------------------------------------------\n    def forward(self, x):  # type: ignore[override]\n        return self.fc(x)\n\n################################################################################\n# 2) GPT-2 (small) next-token classifier wrapper\n################################################################################\n\nclass GPT2LastTokenClassifier(BaseModel):\n    \"\"\"Wrap a *transformers* GPT-2 model to return the logits of **the last** token.\n\n    This reduces the language-modeling objective to the standard classification\n    interface expected by the shared training loop: ``logits = model(x)`` where\n    ``x`` has shape ``(batch, seq_len)`` and the returned tensor has shape\n    ``(batch, vocab_size)`` so that ``F.cross_entropy`` can be applied with a\n    single scalar label per sample.\n    \"\"\"\n\n    def __init__(self, pretrained_name: str = \"gpt2\", **kwargs):  # noqa: D401\n        super().__init__()\n        self.lm = AutoModelForCausalLM.from_pretrained(pretrained_name, **kwargs)\n        # Ensure pad token exists so that *cross_entropy* ignores it if used\n        if self.lm.config.pad_token_id is None:\n            self.lm.config.pad_token_id = self.lm.config.eos_token_id\n\n    # ------------------------------------------------------------------\n    def forward(self, input_ids):  # type: ignore[override]\n        \"\"\"Return the logits of the last position.\"\"\"\n        outputs = self.lm(input_ids=input_ids)\n        logits = outputs.logits  # (batch, seq_len, vocab)\n        return logits[:, -1, :]\n\n################################################################################\n# Model factory\n################################################################################\n\ndef build_model(cfg: Dict):  # noqa: D401 \u2013 simple factory helper\n    \"\"\"Instantiate the requested model as specified in ``cfg[\u0027model\u0027]``.\"\"\"\n\n    name = cfg.get(\"model\", \"dummy\").lower()\n    init_args = cfg.get(\"model_init\", {})\n\n    if name == \"dummy\":\n        return DummyClassifier(**init_args)\n\n    if name in {\"gpt2-small\", \"gpt2\", \"gpt2_small\"}:\n        # Alias mapping for convenience\n        init_args.setdefault(\"pretrained_name\", \"gpt2\")\n        return GPT2LastTokenClassifier(**init_args)\n\n    raise ValueError(f\"Unknown model type: {name}\")", "preprocess_py": "\"\"\"Pre-processing utilities for all datasets used in the OHGW experiments.\n\nThe helper `get_dataloader` is the single public entry-point.  It hides all\ncorpus-specific logic behind a simple `(cfg, split) -\u003e DataLoader` interface so\nthat the training script can stay agnostic to the underlying modality.\n\nImplemented datasets\n--------------------\n* ``dummy``            \u2013 Small synthetic classification toy task for smoke tests\n* ``wikitext103``      \u2013 Word-level language-modeling corpus (HuggingFace)\n\nThe WikiText loader returns pairs ``(input_ids, next_token)`` where\n``input_ids`` is a tensor of shape ``(seq_len,)`` and ``next_token`` is the\ninteger id of the token that immediately follows the sequence.  This converts\nthe language-modeling objective into a standard single-step classification task\nthat is compatible with the generic train loop implemented in *src/train.py*.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom transformers import AutoTokenizer  # type: ignore\nfrom datasets import load_dataset  # type: ignore\n\n__all__ = [\"get_dataloader\"]\n\n################################################################################\n# Generic helpers\n################################################################################\n\nclass DummyDataset(Dataset):\n    \"\"\"10-dimensional random classification task used for CI smoke tests.\"\"\"\n\n    def __init__(self, n_samples: int = 256, input_dim: int = 10, num_classes: int = 2):\n        super().__init__()\n        self.x = torch.randn(n_samples, input_dim)\n        self.y = torch.randint(0, num_classes, (n_samples,))\n\n    # ---------------------------------------------------------------------\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n    def __len__(self):\n        return len(self.x)\n\n################################################################################\n# WikiText-103\n################################################################################\n\nclass _WikiNextTokenDataset(Dataset):\n    \"\"\"Return (seq_len tokens -\u003e next_token) pairs for next-token prediction.\n\n    The corpus is loaded via \ud83e\udd17 *datasets*.  We concatenate *all* text samples in\n    the split, tokenize once and then slice out ``seq_len+1`` contiguous token\n    windows with stride = ``seq_len`` (non-overlapping) which is the standard\n    efficiency trick to avoid re-tokenisation in language modelling.\n    \"\"\"\n\n    def __init__(\n        self,\n        split: str,\n        seq_len: int,\n        tokenizer_name: str = \"gpt2\",\n        max_tokens: int | None = None,\n        noise_prob: float = 0.0,\n    ) -\u003e None:\n        super().__init__()\n        assert 0.0 \u003c= noise_prob \u003c 1.0, \"noise_prob must be in [0,1)\"\n        self.seq_len = seq_len\n        self.noise_prob = noise_prob\n\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        tokenizer.add_special_tokens({\"pad_token\": \"\u003c|pad|\u003e\"})  # GPT2 has no pad\n        self.vocab_size = len(tokenizer)\n        self._pad_id = tokenizer.pad_token_id\n\n        # Load raw text (streaming keeps memory reasonable)\n        raw_ds = load_dataset(\"yehzw/wikitext-103\", split=split, streaming=True)\n        texts: List[str] = []\n        token_budget = 0\n        for item in raw_ds:\n            texts.append(item[\"text\"])\n            if max_tokens is not None and token_budget \u003e= max_tokens:\n                break\n        # Concatenate and tokenise\n        enc = tokenizer(\"\\n\".join(texts))\n        ids: Sequence[int] = enc[\"input_ids\"]\n        if max_tokens is not None:\n            ids = ids[: max_tokens]\n        self.tokens = torch.tensor(ids, dtype=torch.long)\n\n        # Derive number of samples\n        self.num_samples = (len(self.tokens) - 1) // self.seq_len\n\n    # ------------------------------------------------------------------\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        start = idx * self.seq_len\n        end = start + self.seq_len + 1  # +1 because we also need the target\n        window = self.tokens[start:end]\n        if len(window) \u003c self.seq_len + 1:  # last chunk \u2013 drop (simplest)\n            window = torch.cat(\n                [window, torch.full((self.seq_len + 1 - len(window),), self._pad_id)]\n            )\n        input_ids = window[:-1].clone()\n        target = window[-1].clone()\n\n        # --------------------------------------------------------------\n        # Optional token-level noise injection (used in robustness study)\n        # --------------------------------------------------------------\n        if self.noise_prob \u003e 0 and self.training:  # type: ignore[attr-defined]\n            mask = torch.rand_like(input_ids.float()) \u003c self.noise_prob\n            random_tokens = torch.randint(0, self.vocab_size, size=mask.sum().tolist(), dtype=torch.long)\n            input_ids[mask] = random_tokens\n\n        return input_ids, target\n\n################################################################################\n# Public factory\n################################################################################\n\ndef get_dataloader(cfg: Dict, split: str = \"train\") -\u003e DataLoader:\n    \"\"\"Factory to obtain a ready-to-use *torch.utils.data.DataLoader*.\n\n    Parameters\n    ----------\n    cfg   : Full run YAML config (dict-like)\n    split : One of {\"train\", \"val\", \"test\"}\n    \"\"\"\n\n    dataset_name = cfg.get(\"dataset\", \"dummy\").lower()\n    batch_size = cfg[\"training\"].get(\"batch_size\", 32)\n    # Generic seed for reproducibility of random_split etc.\n    seed = cfg.get(\"seed\", 0)\n\n    # ------------------------------------------------------------------\n    # 1) Dummy synthetic dataset (for CI / smoke tests)\n    # ------------------------------------------------------------------\n    if dataset_name == \"dummy\":\n        base_ds = DummyDataset()\n        num_val = int(0.2 * len(base_ds))\n        num_train = len(base_ds) - num_val\n        train_ds, val_ds = random_split(\n            base_ds, [num_train, num_val], generator=torch.Generator().manual_seed(seed)\n        )\n        selected_ds = {\n            \"train\": train_ds,\n            \"val\": val_ds,\n            \"test\": val_ds,  # no separate test split for dummy\n        }[split]\n        return DataLoader(selected_ds, batch_size=batch_size, shuffle=(split == \"train\"))\n\n    # ------------------------------------------------------------------\n    # 2) WikiText-103 for language-modeling experiments\n    # ------------------------------------------------------------------\n    if dataset_name in {\"wikitext103\", \"wikitext-103\", \"wikitext_103\"}:\n        seq_len = cfg.get(\"dataset_params\", {}).get(\"seq_length\", 1024)\n        max_tokens = cfg.get(\"dataset_params\", {}).get(\"max_tokens\")\n        noise_prob = cfg.get(\"data_noise\", 0.0) if split == \"train\" else 0.0\n\n        ds_split_map = {\"train\": \"train\", \"val\": \"validation\", \"test\": \"test\"}\n        ds = _WikiNextTokenDataset(\n            split=ds_split_map[split],\n            seq_len=seq_len,\n            tokenizer_name=\"gpt2\",\n            max_tokens=max_tokens,\n            noise_prob=noise_prob,\n        )\n        # Attach flag so dataset can check if it is in training mode for noise\n        ds.training = split == \"train\"  # type: ignore[attr-defined]\n\n        return DataLoader(\n            ds,\n            batch_size=batch_size,\n            shuffle=(split == \"train\"),\n            drop_last=True,\n        )\n\n    # ------------------------------------------------------------------\n    # Unknown dataset fallback\n    # ------------------------------------------------------------------\n    raise ValueError(f\"Unsupported dataset: {dataset_name}\")", "pyproject_toml": "[project]\nname = \"ohgw_experiment_framework\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nreadme = \"README.md\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\ndependencies = [\n  \"torch\u003e=2.0\",\n  \"pyyaml\u003e=6.0\",\n  \"numpy\u003e=1.23\",\n  \"matplotlib\u003e=3.8\",\n  \"seaborn\u003e=0.13\",\n  \"tqdm\u003e=4.66\",\n  \"transformers\u003e=4.33\",\n  \"datasets\u003e=2.14\",\n  \"sentencepiece\u003e=0.1.99\"\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "smoke_test_yaml": "# Smoke-test configuration \u2013 runs every *variation* on a tiny synthetic dataset\n# and a trivial model to ensure that the full plumbing works end-to-end inside\n# seconds even on the slowest CI machine.  These runs are **not** meant to be\n# scientifically meaningful.\n\nexperiments:\n  - run_id: pasha-baseline-smoke\n    dataset: dummy\n    model: dummy\n    training:\n      epochs: 1\n      batch_size: 32\n      lr: 0.01\n      device: cpu\n\n  - run_id: pasha-ohgw-vision-tuned-eta-smoke\n    dataset: dummy\n    model: dummy\n    ohgw_eta: 1e-3\n    training:\n      epochs: 1\n      batch_size: 32\n      lr: 0.01\n      device: cpu\n\n  - run_id: pasha-ohgw-low-eta-smoke\n    dataset: dummy\n    model: dummy\n    ohgw_eta: 3e-4\n    training:\n      epochs: 1\n      batch_size: 32\n      lr: 0.01\n      device: cpu\n\n  - run_id: pasha-ohgw-noisy-data-smoke\n    dataset: dummy\n    model: dummy\n    data_noise: 0.15\n    ohgw_eta: 1e-3\n    training:\n      epochs: 1\n      batch_size: 32\n      lr: 0.01\n      device: cpu\n\n# Global defaults (reproducibility)\nseed: 0\n", "train_py": "import argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports\nfrom src.preprocess import get_dataloader  # pylint: disable=import-error\nfrom src.model import build_model  # pylint: disable=import-error\n\n\ndef describe_experiment(cfg: Dict) -\u003e str:\n    \"\"\"Return a human-readable, multi-line experiment description.\"\"\"\n    lines: List[str] = []\n    lines.append(\"=\" * 80)\n    lines.append(f\"RUN ID            : {cfg[\u0027run_id\u0027]}\")\n    lines.append(f\"DATASET           : {cfg[\u0027dataset\u0027]}\")\n    lines.append(f\"MODEL             : {cfg[\u0027model\u0027]}\")\n    lines.append(f\"EPOCHS            : {cfg[\u0027training\u0027][\u0027epochs\u0027]}\")\n    lines.append(f\"BATCH SIZE        : {cfg[\u0027training\u0027][\u0027batch_size\u0027]}\")\n    lines.append(f\"LEARNING RATE     : {cfg[\u0027training\u0027][\u0027lr\u0027]}\")\n    lines.append(\"=\" * 80)\n    return \"\\n\".join(lines)\n\n\ndef accuracy(pred, target):\n    return (pred.argmax(dim=1) == target).float().mean().item()\n\n\ndef train_one_epoch(model, loader, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = F.cross_entropy(logits, y)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * x.size(0)\n        running_acc += (logits.argmax(1) == y).float().sum().item()\n    n = len(loader.dataset)\n    return running_loss / n, running_acc / n\n\n\ndef evaluate(model, loader, device):\n    model.eval()\n    loss_sum, acc_sum = 0.0, 0.0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            loss = F.cross_entropy(logits, y)\n            loss_sum += loss.item() * x.size(0)\n            acc_sum += (logits.argmax(1) == y).float().sum().item()\n    n = len(loader.dataset)\n    return loss_sum / n, acc_sum / n\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run a single experimental variation\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store outputs of this run\")\n    args = parser.parse_args()\n\n    # Import here to avoid heavyweight deps for ultra-lightweight unit tests where torch is patched out.\n    import yaml  # pylint: disable=import-error\n\n    cfg: Dict = yaml.safe_load(Path(args.config).read_text())\n\n    run_id = cfg[\"run_id\"]\n\n    # ------------------------------------------------------------------\n    # I/O preparation\n    # ------------------------------------------------------------------\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n    (results_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Experiment description\n    # ------------------------------------------------------------------\n    print(describe_experiment(cfg))\n\n    # ------------------------------------------------------------------\n    # Device \u0026 seed handling\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg[\"training\"].get(\"device\", \"cuda\") == \"cuda\" else \"cpu\")\n    torch.manual_seed(cfg.get(\"seed\", 0))\n    np.random.seed(cfg.get(\"seed\", 0))\n\n    # ------------------------------------------------------------------\n    # Data pipeline (placeholders handled inside get_dataloader)\n    # ------------------------------------------------------------------\n    train_loader: DataLoader = get_dataloader(cfg, split=\"train\")\n    val_loader: DataLoader = get_dataloader(cfg, split=\"val\")\n\n    # ------------------------------------------------------------------\n    # Model, optimiser, scheduler (simple SGD, extendable via cfg)\n    # ------------------------------------------------------------------\n    model = build_model(cfg).to(device)\n    optimiser = torch.optim.SGD(model.parameters(), lr=cfg[\"training\"][\"lr\"])\n\n    best_val_acc = -1.0\n    history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    for epoch in range(1, cfg[\"training\"][\"epochs\"] + 1):\n        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimiser, device)\n        val_loss, val_acc = evaluate(model, val_loader, device)\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(tr_loss)\n        history[\"train_acc\"].append(tr_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        # Structured per-epoch log line\n        per_epoch_log = {\n            \"run_id\": run_id,\n            \"epoch\": epoch,\n            \"train_loss\": tr_loss,\n            \"train_acc\": tr_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }\n        print(json.dumps(per_epoch_log))\n\n        # Save best checkpoint\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            ckpt_path = results_dir / \"checkpoints\" / \"best.pt\"\n            model.save_checkpoint(\n                ckpt_path,\n                config={\"init_args\": cfg.get(\"model_init\", {})},\n                metadata={\"val_acc\": val_acc, \"epoch\": epoch},\n            )\n\n    # ------------------------------------------------------------------\n    # Final metrics summary\n    # ------------------------------------------------------------------\n    final_results = {\n        \"run_id\": run_id,\n        \"best_val_acc\": best_val_acc,\n        \"num_epochs\": cfg[\"training\"][\"epochs\"],\n        \"history\": history,\n    }\n\n    # Persist metrics\n    with open(results_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(final_results, fp, indent=2)\n\n    # Persist history as numpy for fast bulk access\n    np.save(results_dir / \"history.npy\", history)\n\n    # Final structured output\n    print(json.dumps(final_results))\n\n\nif __name__ == \"__main__\":\n    main()"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "1. OHGW functionality and PASHA scheduler mentioned in the experiment description are **not implemented anywhere in the code base**.  The fields `scheduler` and `ohgw_eta` in the YAML files are ignored, therefore none of the four run variations is actually executed as intended.\n2. `_WikiNextTokenDataset.__getitem__` uses `torch.randint(..., size=mask.sum().tolist())`, but `size` must be a **tuple of ints**; passing a plain int will raise `TypeError` as soon as `data_noise > 0` (run `pasha-ohgw-noisy-data`).\n3. Variable `token_budget` is never incremented, so the `max_tokens` parameter has no effect.\n4. The monkey-patched `ds.training` flag is brittle and not guaranteed to exist; relying on it can break in future PyTorch versions.\n5. Leftover comment markers such as “*place-holders* for the PASHA hyper-parameter search” are still present, signalling incomplete integration.\n\nBecause of these issues the code is **not immediately executable** for the declared full experiment and does not meet the core validation criteria."
}
