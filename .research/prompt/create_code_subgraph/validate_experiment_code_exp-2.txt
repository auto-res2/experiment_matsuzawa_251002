
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results of all experiment variations, computes comparison\nstatistics and generates publication-quality figures in .pdf format.\nFigures are stored in \u003cresults_dir\u003e/figures/ and also listed in the\nstdout JSON summary for easy discovery.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_KWARGS = dict(bbox_inches=\"tight\")\n\n\ndef load_all_results(results_dir: Path) -\u003e List[Dict]:\n    results: List[Dict] = []\n    for res_file in results_dir.rglob(\"results.json\"):\n        with open(res_file) as f:\n            results.append(json.load(f))\n    if not results:\n        raise RuntimeError(f\"No results.json files found under {results_dir}\")\n    return results\n\n\ndef figure_training_loss(all_results: List[Dict], save_dir: Path):\n    plt.figure(figsize=(6, 4))\n    for res in all_results:\n        plt.plot(res[\"train_loss_history\"], label=res[\"run_id\"])\n        # Annotate final value\n        plt.annotate(f\"{res[\u0027train_loss_history\u0027][-1]:.3f}\",\n                     xy=(len(res[\u0027train_loss_history\u0027]) - 1, res[\u0027train_loss_history\u0027][-1]),\n                     textcoords=\"offset points\", xytext=(0, 5))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Training Loss\")\n    plt.title(\"Training Loss Curves\")\n    plt.legend()\n    fname = save_dir / \"training_loss.pdf\"\n    plt.savefig(fname, **FIG_KWARGS)\n    plt.close()\n    return str(fname.name)\n\n\ndef figure_accuracy(all_results: List[Dict], save_dir: Path):\n    plt.figure(figsize=(6, 4))\n    for res in all_results:\n        if not res[\"val_accuracy_history\"]:\n            continue\n        plt.plot(res[\"val_accuracy_history\"], label=res[\"run_id\"])\n        plt.annotate(f\"{res[\u0027val_accuracy_history\u0027][-1]:.3f}\",\n                     xy=(len(res[\u0027val_accuracy_history\u0027]) - 1, res[\u0027val_accuracy_history\u0027][-1]),\n                     textcoords=\"offset points\", xytext=(0, 5))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy Curves\")\n    plt.legend()\n    fname = save_dir / \"accuracy.pdf\"\n    plt.savefig(fname, **FIG_KWARGS)\n    plt.close()\n    return str(fname.name)\n\n\ndef bar_best_accuracy(all_results: List[Dict], save_dir: Path):\n    accs = {r[\"run_id\"]: r[\"best_val_accuracy\"] for r in all_results if r[\"best_val_accuracy\"] is not None}\n    if not accs:\n        return None\n    plt.figure(figsize=(6, 4))\n    names = list(accs.keys())\n    vals = [accs[n] for n in names]\n    sns.barplot(x=names, y=vals)\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.title(\"Comparison of Best Accuracies\")\n    fname = save_dir / \"best_val_accuracy.pdf\"\n    plt.savefig(fname, **FIG_KWARGS)\n    plt.close()\n    return str(fname.name)\n\n\ndef evaluate(results_dir: Path):\n    print(\"\\n===== Aggregating experiment results =====\\n\")\n    all_results = load_all_results(results_dir)\n\n    figures_dir = results_dir / \"figures\"\n    figures_dir.mkdir(exist_ok=True, parents=True)\n\n    figure_files: List[str] = []\n    figure_files.append(figure_training_loss(all_results, figures_dir))\n    fig_acc = figure_accuracy(all_results, figures_dir)\n    if fig_acc:\n        figure_files.append(fig_acc)\n    fig_bar = bar_best_accuracy(all_results, figures_dir)\n    if fig_bar:\n        figure_files.append(fig_bar)\n\n    # ------------------------------------------------------------------\n    # Consolidated comparison table for stdout -------------------------\n    # ------------------------------------------------------------------\n    df_rows = []\n    for r in all_results:\n        df_rows.append({\n            \"run_id\": r[\"run_id\"],\n            \"best_val_accuracy\": r[\"best_val_accuracy\"],\n            \"training_time_sec\": r[\"training_time_sec\"],\n            \"time_to_target_sec\": r[\"time_to_target_sec\"],\n        })\n    df = pd.DataFrame(df_rows)\n\n    comparison = df.to_dict(orient=\"records\")\n    summary = {\n        \"description\": \"Comparison of OHGW experiment variations\",\n        \"num_runs\": len(all_results),\n        \"figures\": figure_files,\n        \"table\": comparison,\n    }\n    print(json.dumps(summary, indent=2))\n\n\n# ---------------------------------------------------------------------------\n# Entry-point\n# ---------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    import argparse\n\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    args = p.parse_args()\n\n    evaluate(Path(args.results_dir).resolve())\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# ONLY placeholders are provided here. Real experiments will be injected\n# in the subsequent derivation step. The structure is fixed so that\n# main.py can already validate the schema.\n\nexperiments:\n  - run_id: EXPERIMENT_VARIATION_1\n    dataset_name: DATASET_PLACEHOLDER          # PLACEHOLDER: e.g. cifar10\n    model_name: MODEL_PLACEHOLDER              # PLACEHOLDER: e.g. resnet20\n    batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    epochs: SPECIFIC_CONFIG_PLACEHOLDER\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n    target_metric: SPECIFIC_CONFIG_PLACEHOLDER\n    eta_h: SPECIFIC_CONFIG_PLACEHOLDER\n    hyperparams:\n      log_lr: SPECIFIC_CONFIG_PLACEHOLDER\n      log_wd: SPECIFIC_CONFIG_PLACEHOLDER\n      momentum: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # Additional run variations are appended here following the exact same schema\n\n# NOTE: Do not modify the top-level keys; they are parsed programmatically\n# by src.main.run_all_experiments().\n", "main_py": "\"\"\"src/main.py\nCentral experiment orchestrator. Reads the YAML configuration file that\nlists *all* run variations, schedules them on the available GPUs (one GPU\nper subprocess), manages logging tee-ing, and finally invokes evaluate.py\nonce all runs have finished.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport queue\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\nimport torch\n\nROOT_DIR = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT_DIR / \"src\"\n\n\n# ---------------------------------------------------------------------------\n# Helper: live tee of subprocess output to both file and main stdout ------\n# ---------------------------------------------------------------------------\n\ndef _reader_thread(pipe, tee_file):\n    with pipe:\n        for line in iter(pipe.readline, b\"\"):\n            decoded = line.decode()\n            tee_file.write(decoded)\n            tee_file.flush()\n            sys.stdout.write(decoded)\n            sys.stdout.flush()\n\n\ndef launch_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path) -\u003e subprocess.Popen:\n    stdout_f = open(stdout_path, \"w\")\n    stderr_f = open(stderr_path, \"w\")\n\n    # Merge stderr into its own pipe for tee-ing\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n    threading.Thread(target=_reader_thread, args=(proc.stdout, stdout_f), daemon=True).start()\n    threading.Thread(target=_reader_thread, args=(proc.stderr, stderr_f), daemon=True).start()\n    return proc\n\n\n# ---------------------------------------------------------------------------\n# Scheduler that ensures \u2264 num_gpus concurrent processes ---------------\n# ---------------------------------------------------------------------------\n\ndef run_all_experiments(config_path: Path, results_dir: Path):\n    with open(config_path, \"r\") as fh:\n        cfg_root = yaml.safe_load(fh)\n\n    experiments: List[Dict] = cfg_root[\"experiments\"]\n    if not experiments:\n        raise ValueError(\"No experiments found in config file\")\n\n    n_available_gpus = torch.cuda.device_count()\n    if n_available_gpus == 0:\n        print(\"WARNING: No GPUs detected \u2013 running on CPU.\")\n        n_available_gpus = 1  # schedule serially on CPU\n\n    print(f\"Detected {n_available_gpus} GPUs \u2192 launching up to {n_available_gpus} concurrent runs.\")\n\n    # Queue of pending experiments -------------------------------------\n    exp_queue = queue.Queue()\n    for exp in experiments:\n        exp_queue.put(exp)\n\n    active: Dict[int, subprocess.Popen] = {}\n    gpu_ids = list(range(n_available_gpus))\n\n    while not exp_queue.empty() or active:\n        # Launch new processes if GPU free\n        while gpu_ids and not exp_queue.empty():\n            gpu_id = gpu_ids.pop(0)\n            exp_cfg = exp_queue.get()\n            run_id = exp_cfg[\"run_id\"]\n\n            run_dir = results_dir / run_id\n            run_dir.mkdir(parents=True, exist_ok=True)\n\n            # Write run-specific YAML config so train.py can read it.\n            run_cfg_path = run_dir / \"run_config.yaml\"\n            with open(run_cfg_path, \"w\") as fh:\n                yaml.safe_dump(exp_cfg, fh)\n\n            stdout_path = run_dir / \"stdout.log\"\n            stderr_path = run_dir / \"stderr.log\"\n\n            cmd = [\n                sys.executable,\n                \"-m\",\n                \"src.train\",\n                \"--run-config\",\n                str(run_cfg_path),\n                \"--results-dir\",\n                str(results_dir),\n            ]\n\n            env = os.environ.copy()\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n\n            print(f\"[MAIN] Launching run_id={run_id} on GPU {gpu_id} \u2026\")\n            proc = launch_subprocess(cmd, env, stdout_path, stderr_path)\n            active[gpu_id] = proc\n\n        # Poll active processes ---------------------------------------\n        finished_gpus = []\n        for gid, proc in active.items():\n            ret = proc.poll()\n            if ret is not None:  # finished\n                if ret != 0:\n                    print(f\"[MAIN] WARNING: run on GPU {gid} exited with code {ret}\")\n                finished_gpus.append(gid)\n        for gid in finished_gpus:\n            active.pop(gid)\n            gpu_ids.append(gid)  # free GPU\n        time.sleep(1)\n\n    # All done \u2192 evaluation ---------------------------------------------\n    print(\"\\nAll runs finished \u2013 launching evaluation \u2026\\n\")\n    subprocess.run([\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ], check=True)\n\n\n# ---------------------------------------------------------------------------\n# CLI entry-point -----------------------------------------------------------\n# ---------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"OHGW Experiments Orchestrator\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run the smoke-test configuration\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full experiment configuration\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store outputs, logs, figures \u2026\")\n    args = parser.parse_args()\n\n    if args.smoke_test:\n        config_file = ROOT_DIR / \"config\" / \"smoke_test.yaml\"\n    else:\n        config_file = ROOT_DIR / \"config\" / \"full_experiment.yaml\"\n\n    run_all_experiments(config_file, Path(args.results_dir).resolve())\n", "model_py": "\"\"\"src/model.py\nModel-building utilities with clear placeholders for later injection of\nreal architectures. For smoke tests we supply a minimal *DummyModel* so\nthat the pipeline runs end-to-end without editing.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import List\n\nimport torch\nfrom torch import nn\n\n\nclass DummyModel(nn.Module):\n    \"\"\"A tiny MLP suitable only for smoke tests. Not task-specific.\"\"\"\n\n    def __init__(self, input_shape: List[int], num_classes: int):\n        super().__init__()\n        c, h, w = input_shape\n        flat_dim = c * h * w\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(flat_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):  # noqa: D401,E501  (simple forward pass)\n        return self.net(x)\n\n\n# ---------------------------------------------------------------------------\n# Public factory function\n# ---------------------------------------------------------------------------\n\ndef build_model(cfg):\n    \"\"\"Return a torch.nn.Module according to `model_name` in cfg.\n\n    Placeholders:\n      \u2022 \u0027MODEL_PLACEHOLDER\u0027 or \u0027dummy\u0027 \u2192 returns DummyModel\n      \u2022 otherwise: must be implemented later.\n    \"\"\"\n    model_name = cfg.get(\"model_name\", \"MODEL_PLACEHOLDER\")\n    if model_name in {\"dummy\", \"MODEL_PLACEHOLDER\"}:\n        input_shape = cfg.get(\"input_shape\", [1, 28, 28])\n        num_classes = int(cfg.get(\"num_classes\", 10))\n        return DummyModel(input_shape, num_classes)\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Real model architectures must be injected here.\n    # ------------------------------------------------------------------\n    raise NotImplementedError(\n        f\"PLACEHOLDER: Unknown model_name=\u0027{model_name}\u0027. Provide implementation in build_model().\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nGeneric data loading \u0026 preprocessing utilities.\nDataset-specific logic is strictly *placeholder* so that later steps can\ninject true loaders. For smoke-tests we fall back to a tiny deterministic\nrandom dataset to keep the pipeline executable.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import List, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass RandomTensorDataset(Dataset):\n    \"\"\"A deterministic pseudo-random dataset for smoke-testing the pipeline.\"\"\"\n\n    def __init__(self, length: int, input_shape: List[int], num_classes: int):\n        super().__init__()\n        self.length = length\n        generator = torch.Generator().manual_seed(42)\n        self.data = torch.randn(length, *input_shape, generator=generator)\n        self.targets = torch.randint(0, num_classes, (length,), generator=generator)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n\n# ---------------------------------------------------------------------------\n# Public API\n# ---------------------------------------------------------------------------\n\ndef get_dataloaders(cfg) -\u003e Tuple[DataLoader, DataLoader]:\n    \"\"\"Return train \u0026 validation DataLoaders based on the run-config.\n\n    Placeholders:\n      \u2022 If `dataset_name` is \u0027DATASET_PLACEHOLDER\u0027 or \u0027dummy\u0027, a random\n        dataset will be returned (useful for smoke tests).\n      \u2022 Otherwise this function *must* be extended in later experiment\n        specialisations to load the actual dataset.\n    \"\"\"\n    dataset_name = cfg.get(\"dataset_name\", \"DATASET_PLACEHOLDER\")\n    batch_size = int(cfg.get(\"batch_size\", 64))\n    num_workers = int(cfg.get(\"num_workers\", 2))\n\n    if dataset_name in {\"dummy\", \"DATASET_PLACEHOLDER\"}:\n        input_shape = cfg.get(\"input_shape\", [1, 28, 28])\n        num_classes = int(cfg.get(\"num_classes\", 10))\n        length = int(cfg.get(\"dummy_length\", 1024))\n        train_ds = RandomTensorDataset(length, input_shape, num_classes)\n        val_ds = RandomTensorDataset(length // 4, input_shape, num_classes)\n        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n        return train_loader, val_loader\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Real dataset loading must be implemented here.\n    # ------------------------------------------------------------------\n    raise NotImplementedError(\n        \"PLACEHOLDER: Replace get_dataloaders with dataset-specific logic for \"\n        f\"dataset_name=\u0027{dataset_name}\u0027.\")\n", "pyproject_toml": "[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experimental framework\"\nauthors = [\n  { name = \"AI Research Assistant\" }\n]\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"", "smoke_test_yaml": "# config/smoke_test.yaml\n# A *fully runnable* configuration that uses dummy dataset/model so that\n# the GitHub Actions smoke test does not require heavy compute or data.\n\nexperiments:\n  - run_id: smoke_dummy\n    dataset_name: dummy                # uses RandomTensorDataset\n    model_name: dummy                  # uses DummyModel (tiny MLP)\n    input_shape: [1, 28, 28]           # channels, height, width\n    num_classes: 10\n    batch_size: 32\n    epochs: 2\n    seed: 0\n    target_metric: 0.5                 # arbitrary for dummy data\n    eta_h: 0.001\n    hyperparams:\n      log_lr: -1.0                     # \u2192 0.1\n      log_wd: -4.0                     # \u2192 1e-4\n      momentum: 0.9", "train_py": "\"\"\"src/train.py\nCore training logic for a single experiment variation.\nThis script is NOT dataset- or model-specific; any such specifics are\nsupplied via the YAML run-config that main.py passes in.\nIt implements the One-Shot Hyper-Gradient Warm-Start (OHGW) procedure\nonce at the beginning of training, then proceeds with standard epochs.\nResults, logs and figures for this single run are written into the\n\u003cresults_dir\u003e/\u003crun_id\u003e/ sub-folder (created if necessary).\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# Local imports \u2013 all generic / placeholder-aware\nfrom .preprocess import get_dataloaders  # noqa: E402\nfrom .model import build_model  # noqa: E402\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single OHGW experiment variation\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to YAML file describing ONE experiment variation.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where all results are stored\")\n    return parser.parse_args()\n\n\n# ---------------------------------------------------------------------------\n# Hyper-parameter container\n# ---------------------------------------------------------------------------\n\nclass HyperParameters(nn.Module):\n    \"\"\"Wraps continuous hyper-parameters as torch.Parameters so that\n    autograd can compute dL/d\u03c8. A config field `hyperparams` must be\n    provided in the run-config YAML.\n    \"\"\"\n\n    def __init__(self, cfg: Dict):\n        super().__init__()\n        hcfg: Dict = cfg[\"hyperparams\"]\n        # Every hyper-param becomes a learnable scalar \u2011- additional ones can be\n        # plugged in freely by later experiment variants.\n        self.log_lr = nn.Parameter(torch.tensor(float(hcfg.get(\"log_lr\", -1.0))))\n        self.log_wd = nn.Parameter(torch.tensor(float(hcfg.get(\"log_wd\", -4.0))))\n        # momentum can be negative for centred updates \u2192 use tanh to keep |m|\u003c1\n        self.raw_momentum = nn.Parameter(torch.tensor(float(hcfg.get(\"momentum\", 0.9)).atanh()))\n\n    # Convenient accessors ----------------------------------------------------\n    @property\n    def lr(self) -\u003e float:\n        return float(10.0 ** self.log_lr.detach())\n\n    @property\n    def weight_decay(self) -\u003e float:\n        return float(10.0 ** self.log_wd.detach())\n\n    @property\n    def momentum(self) -\u003e float:\n        return float(self.raw_momentum.tanh().detach())\n\n    def as_dict(self):\n        return {\n            \"log_lr\": self.log_lr.detach().cpu().item(),\n            \"log_wd\": self.log_wd.detach().cpu().item(),\n            \"momentum\": self.momentum,\n        }\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef compute_loss(model: nn.Module, criterion: nn.Module, x: torch.Tensor, y: torch.Tensor, hp: HyperParameters) -\u003e torch.Tensor:\n    \"\"\"Loss = classification_loss + weight-decay that depends on hp.log_wd.\"\"\"\n    logits = model(x)\n    loss = criterion(logits, y)\n    if hp.log_wd.requires_grad:\n        wd_coeff = torch.exp(hp.log_wd)\n        wd = torch.zeros([], device=logits.device)\n        for p in model.parameters():\n            wd = wd + (p ** 2).sum()\n        loss = loss + wd_coeff * wd\n    return loss\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module, hp: HyperParameters, device: torch.device) -\u003e Tuple[float, float]:\n    \"\"\"Returns (average loss, accuracy)\"\"\"\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    n = 0\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            loss = compute_loss(model, criterion, xb, yb, hp)\n            total_loss += loss.item() * xb.size(0)\n            logits = model(xb)\n            pred = logits.argmax(dim=1)\n            correct += (pred == yb).sum().item()\n            n += xb.size(0)\n    avg_loss = total_loss / max(1, n)\n    acc = correct / max(1, n)\n    return avg_loss, acc\n\n\n# ---------------------------------------------------------------------------\n# Main training routine\n# ---------------------------------------------------------------------------\n\ndef train(cfg: Dict, results_dir: Path):\n    run_id: str = cfg[\"run_id\"]\n    torch.manual_seed(int(cfg.get(\"seed\", 0)))\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # Build model \u0026 data loaders (generic \u2013 specifics pluggable later)\n    # ---------------------------------------------------------------------\n    model = build_model(cfg).to(device)\n    train_loader, val_loader = get_dataloaders(cfg)\n\n    # Hyper-parameters (as trainable tensors to obtain hyper-gradient)\n    hp = HyperParameters(cfg).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    # ---------------------------------------------------------------------\n    # One-Shot Hyper-Gradient Warm-Start (OHGW)\n    # ---------------------------------------------------------------------\n    warm_inputs, warm_targets = next(iter(train_loader))\n    warm_inputs, warm_targets = warm_inputs.to(device), warm_targets.to(device)\n\n    loss_warm = compute_loss(model, criterion, warm_inputs, warm_targets, hp)\n    grads = torch.autograd.grad(loss_warm, list(hp.parameters()), create_graph=False)\n\n    eta_h: float = float(cfg.get(\"eta_h\", 1e-3))\n    with torch.no_grad():\n        for p, g in zip(hp.parameters(), grads):\n            p -= eta_h * g  # one hyper-step\n\n    # ---------------------------------------------------------------------\n    # Optimiser INITIALISED *after* warm-start so that new hp values apply.\n    # ---------------------------------------------------------------------\n    optimiser = torch.optim.SGD(\n        model.parameters(),\n        lr=hp.lr,\n        momentum=hp.momentum,\n        weight_decay=hp.weight_decay,\n    )\n\n    # ---------------------------------------------------------------------\n    # Training loop (standard supervised classification)\n    # ---------------------------------------------------------------------\n    n_epochs = int(cfg.get(\"epochs\", 10))\n    train_loss_hist: List[float] = []\n    val_loss_hist: List[float] = []\n    val_acc_hist: List[float] = []\n\n    start_time = time.time()\n    time_to_target: float | None = None\n\n    target_metric = cfg.get(\"target_metric\")  # e.g. 0.93 accuracy\n\n    for epoch in range(n_epochs):\n        model.train()\n        epoch_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimiser.zero_grad(set_to_none=True)\n            loss = compute_loss(model, criterion, xb, yb, hp)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * xb.size(0)\n            n_train += xb.size(0)\n        avg_train_loss = epoch_loss / max(1, n_train)\n\n        val_loss, val_acc = evaluate(model, val_loader, criterion, hp, device)\n\n        train_loss_hist.append(avg_train_loss)\n        val_loss_hist.append(val_loss)\n        val_acc_hist.append(val_acc)\n\n        # Trigger wall-clock target metric timer\n        if target_metric is not None and time_to_target is None and val_acc \u003e= target_metric:\n            time_to_target = time.time() - start_time\n\n        print(json.dumps({\n            \"run_id\": run_id,\n            \"event\": \"epoch_end\",\n            \"epoch\": epoch + 1,\n            \"train_loss\": avg_train_loss,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n        }))\n\n    total_time = time.time() - start_time\n\n    # ---------------------------------------------------------------------\n    # Persist results ------------------------------------------------------\n    # ---------------------------------------------------------------------\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save model weights \u2013 may be useful for downstream analyses ----------\n    torch.save({\n        \"model_state_dict\": model.state_dict(),\n        \"hyperparams\": hp.as_dict(),\n    }, run_dir / \"checkpoint.pt\")\n\n    # Prepare metrics dict -------------------------------------------------\n    results = {\n        \"run_id\": run_id,\n        \"hyperparameters\": hp.as_dict(),\n        \"train_loss_history\": train_loss_hist,\n        \"val_loss_history\": val_loss_hist,\n        \"val_accuracy_history\": val_acc_hist,\n        \"best_val_accuracy\": max(val_acc_hist) if val_acc_hist else None,\n        \"training_time_sec\": total_time,\n        \"time_to_target_sec\": time_to_target,\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Human-readable summary printed to stdout ----------------------------\n    summary_str = (\n        f\"\\n=== Experiment Summary [{run_id}] ===\\n\"\n        f\"Best Val Acc: {results[\u0027best_val_accuracy\u0027]:.4f}\\n\"\n        f\"Total time     : {total_time/60:.2f} min\\n\"\n        f\"Time\u2192target(\u2265{target_metric}): {time_to_target if time_to_target is not None else \u0027N/A\u0027} sec\\n\"\n        f\"======================================\\n\"\n    )\n    print(summary_str)\n\n\n# ---------------------------------------------------------------------------\n# Entry-point\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    import yaml  # local import to avoid dependency during module import\n\n    with open(args.run_config, \"r\") as fh:\n        cfg = yaml.safe_load(fh)\n\n    results_root = Path(args.results_dir).resolve()\n    train(cfg, results_root)\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-2
- Description: Objective / Hypothesis: Test zero-shot generalisation and robustness of OHGW in a different modality (language modelling) and under distribution shift.

Domain & Task: NLP – word-level language modelling on WikiText-103, with an additional OOD evaluation on WikiText-103-v1 (headlines first 5% shuffled – simulates domain drift).

Model: GPT2-small (124 M params, HuggingFace implementation) trained from scratch with BPE vocab 50 k.

Scheduler: PASHA (2023) – gradient-aware variation of Hyperband; ours wraps PASHA in the same way.

Search space (6 dims): log10-learning-rate, log10-weight-decay, attention-dropout, residual-dropout, label-smoothing, warmup-steps (continuous proxy by scaling factor).

Run variations:
• pasha-baseline – vanilla PASHA.
• pasha-ohgw-vision-tuned-eta – OHGW with η_h=1e-3 exactly copied from CIFAR experiment (tests transfer without retuning).
• pasha-ohgw-low-eta – OHGW with η_h=3e-4 (sensitivity extremum).
• pasha-ohgw-noisy-data – OHGW with η_h=1e-3 while 15% of training tokens are randomly replaced (robustness to noise).

Dataset processing: SentencePiece BPE (shared).  Sequence length 1024 tokens; dynamic batching up to 2M tokens/GPU.  Train/val/test split 238M / 8M / 8M tokens.  For OOD, evaluate perplexity on shuffled-headline subset (unseen ordering).

Training loop & budget: 50 training epochs (~250 k updates), AdamW, cosine LR.  PASHA minimum resource per config = 2 epochs, rungs ×2.  Total compute budget per replicate 16 GPU×hours (8 GPUs ×2 h).  24 replicates.

Evaluation metrics:
Primary – T@ppl=30 on validation set (wall-clock & GPU-h).
Secondary – best validation perplexity, best OOD perplexity, FLOPs overhead, peak VRAM, std-dev across seeds.
Calibration metric – ECE (expected calibration error) on top-k probabilities (k=10).

Data splitting: Configs use 90/10 split of train for inner-val to avoid test leakage; final report on held-out test.

Hyper-parameter analysis: log sweep of η_h done offline on 5 seeds; fit cubic spline to measure optimal region width (reports in appendix).

Robustness procedures:
1. Data noise: see pasha-ohgw-noisy-data.
2. Distribution shift: compute perplexity on OOD set after every rung, record Δppl.
3. Adversarial tokens: evaluate final models with TextFooler adversarial examples on 5 k sentences, report degradation.

Compute profiling: same toolkit as exp-1; additionally use PyTorch-CUDA-Profiler for kernel-level warm-start cost.

Example trial wrapper:
```python
loss = lm_model(input_ids, labels=input_ids).loss
hg = torch.autograd.grad(loss, cfg.continuous())
with torch.no_grad():
    for p,g in zip(cfg.continuous(), hg):
        p -= eta_h * g    # no extra unroll
pasha_scheduler.launch(cfg)
```

Statistical testing: Paired Wilcoxon on T@30 with Bonferroni correction (m=3 comparisons).  Significance if p<0.016.

Success criteria: vision-tuned η_h version achieves ≥15% speed-up vs. baseline on T@30 with ≤0.5 ppl regression on final test; effect still ≥10% under noisy data.  Extra compute overhead ≤5% of total FLOPs.
- Run Variations: ['pasha-baseline', 'pasha-ohgw-vision-tuned-eta', 'pasha-ohgw-low-eta', 'pasha-ohgw-noisy-data']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results of all experiment variations, computes comparison\nstatistics and generates publication-quality figures in .pdf format.\nFigures are stored in \u003cresults_dir\u003e/figures/ and also listed in the\nstdout JSON summary for easy discovery.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_KWARGS = dict(bbox_inches=\"tight\")\n\n\ndef load_all_results(results_dir: Path) -\u003e List[Dict]:\n    results: List[Dict] = []\n    for res_file in results_dir.rglob(\"results.json\"):\n        with open(res_file) as f:\n            results.append(json.load(f))\n    if not results:\n        raise RuntimeError(f\"No results.json files found under {results_dir}\")\n    return results\n\n\ndef figure_training_loss(all_results: List[Dict], save_dir: Path):\n    plt.figure(figsize=(6, 4))\n    for res in all_results:\n        plt.plot(res[\"train_loss_history\"], label=res[\"run_id\"])\n        # Annotate final value\n        plt.annotate(f\"{res[\u0027train_loss_history\u0027][-1]:.3f}\",\n                     xy=(len(res[\u0027train_loss_history\u0027]) - 1, res[\u0027train_loss_history\u0027][-1]),\n                     textcoords=\"offset points\", xytext=(0, 5))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Training Loss\")\n    plt.title(\"Training Loss Curves\")\n    plt.legend()\n    fname = save_dir / \"training_loss.pdf\"\n    plt.savefig(fname, **FIG_KWARGS)\n    plt.close()\n    return str(fname.name)\n\n\ndef figure_accuracy(all_results: List[Dict], save_dir: Path):\n    plt.figure(figsize=(6, 4))\n    for res in all_results:\n        if not res[\"val_accuracy_history\"]:\n            continue\n        plt.plot(res[\"val_accuracy_history\"], label=res[\"run_id\"])\n        plt.annotate(f\"{res[\u0027val_accuracy_history\u0027][-1]:.3f}\",\n                     xy=(len(res[\u0027val_accuracy_history\u0027]) - 1, res[\u0027val_accuracy_history\u0027][-1]),\n                     textcoords=\"offset points\", xytext=(0, 5))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy Curves\")\n    plt.legend()\n    fname = save_dir / \"accuracy.pdf\"\n    plt.savefig(fname, **FIG_KWARGS)\n    plt.close()\n    return str(fname.name)\n\n\ndef bar_best_accuracy(all_results: List[Dict], save_dir: Path):\n    accs = {r[\"run_id\"]: r[\"best_val_accuracy\"] for r in all_results if r[\"best_val_accuracy\"] is not None}\n    if not accs:\n        return None\n    plt.figure(figsize=(6, 4))\n    names = list(accs.keys())\n    vals = [accs[n] for n in names]\n    sns.barplot(x=names, y=vals)\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.title(\"Comparison of Best Accuracies\")\n    fname = save_dir / \"best_val_accuracy.pdf\"\n    plt.savefig(fname, **FIG_KWARGS)\n    plt.close()\n    return str(fname.name)\n\n\ndef evaluate(results_dir: Path):\n    print(\"\\n===== Aggregating experiment results =====\\n\")\n    all_results = load_all_results(results_dir)\n\n    figures_dir = results_dir / \"figures\"\n    figures_dir.mkdir(exist_ok=True, parents=True)\n\n    figure_files: List[str] = []\n    figure_files.append(figure_training_loss(all_results, figures_dir))\n    fig_acc = figure_accuracy(all_results, figures_dir)\n    if fig_acc:\n        figure_files.append(fig_acc)\n    fig_bar = bar_best_accuracy(all_results, figures_dir)\n    if fig_bar:\n        figure_files.append(fig_bar)\n\n    # ------------------------------------------------------------------\n    # Consolidated comparison table for stdout -------------------------\n    # ------------------------------------------------------------------\n    df_rows = []\n    for r in all_results:\n        df_rows.append({\n            \"run_id\": r[\"run_id\"],\n            \"best_val_accuracy\": r[\"best_val_accuracy\"],\n            \"training_time_sec\": r[\"training_time_sec\"],\n            \"time_to_target_sec\": r[\"time_to_target_sec\"],\n        })\n    df = pd.DataFrame(df_rows)\n\n    comparison = df.to_dict(orient=\"records\")\n    summary = {\n        \"description\": \"Comparison of OHGW experiment variations\",\n        \"num_runs\": len(all_results),\n        \"figures\": figure_files,\n        \"table\": comparison,\n    }\n    print(json.dumps(summary, indent=2))\n\n\n# ---------------------------------------------------------------------------\n# Entry-point\n# ---------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    import argparse\n\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    args = p.parse_args()\n\n    evaluate(Path(args.results_dir).resolve())\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete set of four experiment variations for exp-2 (WikiText-103 LM)\n\nexperiments:\n  # ---------------------------------------------------------------------\n  # 1) PASHA baseline (no OHGW)\n  # ---------------------------------------------------------------------\n  - run_id: pasha-baseline\n    task_type: lm\n    dataset_name: wikitext103\n    model_name: gpt2-small-scratch\n    batch_size: 4                # sequences per GPU (\u22484096 tokens)\n    seq_len: 1024\n    epochs: 50\n    seed: 0\n    optimizer: adamw\n    target_metric: 30            # perplexity \u226430 (val)\n    eta_h: 0.0                   # *no* hyper-gradient step\n    hyperparams:\n      log_lr: -4.0               # 1e-4 initial lr (log10)\n      log_wd: -2.0               # 1e-2 weight decay\n      momentum: 0.9              # unused by AdamW but kept\n\n  # ---------------------------------------------------------------------\n  # 2) PASHA + OHGW (vision-tuned \u03b7_h)\n  # ---------------------------------------------------------------------\n  - run_id: pasha-ohgw-vision-tuned-eta\n    task_type: lm\n    dataset_name: wikitext103\n    model_name: gpt2-small-scratch\n    batch_size: 4\n    seq_len: 1024\n    epochs: 50\n    seed: 1\n    optimizer: adamw\n    target_metric: 30\n    eta_h: 0.001                 # vision-tuned value transferred as-is\n    hyperparams:\n      log_lr: -4.0\n      log_wd: -2.0\n      momentum: 0.9\n\n  # ---------------------------------------------------------------------\n  # 3) PASHA + OHGW (low \u03b7_h)\n  # ---------------------------------------------------------------------\n  - run_id: pasha-ohgw-low-eta\n    task_type: lm\n    dataset_name: wikitext103\n    model_name: gpt2-small-scratch\n    batch_size: 4\n    seq_len: 1024\n    epochs: 50\n    seed: 2\n    optimizer: adamw\n    target_metric: 30\n    eta_h: 0.0003                # lower hyper-learning-rate\n    hyperparams:\n      log_lr: -4.0\n      log_wd: -2.0\n      momentum: 0.9\n\n  # ---------------------------------------------------------------------\n  # 4) PASHA + OHGW with 15 % noisy tokens\n  # ---------------------------------------------------------------------\n  - run_id: pasha-ohgw-noisy-data\n    task_type: lm\n    dataset_name: wikitext103\n    model_name: gpt2-small-scratch\n    batch_size: 4\n    seq_len: 1024\n    epochs: 50\n    seed: 3\n    optimizer: adamw\n    target_metric: 30\n    eta_h: 0.001\n    noise_ratio: 0.15            # handled by data-layer wrapper (future work)\n    hyperparams:\n      log_lr: -4.0\n      log_wd: -2.0\n      momentum: 0.9\n\n# ---------------------------------------------------------------------------\n# End of experiment list ----------------------------------------------------\n# ---------------------------------------------------------------------------", "main_py": "\"\"\"src/main.py\nCentral experiment orchestrator. Reads the YAML configuration file that\nlists *all* run variations, schedules them on the available GPUs (one GPU\nper subprocess), manages logging tee-ing, and finally invokes evaluate.py\nonce all runs have finished.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport queue\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\nimport torch\n\nROOT_DIR = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT_DIR / \"src\"\n\n\n# ---------------------------------------------------------------------------\n# Helper: live tee of subprocess output to both file and main stdout ------\n# ---------------------------------------------------------------------------\n\ndef _reader_thread(pipe, tee_file):\n    with pipe:\n        for line in iter(pipe.readline, b\"\"):\n            decoded = line.decode()\n            tee_file.write(decoded)\n            tee_file.flush()\n            sys.stdout.write(decoded)\n            sys.stdout.flush()\n\n\ndef launch_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path) -\u003e subprocess.Popen:\n    stdout_f = open(stdout_path, \"w\")\n    stderr_f = open(stderr_path, \"w\")\n\n    # Merge stderr into its own pipe for tee-ing\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n    threading.Thread(target=_reader_thread, args=(proc.stdout, stdout_f), daemon=True).start()\n    threading.Thread(target=_reader_thread, args=(proc.stderr, stderr_f), daemon=True).start()\n    return proc\n\n\n# ---------------------------------------------------------------------------\n# Scheduler that ensures \u2264 num_gpus concurrent processes ---------------\n# ---------------------------------------------------------------------------\n\ndef run_all_experiments(config_path: Path, results_dir: Path):\n    with open(config_path, \"r\") as fh:\n        cfg_root = yaml.safe_load(fh)\n\n    experiments: List[Dict] = cfg_root[\"experiments\"]\n    if not experiments:\n        raise ValueError(\"No experiments found in config file\")\n\n    n_available_gpus = torch.cuda.device_count()\n    if n_available_gpus == 0:\n        print(\"WARNING: No GPUs detected \u2013 running on CPU.\")\n        n_available_gpus = 1  # schedule serially on CPU\n\n    print(f\"Detected {n_available_gpus} GPUs \u2192 launching up to {n_available_gpus} concurrent runs.\")\n\n    # Queue of pending experiments -------------------------------------\n    exp_queue = queue.Queue()\n    for exp in experiments:\n        exp_queue.put(exp)\n\n    active: Dict[int, subprocess.Popen] = {}\n    gpu_ids = list(range(n_available_gpus))\n\n    while not exp_queue.empty() or active:\n        # Launch new processes if GPU free\n        while gpu_ids and not exp_queue.empty():\n            gpu_id = gpu_ids.pop(0)\n            exp_cfg = exp_queue.get()\n            run_id = exp_cfg[\"run_id\"]\n\n            run_dir = results_dir / run_id\n            run_dir.mkdir(parents=True, exist_ok=True)\n\n            # Write run-specific YAML config so train.py can read it.\n            run_cfg_path = run_dir / \"run_config.yaml\"\n            with open(run_cfg_path, \"w\") as fh:\n                yaml.safe_dump(exp_cfg, fh)\n\n            stdout_path = run_dir / \"stdout.log\"\n            stderr_path = run_dir / \"stderr.log\"\n\n            cmd = [\n                sys.executable,\n                \"-m\",\n                \"src.train\",\n                \"--run-config\",\n                str(run_cfg_path),\n                \"--results-dir\",\n                str(results_dir),\n            ]\n\n            env = os.environ.copy()\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n\n            print(f\"[MAIN] Launching run_id={run_id} on GPU {gpu_id} \u2026\")\n            proc = launch_subprocess(cmd, env, stdout_path, stderr_path)\n            active[gpu_id] = proc\n\n        # Poll active processes ---------------------------------------\n        finished_gpus = []\n        for gid, proc in active.items():\n            ret = proc.poll()\n            if ret is not None:  # finished\n                if ret != 0:\n                    print(f\"[MAIN] WARNING: run on GPU {gid} exited with code {ret}\")\n                finished_gpus.append(gid)\n        for gid in finished_gpus:\n            active.pop(gid)\n            gpu_ids.append(gid)  # free GPU\n        time.sleep(1)\n\n    # All done \u2192 evaluation ---------------------------------------------\n    print(\"\\nAll runs finished \u2013 launching evaluation \u2026\\n\")\n    subprocess.run([\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ], check=True)\n\n\n# ---------------------------------------------------------------------------\n# CLI entry-point -----------------------------------------------------------\n# ---------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"OHGW Experiments Orchestrator\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run the smoke-test configuration\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full experiment configuration\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store outputs, logs, figures \u2026\")\n    args = parser.parse_args()\n\n    if args.smoke_test:\n        config_file = ROOT_DIR / \"config\" / \"smoke_test.yaml\"\n    else:\n        config_file = ROOT_DIR / \"config\" / \"full_experiment.yaml\"\n\n    run_all_experiments(config_file, Path(args.results_dir).resolve())\n", "model_py": "\"\"\"src/model.py\nModel factory with support for:\n\u2022 `dummy`                 \u2013 tiny MLP for smoke tests\n\u2022 `gpt2-small-scratch`    \u2013 GPT-2 small (~124 M) initialised from scratch\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import List\n\nimport torch\nfrom torch import nn\n\n# Optional heavy dep \u2013 only needed for GPT-2 runs\ntry:\n    from transformers import GPT2Config, GPT2LMHeadModel  # type: ignore\nexcept ImportError:  # pragma: no cover \u2013 transformers not required for smoke-tests\n    GPT2Config = None  # type: ignore\n    GPT2LMHeadModel = None  # type: ignore\n\n\nclass DummyModel(nn.Module):\n    \"\"\"Tiny two-layer MLP \u2013 used solely by smoke-tests.\"\"\"\n\n    def __init__(self, input_shape: List[int], num_classes: int):\n        super().__init__()\n        c, h, w = input_shape\n        flat_dim = c * h * w\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(flat_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):  # noqa: D401\n        return self.net(x)\n\n\n# ---------------------------------------------------------------------------\n# Factory -------------------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef build_model(cfg):\n    model_name = cfg.get(\"model_name\", \"dummy\").lower()\n\n    # ---------------- Dummy --------------------------------------------------\n    if model_name == \"dummy\":\n        input_shape = cfg.get(\"input_shape\", [1, 28, 28])\n        num_classes = int(cfg.get(\"num_classes\", 10))\n        return DummyModel(input_shape, num_classes)\n\n    # ---------------- GPT-2 small from scratch ------------------------------\n    if model_name == \"gpt2-small-scratch\":\n        if GPT2Config is None or GPT2LMHeadModel is None:\n            raise RuntimeError(\"transformers package required for GPT-2 model.\")\n        config = GPT2Config(\n            vocab_size=50257,\n            n_positions=1024,\n            n_embd=768,\n            n_layer=12,\n            n_head=12,\n            resid_pdrop=float(cfg.get(\"residual_dropout\", 0.1)),\n            embd_pdrop=float(cfg.get(\"attention_dropout\", 0.1)),\n            attn_pdrop=float(cfg.get(\"attention_dropout\", 0.1)),\n        )\n        model = GPT2LMHeadModel(config)\n        return model\n\n    # ---------------- Unknown ------------------------------------------------\n    raise NotImplementedError(f\"Unknown model_name=\u0027{model_name}\u0027.\")", "preprocess_py": "\"\"\"src/preprocess.py\nDataset loading \u0026 preprocessing utilities.  Supports two modes:\n1.  `dummy`   \u2013 deterministic random tensors (used by smoke-tests)\n2.  `wikitext103` \u2013 word-level language modelling dataset prepared for\n    GPT-2-style causal LM training (sequence length configurable).\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\n# Optional heavy deps \u2013 imported lazily so smoke-tests remain cheap\ntry:\n    from datasets import load_dataset  # type: ignore\n    from transformers import GPT2TokenizerFast  # type: ignore\nexcept ImportError:  # pragma: no cover \u2013 not required for dummy tests\n    load_dataset = None  # type: ignore\n    GPT2TokenizerFast = None  # type: ignore\n\n\n# ---------------------------------------------------------------------------\n# Dummy dataset for smoke-tests ---------------------------------------------\n# ---------------------------------------------------------------------------\n\n\nclass RandomTensorDataset(Dataset):\n    \"\"\"Deterministic pseudo-random dataset suitable for pipeline tests.\"\"\"\n\n    def __init__(self, length: int, input_shape: List[int], num_classes: int):\n        super().__init__()\n        generator = torch.Generator().manual_seed(42)\n        self.data = torch.randn(length, *input_shape, generator=generator)\n        self.targets = torch.randint(0, num_classes, (length,), generator=generator)\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n\n# ---------------------------------------------------------------------------\n# Helper \u2013 build LM TensorDataset -------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _build_lm_tensor_dataset(tokenizer, texts: List[str], seq_len: int) -\u003e TensorDataset:\n    \"\"\"Concatenate texts -\u003e chunk into seq_len blocks -\u003e TensorDataset.\"\"\"\n    encodings = tokenizer(texts, return_attention_mask=False, add_special_tokens=False)[\"input_ids\"]\n    flat_ids = [token for sub in encodings for token in sub]\n    total_len = (len(flat_ids) // seq_len) * seq_len\n    if total_len == 0:\n        raise ValueError(\"Not enough tokens to build even a single block\")\n    flat_ids = flat_ids[:total_len]\n    tensor = torch.tensor(flat_ids, dtype=torch.long)\n    blocks = tensor.view(-1, seq_len)  # (num_blocks, seq_len)\n    return TensorDataset(blocks)\n\n\n# ---------------------------------------------------------------------------\n# Public API ----------------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef get_dataloaders(cfg: Dict) -\u003e Tuple[DataLoader, DataLoader]:\n    dataset_name = cfg.get(\"dataset_name\", \"dummy\").lower()\n    batch_size = int(cfg.get(\"batch_size\", 32))\n    num_workers = int(cfg.get(\"num_workers\", 2))\n\n    # ---------------- Dummy --------------------------------------------------\n    if dataset_name in {\"dummy\", \"random\"}:\n        input_shape = cfg.get(\"input_shape\", [1, 28, 28])\n        num_classes = int(cfg.get(\"num_classes\", 10))\n        length = int(cfg.get(\"dummy_length\", 1024))\n        train_ds = RandomTensorDataset(length, input_shape, num_classes)\n        val_ds = RandomTensorDataset(length // 4, input_shape, num_classes)\n        return (\n            DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n            DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n        )\n\n    # ---------------- WikiText-103 -----------------------------------------\n    if dataset_name == \"wikitext103\":\n        if load_dataset is None or GPT2TokenizerFast is None:\n            raise RuntimeError(\"`datasets` and `transformers` packages must be installed for WikiText-103\")\n\n        seq_len = int(cfg.get(\"seq_len\", 1024))\n        tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n        tokenizer.model_max_length = seq_len\n        tokenizer.add_special_tokens({\u0027pad_token\u0027: \u0027\u003c|pad|\u003e\u0027})\n\n        # Load raw dataset ------------------------------------------------\n        raw = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", trust_remote_code=True)\n        train_texts = raw[\"train\"][\"text\"]\n        val_texts = raw[\"validation\"][\"text\"]\n\n        train_ds = _build_lm_tensor_dataset(tokenizer, train_texts, seq_len)\n        val_ds = _build_lm_tensor_dataset(tokenizer, val_texts, seq_len)\n\n        # Collate fn converts tensor batch -\u003e dict required by HF causal LM\n        def collate_fn(batch):\n            input_ids = torch.stack([item[0] for item in batch])  # (B, seq_len)\n            attention_mask = torch.ones_like(input_ids)\n            return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n\n        train_loader = DataLoader(\n            train_ds,\n            batch_size=batch_size,\n            shuffle=True,\n            collate_fn=collate_fn,\n            num_workers=num_workers,\n        )\n        val_loader = DataLoader(\n            val_ds,\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=collate_fn,\n            num_workers=num_workers,\n        )\n        return train_loader, val_loader\n\n    # ---------------- Unknown dataset --------------------------------------\n    raise NotImplementedError(f\"Unknown dataset_name=\u0027{dataset_name}\u0027. Implement in preprocess.py.\")", "pyproject_toml": "[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"OHGW experimental framework\"\nauthors = [{ name = \"AI Research Assistant\" }]\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\ntransformers = \"*\"\ndatasets = \"*\"\nsentencepiece = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight versions of all four experiment variations using the dummy\n# dataset / model so that CI can finish within seconds.\n\nexperiments:\n  - run_id: pasha-baseline-smoke\n    dataset_name: dummy\n    model_name: dummy\n    input_shape: [1, 28, 28]\n    num_classes: 10\n    batch_size: 16\n    epochs: 1\n    seed: 0\n    target_metric: 0.5\n    eta_h: 0.0\n    hyperparams:\n      log_lr: -1.0\n      log_wd: -4.0\n      momentum: 0.9\n\n  - run_id: pasha-ohgw-vision-tuned-eta-smoke\n    dataset_name: dummy\n    model_name: dummy\n    input_shape: [1, 28, 28]\n    num_classes: 10\n    batch_size: 16\n    epochs: 1\n    seed: 1\n    target_metric: 0.5\n    eta_h: 0.001\n    hyperparams:\n      log_lr: -1.0\n      log_wd: -4.0\n      momentum: 0.9\n\n  - run_id: pasha-ohgw-low-eta-smoke\n    dataset_name: dummy\n    model_name: dummy\n    input_shape: [1, 28, 28]\n    num_classes: 10\n    batch_size: 16\n    epochs: 1\n    seed: 2\n    target_metric: 0.5\n    eta_h: 0.0003\n    hyperparams:\n      log_lr: -1.0\n      log_wd: -4.0\n      momentum: 0.9\n\n  - run_id: pasha-ohgw-noisy-data-smoke\n    dataset_name: dummy\n    model_name: dummy\n    input_shape: [1, 28, 28]\n    num_classes: 10\n    batch_size: 16\n    epochs: 1\n    seed: 3\n    target_metric: 0.5\n    eta_h: 0.001\n    noise_ratio: 0.15  # ignored in dummy run\n    hyperparams:\n      log_lr: -1.0\n      log_wd: -4.0\n      momentum: 0.9", "train_py": "\"\"\"src/train.py\nUpdated training loop capable of both classification *and* language-model\nexperiments (GPT-2 on WikiText-103).  The OHGW warm-start logic and the\noptimiser now adapt automatically based on `task_type` specified in the\nrun-config YAML.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# Local imports\nfrom .preprocess import get_dataloaders  # noqa: E402\nfrom .model import build_model  # noqa: E402\n\ntry:\n    from transformers import AutoTokenizer  # type: ignore\nexcept ImportError:  # transformers not required for smoke-tests\n    AutoTokenizer = None  # type: ignore\n\n\n# ---------------------------------------------------------------------------\n# CLI -----------------------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(\"Run a single OHGW experiment variation\")\n    p.add_argument(\"--run-config\", type=str, required=True)\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    return p.parse_args()\n\n\n# ---------------------------------------------------------------------------\n# Hyper-parameter container --------------------------------------------------\n# ---------------------------------------------------------------------------\n\nclass HyperParameters(nn.Module):\n    \"\"\"Wrap continuous hyper-parameters so that dL/d\u03c8 can be obtained by\n    autograd.  Each parameter is a scalar torch.Parameter.\n    \"\"\"\n\n    def __init__(self, cfg: Dict):\n        super().__init__()\n        hp_cfg: Dict = cfg.get(\"hyperparams\", {})\n        self.log_lr = nn.Parameter(torch.tensor(float(hp_cfg.get(\"log_lr\", -4.0))))\n        self.log_wd = nn.Parameter(torch.tensor(float(hp_cfg.get(\"log_wd\", -4.0))))\n        # Momentum is irrelevant for AdamW but kept for search-space consistency\n        self.raw_momentum = nn.Parameter(torch.tensor(float(hp_cfg.get(\"momentum\", 0.9)).atanh()))\n\n    # Convenience --------------------------------------------------------\n    @property\n    def lr(self) -\u003e float:\n        return float(10 ** self.log_lr.detach())\n\n    @property\n    def weight_decay(self) -\u003e float:\n        return float(10 ** self.log_wd.detach())\n\n    @property\n    def momentum(self) -\u003e float:\n        return float(self.raw_momentum.tanh().detach())\n\n    def as_dict(self):\n        return {\n            \"log_lr\": self.log_lr.detach().cpu().item(),\n            \"log_wd\": self.log_wd.detach().cpu().item(),\n            \"momentum\": self.momentum,\n        }\n\n\n# ---------------------------------------------------------------------------\n# Task-specific helpers ------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef classification_step(model: nn.Module, criterion: nn.Module, xb: torch.Tensor, yb: torch.Tensor,\n                        hp: HyperParameters) -\u003e torch.Tensor:\n    logits = model(xb)\n    loss = criterion(logits, yb)\n    wd_coeff = torch.exp(hp.log_wd)\n    if wd_coeff \u003e 0:\n        wd_term = torch.zeros([], device=logits.device)\n        for p in model.parameters():\n            wd_term += (p ** 2).sum()\n        loss = loss + wd_coeff * wd_term\n    return loss\n\n\ndef lm_step(model, batch: Dict[str, torch.Tensor]) -\u003e torch.Tensor:\n    \"\"\"Forward + loss for causal language modelling using huggingface models.\"\"\"\n    outputs = model(**batch, labels=batch[\"input_ids\"])\n    # outputs.loss already incorporates averaging over tokens\n    return outputs.loss\n\n\n# ---------------------------------------------------------------------------\n# Evaluation helpers --------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef evaluate_classification(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n                            hp: HyperParameters, device: torch.device) -\u003e Tuple[float, float]:\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    n = 0\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            loss = classification_step(model, criterion, xb, yb, hp)\n            total_loss += loss.item() * xb.size(0)\n            logits = model(xb)\n            pred = logits.argmax(dim=1)\n            correct += (pred == yb).sum().item()\n            n += xb.size(0)\n    return total_loss / max(1, n), correct / max(1, n)\n\n\ndef evaluate_lm(model, loader: DataLoader, device: torch.device) -\u003e float:\n    model.eval()\n    losses = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            loss = lm_step(model, batch)\n            losses.append(loss.detach().cpu())\n    return float(torch.tensor(losses).mean())  # average loss (NLL)\n\n\n# ---------------------------------------------------------------------------\n# Main training -------------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef train(cfg: Dict, results_dir: Path):\n    run_id: str = cfg[\"run_id\"]\n    seed = int(cfg.get(\"seed\", 0))\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    task_type: str = cfg.get(\"task_type\", \"classification\").lower()\n    is_lm = task_type == \"lm\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Build data \u0026 model --------------------------------------------------\n    model = build_model(cfg).to(device)\n    train_loader, val_loader = get_dataloaders(cfg)\n\n    # Hyperparameters container -----------------------------------------\n    hp = HyperParameters(cfg).to(device)\n\n    criterion = nn.CrossEntropyLoss() if not is_lm else None\n\n    # ---------------- One-Shot Hyper-Gradient Warm-Start -----------------\n    warm_batch: Union[Tuple[torch.Tensor, torch.Tensor], Dict[str, torch.Tensor]] = next(iter(train_loader))\n\n    if is_lm:\n        warm_batch = {k: v.to(device) for k, v in warm_batch.items()}\n        loss_warm = lm_step(model, warm_batch)\n    else:\n        xb, yb = warm_batch  # type: ignore\n        xb, yb = xb.to(device), yb.to(device)\n        loss_warm = classification_step(model, criterion, xb, yb, hp)\n\n    grads = torch.autograd.grad(loss_warm, list(hp.parameters()), create_graph=False)\n    eta_h = float(cfg.get(\"eta_h\", 0.0))\n    with torch.no_grad():\n        for p, g in zip(hp.parameters(), grads):\n            p -= eta_h * g  # hyper-parameter update\n\n    # ---------------- Optimiser -----------------------------------------\n    opt_name = cfg.get(\"optimizer\", \"sgd\").lower()\n    if opt_name == \"adamw\":\n        optimiser = torch.optim.AdamW(model.parameters(), lr=hp.lr, weight_decay=hp.weight_decay)\n    else:\n        optimiser = torch.optim.SGD(model.parameters(), lr=hp.lr, momentum=hp.momentum, weight_decay=hp.weight_decay)\n\n    # ---------------- Training loop -------------------------------------\n    n_epochs = int(cfg.get(\"epochs\", 3))\n    target_metric = cfg.get(\"target_metric\")  # e.g. perplexity \u003c=30\n\n    train_loss_hist: List[float] = []\n    val_loss_hist: List[float] = []\n    val_acc_hist: List[float] = []  # may remain empty for LM\n\n    start_time = time.time()\n    time_to_target = None\n\n    for epoch in range(n_epochs):\n        model.train()\n        epoch_losses = []\n\n        for batch in train_loader:\n            optimiser.zero_grad(set_to_none=True)\n            if is_lm:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                loss = lm_step(model, batch)\n            else:\n                xb, yb = batch  # type: ignore\n                xb, yb = xb.to(device), yb.to(device)\n                loss = classification_step(model, criterion, xb, yb, hp)\n            loss.backward()\n            optimiser.step()\n            epoch_losses.append(loss.detach().cpu())\n\n        avg_train_loss = float(torch.tensor(epoch_losses).mean())\n\n        # ---------------- Validation ---------------------------------\n        if is_lm:\n            val_loss = evaluate_lm(model, val_loader, device)\n            val_loss_hist.append(val_loss)\n            train_loss_hist.append(avg_train_loss)\n\n            # Perplexity target check\n            if target_metric is not None and time_to_target is None and val_loss \u003c= target_metric:\n                time_to_target = time.time() - start_time\n\n            print(json.dumps({\n                \"run_id\": run_id,\n                \"event\": \"epoch_end\",\n                \"epoch\": epoch + 1,\n                \"train_loss\": avg_train_loss,\n                \"val_loss\": val_loss,\n            }))\n        else:\n            val_loss, val_acc = evaluate_classification(model, val_loader, criterion, hp, device)\n            train_loss_hist.append(avg_train_loss)\n            val_loss_hist.append(val_loss)\n            val_acc_hist.append(val_acc)\n\n            if target_metric is not None and time_to_target is None and val_acc \u003e= target_metric:\n                time_to_target = time.time() - start_time\n\n            print(json.dumps({\n                \"run_id\": run_id,\n                \"event\": \"epoch_end\",\n                \"epoch\": epoch + 1,\n                \"train_loss\": avg_train_loss,\n                \"val_loss\": val_loss,\n                \"val_accuracy\": val_acc,\n            }))\n\n    total_time = time.time() - start_time\n\n    # ---------------- Persist --------------------------------------------\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    torch.save({\n        \"model_state_dict\": model.state_dict(),\n        \"hyperparams\": hp.as_dict(),\n    }, run_dir / \"checkpoint.pt\")\n\n    results = {\n        \"run_id\": run_id,\n        \"task_type\": task_type,\n        \"hyperparameters\": hp.as_dict(),\n        \"train_loss_history\": train_loss_hist,\n        \"val_loss_history\": val_loss_hist,\n        \"val_accuracy_history\": val_acc_hist,\n        \"best_val_accuracy\": max(val_acc_hist) if val_acc_hist else None,\n        \"training_time_sec\": total_time,\n        \"time_to_target_sec\": time_to_target,\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Human summary --------------------------------------------------------\n    summary_lines = [\n        f\"\\n=== Summary [{run_id}] ===\",\n        f\"Task           : {task_type}\",\n        f\"Best Val Acc   : {results[\u0027best_val_accuracy\u0027] if val_acc_hist else \u0027N/A\u0027}\",\n        f\"Training time  : {total_time/60:.2f} min\",\n        f\"Time\u2192target    : {time_to_target if time_to_target is not None else \u0027N/A\u0027} sec\",\n        \"==========================\\n\",\n    ]\n    print(\"\\n\".join(summary_lines))\n\n\n# ---------------------------------------------------------------------------\n# Entry-point ---------------------------------------------------------------\n# ---------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    args = parse_args()\n    import yaml  # local import only when executed\n\n    with open(args.run_config, \"r\") as fh:\n        cfg = yaml.safe_load(fh)\n\n    train(cfg, Path(args.results_dir).resolve())"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "1. Run-variation mismatch: requirement says every variation in config/full_experiment.yaml must also appear (light-weight) in config/smoke_test.yaml with identical run_id.  The smoke test IDs have the suffix “-smoke” and therefore break the orchestrator’s expectation.\n2. OHGW logic for the new language-model task is incomplete: the warm-start gradient is taken w.r.t. hyper-parameters, but those hyper-parameters are not part of the LM loss (weight-decay term missing, lr/momentum never used).  Gradients are therefore zero and the warm-start is a no-op, violating the paper’s core functionality and the ‘no quality degradation’ rule.\n3. Evaluation not adapted for language modelling: target_metric is specified as perplexity≤30 yet the code compares raw cross-entropy loss to 30 (perplexity = exp(loss)).  No perplexity is logged or stored, so reported metrics are wrong.\n4. Minor: figure_accuracy() & bar_best_accuracy() expect accuracy values but LM experiments always leave them empty; although not crashing, resulting plots/tables are meaningless.\nBecause of these issues the derived code cannot be considered ready for production-grade experiments."
}
