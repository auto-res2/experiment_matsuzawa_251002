
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation
   - main.py implements 1 GPU per run variation allocation, queueing runs sequentially when GPUs are insufficient
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAggregates the results of all experiment variations inside a results directory,\ncomputes comparison statistics, and produces publication-ready figures.\nThe evaluation logic is **identical** for every experiment family; therefore\nALL future studies will share the exact same comparison pipeline.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Compare experimental runs in a results dir.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory that contains one sub-folder per run (created by main.py)\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef load_results(result_dir: Path):\n    res_file = result_dir / \"results.json\"\n    if not res_file.exists():\n        raise FileNotFoundError(f\"Missing results.json in {result_dir}\")\n    with open(res_file, \"r\") as fh:\n        return json.load(fh)\n\n\n# -----------------------------------------------------------------------------\n# Figure helpers \u2013 consistent naming convention enforced globally\n# -----------------------------------------------------------------------------\n\ndef bar_plot(df: pd.DataFrame, metric: str, out_path: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(x=\"run_id\", y=metric, data=df, palette=\"viridis\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.4f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.title(metric.replace(\"_\", \" \").title())\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef time_vs_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(8, 4))\n    sns.scatterplot(\n        x=\"training_time_seconds\", y=\"best_val_accuracy\", hue=\"run_id\", data=df, s=100)\n    for _, row in df.iterrows():\n        plt.annotate(row[\"run_id\"], (row[\"training_time_seconds\"], row[\"best_val_accuracy\"]))\n    plt.xlabel(\"Training Time (s)\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.title(\"Efficiency Comparison\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# Entrypoint\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n    results_root = Path(args.results_dir)\n    if not results_root.exists():\n        raise FileNotFoundError(results_root)\n\n    # ---------------------------------------------------------------------\n    # Load all results\n    # ---------------------------------------------------------------------\n    records = []\n    for sub in results_root.iterdir():\n        if (sub / \"results.json\").exists():\n            records.append(load_results(sub))\n    if not records:\n        raise RuntimeError(f\"No result files found in {results_root}\")\n\n    df = pd.DataFrame(records)\n\n    # ---------------------------------------------------------------------\n    # Basic comparisons (can be extended later with statistical tests)\n    # ---------------------------------------------------------------------\n    summary = {\n        \"n_runs\": len(df),\n        \"best_run_by_accuracy\": df.loc[df[\"final_val_accuracy\"].idxmax()][\"run_id\"],\n    }\n\n    # ---------------------------------------------------------------------\n    # Figures\n    # ---------------------------------------------------------------------\n    bar_plot(df, \"final_val_accuracy\", results_root / \"accuracy_comparison.pdf\")\n    bar_plot(df, \"training_time_seconds\", results_root / \"training_time_comparison.pdf\")\n    time_vs_accuracy(df, results_root / \"accuracy_vs_time.pdf\")\n\n    # ---------------------------------------------------------------------\n    # Console output \u2013 description + JSON\n    # ---------------------------------------------------------------------\n    print(\"\\nEVALUATION SUMMARY:\")\n    print(f\"Total runs analysed : {summary[\u0027n_runs\u0027]}\")\n    print(f\"Best run (accuracy) : {summary[\u0027best_run_by_accuracy\u0027]}\")\n\n    print(\"\\nCOMPARISON RESULTS (JSON):\")\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# Template for the complete experimental campaign.\n# PLACEHOLDERS will be replaced in the next pipeline stage.\n\nexperiments:\n  - run_id: PLACEHOLDER_RUN_ID_1  # PLACEHOLDER: unique identifier per variation\n    seed: 0\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: e.g. CIFAR10, IMAGENET\n      path: DATASET_PATH_PLACEHOLDER\n      input_size: SPECIFIC_CONFIG_PLACEHOLDER\n      num_classes: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER  # PLACEHOLDER: e.g. RESNET20, VIT_TINY\n      depth: SPECIFIC_CONFIG_PLACEHOLDER\n    train:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n    optimizer:\n      name: Adam\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n  \n  # Add more experiments here following the same schema.\n  # Each will be launched on one GPU (sequentially queued if GPUs \u003c experiments).\n\n", "main_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nOrchestrator: reads YAML config that defines multiple experimental variations,\nlaunches src.train (one GPU per variation), collects logs, and finally calls\nsrc.evaluate for a consistent comparison.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport queue\nimport shutil\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nimport yaml\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Run smoke or full experiment suite.\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run the lightweight smoke-test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full experiment suite defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where all experiment outputs go.\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Subprocess management helpers\n# -----------------------------------------------------------------------------\n\ndef _stream_process_output(proc: subprocess.Popen, log_path: Path):\n    \"\"\"Mirror subprocess stdout/stderr to main stdout and to a log file.\"\"\"\n    with log_path.open(\"w\") as fh:\n        for line in iter(proc.stdout.readline, b\"\"):  # type: ignore\n            decoded = line.decode(errors=\"replace\")\n            fh.write(decoded)\n            fh.flush()\n            print(decoded, end=\"\")\n    proc.stdout.close()\n\n\n# -----------------------------------------------------------------------------\n# GPU scheduling logic\n# -----------------------------------------------------------------------------\n\ndef get_available_gpus() -\u003e List[int]:\n    return list(range(torch.cuda.device_count()))\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n\n    # ---------------------------------------------------------------------\n    # Config path resolution\n    # ---------------------------------------------------------------------\n    root_dir = Path(__file__).resolve().parent.parent\n    if args.smoke_test:\n        cfg_path = root_dir / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_path = root_dir / \"config\" / \"full_experiment.yaml\"\n\n    with open(cfg_path, \"r\") as fh:\n        cfg = yaml.safe_load(fh)\n\n    experiments = cfg.get(\"experiments\", [])\n    if not experiments:\n        print(\"No experiments defined in config.\")\n        sys.exit(1)\n\n    results_root = Path(args.results_dir).resolve()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    available_gpus = get_available_gpus()\n    if not available_gpus:\n        print(\"No GPU detected \u2013 running on CPU sequentially.\")\n        available_gpus = [-1]  # sentinel for CPU\n\n    gpu_queue = queue.Queue()\n    for gid in available_gpus:\n        gpu_queue.put(gid)\n\n    running_procs = []  # list[(proc, gpu_id)]\n\n    def launch_experiment(exp_cfg: Dict):\n        run_id = exp_cfg[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Write experiment-specific config so the train subprocess can read it.\n        exp_cfg_path = run_dir / \"config.yaml\"\n        with open(exp_cfg_path, \"w\") as fh:\n            yaml.safe_dump(exp_cfg, fh)\n\n        gpu_id = gpu_queue.get()  # blocks until a GPU becomes free\n        env = os.environ.copy()\n        if gpu_id \u003e= 0:\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n        else:\n            env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(exp_cfg_path), \"--results-dir\", str(run_dir)]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env, bufsize=1)\n\n        # Tee stdout/stderr\n        log_path = run_dir / \"stdout.log\"\n        t = threading.Thread(target=_stream_process_output, args=(proc, log_path))\n        t.daemon = True\n        t.start()\n\n        running_procs.append((proc, gpu_id))\n\n    # Launch all experiments (respecting GPU availability)\n    launch_idx = 0\n    while launch_idx \u003c len(experiments) or running_procs:\n        # Launch new ones if GPUs free\n        while launch_idx \u003c len(experiments) and not gpu_queue.empty():\n            exp_cfg = experiments[launch_idx]\n            launch_experiment(exp_cfg)\n            launch_idx += 1\n        # Check running processes\n        time.sleep(5)\n        for proc, gid in running_procs[:]:  # copy to allow removal\n            if proc.poll() is not None:  # finished\n                running_procs.remove((proc, gid))\n                gpu_queue.put(gid)\n\n    # ---------------------------------------------------------------------\n    # All experiments finished \u2013 run evaluation\n    # ---------------------------------------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nModel zoo shared by all experiments.  Fully functional baseline / dummy models\nare provided so that the common core works end-to-end; placeholders mark the\nspots that will be extended with domain-specific architectures later.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n\n# -----------------------------------------------------------------------------\n# Dummy model \u2013 small MLP for synthetic dataset \u0026 smoke tests.\n# -----------------------------------------------------------------------------\n\nclass DummyMLP(nn.Module):\n    def __init__(self, input_size: int = 32, hidden_size: int = 64, num_classes: int = 10):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        in_dim = input_size * input_size\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        return self.net(x)\n\n\n# -----------------------------------------------------------------------------\n# PLACEHOLDER model registry \u2013 to be populated with real architectures.\n# -----------------------------------------------------------------------------\n\n_MODEL_REGISTRY = {\n    \"DUMMY\": DummyMLP,\n    # PLACEHOLDER: Add specific model classes here, e.g. \"RESNET20\": ResNet20,\n}\n\n\n# -----------------------------------------------------------------------------\n# Public helpers\n# -----------------------------------------------------------------------------\n\ndef get_model(cfg: Dict) -\u003e nn.Module:\n    \"\"\"Factory that returns an instantiated model based on cfg[\u0027model\u0027][\u0027name\u0027]\"\"\"\n    model_cfg = cfg[\"model\"]\n    name = model_cfg[\"name\"].upper()\n    if name not in _MODEL_REGISTRY:\n        raise NotImplementedError(\n            f\"Model \u0027{name}\u0027 not implemented. Extend _MODEL_REGISTRY in src/model.py.\"\n        )\n\n    if name == \"DUMMY\":\n        return _MODEL_REGISTRY[name](\n            input_size=int(cfg[\"dataset\"].get(\"input_size\", 32)),\n            hidden_size=int(model_cfg.get(\"hidden_size\", 64)),\n            num_classes=int(cfg[\"dataset\"].get(\"num_classes\", 10)),\n        )\n    else:\n        # PLACEHOLDER: Other models may consume extra kwargs from model_cfg\n        return _MODEL_REGISTRY[name](**model_cfg)\n\n\ndef save_model(model: nn.Module, path):\n    path = str(path)\n    torch.save(model.state_dict(), path)\n\n\ndef load_model(path, cfg, device=None):\n    device = device or (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    model = get_model(cfg)\n    state = torch.load(str(path), map_location=device)\n    model.load_state_dict(state)\n    model.to(device)\n    return model\n", "preprocess_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nCommon data-loading and preprocessing utilities shared by every experiment.\nSpecific domains/datasets will replace the placeholders marked clearly below.\nFor generic smoke-tests and fallback behaviour, a light-weight random dataset\nis provided so that the core pipeline runs end-to-end even without a real\ndataset.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n\n# -----------------------------------------------------------------------------\n# Dummy dataset \u2013 allows CI smoke tests to pass quickly on any machine.\n# -----------------------------------------------------------------------------\n\nclass DummyClassificationDataset(Dataset):\n    \"\"\"Synthetic classification dataset (images flattened to vectors).\"\"\"\n\n    def __init__(self, num_samples: int = 1024, input_size: int = 32, num_classes: int = 10):\n        self.num_samples = num_samples\n        self.input_size = input_size\n        self.num_classes = num_classes\n        self.x = torch.randn(num_samples, input_size * input_size)\n        self.y = torch.randint(0, num_classes, (num_samples,))\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# -----------------------------------------------------------------------------\n# PLACEHOLDER hooks \u2013 will be replaced with real dataset loaders.\n# -----------------------------------------------------------------------------\n\ndef _load_specific_dataset(cfg: dict) -\u003e Dataset:\n    \"\"\"PLACEHOLDER: Will be replaced with specific dataset loading logic.\n    The function must return a torch.utils.data.Dataset object ready for\n    consumption by DataLoader.\n    \"\"\"\n    raise NotImplementedError(\n        \"Dataset \u0027{}\u0027 not implemented. Replace _load_specific_dataset() in preprocess.py or \"\n        \"specify dataset \u0027DUMMY\u0027 for smoke tests.\".format(cfg[\"dataset\"][\"name\"])\n    )\n\n\n# -----------------------------------------------------------------------------\n# Public API \u2013 consumed by train.py.\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg: dict, val_split: float = 0.2) -\u003e Tuple[DataLoader, DataLoader, Optional[DataLoader]]:\n    \"\"\"Return (train, val, test) dataloaders in a device-agnostic way.\"\"\"\n\n    batch_size = int(cfg.get(\"train\", {}).get(\"batch_size\", 32))\n    dataset_cfg = cfg[\"dataset\"]\n    name = dataset_cfg[\"name\"].upper()\n\n    if name == \"DUMMY\":\n        dataset = DummyClassificationDataset(\n            num_samples=int(dataset_cfg.get(\"num_samples\", 1024)),\n            input_size=int(dataset_cfg.get(\"input_size\", 32)),\n            num_classes=int(dataset_cfg.get(\"num_classes\", 10)),\n        )\n    else:\n        dataset = _load_specific_dataset(cfg)  # \u2190 placeholder call\n\n    # ---------------------------------------------------------------------\n    # Split into train / val ( / test )\n    # ---------------------------------------------------------------------\n    val_size = int(len(dataset) * val_split)\n    test_size = int(len(dataset) * val_split) if cfg.get(\"create_test_split\", True) else 0\n    train_size = len(dataset) - val_size - test_size\n    splits = random_split(dataset, [train_size, val_size, test_size]) if test_size else random_split(dataset, [train_size, val_size])\n    dl_train = DataLoader(splits[0], batch_size=batch_size, shuffle=True)\n    dl_val = DataLoader(splits[1], batch_size=batch_size, shuffle=False)\n    dl_test = DataLoader(splits[2], batch_size=batch_size, shuffle=False) if test_size else None\n\n    return dl_train, dl_val, dl_test", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"common-core-ohgw\"\nversion = \"0.1.0\"\ndescription = \"Common experimental foundation for OHGW studies\"\nauthors = [\n    { name = \"Research Automation\", email = \"no-reply@example.com\" }\n]\nrequires-python = \"\u003e=3.9\"\nlicense = {text = \"MIT\"}\ndependencies = [\n    \"torch\u003e=2.0\",\n    \"torchvision\u003e=0.15\",\n    \"pyyaml\u003e=6.0\",\n    \"matplotlib\u003e=3.8\",\n    \"seaborn\u003e=0.13\",\n    \"pandas\u003e=2.0\",\n    \"tqdm\u003e=4.66\",\n    \"psutil\u003e=5.9\"\n]\n\n[project.scripts]\n# Optional entrypoints (not strictly required by the orchestrator)\n", "smoke_test_yaml": "# Lightweight configuration to verify the whole stack in \u003c30 seconds.\n# Uses a synthetic dataset and tiny model so CI can run on CPU if needed.\n\nexperiments:\n  - run_id: dummy_baseline\n    seed: 0\n    dataset:\n      name: DUMMY\n      input_size: 32\n      num_classes: 2\n      num_samples: 256\n    model:\n      name: DUMMY\n      hidden_size: 64\n    train:\n      epochs: 2\n      batch_size: 32\n      lr: 0.01\n    optimizer:\n      name: Adam\n      lr: 0.01\n", "train_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nTrain a single experimental variation.\nThis script is launched as a subprocess by src.main.  It must:\n1.  Load the experiment-specific YAML file passed via --config.\n2.  Build dataloaders (via src.preprocess), model (via src.model) and training infrastructure.\n3.  Run the full training loop, collecting epoch-wise metrics.\n4.  Persist artefacts (model checkpoint, raw metrics, figures) in --results-dir.\n5.  Print \u2013 to **stdout** \u2013 a human-readable experiment description followed by\n    JSON-formatted numeric results (one line!) so the orchestrator can parse it.\n\nCore training logic is COMPLETE.  Only the dataset/model creation depend on\nplaceholders handled in src.preprocess \u0026 src.model.\n\"\"\"\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports ---------------------------------------------------------------\nfrom src import preprocess as pp\nfrom src import model as mdl\n\n# -----------------------------------------------------------------------------\n# Helper utils\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    if seed is None:\n        return\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# -----------------------------------------------------------------------------\n# Main training routine\n# -----------------------------------------------------------------------------\n\ndef train_one_epoch(model: nn.Module, dataloader: DataLoader, criterion, optimizer, device):\n    model.train()\n    epoch_loss = 0.0\n    for batch in dataloader:\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * inputs.size(0)\n    return epoch_loss / len(dataloader.dataset)\n\n\ndef evaluate(model: nn.Module, dataloader: DataLoader, criterion, device):\n    model.eval()\n    loss = 0.0\n    correct = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            inputs, targets = batch\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            outputs = model(inputs)\n            batch_loss = criterion(outputs, targets)\n            loss += batch_loss.item() * inputs.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == targets).sum().item()\n    loss /= len(dataloader.dataset)\n    accuracy = correct / len(dataloader.dataset)\n    return loss, accuracy\n\n\n# -----------------------------------------------------------------------------\n# Plotting helpers\n# -----------------------------------------------------------------------------\n\ndef plot_curves(metrics: dict, run_id: str, out_dir: Path):\n    # Training loss -----------------------------------------------------------\n    plt.figure(figsize=(6, 4))\n    epochs = np.arange(1, len(metrics[\"train_loss\"]) + 1)\n    plt.plot(epochs, metrics[\"train_loss\"], label=\"Train Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss \u2013 {run_id}\")\n    plt.legend()\n    # annotate final value\n    final_val = metrics[\"train_loss\"][-1]\n    plt.annotate(f\"{final_val:.4f}\", xy=(epochs[-1], final_val), xytext=(epochs[-1], final_val))\n    fname = out_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n    # Validation accuracy -----------------------------------------------------\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, metrics[\"val_accuracy\"], label=\"Val Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(f\"Validation Accuracy \u2013 {run_id}\")\n    plt.legend()\n    final_acc = metrics[\"val_accuracy\"][-1]\n    plt.annotate(f\"{final_acc:.4f}\", xy=(epochs[-1], final_acc), xytext=(epochs[-1], final_acc))\n    fname = out_dir / f\"accuracy_{run_id}.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Train a single experimental variation\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store all outputs for this run.\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Entrypoint\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------------------------------------------------------------\n    # Load config\n    # ---------------------------------------------------------------------\n    import yaml  # local import to avoid YAML requirement when not needed\n\n    with open(args.config, \"r\") as fh:\n        cfg = yaml.safe_load(fh)\n\n    run_id = cfg.get(\"run_id\", results_dir.name)\n    seed = cfg.get(\"seed\", 0)\n    set_seed(seed)\n\n    # ---------------------------------------------------------------------\n    # Device assignment \u2013 user controls visible GPU via CUDA_VISIBLE_DEVICES\n    # ---------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # Data\n    # ---------------------------------------------------------------------\n    dl_train, dl_val, dl_test = pp.get_dataloaders(cfg)\n\n    # ---------------------------------------------------------------------\n    # Model \u0026 optimisation\n    # ---------------------------------------------------------------------\n    model = mdl.get_model(cfg).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optim_cfg = cfg.get(\"optimizer\", {\"name\": \"Adam\", \"lr\": cfg.get(\"train\", {}).get(\"lr\", 1e-3)})\n    optimizer = torch.optim.Adam(model.parameters(), lr=optim_cfg[\"lr\"])\n\n    num_epochs = int(cfg.get(\"train\", {}).get(\"epochs\", 1))\n\n    # Containers for metrics\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_accuracy\": [],\n    }\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    start_time = time.time()\n    for epoch in range(1, num_epochs + 1):\n        ep_train_loss = train_one_epoch(model, dl_train, criterion, optimizer, device)\n        ep_val_loss, ep_val_acc = evaluate(model, dl_val, criterion, device)\n\n        history[\"train_loss\"].append(ep_train_loss)\n        history[\"val_loss\"].append(ep_val_loss)\n        history[\"val_accuracy\"].append(ep_val_acc)\n\n        # Minimal logging to console for progress visibility\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch,\n                    \"train_loss\": ep_train_loss,\n                    \"val_loss\": ep_val_loss,\n                    \"val_accuracy\": ep_val_acc,\n                }\n            )\n        )\n        sys.stdout.flush()\n\n    end_time = time.time()\n\n    # ---------------------------------------------------------------------\n    # Final evaluation on test set (if provided)\n    # ---------------------------------------------------------------------\n    test_loss, test_acc = (None, None)\n    if dl_test is not None:\n        test_loss, test_acc = evaluate(model, dl_test, criterion, device)\n\n    # ---------------------------------------------------------------------\n    # Save artefacts\n    # ---------------------------------------------------------------------\n    ckpt_path = results_dir / \"model.pth\"\n    mdl.save_model(model, ckpt_path)\n\n    # Store raw metrics -------------------------------------------------------\n    results = {\n        \"run_id\": run_id,\n        \"seed\": seed,\n        \"num_epochs\": num_epochs,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"final_val_accuracy\": history[\"val_accuracy\"][-1],\n        \"best_val_accuracy\": max(history[\"val_accuracy\"]),\n        \"epoch_of_best_accuracy\": int(np.argmax(history[\"val_accuracy\"]) + 1),\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc,\n        \"training_time_seconds\": end_time - start_time,\n        **history,\n    }\n    with open(results_dir / \"results.json\", \"w\") as fh:\n        json.dump(results, fh, indent=2)\n\n    # Figures ---------------------------------------------------------------\n    plot_curves(history, run_id, results_dir)\n\n    # ---------------------------------------------------------------------\n    # Experiment description (human-readable) then machine-readable metrics\n    # ---------------------------------------------------------------------\n    description_lines = [\n        f\"Run ID            : {run_id}\",\n        f\"Dataset           : {cfg[\u0027dataset\u0027][\u0027name\u0027]}\",\n        f\"Model             : {cfg[\u0027model\u0027][\u0027name\u0027]}\",\n        f\"Epochs            : {num_epochs}\",\n        f\"Batch size        : {cfg.get(\u0027train\u0027, {}).get(\u0027batch_size\u0027, \u0027N/A\u0027)}\",\n        f\"Optimizer / lr    : Adam / {optim_cfg[\u0027lr\u0027]}\",\n        f\"Seed              : {seed}\",\n        f\"Device            : {device}\",\n    ]\n    print(\"\\nEXPERIMENT DESCRIPTION:\")\n    print(\"\\n\".join(description_lines))\n\n    print(\"\\nEXPERIMENTAL RESULTS:\")\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    main()"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Requirement 4 violated – src.main merges stderr into stdout (Popen(..., stderr=STDOUT)) and writes only one log file (stdout.log). Spec mandates two files, stdout.log & stderr.log, each capturing respective streams. All other core-logic, placeholder usage, 8-file structure, training/eval, model saving, visualization etc. are correct, but missing separate stderr redirection prevents full compliance."
}
