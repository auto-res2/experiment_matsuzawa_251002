
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation
   - main.py implements 1 GPU per run variation allocation, queueing runs sequentially when GPUs are insufficient
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nsns.set(style=\"whitegrid\")\n\n\nFIGURE_NAMING = {\n    \"training_loss\": \"training_loss.pdf\",\n    \"accuracy\": \"accuracy.pdf\",\n}\n\n\ndef load_results(results_dir: Path) -\u003e pd.DataFrame:\n    \"\"\"Load metrics.jsonl from all experiment variations.\"\"\"\n    frames: List[pd.DataFrame] = []\n    for run_dir in results_dir.iterdir():\n        metrics_file = run_dir / \"metrics.jsonl\"\n        if metrics_file.exists():\n            df = pd.read_json(metrics_file, lines=True)\n            frames.append(df)\n    if not frames:\n        raise RuntimeError(\"No result files found in results directory.\")\n    return pd.concat(frames, ignore_index=True)\n\n\ndef plot_training_loss(df: pd.DataFrame, save_path: Path):\n    plt.figure(figsize=(8, 5))\n    sns.lineplot(data=df, x=\"epoch\", y=\"train_loss\", hue=\"run_id\")\n    for run_id, sub in df.groupby(\"run_id\"):\n        end_point = sub.sort_values(\"epoch\").iloc[-1]\n        plt.text(end_point[\"epoch\"], end_point[\"train_loss\"], f\"{end_point[\u0027train_loss\u0027]:.3f}\")\n    plt.title(\"Training Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(title=\"Run\")\n    plt.tight_layout()\n    plt.savefig(str(save_path / FIGURE_NAMING[\"training_loss\"]), bbox_inches=\"tight\", format=\"pdf\")\n    plt.close()\n\n\ndef plot_accuracy(df: pd.DataFrame, save_path: Path):\n    plt.figure(figsize=(8, 5))\n    sns.lineplot(data=df, x=\"epoch\", y=\"val_acc\", hue=\"run_id\")\n    for run_id, sub in df.groupby(\"run_id\"):\n        end_point = sub.sort_values(\"epoch\").iloc[-1]\n        plt.text(end_point[\"epoch\"], end_point[\"val_acc\"], f\"{end_point[\u0027val_acc\u0027]:.3f}\")\n    plt.title(\"Validation Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(title=\"Run\")\n    plt.tight_layout()\n    plt.savefig(str(save_path / FIGURE_NAMING[\"accuracy\"]), bbox_inches=\"tight\", format=\"pdf\")\n    plt.close()\n\n\ndef aggregate_final_metrics(results_dir: Path) -\u003e Dict[str, Dict[str, float]]:\n    aggregated = {}\n    for run_dir in results_dir.iterdir():\n        results_file = run_dir / \"results.json\"\n        if results_file.exists():\n            with open(results_file) as fp:\n                res = json.load(fp)\n            aggregated[run_dir.name] = res\n    return aggregated\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Aggregate and visualize results.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    print(\"Generating figures and aggregated metrics \u2026\")\n    df = load_results(results_dir)\n    plot_training_loss(df, results_dir)\n    plot_accuracy(df, results_dir)\n\n    aggregated = aggregate_final_metrics(results_dir)\n\n    # Print comparison results to stdout in JSON format\n    print(json.dumps({\"comparison\": aggregated}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# Full experiment template. Placeholders will be replaced in next stage.\n\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER  # Will be filled with actual dataset, e.g., cifar10\n      root: DATA_PATH_PLACEHOLDER\n      num_classes: NUM_CLASSES_PLACEHOLDER\n      has_test_split: true\n    model:\n      name: MODEL_PLACEHOLDER  # e.g., resnet20\n      depth: MODEL_DEPTH_PLACEHOLDER\n    optimizer:\n      name: OPTIMIZER_PLACEHOLDER  # e.g., sgd\n      lr: LR_PLACEHOLDER\n      momentum: MOMENTUM_PLACEHOLDER\n      weight_decay: WD_PLACEHOLDER\n    training:\n      epochs: EPOCHS_PLACEHOLDER\n      batch_size: BATCH_SIZE_PLACEHOLDER\n      num_workers: 8\n    lr_scheduler:\n      step_size: 60\n      gamma: 0.2\n  # Additional experiments can be added below following the same schema.\n  # {", "main_py": "import argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nimport queue\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport yaml\nimport torch\n\n\n# ------------------------- Helper Utilities ------------------------------------- #\n\ndef discover_gpus() -\u003e List[int]:\n    \"\"\"Return list of available GPU indices.\"\"\"\n    num = torch.cuda.device_count()\n    return list(range(num))\n\n\ndef tee_stream(src, dst_file, prefix: str = \"\"):\n    \"\"\"Forward stream lines both to stdout and a log file.\"\"\"\n    for line in iter(src.readline, b\"\"):\n        decoded = line.decode(\"utf-8\", errors=\"replace\")\n        sys.stdout.write(prefix + decoded)\n        dst_file.write(decoded)\n        dst_file.flush()\n    src.close()\n\n\n# ------------------------- Orchestrator ----------------------------------------- #\nclass ExperimentRunner:\n    def __init__(self, config_path: Path, results_dir: Path):\n        self.config_path = config_path\n        self.results_dir = results_dir\n        self.gpus = discover_gpus()\n        if not self.gpus:\n            raise RuntimeError(\"No GPUs detected; at least one GPU is required.\")\n\n        with open(config_path, \"r\") as fp:\n            self.cfg = yaml.safe_load(fp)\n        self.experiments: List[Dict] = self.cfg[\"experiments\"]\n\n    def run_all(self):\n        pending = self.experiments.copy()\n        running: Dict[subprocess.Popen, int] = {}\n        gpu_queue = queue.Queue()\n        for g in self.gpus:\n            gpu_queue.put(g)\n\n        def launch(experiment: Dict, gpu_idx: int):\n            run_id = experiment[\"run_id\"]\n            run_dir = self.results_dir / run_id\n            run_dir.mkdir(parents=True, exist_ok=True)\n            # dump individual experiment config\n            exp_cfg_path = run_dir / \"config.yaml\"\n            with open(exp_cfg_path, \"w\") as fp:\n                yaml.safe_dump(experiment, fp)\n\n            env = os.environ.copy()\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_idx)\n\n            stdout_file = open(run_dir / \"stdout.log\", \"w\", buffering=1)\n            stderr_file = open(run_dir / \"stderr.log\", \"w\", buffering=1)\n\n            process = subprocess.Popen([\n                sys.executable, \"-m\", \"src.train\",\n                \"--config\", str(exp_cfg_path),\n                \"--results-dir\", str(self.results_dir)\n            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, bufsize=1)\n\n            # Launch tee threads\n            threading.Thread(target=tee_stream, args=(process.stdout, stdout_file, f\"[{run_id}] \"), daemon=True).start()\n            threading.Thread(target=tee_stream, args=(process.stderr, stderr_file, f\"[{run_id}][ERR] \"), daemon=True).start()\n            running[process] = gpu_idx\n\n        # Main scheduling loop\n        while pending or running:\n            # Launch experiments if GPUs available\n            while not gpu_queue.empty() and pending:\n                gpu_idx = gpu_queue.get()\n                experiment = pending.pop(0)\n                launch(experiment, gpu_idx)\n            # Poll running processes\n            finished = []\n            for proc in list(running.keys()):\n                ret = proc.poll()\n                if ret is not None:\n                    gpu_idx = running.pop(proc)\n                    if ret != 0:\n                        print(f\"Run with GPU {gpu_idx} exited with code {ret}\")\n                    gpu_queue.put(gpu_idx)\n                    finished.append(proc)\n            if not pending and not running:\n                break\n        print(\"All experiment variations completed.\")\n\n        # Trigger evaluation\n        subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(self.results_dir)])\n\n\n# ------------------------- CLI --------------------------------------------------- #\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for OHGW experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Where to save figures, logs and metrics.\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    config_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    runner = ExperimentRunner(config_path, results_dir)\n    runner.run_all()\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"Model definitions and factory utilities.\n\nContains baseline, proposed and ablation architectures. Only dataset- or\nexperiment-specific details (like number of classes) are passed in as\nconstructor arguments so that the common core remains unchanged across\nexperimental variations.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# ---------------------------- Baseline Model ------------------------------------ #\nclass _SimpleCNN(nn.Module):\n    \"\"\"A minimal CNN that works for most 32\u00d732 image classification tasks.\"\"\"\n\n    def __init__(self, num_classes: int = 10, input_channels: int = 3):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 16\u00d716\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 8\u00d78\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.flatten(1)\n        return self.classifier(x)\n\n\n# ---------------------------- Placeholder Models -------------------------------- #\nclass MODEL_PLACEHOLDER(nn.Module):\n    \"\"\"PLACEHOLDER: Will be replaced with specific model architecture.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        raise NotImplementedError(\"Model placeholder \u2013 to be replaced in derived experiments.\")\n\n\n# ---------------------------- Model Factory ------------------------------------- #\nMODEL_REGISTRY = {\n    \"simple_cnn\": _SimpleCNN,\n    # PLACEHOLDER: register additional models here.\n}\n\n\ndef get_model(model_cfg: dict | str, *, num_classes: int, input_shape: tuple[int, ...]):\n    \"\"\"Instantiate a model by name or full config.\n\n    Parameters\n    ----------\n    model_cfg: dict | str\n        Either a simple string name (e.g., \"simple_cnn\") or a dictionary\n        with keys {\"name\": \u003cname\u003e, **kwargs}\n    num_classes: int\n        Number of output classes for classification.\n    input_shape: tuple[int, ...]\n        Input tensor shape so that certain models can infer required\n        layer dimensions.\n    \"\"\"\n    if isinstance(model_cfg, str):\n        model_name = model_cfg\n        kwargs = {}\n    else:\n        model_name = model_cfg.get(\"name\")\n        kwargs = {k: v for k, v in model_cfg.items() if k != \"name\"}\n\n    model_name = model_name.lower()\n    if model_name not in MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model \u0027{model_name}\u0027. Available: {list(MODEL_REGISTRY.keys())}\")\n\n    # Automatically inject num_classes / input_channels if not supplied\n    kwargs.setdefault(\"num_classes\", num_classes)\n    kwargs.setdefault(\"input_channels\", input_shape[0])\n\n    return MODEL_REGISTRY[model_name](**kwargs)", "preprocess_py": "\"\"\"Common data preprocessing utilities with dataset placeholders.\n\nThis module centralises all dataset-related handling such that swapping\nfrom one dataset to the next can be done by *only* editing the dataset\nconfiguration dictionary, without touching any other training or\nevaluation code.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom pathlib import Path\nfrom typing import Tuple, Dict, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\n\n\n# --------------------------- Synthetic Placeholder ------------------------------ #\nclass _SyntheticClassificationDataset(Dataset):\n    \"\"\"Very small synthetic dataset for smoke testing.\n\n    Generates random images and random labels on the fly; deterministic\n    given the global PyTorch RNG state. Assumes image classification\n    with C classes.\n    \"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, int, int], num_classes: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.input_shape = input_shape  # (C, H, W)\n        self.num_classes = num_classes\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        img = torch.randn(*self.input_shape)\n        label = torch.randint(0, self.num_classes, (1,)).item()\n        return img, label\n\n\n# --------------------------- Dataset Factory ------------------------------------ #\n\ndef _build_transforms(training: bool, dataset_cfg: Dict[str, Any]):\n    \"\"\"Return torchvision Transform pipeline depending on training/val.\"\"\"\n    # PLACEHOLDER: Replace or augment with dataset-specific transforms\n    if training:\n        return T.Compose([\n            T.RandomHorizontalFlip(),\n            T.Normalize(mean=dataset_cfg.get(\"mean\", (0.5, 0.5, 0.5)),\n                        std=dataset_cfg.get(\"std\", (0.5, 0.5, 0.5)))\n        ])\n    else:\n        return T.Compose([\n            T.Normalize(mean=dataset_cfg.get(\"mean\", (0.5, 0.5, 0.5)),\n                        std=dataset_cfg.get(\"std\", (0.5, 0.5, 0.5)))\n        ])\n\n\ndef _load_real_dataset(dataset_cfg: Dict[str, Any], split: str):\n    \"\"\"PLACEHOLDER: Will be replaced with specific dataset loading logic.\"\"\"\n    raise NotImplementedError(\"Real dataset loading not yet implemented in common core foundation.\")\n\n\n# --------------------------- Public API ----------------------------------------- #\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any],\n                    training_cfg: Dict[str, Any],\n                    smoke_test: bool = False,\n                    include_test: bool = False):\n    \"\"\"Return train/val(/test) dataloaders given configs.\n\n    Parameters\n    ----------\n    dataset_cfg: Dict[str, Any]\n        Dataset-specific configuration (root, name, etc.)\n    training_cfg: Dict[str, Any]\n        Training hyperparameters such as batch_size, num_workers.\n    smoke_test: bool\n        If True, use a tiny synthetic dataset regardless of configuration.\n    include_test: bool\n        If True, also returns test_loader; else None.\n    \"\"\"\n    batch_size = training_cfg.get(\"batch_size\", 64)\n    num_workers = training_cfg.get(\"num_workers\", 4)\n\n    if smoke_test:\n        input_shape = (3, 32, 32)\n        num_classes = dataset_cfg.get(\"num_classes\", 10)\n        num_train = training_cfg.get(\"smoke_num_train\", 256)\n        num_val = training_cfg.get(\"smoke_num_val\", 64)\n        train_dataset = _SyntheticClassificationDataset(num_train, input_shape, num_classes)\n        val_dataset = _SyntheticClassificationDataset(num_val, input_shape, num_classes)\n        test_dataset = _SyntheticClassificationDataset(num_val, input_shape, num_classes)\n    else:\n        # Load real datasets. To be replaced in subsequent steps.\n        train_dataset = _load_real_dataset(dataset_cfg, split=\"train\")\n        val_dataset = _load_real_dataset(dataset_cfg, split=\"val\")\n        test_dataset = _load_real_dataset(dataset_cfg, split=\"test\") if include_test else None\n\n        # For real datasets we could inspect a sample to get shape/num_classes\n        sample, _ = train_dataset[0]\n        input_shape = tuple(sample.shape)\n        # naive heuristic to guess num classes\n        label_set = set()\n        for _, lbl in (train_dataset[i] for i in range(min(len(train_dataset), 1024))):\n            label_set.add(int(lbl))\n        num_classes = len(label_set)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = None\n    if include_test and test_dataset is not None:\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, num_classes, input_shape if not include_test else (train_loader, val_loader, num_classes, input_shape, test_loader)", "pyproject_toml": "[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for One-Shot Hyper-Gradient Warm-Starts experiments\"\nrequires-python = \"\u003e=3.10\"\n\ndependencies = [\n    \"torch\u003e=2.0.0\",\n    \"torchvision\u003e=0.15.0\",\n    \"PyYAML\u003e=6.0\",\n    \"matplotlib\u003e=3.7.0\",\n    \"seaborn\u003e=0.12.2\",\n    \"tqdm\u003e=4.65.0\",\n    \"pandas\u003e=2.0.0\",\n    \"scikit-learn\u003e=1.3.0\"\n]\n\n[build-system]\nrequires = [\"setuptools\u003e=63.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"", "smoke_test_yaml": "# Lightweight configuration used by CI smoke test.\n# Only synthetic data is used so the test finishes within seconds.\n\nexperiments:\n  - run_id: smoke_baseline\n    smoke_test: true\n    dataset:\n      name: synthetic_classification  # DATASET_PLACEHOLDER\n      num_classes: 10\n      has_test_split: false\n    model:\n      name: simple_cnn  # MODEL_PLACEHOLDER default\n    optimizer:\n      name: sgd\n      lr: 0.01\n      momentum: 0.9\n      weight_decay: 0.0\n    training:\n      epochs: 2\n      batch_size: 32\n      num_workers: 2", "train_py": "import argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Any, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n# ----------------------------- Utility Functions -------------------------------- #\n\ndef seed_everything(seed: int) -\u003e None:\n    \"\"\"Seed all major RNGs for reproducibility.\"\"\"\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef save_checkpoint(state: Dict[str, Any], checkpoint_dir: Path, is_best: bool = False) -\u003e None:\n    \"\"\"Save model/optimizer/epoch state dictionary.\"\"\"\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    filename = checkpoint_dir / \"model_last.pth\"\n    torch.save(state, filename)\n    if is_best:\n        best_filename = checkpoint_dir / \"model_best.pth\"\n        torch.save(state, best_filename)\n\n\ndef train_one_epoch(model: nn.Module,\n                    dataloader: DataLoader,\n                    criterion: nn.Module,\n                    optimizer: optim.Optimizer,\n                    device: torch.device) -\u003e Tuple[float, float]:\n    \"\"\"Train for exactly one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    running_correct = 0\n    total_samples = 0\n\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        running_correct += torch.sum(preds == targets).item()\n        total_samples += inputs.size(0)\n\n    epoch_loss = running_loss / total_samples\n    epoch_acc = running_correct / total_samples\n    return epoch_loss, epoch_acc\n\n\ndef validate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -\u003e Tuple[float, float]:\n    model.eval()\n    running_loss = 0.0\n    running_correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            running_correct += torch.sum(preds == targets).item()\n            total_samples += inputs.size(0)\n\n    epoch_loss = running_loss / total_samples\n    epoch_acc = running_correct / total_samples\n    return epoch_loss, epoch_acc\n\n\n# ----------------------------- Main Training Script ------------------------------ #\n\ndef run_experiment(config: Dict[str, Any], results_dir: Path, resume: bool = False) -\u003e None:\n    run_id = config[\"run_id\"]\n    seed = config.get(\"seed\", 42)\n    seed_everything(seed)\n\n    # GPU handling: rely on CUDA_VISIBLE_DEVICES set by main.py\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # -------------------------------------------------------------------------\n    # 1. Data\n    # -------------------------------------------------------------------------\n    train_loader, val_loader, num_classes, input_shape = get_dataloaders(config[\"dataset\"],\n                                                                        config[\"training\"],\n                                                                        smoke_test=config.get(\"smoke_test\", False))\n    # -------------------------------------------------------------------------\n    # 2. Model\n    # -------------------------------------------------------------------------\n    model = get_model(config[\"model\"], num_classes=num_classes, input_shape=input_shape)\n    model.to(device)\n\n    # -------------------------------------------------------------------------\n    # 3. Optimizer \u0026 Loss\n    # -------------------------------------------------------------------------\n    criterion = nn.CrossEntropyLoss()\n    optimizer_cfg = config[\"optimizer\"]\n    opt_name = optimizer_cfg.get(\"name\", \"SGD\").lower()\n    lr = optimizer_cfg.get(\"lr\", 0.01)\n    weight_decay = optimizer_cfg.get(\"weight_decay\", 0.0)\n    momentum = optimizer_cfg.get(\"momentum\", 0.9)\n\n    if opt_name == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif opt_name == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {opt_name}\")\n\n    # Scheduler placeholder \u2013 can be disabled if not provided\n    scheduler = None\n    if \"lr_scheduler\" in config:\n        sched_cfg = config[\"lr_scheduler\"]\n        step_size = sched_cfg.get(\"step_size\", 30)\n        gamma = sched_cfg.get(\"gamma\", 0.1)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # -------------------------------------------------------------------------\n    # 4. Checkpoint (resume if asked)\n    # -------------------------------------------------------------------------\n    checkpoint_dir = results_dir / run_id / \"checkpoints\"\n    start_epoch = 0\n    best_val_acc = 0.\n    if resume:\n        ckpt_file = checkpoint_dir / \"model_last.pth\"\n        if ckpt_file.exists():\n            state = torch.load(ckpt_file, map_location=device)\n            model.load_state_dict(state[\"state_dict\"])\n            optimizer.load_state_dict(state[\"optimizer\"])\n            if scheduler and \"scheduler\" in state:\n                scheduler.load_state_dict(state[\"scheduler\"])\n            start_epoch = state[\"epoch\"] + 1\n            best_val_acc = state.get(\"best_val_acc\", 0.)\n            print(f\"Resumed from epoch {start_epoch}\")\n\n    # -------------------------------------------------------------------------\n    # 5. Training Loop\n    # -------------------------------------------------------------------------\n    n_epochs = config[\"training\"].get(\"epochs\", 100)\n    log_every = max(1, n_epochs // 10)\n\n    # Prepare metrics storage\n    metrics_path = results_dir / run_id / \"metrics.jsonl\"\n    metrics_path.parent.mkdir(parents=True, exist_ok=True)\n    metrics_file = metrics_path.open(\"a\" if resume else \"w\", buffering=1)\n\n    for epoch in range(start_epoch, n_epochs):\n        epoch_start = time.time()\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        if scheduler is not None:\n            scheduler.step()\n\n        epoch_time = time.time() - epoch_start\n        log_dict = {\n            \"run_id\": run_id,\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"epoch_time\": epoch_time,\n        }\n        print(json.dumps(log_dict))\n        metrics_file.write(json.dumps(log_dict) + \"\\n\")\n\n        # Checkpoint handling\n        is_best = val_acc \u003e best_val_acc\n        best_val_acc = max(best_val_acc, val_acc)\n        save_checkpoint({\n            \"epoch\": epoch,\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"scheduler\": scheduler.state_dict() if scheduler else None,\n            \"best_val_acc\": best_val_acc,\n            \"config\": config,\n        }, checkpoint_dir, is_best=is_best)\n\n    metrics_file.close()\n\n    # -------------------------------------------------------------------------\n    # 6. Final test evaluation if test split given\n    # -------------------------------------------------------------------------\n    test_metrics = {}\n    if config[\"dataset\"].get(\"has_test_split\", False):\n        _, _, _, test_loader = get_dataloaders(config[\"dataset\"],\n                                               config[\"training\"],\n                                               smoke_test=config.get(\"smoke_test\", False),\n                                               include_test=True)\n        test_loss, test_acc = validate(model, test_loader, criterion, device)\n        test_metrics = {\"test_loss\": test_loss, \"test_acc\": test_acc}\n\n    # -------------------------------------------------------------------------\n    # 7. Save final aggregated results\n    # -------------------------------------------------------------------------\n    final_results = {\n        \"run_id\": run_id,\n        \"best_val_acc\": best_val_acc,\n        **test_metrics,\n    }\n    results_file = results_dir / run_id / \"results.json\"\n    with results_file.open(\"w\") as fp:\n        json.dump(final_results, fp, indent=2)\n\n\n# --------------------------------------------------------------------------------#\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Base directory to write results into.\")\n    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from last checkpoint if available\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    import yaml  # Local import to ensure dependency only when training\n\n    with open(args.config, \"r\") as fp:\n        config = yaml.safe_load(fp)\n\n    run_experiment(config, Path(args.results_dir), resume=args.resume)\n\n\nif __name__ == \"__main__\":\n    main()"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "The foundation is very close but fails the “complete core-logic implementation” criterion:\n\n• In train.py the call for the optional test evaluation\n\n    _, _, _, test_loader = get_dataloaders(..., include_test=True)\n\nexpects 4 return values, while preprocess.get_dataloaders() returns 5 when include_test=True (train_loader, val_loader, num_classes, input_shape, test_loader).  This causes a ValueError as soon as a config sets `has_test_split: true`, so the generic training / evaluation path is broken.\n\nAll other criteria (file count, CLI orchestration, placeholder usage, GPU scheduling, model saving/loading, figure generation, PyTorch-only implementation, etc.) are satisfied, but the above bug means the core training-evaluation loop is not yet fully functional for future experiments that include a test split."
}
