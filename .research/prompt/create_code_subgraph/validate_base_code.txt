
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates *all* run variations residing in \u003cresults_dir\u003e/\u003crun_id\u003e/results.json,\ncomputes comparison statistics and produces publication-quality figures (.pdf).\n\nThe script prints the aggregated table as JSON to stdout for structured logging.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nFIGURE_DPI = 300\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    return parser.parse_args()\n\n\n# ----------------------- Helper: Figure utilities -------------------------- #\n\ndef annotate_bars(ax):\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\", (p.get_x() + p.get_width() / 2.0, height), ha=\"center\", va=\"bottom\")\n\n\n# -------------------------------- Main ------------------------------------- #\n\ndef main() -\u003e None:\n    args = parse_args()\n    root = Path(args.results_dir).expanduser().resolve()\n\n    records: List[Dict] = []\n    for run_dir in root.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as fp:\n                rec = json.load(fp)\n            records.append(rec)\n\n    if not records:\n        raise RuntimeError(f\"No results.json found in {root}\")\n\n    df = pd.DataFrame([\n        {\n            \"run_id\": r[\"run_id\"],\n            \"test_acc\": r[\"test_acc\"],\n            \"best_val_acc\": r[\"best_val_acc\"],\n        }\n        for r in records\n    ])\n\n    # --------------------------- Bar plot (test_acc) ---------------------- #\n    sns.set(style=\"whitegrid\")\n    fig, ax = plt.subplots(figsize=(6, 4), dpi=FIGURE_DPI)\n    sns.barplot(data=df, x=\"run_id\", y=\"test_acc\", ax=ax, palette=\"viridis\")\n    annotate_bars(ax)\n    ax.set_title(\"Test Accuracy by Run Variation\")\n    ax.set_xlabel(\"Run Variation\")\n    ax.set_ylabel(\"Test Accuracy\")\n    plt.tight_layout()\n    fig_path = root / \"accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n\n    # ----------------------- Training loss curves per run ------------------ #\n    fig2, ax2 = plt.subplots(figsize=(6, 4), dpi=FIGURE_DPI)\n    for rec in records:\n        hist = rec[\"history\"]\n        ax2.plot(hist[\"epoch\"], hist[\"train_loss\"], label=rec[\"run_id\"])\n        # annotate final value\n        ax2.annotate(\n            f\"{hist[\u0027train_loss\u0027][-1]:.3f}\",\n            (hist[\"epoch\"][-1], hist[\"train_loss\"][-1]),\n            textcoords=\"offset points\",\n            xytext=(0, 5),\n            ha=\"center\",\n        )\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"Training Loss\")\n    ax2.set_title(\"Training Loss Curves\")\n    ax2.legend()\n    plt.tight_layout()\n    fig2_path = root / \"training_loss.pdf\"\n    fig2.savefig(fig2_path, bbox_inches=\"tight\")\n\n    # ------------------------ Stdout structured JSON ----------------------- #\n    summary = {\n        \"experiment_description\": \"Comparison of run variations \u2013 test accuracy and training dynamics.\",\n        \"data\": df.to_dict(orient=\"records\"),\n        \"figures\": [fig_path.name, fig2_path.name],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# Template for the *real* experiments.  This file will be programmatically\n# enriched with concrete datasets, models and hyper-parameter schedules in the\n# next pipeline step.  Here we only define skeletons so that the common core\n# foundation is executable and parsable.\n\nglobal:\n  seed: 123\n  dataset: DATASET_PLACEHOLDER  # PLACEHOLDER: actual dataset path/name will be injected\n  model:\n    name: MODEL_PLACEHOLDER     # PLACEHOLDER: e.g. resnet20, vit-tiny\n  training:\n    epochs: 50\n    batch_size: 128\n    learning_rate: 0.1\n    momentum: 0.9\n    weight_decay: 0.0005\n    label_smoothing: 0.1\n    ohgw_enable: false\n    hyper_lr: 0.001\n\nrun_variations:\n  - run_id: baseline\n    # vanilla scheduler, no OHGW\n\n  - run_id: ohgw_enabled\n    training:\n      ohgw_enable: true\n\n  - run_id: random_perturbation\n    training:\n      ohgw_enable: false  # Acts as control where only random noise is applied (implemented later)\n\n  - run_id: multi_step_hypergradient\n    training:\n      ohgw_enable: true\n      hyper_lr: 0.0005      # different hyper-gradient step size (ablation)\n", "main_py": "\"\"\"src/main.py\nOrchestrates *all* experiment variations defined in a YAML file.  Usage:\n\n# Smoke test\nuv run python -m src.main --smoke-test --results-dir ./results_smoke\n\n# Full experiment\nuv run python -m src.main --full-experiment --results-dir ./results_full\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport yaml\n\n# -------------------------- Utility functions ------------------------------ #\n\ndef deep_update(base: Dict[str, Any], override: Dict[str, Any]) -\u003e Dict[str, Any]:\n    \"\"\"Recursively update *base* with values from *override* without modifying the\n    original inputs and *return* the merged dictionary.\"\"\"\n    out = dict(base)\n    for k, v in override.items():\n        if isinstance(v, dict) and isinstance(out.get(k), dict):\n            out[k] = deep_update(out[k], v)\n        else:\n            out[k] = v\n    return out\n\n\nclass Tee:\n    \"\"\"Simple tee-like wrapper duplicating stream output to both console and file.\"\"\"\n\n    def __init__(self, file_path: Path, std_stream):\n        self.file = file_path.open(\"w\")\n        self.std_stream = std_stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.file.flush()\n        self.std_stream.write(data)\n        self.std_stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.std_stream.flush()\n\n\n# ----------------------------- CLI parsing --------------------------------- #\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    return parser.parse_args()\n\n\n# --------------------------- Main orchestrator ----------------------------- #\n\ndef main() -\u003e None:\n    args = parse_args()\n\n    cfg_file = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_file, \"r\") as fp:\n        master_cfg = yaml.safe_load(fp)\n\n    base_cfg = master_cfg.get(\"global\", {})\n    run_variations = master_cfg[\"run_variations\"]\n\n    results_root = Path(args.results_dir).expanduser().resolve()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    python_exec = sys.executable\n\n    for run_cfg in run_variations:\n        run_id = run_cfg[\"run_id\"]\n        merged_cfg = deep_update(base_cfg, run_cfg)\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist merged config for reproducibility and for train.py to consume\n        cfg_path = run_dir / \"config.yaml\"\n        with open(cfg_path, \"w\") as fp:\n            yaml.safe_dump(merged_cfg, fp)\n\n        # --------------------- Launch subprocess train.py ------------------- #\n        cmd = [python_exec, \"-m\", \"src.train\", \"--config\", str(cfg_path), \"--run-id\", run_id, \"--results-dir\", str(results_root)]\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) as proc, \\\n                Tee(stdout_log, sys.stdout) as stdout_tee, Tee(stderr_log, sys.stderr) as stderr_tee:\n            # Real-time forward logs while capturing\n            for line in proc.stdout:\n                stdout_tee.write(line)\n            for line in proc.stderr:\n                stderr_tee.write(line)\n            proc.wait()\n            if proc.returncode != 0:\n                raise RuntimeError(f\"Subprocess for run {run_id} exited with code {proc.returncode}\")\n\n    # ----------------------- After all runs: evaluation ------------------- #\n    eval_cmd = [python_exec, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel architecture implementations plus checkpoint utilities.  All experiments\nmust import *only* from this file for maximal consistency.\n\"\"\"\nfrom __future__ import annotations\n\nimport torch\nimport torch.nn as nn\nimport yaml\nfrom pathlib import Path\nfrom typing import Any, Dict\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Reasonably strong yet lightweight baseline for CIFAR-sized images.\"\"\"\n\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n\n# PLACEHOLDER: Additional model classes (e.g. ResNet, Transformer) will be added\n# in the derive-specific stage.\n\n\n# --------------------------- Public Factories ------------------------------ #\n\ndef build_model(cfg: Dict[str, Any], num_classes: int):\n    name = cfg[\"model\"].get(\"name\", \"MODEL_PLACEHOLDER\").lower()\n    if \"placeholder\" in name:\n        return SimpleCNN(num_classes=num_classes)\n    if name == \"simplecnn\":\n        return SimpleCNN(num_classes=num_classes)\n\n    # PLACEHOLDER: Will be replaced by specific models (e.g. resnet20, vit-base)\n    raise NotImplementedError(f\"Model \u0027{name}\u0027 not yet implemented in common foundation.\")\n\n\n# ----------------------- Checkpoint save / load --------------------------- #\n\ndef save_checkpoint(path: Path, model: nn.Module, hyperparams: Dict[str, float], epoch: int):\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save({\n        \"model_state_dict\": model.state_dict(),\n        \"hyperparams\": hyperparams,\n        \"epoch\": epoch,\n    }, path)\n\n\ndef load_checkpoint(path: Path, cfg: Dict[str, Any], num_classes: int):\n    ckpt = torch.load(path, map_location=\"cpu\")\n    model = build_model(cfg, num_classes=num_classes)\n    model.load_state_dict(ckpt[\"model_state_dict\"])\n    return model\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data loading \u0026 preprocessing.  Dataset-specific logic is isolated behind\nCLEAR placeholders so later steps can drop-in real datasets without touching\ncore training logic.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\n# ----------------------------- Placeholders -------------------------------- #\n# PLACEHOLDER: Additional dataset imports (e.g. ImageNet, WikiText) will be\n# inserted automatically in later steps.\n# --------------------------------------------------------------------------- #\n\n\nDEFAULT_NUM_WORKERS = 4\n\n\nclass SyntheticPlaceholderDataset(Dataset):\n    \"\"\"A lightweight synthetic dataset so that the framework is runnable even\n    when the real dataset has not yet been injected.\n    \"\"\"\n\n    def __init__(self, n_samples: int = 1024, num_classes: int = 10):\n        self.x = torch.randn(n_samples, 3, 32, 32)\n        self.y = torch.randint(0, num_classes, (n_samples,))\n        self.num_classes = num_classes\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# ---------------------------- Public API ----------------------------------- #\n\ndef get_dataloaders(cfg) -\u003e Tuple[DataLoader, DataLoader, DataLoader, int]:\n    \"\"\"Returns train, val, test DataLoaders AND num_classes.\"\"\"\n    dataset_name = cfg.get(\"dataset\", \"DATASET_PLACEHOLDER\")\n    batch_size = int(cfg[\"training\"].get(\"batch_size\", 128))\n    num_workers = int(cfg.get(\"num_workers\", DEFAULT_NUM_WORKERS))\n\n    if \"PLACEHOLDER\" in dataset_name:\n        dataset = SyntheticPlaceholderDataset(n_samples=2048)\n        num_classes = dataset.num_classes\n        lengths = [int(0.6 * len(dataset)), int(0.2 * len(dataset))]\n        lengths.append(len(dataset) - sum(lengths))\n        train_set, val_set, test_set = random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n        return train_loader, val_loader, test_loader, num_classes\n\n    if dataset_name.lower() == \"cifar10\":\n        data_root = Path(cfg.get(\"data_root\", \"~/.cache\"))\n        transform = T.Compose([T.ToTensor()])\n        full_train = CIFAR10(root=data_root, train=True, download=True, transform=transform)\n        test_set = CIFAR10(root=data_root, train=False, download=True, transform=transform)\n        num_classes = 10\n        train_len = int(0.9 * len(full_train))\n        val_len = len(full_train) - train_len\n        train_set, val_set = random_split(full_train, [train_len, val_len], generator=torch.Generator().manual_seed(42))\n\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n        return train_loader, val_loader, test_loader, num_classes\n\n    # ----------------- PLACEHOLDER for additional datasets -----------------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic (e.g.\n    # ImageNet, WikiText-103, Atari frames etc.) in later steps.\n    # ----------------------------------------------------------------------- #\n\n    raise NotImplementedError(f\"Dataset \u0027{dataset_name}\u0027 is not supported in the common core foundation.\")\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for One-Shot Hyper-Gradient Warm-Start experiments\"\nrequires-python = \"\u003e=3.9\"\nreadme = \"README.md\"\nauthors = [\n    { name = \"Research Automation\", email = \"automation@example.com\" }\n]\nlicense = { text = \"MIT\" }\n\n[project.dependencies]\ntorch = \"*\"  # GPU package is automatically selected by PyPI / index URL\ntorchvision = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\nnumpy = \"*\"\nscikit-learn = \"*\"\ntqdm = \"*\"\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight configuration that runs in seconds using the synthetic placeholder\ndataset.  Serves as CI smoke test to ensure the full pipeline executes without\nGPU or large-scale data.\n\nglobal:\n  seed: 42\n  dataset: DATASET_PLACEHOLDER  # PLACEHOLDER: will be swapped with real dataset\n  model:\n    name: MODEL_PLACEHOLDER      # PLACEHOLDER: to be replaced\n  training:\n    epochs: 2                   # super short\n    batch_size: 32\n    learning_rate: 0.01\n    momentum: 0.9\n    weight_decay: 0.0005\n    label_smoothing: 0.0\n    ohgw_enable: false\n\nrun_variations:\n  - run_id: smoke_baseline\n    # Inherit global completely\n\n  - run_id: smoke_ohgw\n    training:\n      ohgw_enable: true\n", "train_py": "\"\"\"src/train.py\nRuns a single experiment variation. The script expects:\n    --config \u003cpath\u003e: YAML file with the merged *single-run* configuration\n    --run-id \u003cstr\u003e : Unique identifier for this run (taken from YAML)\n    --results-dir \u003cpath\u003e: Directory in which to save checkpoints, logs and metrics\n\nThe script prints epoch-wise metrics AND the final summary (including the mandatory\n`run_id` field) as JSON to stdout.  A matching results.json file is also written\ninto the run-specific folder so that evaluate.py can pick it up later.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport random\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport yaml\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\n# Local imports \u2013 all common across experiments\nfrom . import preprocess as data_utils  # noqa: E402\nfrom . import model as model_utils       # noqa: E402\n\n\ndef set_seed(seed: int) -\u003e None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n\n\n# -------------------------------- OHGW implementation ----------------------- #\n\ndef label_smoothing_cross_entropy(logits: Tensor, target: Tensor, smoothing: float) -\u003e Tensor:\n    \"\"\"Cross-entropy with label smoothing.\n    Args:\n        logits: (B, C) raw model outputs\n        target: (B,) integer labels\n        smoothing: float in [0, 1)\n    \"\"\"\n    num_classes = logits.size(1)\n    with torch.no_grad():\n        true_dist = torch.zeros_like(logits)\n        true_dist.fill_(smoothing / (num_classes - 1))\n        true_dist.scatter_(1, target.unsqueeze(1), 1.0 - smoothing)\n    log_probs = F.log_softmax(logits, dim=1)\n    return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ndef ohgw_warm_start(\n    model: torch.nn.Module,\n    train_loader: DataLoader,\n    device: torch.device,\n    weight_decay_param: torch.nn.Parameter,\n    label_smoothing_param: torch.nn.Parameter,\n    hyper_lr: float,\n) -\u003e Tuple[float, float]:\n    \"\"\"One-Shot Hyper-Gradient Warm-Start (OHGW).\n\n    Runs a single *mini-batch* forward/backward pass to obtain \u2202L/\u2202\u03c8 for each\n    continuous hyper-parameter \u03c8 (here: weight_decay and label_smoothing), then\n    applies one SGD step in hyper-parameter space.\n\n    Returns\n    -------\n    new_weight_decay, new_label_smoothing (floats)\n    \"\"\"\n    model.train()\n    data_iter = iter(train_loader)\n    inputs, targets = next(data_iter)\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    logits = model(inputs)\n    ce_loss = label_smoothing_cross_entropy(logits, targets, label_smoothing_param)\n\n    l2_penalty = 0.0\n    for p in model.parameters():\n        l2_penalty = l2_penalty + torch.sum(p.pow(2))\n    l2_penalty = 0.5 * weight_decay_param * l2_penalty\n\n    loss = ce_loss + l2_penalty\n\n    grads = torch.autograd.grad(loss, [weight_decay_param, label_smoothing_param])\n\n    with torch.no_grad():\n        weight_decay_param -= hyper_lr * grads[0]\n        label_smoothing_param -= hyper_lr * grads[1]\n        weight_decay_param.clamp_(min=0.0)\n        label_smoothing_param.clamp_(0.0, 0.5)\n\n    return float(weight_decay_param.item()), float(label_smoothing_param.item())\n\n\n# -------------------------- Training \u0026 Evaluation --------------------------- #\n\ndef accuracy(logits: Tensor, targets: Tensor) -\u003e float:\n    preds = torch.argmax(logits, dim=1)\n    return (preds == targets).float().mean().item()\n\n\ndef train_one_epoch(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    weight_decay: float,\n    label_smoothing: float,\n) -\u003e Tuple[float, float]:\n    \"\"\"Returns mean loss and accuracy for this epoch.\"\"\"\n    model.train()\n    epoch_loss = 0.0\n    epoch_acc = 0.0\n    num_batches = 0\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(inputs)\n        ce = label_smoothing_cross_entropy(logits, targets, label_smoothing)\n        l2 = 0.0\n        for p in model.parameters():\n            l2 = l2 + torch.sum(p.pow(2))\n        l2 = 0.5 * weight_decay * l2\n        loss = ce + l2\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += accuracy(logits, targets)\n        num_batches += 1\n\n    return epoch_loss / num_batches, epoch_acc / num_batches\n\n\ndef evaluate(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    weight_decay: float,\n    label_smoothing: float,\n) -\u003e Tuple[float, float]:\n    \"\"\"Validation/Test loop without gradient tracking.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    num_batches = 0\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            ce = label_smoothing_cross_entropy(logits, targets, label_smoothing)\n            l2 = 0.0\n            for p in model.parameters():\n                l2 = l2 + torch.sum(p.pow(2))\n            l2 = 0.5 * weight_decay * l2\n            loss = ce + l2\n\n            running_loss += loss.item()\n            running_acc += accuracy(logits, targets)\n            num_batches += 1\n    return running_loss / num_batches, running_acc / num_batches\n\n\n# ---------------------------- Main Entrypoint ------------------------------- #\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, required=True)\n    parser.add_argument(\"--run-id\", type=str, required=True)\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    return parser.parse_args()\n\n\ndef main() -\u003e None:\n    args = parse_args()\n\n    # Load single-run config\n    with open(args.config, \"r\") as fp:\n        cfg = yaml.safe_load(fp)\n\n    run_id: str = args.run_id\n    results_dir = Path(args.results_dir).expanduser().resolve() / run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # -------------------------------- Seeds -------------------------------- #\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------ Data ----------------------------------- #\n    train_loader, val_loader, test_loader, num_classes = data_utils.get_dataloaders(cfg)\n\n    # ----------------------------- Model ----------------------------------- #\n    model = model_utils.build_model(cfg, num_classes=num_classes).to(device)\n\n    # --------------------- Hyper-parameters as tensors --------------------- #\n    init_wd = float(cfg[\"training\"].get(\"weight_decay\", 5e-4))\n    init_ls = float(cfg[\"training\"].get(\"label_smoothing\", 0.0))\n    weight_decay_param = torch.nn.Parameter(torch.tensor(init_wd, device=device))\n    label_smoothing_param = torch.nn.Parameter(torch.tensor(init_ls, device=device))\n\n    lr = float(cfg[\"training\"].get(\"learning_rate\", 1e-3))\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=cfg[\"training\"].get(\"momentum\", 0.9))\n\n    # ----------------- One-Shot Hyper-Gradient Warm-Start ------------------ #\n    if cfg[\"training\"].get(\"ohgw_enable\", False):\n        new_wd, new_ls = ohgw_warm_start(\n            model,\n            train_loader,\n            device,\n            weight_decay_param,\n            label_smoothing_param,\n            hyper_lr=float(cfg[\"training\"].get(\"hyper_lr\", 1e-3)),\n        )\n        weight_decay_param = torch.tensor(new_wd, device=device)\n        label_smoothing_param = torch.tensor(new_ls, device=device)\n\n    # ---------------------------- Training -------------------------------- #\n    epochs = int(cfg[\"training\"].get(\"epochs\", 20))\n    best_val_acc = -1.0\n    best_epoch = 0\n    history: Dict[str, List[float]] = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        train_loss, train_acc = train_one_epoch(\n            model, train_loader, optimizer, device, float(weight_decay_param), float(label_smoothing_param)\n        )\n        val_loss, val_acc = evaluate(\n            model, val_loader, device, float(weight_decay_param), float(label_smoothing_param)\n        )\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        # Save best checkpoint\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch\n            ckpt_path = results_dir / \"best_model.pt\"\n            model_utils.save_checkpoint(\n                ckpt_path,\n                model,\n                {\n                    \"weight_decay\": float(weight_decay_param),\n                    \"label_smoothing\": float(label_smoothing_param),\n                },\n                epoch,\n            )\n\n        # Per-epoch JSON logging to stdout so GitHub Actions can capture it\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"stage\": \"epoch_end\",\n                    \"epoch\": epoch,\n                    \"train_loss\": train_loss,\n                    \"train_acc\": train_acc,\n                    \"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                }\n            )\n        )\n        sys.stdout.flush()\n\n    # --------------------------- Final Test -------------------------------- #\n    # Load *best* checkpoint for unbiased test evaluation\n    best_model = model_utils.load_checkpoint(results_dir / \"best_model.pt\", cfg, num_classes).to(device)\n    test_loss, test_acc = evaluate(best_model, test_loader, device, float(weight_decay_param), float(label_smoothing_param))\n\n    summary = {\n        \"run_id\": run_id,\n        \"best_epoch\": best_epoch,\n        \"best_val_acc\": best_val_acc,\n        \"test_loss\": test_loss,\n        \"test_acc\": test_acc,\n        \"history\": history,\n        \"config\": cfg,\n    }\n\n    # Write results.json\n    with open(results_dir / \"results.json\", \"w\") as fp:\n        json.dump(summary, fp, indent=2)\n\n    # And print summary to stdout (mandatory)\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "main.py uses Tee class as a context-manager (\"with Tee(...) as tee\") but Tee does not implement __enter__/__exit__, causing AttributeError before any training starts.  Therefore the core orchestration logic is incomplete / non-functional and base foundation cannot run end-to-end."
}
