
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "import argparse\nimport datetime\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n###########################################################################\n# Helper utilities\n###########################################################################\n\ndef _parse_args():\n    parser = argparse.ArgumentParser(\"Aggregate \u0026 visualise results of all variations\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Parent directory that contains one sub-dir per run (created by main.py).\")\n    return parser.parse_args()\n\n\ndef _load_results(results_dir: Path) -\u003e List[dict]:\n    runs = []\n    for p in results_dir.iterdir():\n        if p.is_dir() and (p / \"results.json\").exists():\n            with open(p / \"results.json\", \"r\") as fp:\n                runs.append(json.load(fp))\n    return runs\n\n\n###########################################################################\n# Figure helpers (all .pdf, annotated, legend etc.)\n###########################################################################\n\ndef _save_training_curves(df: pd.DataFrame, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.lineplot(data=df, x=\"epoch\", y=\"train_loss\", hue=\"run_id\", ax=ax)\n    for run_id, sub in df.groupby(\"run_id\"):\n        final_row = sub.iloc[-1]\n        ax.annotate(f\"{final_row[\u0027train_loss\u0027]:.3f}\",\n                    xy=(final_row[\u0027epoch\u0027], final_row[\u0027train_loss\u0027]), xytext=(0, 5),\n                    textcoords=\"offset points\", ha=\"center\", fontsize=8)\n    ax.set_title(\"Training Loss per Epoch\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_xlabel(\"Epoch\")\n    ax.legend()\n    fname = out_dir / \"training_loss.pdf\"\n    fig.savefig(fname, bbox_inches=\"tight\")\n    plt.close(fig)\n    print(f\"Generated figure: {fname.name}\")\n\n\ndef _save_validation_curves(df: pd.DataFrame, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.lineplot(data=df, x=\"epoch\", y=\"val_loss\", hue=\"run_id\", ax=ax)\n    for run_id, sub in df.groupby(\"run_id\"):\n        final_row = sub.iloc[-1]\n        ax.annotate(f\"{final_row[\u0027val_loss\u0027]:.3f}\",\n                    xy=(final_row[\u0027epoch\u0027], final_row[\u0027val_loss\u0027]), xytext=(0, 5),\n                    textcoords=\"offset points\", ha=\"center\", fontsize=8)\n    ax.set_title(\"Validation Loss per Epoch\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_xlabel(\"Epoch\")\n    ax.legend()\n    fname = out_dir / \"validation_loss.pdf\"\n    fig.savefig(fname, bbox_inches=\"tight\")\n    plt.close(fig)\n    print(f\"Generated figure: {fname.name}\")\n\n\ndef _save_final_metric_bar(runs: List[dict], out_dir: Path):\n    metric_name = runs[0][\"primary_metric_name\"]\n    fig, ax = plt.subplots(figsize=(6, 4))\n    run_ids = [r[\"run_id\"] for r in runs]\n    values = [r[\"primary_metric\"] for r in runs]\n    sns.barplot(x=run_ids, y=values, ax=ax)\n    for idx, val in enumerate(values):\n        ax.annotate(f\"{val:.3f}\", xy=(idx, val), xytext=(0, 2), textcoords=\"offset points\", ha=\"center\")\n    ax.set_ylabel(metric_name)\n    ax.set_title(f\"Final {metric_name} per Run\")\n    fname = out_dir / f\"final_{metric_name}.pdf\"\n    fig.savefig(fname, bbox_inches=\"tight\")\n    plt.close(fig)\n    print(f\"Generated figure: {fname.name}\")\n\n###########################################################################\n# Main\n###########################################################################\n\ndef main():\n    args = _parse_args()\n    res_dir = Path(args.results_dir)\n    runs = _load_results(res_dir)\n    if not runs:\n        print(\"[evaluate.py] No runs found \u2013 nothing to evaluate.\", file=sys.stderr)\n        sys.exit(1)\n\n    ######################################################################\n    # 1) Print Experiment Description BEFORE numerical results\n    ######################################################################\n    description = f\"\"\"\\\nExperiment description\\n======================\\nDate/Time : {datetime.datetime.now().isoformat()}\\nRuns      : {[r[\u0027run_id\u0027] for r in runs]}\\nPrimary metric : {runs[0][\u0027primary_metric_name\u0027]}\\n\"\"\"\n    print(description)\n\n    ######################################################################\n    # 2) Collate epoch-wise data into tidy frame\n    ######################################################################\n    rows = []\n    for r in runs:\n        for e, tl, vl in zip(r[\"epochs\"], r[\"train_loss\"], r[\"val_loss\"]):\n            rows.append({\"run_id\": r[\"run_id\"], \"epoch\": e, \"train_loss\": tl, \"val_loss\": vl})\n    df = pd.DataFrame(rows)\n\n    ######################################################################\n    # 3) Generate figures (all saved to .pdf)\n    ######################################################################\n    _save_training_curves(df, res_dir)\n    _save_validation_curves(df, res_dir)\n    _save_final_metric_bar(runs, res_dir)\n\n    ######################################################################\n    # 4) Print aggregated numerical comparison (JSON)\n    ######################################################################\n    comparison = {r[\"run_id\"]: r[\"primary_metric\"] for r in runs}\n    print(json.dumps({\"comparison_primary_metric\": comparison}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# FULL EXPERIMENT CONFIG TEMPLATE\n# All concrete datasets / models / search spaces will be filled in the\n# derive-specific phase.  Only the skeleton \u0026 placeholder keys are\n# defined here so that the COMMON CORE FOUNDATION can already parse\n# and validate the structure.\n\nglobal:\n  # PLACEHOLDER: common defaults (batch_size, epochs, etc.) to be overridden\n  batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n  epochs: SPECIFIC_CONFIG_PLACEHOLDER\n  dataset:\n    name: DATASET_PLACEHOLDER   # e.g. CIFAR10, ImageNet, WikiText103\n\nexperiments:\n  # PLACEHOLDER_EXPERIMENTS \u2013 will be populated with concrete runs\n  # Example entry structure (to be replaced):\n  # - run_id: asha_baseline\n  #   model_name: MODEL_PLACEHOLDER\n  #   hyperparameters: {}\n  # - run_id: asha_ohgw\n  #   model_name: MODEL_PLACEHOLDER\n  #   hyperparameters:\n  #     weight_decay: 0.0005\n  \n  []\n", "main_py": "import argparse\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport yaml\n\n###########################################################################\n# Helpers for tee-style logging of sub-process output\n###########################################################################\n\ndef _stream_consumer(src_stream, dst_file):\n    for line in iter(src_stream.readline, \u0027\u0027):\n        sys.stdout.write(line)\n        dst_file.write(line)\n        dst_file.flush()\n    src_stream.close()\n\n\ndef _run_subprocess(cmd, stdout_path: Path, stderr_path: Path):\n    with open(stdout_path, \u0027w\u0027) as out_f, open(stderr_path, \u0027w\u0027) as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        t_out = threading.Thread(target=_stream_consumer, args=(proc.stdout, out_f))\n        t_err = threading.Thread(target=_stream_consumer, args=(proc.stderr, err_f))\n        t_out.start(); t_err.start()\n        proc.wait()\n        t_out.join(); t_err.join()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {cmd} exited with code {proc.returncode}\")\n\n###########################################################################\n# CLI parsing\n###########################################################################\n\ndef _parse_args():\n    parser = argparse.ArgumentParser(\"Master orchestrator for all experiments\")\n    mode = parser.add_mutually_exclusive_group(required=True)\n    mode.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run the lightweight smoke-test YAML config.\")\n    mode.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full experiment YAML config.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where all outputs / logs / figures are saved.\")\n    return parser.parse_args()\n\n###########################################################################\n# Main driver\n###########################################################################\n\ndef main():\n    args = _parse_args()\n    cfg_file = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_file, \"r\") as fp:\n        top_cfg = yaml.safe_load(fp)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    global_cfg = top_cfg.get(\"global\", {})\n    experiments = top_cfg.get(\"experiments\", [])\n\n    for exp_cfg in experiments:\n        run_id = exp_cfg[\"run_id\"]\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Merge \u2192 global settings \u003c exp settings (exp overrides)\n        merged = {**global_cfg, **exp_cfg}\n        merged_cfg_path = run_dir / \"config.yaml\"\n        with open(merged_cfg_path, \"w\") as fp:\n            yaml.safe_dump(merged, fp)\n\n        cmd = [sys.executable, \"-m\", \"src.train\",\n               \"--config\", str(merged_cfg_path),\n               \"--results-dir\", str(results_dir),\n               \"--run-id\", run_id]\n        if args.smoke_test:\n            cmd.append(\"--smoke-test\")\n\n        print(f\"[main.py] Launching run \u0027{run_id}\u0027 \u2192 cmd: {\u0027 \u0027.join(cmd)}\")\n        _run_subprocess(cmd, run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs: aggregated evaluation\n    # ------------------------------------------------------------------\n    print(\"[main.py] All runs finished. Invoking evaluator\u2026\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"\nModel implementations \u0026 factory.  The COMMON CORE ships only with a\nvery small, task-agnostic MLP so that smoke tests can run.  All\nreal-world architectures will be plugged in later by extending\nModelFactory.\n\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nimport torch\nimport torch.nn as nn\n\n\nclass GenericMLPModel(nn.Module):\n    \"\"\"Simple MLP that flattens arbitrary input tensors.\"\"\"\n\n    def __init__(self, input_shape: Tuple[int, ...], output_dim: int,\n                 hidden_dims: Tuple[int, ...] = (128, 128),\n                 task_type: str = \"classification\"):\n        super().__init__()\n        self.task_type = task_type\n        in_features = 1\n        for d in input_shape:\n            in_features *= d\n\n        layers = []\n        last = in_features\n        for h in hidden_dims:\n            layers += [nn.Linear(last, h), nn.ReLU()]\n            last = h\n        layers.append(nn.Linear(last, output_dim))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = torch.flatten(x, start_dim=1)\n        return self.net(x)\n\n\nclass ModelFactory:\n    \"\"\"Factory that hides away the conditional creation logic.\"\"\"\n\n    @staticmethod\n    def create(model_name: str, meta_info: Dict[str, Any], model_cfg: Dict[str, Any]):\n        name = model_name.lower()\n        if name == \"dummy_mlp\":\n            input_shape = tuple(meta_info[\"input_shape\"])\n            task_type = meta_info[\"task_type\"]\n            if task_type == \"classification\":\n                output_dim = int(meta_info[\"num_classes\"])\n            else:\n                output_dim = model_cfg.get(\"output_dim\", 1)\n            hidden_dims = tuple(model_cfg.get(\"hidden_dims\", (128, 128)))\n            return GenericMLPModel(input_shape, output_dim, hidden_dims, task_type)\n\n        # ------------------------------------------------------------------\n        # PLACEHOLDER for real architectures (ResNet, Transformer, etc.)\n        # ------------------------------------------------------------------\n        raise NotImplementedError(f\"Model \u0027{model_name}\u0027 is not implemented yet. Extend ModelFactory in derive stage.\")\n", "preprocess_py": "\"\"\"\nGeneric, task-agnostic data-loading utility with clear placeholders for\nfuture, dataset-specific implementations.  For now, a synthetic\nDummyDataset enables end-to-end smoke tests without any external files.\n\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n# -------------------------------------------------------------\n# DummyDataset \u2013 keeps the COMMON CORE independent of any domain\n# -------------------------------------------------------------\n\nclass DummyDataset(Dataset):\n    \"\"\"Synthetic data supporting both classification \u0026 regression.\"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, ...],\n                 task_type: str = \"classification\", num_classes: int = 10,\n                 regression_target_shape: Tuple[int, ...] = (1,)):\n        super().__init__()\n        self.num_samples = num_samples\n        self.input_shape = input_shape\n        self.task_type = task_type\n        self.num_classes = num_classes\n        self.regression_target_shape = regression_target_shape\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx: int):\n        x = torch.randn(self.input_shape, dtype=torch.float32)\n        if self.task_type == \"classification\":\n            y = torch.randint(0, self.num_classes, (1,), dtype=torch.long).squeeze(0)\n        else:\n            y = torch.randn(self.regression_target_shape, dtype=torch.float32)\n        return {\"inputs\": x, \"targets\": y}\n\n\n# -------------------------------------------------------------\n# Factory \u2013 returns train / validation loaders + meta info\n# -------------------------------------------------------------\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any], batch_size: int, *,\n                    num_workers: int = 0, smoke: bool = False):\n    \"\"\"Return (train_loader, val_loader, meta_info).\"\"\"\n\n    name = dataset_cfg.get(\"name\", \"DUMMY\").upper()\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER SECTION \u2013 extend with real datasets in next stage\n    # ------------------------------------------------------------------\n    if name == \"DUMMY\":\n        task_type = dataset_cfg.get(\"task_type\", \"classification\")\n        n_samples = dataset_cfg.get(\"num_samples_smoke\" if smoke else \"num_samples\", 1024)\n        input_shape = tuple(dataset_cfg.get(\"input_shape\", (1, 28)))\n        n_classes = dataset_cfg.get(\"num_classes\", 10)\n        dataset = DummyDataset(n_samples, input_shape, task_type, n_classes)\n    # ------------------------------------------------------------------\n    else:\n        raise NotImplementedError(\n            f\"Dataset \u0027{name}\u0027 is not yet implemented. Replace this placeholder with actual loader logic.\")\n\n    # Train/Val split\n    val_fraction = dataset_cfg.get(\"val_fraction\", 0.1)\n    val_size = int(len(dataset) * val_fraction)\n    train_size = len(dataset) - val_size\n    train_ds, val_ds = random_split(dataset, [train_size, val_size],\n                                    generator=torch.Generator().manual_seed(42))\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n\n    meta = {\n        \"input_shape\": dataset.input_shape,\n        \"task_type\": dataset.task_type,\n        \"num_classes\": dataset.num_classes if hasattr(dataset, \"num_classes\") else None,\n    }\n    return train_loader, val_loader, meta\n", "pyproject_toml": "[project]\nname = \"ohgw-common-core\"\nversion = \"0.1.0\"\ndescription = \"COMMON CORE FOUNDATION for One-Shot Hyper-Gradient Warm-Start experiments\"\nrequires-python = \"\u003e=3.10\"\n\n[project.dependencies]\ntorch = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\ntqdm = \"*\"\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n", "smoke_test_yaml": "# Lightweight configuration that runs entirely on synthetic data so CI / smoke tests\n# finish within seconds.  All heavy-weight dataset or model references are replaced\n# by descriptive placeholders.\n\nglobal:\n  seed: 42\n  batch_size: 8\n  epochs: 2  # short \u21d2 smoke tests only\n\n  # ------------------------------------------------------------------\n  # Dataset placeholder \u2013 real loaders will replace \"DUMMY\" in later step\n  # ------------------------------------------------------------------\n  dataset:\n    name: DUMMY                # PLACEHOLDER: will switch to real dataset later\n    task_type: classification\n    num_samples_smoke: 64\n    input_shape: [1, 10]\n    num_classes: 3\n    val_fraction: 0.2\n\n  # ------------------------------------------------------------------\n  # Model placeholder\n  # ------------------------------------------------------------------\n  model_name: dummy_mlp        # PLACEHOLDER: e.g. resnet20, transformer\n  model: {}\n\n  # ------------------------------------------------------------------\n  # Optimiser \u0026 warm-start hyper-params\n  # ------------------------------------------------------------------\n  optimizer:\n    name: sgd\n    lr: 0.01\n    momentum: 0.0\n    weight_decay: 0.0\n\n  eta_h: 0.001\n\nexperiments:\n  - run_id: baseline\n    # Baseline \u2013 no hyper-parameters warm-started\n    hyperparameters: {}\n\n  - run_id: ohgw\n    # Proposed method: single hyper-gradient step on weight_decay\n    hyperparameters:\n      weight_decay: 0.0005\n", "train_py": "import argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport torch\nimport torch.nn as nn\nimport yaml\nfrom tqdm import tqdm\n\nfrom src import preprocess\nfrom src import model as model_lib\n\n###########################################################################\n# Utility helpers\n###########################################################################\n\ndef _parse_args():\n    parser = argparse.ArgumentParser(\"Train a single experimental variation\")\n    parser.add_argument(\"--config\", type=str, required=True,\n                        help=\"Path to YAML file that contains the merged (global+run-specific) configuration for THIS run only.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where all run sub-directories live.\")\n    parser.add_argument(\"--run-id\", type=str, required=True,\n                        help=\"Unique identifier for this run (taken from config file by main.py).\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\",\n                        help=\"If true, will override heavy configs so we can finish very quickly.\")\n    return parser.parse_args()\n\n\ndef _select_device(cfg: Dict[str, Any]) -\u003e torch.device:\n    \"\"\"Return CUDA device if available, else CPU.\"\"\"\n    if torch.cuda.is_available():\n        gpu_idx = cfg.get(\"gpu\", 0)\n        return torch.device(f\"cuda:{gpu_idx}\")\n    return torch.device(\"cpu\")\n\n\n###########################################################################\n# Generic (task-agnostic) components\n###########################################################################\n\ndef _build_optimizer(opt_cfg: Dict[str, Any], parameters):\n    name = opt_cfg.get(\"name\", \"sgd\").lower()\n    lr = opt_cfg.get(\"lr\", 1e-3)\n    weight_decay = opt_cfg.get(\"weight_decay\", 0.0)\n    momentum = opt_cfg.get(\"momentum\", 0.0)\n\n    if name == \"sgd\":\n        return torch.optim.SGD(parameters, lr=lr, momentum=momentum, weight_decay=weight_decay)\n    if name == \"adam\":\n        betas = tuple(opt_cfg.get(\"betas\", (0.9, 0.999)))\n        return torch.optim.Adam(parameters, lr=lr, betas=betas, weight_decay=weight_decay)\n    raise NotImplementedError(f\"Optimizer \u0027{name}\u0027 is not implemented in the COMMON CORE FOUNDATION.\")\n\n\ndef _get_loss_fn(task_type: str, loss_cfg: Dict[str, Any]):\n    \"\"\"Return a PyTorch loss function based on task_type or explicit cfg.\"\"\"\n    name = (loss_cfg.get(\"name\") or (\"cross_entropy\" if task_type == \"classification\" else \"mse\")).lower()\n    if name == \"cross_entropy\":\n        return nn.CrossEntropyLoss()\n    if name == \"mse\":\n        return nn.MSELoss()\n    if name == \"l1\":\n        return nn.L1Loss()\n    raise NotImplementedError(f\"Loss \u0027{name}\u0027 is not implemented in COMMON CORE FOUNDATION.\")\n\n\n###########################################################################\n# One-Shot Hyper-Gradient Warm-Start (OHGW)\n###########################################################################\n\ndef _ohgw_step(model: nn.Module,\n               hyperparams: Dict[str, torch.Tensor],\n               batch: Dict[str, torch.Tensor],\n               eta_h: float,\n               loss_fn: nn.Module,\n               device: torch.device):\n    \"\"\"Perform ONE cheap hyper-gradient update on the given hyper-parameters.\"\"\"\n    inputs = batch[\"inputs\"].to(device)\n    targets = batch[\"targets\"].to(device)\n\n    outputs = model(inputs)\n    loss = loss_fn(outputs, targets)\n\n    # Example of integrating weight decay directly into loss so that\n    # its gradient w.r.t. the hyper-parameter can be computed.\n    if \"weight_decay\" in hyperparams:\n        wd = hyperparams[\"weight_decay\"]\n        weight_penalty = sum((p ** 2).sum() for p in model.parameters())\n        loss = loss + wd * weight_penalty\n\n    grads = torch.autograd.grad(loss, list(hyperparams.values()), allow_unused=True, retain_graph=False)\n    for (name, param), g in zip(hyperparams.items(), grads):\n        if g is None:\n            continue  # hyper-parameter not actually in graph\n        with torch.no_grad():\n            param -= eta_h * g  # inplace update\n\n###########################################################################\n# Training loop for a single run\n###########################################################################\n\ndef _train_single_run(cfg: Dict[str, Any], results_root: Path, smoke: bool):\n    torch.manual_seed(cfg.get(\"seed\", 42))\n\n    device = _select_device(cfg)\n\n    # ------------------------------------------------------------------\n    # 1) Data\n    # ------------------------------------------------------------------\n    batch_size = cfg.get(\"batch_size\", 32)\n    train_loader, val_loader, meta = preprocess.get_dataloaders(cfg[\"dataset\"],\n                                                               batch_size=batch_size,\n                                                               num_workers=0,\n                                                               smoke=smoke)\n\n    # ------------------------------------------------------------------\n    # 2) Model\n    # ------------------------------------------------------------------\n    model = model_lib.ModelFactory.create(cfg.get(\"model_name\", \"dummy_mlp\"),\n                                          meta_info=meta,\n                                          model_cfg=cfg.get(\"model\", {}))\n    model.to(device)\n\n    # ------------------------------------------------------------------\n    # 3) Loss \u0026 Optimizer\n    # ------------------------------------------------------------------\n    loss_fn = _get_loss_fn(meta[\"task_type\"], cfg.get(\"loss\", {}))\n    optimizer = _build_optimizer(cfg.get(\"optimizer\", {}), model.parameters())\n\n    # ------------------------------------------------------------------\n    # 4) Hyper-parameters to be warm-started via OHGW\n    # ------------------------------------------------------------------\n    raw_hparams = cfg.get(\"hyperparameters\", {})\n    hyperparams = {k: torch.tensor(v, dtype=torch.float32, device=device, requires_grad=True)\n                   for k, v in raw_hparams.items()}\n\n    # ------------------------------------------------------------------\n    # 5) One-Shot Hyper-Gradient Warm-Start\n    # ------------------------------------------------------------------\n    eta_h = cfg.get(\"eta_h\", 1e-3)\n    _ohgw_step(model, hyperparams, next(iter(train_loader)), eta_h, loss_fn, device)\n\n    # e.g. if weight_decay was updated during OHGW, reflect it inside optimizer\n    if \"weight_decay\" in hyperparams:\n        for pg in optimizer.param_groups:\n            pg[\"weight_decay\"] = float(hyperparams[\"weight_decay\"].detach().cpu().item())\n\n    # ------------------------------------------------------------------\n    # 6) Epoch loop\n    # ------------------------------------------------------------------\n    max_epochs = 1 if smoke else cfg.get(\"epochs\", 10)\n\n    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(max_epochs):\n        # ----------------------- TRAIN -----------------------------\n        model.train()\n        running, n_samples = 0.0, 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Train [epoch {epoch}]\"):\n            optimizer.zero_grad()\n            x, y = batch[\"inputs\"].to(device), batch[\"targets\"].to(device)\n            preds = model(x)\n            loss = loss_fn(preds, y)\n            if \"weight_decay\" in hyperparams:\n                wd = hyperparams[\"weight_decay\"]\n                loss = loss + wd * sum((p ** 2).sum() for p in model.parameters())\n            loss.backward()\n            optimizer.step()\n            running += loss.item() * x.size(0)\n            n_samples += x.size(0)\n        train_loss = running / n_samples\n\n        # ----------------------- VAL -------------------------------\n        model.eval()\n        val_running, val_samples = 0.0, 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, y = batch[\"inputs\"].to(device), batch[\"targets\"].to(device)\n                preds = model(x)\n                vloss = loss_fn(preds, y)\n                if \"weight_decay\" in hyperparams:\n                    wd = hyperparams[\"weight_decay\"]\n                    vloss = vloss + wd * sum((p ** 2).sum() for p in model.parameters())\n                val_running += vloss.item() * x.size(0)\n                val_samples += x.size(0)\n        val_loss = val_running / val_samples\n\n        # ----------------------- LOG -------------------------------\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n\n        print(json.dumps({\n            \"run_id\": cfg[\"run_id\"],\n            \"event\": \"epoch_end\",\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss\n        }), flush=True)\n\n    # ------------------------------------------------------------------\n    # 7) Persist results \u0026 artefacts\n    # ------------------------------------------------------------------\n    run_dir = results_root / cfg[\"run_id\"]\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    results = {\n        \"run_id\": cfg[\"run_id\"],\n        \"seed\": cfg.get(\"seed\", 42),\n        \"primary_metric_name\": \"val_loss\",\n        \"primary_metric\": history[\"val_loss\"][-1],\n        \"train_loss\": history[\"train_loss\"],\n        \"val_loss\": history[\"val_loss\"],\n        \"epochs\": history[\"epoch\"],\n        \"hyperparameters\": {k: float(v.detach().cpu().item()) for k, v in hyperparams.items()}\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # Save state dict for potential later re-use / analysis\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n\n###########################################################################\n# Entry point\n###########################################################################\n\ndef main():\n    args = _parse_args()\n    with open(args.config, \"r\") as fp:\n        cfg = yaml.safe_load(fp)\n    cfg[\"run_id\"] = args.run_id  # enforced consistency\n    _train_single_run(cfg, Path(args.results_dir), args.smoke_test)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Criteria not fully met: (1) Model loading mechanism is missing – code only saves state_dict but nowhere provides a function / path to reload models, so core infrastructure is incomplete. (4) Sub-process stderr is forwarded to main stdout instead of stderr, and evaluate.py prints human-readable text before the required JSON block, risking downstream parsers. Therefore base foundation is not yet fully solid."
}
