
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation
   - main.py implements 1 GPU per run variation allocation, queueing runs sequentially when GPUs are insufficient
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\nFIG_TOPIC_FINAL_ACC = \u0027final_accuracy\u0027\nFIG_TOPIC_WALLCLOCK = \u0027time_to_target\u0027\n\n\ndef load_all_results(results_dir: Path):\n    records = []\n    for run_dir in results_dir.iterdir():\n        res_file = run_dir / \u0027results.json\u0027\n        if res_file.exists():\n            with res_file.open() as f:\n                res = json.load(f)\n            record = {\n                \u0027run_id\u0027: res[\u0027run_id\u0027],\n                \u0027dataset\u0027: res[\u0027config\u0027][\u0027dataset\u0027][\u0027name\u0027],\n                \u0027model\u0027: res[\u0027config\u0027][\u0027model\u0027][\u0027name\u0027],\n                \u0027use_ohgw\u0027: res[\u0027config\u0027].get(\u0027use_ohgw\u0027, False),\n                \u0027final_test_acc\u0027: res[\u0027final_metrics\u0027][\u0027test_acc\u0027],\n                \u0027wallclock_s\u0027: res[\u0027final_metrics\u0027][\u0027training_wallclock_s\u0027]\n            }\n            # Optional threshold time if provided\n            target = res[\u0027config\u0027][\u0027evaluation\u0027].get(\u0027target_acc\u0027) if \u0027evaluation\u0027 in res[\u0027config\u0027] else None\n            if target is not None:\n                # Define helper to find epoch meeting threshold\n                epochs = np.array(res[\u0027epoch_metrics\u0027][\u0027epoch\u0027])\n                val_acc = np.array(res[\u0027epoch_metrics\u0027][\u0027val_acc\u0027])\n                meets = np.where(val_acc \u003e= target)[0]\n                record[\u0027time_to_target\u0027] = res[\u0027final_metrics\u0027][\u0027training_wallclock_s\u0027] if len(meets) == 0 else (\n                    res[\u0027final_metrics\u0027][\u0027training_wallclock_s\u0027] * (meets[0] + 1) / len(epochs))\n            records.append(record)\n    return pd.DataFrame(records)\n\n\ndef barplot(df: pd.DataFrame, x: str, y: str, hue: str, ylabel: str, title: str, fig_name: str, output_dir: Path):\n    plt.figure(figsize=(8, 5))\n    ax = sns.barplot(data=df, x=x, y=y, hue=hue)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\", (p.get_x() + p.get_width() / 2., height), ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=8)\n    plt.title(title)\n    plt.ylabel(ylabel)\n    plt.tight_layout()\n    file_path = output_dir / f\"{fig_name}.pdf\"\n    plt.savefig(file_path, bbox_inches=\u0027tight\u0027)\n    plt.close()\n    return str(file_path.name)\n\n\ndef evaluate(results_dir: str):\n    results_dir = Path(results_dir)\n    df = load_all_results(results_dir)\n    if df.empty:\n        print(json.dumps({\u0027event\u0027: \u0027evaluate_completed\u0027, \u0027message\u0027: \u0027No results found.\u0027}))\n        return\n\n    # ------------------------------------------------------------------\n    # Figure 1: Final Test Accuracy grouped by OHGW vs baseline\n    # ------------------------------------------------------------------\n    acc_fig = barplot(df, x=\u0027run_id\u0027, y=\u0027final_test_acc\u0027, hue=\u0027use_ohgw\u0027,\n                      ylabel=\u0027Final Test Accuracy\u0027,\n                      title=\u0027Final Test Accuracy per Run (OHGW vs Baseline)\u0027,\n                      fig_name=f\u0027{FIG_TOPIC_FINAL_ACC}\u0027, output_dir=results_dir)\n\n    # ------------------------------------------------------------------\n    # Figure 2: Wall-clock time to target accuracy (if available)\n    # ------------------------------------------------------------------\n    time_fig = None\n    if \u0027time_to_target\u0027 in df.columns and not df[\u0027time_to_target\u0027].isnull().all():\n        time_fig = barplot(df, x=\u0027run_id\u0027, y=\u0027time_to_target\u0027, hue=\u0027use_ohgw\u0027,\n                           ylabel=\u0027Wall-clock seconds to target\u0027,\n                           title=\u0027Time to Target Accuracy (OHGW vs Baseline)\u0027,\n                           fig_name=FIG_TOPIC_WALLCLOCK, output_dir=results_dir)\n\n    comparison_summary = {\n        \u0027event\u0027: \u0027evaluate_completed\u0027,\n        \u0027num_runs\u0027: len(df),\n        \u0027best_accuracy\u0027: df[\u0027final_test_acc\u0027].max(),\n        \u0027accuracy_improvement_mean\u0027: df.groupby(\u0027use_ohgw\u0027)[\u0027final_test_acc\u0027].mean().to_dict(),\n        \u0027figures\u0027: [f for f in [acc_fig, time_fig] if f is not None]\n    }\n\n    print(json.dumps(comparison_summary))\n\n\nif __name__ == \u0027__main__\u0027:\n    parser = argparse.ArgumentParser(description=\u0027Aggregate and compare experimental results.\u0027)\n    parser.add_argument(\u0027--results-dir\u0027, required=True, type=str, help=\u0027Path that contains run sub-directories with results.json files.\u0027)\n    args = parser.parse_args()\n\n    evaluate(args.results_dir)\n", "full_experiment_yaml": "experiments:\n  # PLACEHOLDER: Insert all real experiment variations here.\n  # Each list item defines one experimental variation. Copy the same keys as in smoke_test.yaml and override\n  # dataset/model/hyperparameters/training specifics. Ensure every variation has a unique run_id.\n  - run_id: DATASET_PLACEHOLDER_BASELINE\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: value\n    model:\n      name: MODEL_PLACEHOLDER\n    hyperparameters:\n      lr: 0.1\n      weight_decay: 0.0005\n    training:\n      epochs: 200\n    use_ohgw: false\n\n  - run_id: DATASET_PLACEHOLDER_OHGW\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: value\n    model:\n      name: MODEL_PLACEHOLDER\n    hyperparameters:\n      lr: 0.1\n      weight_decay: 0.0005\n    training:\n      epochs: 200\n    use_ohgw: true\n    eta_h: 0.001\n  # Add further ablations or scheduler variants as needed.\n", "main_py": "import argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom queue import Queue\nfrom typing import Dict, List\n\nimport torch\nimport yaml\n\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nCONFIG_DIR = PROJECT_ROOT / \u0027config\u0027\nSRC_DIR = PROJECT_ROOT / \u0027src\u0027\n\n\n# ----------------------------------------------------------------------------\n# Helpers for GPU allocation\n# ----------------------------------------------------------------------------\n\ndef get_available_gpu_indices() -\u003e List[int]:\n    \"\"\"Return list of visible GPU indices based on torch.cuda and env vars.\"\"\"\n    if not torch.cuda.is_available():\n        return []\n    visible = os.environ.get(\u0027CUDA_VISIBLE_DEVICES\u0027, None)\n    if visible is not None:\n        return list(range(len(visible.split(\u0027,\u0027))))\n    return list(range(torch.cuda.device_count()))\n\n\n# ----------------------------------------------------------------------------\n# Sub-process management with tee-style logging\n# ----------------------------------------------------------------------------\n\ndef tee_stream(stream, log_file_handle, prefix):\n    for line in iter(stream.readline, \u0027\u0027):\n        if not line:\n            break\n        log_file_handle.write(line)\n        log_file_handle.flush()\n        sys.stdout.write(f\"[{prefix}] {line}\")\n        sys.stdout.flush()\n    stream.close()\n\n\ndef launch_run(run_cfg: Dict, results_dir: Path, gpu_id: int):\n    run_id = run_cfg[\u0027run_id\u0027]\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Serialize individual run config to YAML for the child process\n    config_path = run_dir / \u0027config.yaml\u0027\n    with config_path.open(\u0027w\u0027) as f:\n        yaml.safe_dump(run_cfg, f)\n\n    cmd = [sys.executable, \u0027-m\u0027, \u0027src.train\u0027, \u0027--config\u0027, str(config_path), \u0027--results-dir\u0027, str(results_dir), \u0027--gpu-id\u0027, str(gpu_id)]\n\n    stdout_file = run_dir / \u0027stdout.log\u0027\n    stderr_file = run_dir / \u0027stderr.log\u0027\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n\n    # Spawn tee threads\n    tee_threads = []\n    tee_threads.append(threading.Thread(target=tee_stream, args=(proc.stdout, stdout_file.open(\u0027w\u0027), run_id + \u0027:STDOUT\u0027), daemon=True))\n    tee_threads.append(threading.Thread(target=tee_stream, args=(proc.stderr, stderr_file.open(\u0027w\u0027), run_id + \u0027:STDERR\u0027), daemon=True))\n    for t in tee_threads:\n        t.start()\n\n    return proc, tee_threads\n\n\n# ----------------------------------------------------------------------------\n# Main orchestration\n# ----------------------------------------------------------------------------\n\ndef main(is_smoke_test: bool, results_dir: str):\n    results_dir = Path(results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Select config file\n    yaml_path = CONFIG_DIR / (\u0027smoke_test.yaml\u0027 if is_smoke_test else \u0027full_experiment.yaml\u0027)\n    with yaml_path.open(\u0027r\u0027) as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg[\u0027experiments\u0027]\n\n    gpu_indices = get_available_gpu_indices() or [None]  # CPU fallback\n    gpu_queue = Queue()\n    for idx in gpu_indices:\n        gpu_queue.put(idx)\n\n    processes = []  # list of tuples (proc, threads, gpu_id)\n\n    def worker(experiment_cfg):\n        gpu_id = gpu_queue.get()  # blocks until a GPU is free\n        try:\n            proc, tee_threads = launch_run(experiment_cfg, results_dir, gpu_id if gpu_id is not None else -1)\n            proc.wait()\n            for t in tee_threads:\n                t.join()\n        finally:\n            gpu_queue.put(gpu_id)\n\n    # Sequential submission in background threads to honour GPU queue\n    threads = []\n    for exp_cfg in experiments:\n        t = threading.Thread(target=worker, args=(exp_cfg,), daemon=False)\n        t.start()\n        threads.append(t)\n\n    # Wait until all experiments have finished\n    for t in threads:\n        t.join()\n\n    # Evaluation aggregation\n    eval_cmd = [sys.executable, \u0027-m\u0027, \u0027src.evaluate\u0027, \u0027--results-dir\u0027, str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \u0027__main__\u0027:\n    parser = argparse.ArgumentParser(description=\u0027Main orchestration script for OHGW experiments\u0027)\n    mode_group = parser.add_mutually_exclusive_group(required=True)\n    mode_group.add_argument(\u0027--smoke-test\u0027, action=\u0027store_true\u0027)\n    mode_group.add_argument(\u0027--full-experiment\u0027, action=\u0027store_true\u0027)\n    parser.add_argument(\u0027--results-dir\u0027, required=True, type=str, help=\u0027Where to store all experiment outputs\u0027)\n    args = parser.parse_args()\n\n    main(is_smoke_test=args.smoke_test, results_dir=args.results_dir)\n", "model_py": "\"\"\"Base model definitions.\n\nThe framework supports multiple architectures (baseline, proposed, ablations).\nModel-specific details can be filled in later while maintaining a consistent\nAPI thanks to the factory `build_model` function.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\n\nclass BaseModel(nn.Module):\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass BaselineMLP(BaseModel):\n    \"\"\"A simple multi-layer perceptron suitable for synthetic data.\"\"\"\n\n    def __init__(self, num_classes: int, input_dim: int = 32):\n        super().__init__(num_classes)\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        return self.classifier(x)\n\n\nclass ProposedModel(BaseModel):\n    \"\"\"Placeholder for the OHGW-enhanced model architecture.\n\n    The proposed method (OHGW) is algorithmic and *not* architectural, so we\n    currently reuse the BaselineMLP. Specific experiments may override with\n    domain-appropriate nets (e.g. ResNet, Transformer).\"\"\"\n\n    def __init__(self, num_classes: int, input_dim: int = 32):\n        super().__init__(num_classes)\n        self.model = BaselineMLP(num_classes, input_dim=input_dim)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass AblationModel(BaseModel):\n    \"\"\"A variant where OHGW effects are neutralised (e.g. random warm-start).\"\"\"\n\n    def __init__(self, num_classes: int, input_dim: int = 32):\n        super().__init__(num_classes)\n        self.model = BaselineMLP(num_classes, input_dim=input_dim)\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# ----------------------------------------------------------------------------\n# Factory\n# ----------------------------------------------------------------------------\n\ndef build_model(model_cfg: dict, num_classes: int):\n    name = model_cfg[\u0027name\u0027]\n    if name == \u0027baseline_mlp\u0027:\n        return BaselineMLP(num_classes=num_classes, input_dim=model_cfg.get(\u0027input_dim\u0027, 32))\n    elif name == \u0027proposed_mlp\u0027:\n        return ProposedModel(num_classes=num_classes, input_dim=model_cfg.get(\u0027input_dim\u0027, 32))\n    elif name == \u0027ablation_mlp\u0027:\n        return AblationModel(num_classes=num_classes, input_dim=model_cfg.get(\u0027input_dim\u0027, 32))\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Add new architectures here.\n    # ------------------------------------------------------------------\n    else:\n        raise ValueError(f\"Unknown model name \u0027{name}\u0027.\")\n", "preprocess_py": "\"\"\"Common data preprocessing utilities with dataset placeholders.\n\nDataset-specific logic should be inserted in the designated PLACEHOLDER blocks\nin a later derivation step. Until then, a small synthetic dataset is provided\nso that the entire pipeline is fully executable (useful for smoke tests and\nCI).\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n\nclass SyntheticClassificationDataset(Dataset):\n    \"\"\"A tiny synthetic dataset for quick smoke tests (10 classes).\"\"\"\n\n    def __init__(self, num_samples: int = 512, num_features: int = 32, num_classes: int = 10):\n        super().__init__()\n        self.x = torch.randn(num_samples, num_features)\n        self.y = torch.randint(0, num_classes, (num_samples,))\n        self.num_classes = num_classes\n\n    def __len__(self):\n        return self.x.size(0)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# ----------------------------------------------------------------------------\n# Public API\n# ----------------------------------------------------------------------------\n\ndef build_dataloaders(dataset_cfg: dict, quick_run: bool = False) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Return train/val/test dataloaders as per dataset configuration.\n\n    The routine is fully functional thanks to the synthetic fallback dataset.\n    When integrating real datasets, replace the PLACEHOLDER block with proper\n    loading, preprocessing and splitting logic.\n    \"\"\"\n    name = dataset_cfg[\u0027name\u0027]\n    batch_size = dataset_cfg.get(\u0027batch_size\u0027, 64)\n    num_workers = dataset_cfg.get(\u0027num_workers\u0027, 4)\n\n    # ------------------------------------------------------------------\n    # Synthetic fallback dataset (default)\n    # ------------------------------------------------------------------\n    if name == \u0027RANDOM_TINY\u0027:\n        base_ds = SyntheticClassificationDataset(num_samples=1024 if not quick_run else 128,\n                                                 num_features=dataset_cfg.get(\u0027num_features\u0027, 32),\n                                                 num_classes=dataset_cfg.get(\u0027num_classes\u0027, 10))\n        n_total = len(base_ds)\n        n_train = int(0.7 * n_total)\n        n_val = int(0.15 * n_total)\n        n_test = n_total - n_train - n_val\n        train_ds, val_ds, test_ds = random_split(base_ds, [n_train, n_val, n_test])\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    # ------------------------------------------------------------------\n    else:\n        raise NotImplementedError(f\"Dataset \u0027{name}\u0027 not implemented yet. Replace placeholder with actual loader.\")\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, test_loader\n\n\ndef get_num_classes(dataset_cfg: dict) -\u003e int:\n    if dataset_cfg[\u0027name\u0027] == \u0027RANDOM_TINY\u0027:\n        return dataset_cfg.get(\u0027num_classes\u0027, 10)\n    # PLACEHOLDER: Adjust when adding real datasets\n    return dataset_cfg.get(\u0027num_classes\u0027)\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experimental framework.\"\nrequires-python = \"\u003e=3.9\"\nauthors = [\n  { name = \"AI Researcher\", email = \"noreply@example.com\" }\n]\nlicense = { text = \"MIT\" }\n\n[project.dependencies]\n# Deep learning\ntorch = \"*\"\n# Data \u0026 utils\nnumpy = \"*\"\nscikit-learn = \"*\"\n# Plotting\nmatplotlib = \"*\"\nseaborn = \"*\"\n# Progress bars\ntqdm = \"*\"\n# YAML\nPyYAML = \"*\"\n# DataFrames\npandas = \"*\"\n", "smoke_test_yaml": "experiments:\n  - run_id: baseline_random\n    dataset:\n      name: RANDOM_TINY\n      batch_size: 64\n      num_classes: 10\n      num_features: 32\n    model:\n      name: baseline_mlp\n      input_dim: 32\n    hyperparameters:\n      lr: 0.1\n      weight_decay: 0.0005\n      momentum: 0.9\n    training:\n      epochs: 3\n    use_ohgw: false\n    quick_run: true\n\n  - run_id: ohgw_random\n    dataset:\n      name: RANDOM_TINY\n      batch_size: 64\n      num_classes: 10\n      num_features: 32\n    model:\n      name: proposed_mlp\n      input_dim: 32\n    hyperparameters:\n      lr: 0.1\n      weight_decay: 0.0005\n      momentum: 0.9\n    training:\n      epochs: 3\n    use_ohgw: true\n    eta_h: 0.001\n    quick_run: true\n", "train_py": "import argparse\nimport json\nimport os\nimport random\nimport sys\nimport time\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# Local imports\nfrom .preprocess import build_dataloaders, get_num_classes\nfrom .model import build_model\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef apply_hypergradient_warmstart(model: nn.Module,\n                                  dataloader: torch.utils.data.DataLoader,\n                                  hyperparams: dict,\n                                  device: torch.device,\n                                  eta_h: float):\n    \"\"\"Perform a single mini-batch hyper-gradient warm-start step.\n\n    Only hyper-parameters that are `torch.Tensor` with `requires_grad=True`\n    will receive gradients. This design makes the routine generic and\n    extensible: any continuous hyper-parameter you wish to optimise merely\n    has to be represented as such a tensor and referenced in the forward\n    pass or auxiliary regularisation term.\n    \"\"\"\n    model.train()\n    # Grab exactly one mini-batch\n    try:\n        batch = next(iter(dataloader))\n    except StopIteration:\n        raise RuntimeError(\"Dataloader exhausted while attempting warm-start mini-batch\")\n\n    inputs, targets = batch[0].to(device), batch[1].to(device)\n\n    # Forward\n    outputs = model(inputs)\n    criterion = nn.CrossEntropyLoss()\n    loss = criterion(outputs, targets)\n\n    # Optional: include weight-decay L2 penalty if weight_decay hyper-param supplied\n    if \u0027weight_decay\u0027 in hyperparams:\n        wd = hyperparams[\u0027weight_decay\u0027]  # tensor\n        l2_norm = sum((p ** 2).sum() for p in model.parameters())\n        loss = loss + 0.5 * wd * l2_norm\n\n    # Backward wrt model params AND hyper-params\n    grads = torch.autograd.grad(loss, list(model.parameters()) + list(hyperparams.values()), create_graph=False)\n    hyper_grads = grads[len(list(model.parameters())):]  # last elements correspond to hyperparams\n\n    # Apply one SGD step on hyper-parameters\n    with torch.no_grad():\n        for (name, tensor), g in zip(hyperparams.items(), hyper_grads):\n            if g is None:\n                continue  # hyper-param did not participate in graph\n            tensor -= eta_h * g\n            # Clamp common hyper-parameters to valid range\n            if name == \u0027weight_decay\u0027:\n                tensor.clamp_(min=0.0, max=1.0)\n            if name == \u0027lr\u0027:\n                tensor.clamp_(min=1e-6, max=10.0)\n\n    return {name: float(tensor.detach().cpu().item()) for name, tensor in hyperparams.items()}\n\n\ndef train(config_path: str, results_dir: str, gpu_id: int):\n    # ------------------------------------------------------------------\n    # 1. Load configuration\n    # ------------------------------------------------------------------\n    with open(config_path, \u0027r\u0027) as f:\n        cfg = yaml.safe_load(f)\n\n    run_id = cfg[\u0027run_id\u0027]\n\n    # ------------------------------------------------------------------\n    # 2. Reproducibility and device placement\n    # ------------------------------------------------------------------\n    seed = cfg.get(\u0027seed\u0027, 42)\n    set_seed(seed)\n    device = torch.device(f\u0027cuda:{gpu_id}\u0027 if torch.cuda.is_available() else \u0027cpu\u0027)\n\n    # ------------------------------------------------------------------\n    # 3. Data\n    # ------------------------------------------------------------------\n    loaders = build_dataloaders(cfg[\u0027dataset\u0027], quick_run=cfg.get(\u0027quick_run\u0027, False))\n    train_loader, val_loader, test_loader = loaders\n    num_classes = get_num_classes(cfg[\u0027dataset\u0027])\n\n    # ------------------------------------------------------------------\n    # 4. Model \u0026 Optimiser\n    # ------------------------------------------------------------------\n    model = build_model(cfg[\u0027model\u0027], num_classes=num_classes).to(device)\n\n    # --- Hyper-parameters as differentiable tensors --------------------------------\n    hyperparams = {}\n    hp_cfg = cfg.get(\u0027hyperparameters\u0027, {})\n    # Learning-rate (log space is common but here we keep real value for simplicity)\n    lr_tensor = torch.tensor(float(hp_cfg.get(\u0027lr\u0027, 0.1)), dtype=torch.float32, device=device, requires_grad=True)\n    hyperparams[\u0027lr\u0027] = lr_tensor\n    wd_tensor = torch.tensor(float(hp_cfg.get(\u0027weight_decay\u0027, 5e-4)), dtype=torch.float32, device=device, requires_grad=True)\n    hyperparams[\u0027weight_decay\u0027] = wd_tensor\n\n    # Build optimiser using the CURRENT lr / wd values (will be mutated later)\n    optimiser = optim.SGD(model.parameters(), lr=float(lr_tensor.detach().cpu()), momentum=hp_cfg.get(\u0027momentum\u0027, 0.9))\n\n    # ------------------------------------------------------------------\n    # 5. One-Shot Hyper-Gradient Warm-Start (OHGW) \u2014 if enabled\n    # ------------------------------------------------------------------\n    if cfg.get(\u0027use_ohgw\u0027, False):\n        eta_h = cfg.get(\u0027eta_h\u0027, 1e-3)\n        _updated_vals = apply_hypergradient_warmstart(model, train_loader, hyperparams, device, eta_h)\n        # Reflect updated hyper-params inside optimiser param groups\n        optimiser.param_groups[0][\u0027lr\u0027] = _updated_vals[\u0027lr\u0027]\n        print(json.dumps({\n            \u0027run_id\u0027: run_id,\n            \u0027event\u0027: \u0027hypergradient_warmstart_completed\u0027,\n            \u0027updated_hyperparams\u0027: _updated_vals\n        }))\n        sys.stdout.flush()\n\n    # ------------------------------------------------------------------\n    # 6. Training loop\n    # ------------------------------------------------------------------\n    criterion = nn.CrossEntropyLoss()\n\n    epochs = cfg[\u0027training\u0027][\u0027epochs\u0027]\n    metrics_history = {\n        \u0027epoch\u0027: [],\n        \u0027train_loss\u0027: [],\n        \u0027val_acc\u0027: []\n    }\n\n    start_time = time.time()\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            optimiser.zero_grad(set_to_none=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            if \u0027weight_decay\u0027 in hyperparams:\n                wd = hyperparams[\u0027weight_decay\u0027]\n                l2_norm = sum((p ** 2).sum() for p in model.parameters())\n                loss = loss + 0.5 * wd * l2_norm\n\n            loss.backward()\n            optimiser.step()\n\n            running_loss += loss.item() * inputs.size(0)\n\n        train_loss_epoch = running_loss / len(train_loader.dataset)\n\n        # Validation\n        model.eval()\n        val_preds, val_targets = [], []\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs = inputs.to(device)\n                outputs = model(inputs)\n                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n                val_preds.append(preds)\n                val_targets.append(targets.numpy())\n        val_preds = np.concatenate(val_preds)\n        val_targets = np.concatenate(val_targets)\n        val_acc = accuracy_score(val_targets, val_preds)\n\n        metrics_history[\u0027epoch\u0027].append(epoch)\n        metrics_history[\u0027train_loss\u0027].append(train_loss_epoch)\n        metrics_history[\u0027val_acc\u0027].append(val_acc)\n\n        # Live log epoch metrics\n        print(json.dumps({\n            \u0027run_id\u0027: run_id,\n            \u0027event\u0027: \u0027epoch_end\u0027,\n            \u0027epoch\u0027: epoch,\n            \u0027train_loss\u0027: train_loss_epoch,\n            \u0027val_acc\u0027: val_acc\n        }))\n        sys.stdout.flush()\n\n    training_wallclock_s = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # 7. Test Evaluation\n    # ------------------------------------------------------------------\n    model.eval()\n    test_preds, test_targets = [], []\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            test_preds.append(preds)\n            test_targets.append(targets.numpy())\n    test_preds = np.concatenate(test_preds)\n    test_targets = np.concatenate(test_targets)\n    test_acc = accuracy_score(test_targets, test_preds)\n\n    # ------------------------------------------------------------------\n    # 8. Persist metrics \u0026 artefacts\n    # ------------------------------------------------------------------\n    run_dir = Path(results_dir) / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    results = {\n        \u0027run_id\u0027: run_id,\n        \u0027config\u0027: cfg,\n        \u0027final_metrics\u0027: {\n            \u0027val_acc\u0027: float(metrics_history[\u0027val_acc\u0027][-1]),\n            \u0027test_acc\u0027: float(test_acc),\n            \u0027training_wallclock_s\u0027: training_wallclock_s\n        },\n        \u0027epoch_metrics\u0027: metrics_history\n    }\n\n    with (run_dir / \u0027results.json\u0027).open(\u0027w\u0027) as f:\n        json.dump(results, f, indent=2)\n\n    # ------------------------------------------------------------------\n    # 9. Generate Figures\n    # ------------------------------------------------------------------\n    # 9.1 Training Loss Curve\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=metrics_history[\u0027epoch\u0027], y=metrics_history[\u0027train_loss\u0027], marker=\u0027o\u0027, label=\u0027Train Loss\u0027)\n    final_loss = metrics_history[\u0027train_loss\u0027][-1]\n    plt.annotate(f\"{final_loss:.4f}\", xy=(metrics_history[\u0027epoch\u0027][-1], final_loss),\n                 xytext=(-20, 10), textcoords=\u0027offset points\u0027)\n    plt.xlabel(\u0027Epoch\u0027)\n    plt.ylabel(\u0027Loss\u0027)\n    plt.title(\u0027Training Loss Curve\u0027)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(run_dir / \u0027training_loss.pdf\u0027, bbox_inches=\u0027tight\u0027)\n    plt.close()\n\n    # 9.2 Validation Accuracy Curve\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=metrics_history[\u0027epoch\u0027], y=metrics_history[\u0027val_acc\u0027], marker=\u0027o\u0027, label=\u0027Val Acc\u0027)\n    final_acc = metrics_history[\u0027val_acc\u0027][-1]\n    plt.annotate(f\"{final_acc:.4f}\", xy=(metrics_history[\u0027epoch\u0027][-1], final_acc),\n                 xytext=(-20, 10), textcoords=\u0027offset points\u0027)\n    plt.xlabel(\u0027Epoch\u0027)\n    plt.ylabel(\u0027Accuracy\u0027)\n    plt.title(\u0027Validation Accuracy Curve\u0027)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(run_dir / \u0027accuracy.pdf\u0027, bbox_inches=\u0027tight\u0027)\n    plt.close()\n\n    # 9.3 Confusion Matrix on Test Set (annotated)\n    cm = confusion_matrix(test_targets, test_preds)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\u0027d\u0027, cmap=\u0027Blues\u0027)\n    plt.title(\u0027Test Confusion Matrix\u0027)\n    plt.ylabel(\u0027True label\u0027)\n    plt.xlabel(\u0027Predicted label\u0027)\n    plt.tight_layout()\n    plt.savefig(run_dir / \u0027confusion_matrix.pdf\u0027, bbox_inches=\u0027tight\u0027)\n    plt.close()\n\n    # ------------------------------------------------------------------\n    # 10. Print final results to stdout\n    # ------------------------------------------------------------------\n    experiment_description = (\n        f\"Run ID: {run_id}\\n\"\n        f\"Dataset: {cfg[\u0027dataset\u0027][\u0027name\u0027]}\\n\"\n        f\"Model:   {cfg[\u0027model\u0027][\u0027name\u0027]}\\n\"\n        f\"Use OHGW: {cfg.get(\u0027use_ohgw\u0027, False)}\\n\"\n        f\"Epochs:  {epochs}\\n\"\n        f\"Final Validation Accuracy: {metrics_history[\u0027val_acc\u0027][-1]:.4f}\\n\"\n        f\"Final Test Accuracy:       {test_acc:.4f}\\n\"\n        f\"Training Wall-clock (s):  {training_wallclock_s:.1f}\"\n    )\n    print(experiment_description)\n\n    print(json.dumps({\n        \u0027run_id\u0027: run_id,\n        \u0027event\u0027: \u0027training_completed\u0027,\n        \u0027final_val_acc\u0027: metrics_history[\u0027val_acc\u0027][-1],\n        \u0027final_test_acc\u0027: test_acc,\n        \u0027training_wallclock_s\u0027: training_wallclock_s,\n        \u0027figures\u0027: [\u0027training_loss.pdf\u0027, \u0027accuracy.pdf\u0027, \u0027confusion_matrix.pdf\u0027]\n    }))\n    sys.stdout.flush()\n\n\nif __name__ == \u0027__main__\u0027:\n    parser = argparse.ArgumentParser(description=\u0027Run a single training experiment variation.\u0027)\n    parser.add_argument(\u0027--config\u0027, required=True, type=str, help=\u0027Path to YAML file describing this run.\u0027)\n    parser.add_argument(\u0027--results-dir\u0027, required=True, type=str, help=\u0027Directory to save results and artefacts.\u0027)\n    parser.add_argument(\u0027--gpu-id\u0027, required=True, type=int, help=\u0027GPU index to use for this run.\u0027)\n    args = parser.parse_args()\n\n    train(args.config, args.results_dir, args.gpu_id)\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Core requirement violated: model persistence is missing. train.py neither checkpoints the network during training nor provides a load routine, so reproducibility / later re-evaluation cannot be guaranteed. All other foundation elements are present, but absence of save/load logic means criterion 1 (complete core logic) is not fully met."
}
