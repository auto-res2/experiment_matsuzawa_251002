
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation
   - main.py implements 1 GPU per run variation allocation, queueing runs sequentially when GPUs are insufficient
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all experiment variations and produces comparison figures.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # head-less\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nFIG_TOPICS = [\n    (\"training_loss\", \"train_loss\"),\n    (\"accuracy\", \"val_accuracy\"),\n]\n\n\ndef load_results(results_dir: Path) -\u003e List[Dict]:\n    runs = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                runs.append(json.load(f))\n    return runs\n\n\ndef plot_metric_over_epochs(runs: List[Dict], metric_key: str, filename: str, results_dir: Path):\n    plt.figure(figsize=(6, 4))\n    for res in runs:\n        history = pd.DataFrame(res[\"history\"])\n        sns.lineplot(x=\"epoch\", y=metric_key, data=history, label=res[\"run_id\"])\n        # annotate final value\n        final_val = history[metric_key].iloc[-1]\n        plt.annotate(f\"{final_val:.3f}\", (history[\"epoch\"].iloc[-1], final_val))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(metric_key.replace(\"_\", \" \").title())\n    plt.legend()\n    plt.tight_layout()\n    outpath = results_dir / filename\n    plt.savefig(outpath, format=\"pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef summary_table(runs: List[Dict]):\n    records = []\n    for res in runs:\n        records.append({\n            \"run_id\": res[\"run_id\"],\n            \"test_accuracy\": res[\"test_accuracy\"],\n            \"test_loss\": res[\"test_loss\"],\n        })\n    return pd.DataFrame.from_records(records)\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    results_dir = Path(args.results_dir)\n    runs = load_results(results_dir)\n    if len(runs) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    # Plot metrics\n    for topic, key in FIG_TOPICS:\n        plot_metric_over_epochs(runs, key, f\"{topic}.pdf\", results_dir)\n\n    # Summary table\n    table = summary_table(runs)\n\n    # Print JSON summary to STDOUT\n    print(table.to_json(orient=\"records\"))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# Template for the *real* experiments. Placeholders must be replaced with actual\n# dataset / model / scheduler details in a subsequent derivation step.\n\nexperiments:\n  - run_id: BASELINE_SCHEDULER  # e.g. ASHA\n    dataset: DATASET_PLACEHOLDER  # PLACEHOLDER\n    model: MODEL_PLACEHOLDER      # PLACEHOLDER\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    hyperparameters:\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      weight_decay: SPECIFIC_CONFIG_PLACEHOLDER\n      momentum: SPECIFIC_CONFIG_PLACEHOLDER\n    ohgw:\n      enabled: false\n\n  - run_id: BASELINE_SCHEDULER_OHGW  # e.g. ASHA+OHGW\n    dataset: DATASET_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    hyperparameters:\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      weight_decay: SPECIFIC_CONFIG_PLACEHOLDER\n      momentum: SPECIFIC_CONFIG_PLACEHOLDER\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n\n# Additional variations (random warm-start, multi-step OHGW, ablations, etc.)\n# will be appended here by the derive_specific step.\n\n# Note: main.py will run each entry with exactly one GPU.\n\n# -----------------------------------------------------------------------------\n# GLOBAL CONFIGURATION (optional): seed, dataset paths \u2026 can be added at the\n# top-level if needed and will be inherited by each experiment unless overridden.\n# -----------------------------------------------------------------------------\nfull_experiment_seed: 42\n\n# end of file\n", "main_py": "\"\"\"src/main.py\nCoordinator that launches every experiment variation defined in the YAML config\nfile, allocates exactly one GPU per variation, collects logs, and triggers the\npost-hoc comparison plots.\n\"\"\"\nimport argparse\nimport json\nimport os\nimport queue\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport torch\nimport yaml\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / \"config\"\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\n\ndef tee_stream(src, dst_file, dst_stream):\n    \"\"\"Reads lines from a stream and writes them to both a file \u0026 another stream.\"\"\"\n    with dst_file:\n        for line in iter(src.readline, \"\"):\n            dst_file.write(line)\n            dst_file.flush()\n            dst_stream.write(line)\n            dst_stream.flush()\n    src.close()\n\n\ndef start_subprocess(cmd: List[str], run_dir: Path, env: Dict[str, str]):\n    \"\"\"Launch subprocess and duplicate its stdout/stderr into log files.\"\"\"\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1, env=env)\n\n    # Threads to tee stdout / stderr\n    threading.Thread(\n        target=tee_stream,\n        args=(proc.stdout, open(stdout_path, \"w\"), sys.stdout),\n        daemon=True,\n    ).start()\n    threading.Thread(\n        target=tee_stream,\n        args=(proc.stderr, open(stderr_path, \"w\"), sys.stderr),\n        daemon=True,\n    ).start()\n    return proc\n\n\n# -----------------------------------------------------------------------------\n# Main orchestration\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--smoke-test\", action=\"store_true\", default=False)\n    p.add_argument(\"--full-experiment\", action=\"store_true\", default=False)\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    return p.parse_args()\n\n\ndef load_config(smoke_test: bool) -\u003e Dict[str, Any]:\n    if smoke_test:\n        cfg_path = CONFIG_DIR / \"smoke_test.yaml\"\n    else:\n        cfg_path = CONFIG_DIR / \"full_experiment.yaml\"\n    with open(cfg_path) as f:\n        return yaml.safe_load(f)\n\n\ndef main():\n    args = parse_args()\n    if not (args.smoke_test ^ args.full_experiment):\n        raise ValueError(\"Exactly one of --smoke-test or --full-experiment must be supplied.\")\n\n    cfg = load_config(args.smoke_test)\n    experiments: List[Dict[str, Any]] = cfg.get(\"experiments\", [])\n    if len(experiments) == 0:\n        raise RuntimeError(\"No experiments found in config file.\")\n\n    results_dir = Path(args.results_dir).resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    n_gpus = torch.cuda.device_count()\n    gpu_queue = queue.Queue()\n    for i in range(n_gpus):\n        gpu_queue.put(i)\n\n    processes = []\n    experiment_queue = queue.Queue()\n    for exp in experiments:\n        experiment_queue.put(exp)\n\n    def launch_next_if_possible():\n        while not experiment_queue.empty() and not gpu_queue.empty():\n            exp_cfg = experiment_queue.get()\n            gpu_id = gpu_queue.get()\n            run_id = exp_cfg[\"run_id\"]\n            run_dir = results_dir / run_id\n            run_dir.mkdir(parents=True, exist_ok=True)\n\n            # Write per-run config to the run directory for full provenance.\n            cfg_path = run_dir / \"config.yaml\"\n            with open(cfg_path, \"w\") as f:\n                yaml.safe_dump(exp_cfg, f)\n\n            cmd = [\n                sys.executable,\n                \"-m\",\n                \"src.train\",\n                \"--config\",\n                str(cfg_path),\n                \"--run-id\",\n                run_id,\n                \"--results-dir\",\n                str(results_dir),\n            ]\n            if args.smoke_test:\n                cmd.append(\"--smoke-test\")\n\n            # Environment: pin process to exactly one GPU.\n            env = os.environ.copy()\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n\n            proc = start_subprocess(cmd, run_dir, env)\n            processes.append((proc, gpu_id))\n\n    # Kick-off initial batch.\n    launch_next_if_possible()\n\n    # Monitor.\n    while processes:\n        for proc, gpu_id in list(processes):  # copy to allow removal inside loop\n            ret = proc.poll()\n            if ret is not None:  # finished\n                processes.remove((proc, gpu_id))\n                gpu_queue.put(gpu_id)  # release GPU\n                launch_next_if_possible()\n        if processes:\n            # Avoid busy-waiting.\n            for _ in range(5):\n                if any(proc.poll() is not None for proc, _ in processes):\n                    break\n                else:\n                    import time\n\n                    time.sleep(1)\n\n    # ------------------------------------------------------------------\n    # All training done \u2192 call evaluator.\n    # ------------------------------------------------------------------\n    cmd_eval = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(cmd_eval, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel architectures and hyper-parameter container.\n\"\"\"\nimport torch\nimport torch.nn as nn\nfrom typing import Dict\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Minimal CNN good enough for synthetic data \u0026 unit-tests.\"\"\"\n\n    def __init__(self, num_classes: int = 10, input_channels: int = 3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(input_channels, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass HParamDict(nn.Module):\n    \"\"\"Container that treats hyper-parameters as differentiable tensors.\"\"\"\n\n    def __init__(self, hparam_cfg: Dict[str, float]):\n        super().__init__()\n        for name, value in hparam_cfg.items():\n            # Only continuous hyper-parameters are made differentiable.\n            param = nn.Parameter(torch.tensor(float(value), dtype=torch.float32), requires_grad=True)\n            self.register_parameter(name, param)\n\n\n# -----------------------------------------------------------------------------\n# Factory\n# -----------------------------------------------------------------------------\n\ndef get_model(cfg: Dict, ds_info: Dict, smoke_test: bool = False) -\u003e nn.Module:\n    model_name = cfg.get(\"model\", \"MODEL_PLACEHOLDER\")\n\n    if smoke_test or model_name in {\"MODEL_PLACEHOLDER\", \"DUMMY_MODEL\", \"SIMPLE_CNN\"}:\n        return SimpleCNN(num_classes=ds_info[\"num_classes\"], input_channels=ds_info[\"input_shape\"][0])\n\n    # ----------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with model-specific construction logic.\n    # ----------------------------------------------------------------------\n    raise NotImplementedError(f\"Model \u0027{model_name}\u0027 is not implemented yet.\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon preprocessing \u0026 dataloader utilities with dataset placeholders.\n\"\"\"\nfrom typing import Tuple, Dict\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass SyntheticClassificationDataset(Dataset):\n    \"\"\"A tiny random dataset used for smoke-tests or placeholder experiments.\"\"\"\n\n    def __init__(self, num_samples: int = 256, num_classes: int = 2, input_shape=(3, 32, 32)):\n        self.num_samples = num_samples\n        self.num_classes = num_classes\n        self.input_shape = input_shape\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = torch.randn(*self.input_shape)\n        y = torch.randint(0, self.num_classes, (1,)).item()\n        return x, y\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(\n    cfg: Dict,\n    smoke_test: bool = False,\n) -\u003e Tuple[DataLoader, DataLoader, DataLoader, Dict]:\n    \"\"\"Return train/val/test loaders *and* dataset meta-information.\n\n    Placeholders are used until replaced in specific experiment derivations.\n    \"\"\"\n    dataset_name = cfg.get(\"dataset\", \"DATASET_PLACEHOLDER\")\n    batch_size = int(cfg.get(\"training\", {}).get(\"batch_size\", 128))\n\n    if smoke_test or dataset_name in {\"DATASET_PLACEHOLDER\", \"DUMMY_DATASET\"}:\n        # Tiny synthetic data so the smoke-test finishes in \u003c2 seconds.\n        train_ds = SyntheticClassificationDataset(num_samples=64)\n        val_ds = SyntheticClassificationDataset(num_samples=32)\n        test_ds = SyntheticClassificationDataset(num_samples=32)\n        num_classes = 2\n        input_shape = (3, 32, 32)\n    else:\n        # ------------------------------------------------------------------\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic.\n        # ------------------------------------------------------------------\n        raise NotImplementedError(\n            f\"Dataset loading for \u0027{dataset_name}\u0027 has not been implemented yet.\"\n        )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader, test_loader, {\n        \"num_classes\": num_classes,\n        \"input_shape\": input_shape,\n    }\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nrequires-python = \"\u003e=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\npyyaml = \"*\"\npandas = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\ntqdm = \"*\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# A *very* small configuration that can finish within seconds on CPU / GPU,\n# used by CI for quick regression checks.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: DUMMY_DATASET  # PLACEHOLDER: synthetic data is used automatically\n    model: DUMMY_MODEL      # PLACEHOLDER: SimpleCNN is used automatically\n    training:\n      epochs: 1\n      batch_size: 32\n    hyperparameters:\n      lr: 0.01\n      weight_decay: 0.0\n      momentum: 0.0\n    ohgw:\n      enabled: false\n\n  - run_id: dummy_ohgw\n    dataset: DUMMY_DATASET  # synthetic\n    model: DUMMY_MODEL\n    training:\n      epochs: 1\n      batch_size: 32\n    hyperparameters:\n      lr: 0.01\n      weight_decay: 0.0\n      momentum: 0.0\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n", "train_py": "\"\"\"src/train.py\nRuns a single experiment variation. All results (metrics, model, logs) are written\ninto \u003cresults_dir\u003e/\u003crun_id\u003e/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\nfrom src import preprocess\nfrom src import model as model_lib\n\n\n# -----------------------------------------------------------------------------\n# OHGW \u2014 One-Shot Hyper-Gradient Warm-Start utilities\n# -----------------------------------------------------------------------------\nclass OHGW:\n    \"\"\"Utility class that performs one stochastic hyper-gradient step.\"\"\"\n\n    @staticmethod\n    def apply(\n        model: nn.Module,\n        criterion: nn.Module,\n        hparams: model_lib.HParamDict,\n        batch,\n        device,\n        eta_h: float = 1e-3,\n    ) -\u003e None:\n        \"\"\"Compute \u2202L/\u2202\u03c8 for every hyper-parameter \u03c8 and do \u03c8\u2190\u03c8\u2212\u03b7_h\u2202L/\u2202\u03c8.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n        criterion : loss function\n        hparams : HParamDict containing *continuous* hyper-parameters\n        batch : (inputs, targets)\n        device : torch device\n        eta_h : learning-rate in hyper-parameter space (fixed, small)\n        \"\"\"\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        # Forward once.\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # Compute gradient of loss w.r.t hyper-parameters (very cheap).\n        grads = torch.autograd.grad(\n            loss, tuple(hparams.parameters()), retain_graph=False, create_graph=False\n        )\n\n        # One SGD step in hyper-parameter space (in-place, no grad\u2010tracking).\n        with torch.no_grad():\n            for p, g in zip(hparams.parameters(), grads):\n                if g is None:\n                    continue  # Hyper-parameter not connected to graph\n                p -= eta_h * g\n\n\n# -----------------------------------------------------------------------------\n# Training / evaluation helpers\n# -----------------------------------------------------------------------------\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    return running_loss / len(loader.dataset)\n\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_targets.extend(targets.cpu().numpy())\n    avg_loss = running_loss / len(loader.dataset)\n    acc = accuracy_score(all_targets, all_preds)\n    return avg_loss, acc\n\n\n# -----------------------------------------------------------------------------\n# Main entry point\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config\", type=str, required=True)\n    p.add_argument(\"--run-id\", type=str, required=True)\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    p.add_argument(\"--smoke-test\", action=\"store_true\", default=False)\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    with open(args.config, \"r\") as f:\n        cfg: Dict[str, Any] = json.load(f) if args.config.endswith(\".json\") else __import__(\"yaml\").safe_load(f)\n\n    run_id = args.run_id\n    results_root = Path(args.results_dir)\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Reproducibility \u0026 device setup\n    # ------------------------------------------------------------------\n    seed = cfg.get(\"seed\", 42)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, test_loader, ds_info = preprocess.get_dataloaders(\n        cfg, smoke_test=args.smoke_test\n    )\n\n    # ------------------------------------------------------------------\n    # Model \u0026 hyper-parameters\n    # ------------------------------------------------------------------\n    model = model_lib.get_model(cfg, ds_info, smoke_test=args.smoke_test).to(device)\n\n    hparam_dict = model_lib.HParamDict(cfg.get(\"hyperparameters\", {})).to(device)\n\n    # Attach hyper-parameters to model for automatic inclusion in `.parameters()`\n    model.register_module(\"hparams\", hparam_dict)\n\n    # Optimizer (using possibly updated hyper-parameters later)\n    lr = float(cfg.get(\"hyperparameters\", {}).get(\"lr\", 1e-3))\n    weight_decay = float(cfg.get(\"hyperparameters\", {}).get(\"weight_decay\", 0.0))\n    momentum = float(cfg.get(\"hyperparameters\", {}).get(\"momentum\", 0.0))\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n    criterion = nn.CrossEntropyLoss()\n\n    # ------------------------------------------------------------------\n    # One-Shot Hyper-Gradient Warm-Start (OHGW)\n    # ------------------------------------------------------------------\n    if cfg.get(\"ohgw\", {}).get(\"enabled\", False):\n        eta_h = float(cfg.get(\"ohgw\", {}).get(\"eta_h\", 1e-3))\n        first_batch = next(iter(train_loader))\n        OHGW.apply(model, criterion, hparam_dict, first_batch, device, eta_h=eta_h)\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    epochs = int(cfg.get(\"training\", {}).get(\"epochs\", 10))\n    history = []\n    for epoch in range(1, epochs + 1):\n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        history.append({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n        })\n        print(json.dumps({\n            \"run_id\": run_id,\n            \"event\": \"epoch_complete\",\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n        }), flush=True)\n\n    # ------------------------------------------------------------------\n    # Final evaluation on test set\n    # ------------------------------------------------------------------\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n\n    # Save model\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n\n    # Save results\n    results = {\n        \"run_id\": run_id,\n        \"config\": cfg,\n        \"history\": history,\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc,\n    }\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Emit final JSON to stdout (for GitHub Actions log capture)\n    print(json.dumps({\n        \"run_id\": run_id,\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc,\n    }))\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Training code is not runnable: train.py calls model.register_module(\"hparams\", hparam_dict) but nn.Module has no register_module method (should use add_module or setattr). This raises AttributeError and prevents the training loop/evaluation from running, so the core logic is incomplete/unverified, violating requirement 1."
}
