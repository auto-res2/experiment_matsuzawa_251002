
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation
   - main.py implements 1 GPU per run variation allocation, queueing runs sequentially when GPUs are insufficient
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "\"\"\"\nEvaluation \u0026 Visualisation Script (COMMON CORE FOUNDATION)\n=========================================================\n\u2022 Collects all \u003cresults_dir\u003e/\u003crun_id\u003e/results.json produced by train.py.\n\u2022 Computes comparative statistics and prints them as JSON.\n\u2022 Generates publication-ready .pdf figures with numeric annotations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport argparse\nfrom matplotlib.ticker import MaxNLocator\n\nsns.set_style(\"whitegrid\")\n\n\n# ------------------------- Helper Functions ------------------------- #\n\ndef _load_all_results(results_root: Path) -\u003e List[Dict]:\n    res = []\n    for results_json in results_root.glob(\"*/results.json\"):\n        with results_json.open(\"r\", encoding=\"utf-8\") as f:\n            res.append(json.load(f))\n    if not res:\n        raise RuntimeError(f\"No results.json files found under {results_root}\")\n    return res\n\n\ndef _plot_training_curves(results: List[Dict], metric: str, ylabel: str, filename: str, results_root: Path) -\u003e None:\n    plt.figure(figsize=(6, 4))\n    for run in results:\n        epochs = [m[\"epoch\"] for m in run[\"metrics_per_epoch\"]]\n        values = [m[metric] for m in run[\"metrics_per_epoch\"]]\n        plt.plot(epochs, values, label=run[\"run_id\"])\n        # Annotate final value\n        plt.annotate(f\"{values[-1]:.3f}\", xy=(epochs[-1], values[-1]), xytext=(5, 0), textcoords=\"offset points\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.legend()\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    out_path = results_root / f\"{filename}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef _plot_final_metric_bar(results: List[Dict], metric: str, ylabel: str, filename: str, results_root: Path) -\u003e None:\n    plt.figure(figsize=(6, 4))\n    run_ids = [r[\"run_id\"] for r in results]\n    values = [r[metric] for r in results]\n    sns.barplot(x=run_ids, y=values, palette=\"muted\")\n    ax = plt.gca()\n    for i, v in enumerate(values):\n        ax.annotate(f\"{v:.3f}\", xy=(i, v), xytext=(0, 3), textcoords=\"offset points\", ha=\"center\")\n    plt.ylabel(ylabel)\n    plt.xlabel(\"Run ID\")\n    out_path = results_root / f\"{filename}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\n# ---------------------------- CLI ---------------------------- #\n\ndef parse_args() -\u003e argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Evaluate and plot COMMON CORE experiments\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root dir containing run sub-folders with results.json\")\n    return parser.parse_args()\n\n\ndef main() -\u003e None:\n    args = parse_args()\n    results_root = Path(args.results_dir).expanduser().resolve()\n\n    # Load -------------------------------------------------------------------\n    runs = _load_all_results(results_root)\n\n    # Figures ---------------------------------------------------------------\n    _plot_training_curves(runs, metric=\"train_loss\", ylabel=\"Training Loss\", filename=\"training_loss\", results_root=results_root)\n    _plot_training_curves(runs, metric=\"val_accuracy\", ylabel=\"Validation Accuracy\", filename=\"val_accuracy\", results_root=results_root)\n    _plot_final_metric_bar(runs, metric=\"final_val_accuracy\", ylabel=\"Final Validation Accuracy\", filename=\"final_val_accuracy\", results_root=results_root)\n\n    # Comparative statistics ------------------------------------------------\n    comparison = {\n        run[\"run_id\"]: {\n            \"final_val_accuracy\": run[\"final_val_accuracy\"],\n            \"best_val_accuracy\": run[\"best_val_accuracy\"],\n            \"final_train_loss\": run[\"final_train_loss\"],\n        }\n        for run in runs\n    }\n\n    # Output description + JSON --------------------------------------------\n    print(\"\\n=============== Evaluation Summary ===============\")\n    print(\"Compared runs:\", \", \".join(comparison.keys()))\n    print(\"Figures generated: training_loss.pdf, val_accuracy.pdf, final_val_accuracy.pdf\")\n    print(\"=================================================\\n\")\n\n    print(json.dumps({\"comparison\": comparison}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# Full experiment template (placeholders only). Populate in specialised step.\nexperiments:\n  # --------------------------------------------------------------------\n  # Example:\n  # - run_id: ASHA_CIFAR10_BASELINE\n  #   model: MODEL_PLACEHOLDER\n  #   dataset: DATASET_PLACEHOLDER\n  #   training: SPECIFIC_CONFIG_PLACEHOLDER\n  #   ohgw:\n  #     enabled: false\n  # --------------------------------------------------------------------\n\n  # PLACEHOLDER: Add full experiment configurations here.\n  \n  \n", "main_py": "\"\"\"\nMain Orchestrator for COMMON CORE Experiments\n============================================\nHandles:\n\u2022 Reading the YAML experiment file (smoke or full).\n\u2022 Scheduling each run (train.py) with 1 GPU per experiment.\n\u2022 Concurrent execution if #GPUs \u2265 #concurrent runs, otherwise sequential queue.\n\u2022 Tee-like logging to per-run stdout/stderr files **and** forwarding to the main\n  process\u0027 stdout/stderr for GitHub Actions visibility.\n\u2022 After all runs finish, triggers evaluate.py.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport yaml\nimport torch\n\n# ------------------------- Helper Functions ------------------------- #\n\ndef _read_yaml(path: Path) -\u003e Dict[str, Any]:\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)\n\n\ndef _available_gpus() -\u003e List[int]:\n    if not torch.cuda.is_available():\n        return []\n    return list(range(torch.cuda.device_count()))\n\n\nclass _Tee(threading.Thread):\n    \"\"\"Thread that copies pipe stream to both console and file.\"\"\"\n\n    def __init__(self, stream, log_file: Path):\n        super().__init__(daemon=True)\n        self.stream = stream\n        self.log_f = log_file.open(\"wb\")\n\n    def run(self):  # noqa: D401\n        for chunk in iter(lambda: self.stream.read(1024), b\"\"):  # noqa: WPS432\n            if not chunk:\n                break\n            sys.stdout.buffer.write(chunk) if self.stream is subprocess.PIPE else None  # noqa: WPS465\n            self.log_f.write(chunk)\n            self.log_f.flush()\n        self.stream.close()\n        self.log_f.close()\n\n\n# ------------------------- Experiment Runner ------------------------ #\n\ndef _launch_experiment(exp_cfg: Dict[str, Any], cfg_path: Path, results_dir: Path, gpu_id: int, smoke: bool):\n    env = os.environ.copy()\n    if gpu_id \u003e= 0:\n        env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n    else:\n        env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n    run_id = exp_cfg[\"run_id\"]\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--config\",\n        str(cfg_path),\n        \"--run-id\",\n        run_id,\n        \"--gpu-id\",\n        str(gpu_id),\n        \"--results-dir\",\n        str(results_dir),\n    ]\n    if smoke:\n        cmd.append(\"--smoke-test\")\n\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n\n    proc = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        bufsize=1,\n    )\n\n    # Start tee threads\n    _Tee(proc.stdout, stdout_path).start()  # type: ignore[arg-type]\n    _Tee(proc.stderr, stderr_path).start()  # type: ignore[arg-type]\n    return proc\n\n\n# ---------------------------- CLI ----------------------------------- #\n\ndef parse_args() -\u003e argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"COMMON CORE experiment orchestrator\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke tests defined in smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiments defined in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store all outputs (figures, logs, metrics)\")\n    return parser.parse_args()\n\n\n# ---------------------------- Main ----------------------------------- #\n\ndef main() -\u003e None:\n    args = parse_args()\n    cfg_file = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    if not cfg_file.exists():\n        raise FileNotFoundError(f\"Config file {cfg_file} not found\")\n    cfg = _read_yaml(cfg_file)\n\n    experiments = cfg.get(\"experiments\", [])\n    if not experiments:\n        raise ValueError(\"No experiments specified in configuration file\")\n\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    if results_dir.exists():\n        shutil.rmtree(results_dir)\n    results_dir.mkdir(parents=True, exist_ok=False)\n\n    gpus = _available_gpus()\n    gpu_queue: List[int] = gpus.copy()\n    pending = experiments.copy()\n    running: List[Any] = []  # list of tuples (proc, gpu_id, run_id)\n\n    print(\"Detected GPUs:\", gpus)\n    print(f\"Launching {len(experiments)} experiment(s)...\")\n\n    while pending or running:\n        # Launch as many as GPUs allow\n        while pending and gpu_queue:\n            exp_cfg = pending.pop(0)\n            gpu_id = gpu_queue.pop(0) if gpus else -1\n            proc = _launch_experiment(exp_cfg, cfg_file, results_dir, gpu_id, smoke=args.smoke_test)\n            running.append((proc, gpu_id, exp_cfg[\"run_id\"]))\n            print(f\"[Main] Launched \u0027{exp_cfg[\u0027run_id\u0027]}\u0027 on GPU {gpu_id}\")\n            time.sleep(1)  # small gap to stagger IO\n\n        # Poll running processes\n        for (proc, gpu_id, run_id) in running.copy():  # copy to avoid modification during iteration\n            ret = proc.poll()\n            if ret is not None:\n                if ret != 0:\n                    print(f\"[Main] Run \u0027{run_id}\u0027 exited with code {ret}\")\n                else:\n                    print(f\"[Main] Run \u0027{run_id}\u0027 completed successfully\")\n                running.remove((proc, gpu_id, run_id))\n                if gpu_id \u003e= 0:\n                    gpu_queue.append(gpu_id)\n        time.sleep(2)\n\n    # All done ------------------------------------------------------------\n    print(\"All experiments finished. Proceeding to evaluation...\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"\nModel Architectures (COMMON CORE FOUNDATION)\n===========================================\n\u2022 Provides minimal yet fully functional reference models (SimpleCNN \u0026 MLP) so\n  that the framework can run end-to-end out-of-the-box.\n\u2022 Additional task-specific models should be added in specialised steps by\n  extending `get_model()`.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Tuple, Dict, Any\n\nimport torch\nfrom torch import nn\n\n\n# ------------------------- Simple CNN -------------------------------- #\nclass _SimpleCNN(nn.Module):\n    def __init__(self, input_shape: Tuple[int, ...], num_classes: int):\n        super().__init__()\n        c, h, w = input_shape\n        self.features = nn.Sequential(\n            nn.Conv2d(c, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        feat_h, feat_w = h // 4, w // 4\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * feat_h * feat_w, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:  # noqa: D401\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# ------------------------- Simple MLP -------------------------------- #\nclass _SimpleMLP(nn.Module):\n    def __init__(self, input_shape: Tuple[int, ...], num_classes: int):\n        super().__init__()\n        flat_features = int(torch.tensor(input_shape).prod())\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(flat_features, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:  # noqa: D401\n        return self.net(x)\n\n\n# ---------------------- Public Factory Function ---------------------- #\n\ndef get_model(model_cfg: Dict[str, Any], *, input_shape: Tuple[int, ...], num_classes: int) -\u003e nn.Module:\n    \"\"\"Return an instantiated torch.nn.Module based on the config.\"\"\"\n    name = model_cfg[\"name\"].upper()\n\n    if name == \"SIMPLE_CNN\":\n        return _SimpleCNN(input_shape, num_classes)\n    if name == \"SIMPLE_MLP\":\n        return _SimpleMLP(input_shape, num_classes)\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: integrate domain-specific models here (e.g., ResNet50)\n    # ------------------------------------------------------------------\n    raise NotImplementedError(f\"Model name \u0027{name}\u0027 is not implemented in the COMMON CORE.\")\n", "preprocess_py": "\"\"\"\nData Pre-Processing \u0026 DataLoader Factory (COMMON CORE FOUNDATION)\n================================================================\n\u2022 Provides a single entry point `load_dataloaders()` used by all experiments.\n\u2022 Contains a fully functional synthetic dataset for smoke tests so that the\n  whole pipeline runs end-to-end without external data.\n\u2022 All *real* dataset loading must be implemented in the placeholder section.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple, Any, Dict\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n\n# ------------------------- Synthetic Dataset ------------------------- #\nclass _SyntheticClassificationDataset(Dataset):\n    \"\"\"A tiny synthetic classification dataset for smoke testing.\"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, ...], num_classes: int, seed: int = 0):\n        super().__init__()\n        g = torch.Generator()\n        g.manual_seed(seed)\n        self.data = torch.randn((num_samples, *input_shape), generator=g)\n        self.targets = torch.randint(0, num_classes, (num_samples,), generator=g)\n\n    def __len__(self) -\u003e int:  # noqa: D401\n        return len(self.targets)\n\n    def __getitem__(self, idx: int) -\u003e Tuple[torch.Tensor, torch.Tensor]:  # noqa: D401\n        return self.data[idx], self.targets[idx]\n\n\n# ----------------------- Public Loader Function ----------------------- #\n\ndef load_dataloaders(\n    dataset_cfg: Dict[str, Any],\n    batch_size: int,\n    smoke_test: bool = False,\n) -\u003e Tuple[DataLoader, DataLoader, Tuple[int, ...], int]:\n    \"\"\"Return (train_loader, val_loader, input_shape, num_classes).\"\"\"\n\n    name = dataset_cfg[\"name\"]\n\n    # ------------------------------------------------------------------\n    # 1) Synthetic dataset (always available) ---------------------------\n    # ------------------------------------------------------------------\n    if name == \"SYNTHETIC_PLACEHOLDER\" or smoke_test:\n        input_shape = tuple(dataset_cfg.get(\"input_shape\", (3, 32, 32)))\n        num_classes = int(dataset_cfg.get(\"num_classes\", 10))\n        num_samples = int(dataset_cfg.get(\"num_samples\", 512))\n        full_ds = _SyntheticClassificationDataset(num_samples, input_shape, num_classes)\n        train_size = int(0.8 * len(full_ds))\n        val_size = len(full_ds) - train_size\n        train_ds, val_ds = random_split(full_ds, [train_size, val_size], generator=torch.Generator().manual_seed(0))\n        return (\n            DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0),\n            DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0),\n            input_shape,\n            num_classes,\n        )\n\n    # ------------------------------------------------------------------\n    # 2) Real dataset placeholder --------------------------------------\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    # Example below shows the expected return signature but raises for now.\n    raise NotImplementedError(\n        \"Dataset loading for name=\u0027%s\u0027 is not implemented in the COMMON CORE. \"\n        \"Please provide an implementation in the specialised experiment step.\" % name\n    )\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\n\n[project]\nname = \"common-core-ohgw\"\nversion = \"0.1.0\"\nauthors = [{ name = \"Research Team\", email = \"research@example.com\" }]\ndescription = \"COMMON CORE FOUNDATION for OHGW experiments\"\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.8\"\nlicense = { text = \"MIT\" }\n\n[project.dependencies]\ntorch = \"*\"\nPyYAML = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\ntqdm = \"*\"\n\n# optional dev dependencies\n[project.optional-dependencies]\ndev = [\"ruff\", \"pytest\"]\n", "smoke_test_yaml": "# Lightweight configuration for CI smoke tests.\nexperiments:\n  - run_id: baseline\n    seed: 0\n    model:\n      name: SIMPLE_CNN\n    dataset:\n      name: SYNTHETIC_PLACEHOLDER       # PLACEHOLDER: replace with real dataset in specialised experiments\n      input_shape: [3, 32, 32]\n      num_classes: 10\n      num_samples: 512\n    training:\n      epochs: 2\n      batch_size: 32\n      optimizer:\n        type: sgd\n        lr: 0.01\n        momentum: 0.9\n        weight_decay: 0.0005\n    ohgw:\n      enabled: false\n\n  - run_id: ohgw\n    seed: 0\n    model:\n      name: SIMPLE_CNN\n    dataset:\n      name: SYNTHETIC_PLACEHOLDER\n      input_shape: [3, 32, 32]\n      num_classes: 10\n      num_samples: 512\n    training:\n      epochs: 2\n      batch_size: 32\n      optimizer:\n        type: sgd\n        lr: 0.01\n        momentum: 0.9\n        weight_decay: 0.0005\n    ohgw:\n      enabled: true\n      eta: 0.001\n\n", "train_py": "\"\"\"\nTrain Script for a Single Experiment Variation (COMMON CORE FOUNDATION)\n=====================================================================\n\u2022 Reads the global YAML configuration file and selects the experiment block by --run-id.\n\u2022 Supports OHGW warm-start, random warm-start placeholder, and vanilla training.\n\u2022 Logs JSON-formatted epoch metrics and final results to stdout and to \u003cresults_dir\u003e/\u003crun_id\u003e/results.json.\n\u2022 Always prints a natural-language experiment description *before* numeric data.\n\u2022 Uses one GPU chosen by --gpu-id (falls back to CPU if unavailable).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport yaml\n\nsys.path.append(str(Path(__file__).resolve().parent))\nfrom preprocess import load_dataloaders  # noqa: E402  # pylint: disable=C0413\nfrom model import get_model  # noqa: E402  # pylint: disable=C0413\n\n\n# ------------------------- Utility Functions ------------------------- #\n\ndef _describe_experiment(run_cfg: Dict[str, Any]) -\u003e str:\n    \"\"\"Return a human-readable description for the current run.\"\"\"\n    lines = [\n        f\"Run-ID: {run_cfg[\u0027run_id\u0027]}\",\n        f\"Model: {run_cfg[\u0027model\u0027][\u0027name\u0027]}\",\n        f\"Dataset: {run_cfg[\u0027dataset\u0027][\u0027name\u0027]}\",\n        f\"Epochs: {run_cfg[\u0027training\u0027][\u0027epochs\u0027]}\",\n        f\"Batch-size: {run_cfg[\u0027training\u0027][\u0027batch_size\u0027]}\",\n    ]\n    if run_cfg.get(\"ohgw\", {}).get(\"enabled\", False):\n        lines.append(\"OHGW: ENABLED (\u03b7_h=%.1e)\" % run_cfg[\"ohgw\"].get(\"eta\", 1e-3))\n    else:\n        lines.append(\"OHGW: disabled\")\n    if run_cfg.get(\"random_warm_start\", {}).get(\"enabled\", False):\n        lines.append(\"Random Warm-Start: ENABLED\")\n    return \"\\n\".join(lines)\n\n\ndef _weight_decay_loss(model: nn.Module, weight_decay: torch.Tensor) -\u003e torch.Tensor:\n    \"\"\"L2 regularisation term that is differentiable w.r.t. weight_decay.\"\"\"\n    l2 = torch.tensor(0.0, device=weight_decay.device)\n    for p in model.parameters():\n        l2 = l2 + p.pow(2).sum()\n    return 0.5 * weight_decay * l2\n\n\n# --------------------------- Main Training --------------------------- #\n\ndef train(run_cfg: Dict[str, Any], device: torch.device, results_dir: Path, smoke_test: bool = False) -\u003e None:\n    torch.manual_seed(run_cfg.get(\"seed\", 0))\n\n    # 1) Data -----------------------------------------------------------------\n    train_loader, val_loader, input_shape, num_classes = load_dataloaders(\n        dataset_cfg=run_cfg[\"dataset\"],\n        batch_size=run_cfg[\"training\"][\"batch_size\"],\n        smoke_test=smoke_test,\n    )\n\n    # 2) Model ----------------------------------------------------------------\n    model = get_model(run_cfg[\"model\"], input_shape=input_shape, num_classes=num_classes)\n    model.to(device)\n\n    # 3) Optimiser \u0026 Loss ------------------------------------------------------\n    opt_cfg = run_cfg[\"training\"].get(\"optimizer\", {})\n    base_lr = float(opt_cfg.get(\"lr\", 1e-2))\n    momentum = float(opt_cfg.get(\"momentum\", 0.9))\n    # Hyper-parameter tensors (continuous) that we *may* warm-start via OHGW\n    hyperparams = {\n        \"weight_decay\": torch.tensor(float(opt_cfg.get(\"weight_decay\", 0.0)), requires_grad=True, device=device)\n    }\n\n    optimizer = torch.optim.SGD(\n        params=model.parameters(),\n        lr=base_lr,\n        momentum=momentum,\n        weight_decay=float(hyperparams[\"weight_decay\"].item()),  # will be patched after OHGW\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # 4) Optional Warm-Start ---------------------------------------------------\n    if run_cfg.get(\"ohgw\", {}).get(\"enabled\", False):\n        eta_h = float(run_cfg[\"ohgw\"].get(\"eta\", 1e-3))\n        model.train()\n        x_b, y_b = next(iter(train_loader))\n        x_b, y_b = x_b.to(device), y_b.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(x_b)\n        loss = criterion(logits, y_b) + _weight_decay_loss(model, hyperparams[\"weight_decay\"])\n        grads = torch.autograd.grad(loss, list(hyperparams.values()), retain_graph=False, create_graph=False)\n        with torch.no_grad():\n            for (name, hp), g in zip(hyperparams.items(), grads):\n                hyperparams[name] -= eta_h * g\n        # Patch optimizer with new weight-decay\n        for pg in optimizer.param_groups:\n            pg[\"weight_decay\"] = float(hyperparams[\"weight_decay\"].item())\n\n    elif run_cfg.get(\"random_warm_start\", {}).get(\"enabled\", False):\n        delta = float(run_cfg[\"random_warm_start\"].get(\"delta\", 1e-4))\n        with torch.no_grad():\n            hyperparams[\"weight_decay\"] += torch.empty_like(hyperparams[\"weight_decay\"]).uniform_(-delta, delta)\n        for pg in optimizer.param_groups:\n            pg[\"weight_decay\"] = float(hyperparams[\"weight_decay\"].item())\n\n    # 5) Training Loop ---------------------------------------------------------\n    epochs = int(run_cfg[\"training\"][\"epochs\"])\n    metrics_per_epoch: List[Dict[str, Any]] = []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss, running_correct, running_total = 0.0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n        for xb, yb in pbar:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb) + _weight_decay_loss(model, hyperparams[\"weight_decay\"])\n            loss.backward()\n            optimizer.step()\n\n            # stats\n            running_loss += loss.item() * yb.size(0)\n            preds = logits.argmax(dim=1)\n            running_correct += (preds == yb).sum().item()\n            running_total += yb.size(0)\n            pbar.set_postfix({\"loss\": running_loss / running_total})\n\n        train_loss = running_loss / running_total\n        train_acc = running_correct / running_total\n\n        # Validation ---------------------------------------------------------\n        model.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb)\n                loss = criterion(logits, yb) + _weight_decay_loss(model, hyperparams[\"weight_decay\"])\n                val_loss += loss.item() * yb.size(0)\n                val_correct += (logits.argmax(dim=1) == yb).sum().item()\n                val_total += yb.size(0)\n        val_loss /= val_total\n        val_acc = val_correct / val_total\n\n        epoch_metrics = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n        }\n        metrics_per_epoch.append(epoch_metrics)\n\n        print(json.dumps({\"run_id\": run_cfg[\"run_id\"], \"epoch_metrics\": epoch_metrics}), flush=True)\n\n    # 6) Persist Results -------------------------------------------------------\n    result = {\n        \"run_id\": run_cfg[\"run_id\"],\n        \"final_train_loss\": metrics_per_epoch[-1][\"train_loss\"],\n        \"final_val_accuracy\": metrics_per_epoch[-1][\"val_accuracy\"],\n        \"best_val_accuracy\": max(m[\"val_accuracy\"] for m in metrics_per_epoch),\n        \"metrics_per_epoch\": metrics_per_epoch,\n    }\n    out_path = results_dir / run_cfg[\"run_id\"]\n    out_path.mkdir(parents=True, exist_ok=True)\n    with (out_path / \"results.json\").open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(result, f, indent=2)\n\n    # Final summary to stdout --------------------------------------------------\n    print(json.dumps(result), flush=True)\n\n\n# ---------------------------------- CLI ------------------------------------- #\n\ndef parse_args() -\u003e argparse.Namespace:  # noqa: D401\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation (COMMON CORE)\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config file with all experiments\")\n    parser.add_argument(\"--run-id\", type=str, required=True, help=\"Which experiment block to execute\")\n    parser.add_argument(\"--gpu-id\", type=int, default=-1, help=\"GPU index to use; \u003c0 for CPU\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory for experiment outputs\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", default=False, help=\"Run a lightweight smoke test\")\n    return parser.parse_args()\n\n\ndef main() -\u003e None:\n    args = parse_args()\n\n    # Load YAML --------------------------------------------------------------\n    with open(args.config, \"r\", encoding=\"utf-8\") as f:\n        cfg = yaml.safe_load(f)\n    run_cfgs = {exp[\"run_id\"]: exp for exp in cfg.get(\"experiments\", [])}\n    if args.run_id not in run_cfgs:\n        raise ValueError(f\"Run-ID \u0027{args.run_id}\u0027 not found in {args.config}\")\n    run_cfg = run_cfgs[args.run_id]\n\n    # Device -----------------------------------------------------------------\n    if args.gpu_id \u003e= 0 and torch.cuda.is_available():\n        device = torch.device(f\"cuda:{args.gpu_id}\")\n    else:\n        device = torch.device(\"cpu\")\n\n    # Description ------------------------------------------------------------\n    description = _describe_experiment(run_cfg)\n    print(\"\\n========== Experiment Description ==========\")\n    print(description)\n    print(\"==========================================\\n\")\n\n    # Train ------------------------------------------------------------------\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    train(run_cfg, device, results_dir, smoke_test=args.smoke_test)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Criterion 1 violated – train.py never saves the trained model (e.g. via torch.save) and no loading utility exists, so the mandatory “model saving/loading mechanisms” are missing. All other core logic is present, but without persistence the base foundation is incomplete."
}
