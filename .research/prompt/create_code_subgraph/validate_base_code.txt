
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "#!/usr/bin/env python\n\"\"\"\nEvaluation \u0026 Comparison Script\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nReads all \u003cresults_dir\u003e/**/results.json produced by train.py, aggregates the\nmetrics, performs statistical comparisons and produces publication-ready\nfigures (*.pdf).  The metrics themselves are printed to STDOUT as JSON.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical test\nfrom scipy.stats import wilcoxon\n\nFIG_NAMING_HELP = \"\"\"Figure file names follow \u003ctopic\u003e_\u003ccondition\u003e.pdf as required.\"\"\"\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef load_all_results(results_dir: Path):\n    rows = []\n    for results_file in results_dir.rglob(\"results.json\"):\n        with open(results_file, \"r\") as f:\n            data = json.load(f)\n            rows.append(data)\n    if not rows:\n        raise RuntimeError(f\"No results.json found in {results_dir}\")\n    return pd.DataFrame(rows)\n\n# Figures helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef bar_final_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(6,4))\n    sns.barplot(x=\"run_id\", y=\"final_test_accuracy\", data=df)\n    plt.ylabel(\"Final Test Accuracy\")\n    plt.xlabel(\"\")\n    plt.xticks(rotation=45, ha=\"right\")\n    for i, row in enumerate(df.itertuples()):\n        plt.text(i, row.final_test_accuracy+0.002, f\"{row.final_test_accuracy*100:.2f}%\", ha=\u0027center\u0027, va=\u0027bottom\u0027)\n    plt.tight_layout()\n    fname = out_path / \"accuracy_comparison.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname.name\n\n\ndef bar_training_time(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(6,4))\n    sns.barplot(x=\"run_id\", y=\"training_time_sec\", data=df)\n    plt.ylabel(\"Training Time (s)\")\n    plt.xlabel(\"\")\n    plt.xticks(rotation=45, ha=\"right\")\n    for i, row in enumerate(df.itertuples()):\n        plt.text(i, row.training_time_sec+1, f\"{row.training_time_sec/60:.1f}m\", ha=\u0027center\u0027, va=\u0027bottom\u0027)\n    plt.tight_layout()\n    fname = out_path / \"training_time_comparison.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname.name\n\n# Statistical test helper\n\ndef pairwise_wilcoxon(df: pd.DataFrame, metric: str):\n    results = {}\n    for (i, j) in itertools.combinations(df.index, 2):\n        try:\n            stat, p = wilcoxon([df.loc[i, metric]], [df.loc[j, metric]])\n            results[(df.loc[i, \u0027run_id\u0027], df.loc[j, \u0027run_id\u0027])] = float(p)\n        except ValueError:\n            # Identical values \u2192 skip\n            results[(df.loc[i, \u0027run_id\u0027], df.loc[j, \u0027run_id\u0027])] = 1.0\n    return results\n\n# Main \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", required=True, type=str)\n    parser.add_argument(\"--config-file\", required=False, type=str, default=None)\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    df = load_all_results(results_dir)\n\n    # Generate figures\n    figure_dir = results_dir\n    acc_fig = bar_final_accuracy(df, figure_dir)\n    time_fig = bar_training_time(df, figure_dir)\n\n    # Statistical comparison on final_test_accuracy\n    wilcoxon_p = pairwise_wilcoxon(df, \"final_test_accuracy\")\n\n    comparison = {\n        \"num_runs\": len(df),\n        \"figures\": [acc_fig, time_fig],\n        \"wilcoxon_p_final_accuracy\": wilcoxon_p,\n    }\n\n    # Experiment description (if config provided)\n    description = {}\n    if args.config_file:\n        import yaml\n        with open(args.config_file, \u0027r\u0027) as f:\n            description = yaml.safe_load(f)\n\n    print(json.dumps({\"experiment_description\": description}, indent=2))\n    print(json.dumps({\"comparison_results\": comparison}, indent=2))\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# Full Experiment Configuration Template\n# Real experiments will populate this file with dozens of variations.\n\nexperiments:\n  # PLACEHOLDER: Add actual experiment configurations here.  Each entry\n  # must at least contain: run_id, dataset, model, training settings.\n  # Example skeleton (will be replaced):\n  # - run_id: \"baseline_cifar10_asha\"\n  #   description: \"ASHA baseline on CIFAR-10 w/ ResNet-20\"\n  #   dataset:\n  #     name: \"cifar10\"\n  #   model:\n  #     type: \"resnet20\"\n  #   training:\n  #     epochs: 200\n  #     batch_size: 128\n  #     learning_rate: 0.1\n  #   hyperparameters:\n  #     weight_decay: 0.0005\n  #     momentum: 0.9\n  #     label_smoothing: 0.1\n  #   algorithm:\n  #     ohgw:\n  #       enabled: false\n  # - run_id: \"ohgw_cifar10_asha\"\n  #   description: \"ASHA + OHGW proposal on CIFAR-10 w/ ResNet-20\"\n  #   \u003csame as above but algorithm.ohgw.enabled: true\u003e\n  \n  \n  ", "main_py": "#!/usr/bin/env python\n\"\"\"\nMain Orchestrator Script\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nRuns *all* experiments defined in a YAML configuration file sequentially.\n(Done instead of parallel to avoid GPU contention on shared CI machines.)\n\nUsage:\n  uv run python -m src.main --smoke-test  --results-dir \u003cpath\u003e\n  uv run python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport subprocess\nimport sys\nfrom pathlib import Path\nimport yaml\nimport shutil\nimport os\nimport json\n\n# Helper: tee-like logging ------------------------------------------------\nclass Tee:\n    def __init__(self, file_path: Path, stream):\n        self.file = open(file_path, \"w\")\n        self.stream = stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.file.flush()\n        self.stream.write(data)\n        self.stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.stream.flush()\n\n# -----------------------------------------------------------------------\n\ndef launch_subprocess(cmd, stdout_path: Path, stderr_path: Path):\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n        for line in proc.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in proc.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        proc.wait()\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess {\u0027 \u0027.join(cmd)} exited with code {proc.returncode}\")\n\n# -----------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser()\n    grp = parser.add_mutually_exclusive_group(required=True)\n    grp.add_argument(\"--smoke-test\", action=\"store_true\")\n    grp.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    cfg_file = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    experiments = config.get(\"experiments\", [])\n    if not experiments:\n        print(\"No experiments found in config \u2013 nothing to run.\")\n        sys.exit(0)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Write per-run YAML so the worker script only gets what it needs\n        run_cfg_path = run_dir / \"run_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--run-id\", run_id, \"--results-dir\", str(results_dir)]\n        print(f\"\\n=== Running {run_id} ===\")\n        launch_subprocess(cmd, stdout_path, stderr_path)\n        print(f\"=== Finished {run_id} ===\\n\")\n\n    # After all runs: aggregate / evaluate\n    print(\"\\n### All runs completed \u2013 launching evaluation ###\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir), \"--config-file\", str(cfg_file)]\n    launch_subprocess(eval_cmd, results_dir / \"evaluate_stdout.log\", results_dir / \"evaluate_stderr.log\")\n\nif __name__ == \"__main__\":\n    main()", "model_py": "#!/usr/bin/env python\n\"\"\"\nModel architectures provided to every experiment.  This module intentionally\ncontains only *generic* models that are dataset-agnostic.  Specific, larger\nmodels (e.g., ResNet-20, ViT-Small) will be INSERTED in later steps by\nsimply adding new entries to the registry.\n\nThe build_model(cfg, input_shape, num_classes) factory ensures that training\nscripts never need to change when we swap architectures.\n\"\"\"\nfrom __future__ import annotations\nimport math\nimport torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Simple baseline MLP \u2013 works for tabular or flattened image inputs\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x):\n        if x.ndim \u003e 2:\n            x = torch.flatten(x, 1)\n        return self.net(x)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Simple CNN baseline \u2013 works for small images such as 32\u00d732\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nclass SimpleCNN(nn.Module):\n    def __init__(self, in_channels: int, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 32, 3, padding=1), nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128), nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n\n# Registry \u2013 maps lowercase names to classes/constructor lambdas\nMODEL_REGISTRY = {\n    \"simple_mlp\": lambda cfg, input_shape, num_classes: SimpleMLP(input_dim=input_shape[0],\n                                                                   hidden_dim=int(cfg[\"model\"].get(\"hidden_dim\", 64)),\n                                                                   num_classes=num_classes),\n    \"simple_cnn\": lambda cfg, input_shape, num_classes: SimpleCNN(in_channels=input_shape[0],\n                                                                  num_classes=num_classes),\n\n    # PLACEHOLDER: Will be replaced with dataset-specific larger models, e.g. \"resnet20\", \"vit_small\", ...\n}\n\n\ndef build_model(cfg: dict, input_shape: Tuple[int, ...], num_classes: int):\n    model_name = cfg.get(\"model\", {}).get(\"type\", \"simple_mlp\").lower()\n    if model_name not in MODEL_REGISTRY:\n        raise ValueError(f\"Model \u0027{model_name}\u0027 not found in registry.  Please add it in model.py.\")\n    return MODEL_REGISTRY[model_name](cfg, input_shape, num_classes)", "preprocess_py": "#!/usr/bin/env python\n\"\"\"\nCommon Pre-Processing / Data-Loader Utilities.\nThis module centralises ALL data handling logic so that every experimental\nvariation uses identical preprocessing steps.  Dataset-specific loading is\ninserted via clearly marked placeholders.\n\"\"\"\nfrom __future__ import annotations\nimport os\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import datasets, transforms\n\n# ---------------------------------------------------------------------\n# Synthetic dataset for smoke tests \u2013 ensures the framework is runnable\n# even before real datasets are wired in.\n# ---------------------------------------------------------------------\nclass SyntheticClassificationDataset(Dataset):\n    \"\"\"Random vectors with random labels \u2013 deterministic given a seed.\"\"\"\n    def __init__(self, num_samples: int = 1000, input_dim: int = 20, num_classes: int = 2, seed: int = 42):\n        import numpy as np\n        rng = np.random.default_rng(seed)\n        self.x = torch.tensor(rng.normal(size=(num_samples, input_dim)).astype(\u0027float32\u0027))\n        self.y = torch.tensor(rng.integers(0, num_classes, size=num_samples).astype(\u0027int64\u0027))\n        self.num_classes = num_classes\n        self.input_dim = input_dim\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n# ---------------------------------------------------------------------\n# Public API\n# ---------------------------------------------------------------------\n\ndef _load_dataset(cfg: dict):\n    \"\"\"Factory that returns (train_dataset, test_dataset).  Uses PLACEHOLDER\n    tags so that specific datasets can be inserted later.\n    \"\"\"\n    ds_cfg = cfg.get(\"dataset\", {})\n    name = ds_cfg.get(\"name\", \"DATASET_PLACEHOLDER\").lower()\n\n    # PLACEHOLDER: Will be replaced with CIFAR-10 / ImageNet \u2026 loader\n    if name in {\"synthetic\", \"DATASET_PLACEHOLDER\"}:\n        num_samples = int(ds_cfg.get(\"num_samples\", 1000))\n        input_dim = int(ds_cfg.get(\"input_dim\", 20))\n        num_classes = int(ds_cfg.get(\"num_classes\", 2))\n        full_dataset = SyntheticClassificationDataset(num_samples=num_samples,\n                                                      input_dim=input_dim,\n                                                      num_classes=num_classes,\n                                                      seed=cfg.get(\"seed\", 42))\n        # 80/10/10 split\n        n_total = len(full_dataset)\n        n_train = int(0.8 * n_total)\n        n_val   = int(0.1 * n_total)\n        n_test  = n_total - n_train - n_val\n        train_ds, val_ds, test_ds = random_split(full_dataset, [n_train, n_val, n_test])\n        return train_ds, val_ds, test_ds\n\n    # Example of dataset-specific branch (will be overridden / extended)\n    elif name == \"cifar10\":\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n        train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n        test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n        # create val split\n        n_train = int(0.9 * len(train_ds))\n        n_val   = len(train_ds) - n_train\n        train_ds, val_ds = random_split(train_ds, [n_train, n_val])\n        return train_ds, val_ds, test_ds\n\n    else:\n        raise ValueError(f\"Unknown dataset name: {name}\")\n\n\ndef get_dataloaders(cfg: dict):\n    train_ds, val_ds, test_ds = _load_dataset(cfg)\n    batch_size = int(cfg.get(\"training\", {}).get(\"batch_size\", 64))\n    num_workers = int(cfg.get(\"training\", {}).get(\"num_workers\", 4))\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    return train_loader, val_loader, test_loader\n\n\ndef get_input_shape_num_classes(cfg: dict):\n    ds_cfg = cfg.get(\"dataset\", {})\n    name = ds_cfg.get(\"name\", \"DATASET_PLACEHOLDER\").lower()\n    if name in {\"synthetic\", \"DATASET_PLACEHOLDER\"}:\n        input_dim = int(ds_cfg.get(\"input_dim\", 20))\n        num_classes = int(ds_cfg.get(\"num_classes\", 2))\n        return (input_dim,), num_classes\n    elif name == \"cifar10\":\n        return (3, 32, 32), 10\n    else:\n        # Needs manual insertion for other datasets\n        raise ValueError(f\"Dataset {name} must define input shape and num classes.\")", "pyproject_toml": "[project]\nname = \"ohgw-experiments\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n  \"torch\u003e=2.0\",\n  \"torchvision\u003e=0.15\",\n  \"tqdm\u003e=4.64\",\n  \"pyyaml\u003e=6.0\",\n  \"matplotlib\u003e=3.7\",\n  \"seaborn\u003e=0.12\",\n  \"pandas\u003e=1.5\",\n  \"scipy\u003e=1.10\"\n]\n\n[build-system]\nrequires = [\"setuptools\u003e=61\"]\nbuild-backend = \"setuptools.build_meta\"", "smoke_test_yaml": "# Smoke Test Configuration Template\n# This very small configuration is executed on CI to ensure that the complete\n# stack functions without heavy compute requirements.\n\nexperiments:\n  - run_id: \"smoke_baseline\"\n    description: \"Baseline smoke-test with synthetic data and SimpleMLP.\"\n    dataset:\n      name: \"synthetic\"          # PLACEHOLDER: will be replaced with real dataset name\n      num_samples: 200           # tiny dataset for fast check\n      input_dim: 20\n      num_classes: 2\n    model:\n      type: \"simple_mlp\"         # PLACEHOLDER: real model e.g. resnet20\n      hidden_dim: 32\n    training:\n      epochs: 2\n      batch_size: 32\n      learning_rate: 0.001\n      optimizer: \"sgd\"\n    hyperparameters:\n      weight_decay: 0.0\n      label_smoothing: 0.0\n    algorithm:\n      ohgw:\n        enabled: false           # turned off for smoke test\n    save_model: false", "train_py": "#!/usr/bin/env python\n\"\"\"\nTrain Script \u2013 Runs ONE experiment variation.\nThis script is invoked as a subprocess by main.py.  It must:\n1.   Load the per-run YAML configuration.\n2.   Build data-set loaders (via src.preprocess).\n3.   Build the model (via src.model).\n4.   (Optional) run One-Shot Hyper-Gradient Warm-Start (OHGW).\n5.   Train for the configured number of epochs.\n6.   Evaluate on validation / test.\n7.   Save figures \u0026 JSON metrics into \u003cresults_dir\u003e/\u003crun_id\u003e/.\n8.   Print experiment description followed by JSON metrics to STDOUT.\n\nThe implementation below contains COMPLETE, production-ready logic.  All\nplaces that will differ between experimental variations are clearly marked\nwith  # PLACEHOLDER comments so the next pipeline step can safely replace\nthem.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom time import time\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# local imports (absolute to src package)\nfrom src.preprocess import get_dataloaders, get_input_shape_num_classes\nfrom src.model import build_model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reproducibility helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef set_seed(seed: int) -\u003e None:\n    import random, numpy as np\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n# OHGW implementation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef ohgw_warm_start(\n    model: nn.Module,\n    batch: tuple[torch.Tensor, torch.Tensor],\n    hyperparams: dict[str, torch.Tensor],\n    device: torch.device,\n    eta_h: float,\n    num_classes: int,\n) -\u003e dict[str, float]:\n    \"\"\"Perform *one* stochastic hyper-gradient step as described in OHGW.\n\n    The hyper-parameters we support in the common foundation are:\n      \u2013 weight_decay  (float)\n      \u2013 label_smoothing (float)\n    which both *do* appear in the computational graph.  Learning-rate is\n    updated by the scheduler later and therefore ignored here.\n\n    Returns the **updated** hyper-parameter dictionary with plain Python\n    floats so subsequent code can feed them into PyTorch optimizers.\n    \"\"\"\n    model.train()\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    wd = hyperparams.get(\"weight_decay\", torch.tensor(0.0, device=device, requires_grad=True))\n    ls = hyperparams.get(\"label_smoothing\", torch.tensor(0.0, device=device, requires_grad=True))\n\n    outputs = model(inputs)\n    log_probs = torch.log_softmax(outputs, dim=1)\n    with torch.no_grad():\n        true_dist = torch.zeros_like(log_probs)\n        true_dist.scatter_(1, targets.unsqueeze(1), 1.0)\n        true_dist = (1 - ls) * true_dist + ls / num_classes\n    ce_loss = -(true_dist * log_probs).sum(dim=1).mean()\n\n    reg_loss = 0.5 * wd * sum(p.pow(2).sum() for p in model.parameters())\n    loss = ce_loss + reg_loss\n\n    grads = torch.autograd.grad(loss, (wd, ls), retain_graph=False, allow_unused=True)\n\n    new_wd = wd.detach() - eta_h * (grads[0].detach() if grads[0] is not None else 0.0)\n    new_ls = ls.detach() - eta_h * (grads[1].detach() if grads[1] is not None else 0.0)\n\n    # Clamp to sensible bounds\n    new_wd = new_wd.clamp(min=0.0)\n    new_ls = new_ls.clamp(min=0.0, max=0.3)  # label smoothing typically small\n\n    return {\"weight_decay\": float(new_wd.item()), \"label_smoothing\": float(new_ls.item())}\n\n# Training helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef train_one_epoch(model: nn.Module,\n                    loader: DataLoader,\n                    criterion,\n                    optimizer: torch.optim.Optimizer,\n                    device: torch.device,\n                    label_smoothing: float,\n                    num_classes: int) -\u003e tuple[float, float]:\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        log_probs = torch.log_softmax(outputs, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.scatter_(1, targets.unsqueeze(1), 1.0)\n            true_dist = (1 - label_smoothing) * true_dist + label_smoothing / num_classes\n        loss = -(true_dist * log_probs).sum(dim=1).mean()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n        total += targets.size(0)\n\n    return running_loss / total, correct / total\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, device: torch.device, label_smoothing: float, num_classes: int):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")  # we implement smoothing manually\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            log_probs = torch.log_softmax(outputs, dim=1)\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.scatter_(1, targets.unsqueeze(1), 1.0)\n            true_dist = (1 - label_smoothing) * true_dist + label_smoothing / num_classes\n            loss = -(true_dist * log_probs).sum(dim=1)\n            running_loss += loss.sum().item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(targets).sum().item()\n            total += targets.size(0)\n    return running_loss / total, correct / total\n\n# Plotting \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef save_curves(history, run_dir: Path, run_id: str):\n    sns.set(style=\"darkgrid\")\n    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n\n    # Loss Curve\n    plt.figure(figsize=(6,4))\n    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training / Validation Loss \u2013 {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{history[\u0027val_loss\u0027][-1]:.3f}\",\n                 xy=(epochs[-1], history[\u0027val_loss\u0027][-1]), xytext=(5,5),\n                 textcoords=\u0027offset points\u0027)\n    fname = run_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n    # Accuracy Curve\n    plt.figure(figsize=(6,4))\n    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(f\"Accuracy \u2013 {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{history[\u0027val_acc\u0027][-1]*100:.2f}%\",\n                 xy=(epochs[-1], history[\u0027val_acc\u0027][-1]), xytext=(5,-15),\n                 textcoords=\u0027offset points\u0027)\n    fname2 = run_dir / f\"accuracy_{run_id}.pdf\"\n    plt.savefig(fname2, bbox_inches=\"tight\")\n    plt.close()\n\n    return [str(fname.name), str(fname2.name)]\n\n# Main entry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", required=True, type=str, help=\"Path to per-run YAML config file\")\n    parser.add_argument(\"--run-id\", required=True, type=str)\n    parser.add_argument(\"--results-dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    # ------------------------------------------------------------------\n    # Preparations\n    # ------------------------------------------------------------------\n    import yaml\n    with open(args.config, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    run_id = args.run_id\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(cfg.get(\"seed\", 42))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Dataset\n    train_loader, val_loader, test_loader = get_dataloaders(cfg)\n    input_shape, num_classes = get_input_shape_num_classes(cfg)\n\n    # Model\n    model = build_model(cfg, input_shape=input_shape, num_classes=num_classes).to(device)\n\n    # Hyper-parameters\n    lr = float(cfg[\"training\"].get(\"learning_rate\", 1e-3))\n    weight_decay = float(cfg[\"hyperparameters\"].get(\"weight_decay\", 0.0))\n    momentum = float(cfg[\"hyperparameters\"].get(\"momentum\", 0.9))\n    label_smoothing = float(cfg[\"hyperparameters\"].get(\"label_smoothing\", 0.0))\n\n    # Optimizer \u2013 only SGD \u0026 Adam provided, extend as needed.\n    opt_name = cfg[\"training\"].get(\"optimizer\", \"sgd\").lower()\n    if opt_name == \"sgd\":\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif opt_name == \"adam\":\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {opt_name}\")\n\n    # Optionally run OHGW warm start\n    if cfg.get(\"algorithm\", {}).get(\"ohgw\", {}).get(\"enabled\", False):\n        eta_h = float(cfg[\"algorithm\"][\"ohgw\"].get(\"eta_h\", 1e-3))\n        batch = next(iter(train_loader))\n        new_hparams = ohgw_warm_start(model, batch, {\n            \"weight_decay\": torch.tensor(weight_decay, requires_grad=True, device=device),\n            \"label_smoothing\": torch.tensor(label_smoothing, requires_grad=True, device=device)\n        }, device, eta_h, num_classes)\n        weight_decay = new_hparams[\"weight_decay\"]\n        label_smoothing = new_hparams[\"label_smoothing\"]\n        # Re-instantiate optimizer with updated weight-decay\n        if opt_name == \"sgd\":\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n        else:\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    # Criterion is implicit inside loops (for label smoothing), so nothing to instantiate\n\n    # Training Loop\n    epochs = int(cfg[\"training\"].get(\"epochs\", 10))\n    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n    best_val_acc = 0.0\n    start_time = time()\n    for epoch in range(1, epochs + 1):\n        ep_train_loss, ep_train_acc = train_one_epoch(model, train_loader, None, optimizer, device, label_smoothing, num_classes)\n        ep_val_loss, ep_val_acc = evaluate(model, val_loader, device, label_smoothing, num_classes)\n\n        history[\"train_loss\"].append(ep_train_loss)\n        history[\"val_loss\"].append(ep_val_loss)\n        history[\"train_acc\"].append(ep_train_acc)\n        history[\"val_acc\"].append(ep_val_acc)\n        best_val_acc = max(best_val_acc, ep_val_acc)\n\n        # Progress bar / log\n        tqdm.write(f\"[Run:{run_id}] Epoch {epoch}/{epochs} \u2013 train_loss:{ep_train_loss:.4f}, val_acc:{ep_val_acc*100:.2f}%\")\n\n    total_time = time() - start_time\n\n    # Final Test Evaluation\n    test_loss, test_acc = evaluate(model, test_loader, device, label_smoothing, num_classes)\n\n    # Save model if requested\n    if cfg.get(\"save_model\", False):\n        torch.save(model.state_dict(), run_dir / \"model.pt\")\n\n    # Save curves\n    figure_files = save_curves(history, run_dir, run_id)\n\n    # Metrics to JSON\n    metrics = {\n        \"run_id\": run_id,\n        \"best_val_accuracy\": best_val_acc,\n        \"final_test_accuracy\": test_acc,\n        \"final_test_loss\": test_loss,\n        \"training_time_sec\": total_time,\n        \"epochs\": epochs,\n        \"hyperparameters\": {\n            \"learning_rate\": lr,\n            \"weight_decay\": weight_decay,\n            \"momentum\": momentum,\n            \"label_smoothing\": label_smoothing\n        },\n        \"figures\": figure_files,\n    }\n\n    # Persist metrics\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # ------------------------------------------------------------------\n    # Experiment description + JSON results to STDOUT (for GH-Actions)\n    # ------------------------------------------------------------------\n    description = {\n        \"description\": cfg.get(\"description\", \"No description provided.\"),\n        \"dataset\": cfg.get(\"dataset\", {}),\n        \"model\": cfg.get(\"model\", {}),\n        \"algorithm\": cfg.get(\"algorithm\", {}),\n        \"training\": cfg.get(\"training\", {}),\n    }\n    print(json.dumps({\"experiment_description\": description}, indent=2))\n    print(json.dumps({\"experimental_results\": metrics}, indent=2))\n\nif __name__ == \"__main__\":\n    main()"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "The base package already hard-codes a concrete dataset loader (CIFAR-10 branch inside src/preprocess.py).  According to the “No Premature Specialization” rule the base layer must **not** contain real dataset-specific loading logic – it should expose only placeholders that will be filled later.  Including real torchvision.CIFAR10 handling violates requirement 7 (and partially requirement 2), therefore the current foundation is not yet acceptable as the generic base code."
}
