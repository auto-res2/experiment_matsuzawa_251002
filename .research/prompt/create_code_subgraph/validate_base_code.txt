
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation
   - main.py implements 1 GPU per run variation allocation, queueing runs sequentially when GPUs are insufficient
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Generated Base Code Files
{"evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\nAggregates *post-hoc* results from all experimental variations inside results_dir\nand produces comparison figures \u0026 JSON summary.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # headless\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nFIG_NAMING = {\n    \"loss_curve\": \"training_loss.pdf\",\n    \"accuracy_bar\": \"accuracy_bar.pdf\",\n}\n\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _load_results(results_dir: Path) -\u003e List[Dict]:\n    runs: List[Dict] = []\n    for result_file in results_dir.rglob(\"results.json\"):\n        with open(result_file, \"r\") as f:\n            runs.append(json.load(f))\n    if not runs:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n    return runs\n\n\ndef _plot_loss_curves(runs: List[Dict], save_dir: Path) -\u003e None:\n    plt.figure(figsize=(6, 4))\n    for run in runs:\n        y = run[\"train_loss_curve\"]\n        x = list(range(1, len(y) + 1))\n        plt.plot(x, y, label=run[\"run_id\"])\n        # Annotate final value\n        plt.annotate(f\"{y[-1]:.2f}\", xy=(x[-1], y[-1]), xytext=(3, 0), textcoords=\"offset points\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Training Loss\")\n    plt.title(\"Training Loss Curves\")\n    plt.legend()\n    plt.tight_layout()\n    out_path = save_dir / FIG_NAMING[\"loss_curve\"]\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef _plot_accuracy_bar(runs: List[Dict], save_dir: Path) -\u003e None:\n    plt.figure(figsize=(6, 4))\n    run_ids = [r[\"run_id\"] for r in runs]\n    accuracies = [r[\"final_test_acc\"] for r in runs]\n    sns.barplot(x=run_ids, y=accuracies, palette=\"deep\")\n    for idx, acc in enumerate(accuracies):\n        plt.text(idx, acc + 0.5, f\"{acc:.2f}\", ha=\"center\")\n    plt.ylabel(\"Test Accuracy (%)\")\n    plt.title(\"Final Test Accuracy Comparison\")\n    plt.tight_layout()\n    out_path = save_dir / FIG_NAMING[\"accuracy_bar\"]\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args() -\u003e argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Aggregate \u0026 visualise experiment results\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory with sub-folders for each run variation\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    results_dir = Path(args.results_dir)\n    runs = _load_results(results_dir)\n\n    # Produce figures\n    _plot_loss_curves(runs, results_dir)\n    _plot_accuracy_bar(runs, results_dir)\n\n    # Consolidate numeric table\n    summary = {\n        run[\"run_id\"]: {\n            \"val_acc\": run[\"final_val_acc\"],\n            \"test_acc\": run[\"final_test_acc\"],\n            \"best_epoch\": run[\"best_epoch\"],\n        } for run in runs\n    }\n\n    # Print JSON summary to stdout\n    print(json.dumps({\"comparison_summary\": summary}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "experiments:\n  - run_id: \"baseline_full\"\n    seed: 42\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"  # PLACEHOLDER: actual dataset name will be injected later\n      batch_size: 128\n    model:\n      name: \"MODEL_PLACEHOLDER\"    # PLACEHOLDER: real model will be specified in next phase\n      params: {}\n    training:\n      epochs: 90\n      optimizer: \"sgd\"\n      lr: 0.1\n      momentum: 0.9\n      lr_scheduler:\n        step_size: 30\n        gamma: 0.1\n  # Additional experiment variations will be appended in the derive_specific stage\n", "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nOrchestrator script: reads experiment YAML, launches one subprocess per run\nvariation, allocates GPUs, collects logs, and finally triggers evaluation.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport queue\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport yaml\nimport torch\n\n# -----------------------------------------------------------------------------\n# Logging helpers\n# -----------------------------------------------------------------------------\n\ndef _tee_stream(stream, filepath: Path):\n    \"\"\"Write `stream` both to stdout/stderr and to `filepath`.\"\"\"\n\n    f = open(filepath, \"wb\")\n\n    def _reader():\n        for line in iter(stream.readline, b\"\"):\n            sys.stdout.buffer.write(line)\n            f.write(line)\n        stream.close()\n        f.close()\n\n    t = threading.Thread(target=_reader)\n    t.daemon = True\n    t.start()\n    return t\n\n\n# -----------------------------------------------------------------------------\n# GPU assignment utilities\n# -----------------------------------------------------------------------------\n\ndef _available_gpus() -\u003e List[int]:\n    return list(range(torch.cuda.device_count()))\n\n\n# -----------------------------------------------------------------------------\n# Subprocess worker\n# -----------------------------------------------------------------------------\n\ndef _launch_run(run_cfg: Dict[str, Any], all_cfg_path: Path, results_dir: Path, gpu_id: int, smoke_test: bool):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    stdout_log = run_dir / \"stdout.log\"\n    stderr_log = run_dir / \"stderr.log\"\n\n    cmd = [\n        sys.executable, \"-m\", \"src.train\",\n        \"--config\", str(all_cfg_path),\n        \"--run-id\", run_id,\n        \"--results-dir\", str(results_dir),\n    ]\n    if smoke_test:\n        cmd.append(\"--smoke-test\")\n\n    env = os.environ.copy()\n    env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, bufsize=1)\n    # Tee output\n    _tee_stream(proc.stdout, stdout_log)\n    _tee_stream(proc.stderr, stderr_log)\n    return proc\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args() -\u003e argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Run full experiment suite or smoke test\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Execute smoke test experiments\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Execute full experiment suite\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory where all outputs will be saved\")\n    return parser.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n    config_file = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    with open(config_file, \"r\") as f:\n        cfg_root: Dict[str, Any] = yaml.safe_load(f)\n\n    runs_cfg: List[Dict[str, Any]] = cfg_root[\"experiments\"]\n    gpus = _available_gpus() or [None]  # CPU fallback\n    gpu_queue: \"queue.Queue[int]\" = queue.Queue()\n    for gid in gpus:\n        gpu_queue.put(gid)\n\n    processes: List[subprocess.Popen] = []\n\n    # ------------------------------------------------------------------\n    # Launch loop \u2013 ensures max concurrency = #GPUs\n    # ------------------------------------------------------------------\n    for run_cfg in runs_cfg:\n        gpu_id = gpu_queue.get()  # blocks until a GPU becomes free\n        print(json.dumps({\"event\": \"launch\", \"run_id\": run_cfg[\"run_id\"], \"gpu_id\": gpu_id}))\n        proc = _launch_run(run_cfg, config_file, results_dir, gpu_id=gpu_id if gpu_id is not None else 0, smoke_test=args.smoke_test)\n        proc.gpu_id = gpu_id  # type: ignore\n        processes.append(proc)\n\n        # If all GPUs busy, wait for at least one to free\n        while len(processes) \u003e= len(gpus):\n            done, processes = _wait_for_any(processes, gpu_queue)\n            if done:\n                break\n\n    # Wait for remaining\n    while processes:\n        _wait_for_any(processes, gpu_queue)\n\n    # ------------------------------------------------------------------\n    # After all runs complete \u2013 call evaluator\n    # ------------------------------------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\n# -----------------------------------------------------------------------------\n# Helper: Wait for *any* process to finish and recycle its GPU\n# -----------------------------------------------------------------------------\n\ndef _wait_for_any(processes: List[subprocess.Popen], gpu_queue: \"queue.Queue[int]\") -\u003e tuple[bool, List[subprocess.Popen]]:\n    \"\"\"Blocks until at least one subprocess terminates, updates list, recycles GPU.\"\"\"\n    import time\n    while True:\n        for proc in processes:\n            ret = proc.poll()\n            if ret is not None:\n                # Finished\n                gpu_queue.put(proc.gpu_id)  # type: ignore\n                processes.remove(proc)\n                return True, processes\n        time.sleep(1)  # avoid busy-wait\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py\nHouses *all* model architectures required by the experiments.  \nOnly dataset / model specific placeholders are allowed.  \nThe base logic for instantiation and save/load is **fully implemented** here.\n\"\"\"\nfrom __future__ import annotations\n\nimport torch\nimport torch.nn as nn\n\n# -----------------------------------------------------------------------------\n# Toy baseline network \u2013 always available (works for any 32\u00d732 RGB input)\n# -----------------------------------------------------------------------------\n\n\nclass _ToyNet(nn.Module):\n    \"\"\"A very small ConvNet suitable for smoke tests and synthetic data.\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 16\u00d716\u00d716\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 32\u00d78\u00d78\n            nn.Flatten(),\n        )\n        self.classifier = nn.Linear(32 * 8 * 8, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n\n\n# -----------------------------------------------------------------------------\n# Factory\n# -----------------------------------------------------------------------------\n\ndef build_model(model_cfg: dict, num_classes: int):\n    \"\"\"Constructs the desired model according to `model_cfg`.\n\n    Expected keys in model_cfg:\n      \u2013 name: str (\"MODEL_PLACEHOLDER\", \"toynet\", \"resnet18\", ...)\n      \u2013 params: dict (optional, model-specific hyper-parameters)\n    \"\"\"\n    name = model_cfg.get(\"name\", \"MODEL_PLACEHOLDER\").lower()\n    params = model_cfg.get(\"params\", {})\n\n    if name in {\"model_placeholder\", \"toynet\"}:\n        return _ToyNet(num_classes=num_classes)\n    elif name == \"resnet18\":\n        from torchvision.models import resnet18  # lazy import\n        return resnet18(num_classes=num_classes)\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Additional models will be added in specific experiment phases\n    # ------------------------------------------------------------------\n    else:\n        raise ValueError(f\"Model \u0027{name}\u0027 not recognised in common core foundation.\")\n", "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py\nShared data-loading \u0026 preprocessing utilities.\nSupports real datasets *and* generates synthetic data when placeholders are used\nso that the common core can run without replacement.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split, TensorDataset\n\n# -----------------------------------------------------------------------------\n# Constants\n# -----------------------------------------------------------------------------\n\n_DATA_ROOT = Path.home() / \".cache\" / \"ohgw_data\"\n_DATA_ROOT.mkdir(parents=True, exist_ok=True)\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any], smoke_test: bool = False) -\u003e Tuple[DataLoader, DataLoader, DataLoader, int]:\n    \"\"\"Builds train/val/test dataloaders according to `dataset_cfg`.\n\n    Returns: (train_loader, val_loader, test_loader, num_classes)\n    \"\"\"\n    name: str = dataset_cfg.get(\"name\", \"DATASET_PLACEHOLDER\")\n    batch_size: int = dataset_cfg.get(\"batch_size\", 32)\n\n    if name == \"DATASET_PLACEHOLDER\":\n        return _synthetic_classification(batch_size=batch_size, smoke_test=smoke_test)\n    elif name.lower() == \"cifar10\":\n        return _cifar10(batch_size=batch_size, smoke_test=smoke_test)\n    else:\n        raise ValueError(f\"Dataset {name} not recognised by common core foundation.\")\n\n\n# -----------------------------------------------------------------------------\n# Dataset implementations\n# -----------------------------------------------------------------------------\n\ndef _synthetic_classification(batch_size: int, smoke_test: bool) -\u003e Tuple[DataLoader, DataLoader, DataLoader, int]:\n    \"\"\"Generates a toy dataset of random images (3\u00d732\u00d732) with 10 classes.\"\"\"\n    total = 1024 if not smoke_test else 128\n    img_shape = (3, 32, 32)\n    num_classes = 10\n    rng = np.random.RandomState(0)\n    data = rng.rand(total, *img_shape).astype(np.float32)\n    labels = rng.randint(0, num_classes, size=(total,)).astype(np.int64)\n\n    tensor_x = torch.tensor(data)\n    tensor_y = torch.tensor(labels)\n    dataset = TensorDataset(tensor_x, tensor_y)\n\n    n_val = total // 10\n    n_test = total // 10\n    n_train = total - n_val - n_test\n    train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test],\n                                             generator=torch.Generator().manual_seed(42))\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, test_loader, num_classes\n\n\ndef _cifar10(batch_size: int, smoke_test: bool) -\u003e Tuple[DataLoader, DataLoader, DataLoader, int]:\n    transform = T.Compose([\n        T.ToTensor(),\n        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\n    train_set = torchvision.datasets.CIFAR10(root=_DATA_ROOT, train=True, download=True, transform=transform)\n    test_set = torchvision.datasets.CIFAR10(root=_DATA_ROOT, train=False, download=True, transform=transform)\n\n    # Split off validation\n    val_ratio = 0.1\n    n_total = len(train_set)\n    n_val = int(n_total * val_ratio)\n    n_train = n_total - n_val\n    train_ds, val_ds = random_split(train_set, [n_train, n_val],\n                                    generator=torch.Generator().manual_seed(42))\n\n    if smoke_test:\n        # Subsample to speed up\n        train_ds = torch.utils.data.Subset(train_ds, list(range(0, len(train_ds), 50)))\n        val_ds = torch.utils.data.Subset(val_ds, list(range(0, len(val_ds), 50)))\n        test_set = torch.utils.data.Subset(test_set, list(range(0, len(test_set), 50)))\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    return train_loader, val_loader, test_loader, 10\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nrequires-python = \"\u003e=3.9\"\nauthors = [\n  {name = \"Researcher\", email = \"researcher@example.com\"}\n]\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntorchvision = \"\u003e=0.15.0\"\npyyaml = \"^6.0\"\nmatplotlib = \"^3.7.0\"\nseaborn = \"^0.12.2\"\nnumpy = \"^1.23\"\ntqdm = \"^4.65.0\"\n", "smoke_test_yaml": "experiments:\n  - run_id: \"baseline_smoke\"\n    seed: 42\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"  # PLACEHOLDER: replace with actual dataset name in specific experiments\n      batch_size: 32\n    model:\n      name: \"MODEL_PLACEHOLDER\"    # PLACEHOLDER: replace with specific model in subsequent steps\n      params: {}\n    training:\n      epochs: 2   # Kept minimal for smoke test\n      optimizer: \"adam\"\n      lr: 0.001\n", "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nRuns a single experimental variation defined in a YAML configuration file.  \nThis script is **never** aware of any other variations \u2013 orchestration is the job of main.py.\nIt must therefore be completely self-contained: setup, train, save artefacts, print JSON.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport random\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom . import preprocess  # noqa: E402 \u2013 local import\nfrom . import model as model_lib  # noqa: E402 \u2013 local import\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\n\ndef set_seed(seed: int) -\u003e None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy(output: torch.Tensor, target: torch.Tensor, topk: Tuple[int, ...] = (1,)) -\u003e List[torch.Tensor]:\n    \"\"\"Computes the precision@k for the specified values of k.\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        # Obtain top-k predictions\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res: List[torch.Tensor] = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\n# -----------------------------------------------------------------------------\n# Trainer class \u2013 keeps state and checkpointing logic together\n# -----------------------------------------------------------------------------\n\nclass Trainer:\n    def __init__(self, cfg: Dict[str, Any], results_dir: Path, smoke_test: bool):\n        self.cfg = cfg\n        self.run_id: str = cfg[\"run_id\"]\n        self.device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.results_dir = results_dir\n        self.checkpoint_dir = self.results_dir / \"checkpoints\"\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.history: Dict[str, List[float]] = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n        self.best_val_acc: float = -float(\"inf\")\n        self.best_epoch: int = -1\n        self.smoke_test = smoke_test\n\n        set_seed(cfg.get(\"seed\", 42))\n\n        # ------------------------------------------------------------------\n        # Data preparation\n        # ------------------------------------------------------------------\n        self.train_loader, self.val_loader, self.test_loader, num_classes = preprocess.get_dataloaders(cfg[\"dataset\"], smoke_test)\n\n        # ------------------------------------------------------------------\n        # Model \u0026 optimisation\n        # ------------------------------------------------------------------\n        self.model = model_lib.build_model(cfg[\"model\"], num_classes)\n        self.model.to(self.device)\n\n        train_cfg = cfg[\"training\"]\n        lr: float = train_cfg.get(\"lr\", 1e-3)\n        optimizer_name: str = train_cfg.get(\"optimizer\", \"adam\").lower()\n        if optimizer_name == \"sgd\":\n            self.optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=train_cfg.get(\"momentum\", 0.9))\n        elif optimizer_name == \"adam\":\n            self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        else:\n            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n\n        self.criterion = nn.CrossEntropyLoss().to(self.device)\n        self.epochs: int = train_cfg.get(\"epochs\", 10)\n\n        # Scheduler (optional)\n        sched_cfg = train_cfg.get(\"lr_scheduler\", None)\n        if sched_cfg:\n            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=sched_cfg.get(\"step_size\", 30),\n                                                       gamma=sched_cfg.get(\"gamma\", 0.1))\n        else:\n            self.scheduler = None\n\n    # ------------------------------------------------------------------\n    # Training / validation loops\n    # ------------------------------------------------------------------\n    def train_one_epoch(self, epoch: int) -\u003e float:\n        self.model.train()\n        running_loss: float = 0.0\n        pbar = tqdm(self.train_loader, desc=f\"[{self.run_id}] Epoch {epoch} \u2013 train\", leave=False)\n        for inputs, targets in pbar:\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            loss.backward()\n            self.optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        epoch_loss = running_loss / len(self.train_loader.dataset)\n        self.history[\"train_loss\"].append(epoch_loss)\n        return epoch_loss\n\n    @torch.no_grad()\n    def validate(self, epoch: int) -\u003e Tuple[float, float]:\n        self.model.eval()\n        running_loss: float = 0.0\n        correct_top1: float = 0.0\n        total: int = 0\n        for inputs, targets in self.val_loader:\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n\n            acc1 = accuracy(outputs, targets, topk=(1,))[0]\n            correct_top1 += acc1.item() * inputs.size(0) / 100.0\n            total += inputs.size(0)\n\n        epoch_loss = running_loss / len(self.val_loader.dataset)\n        epoch_acc = (correct_top1 / total) * 100.0\n        self.history[\"val_loss\"].append(epoch_loss)\n        self.history[\"val_acc\"].append(epoch_acc)\n        return epoch_loss, epoch_acc\n\n    # ------------------------------------------------------------------\n    # Checkpoint helpers\n    # ------------------------------------------------------------------\n    def _save_checkpoint(self, epoch: int, val_acc: float) -\u003e None:\n        ckpt_path = self.checkpoint_dir / f\"epoch_{epoch}.pt\"\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": self.model.state_dict(),\n            \"optimizer_state\": self.optimizer.state_dict(),\n            \"val_acc\": val_acc,\n            \"rng_state\": torch.get_rng_state(),\n        }, ckpt_path)\n\n        # Track best\n        if val_acc \u003e self.best_val_acc:\n            best_path = self.checkpoint_dir / \"best_model.pt\"\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state\": self.model.state_dict(),\n                \"optimizer_state\": self.optimizer.state_dict(),\n                \"val_acc\": val_acc,\n            }, best_path)\n            self.best_val_acc = val_acc\n            self.best_epoch = epoch\n\n    def _load_best(self) -\u003e None:\n        best_path = self.checkpoint_dir / \"best_model.pt\"\n        if not best_path.exists():\n            raise FileNotFoundError(f\"Best checkpoint not found for run {self.run_id}\")\n        state = torch.load(best_path, map_location=self.device)\n        self.model.load_state_dict(state[\"model_state\"])\n\n    # ------------------------------------------------------------------\n    # Public driver\n    # ------------------------------------------------------------------\n    def fit(self) -\u003e Dict[str, Any]:\n        for epoch in range(1, self.epochs + 1):\n            train_loss = self.train_one_epoch(epoch)\n            val_loss, val_acc = self.validate(epoch)\n\n            if self.scheduler is not None:\n                self.scheduler.step()\n\n            # Logging to console\n            print(json.dumps({\n                \"run_id\": self.run_id,\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n            }))\n\n            # Save checkpoint for this epoch \u0026 best\n            self._save_checkpoint(epoch, val_acc)\n\n            # Early stop (optional simple patience)\n            if self.cfg[\"training\"].get(\"early_stopping_patience\") is not None:\n                patience = self.cfg[\"training\"].get(\"early_stopping_patience\")\n                if epoch - self.best_epoch \u003e patience:\n                    break\n\n        # ------------------------------------------------------------------\n        # Final evaluation on validation and test sets using best model\n        # ------------------------------------------------------------------\n        self._load_best()\n        final_val_loss, final_val_acc = self.validate(self.best_epoch)\n        final_test_loss, final_test_acc = self._evaluate_test_set()\n\n        final_metrics = {\n            \"run_id\": self.run_id,\n            \"best_epoch\": self.best_epoch,\n            \"final_val_loss\": final_val_loss,\n            \"final_val_acc\": final_val_acc,\n            \"final_test_loss\": final_test_loss,\n            \"final_test_acc\": final_test_acc,\n            \"train_loss_curve\": self.history[\"train_loss\"],\n            \"val_loss_curve\": self.history[\"val_loss\"],\n            \"val_acc_curve\": self.history[\"val_acc\"],\n        }\n        return final_metrics\n\n    @torch.no_grad()\n    def _evaluate_test_set(self) -\u003e Tuple[float, float]:\n        self.model.eval()\n        running_loss: float = 0.0\n        correct_top1: float = 0.0\n        total: int = 0\n        for inputs, targets in self.test_loader:\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n\n            acc1 = accuracy(outputs, targets, topk=(1,))[0]\n            correct_top1 += acc1.item() * inputs.size(0) / 100.0\n            total += inputs.size(0)\n\n        test_loss = running_loss / len(self.test_loader.dataset)\n        test_acc = (correct_top1 / total) * 100.0\n        return test_loss, test_acc\n\n\n# -----------------------------------------------------------------------------\n# CLI Interface\n# -----------------------------------------------------------------------------\n\ndef parse_args() -\u003e argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental variation\")\n    parser.add_argument(\"--config\", required=True, type=str, help=\"Path to YAML file with all experiment configs\")\n    parser.add_argument(\"--run-id\", required=True, type=str, help=\"Identifier of the run variation to execute\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory where outputs will be stored\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run a quick smoke-test version (fewer epochs, tiny data)\")\n    return parser.parse_args()\n\n\ndef main():\n    from yaml import safe_load  # Imported here to keep global namespace clean\n\n    args = parse_args()\n    with open(args.config, \"r\") as f:\n        config_root: Dict[str, Any] = safe_load(f)\n\n    # Look up the specific run config\n    matching = [c for c in config_root[\"experiments\"] if c[\"run_id\"] == args.run_id]\n    if not matching:\n        raise ValueError(f\"Run-id {args.run_id} not found in config file {args.config}\")\n    cfg = matching[0]\n\n    # Adjust epochs for smoke test if requested \u2013 do *not* modify placeholder scope\n    if args.smoke_test:\n        cfg = json.loads(json.dumps(cfg))  # deep copy via JSON\n        cfg[\"training\"][\"epochs\"] = min(2, cfg[\"training\"].get(\"epochs\", 2))\n\n    run_dir = Path(args.results_dir) / cfg[\"run_id\"]\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    trainer = Trainer(cfg, run_dir, args.smoke_test)\n    final_metrics = trainer.fit()\n\n    # Save results to disk\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(final_metrics, f, indent=2)\n\n    # Pretty print summary to stdout (JSON-formatted per requirement)\n    print(json.dumps(final_metrics))\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Violates “No Premature Specialization” criterion – preprocess.py already contains concrete CIFAR-10 loading logic (real dataset handling). The base foundation must not include real dataset-specific code; only clear placeholders are allowed. Replace concrete dataset code with placeholders or synthetic data only."
}
