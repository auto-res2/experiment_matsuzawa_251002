
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['asha-baseline', 'asha-random-warm', 'asha-ohgw-1step', 'asha-ohgw-3step']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Current Experiment (to generate code for)
- Experiment ID: exp-1
- Description: Objective / Hypothesis: Quantify the raw efficiency gains of One-Shot Hyper-Gradient Warm-Starts (OHGW) for classical image-classification hyper-parameter optimisation and disentangle the effect of merely perturbing the hyper-parameters from using the true hyper-gradient.

Domain & Task: Vision – CIFAR-10 image classification.

Models: ResNet-20 (main), ResNet-50 (sanity-check on a deeper network – evaluated only for the best scheduler variant after selection).

Schedulers under test: ASHA (official implementation in Ray-Tune v2.6).

Search space (5-dim continuous): log10-learning-rate, log10-weight-decay, momentum, RandAugment-magnitude, label-smoothing.

Run variations:
• asha-baseline – vanilla ASHA.
• asha-random-warm – perform one random Gaussian step (σ=0.01) in ψ instead of a hyper-gradient step (control).* 
• asha-ohgw-1step – proposed method (η_h=1e-3, one step).
• asha-ohgw-3step – same but apply three successive hyper-gradient steps (ablation for diminishing returns).

Datasets & Pre-processing: CIFAR-10, standard train/val/test split = 45k/5k/10k.  Per-image Z-score normalisation, RandAugment (m searchable), Cutout(16), label-smoothing (ε searchable).  Validation set is stratified.

Training loop: 200 epochs (budget for full training), Cosine LR, batch size 128, SGD-M.  Weight initialisation He-normal.  Mixed precision (AMP).  Early-stopping OFF (to reveal full curve).

Evaluation metrics:
Primary – T@93%: wall-clock hours & GPU hours needed to first reach 93% val accuracy.
Secondary – best test accuracy after 200 epochs; AUC(score-vs-time); overhead-FLOPs ratio; peak VRAM; seed-wise variance.

Protocol: 32 independent HPO replicates (seed-paired across variations).  Each replicate is allowed 12 A100-GPU hours (4 GPUs×3 h) exactly.  Statistics: median±IQR, Wilcoxon signed-rank (α=0.05) on paired T@93%.

Hyper-parameter sensitivity: inside asha-ohgw-1step, sweep η_h ∈ {1e-4, 3e-4, 1e-3, 3e-3} on an extra 8 seeds; fit LOESS to quantify curvature.

Robustness: inject 10% symmetric label noise on a duplicate run; compare % speed-up retained.

Compute efficiency: PyTorch-Profiler to record FLOPs; NVML + psutil for VRAM/RAM; timestamps logged by Lightning Callbacks.

Example code fragment (within Ray trial):
```python
# one-shot HG warm-start
loss = model(train_mb)                    # forward
hyper_grads = torch.autograd.grad(loss, cfg.continuous())
with torch.no_grad():
    for p, g in zip(cfg.continuous(), hyper_grads):
        p -= 1e-3 * g                     # η_h
# hand control back to ASHA
train_full(cfg)
```

Computational budget on cluster: 8 concurrent trials * 4 GPUs each = 32 GPUs (< half of node).  Wall-clock expected 18h incl. repeats.

Success criteria: ≥15% lower median T@93% for asha-ohgw-1step than asha-baseline with p<0.05 and ≤0.2% drop in final test accuracy.

Footnote *: random-warm uses identical σ distribution as the empirical magnitude of a hyper-gradient step, ensuring fair energy injection.
- Run Variations: ['asha-baseline', 'asha-random-warm', 'asha-ohgw-1step', 'asha-ohgw-3step']

# Base Code
{'train_py': 'import argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\ndef set_seed(seed: int):\n    """Set random seed for reproducibility."""\n    import random\n    import numpy as np\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy(output: torch.Tensor, target: torch.Tensor) -> float:\n    """Compute top-1 accuracy."""\n    with torch.no_grad():\n        preds = output.argmax(dim=1)\n        correct = preds.eq(target).sum().item()\n    return correct / target.size(0)\n\n\ndef train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * x.size(0)\n        running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            output = model(x)\n            loss = criterion(output, y)\n            running_loss += loss.item() * x.size(0)\n            running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef run_experiment(cfg: Dict[str, Any], results_dir: Path, smoke_test: bool):\n    run_id = cfg["run_id"]\n    seed = cfg.get("seed", 0)\n    set_seed(seed)\n\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n    batch_size = cfg["training"].get("batch_size", 128)\n    num_workers = cfg["training"].get("num_workers", 4)\n    max_epochs = cfg["training"].get("epochs", 20)\n    threshold = cfg["evaluation"].get("threshold")  # Optional\n\n    # Adjust for smoke test.\n    if smoke_test:\n        max_epochs = min(2, max_epochs)\n        batch_size = min(32, batch_size)\n\n    train_loader, val_loader, test_loader, num_classes, input_shape = get_dataloaders(\n        cfg["dataset"], batch_size=batch_size, num_workers=num_workers, smoke_test=smoke_test\n    )\n\n    model = get_model(cfg["model"], num_classes=num_classes, input_shape=input_shape).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer_cfg = cfg["training"].get("optimizer", {"name": "SGD", "lr": 0.1, "momentum": 0.9})\n    if optimizer_cfg["name"].lower() == "sgd":\n        optimizer = optim.SGD(model.parameters(), lr=optimizer_cfg["lr"], momentum=optimizer_cfg.get("momentum", 0))\n    elif optimizer_cfg["name"].lower() == "adam":\n        optimizer = optim.Adam(model.parameters(), lr=optimizer_cfg["lr"])\n    else:\n        raise ValueError(f"Unsupported optimizer {optimizer_cfg[\'name\']}")\n\n    results = {\n        "run_id": run_id,\n        "config": cfg,\n        "epoch_metrics": [],\n        "best_val_accuracy": 0.0,\n        "best_epoch": 0,\n        "time_to_threshold": None,\n    }\n\n    start_time = time.time()\n    for epoch in range(1, max_epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        elapsed = time.time() - start_time\n\n        results["epoch_metrics"].append(\n            {\n                "epoch": epoch,\n                "train_loss": train_loss,\n                "train_acc": train_acc,\n                "val_loss": val_loss,\n                "val_acc": val_acc,\n                "elapsed_sec": elapsed,\n            }\n        )\n\n        if val_acc > results["best_val_accuracy"]:\n            results["best_val_accuracy"] = val_acc\n            results["best_epoch"] = epoch\n            # Save checkpoint\n            torch.save(model.state_dict(), results_dir / "best_model.pt")\n\n        if threshold is not None and results["time_to_threshold"] is None and val_acc >= threshold:\n            results["time_to_threshold"] = elapsed / 3600  # hours\n\n    # Final test evaluation\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n    results["final_test_accuracy"] = test_acc\n    results["final_test_loss"] = test_loss\n\n    # Persist results\n    with open(results_dir / "results.json", "w") as f:\n        json.dump(results, f, indent=2)\n\n    # Print JSON to stdout for GitHub Actions to capture\n    print(json.dumps(results))\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description="Run a single experiment variation.")\n    parser.add_argument("--config", type=str, required=True, help="Path to run-specific YAML config file.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Directory to store outputs.")\n    parser.add_argument("--smoke-test", action="store_true", help="Run quick smoke test.")\n    args = parser.parse_args()\n\n    import yaml\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    run_dir = Path(args.results_dir)\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    run_experiment(cfg, run_dir, args.smoke_test)\n', 'evaluate_py': 'import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport matplotlib\nmatplotlib.use("Agg")  # For non-interactive backends\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIGURE_PARAMS = {\n    "pdf.fonttype": 42,\n    "ps.fonttype": 42,\n    "font.size": 12,\n    "legend.fontsize": 10,\n}\nplt.rcParams.update(FIGURE_PARAMS)\n\n\ndef load_results(results_dir: Path) -> List[Dict[str, Any]]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / "results.json"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef plot_best_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x="run_id", y="best_val_accuracy", data=df)\n    plt.ylabel("Best Validation Accuracy")\n    plt.xlabel("Run ID")\n    plt.ylim(0, 1)\n    # Annotate bars\n    for idx, row in df.iterrows():\n        plt.text(idx, row["best_val_accuracy"] + 0.01, f"{row[\'best_val_accuracy\']*100:.1f}%", ha="center")\n    plt.tight_layout()\n    plt.savefig(out_path / "accuracy.pdf", bbox_inches="tight")\n    plt.close()\n\n\ndef plot_loss_curves(all_results: List[Dict[str, Any]], out_path: Path):\n    for res in all_results:\n        run_id = res["run_id"]\n        epochs = [m["epoch"] for m in res["epoch_metrics"]]\n        train_losses = [m["train_loss"] for m in res["epoch_metrics"]]\n        val_losses = [m["val_loss"] for m in res["epoch_metrics"]]\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, train_losses, label="Train loss")\n        plt.plot(epochs, val_losses, label="Val loss")\n        plt.xlabel("Epoch")\n        plt.ylabel("Loss")\n        plt.title(f"Loss Curve - {run_id}")\n        # Annotate final values\n        plt.text(epochs[-1], val_losses[-1], f"{val_losses[-1]:.3f}")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(out_path / f"training_loss_{run_id}.pdf", bbox_inches="tight")\n        plt.close()\n\n\ndef main(results_dir: str):\n    results_path = Path(results_dir)\n    all_results = load_results(results_path)\n    if len(all_results) == 0:\n        raise RuntimeError(f"No results.json found in {results_dir}")\n\n    # Convert to pandas DataFrame for easy handling\n    df = pd.DataFrame([\n        {\n            "run_id": r["run_id"],\n            "best_val_accuracy": r["best_val_accuracy"],\n            "time_to_threshold": r.get("time_to_threshold"),\n            "final_test_accuracy": r.get("final_test_accuracy"),\n        }\n        for r in all_results\n    ])\n\n    # Print numerical comparison to stdout\n    comparison = df.to_dict(orient="records")\n    print(json.dumps({"comparison": comparison}, indent=2))\n\n    # Create figure output directory\n    figs_dir = results_path / "figures"\n    figs_dir.mkdir(exist_ok=True)\n\n    # Plot best accuracy bar chart\n    plot_best_accuracy(df, figs_dir)\n\n    # Plot loss curves per run\n    plot_loss_curves(all_results, figs_dir)\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description="Evaluate and compare experiment variations.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Root directory containing all run subdirs.")\n    args = parser.parse_args()\n    main(args.results_dir)\n', 'preprocess_py': '"""Common data loading & preprocessing with dataset placeholders."""\n\nfrom typing import Tuple, Any, Dict\nfrom pathlib import Path\n\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, random_split\n\n\n# PLACEHOLDER: Additional dataset-specific imports can be added here.\n\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any], *, batch_size: int, num_workers: int, smoke_test: bool)\n    -> Tuple[DataLoader, DataLoader, DataLoader, int, Any]:\n    """Return train/val/test DataLoaders, num_classes and input_shape.\n\n    This function implements full data pipeline. Dataset-specific logic is handled\n    by branching on dataset_cfg["type"]. Add new branches when introducing new\n    datasets in derived experiments.\n\n    Returns:\n        train_loader, val_loader, test_loader, num_classes, input_shape\n    """\n    dataset_type = dataset_cfg["type"]\n    data_root = Path(dataset_cfg.get("root", "./data"))\n\n    if dataset_type == "FakeData":  # Generic vision dummy data for smoke tests\n        num_classes = dataset_cfg.get("num_classes", 10)\n        image_size = dataset_cfg.get("image_size", (3, 32, 32))\n        transform = transforms.ToTensor()\n        full_dataset = datasets.FakeData(\n            size=dataset_cfg.get("size", 2000),\n            image_size=image_size,\n            num_classes=num_classes,\n            transform=transform,\n        )\n        val_size = int(0.2 * len(full_dataset))\n        test_size = int(0.1 * len(full_dataset))\n        train_size = len(full_dataset) - val_size - test_size\n        train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n        input_shape = image_size\n\n    elif dataset_type == "DATASET_PLACEHOLDER":\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic in derived experiments\n        raise NotImplementedError("Dataset type DATASET_PLACEHOLDER must be implemented in derived experiment.")\n\n    else:\n        raise ValueError(f"Unknown dataset type: {dataset_type}")\n\n    if smoke_test:\n        # Use small subset for speed\n        train_set = torch.utils.data.Subset(train_set, range(min(256, len(train_set))))\n        val_set = torch.utils.data.Subset(val_set, range(min(256, len(val_set))))\n        test_set = torch.utils.data.Subset(test_set, range(min(256, len(test_set))))\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, test_loader, num_classes, input_shape\n', 'model_py': '"""Model architectures and registry."""\n\nfrom typing import Dict, Any\nimport torch.nn as nn\nimport torch\n\n\nclass MLP(nn.Module):\n    """Simple Multi-Layer Perceptron for tabular or flattened image data."""\n\n    def __init__(self, input_dim: int, num_classes: int, hidden_dims=(256, 128)):\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for h in hidden_dims:\n            layers += [nn.Linear(prev_dim, h), nn.ReLU(inplace=True)]\n            prev_dim = h\n        layers.append(nn.Linear(prev_dim, num_classes))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if x.dim() > 2:\n            x = torch.flatten(x, 1)\n        return self.net(x)\n\n\nclass SimpleCNN(nn.Module):\n    """Minimal CNN suitable for 32×32 images – serves as baseline.\n    This keeps dependencies minimal yet provides a working example."""\n\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# Registry mapping model_type string to constructor\n_MODEL_REGISTRY = {\n    "MLP": MLP,\n    "SimpleCNN": SimpleCNN,\n    "MODEL_PLACEHOLDER": None,  # PLACEHOLDER: replaced in derivative experiments\n}\n\n\ndef get_model(model_cfg: Dict[str, Any], *, num_classes: int, input_shape):\n    """Return instantiated nn.Module based on config."""\n    model_type = model_cfg["type"]\n    if model_type == "MLP":\n        if isinstance(input_shape, (tuple, list)):\n            flat_dim = int(torch.prod(torch.tensor(input_shape)))\n        else:\n            flat_dim = input_shape\n        return MLP(flat_dim, num_classes)\n    elif model_type == "SimpleCNN":\n        return SimpleCNN(num_classes)\n    elif model_type == "MODEL_PLACEHOLDER":\n        # PLACEHOLDER: Replace with specific model logic in derived experiments\n        raise NotImplementedError("MODEL_PLACEHOLDER must be replaced in the derived experiment step.")\n    else:\n        raise ValueError(f"Unknown model type {model_type}")\n', 'main_py': 'import argparse\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport yaml\nimport json\nimport datetime\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    """Run subprocess while teeing its stdout / stderr to files and console."""\n    with stdout_path.open("wb") as out_f, stderr_path.open("wb") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1)\n        assert process.stdout is not None and process.stderr is not None\n        # Stream stdout\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if not stdout_line and not stderr_line and process.poll() is not None:\n                break\n            if stdout_line:\n                sys.stdout.buffer.write(stdout_line)\n                out_f.write(stdout_line)\n                out_f.flush()\n            if stderr_line:\n                sys.stderr.buffer.write(stderr_line)\n                err_f.write(stderr_line)\n                err_f.flush()\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n\ndef launch_train(run_cfg: Dict[str, Any], results_root: Path, smoke_test: bool):\n    run_id = run_cfg["run_id"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save run-specific config so that train.py can read it.\n    cfg_path = run_dir / "config.yaml"\n    with cfg_path.open("w") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # Build subprocess command\n    cmd = [\n        sys.executable,\n        "-m",\n        "src.train",\n        "--config",\n        str(cfg_path),\n        "--results-dir",\n        str(run_dir),\n    ]\n    if smoke_test:\n        cmd.append("--smoke-test")\n\n    stdout_path = run_dir / "stdout.log"\n    stderr_path = run_dir / "stderr.log"\n    tee_subprocess(cmd, stdout_path, stderr_path)\n\n\ndef run_all(cfg_file: str, results_dir: str, smoke_test: bool):\n    with open(cfg_file) as f:\n        exp_cfg = yaml.safe_load(f)\n\n    if "experiments" not in exp_cfg:\n        raise KeyError("Config YAML must contain \'experiments\' list.")\n\n    results_root = Path(results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    description = exp_cfg.get("description", "No description provided.")\n    print(\n        json.dumps(\n            {\n                "timestamp": datetime.datetime.now().isoformat(),\n                "experiment_description": description,\n            },\n            indent=2,\n        )\n    )\n\n    for run_cfg in exp_cfg["experiments"]:\n        print(json.dumps({"status": "starting", "run_id": run_cfg["run_id"]}))\n        launch_train(run_cfg, results_root, smoke_test)\n        print(json.dumps({"status": "finished", "run_id": run_cfg["run_id"]}))\n\n    # After all runs, launch evaluation\n    eval_cmd = [sys.executable, "-m", "src.evaluate", "--results-dir", str(results_root)]\n    tee_subprocess(eval_cmd, results_root / "evaluate_stdout.log", results_root / "evaluate_stderr.log")\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description="Main orchestrator for OHGW experiments.")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true", help="Run smoke test config.")\n    group.add_argument("--full-experiment", action="store_true", help="Run full experiment config.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Directory where results will be stored.")\n    args = parser.parse_args()\n\n    root_dir = Path(__file__).resolve().parent.parent  # project root\n    if args.smoke_test:\n        cfg_file = root_dir / "config" / "smoke_test.yaml"\n    else:\n        cfg_file = root_dir / "config" / "full_experiment.yaml"\n\n    run_all(str(cfg_file), args.results_dir, args.smoke_test)\n', 'pyproject_toml': '[build-system]\nrequires = ["setuptools>=42", "wheel"]\n\n[project]\nname = "ohgw-experiments"\nversion = "0.1.0"\nrequires-python = ">=3.9"\ndependencies = [\n    "torch>=2.0",\n    "torchvision>=0.15",\n    "tqdm",\n    "pyyaml",\n    "pandas",\n    "matplotlib",\n    "seaborn",\n    "scikit-learn",\n]\n', 'smoke_test_yaml': 'description: "Smoke test for CI – uses FakeData and minimal epochs."\nexperiments:\n  - run_id: smoke_simplecnn\n    seed: 0\n    dataset:\n      type: FakeData  # PLACEHOLDER will be replaced with real dataset in full experiments\n      size: 500\n      num_classes: 10\n    model:\n      type: SimpleCNN  # baseline model for dummy data\n    training:\n      batch_size: 32\n      epochs: 2\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    evaluation:\n      threshold: 0.2  # arbitrary low threshold to satisfy metric\n', 'full_experiment_yaml': 'description: "FULL EXPERIMENT CONFIG – TO BE POPULATED WITH REAL DATASETS AND MODELS IN DERIVED STEP"\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER_1  # PLACEHOLDER: replace with actual identifier\n    seed: 0\n    dataset: DATASET_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    training: SPECIFIC_CONFIG_PLACEHOLDER\n    evaluation:\n      threshold: SPECIFIC_CONFIG_PLACEHOLDER\n  # Additional experiments can be added here\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: timm/resnet200d.ra2_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet200d.ra2_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)


from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet200d.ra2_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 128, 128])
    #  torch.Size([1, 256, 64, 64])
    #  torch.Size([1, 512, 32, 32])
    #  torch.Size([1, 1024, 16, 16])
    #  torch.Size([1, 2048, 8, 8])

    print(o.shape)


from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet200d.ra2_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 8, 8) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: timm/resnetrs200.tf_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnetrs200.tf_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetrs200.tf_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 128, 128])
    #  torch.Size([1, 256, 64, 64])
    #  torch.Size([1, 512, 32, 32])
    #  torch.Size([1, 1024, 16, 16])
    #  torch.Size([1, 2048, 8, 8])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetrs200.tf_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 8, 8) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: BVRA/resnet18.in1k_ft_df20_299

- Code: import timm
import torch
import torchvision.transforms as T
from PIL import Image
from urllib.request import urlopen
model = timm.create_model("hf-hub:BVRA/resnet18.in1k_ft_df20_299", pretrained=True)
model = model.eval()
train_transforms = T.Compose([T.Resize((299, 299)), 
                              T.ToTensor(), 
                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) 
img = Image.open(PATH_TO_YOUR_IMAGE)
output = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor


- ID: BVRA/resnet50.in1k_ft_df20_299

- Code: import timm
import torch
import torchvision.transforms as T
from PIL import Image
from urllib.request import urlopen
model = timm.create_model("hf-hub:BVRA/resnet50.in1k_ft_df20_299", pretrained=True)
model = model.eval()
train_transforms = T.Compose([T.Resize((299, 299)), 
                              T.ToTensor(), 
                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) 
img = Image.open(PATH_TO_YOUR_IMAGE)
output = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor
# output is a (1, num_features) shaped tensor



- ID: timm/resnet50.a1_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm
import torch

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet50.a1_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.a1_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.a1_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: timm/wide_resnet50_2.racm_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('wide_resnet50_2.racm_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'wide_resnet50_2.racm_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 256, 56, 56])
    #  torch.Size([1, 512, 28, 28])
    #  torch.Size([1, 1024, 14, 14])
    #  torch.Size([1, 2048, 7, 7])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'wide_resnet50_2.racm_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: timm/resnet50.ram_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet50.ram_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.ram_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 256, 56, 56])
    #  torch.Size([1, 512, 28, 28])
    #  torch.Size([1, 1024, 14, 14])
    #  torch.Size([1, 2048, 7, 7])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.ram_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: microsoft/resnet-50

- Code: from transformers import AutoImageProcessor, ResNetForImageClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
model = ResNetForImageClassification.from_pretrained("microsoft/resnet-50")

inputs = processor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])


- ID: timm/resnetv2_50x1_bit.goog_in21k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnetv2_50x1_bit.goog_in21k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)



from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetv2_50x1_bit.goog_in21k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 256, 56, 56])
    #  torch.Size([1, 512, 28, 28])
    #  torch.Size([1, 1024, 14, 14])
    #  torch.Size([1, 2048, 7, 7])

    print(o.shape)



from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetv2_50x1_bit.goog_in21k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: stanford-crfm/alias-gpt2-small-x21

- Code: from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("stanford-crfm/alias-gpt2-small-x21")

model = AutoModelForCausalLM.from_pretrained("stanford-crfm/alias-gpt2-small-x21")



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: uoft-cs/cifar10


- ID: uoft-cs/cifar100


- ID: tanganke/cifar100


- ID: tanganke/cifar10


- ID: uoft-cs/cifar100


- ID: tanganke/cifar100


- ID: yehzw/wikitext-103


- ID: vish26/wikitext-103-v1-cleaned





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\ndef set_seed(seed: int):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy(output: torch.Tensor, target: torch.Tensor) -> float:\n    \"\"\"Compute top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        preds = output.argmax(dim=1)\n        correct = preds.eq(target).sum().item()\n    return correct / target.size(0)\n\n\ndef train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for x, y in loader:\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast():\n            output = model(x)\n            loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * x.size(0)\n        running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast():\n                output = model(x)\n                loss = criterion(output, y)\n            running_loss += loss.item() * x.size(0)\n            running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef _apply_random_warm(model: nn.Module, sigma: float):\n    \"\"\"Additive Gaussian noise to model parameters (control experiment).\"\"\"\n    with torch.no_grad():\n        for p in model.parameters():\n            if p.requires_grad:\n                p.add_(torch.randn_like(p) * sigma)\n\n\ndef _apply_ohgw(model: nn.Module, loader: DataLoader, device: torch.device, eta: float, steps: int, criterion):\n    \"\"\"One-Shot Hyper-Gradient Warm-Start approximation – we take a few miniature SGD steps\n    on a single mini-batch prior to the main training loop. This is an *approximation* of\n    the method described in the paper and keeps implementation effort minimal while\n    still touching the right compute graph pieces for demonstration purposes.\"\"\"\n    model.train()\n    mb_iter = iter(loader)\n    try:\n        x, y = next(mb_iter)\n    except StopIteration:\n        return  # empty loader in smoke tests\n    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\n    for _ in range(steps):\n        model.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast():\n            loss = criterion(model(x), y)\n        loss.backward()\n        with torch.no_grad():\n            for p in model.parameters():\n                if p.grad is not None:\n                    p.add_( -eta * p.grad )\n\n\ndef maybe_apply_warm_start(model: nn.Module, train_loader: DataLoader, device: torch.device, training_cfg: Dict[str, Any], criterion):\n    ws_cfg = training_cfg.get(\"warm_start\")\n    if ws_cfg is None:\n        return  # vanilla training\n    ws_type = ws_cfg[\"type\"].lower()\n    if ws_type == \"random\":\n        sigma = float(ws_cfg.get(\"sigma\", 0.01))\n        _apply_random_warm(model, sigma)\n    elif ws_type == \"ohgw\":\n        eta = float(ws_cfg.get(\"eta\", 1e-3))\n        steps = int(ws_cfg.get(\"steps\", 1))\n        _apply_ohgw(model, train_loader, device, eta, steps, criterion)\n    else:\n        raise ValueError(f\"Unknown warm_start type: {ws_type}\")\n\n\ndef run_experiment(cfg: Dict[str, Any], results_dir: Path, smoke_test: bool):\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 0)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = cfg[\"training\"].get(\"batch_size\", 128)\n    num_workers = cfg[\"training\"].get(\"num_workers\", 4)\n    max_epochs = cfg[\"training\"].get(\"epochs\", 20)\n    threshold = cfg[\"evaluation\"].get(\"threshold\")  # Optional\n\n    # Adjust for smoke test\n    if smoke_test:\n        max_epochs = min(2, max_epochs)\n        batch_size = min(32, batch_size)\n\n    train_loader, val_loader, test_loader, num_classes, input_shape = get_dataloaders(\n        cfg[\"dataset\"], batch_size=batch_size, num_workers=num_workers, smoke_test=smoke_test\n    )\n\n    model = get_model(cfg[\"model\"], num_classes=num_classes, input_shape=input_shape).to(device)\n\n    # Criterion – support label smoothing if requested\n    label_smoothing = float(cfg[\"dataset\"].get(\"label_smoothing\", 0.0))\n    try:\n        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n    except TypeError:  # Older torch\n        criterion = nn.CrossEntropyLoss()\n\n    optimizer_cfg = cfg[\"training\"].get(\"optimizer\", {\"name\": \"SGD\", \"lr\": 0.1, \"momentum\": 0.9})\n    weight_decay = float(cfg[\"training\"].get(\"weight_decay\", 0.0))\n\n    if optimizer_cfg[\"name\"].lower() == \"sgd\":\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=float(optimizer_cfg[\"lr\"]),\n            momentum=float(optimizer_cfg.get(\"momentum\", 0.0)),\n            weight_decay=weight_decay,\n        )\n    elif optimizer_cfg[\"name\"].lower() == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=float(optimizer_cfg[\"lr\"]), weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer {optimizer_cfg['name']}\")\n\n    # Cosine LR schedule\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n\n    # Optional warm-start step(s)\n    maybe_apply_warm_start(model, train_loader, device, cfg[\"training\"], criterion)\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": cfg,\n        \"epoch_metrics\": [],\n        \"best_val_accuracy\": 0.0,\n        \"best_epoch\": 0,\n        \"time_to_threshold\": None,\n    }\n\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler()\n    for epoch in range(1, max_epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        scheduler.step()\n\n        elapsed = time.time() - start_time\n\n        results[\"epoch_metrics\"].append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n                \"elapsed_sec\": elapsed,\n                \"lr\": optimizer.param_groups[0][\"lr\"],\n            }\n        )\n\n        if val_acc > results[\"best_val_accuracy\"]:\n            results[\"best_val_accuracy\"] = val_acc\n            results[\"best_epoch\"] = epoch\n            torch.save(model.state_dict(), results_dir / \"best_model.pt\")\n\n        if threshold is not None and results[\"time_to_threshold\"] is None and val_acc >= threshold:\n            results[\"time_to_threshold\"] = elapsed / 3600.0  # convert to hours\n\n    # Final test evaluation\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n    results[\"final_test_accuracy\"] = test_acc\n    results[\"final_test_loss\"] = test_loss\n\n    # Save results\n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Emit to stdout for CI visibility\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run-specific YAML config file.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store outputs.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test.\")\n    args = parser.parse_args()\n\n    import yaml\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    run_dir = Path(args.results_dir)\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    run_experiment(cfg, run_dir, args.smoke_test)\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # For non-interactive backends\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIGURE_PARAMS = {\n    \"pdf.fonttype\": 42,\n    \"ps.fonttype\": 42,\n    \"font.size\": 12,\n    \"legend.fontsize\": 10,\n}\nplt.rcParams.update(FIGURE_PARAMS)\n\n\ndef load_results(results_dir: Path) -> List[Dict[str, Any]]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef plot_best_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=\"best_val_accuracy\", data=df)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.ylim(0, 1)\n    # Annotate bars\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"best_val_accuracy\"] + 0.01, f\"{row['best_val_accuracy']*100:.1f}%\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(out_path / \"accuracy.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef plot_loss_curves(all_results: List[Dict[str, Any]], out_path: Path):\n    for res in all_results:\n        run_id = res[\"run_id\"]\n        epochs = [m[\"epoch\"] for m in res[\"epoch_metrics\"]]\n        train_losses = [m[\"train_loss\"] for m in res[\"epoch_metrics\"]]\n        val_losses = [m[\"val_loss\"] for m in res[\"epoch_metrics\"]]\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, train_losses, label=\"Train loss\")\n        plt.plot(epochs, val_losses, label=\"Val loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"Loss Curve - {run_id}\")\n        # Annotate final values\n        plt.text(epochs[-1], val_losses[-1], f\"{val_losses[-1]:.3f}\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(out_path / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n        plt.close()\n\n\ndef main(results_dir: str):\n    results_path = Path(results_dir)\n    all_results = load_results(results_path)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json found in {results_dir}\")\n\n    # Convert to pandas DataFrame for easy handling\n    df = pd.DataFrame([\n        {\n            \"run_id\": r[\"run_id\"],\n            \"best_val_accuracy\": r[\"best_val_accuracy\"],\n            \"time_to_threshold\": r.get(\"time_to_threshold\"),\n            \"final_test_accuracy\": r.get(\"final_test_accuracy\"),\n        }\n        for r in all_results\n    ])\n\n    # Print numerical comparison to stdout\n    comparison = df.to_dict(orient=\"records\")\n    print(json.dumps({\"comparison\": comparison}, indent=2))\n\n    # Create figure output directory\n    figs_dir = results_path / \"figures\"\n    figs_dir.mkdir(exist_ok=True)\n\n    # Plot best accuracy bar chart\n    plot_best_accuracy(df, figs_dir)\n\n    # Plot loss curves per run\n    plot_loss_curves(all_results, figs_dir)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory containing all run subdirs.\")\n    args = parser.parse_args()\n    main(args.results_dir)\n",
    "preprocess_py": "\"\"\"Data loading & augmentation utilities for vision benchmarks (CIFAR-10 for this\nexperiment) and a FakeData fallback for CI smoke tests.\"\"\"\n\nfrom typing import Tuple, Any, Dict\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\n\ndef _cifar10_transforms(train: bool, randaugment_magnitude: int, cutout: bool):\n    \"\"\"Return torchvision transforms suitable for CIFAR-10 training / evaluation.\"\"\"\n    if train:\n        augments = [\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n        ]\n        if randaugment_magnitude > 0:\n            augments.append(transforms.RandAugment(num_ops=2, magnitude=randaugment_magnitude))\n        augments.append(transforms.ToTensor())\n        augments.append(transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)))\n        if cutout:\n            # Use RandomErasing as Cutout analogue\n            augments.append(transforms.RandomErasing(p=0.5, scale=(0.05, 0.15)))\n        return transforms.Compose(augments)\n    else:\n        return transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n            ]\n        )\n\n\ndef get_dataloaders(\n    dataset_cfg: Dict[str, Any],\n    *,\n    batch_size: int,\n    num_workers: int,\n    smoke_test: bool,\n) -> Tuple[DataLoader, DataLoader, DataLoader, int, Any]:\n    \"\"\"Create train / val / test dataloaders.\n\n    Args:\n        dataset_cfg: configuration dict\n        batch_size: per-loader batch size\n        num_workers: dataloader workers\n        smoke_test: drastically reduce dataset sizes for CI\n\n    Returns:\n        train_loader, val_loader, test_loader, num_classes, input_shape\n    \"\"\"\n\n    dataset_type = dataset_cfg[\"type\"].lower()\n    data_root = Path(dataset_cfg.get(\"root\", \"./data\"))\n    rand_m = int(dataset_cfg.get(\"randaugment_magnitude\", 9))\n    cutout = bool(dataset_cfg.get(\"cutout\", True))\n    val_split = float(dataset_cfg.get(\"val_split\", 0.1))\n\n    if dataset_type == \"cifar10\":\n        train_transform = _cifar10_transforms(True, rand_m, cutout)\n        test_transform = _cifar10_transforms(False, rand_m, cutout)\n\n        full_train = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n        test_set = datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_transform)\n\n        val_size = int(len(full_train) * val_split)\n        train_size = len(full_train) - val_size\n        train_set, val_set = random_split(full_train, [train_size, val_size])\n\n        num_classes = 10\n        input_shape = (3, 32, 32)\n\n    elif dataset_type == \"fakedata\":\n        # Simple synthetic dataset for smoke tests\n        num_classes = int(dataset_cfg.get(\"num_classes\", 10))\n        image_size = tuple(dataset_cfg.get(\"image_size\", (3, 32, 32)))\n        transform = transforms.ToTensor()\n        full_dataset = datasets.FakeData(\n            size=int(dataset_cfg.get(\"size\", 2000)),\n            image_size=image_size,\n            num_classes=num_classes,\n            transform=transform,\n        )\n        val_size = int(0.2 * len(full_dataset))\n        test_size = int(0.1 * len(full_dataset))\n        train_size = len(full_dataset) - val_size - test_size\n        train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n        input_shape = image_size\n\n    else:\n        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n\n    if smoke_test:\n        # Clip each split to 256 samples for speed\n        train_set = torch.utils.data.Subset(train_set, range(min(256, len(train_set))))\n        val_set = torch.utils.data.Subset(val_set, range(min(256, len(val_set))))\n        test_set = torch.utils.data.Subset(test_set, range(min(256, len(test_set))))\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n    return train_loader, val_loader, test_loader, num_classes, input_shape\n",
    "model_py": "\"\"\"Model definitions & registry for CIFAR-10 experiments (ResNet-20 / ResNet-50).\nThe implementations follow the originalTiny ResNet paper (He et al. 2016) with\nminor adaptations for 32×32 images.\"\"\"\n\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\n\n\n#########################################\n#               Blocks                 #\n#########################################\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes: int, planes: int, stride: int = 1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.downsample = None\n        if stride != 1 or in_planes != planes:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n    def forward(self, x):\n        identity = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes: int, planes: int, stride: int = 1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        if stride != 1 or in_planes != planes * self.expansion:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * self.expansion),\n            )\n\n    def forward(self, x):\n        identity = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n\n\n#########################################\n#              ResNet                 #\n#########################################\n\nclass _ResNetCIFAR(nn.Module):\n    \"\"\"Generic ResNet for CIFAR-style small images.\"\"\"\n\n    def __init__(self, block, layers, num_classes=10):\n        super().__init__()\n        self.in_planes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64 * block.expansion, num_classes)\n\n        # Parameter initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes: int, blocks: int, stride: int):\n        layers = [block(self.in_planes, planes, stride)]\n        self.in_planes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_planes, planes, 1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\nclass ResNet20(_ResNetCIFAR):\n    def __init__(self, num_classes=10):\n        super().__init__(BasicBlock, [3, 3, 3], num_classes=num_classes)\n\n\nclass ResNet32(_ResNetCIFAR):\n    def __init__(self, num_classes=10):\n        super().__init__(BasicBlock, [5, 5, 5], num_classes=num_classes)\n\n\nclass ResNet44(_ResNetCIFAR):\n    def __init__(self, num_classes=10):\n        super().__init__(BasicBlock, [7, 7, 7], num_classes=num_classes)\n\n\nclass ResNet56(_ResNetCIFAR):\n    def __init__(self, num_classes=10):\n        super().__init__(BasicBlock, [9, 9, 9], num_classes=num_classes)\n\n\nclass ResNet50(_ResNetCIFAR):\n    \"\"\"Bottleneck-based ResNet-50 adapted for 32×32 images (depth 50).\"\"\"\n\n    def __init__(self, num_classes=10):\n        super().__init__(Bottleneck, [3, 4, 6], num_classes=num_classes)\n\n\n#########################################\n#              Registry               #\n#########################################\n\n_MODEL_REGISTRY = {\n    \"ResNet20\": ResNet20,\n    \"ResNet32\": ResNet32,\n    \"ResNet44\": ResNet44,\n    \"ResNet56\": ResNet56,\n    \"ResNet50\": ResNet50,\n}\n\n\ndef get_model(model_cfg: Dict[str, Any], *, num_classes: int, input_shape):\n    model_type = model_cfg[\"type\"]\n    if model_type not in _MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model type: {model_type}. Available: {list(_MODEL_REGISTRY.keys())}\")\n    return _MODEL_REGISTRY[model_type](num_classes=num_classes)\n",
    "main_py": "import argparse\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport yaml\nimport json\nimport datetime\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run subprocess while teeing its stdout / stderr to files and console.\"\"\"\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1)\n        assert process.stdout is not None and process.stderr is not None\n        # Stream stdout\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if not stdout_line and not stderr_line and process.poll() is not None:\n                break\n            if stdout_line:\n                sys.stdout.buffer.write(stdout_line)\n                out_f.write(stdout_line)\n                out_f.flush()\n            if stderr_line:\n                sys.stderr.buffer.write(stderr_line)\n                err_f.write(stderr_line)\n                err_f.flush()\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n\ndef launch_train(run_cfg: Dict[str, Any], results_root: Path, smoke_test: bool):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save run-specific config so that train.py can read it.\n    cfg_path = run_dir / \"config.yaml\"\n    with cfg_path.open(\"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # Build subprocess command\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--config\",\n        str(cfg_path),\n        \"--results-dir\",\n        str(run_dir),\n    ]\n    if smoke_test:\n        cmd.append(\"--smoke-test\")\n\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    tee_subprocess(cmd, stdout_path, stderr_path)\n\n\ndef run_all(cfg_file: str, results_dir: str, smoke_test: bool):\n    with open(cfg_file) as f:\n        exp_cfg = yaml.safe_load(f)\n\n    if \"experiments\" not in exp_cfg:\n        raise KeyError(\"Config YAML must contain 'experiments' list.\")\n\n    results_root = Path(results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    description = exp_cfg.get(\"description\", \"No description provided.\")\n    print(\n        json.dumps(\n            {\n                \"timestamp\": datetime.datetime.now().isoformat(),\n                \"experiment_description\": description,\n            },\n            indent=2,\n        )\n    )\n\n    for run_cfg in exp_cfg[\"experiments\"]:\n        print(json.dumps({\"status\": \"starting\", \"run_id\": run_cfg[\"run_id\"]}))\n        launch_train(run_cfg, results_root, smoke_test)\n        print(json.dumps({\"status\": \"finished\", \"run_id\": run_cfg[\"run_id\"]}))\n\n    # After all runs, launch evaluation\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    tee_subprocess(eval_cmd, results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for OHGW experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke test config.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment config.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where results will be stored.\")\n    args = parser.parse_args()\n\n    root_dir = Path(__file__).resolve().parent.parent  # project root\n    if args.smoke_test:\n        cfg_file = root_dir / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_file = root_dir / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_file), args.results_dir, args.smoke_test)\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\n\n[project]\nname = \"ohgw-experiments\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"torch>=2.0\",\n    \"torchvision>=0.15\",\n    \"tqdm\",\n    \"pyyaml\",\n    \"pandas\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"scikit-learn\",\n]\n",
    "smoke_test_yaml": "description: \"Smoke test for CI – covers all scheduler variations with dummy data.\"\nexperiments:\n  - run_id: smoke_asha_baseline\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n      image_size: [3, 32, 32]\n    model:\n      type: ResNet20\n    training:\n      batch_size: 32\n      epochs: 2\n      num_workers: 0\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    evaluation:\n      threshold: 0.2\n  - run_id: smoke_asha_random_warm\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n      image_size: [3, 32, 32]\n    model:\n      type: ResNet20\n    training:\n      batch_size: 32\n      epochs: 2\n      num_workers: 0\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n      warm_start:\n        type: random\n        sigma: 0.01\n    evaluation:\n      threshold: 0.2\n  - run_id: smoke_asha_ohgw_1step\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n      image_size: [3, 32, 32]\n    model:\n      type: ResNet20\n    training:\n      batch_size: 32\n      epochs: 2\n      num_workers: 0\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n      warm_start:\n        type: ohgw\n        eta: 0.001\n        steps: 1\n    evaluation:\n      threshold: 0.2\n  - run_id: smoke_asha_ohgw_3step\n    seed: 0\n    dataset:\n      type: FakeData\n      size: 500\n      num_classes: 10\n      image_size: [3, 32, 32]\n    model:\n      type: ResNet20\n    training:\n      batch_size: 32\n      epochs: 2\n      num_workers: 0\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n      warm_start:\n        type: ohgw\n        eta: 0.001\n        steps: 3\n    evaluation:\n      threshold: 0.2\n",
    "full_experiment_yaml": "description: \"Exp-1: CIFAR-10 efficiency gains of One-Shot Hyper-Gradient Warm-Starts (OHGW).\"\nexperiments:\n  - run_id: asha-baseline\n    seed: 0\n    dataset:\n      type: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout: true\n      label_smoothing: 0.1\n      val_split: 0.1\n    model:\n      type: ResNet20\n    training:\n      batch_size: 128\n      epochs: 200\n      num_workers: 4\n      optimizer:\n        name: SGD\n        lr: 0.1\n        momentum: 0.9\n      weight_decay: 0.0001\n    evaluation:\n      threshold: 0.93\n\n  - run_id: asha-random-warm\n    seed: 0\n    dataset:\n      type: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout: true\n      label_smoothing: 0.1\n      val_split: 0.1\n    model:\n      type: ResNet20\n    training:\n      batch_size: 128\n      epochs: 200\n      num_workers: 4\n      optimizer:\n        name: SGD\n        lr: 0.1\n        momentum: 0.9\n      weight_decay: 0.0001\n      warm_start:\n        type: random\n        sigma: 0.01\n    evaluation:\n      threshold: 0.93\n\n  - run_id: asha-ohgw-1step\n    seed: 0\n    dataset:\n      type: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout: true\n      label_smoothing: 0.1\n      val_split: 0.1\n    model:\n      type: ResNet20\n    training:\n      batch_size: 128\n      epochs: 200\n      num_workers: 4\n      optimizer:\n        name: SGD\n        lr: 0.1\n        momentum: 0.9\n      weight_decay: 0.0001\n      warm_start:\n        type: ohgw\n        eta: 0.001\n        steps: 1\n    evaluation:\n      threshold: 0.93\n\n  - run_id: asha-ohgw-3step\n    seed: 0\n    dataset:\n      type: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout: true\n      label_smoothing: 0.1\n      val_split: 0.1\n    model:\n      type: ResNet20\n    training:\n      batch_size: 128\n      epochs: 200\n      num_workers: 4\n      optimizer:\n        name: SGD\n        lr: 0.1\n        momentum: 0.9\n      weight_decay: 0.0001\n      warm_start:\n        type: ohgw\n        eta: 0.001\n        steps: 3\n    evaluation:\n      threshold: 0.93\n"
}
