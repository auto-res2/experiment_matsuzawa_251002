
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments with one GPU per run variation. If GPUs are insufficient, experiments run sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations with 1 GPU per variation)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment in parallel across available GPUs, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. GPU Allocation: main.py assigns 1 GPU per run variation. If available GPUs < total variations, queue remaining runs sequentially.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: The foundation is very close but fails the “complete core-logic implementation” criterion:

• In train.py the call for the optional test evaluation

    _, _, _, test_loader = get_dataloaders(..., include_test=True)

expects 4 return values, while preprocess.get_dataloaders() returns 5 when include_test=True (train_loader, val_loader, num_classes, input_shape, test_loader).  This causes a ValueError as soon as a config sets `has_test_split: true`, so the generic training / evaluation path is broken.

All other criteria (file count, CLI orchestration, placeholder usage, GPU scheduling, model saving/loading, figure generation, PyTorch-only implementation, etc.) are satisfied, but the above bug means the core training-evaluation loop is not yet fully functional for future experiments that include a test split.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom . import preprocess\nfrom . import model as model_lib\n\n\n########################################################################################################################\n# Helper functions                                                                                                     #\n########################################################################################################################\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Set the random seed for reproducibility across torch / numpy / python.\"\"\"\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef _prepare_optimizer(cfg: Dict[str, Any], model: nn.Module) -> optim.Optimizer:\n    lr = cfg.get(\"learning_rate\", 1e-3)\n    momentum = cfg.get(\"other_hyperparameters\", {}).get(\"momentum\", 0.9)\n    weight_decay = cfg.get(\"other_hyperparameters\", {}).get(\"weight_decay\", 0.0)\n\n    # Default to SGD; can be expanded via config.\n    if cfg.get(\"optimizer\", \"sgd\").lower() == \"adam\":\n        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n\n\ndef _accuracy(output: torch.Tensor, target: torch.Tensor, topk: Tuple[int, ...] = (1,)) -> List[float]:\n    \"\"\"Compute accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append((correct_k.mul_(100.0 / batch_size)).item())\n        return res\n\n\ndef train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device, epoch: int, total_epochs: int):\n    model.train()\n    running_loss = 0.0\n    acc_sum, n_batches = 0.0, 0\n\n    pbar = tqdm(loader, desc=f\"Train Epoch [{epoch+1}/{total_epochs}]\", leave=False)\n    for inputs, targets in pbar:\n        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        batch_acc = _accuracy(outputs, targets, topk=(1,))[0]\n        running_loss += loss.item()\n        acc_sum += batch_acc\n        n_batches += 1\n        pbar.set_postfix({\"loss\": f\"{running_loss/n_batches:.4f}\", \"acc\": f\"{acc_sum/n_batches:.2f}\"})\n    return running_loss / n_batches, acc_sum / n_batches\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion, device):\n    model.eval()\n    loss_sum, acc_sum, n_batches = 0.0, 0.0, 0\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss_sum += loss.item()\n            acc_sum += _accuracy(outputs, targets, topk=(1,))[0]\n            n_batches += 1\n    return loss_sum / n_batches, acc_sum / n_batches\n\n\ndef plot_curves(run_dir: Path, run_id: str, epochs: int, train_losses: List[float], val_losses: List[float], val_accs: List[float]):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set_style(\"whitegrid\")\n\n    # Training & Validation Loss ------------------------------------------------------------------\n    plt.figure(figsize=(6, 4))\n    plt.plot(range(1, epochs + 1), train_losses, label=\"Train Loss\")\n    plt.plot(range(1, epochs + 1), val_losses, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training & Validation Loss ({run_id})\")\n    plt.legend()\n    plt.annotate(f\"Final: {val_losses[-1]:.3f}\", xy=(epochs, val_losses[-1]),\n                 xytext=(epochs * 0.7, max(val_losses) * 0.9),\n                 arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n    plt.tight_layout()\n    fname_loss = run_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(fname_loss, bbox_inches=\"tight\")\n    plt.close()\n\n    # Validation Accuracy -------------------------------------------------------------------------\n    plt.figure(figsize=(6, 4))\n    plt.plot(range(1, epochs + 1), val_accs, label=\"Val Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(f\"Validation Accuracy ({run_id})\")\n    plt.legend()\n    plt.annotate(f\"Best: {max(val_accs):.2f}%\", xy=(val_accs.index(max(val_accs))+1, max(val_accs)),\n                 xytext=(epochs * 0.6, max(val_accs) * 0.8),\n                 arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n    plt.tight_layout()\n    fname_acc = run_dir / f\"val_accuracy_{run_id}.pdf\"\n    plt.savefig(fname_acc, bbox_inches=\"tight\")\n    plt.close()\n\n    return str(fname_loss), str(fname_acc)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation (training + evaluation)\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run.\")\n    parser.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run (used in logging paths).\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs (figures, logs, model, metrics) are stored.\")\n    args = parser.parse_args()\n\n    # Prepare directories -------------------------------------------------------------------------------------------------\n    run_dir = Path(args.results_dir) / args.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Load config ----------------------------------------------------------------------------------------------------------\n    import yaml\n    with open(args.config, \"r\") as f:\n        cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    # Seed & device --------------------------------------------------------------------------------------------------------\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data loaders ---------------------------------------------------------------------------------------------------------\n    train_loader, val_loader, num_classes, input_shape, test_loader = preprocess.get_dataloaders(\n        dataset_name=cfg.get(\"dataset\", \"DATASET_PLACEHOLDER\"),  # PLACEHOLDER will be replaced by specific dataset name\n        batch_size=cfg.get(\"batch_size\", 128),\n        val_split=cfg.get(\"val_split\", 0.1),\n        test_split=cfg.get(\"test_split\", 0.1),\n        num_workers=cfg.get(\"num_workers\", 4),\n        include_test=cfg.get(\"has_test_split\", False),\n    )\n\n    # Model & criterion ----------------------------------------------------------------------------------------------------\n    model = model_lib.create_model(model_name=cfg.get(\"model\", \"MODEL_PLACEHOLDER\"),\n                                   num_classes=num_classes, input_shape=input_shape)\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = _prepare_optimizer(cfg, model)\n\n    epochs = int(cfg.get(\"epochs\", 10))\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    val_accs: List[float] = []\n\n    # Training loop --------------------------------------------------------------------------------------------------------\n    best_val_acc = 0.0\n    best_epoch = 0\n    for epoch in range(epochs):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, epochs)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        # Save checkpoint if best\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch + 1\n            torch.save({\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"epoch\": epoch,\n                \"val_acc\": val_acc,\n            }, run_dir / \"best_model.pt\")\n\n        # Epoch-level logging ---------------------------------------------------------------------------------------------\n        print(json.dumps({\n            \"run_id\": args.run_id,\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }))\n        sys.stdout.flush()\n\n    # Final evaluation (optional test set) ---------------------------------------------------------------------------------\n    test_acc = None\n    test_loss = None\n    if cfg.get(\"has_test_split\", False) and test_loader is not None:\n        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n\n    # Figures --------------------------------------------------------------------------------------------------------------\n    loss_fig, acc_fig = plot_curves(run_dir, args.run_id, epochs, train_losses, val_losses, val_accs)\n\n    # Save results ---------------------------------------------------------------------------------------------------------\n    results = {\n        \"run_id\": args.run_id,\n        \"config\": cfg,\n        \"best_val_accuracy\": best_val_acc,\n        \"best_val_epoch\": best_epoch,\n        \"final_val_accuracy\": val_accs[-1],\n        \"test_accuracy\": test_acc,\n        \"epochs\": epochs,\n        \"metrics\": {\n            \"train_loss\": train_losses,\n            \"val_loss\": val_losses,\n            \"val_accuracy\": val_accs,\n        },\n        \"figures\": {\n            \"training_loss\": os.path.basename(loss_fig),\n            \"val_accuracy\": os.path.basename(acc_fig),\n        },\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Print final summary for downstream parsers ---------------------------------------------------------------------------\n    print(json.dumps({\n        \"run_id\": args.run_id,\n        \"best_val_accuracy\": best_val_acc,\n        \"test_accuracy\": test_acc,\n        \"figure_files\": [results[\"figures\"][\"training_loss\"], results[\"figures\"][\"val_accuracy\"]],\n    }))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n########################################################################################################################\n# Evaluation utilities                                                                                                 #\n########################################################################################################################\n\nPLOT_TOPICS = {\n    \"final_accuracy\": \"final_accuracy.pdf\",\n    \"best_val_accuracy\": \"best_val_accuracy.pdf\",\n}\n\ndef load_results(results_dir: Path) -> pd.DataFrame:\n    \"\"\"Aggregate all results.json files into a single DataFrame.\"\"\"\n    records: List[Dict[str, Any]] = []\n    for run_path in results_dir.iterdir():\n        if not run_path.is_dir():\n            continue\n        res_file = run_path / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as f:\n                records.append(json.load(f))\n    return pd.json_normalize(records)\n\n\ndef plot_bar(df: pd.DataFrame, column: str, out_path: Path, title: str):\n    sns.set_style(\"whitegrid\")\n    plt.figure(figsize=(max(4, len(df) * 0.6), 4))\n    ax = sns.barplot(x=\"run_id\", y=column, data=df)\n    for idx, row in df.iterrows():\n        ax.text(idx, row[column] + 0.01, f\"{row[column]:.2f}\", ha=\"center\")\n    plt.title(title)\n    plt.xlabel(\"Run ID\")\n    plt.ylabel(column.replace(\"_\", \" \").title())\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Aggregate and compare experiment results\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory with all run subfolders.\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    df = load_results(results_dir)\n\n    if df.empty:\n        print(json.dumps({\n            \"error\": \"No results found in directory.\",\n            \"results_dir\": str(results_dir)\n        }))\n        return\n\n    # Compute comparison metrics -----------------------------------------------------------------------------------------\n    comparison = {\n        row[\"run_id\"]: {\n            \"best_val_accuracy\": row[\"best_val_accuracy\"],\n            \"final_val_accuracy\": row[\"final_val_accuracy\"],\n            \"test_accuracy\": row.get(\"test_accuracy\", None)\n        }\n        for _, row in df.iterrows()\n    }\n\n    # Figures -----------------------------------------------------------------------------------------------------------\n    plot_bar(df, \"best_val_accuracy\", results_dir / PLOT_TOPICS[\"best_val_accuracy\"], \"Best Validation Accuracy\")\n    plot_bar(df, \"final_val_accuracy\", results_dir / PLOT_TOPICS[\"final_accuracy\"], \"Final Validation Accuracy\")\n\n    # Print comparison JSON ---------------------------------------------------------------------------------------------\n    print(json.dumps({\n        \"comparison\": comparison,\n        \"figure_files\": list(PLOT_TOPICS.values()),\n    }, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"Common preprocessing and data-loading utilities.\n\nThis module defines a fully functional, dataset-agnostic loader with *placeholders* for\nadding specific datasets.  The function `get_dataloaders` **always** returns *five* objects\nin a fixed order to guarantee compatibility with future experiments:\n    1. `train_loader` – DataLoader for training data\n    2. `val_loader`   – DataLoader for validation data\n    3. `num_classes`  – Total number of classes\n    4. `input_shape`  – Shape tuple of a single input sample (*without* batch-dim)\n    5. `test_loader`  – DataLoader for held-out test data (or `None` if `include_test=False`)\n\nOnly the dataset-specific part needs to be replaced in follow-up steps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple, Optional, Any\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# Torchvision is used for a few built-in datasets that can serve as smoke-test targets.\nimport torchvision\nfrom torchvision import transforms\n\n\n########################################################################################################################\n# Synthetic fallback dataset (used for smoke tests if actual dataset placeholders are not replaced yet)                 #\n########################################################################################################################\n\nclass RandomClassificationDataset(Dataset):\n    \"\"\"Randomly generated images and labels for quick smoke tests.\"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, int, int], num_classes: int):\n        self.num_samples = num_samples\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):  # noqa: D401 – simple getitem\n        x = torch.randn(self.input_shape)\n        y = torch.randint(0, self.num_classes, ()).item()\n        return x, y\n\n\n########################################################################################################################\n# Main loader                                                                                                           #\n########################################################################################################################\n\ndef _load_dataset(dataset_name: str, data_dir: Optional[str | Path] = None) -> Tuple[Dataset, Tuple[int, int, int], int]:\n    \"\"\"Return training dataset, input‐shape and number of classes.\n\n    This helper function isolates dataset-specific logic.  It gracefully falls back on a\n    *random* synthetic dataset for unknown names so that the core framework remains fully\n    runnable even before real datasets are plugged in.\n    \"\"\"\n\n    dataset_name = dataset_name.lower()\n    data_dir = Path(data_dir or \"./data\").expanduser()\n    data_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------------------------\n    # PLACEHOLDER: standard datasets go here.  More can be added simply by extending\n    #              the `elif` chain or refactoring into a registry.\n    # ----------------------------------------------------------------------------------\n    if dataset_name in {\"mnist\"}:\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ])\n        train_set = torchvision.datasets.MNIST(root=data_dir, train=True, transform=transform, download=True)\n        input_shape = (1, 28, 28)\n        num_classes = 10\n    elif dataset_name in {\"cifar10\", \"cifar-10\"}:\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n        ])\n        train_set = torchvision.datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=True)\n        input_shape = (3, 32, 32)\n        num_classes = 10\n    # ----------------------------------------------------------------------------------\n    # Default synthetic dataset to guarantee smoke-test viability.\n    # ----------------------------------------------------------------------------------\n    else:\n        num_samples = 1_000\n        input_shape = (3, 32, 32)\n        num_classes = 10\n        train_set = RandomClassificationDataset(num_samples=num_samples, input_shape=input_shape, num_classes=num_classes)\n\n    return train_set, input_shape, num_classes\n\n\ndef get_dataloaders(dataset_name: str,\n                    batch_size: int = 128,\n                    val_split: float = 0.1,\n                    test_split: float = 0.1,\n                    num_workers: int = 4,\n                    include_test: bool = False,\n                    seed: int = 42,\n                    data_dir: Optional[str | Path] = None):\n    \"\"\"Create train / val (/ test) DataLoaders for the specified dataset.\n\n    Parameters\n    ----------\n    dataset_name: str\n        Name of the dataset (case-insensitive).  Non-recognised names fall back to a\n        synthetic random dataset so that the rest of the pipeline keeps working.\n    batch_size: int\n        Samples per batch.\n    val_split: float\n        Fraction of data reserved for validation.\n    test_split: float\n        Fraction reserved for test (only used if `include_test=True`).\n    num_workers: int\n        Dataloader worker processes.\n    include_test: bool\n        Whether to return an actual test DataLoader.  If `False` a dummy `None` is\n        returned to keep the 5-tuple interface intact.\n    seed: int\n        RNG seed for consistent splits.\n    data_dir: str | Path | None\n        Directory for on-disk datasets (ignored by synthetic dataset).\n    \"\"\"\n\n    # Load full training dataset -----------------------------------------------------------------------------------------\n    full_train_set, input_shape, num_classes = _load_dataset(dataset_name, data_dir)\n\n    # Ensure deterministic split -----------------------------------------------------------------------------------------\n    n_total = len(full_train_set)\n    n_val = int(n_total * val_split)\n    n_test = int(n_total * test_split) if include_test else 0\n    n_train = n_total - n_val - n_test\n\n    assert n_train > 0 and n_val > 0, \"Dataset too small for the requested splits.\"\n\n    generator = torch.Generator().manual_seed(seed)\n    train_set, val_set, *rest = random_split(full_train_set, [n_train, n_val, n_test], generator=generator)\n    test_set = rest[0] if include_test else None\n\n    # Build loaders ------------------------------------------------------------------------------------------------------\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    test_loader = None\n    if include_test:\n        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n    return train_loader, val_loader, num_classes, input_shape, test_loader\n",
    "model_py": "\"\"\"Model Zoo with placeholder strategy.\n\nContains simple but *complete* reference architectures that can be swapped for more\ncomplex ones in follow-up steps.  Only the factory function `create_model` needs to be\ncalled by training scripts; all models adhere to a common *nn.Module* interface.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as torchvision_models\n\n\n########################################################################################################################\n# Basic reference architectures                                                                                         #\n########################################################################################################################\n\nclass SimpleCNN(nn.Module):\n    \"\"\"A lightweight CNN suitable for 32×32 images (e.g. CIFAR).\"\"\"\n\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(128, num_classes)\n\n    def forward(self, x):  # noqa: D401 – simple forward\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n\n\nclass MNISTMLP(nn.Module):\n    \"\"\"Very small MLP for 28×28 grayscale images to keep smoke tests lightweight.\"\"\"\n\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(28 * 28, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):  # noqa: D401\n        return self.model(x)\n\n\n########################################################################################################################\n# Model factory                                                                                                         #\n########################################################################################################################\n\ndef create_model(model_name: str, num_classes: int, input_shape: tuple[int, int, int], **kwargs):\n    \"\"\"Return an *nn.Module* instance matching `model_name`.\n\n    The function guarantees a *valid* model is always returned even if a placeholder\n    string is passed – ensuring that the training script never fails in early phases of\n    experiment creation.\n    \"\"\"\n\n    model_name = (model_name or \"MODEL_PLACEHOLDER\").lower()\n\n    # ----------------------------------------------------------------------------------\n    # PLACEHOLDER: swap / extend with research-specific models (e.g. ResNet-20, ViT).\n    # ----------------------------------------------------------------------------------\n    if model_name in {\"simple_cnn\", \"baseline\"}:\n        return SimpleCNN(num_classes)\n    elif model_name in {\"mnist_mlp\"}:\n        return MNISTMLP(num_classes)\n    elif model_name in {\"resnet18\"}:\n        # Use torchvision implementation for convenience.\n        model = torchvision_models.resnet18(weights=None, num_classes=num_classes)\n        # Adjust first conv for 1-channel inputs if needed.\n        if input_shape[0] == 1:\n            model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        return model\n    else:\n        # Fallback to a generic small CNN to keep the pipeline functional.\n        return SimpleCNN(num_classes)\n",
    "main_py": "\"\"\"Main orchestrator handling multi-experiment execution and result aggregation.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nimport time\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport torch\nimport yaml\n\n\n########################################################################################################################\n# Utility functions                                                                                                     #\n########################################################################################################################\n\ndef _stream_subprocess_output(proc: subprocess.Popen, log_fp):\n    \"\"\"Stream a subprocess' stdout/stderr both to console and a log file (tee).\"\"\"\n    while True:\n        line = proc.stdout.readline()  # type: ignore[attr-defined]\n        if not line and proc.poll() is not None:\n            break\n        if line:\n            decoded = line.decode(\"utf-8\", errors=\"replace\")\n            print(decoded, end=\"\")\n            log_fp.write(decoded)\n            log_fp.flush()\n\n\ndef _launch_run(run_cfg: Dict[str, Any], gpu_id: int | None, results_dir: Path) -> subprocess.Popen:\n    \"\"\"Spawn training subprocess for a single run and return *Popen* handle.\"\"\"\n\n    run_id = run_cfg[\"name\"]\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Dump per-run config to YAML so that train.py can read it.\n    cfg_path = run_dir / \"config.yaml\"\n    with open(cfg_path, \"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(cfg_path), \"--run-id\", run_id, \"--results-dir\", str(results_dir)]\n\n    env = os.environ.copy()\n    if gpu_id is not None:\n        env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n    else:\n        env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n    # Redirect stdout / stderr to PIPE so we can tee.\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env)  # type: ignore[arg-type]\n\n    # Log file setup.\n    log_fp = open(run_dir / \"stdout.log\", \"w\", encoding=\"utf-8\")\n\n    # Stream in background.\n    import threading\n\n    t = threading.Thread(target=_stream_subprocess_output, args=(proc, log_fp), daemon=True)\n    t.start()\n\n    # Attach helper attributes for later housekeeping.\n    proc._log_fp = log_fp  # type: ignore[attr-defined]\n    proc._thread = t       # type: ignore[attr-defined]\n    proc._gpu_id = gpu_id  # type: ignore[attr-defined]\n\n    return proc\n\n\ndef _execute_experiments(experiments: List[Dict[str, Any]], results_dir: Path):\n    n_gpus = torch.cuda.device_count()\n    available_gpus = deque(range(n_gpus))\n    pending = deque(experiments)\n    running: List[subprocess.Popen] = []\n\n    while pending or running:\n        # Launch new runs while GPUs are available ------------------------------------------------\n        while pending and (available_gpus or n_gpus == 0):\n            cfg = pending.popleft()\n            gpu_id = available_gpus.popleft() if n_gpus > 0 else None\n            proc = _launch_run(cfg, gpu_id, results_dir)\n            running.append(proc)\n\n        # Poll running processes ------------------------------------------------------------------\n        time.sleep(1)\n        still_running = []\n        for proc in running:\n            if proc.poll() is None:  # Still alive\n                still_running.append(proc)\n            else:\n                # Clean-up finished run -----------------------------------------------------------\n                proc._thread.join()  # type: ignore[attr-defined]\n                proc._log_fp.close()  # type: ignore[attr-defined]\n                if proc._gpu_id is not None:\n                    available_gpus.append(proc._gpu_id)  # type: ignore[attr-defined]\n        running = still_running\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OHGW Experiment Orchestrator\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run the lightweight smoke-test set of experiments.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full experiment list.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where all outputs will be stored.\")\n    args = parser.parse_args()\n\n    # Resolve configuration file -----------------------------------------------------------------------------------------\n    repo_root = Path(__file__).resolve().parents[1]\n    if args.smoke_test:\n        cfg_path = repo_root / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_path = repo_root / \"config\" / \"full_experiment.yaml\"\n\n    with open(cfg_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    experiments = config.get(\"experiments\", [])\n    assert experiments, f\"No experiments found in {cfg_path}.\"\n\n    # Prepare results directory ------------------------------------------------------------------------------------------\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    if results_dir.exists():\n        # Safety: don't overwrite previous results inadvertently.\n        backup_dir = results_dir.with_name(results_dir.name + \"_backup\")\n        shutil.move(str(results_dir), str(backup_dir))\n        print(f\"[main] Existing results dir moved to {backup_dir}\")\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Kick off experiments -----------------------------------------------------------------------------------------------\n    _execute_experiments(experiments, results_dir)\n\n    # Aggregate / evaluate -----------------------------------------------------------------------------------------------\n    subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)], check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"ohgw-experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experimental evaluations\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15.0\",\n    \"matplotlib>=3.7.0\",\n    \"seaborn>=0.12.0\",\n    \"PyYAML>=6.0\",\n    \"pandas>=1.5.0\",\n    \"tqdm>=4.65.0\",\n]\n\n[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n",
    "smoke_test_yaml": "# Lightweight smoke-test config.  Runs quickly on CPU / single GPU using synthetic data.\n\nglobal:\n  num_workers: 2\n  val_split: 0.2\n  test_split: 0.1\n  seed: 42\n\nexperiments:\n  - name: smoke_random\n    dataset: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with specific dataset in real experiments\n    model: MODEL_PLACEHOLDER      # PLACEHOLDER: Will be replaced with specific model architecture\n    epochs: 2\n    batch_size: 32\n    learning_rate: 0.001\n    has_test_split: false\n    other_hyperparameters:\n      momentum: 0.9\n",
    "full_experiment_yaml": "# Template for the *complete* experiment matrix.  To be populated in the next step.\n# Each entry under `experiments:` should specify at least `name`, `dataset`, `model`,\n# and necessary hyper-parameters.  The remaining structure mirrors `smoke_test.yaml`.\n\nexperiments:\n  - name: EXPERIMENT_PLACEHOLDER  # PLACEHOLDER: Will be replaced by actual experiment blocks\n    dataset: DATASET_PLACEHOLDER  # PLACEHOLDER: dataset name (e.g. cifar10)\n    model: MODEL_PLACEHOLDER      # PLACEHOLDER: model architecture (e.g. resnet20)\n    epochs: SPECIFIC_CONFIG_PLACEHOLDER\n    batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    learning_rate: SPECIFIC_CONFIG_PLACEHOLDER\n    has_test_split: SPECIFIC_CONFIG_PLACEHOLDER\n    other_hyperparameters:\n      momentum: SPECIFIC_CONFIG_PLACEHOLDER\n      weight_decay: SPECIFIC_CONFIG_PLACEHOLDER\n      scheduler: SPECIFIC_CONFIG_PLACEHOLDER\n\n# Global-level defaults can optionally be defined here.\n# global:\n#   num_workers: 8\n#   val_split: 0.1\n#   test_split: 0.1\n#   seed: 42\n\n# End of template\n\n# -----------------------------------------------------------------------------------\n# PLACEHOLDER: Devs will replicate the above block per run-variation and fill in real\n#              values (datasets, hyper-params, etc.) before executing full experiments.\n# -----------------------------------------------------------------------------------\n\n# vim: set filetype=yaml:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
}
