
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: Nearly all infrastructure is in place, but two core-criteria are still violated:
1. The code only implements model *saving* (train.py → model.pt).  No complementary model *loading* routine exists anywhere in the 8 files, so requirement 1 “Model saving/loading mechanisms are implemented” is not fully satisfied.
2. main.py’s stdout/stderr tee logic is unreliable:  _stream_with_tee decides whether to forward to sys.stdout or sys.stderr based on `stream.name == "stdout"`, yet `subprocess.Popen(...).stdout`/`.stderr` TextIOWrappers never have this exact name; therefore all lines will be routed to the fallback branch, breaking criterion 4 (“forwarding to main stdout/stderr”).  Fixing these issues is necessary before the base can be considered ready.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport random\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\n\nfrom . import preprocess as pp  # type: ignore\nfrom . import model as mdl  # type: ignore\n\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Utility to set all relevant random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _train_one_epoch(model: nn.Module, dataloader: torch.utils.data.DataLoader, criterion, optimizer, device: torch.device) -> float:\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (inputs, targets) in enumerate(dataloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    epoch_loss = running_loss / len(dataloader.dataset)\n    return epoch_loss\n\n\ndef _evaluate(model: nn.Module, dataloader: torch.utils.data.DataLoader, criterion, device: torch.device):\n    \"\"\"Returns (loss, accuracy) on the given dataloader.\"\"\"\n    model.eval()\n    loss_accum, correct = 0.0, 0\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss_accum += loss.item() * inputs.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == targets).sum().item()\n    total = len(dataloader.dataset)\n    return loss_accum / total, correct / total\n\n\ndef save_state(model: nn.Module, optimizer: optim.Optimizer, epoch: int, path: Path, config: Dict):\n    state = {\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"config\": config,\n    }\n    torch.save(state, path)\n\n\ndef load_state(model: nn.Module, optimizer: optim.Optimizer, path: Path, device: torch.device):\n    state = torch.load(path, map_location=device)\n    model.load_state_dict(state[\"model_state_dict\"])\n    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n    start_epoch = state.get(\"epoch\", 0) + 1\n    return start_epoch\n\n\ndef run_training(run_config: Dict, results_dir: Path):\n    run_id = run_config[\"run_id\"]\n    run_seed = run_config.get(\"seed\", 42)\n    set_seed(run_seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ----- DATA LOADING -----\n    train_loader, val_loader, _ = pp.get_data_loaders(run_config)\n\n    # ----- MODEL -----\n    model = mdl.ModelFactory.create_model(run_config[\"model\"], run_config)\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run_config.get(\"learning_rate\", 1e-3))\n\n    num_epochs = run_config.get(\"epochs\", 1)\n    run_path = results_dir / run_id\n    run_path.mkdir(parents=True, exist_ok=True)\n\n    # Save the exact run configuration for transparency/reproducibility\n    with open(run_path / \"config.yaml\", \"w\") as f:\n        yaml.safe_dump(run_config, f)\n\n    checkpoint_path = run_path / \"model.pt\"\n    start_epoch = 1\n    if checkpoint_path.exists():\n        start_epoch = load_state(model, optimizer, checkpoint_path, device)\n        print(f\"[INFO] Resuming {run_id} from epoch {start_epoch}\")\n\n    epoch_metrics: List[Dict] = []\n\n    for epoch in range(start_epoch, num_epochs + 1):\n        train_loss = _train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = _evaluate(model, val_loader, criterion, device)\n\n        epoch_metrics.append({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n        })\n\n        print(json.dumps({\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"run_id\": run_id,\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n        }))\n\n        # Always keep the latest checkpoint\n        save_state(model, optimizer, epoch, checkpoint_path, run_config)\n\n    final_metrics = epoch_metrics[-1]\n    results_obj = {\n        \"run_id\": run_id,\n        \"config\": run_config,\n        \"epoch_metrics\": epoch_metrics,\n        \"final\": final_metrics,\n    }\n\n    with open(run_path / \"results.json\", \"w\") as f:\n        json.dump(results_obj, f, indent=2)\n\n    # Explicit final JSON for GitHub Actions to pick up\n    print(json.dumps({\n        \"run_id\": run_id,\n        \"status\": \"completed\",\n        \"final_train_loss\": final_metrics[\"train_loss\"],\n        \"final_val_loss\": final_metrics[\"val_loss\"],\n        \"final_val_accuracy\": final_metrics[\"val_accuracy\"],\n    }))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the YAML file with the run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be stored.\")\n    args = parser.parse_args()\n\n    cfg_path = Path(args.config)\n    results_directory = Path(args.results_dir)\n    with open(cfg_path, \"r\") as f:\n        run_configuration = yaml.safe_load(f)\n\n    run_training(run_configuration, results_directory)",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\n\ndef load_all_results(results_dir: Path) -> List[dict]:\n    results = []\n    for run_folder in results_dir.iterdir():\n        if run_folder.is_dir():\n            result_file = run_folder / \"results.json\"\n            if result_file.exists():\n                with open(result_file, \"r\") as f:\n                    results.append(json.load(f))\n    return results\n\n\ndef plot_final_accuracy(results: List[dict], results_dir: Path):\n    run_ids = [r[\"run_id\"] for r in results]\n    accuracies = [r[\"final\"][\"val_accuracy\"] for r in results]\n\n    plt.figure(figsize=(8, 5))\n    ax = sns.barplot(x=run_ids, y=accuracies, palette=\"viridis\")\n    ax.set_ylabel(\"Validation Accuracy\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylim(0, 1)\n\n    # Annotate bars\n    for i, acc in enumerate(accuracies):\n        ax.text(i, acc + 0.01, f\"{acc:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n\n    plt.title(\"Final Validation Accuracy Comparison\")\n    fig_path = results_dir / \"accuracy_comparison.pdf\"\n    plt.savefig(fig_path, bbox_inches=\"tight\")\n    plt.close()\n    return str(fig_path.name)\n\n\ndef summarize_results(results: List[dict]):\n    summary = {\n        r[\"run_id\"]: {\n            \"final_val_accuracy\": r[\"final\"][\"val_accuracy\"],\n            \"final_val_loss\": r[\"final\"][\"val_loss\"],\n        } for r in results\n    }\n    return summary\n\n\ndef main(results_dir: Path):\n    results = load_all_results(results_dir)\n    if not results:\n        print(\"No results found for evaluation.\")\n        return\n\n    description = (\n        \"This evaluation compares the final validation accuracy across all run variations. \"\n        \"Each bar represents one experimental run defined in the configuration file.\"\n    )\n    # Print experiment description first\n    print(description)\n\n    # Generate and save figures\n    figure_name = plot_final_accuracy(results, results_dir)\n\n    # Print numerical data\n    summary = summarize_results(results)\n    print(json.dumps(summary, indent=2))\n\n    # Print figure names\n    print(f\"Generated figure: {figure_name}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Location of all run folders.\")\n    args = parser.parse_args()\n    main(Path(args.results_dir))",
    "preprocess_py": "\"\"\"Common preprocessing utilities with dataset placeholders.\n\nThis module provides a unified data-loading API that *all* experimental\nvariations must use. Only the dataset-specific logic may be overwritten in\nfuture steps. Everything else (normalisation, augmentation hooks, worker\nseed-setting) is fixed to guarantee comparability across experiments.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as T\n\n\nclass _SyntheticClassificationDataset(data.Dataset):\n    \"\"\"A small synthetic dataset used as a fallback when no real dataset is specified.\n\n    It generates random images (3×32×32) and random labels for a configurable\n    number of classes. Although meaningless for research, it ensures that the\n    common core foundation is fully runnable end-to-end even before real\n    datasets are plugged in.\n    \"\"\"\n\n    def __init__(self, length: int = 512, num_classes: int = 10):\n        self.length = length\n        self.num_classes = num_classes\n        self.images = torch.randn(length, 3, 32, 32)\n        self.labels = torch.randint(0, num_classes, (length,))\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n\n# ------------------------------------------------------------------------------------------------------------------\n# PUBLIC API\n# ------------------------------------------------------------------------------------------------------------------\n\ndef get_data_loaders(config: dict) -> Tuple[data.DataLoader, data.DataLoader, data.DataLoader]:\n    \"\"\"Return train/val/test dataloaders according to the experiment config.\n\n    Parameters\n    ----------\n    config : dict\n        Run-specific configuration dictionary read from YAML.\n\n    Returns\n    -------\n    Tuple containing train_loader, val_loader, test_loader\n    \"\"\"\n\n    dataset_name = config.get(\"dataset\", \"DATASET_PLACEHOLDER\")  # PLACEHOLDER: Will be replaced with specific dataset logic\n    batch_size = config.get(\"batch_size\", 64)\n    num_workers = config.get(\"num_workers\", 2)\n\n    # Common transforms – can be extended but *should not* be changed across runs.\n    transform = T.Compose([\n        T.ToTensor(),\n        # Normalisation placeholder (will change with real dataset stats)\n        T.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n    ])\n\n    if dataset_name == \"DATASET_PLACEHOLDER\":\n        # Synthetic fallback ensures pipeline integrity\n        train_set = _SyntheticClassificationDataset(length=512, num_classes=10)\n        val_set = _SyntheticClassificationDataset(length=128, num_classes=10)\n        test_set = _SyntheticClassificationDataset(length=128, num_classes=10)\n    else:\n        # --------------------------------------------------------------\n        # PLACEHOLDER: Will be replaced with real dataset loading logic\n        # --------------------------------------------------------------\n        raise NotImplementedError(\n            \"Dataset loading for '%s' has not yet been implemented. \"\n            \"This will be provided in the dataset-specific derivation step.\" % dataset_name\n        )\n\n    def _seed_worker(worker_id):\n        seed = torch.initial_seed() % 2**32\n        random.seed(seed)\n        np.random.seed(seed)\n\n    g = torch.Generator()\n    g.manual_seed(config.get(\"seed\", 42))\n\n    train_loader = data.DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        worker_init_fn=_seed_worker,\n        generator=g,\n    )\n    val_loader = data.DataLoader(\n        val_set,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        worker_init_fn=_seed_worker,\n        generator=g,\n    )\n    test_loader = data.DataLoader(\n        test_set,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        worker_init_fn=_seed_worker,\n        generator=g,\n    )\n    return train_loader, val_loader, test_loader",
    "model_py": "\"\"\"Model definitions and factory for the common experimental framework.\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# -------------------------------------------------------------\n# Simple CNN: used as default when MODEL_PLACEHOLDER is selected\n# -------------------------------------------------------------\nclass _SimpleCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # 32×16×16\n        x = self.pool(F.relu(self.conv2(x)))  # 64×8×8\n        x = self.pool(F.relu(self.conv3(x)))  # 128×4×4\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\nclass ModelFactory:\n    \"\"\"Factory that instantiates models by symbolic name.\n\n    Only names for which the core framework has an implementation may be\n    referenced here. Experiment-specific models should either extend this class\n    or monkey-patch it in the derivation step.\n    \"\"\"\n\n    @staticmethod\n    def create_model(model_name: str, config: Dict):\n        if model_name == \"MODEL_PLACEHOLDER\":  # PLACEHOLDER: Will be replaced later\n            num_classes = config.get(\"num_classes\", 10)\n            return _SimpleCNN(num_classes=num_classes)\n        else:\n            # ------------------------------------------------------------\n            # PLACEHOLDER FOR ADDITIONAL MODELS (ResNet, ViT, etc.)\n            # ------------------------------------------------------------\n            raise NotImplementedError(\n                f\"Model '{model_name}' is not implemented in the common core.\"\n            )",
    "main_py": "import argparse\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom threading import Thread\nfrom typing import Dict, List\n\nimport yaml\n\n\n# ------------------------------------------------------------------------------------------------------------------\n# HELPER FUNCTIONS\n# ------------------------------------------------------------------------------------------------------------------\n\ndef _stream_redirect(pipe, tee_file, parent_stream):\n    \"\"\"Continuously read *pipe* and write each line to both tee_file and parent_stream.\"\"\"\n    for line in iter(pipe.readline, \"\"):\n        tee_file.write(line)\n        tee_file.flush()\n        parent_stream.write(line)\n        parent_stream.flush()\n    pipe.close()\n\n\ndef _run_subprocess(cmd: List[str], run_id: str, results_dir: Path):\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    with (run_dir / \"stdout.log\").open(\"w\") as stdout_f, (run_dir / \"stderr.log\").open(\"w\") as stderr_f:\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            bufsize=1,\n        )\n\n        # Spawn two threads for tee behaviour\n        t_out = Thread(target=_stream_redirect, args=(process.stdout, stdout_f, sys.stdout), daemon=True)\n        t_err = Thread(target=_stream_redirect, args=(process.stderr, stderr_f, sys.stderr), daemon=True)\n        t_out.start()\n        t_err.start()\n\n        process.wait()\n        t_out.join()\n        t_err.join()\n\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, cmd)\n\n\n# ------------------------------------------------------------------------------------------------------------------\n# MAIN ORCHESTRATION\n# ------------------------------------------------------------------------------------------------------------------\n\ndef _prepare_runs(config: Dict) -> List[Dict]:\n    return config.get(\"runs\", [])\n\n\ndef execute_experiments(config_path: Path, results_dir: Path):\n    with open(config_path, \"r\") as f:\n        experiment_config = yaml.safe_load(f)\n\n    run_variations = _prepare_runs(experiment_config)\n    if not run_variations:\n        raise ValueError(\"No run variations found in the configuration file.\")\n\n    for run_cfg in run_variations:\n        run_id = run_cfg[\"run_id\"]\n\n        # Persist individual run config to a temp file\n        with tempfile.NamedTemporaryFile(\"w\", suffix=\".yaml\", delete=False) as tmp:\n            yaml.safe_dump(run_cfg, tmp)\n            tmp_path = Path(tmp.name)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(tmp_path),\n            \"--results-dir\",\n            str(results_dir),\n        ]\n        print(f\"\\n[MAIN] Launching run '{run_id}'...\")\n        _run_subprocess(cmd, run_id, results_dir)\n\n        # After completing the run, delete the temp config file\n        tmp_path.unlink(missing_ok=True)\n\n    # After all runs – trigger evaluation\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ]\n    print(\"\\n[MAIN] Launching evaluation over all runs...\")\n    _run_subprocess(eval_cmd, \"evaluation\", results_dir)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Experiment Orchestrator\")\n    parser.add_argument(\n        \"--smoke-test\",\n        action=\"store_true\",\n        help=\"Run the lightweight smoke test defined in config/smoke_test.yaml\",\n    )\n    parser.add_argument(\n        \"--full-experiment\",\n        action=\"store_true\",\n        help=\"Run the full experiment defined in config/full_experiment.yaml\",\n    )\n    parser.add_argument(\n        \"--results-dir\",\n        type=str,\n        required=True,\n        help=\"Where all outputs (figures, logs, metrics) should be stored.\",\n    )\n    args = parser.parse_args()\n\n    if args.smoke_test == args.full_experiment:\n        parser.error(\"Select exactly one of --smoke-test or --full-experiment.\")\n\n    cfg_file = Path(\"config/smoke_test.yaml\") if args.smoke_test else Path(\"config/full_experiment.yaml\")\n    results_directory = Path(args.results_dir)\n    results_directory.mkdir(parents=True, exist_ok=True)\n\n    execute_experiments(cfg_file, results_directory)",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ohgw-common-core\"\nversion = \"0.0.1\"\ndescription = \"Common core foundation for OHGW experiments\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\n# Core DL stack\ntorch = \"*\"\ntorchvision = \"*\"\n\n# Utilities\nnumpy = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"",
    "smoke_test_yaml": "# Lightweight smoke test configuration\nexperiment_name: smoke_test\nruns:\n  - run_id: baseline_smoke\n    model: MODEL_PLACEHOLDER           # PLACEHOLDER: Will be replaced with actual model name\n    dataset: DATASET_PLACEHOLDER       # PLACEHOLDER: Will be replaced with actual dataset name\n    epochs: 1\n    batch_size: 16\n    seed: 42\n",
    "full_experiment_yaml": "# Full experiment template\nexperiment_name: full_experiment\nruns:\n  - run_id: baseline_full\n    model: MODEL_PLACEHOLDER           # PLACEHOLDER: Replace with baseline model (e.g., ResNet18)\n    dataset: DATASET_PLACEHOLDER       # PLACEHOLDER: Replace with dataset (e.g., CIFAR10)\n    epochs: 50                         # Will be tuned per-dataset later\n    batch_size: 128\n    learning_rate: 0.001\n    seed: 1\n  - run_id: proposed_full\n    model: MODEL_PLACEHOLDER           # PLACEHOLDER: Replace with proposed model variant\n    dataset: DATASET_PLACEHOLDER       # PLACEHOLDER: Same dataset as baseline\n    epochs: 50\n    batch_size: 128\n    learning_rate: 0.001\n    seed: 2\n# Additional run variations (ablations, scheduler tweaks, etc.) will be appended programmatically in later steps\n"
}
