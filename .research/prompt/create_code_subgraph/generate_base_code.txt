
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback




# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\ndef set_seed(seed: int):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    import random\n    import numpy as np\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy(output: torch.Tensor, target: torch.Tensor) -> float:\n    \"\"\"Compute top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        preds = output.argmax(dim=1)\n        correct = preds.eq(target).sum().item()\n    return correct / target.size(0)\n\n\ndef train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * x.size(0)\n        running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            output = model(x)\n            loss = criterion(output, y)\n            running_loss += loss.item() * x.size(0)\n            running_acc += accuracy(output, y) * x.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef run_experiment(cfg: Dict[str, Any], results_dir: Path, smoke_test: bool):\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 0)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = cfg[\"training\"].get(\"batch_size\", 128)\n    num_workers = cfg[\"training\"].get(\"num_workers\", 4)\n    max_epochs = cfg[\"training\"].get(\"epochs\", 20)\n    threshold = cfg[\"evaluation\"].get(\"threshold\")  # Optional\n\n    # Adjust for smoke test.\n    if smoke_test:\n        max_epochs = min(2, max_epochs)\n        batch_size = min(32, batch_size)\n\n    train_loader, val_loader, test_loader, num_classes, input_shape = get_dataloaders(\n        cfg[\"dataset\"], batch_size=batch_size, num_workers=num_workers, smoke_test=smoke_test\n    )\n\n    model = get_model(cfg[\"model\"], num_classes=num_classes, input_shape=input_shape).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer_cfg = cfg[\"training\"].get(\"optimizer\", {\"name\": \"SGD\", \"lr\": 0.1, \"momentum\": 0.9})\n    if optimizer_cfg[\"name\"].lower() == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=optimizer_cfg[\"lr\"], momentum=optimizer_cfg.get(\"momentum\", 0))\n    elif optimizer_cfg[\"name\"].lower() == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=optimizer_cfg[\"lr\"])\n    else:\n        raise ValueError(f\"Unsupported optimizer {optimizer_cfg['name']}\")\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": cfg,\n        \"epoch_metrics\": [],\n        \"best_val_accuracy\": 0.0,\n        \"best_epoch\": 0,\n        \"time_to_threshold\": None,\n    }\n\n    start_time = time.time()\n    for epoch in range(1, max_epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        elapsed = time.time() - start_time\n\n        results[\"epoch_metrics\"].append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n                \"elapsed_sec\": elapsed,\n            }\n        )\n\n        if val_acc > results[\"best_val_accuracy\"]:\n            results[\"best_val_accuracy\"] = val_acc\n            results[\"best_epoch\"] = epoch\n            # Save checkpoint\n            torch.save(model.state_dict(), results_dir / \"best_model.pt\")\n\n        if threshold is not None and results[\"time_to_threshold\"] is None and val_acc >= threshold:\n            results[\"time_to_threshold\"] = elapsed / 3600  # hours\n\n    # Final test evaluation\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n    results[\"final_test_accuracy\"] = test_acc\n    results[\"final_test_loss\"] = test_loss\n\n    # Persist results\n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Print JSON to stdout for GitHub Actions to capture\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run-specific YAML config file.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store outputs.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test.\")\n    args = parser.parse_args()\n\n    import yaml\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    run_dir = Path(args.results_dir)\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    run_experiment(cfg, run_dir, args.smoke_test)\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # For non-interactive backends\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIGURE_PARAMS = {\n    \"pdf.fonttype\": 42,\n    \"ps.fonttype\": 42,\n    \"font.size\": 12,\n    \"legend.fontsize\": 10,\n}\nplt.rcParams.update(FIGURE_PARAMS)\n\n\ndef load_results(results_dir: Path) -> List[Dict[str, Any]]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef plot_best_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=\"best_val_accuracy\", data=df)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.ylim(0, 1)\n    # Annotate bars\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"best_val_accuracy\"] + 0.01, f\"{row['best_val_accuracy']*100:.1f}%\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(out_path / \"accuracy.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef plot_loss_curves(all_results: List[Dict[str, Any]], out_path: Path):\n    for res in all_results:\n        run_id = res[\"run_id\"]\n        epochs = [m[\"epoch\"] for m in res[\"epoch_metrics\"]]\n        train_losses = [m[\"train_loss\"] for m in res[\"epoch_metrics\"]]\n        val_losses = [m[\"val_loss\"] for m in res[\"epoch_metrics\"]]\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, train_losses, label=\"Train loss\")\n        plt.plot(epochs, val_losses, label=\"Val loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"Loss Curve - {run_id}\")\n        # Annotate final values\n        plt.text(epochs[-1], val_losses[-1], f\"{val_losses[-1]:.3f}\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(out_path / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n        plt.close()\n\n\ndef main(results_dir: str):\n    results_path = Path(results_dir)\n    all_results = load_results(results_path)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json found in {results_dir}\")\n\n    # Convert to pandas DataFrame for easy handling\n    df = pd.DataFrame([\n        {\n            \"run_id\": r[\"run_id\"],\n            \"best_val_accuracy\": r[\"best_val_accuracy\"],\n            \"time_to_threshold\": r.get(\"time_to_threshold\"),\n            \"final_test_accuracy\": r.get(\"final_test_accuracy\"),\n        }\n        for r in all_results\n    ])\n\n    # Print numerical comparison to stdout\n    comparison = df.to_dict(orient=\"records\")\n    print(json.dumps({\"comparison\": comparison}, indent=2))\n\n    # Create figure output directory\n    figs_dir = results_path / \"figures\"\n    figs_dir.mkdir(exist_ok=True)\n\n    # Plot best accuracy bar chart\n    plot_best_accuracy(df, figs_dir)\n\n    # Plot loss curves per run\n    plot_loss_curves(all_results, figs_dir)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory containing all run subdirs.\")\n    args = parser.parse_args()\n    main(args.results_dir)\n",
    "preprocess_py": "\"\"\"Common data loading & preprocessing with dataset placeholders.\"\"\"\n\nfrom typing import Tuple, Any, Dict\nfrom pathlib import Path\n\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, random_split\n\n\n# PLACEHOLDER: Additional dataset-specific imports can be added here.\n\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any], *, batch_size: int, num_workers: int, smoke_test: bool)\n    -> Tuple[DataLoader, DataLoader, DataLoader, int, Any]:\n    \"\"\"Return train/val/test DataLoaders, num_classes and input_shape.\n\n    This function implements full data pipeline. Dataset-specific logic is handled\n    by branching on dataset_cfg[\"type\"]. Add new branches when introducing new\n    datasets in derived experiments.\n\n    Returns:\n        train_loader, val_loader, test_loader, num_classes, input_shape\n    \"\"\"\n    dataset_type = dataset_cfg[\"type\"]\n    data_root = Path(dataset_cfg.get(\"root\", \"./data\"))\n\n    if dataset_type == \"FakeData\":  # Generic vision dummy data for smoke tests\n        num_classes = dataset_cfg.get(\"num_classes\", 10)\n        image_size = dataset_cfg.get(\"image_size\", (3, 32, 32))\n        transform = transforms.ToTensor()\n        full_dataset = datasets.FakeData(\n            size=dataset_cfg.get(\"size\", 2000),\n            image_size=image_size,\n            num_classes=num_classes,\n            transform=transform,\n        )\n        val_size = int(0.2 * len(full_dataset))\n        test_size = int(0.1 * len(full_dataset))\n        train_size = len(full_dataset) - val_size - test_size\n        train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n        input_shape = image_size\n\n    elif dataset_type == \"DATASET_PLACEHOLDER\":\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic in derived experiments\n        raise NotImplementedError(\"Dataset type DATASET_PLACEHOLDER must be implemented in derived experiment.\")\n\n    else:\n        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n\n    if smoke_test:\n        # Use small subset for speed\n        train_set = torch.utils.data.Subset(train_set, range(min(256, len(train_set))))\n        val_set = torch.utils.data.Subset(val_set, range(min(256, len(val_set))))\n        test_set = torch.utils.data.Subset(test_set, range(min(256, len(test_set))))\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, test_loader, num_classes, input_shape\n",
    "model_py": "\"\"\"Model architectures and registry.\"\"\"\n\nfrom typing import Dict, Any\nimport torch.nn as nn\nimport torch\n\n\nclass MLP(nn.Module):\n    \"\"\"Simple Multi-Layer Perceptron for tabular or flattened image data.\"\"\"\n\n    def __init__(self, input_dim: int, num_classes: int, hidden_dims=(256, 128)):\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for h in hidden_dims:\n            layers += [nn.Linear(prev_dim, h), nn.ReLU(inplace=True)]\n            prev_dim = h\n        layers.append(nn.Linear(prev_dim, num_classes))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if x.dim() > 2:\n            x = torch.flatten(x, 1)\n        return self.net(x)\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Minimal CNN suitable for 32×32 images – serves as baseline.\n    This keeps dependencies minimal yet provides a working example.\"\"\"\n\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# Registry mapping model_type string to constructor\n_MODEL_REGISTRY = {\n    \"MLP\": MLP,\n    \"SimpleCNN\": SimpleCNN,\n    \"MODEL_PLACEHOLDER\": None,  # PLACEHOLDER: replaced in derivative experiments\n}\n\n\ndef get_model(model_cfg: Dict[str, Any], *, num_classes: int, input_shape):\n    \"\"\"Return instantiated nn.Module based on config.\"\"\"\n    model_type = model_cfg[\"type\"]\n    if model_type == \"MLP\":\n        if isinstance(input_shape, (tuple, list)):\n            flat_dim = int(torch.prod(torch.tensor(input_shape)))\n        else:\n            flat_dim = input_shape\n        return MLP(flat_dim, num_classes)\n    elif model_type == \"SimpleCNN\":\n        return SimpleCNN(num_classes)\n    elif model_type == \"MODEL_PLACEHOLDER\":\n        # PLACEHOLDER: Replace with specific model logic in derived experiments\n        raise NotImplementedError(\"MODEL_PLACEHOLDER must be replaced in the derived experiment step.\")\n    else:\n        raise ValueError(f\"Unknown model type {model_type}\")\n",
    "main_py": "import argparse\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport yaml\nimport json\nimport datetime\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run subprocess while teeing its stdout / stderr to files and console.\"\"\"\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1)\n        assert process.stdout is not None and process.stderr is not None\n        # Stream stdout\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if not stdout_line and not stderr_line and process.poll() is not None:\n                break\n            if stdout_line:\n                sys.stdout.buffer.write(stdout_line)\n                out_f.write(stdout_line)\n                out_f.flush()\n            if stderr_line:\n                sys.stderr.buffer.write(stderr_line)\n                err_f.write(stderr_line)\n                err_f.flush()\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n\ndef launch_train(run_cfg: Dict[str, Any], results_root: Path, smoke_test: bool):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save run-specific config so that train.py can read it.\n    cfg_path = run_dir / \"config.yaml\"\n    with cfg_path.open(\"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # Build subprocess command\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--config\",\n        str(cfg_path),\n        \"--results-dir\",\n        str(run_dir),\n    ]\n    if smoke_test:\n        cmd.append(\"--smoke-test\")\n\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    tee_subprocess(cmd, stdout_path, stderr_path)\n\n\ndef run_all(cfg_file: str, results_dir: str, smoke_test: bool):\n    with open(cfg_file) as f:\n        exp_cfg = yaml.safe_load(f)\n\n    if \"experiments\" not in exp_cfg:\n        raise KeyError(\"Config YAML must contain 'experiments' list.\")\n\n    results_root = Path(results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    description = exp_cfg.get(\"description\", \"No description provided.\")\n    print(\n        json.dumps(\n            {\n                \"timestamp\": datetime.datetime.now().isoformat(),\n                \"experiment_description\": description,\n            },\n            indent=2,\n        )\n    )\n\n    for run_cfg in exp_cfg[\"experiments\"]:\n        print(json.dumps({\"status\": \"starting\", \"run_id\": run_cfg[\"run_id\"]}))\n        launch_train(run_cfg, results_root, smoke_test)\n        print(json.dumps({\"status\": \"finished\", \"run_id\": run_cfg[\"run_id\"]}))\n\n    # After all runs, launch evaluation\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    tee_subprocess(eval_cmd, results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for OHGW experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke test config.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment config.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where results will be stored.\")\n    args = parser.parse_args()\n\n    root_dir = Path(__file__).resolve().parent.parent  # project root\n    if args.smoke_test:\n        cfg_file = root_dir / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_file = root_dir / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_file), args.results_dir, args.smoke_test)\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\n\n[project]\nname = \"ohgw-experiments\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"torch>=2.0\",\n    \"torchvision>=0.15\",\n    \"tqdm\",\n    \"pyyaml\",\n    \"pandas\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"scikit-learn\",\n]\n",
    "smoke_test_yaml": "description: \"Smoke test for CI – uses FakeData and minimal epochs.\"\nexperiments:\n  - run_id: smoke_simplecnn\n    seed: 0\n    dataset:\n      type: FakeData  # PLACEHOLDER will be replaced with real dataset in full experiments\n      size: 500\n      num_classes: 10\n    model:\n      type: SimpleCNN  # baseline model for dummy data\n    training:\n      batch_size: 32\n      epochs: 2\n      optimizer:\n        name: SGD\n        lr: 0.01\n        momentum: 0.9\n    evaluation:\n      threshold: 0.2  # arbitrary low threshold to satisfy metric\n",
    "full_experiment_yaml": "description: \"FULL EXPERIMENT CONFIG – TO BE POPULATED WITH REAL DATASETS AND MODELS IN DERIVED STEP\"\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER_1  # PLACEHOLDER: replace with actual identifier\n    seed: 0\n    dataset: DATASET_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    training: SPECIFIC_CONFIG_PLACEHOLDER\n    evaluation:\n      threshold: SPECIFIC_CONFIG_PLACEHOLDER\n  # Additional experiments can be added here\n"
}
