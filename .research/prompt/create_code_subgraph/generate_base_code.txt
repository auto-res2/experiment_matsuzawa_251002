
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: Core logic is mostly present, but the foundation violates the “Complete Core Logic” criterion: train.py never saves model checkpoints (torch.save) and there is no corresponding load logic, so model saving/loading mechanisms are incomplete. Without this, later resume / evaluation work cannot function. All other criteria are largely satisfied, but the missing checkpoint functionality prevents the base code from being considered ready.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "\"\"\"src/train.py\n------------------\nTrain a single experimental variation.\nThis script is **fully functional** and contains no placeholders in the *core* logic.  \nAll dataset- / model-specific parts are handled by helper modules that already expose sensible default\nbehaviour (random data + tiny CNN) so that smoke-tests run out-of-the-box.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom tqdm import tqdm\n\n# Local imports (namespace package – src has no __init__, PEP-420 compliant)\nfrom . import preprocess as pp  # type: ignore\nfrom . import model as mdl  # type: ignore\n\n\n# ---------------------------- utility functions --------------------------- #\n\ndef set_seed(seed: int):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n    preds = logits.argmax(dim=1)\n    return (preds == targets).float().mean().item() * 100.0\n\n\n# ---------------------------- core training ------------------------------ #\n\ndef train_one_epoch(model: nn.Module, loader, device, optimizer):\n    model.train()\n    total_loss = 0.0\n    total_acc = 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(x)\n        loss = F.cross_entropy(logits, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n        total_acc += accuracy(logits.detach(), y) * x.size(0)\n    n = len(loader.dataset)\n    return total_loss / n, total_acc / n\n\n\ndef validate(model: nn.Module, loader, device):\n    model.eval()\n    total_loss = 0.0\n    total_acc = 0.0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            total_loss += F.cross_entropy(logits, y).item() * x.size(0)\n            total_acc += accuracy(logits, y) * x.size(0)\n    n = len(loader.dataset)\n    return total_loss / n, total_acc / n\n\n\n# ----------------------------- main entry -------------------------------- #\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config-path\", type=str, required=True, help=\"Path to YAML or JSON config file for this run\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store all outputs for this run\")\n    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume from existing checkpoint if present\")\n    args = parser.parse_args()\n\n    import yaml  # imported late to keep global namespace small\n\n    # ------------------------------------------------------------------\n    # 1. load config & create result directory\n    # ------------------------------------------------------------------\n    cfg_path = Path(args.config_path)\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    run_id = cfg[\"run_id\"]\n    result_dir = Path(args.results_dir)\n    result_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save an immutable copy of the config for reproducibility\n    with open(result_dir / \"config_frozen.yaml\", \"w\") as f:\n        yaml.safe_dump(cfg, f)\n\n    # ------------------------------------------------------------------\n    # 2. experiment description (printed BEFORE any numeric results)  \n    # ------------------------------------------------------------------\n    description = {\n        \"run_id\": run_id,\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"dataset\": cfg[\"dataset\"],\n        \"model\": cfg[\"model\"],\n        \"training\": cfg[\"training\"],\n        \"notes\": cfg.get(\"notes\", \"\")\n    }\n    print(\"# EXPERIMENT DESCRIPTION\")\n    print(json.dumps(description, indent=2))\n    sys.stdout.flush()\n\n    # ------------------------------------------------------------------\n    # 3. set-up hardware / seeds\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------\n    # 4. data & model (placeholders handled inside helper modules)\n    # ------------------------------------------------------------------\n    train_loader, val_loader = pp.get_dataloaders(cfg)\n    model = mdl.get_model(cfg[\"model\"][\"name\"], cfg).to(device)\n\n    # Optimiser (SGD with momentum by default – can be overridden in cfg)\n    optim_cfg = cfg[\"training\"]\n    lr = optim_cfg.get(\"lr\", 0.01)\n    momentum = optim_cfg.get(\"momentum\", 0.9)\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=max(optim_cfg.get(\"epochs\", 1) // 3, 1), gamma=0.1)\n\n    # ------------------------------------------------------------------\n    # 5. option to resume from checkpoint\n    # ------------------------------------------------------------------\n    ckpt_path = result_dir / \"checkpoint.pt\"\n    start_epoch = 0\n    best_val_acc = 0.0\n    if args.resume and ckpt_path.exists():\n        ckpt = torch.load(ckpt_path, map_location=device)\n        model.load_state_dict(ckpt[\"model_state_dict\"])\n        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n        start_epoch = ckpt[\"epoch\"] + 1\n        best_val_acc = ckpt.get(\"best_val_acc\", 0.0)\n        print(f\"[INFO] Resumed from epoch {start_epoch}\")\n\n    # ------------------------------------------------------------------\n    # 6. training loop\n    # ------------------------------------------------------------------\n    epochs = optim_cfg.get(\"epochs\", 1)\n    hist = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\n    for epoch in range(start_epoch, epochs):\n        tr_loss, tr_acc = train_one_epoch(model, train_loader, device, optimizer)\n        val_loss, val_acc = validate(model, val_loader, device)\n        scheduler.step()\n\n        hist[\"train_loss\"].append(tr_loss)\n        hist[\"train_acc\"].append(tr_acc)\n        hist[\"val_loss\"].append(val_loss)\n        hist[\"val_acc\"].append(val_acc)\n\n        # Save checkpoint EVERY epoch for maximum reproducibility\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"scheduler_state_dict\": scheduler.state_dict(),\n            \"best_val_acc\": best_val_acc,\n            \"hist\": hist,\n            \"cfg\": cfg,\n        }, ckpt_path)\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n\n        # Progress print (JSON so parent can parse if desired)\n        epoch_log = {\n            \"run_id\": run_id,\n            \"epoch\": epoch,\n            \"train_loss\": tr_loss,\n            \"train_acc\": tr_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }\n        print(json.dumps(epoch_log))\n        sys.stdout.flush()\n\n    # ------------------------------------------------------------------\n    # 7. save final metrics & figures\n    # ------------------------------------------------------------------\n    final_metrics = {\n        \"run_id\": run_id,\n        \"final_val_accuracy\": val_acc,\n        \"best_val_accuracy\": best_val_acc,\n        \"final_val_loss\": val_loss,\n        \"train_epochs\": epochs,\n        \"seed\": seed,\n    }\n    with open(result_dir / \"results.json\", \"w\") as f:\n        json.dump({\"metrics\": final_metrics, \"history\": hist, \"cfg\": cfg}, f, indent=2)\n\n    # -------------------- figure: training loss curve ------------------- #\n    plt.figure(figsize=(6, 4))\n    epochs_axis = list(range(1, epochs + 1))\n    plt.plot(epochs_axis, hist[\"train_loss\"], label=\"Train Loss\")\n    plt.plot(epochs_axis, hist[\"val_loss\"], label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # annotate final value\n    plt.annotate(f\"{hist['val_loss'][-1]:.3f}\", (epochs, hist[\"val_loss\"][-1]))\n    plt.legend()\n    fname_loss = result_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(fname_loss, bbox_inches=\"tight\")\n    plt.close()\n\n    # -------------------- figure: accuracy curve ------------------------ #\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs_axis, hist[\"train_acc\"], label=\"Train Acc\")\n    plt.plot(epochs_axis, hist[\"val_acc\"], label=\"Val Acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(f\"Accuracy – {run_id}\")\n    plt.annotate(f\"{hist['val_acc'][-1]:.2f}%\", (epochs, hist[\"val_acc\"][-1]))\n    plt.legend()\n    fname_acc = result_dir / f\"accuracy_{run_id}.pdf\"\n    plt.savefig(fname_acc, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------\n    # 8. final JSON output (MANDATORY) – must be last thing printed\n    # ------------------------------------------------------------------\n    print(\"# FINAL METRICS\")\n    print(json.dumps(final_metrics))\n    sys.stdout.flush()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\n------------------\nPost-hoc comparison & visualisation across completed runs.\nThe script expects that every sub-directory in `results_dir` contains a `results.json` file\nwith keys  {\"metrics\": ..., \"history\": ..., \"cfg\": ...} as written by `train.py`.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nFIG_DPI = 100  # low DPI is fine – PDFs are vector anyway\n\n\ndef load_all_results(results_dir: Path):\n    rows = []\n    for sub in results_dir.iterdir():\n        res_file = sub / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as f:\n                blob = json.load(f)\n            row = {\n                \"run_id\": blob[\"metrics\"][\"run_id\"],\n                \"final_val_acc\": blob[\"metrics\"][\"final_val_accuracy\"],\n                \"best_val_acc\": blob[\"metrics\"][\"best_val_accuracy\"],\n                \"final_val_loss\": blob[\"metrics\"][\"final_val_loss\"],\n            }\n            rows.append(row)\n    return pd.DataFrame(rows)\n\n\ndef create_accuracy_barplot(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"final_val_acc\", palette=\"deep\")\n    plt.ylabel(\"Final Validation Accuracy (%)\")\n    plt.xlabel(\"\")\n    plt.title(\"Final Validation Accuracy Across Runs\")\n    # annotate values\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"final_val_acc\"] + 0.5, f\"{row['final_val_acc']:.2f}\", ha=\"center\")\n    fname = out_dir / \"accuracy.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return str(fname)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Path that contains sub-directories for each run\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    assert results_dir.exists(), f\"results_dir {results_dir} not found\"\n\n    print(\"# EVALUATION DESCRIPTION\")\n    print(\"Evaluating all runs in\", results_dir)\n\n    df = load_all_results(results_dir)\n    if len(df) == 0:\n        print(\"No result.json files found – nothing to evaluate.\")\n        return\n\n    # --- figure generation --- #\n    fig_path = create_accuracy_barplot(df, results_dir)\n\n    # --- aggregate statistics --- #\n    agg = {\n        \"mean_final_val_acc\": df[\"final_val_acc\"].mean(),\n        \"std_final_val_acc\": df[\"final_val_acc\"].std(),\n        \"num_runs\": len(df),\n        \"figure_accuracy\": fig_path,\n    }\n\n    print(\"# COMPARISON RESULTS\")\n    print(json.dumps(agg))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\n--------------------\nCommon data preprocessing pipeline with explicit placeholders for dataset-specific logic.\nThe default branch implements a *dummy* random dataset so that the framework is runnable\nwithout external data (used in smoke-tests).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torchvision.transforms as T\n\n# -------------------------------------------------------------------------\n# Random placeholder dataset – replaced by real dataset loaders later\n# -------------------------------------------------------------------------\n\n\nclass RandomImageDataset(Dataset):\n    \"\"\"A small random dataset that mimics 32×32 RGB images.\"\"\"\n\n    def __init__(self, num_samples: int = 1000, num_classes: int = 10, image_shape=(3, 32, 32)):\n        self.num_samples = num_samples\n        self.num_classes = num_classes\n        self.image_shape = image_shape\n        self.data = torch.rand(num_samples, *image_shape)\n        self.targets = torch.randint(0, num_classes, (num_samples,))\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n    def __len__(self):\n        return self.num_samples\n\n\n# -------------------------------------------------------------------------\n# Public API\n# -------------------------------------------------------------------------\n\ndef get_dataloaders(cfg: dict):\n    \"\"\"Return (train_loader, val_loader) according to *cfg*.\n\n    cfg structure (subset):\n      dataset:\n        name: DATASET_PLACEHOLDER | actual_dataset_name\n        batch_size: 32\n        num_workers: 4\n        val_split: 0.1  # fraction or int count\n    \"\"\"\n\n    ds_cfg = cfg[\"dataset\"]\n    name = ds_cfg[\"name\"]\n\n    batch_size = ds_cfg.get(\"batch_size\", 32)\n    num_workers = ds_cfg.get(\"num_workers\", 4)\n    val_split = ds_cfg.get(\"val_split\", 0.1)\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER handling – to be replaced in derived experiment configs\n    # ------------------------------------------------------------------\n    if name == \"DATASET_PLACEHOLDER\":\n        num_samples = ds_cfg.get(\"dummy_num_samples\", 500)\n        num_classes = ds_cfg.get(\"num_classes\", 10)\n        full_ds = RandomImageDataset(num_samples=num_samples, num_classes=num_classes)\n    else:\n        raise NotImplementedError(\n            \"Dataset '%s' not implemented in common core foundation – add implementation in derive_specific step\" % name\n        )\n\n    # ------------------------------------------------------------------\n    # validation split\n    # ------------------------------------------------------------------\n    if isinstance(val_split, float):\n        val_len = int(len(full_ds) * val_split)\n    else:\n        val_len = int(val_split)\n    train_len = len(full_ds) - val_len\n    train_ds, val_ds = random_split(full_ds, [train_len, val_len])\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_loader, val_loader\n",
    "model_py": "\"\"\"src/model.py\n----------------\nProvides model architectures.  The default *placeholder* model is a small CNN that\nworks with the random dataset from preprocess.py.  Real models will be registered\nhere in the derive_specific step.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nimport torch\nfrom torch import nn\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"A tiny CNN suitable for 32×32 images with *num_classes* outputs.\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 16×16\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 8×8\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# -------------------------------------------------------------------------\n# Public factory\n# -------------------------------------------------------------------------\n\ndef get_model(name: str, cfg: dict):\n    \"\"\"Return an *nn.Module* for *name*.\n\n    Core models that are part of the method MUST be registered here later.  For now we\n    provide a placeholder implementation that is sufficient for smoke-tests.\n    \"\"\"\n\n    if name == \"MODEL_PLACEHOLDER\":\n        num_classes = cfg[\"dataset\"].get(\"num_classes\", 10)\n        return SimpleCNN(num_classes=num_classes)\n\n    raise NotImplementedError(\n        f\"Model '{name}' is not implemented in common core foundation – add implementation in derive_specific step\"\n    )\n",
    "main_py": "\"\"\"src/main.py\n----------------\nMaster orchestration script – sequentially executes all experimental variations\nspecified in a YAML configuration file and finally triggers evaluation.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom time import perf_counter\n\nimport yaml\n\n# ----------------------------------------------------------------------------\n# helper – tee stdout/stderr to both console and file\n# ----------------------------------------------------------------------------\n\n\ndef _stream_subprocess(proc, log_file):\n    \"\"\"Forward *proc* stdout/stderr to console *and* write to *log_file*.\"\"\"\n    with open(log_file, \"w\") as f:\n        for line in proc.stdout:  # type: ignore\n            txt = line.decode()\n            sys.stdout.write(txt)\n            f.write(txt)\n            sys.stdout.flush()\n        proc.wait()\n\n\n# ----------------------------------------------------------------------------\n# main orchestrator\n# ----------------------------------------------------------------------------\n\n\ndef run_experiment(entry: dict, base_results_dir: Path):\n    run_id = entry[\"run_id\"]\n    run_dir = base_results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write per-run config so child process can load its own YAML\n    run_cfg_path = run_dir / \"run_config.yaml\"\n    with open(run_cfg_path, \"w\") as f:\n        yaml.safe_dump(entry, f)\n\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--config-path\",\n        str(run_cfg_path),\n        \"--results-dir\",\n        str(run_dir),\n    ]\n\n    proc = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        bufsize=1,\n    )\n    _stream_subprocess(proc, run_dir / \"stdout.log\")\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Sub-process for {run_id} failed with exit code {proc.returncode}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml instead of full_experiment.yaml\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Explicitly run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where all runs are stored\")\n    args = parser.parse_args()\n\n    # determine config path\n    if args.smoke_test and args.full_experiment:\n        parser.error(\"--smoke-test and --full-experiment are mutually exclusive\")\n    if args.smoke_test:\n        cfg_file = Path(\"config/smoke_test.yaml\")\n    else:\n        cfg_file = Path(\"config/full_experiment.yaml\")\n    if not cfg_file.exists():\n        raise FileNotFoundError(cfg_file)\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg[\"experiments\"]\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    global_start = perf_counter()\n    for exp in experiments:\n        print(f\"\\n========== RUN {exp['run_id']} ==========\")\n        run_experiment(exp, results_root)\n    total_time = perf_counter() - global_start\n\n    # Trigger evaluation step\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_root),\n    ]\n    subprocess.run(eval_cmd, check=True)\n\n    print(json.dumps({\"all_runs_completed\": True, \"elapsed_sec\": total_time}))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"ohgw_framework\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for OHGW experiments\"\nrequires-python = \">=3.9\"\ndependencies = [\n  \"torch>=2.0.0\",\n  \"torchvision>=0.15.0\",\n  \"tqdm>=4.60.0\",\n  \"pyyaml>=6.0\",\n  \"pandas>=1.4.0\",\n  \"matplotlib>=3.6.0\",\n  \"seaborn>=0.12.0\",\n  \"numpy>=1.23.0\",\n]\n\n[tool.pdm]\ntype = \"library\"\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# Minimal configuration that runs quickly with placeholder data & model.\n\nexperiments:\n  - run_id: \"smoke_placeholder\"\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"  # PLACEHOLDER: replace with real dataset name later\n      batch_size: 8\n      val_split: 0.2\n      dummy_num_samples: 128\n    model:\n      name: \"MODEL_PLACEHOLDER\"    # PLACEHOLDER: replace with real model later\n    training:\n      epochs: 1\n      lr: 0.01\n      momentum: 0.9\n    notes: \"Smoke-test with dummy data to ensure end-to-end pipeline works.\"\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Template for *all* real experimental variations.  Fill in placeholders in the\n# derive_specific step. Each dictionary under `experiments` must specify at least\n#   run_id, dataset, model, training\n# The common core will execute them sequentially.\n\nexperiments:\n  # Example (commented out):\n  # - run_id: \"cifar10_resnet20_baseline\"\n  #   dataset:\n  #     name: \"cifar10\"\n  #     batch_size: 128\n  #     val_split: 0.1\n  #   model:\n  #     name: \"resnet20\"\n  #   training:\n  #     epochs: 200\n  #     lr: 0.1\n  #     momentum: 0.9\n  #   notes: \"Vanilla scheduler baseline\"\n\n  # PLACEHOLDER: Add real experiments here\n\n  - run_id: \"example_placeholder_run\"\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"  # replace later\n      batch_size: 32\n      val_split: 0.1\n    model:\n      name: \"MODEL_PLACEHOLDER\"     # replace later\n    training:\n      epochs: 3\n      lr: 0.01\n      momentum: 0.9\n    notes: \"Template entry – replace with actual experiment definition.\"\n\n# END of full_experiment.yaml\n"
}
