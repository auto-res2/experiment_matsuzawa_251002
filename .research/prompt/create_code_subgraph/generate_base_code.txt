
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments with one GPU per run variation. If GPUs are insufficient, experiments run sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations with 1 GPU per variation)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment in parallel across available GPUs, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. GPU Allocation: main.py assigns 1 GPU per run variation. If available GPUs < total variations, queue remaining runs sequentially.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: Requirement 4 violated – src.main merges stderr into stdout (Popen(..., stderr=STDOUT)) and writes only one log file (stdout.log). Spec mandates two files, stdout.log & stderr.log, each capturing respective streams. All other core-logic, placeholder usage, 8-file structure, training/eval, model saving, visualization etc. are correct, but missing separate stderr redirection prevents full compliance.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport random\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nyaml_available = True\ntry:\n    import yaml  # type: ignore\nexcept ImportError:\n    yaml_available = False\n\n# Use non-interactive backend for PDF saving\nmatplotlib.use(\"Agg\")\n\n# --------------------------- Utility Functions --------------------------- #\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Set random seed for reproducibility across libraries.\"\"\"\n    if seed is None:\n        return\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n    \"\"\"Compute top-1 accuracy (percentage).\"\"\"\n    pred_labels = pred.argmax(dim=1)\n    correct = (pred_labels == target).sum().item()\n    return correct / target.size(0) * 100.0\n\n\n# --------------------------- Training Loop --------------------------- #\n\ndef train_epoch(model: nn.Module,\n                loader: torch.utils.data.DataLoader,\n                criterion: nn.Module,\n                optimizer: optim.Optimizer,\n                device: torch.device) -> float:\n    model.train()\n    running_loss: float = 0.0\n    for images, labels in loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n    return running_loss / len(loader.dataset)\n\n\ndef evaluate_epoch(model: nn.Module,\n                   loader: torch.utils.data.DataLoader,\n                   criterion: nn.Module,\n                   device: torch.device) -> Dict[str, float]:\n    model.eval()\n    running_loss: float = 0.0\n    running_acc: float = 0.0\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n            running_acc += accuracy(outputs, labels) * images.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return {\"loss\": epoch_loss, \"accuracy\": epoch_acc}\n\n\n# --------------------------- Main --------------------------- #\n\ndef run_single_experiment(cfg: Dict, results_dir: Path) -> None:\n    run_id: str = cfg[\"run_id\"]\n    description: str = cfg.get(\"description\", \"No description provided.\")\n\n    # Create directory structure\n    run_dir = results_dir / run_id\n    figures_dir = run_dir / \"figures\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    figures_dir.mkdir(parents=True, exist_ok=True)\n\n    # Persist config for traceability\n    with open(run_dir / \"config.yaml\", \"w\") as fh:\n        yaml.dump(cfg, fh) if yaml_available else fh.write(json.dumps(cfg, indent=2))\n\n    seed = cfg.get(\"training\", {}).get(\"seed\", 42)\n    set_seed(seed)\n\n    # Device handling: assume single GPU visible through CUDA_VISIBLE_DEVICES\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------- Dataset & DataLoader --------------------- #\n    from src.preprocess import get_dataloaders  # local import to avoid circular deps\n\n    dataset_cfg = cfg.get(\"dataset\", {})\n    loaders = get_dataloaders(dataset_cfg,\n                              batch_size=cfg[\"training\"].get(\"batch_size\", 128),\n                              num_workers=cfg[\"training\"].get(\"num_workers\", 4),\n                              smoke_test=cfg.get(\"smoke_test\", False))\n    train_loader, val_loader, test_loader = loaders\n\n    # --------------------------- Model --------------------------- #\n    from src.model import get_model  # local import\n\n    model_cfg = cfg.get(\"model\", {})\n    num_classes = dataset_cfg.get(\"num_classes\", 10)\n    model = get_model(model_cfg.get(\"name\", \"MODEL_PLACEHOLDER\"),\n                      num_classes=num_classes,\n                      model_cfg=model_cfg)\n    model = model.to(device)\n\n    # --------------------------- Optimizer --------------------------- #\n    optimizer_name = cfg[\"training\"].get(\"optimizer\", \"SGD\").lower()\n    lr = cfg[\"training\"].get(\"lr\", 0.1)\n    weight_decay = cfg[\"training\"].get(\"weight_decay\", 0.0)\n    if optimizer_name == \"sgd\":\n        momentum = cfg[\"training\"].get(\"momentum\", 0.9)\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif optimizer_name == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n\n    criterion = nn.CrossEntropyLoss()\n\n    epochs: int = int(cfg[\"training\"].get(\"epochs\", 1))\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    val_accuracies: List[float] = []\n\n    start_time = time.time()\n\n    # --------------------------- Training Loop --------------------------- #\n    for epoch in range(1, epochs + 1):\n        epoch_train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_metrics = evaluate_epoch(model, val_loader, criterion, device)\n\n        train_losses.append(epoch_train_loss)\n        val_losses.append(val_metrics[\"loss\"])\n        val_accuracies.append(val_metrics[\"accuracy\"])\n\n        # Structured epoch-level logging (JSONL)\n        print(json.dumps({\n            \"run_id\": run_id,\n            \"epoch\": epoch,\n            \"train_loss\": epoch_train_loss,\n            \"val_loss\": val_metrics[\"loss\"],\n            \"val_accuracy\": val_metrics[\"accuracy\"]\n        }), flush=True)\n\n    total_time = time.time() - start_time\n\n    # --------------------------- Test Evaluation --------------------------- #\n    test_metrics = evaluate_epoch(model, test_loader, criterion, device)\n\n    # Save final metrics\n    final_results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"epochs\": epochs,\n        \"train_loss\": train_losses,\n        \"val_loss\": val_losses,\n        \"val_accuracy\": val_accuracies,\n        \"test_loss\": test_metrics[\"loss\"],\n        \"test_accuracy\": test_metrics[\"accuracy\"],\n        \"total_training_time_sec\": total_time,\n    }\n    with open(run_dir / \"results.json\", \"w\") as fh:\n        json.dump(final_results, fh, indent=2)\n\n    # Save model checkpoint\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n\n    # --------------------------- Visualization --------------------------- #\n    # Training loss curve\n    fig1, ax1 = plt.subplots()\n    epochs_range = list(range(1, epochs + 1))\n    ax1.plot(epochs_range, train_losses, label=\"Train Loss\")\n    ax1.plot(epochs_range, val_losses, label=\"Val Loss\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.set_title(f\"Training & Validation Loss – {run_id}\")\n    ax1.legend()\n    # Annotate final values\n    ax1.annotate(f\"{val_losses[-1]:.4f}\", xy=(epochs, val_losses[-1]))\n    fig1.tight_layout()\n    loss_fig_path = figures_dir / f\"training_loss_{run_id}.pdf\"\n    fig1.savefig(loss_fig_path, bbox_inches=\"tight\")\n    plt.close(fig1)\n\n    # Accuracy curve\n    fig2, ax2 = plt.subplots()\n    ax2.plot(epochs_range, val_accuracies, label=\"Val Accuracy (%)\")\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"Accuracy (%)\")\n    ax2.set_title(f\"Validation Accuracy – {run_id}\")\n    ax2.legend()\n    ax2.annotate(f\"{val_accuracies[-1]:.2f}%\", xy=(epochs, val_accuracies[-1]))\n    fig2.tight_layout()\n    acc_fig_path = figures_dir / f\"validation_accuracy_{run_id}.pdf\"\n    fig2.savefig(acc_fig_path, bbox_inches=\"tight\")\n    plt.close(fig2)\n\n    # Print summary to stdout (JSON)\n    summary = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"final_val_accuracy\": val_accuracies[-1],\n        \"final_test_accuracy\": test_metrics[\"accuracy\"],\n        \"training_time_sec\": total_time,\n        \"loss_curve\": str(loss_fig_path.name),\n        \"accuracy_curve\": str(acc_fig_path.name)\n    }\n    print(json.dumps(summary), flush=True)\n\n\n# --------------------------- CLI --------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config-path\", type=str, required=True, help=\"Path to the YAML config for this run.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store outputs for this run.\")\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = parse_args()\n    config_path = Path(args.config_path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config path {config_path} does not exist.\")\n    cfg = yaml.safe_load(config_path.read_text()) if yaml_available else json.loads(config_path.read_text())\n    results_dir = Path(args.results_dir)\n    run_single_experiment(cfg, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Use non-interactive backend for PDF saving\nmatplotlib.use(\"Agg\")\n\n\n# --------------------------- Utils --------------------------- #\n\ndef collect_results(results_dir: Path) -> List[Dict]:\n    all_results: List[Dict] = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as fh:\n                data = json.load(fh)\n                all_results.append(data)\n    if not all_results:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n    return all_results\n\n\n# --------------------------- Visualization --------------------------- #\n\ndef plot_final_accuracy(df: pd.DataFrame, save_path: Path) -> None:\n    sns.set(style=\"whitegrid\")\n    fig, ax = plt.subplots(figsize=(6, 4))\n    barplot = sns.barplot(x=\"run_id\", y=\"final_test_accuracy\", data=df, ax=ax, palette=\"viridis\")\n    ax.set_ylabel(\"Final Test Accuracy (%)\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_title(\"Final Test Accuracy Across Runs\")\n    # Annotate bars\n    for p in barplot.patches:\n        value = p.get_height()\n        ax.annotate(f\"{value:.2f}\", (p.get_x() + p.get_width() / 2., value),\n                    ha='center', va='bottom', fontsize=9, color='black')\n    fig.tight_layout()\n    fig.savefig(save_path, bbox_inches=\"tight\")\n    plt.close(fig)\n\n\n# --------------------------- Main --------------------------- #\n\ndef main(results_dir: Path) -> None:\n    all_results = collect_results(results_dir)\n    df = pd.DataFrame(all_results)\n\n    # Compute comparison statistics (example: best accuracy)\n    best_row = df.loc[df[\"final_test_accuracy\"].idxmax()]\n    comparison_summary = {\n        \"best_run\": best_row[\"run_id\"],\n        \"best_accuracy\": best_row[\"final_test_accuracy\"],\n        \"average_accuracy\": df[\"final_test_accuracy\"].mean(),\n        \"std_accuracy\": df[\"final_test_accuracy\"].std(),\n        \"num_runs\": len(df)\n    }\n\n    # Save summary JSON\n    summary_path = results_dir / \"comparison_summary.json\"\n    with open(summary_path, \"w\") as fh:\n        json.dump(comparison_summary, fh, indent=2)\n\n    # Create figure\n    fig_path = results_dir / \"final_accuracy.pdf\"\n    plot_final_accuracy(df, fig_path)\n\n    # Print summary to stdout as JSON\n    print(json.dumps({\"comparison\": comparison_summary, \"figure\": fig_path.name}, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate multiple experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory containing all experiment sub-directories.\")\n    args = parser.parse_args()\n    main(Path(args.results_dir))",
    "preprocess_py": "\"\"\"Common data-loading & preprocessing utilities with dataset placeholders.\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import datasets, transforms\n\n# --------------------------- Dummy Dataset (for smoke tests) --------------------------- #\nclass DummyClassificationDataset(Dataset):\n    \"\"\"Randomly generated images & labels for fast smoke testing.\"\"\"\n\n    def __init__(self, split: str = \"train\", num_samples: int = 1024, num_classes: int = 10,\n                 img_size: Tuple[int, int, int] = (3, 32, 32), seed: int = 0):\n        super().__init__()\n        rng = np.random.RandomState(seed)\n        self.images = rng.rand(num_samples, *img_size).astype(np.float32)\n        self.labels = rng.randint(0, num_classes, size=(num_samples,)).astype(np.int64)\n        self.transform = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = torch.tensor(self.images[idx])\n        label = int(self.labels[idx])\n        return img, label\n\n\n# --------------------------- Dataset Factory --------------------------- #\n\ndef _torchvision_transforms(is_train: bool):\n    if is_train:\n        return transforms.Compose([\n            transforms.ToTensor(),\n        ])\n    else:\n        return transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n\ndef get_dataset(dataset_cfg: dict, split: str, smoke_test: bool = False):\n    \"\"\"Return a Dataset object given config. Handles placeholders & smoke-test fallback.\"\"\"\n    name = dataset_cfg.get(\"name\", \"DATASET_PLACEHOLDER\")\n    if smoke_test or name == \"DummyClassification\":\n        return DummyClassificationDataset(split=split,\n                                          num_samples=dataset_cfg.get(\"num_samples\", 512),\n                                          num_classes=dataset_cfg.get(\"num_classes\", 10))\n    # ---------------- Concrete implementations ---------------- #\n    if name.lower() == \"cifar10\":\n        root = Path(dataset_cfg.get(\"root\", \"./data\"))\n        is_train = split == \"train\"\n        return datasets.CIFAR10(root=str(root), train=is_train, download=True,\n                               transform=_torchvision_transforms(is_train))\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented in common foundation.\")\n\n\n# --------------------------- DataLoader builder --------------------------- #\n\ndef get_dataloaders(dataset_cfg: dict, batch_size: int, num_workers: int, smoke_test: bool = False):\n    \"\"\"Return (train_loader, val_loader, test_loader) given dataset config.\"\"\"\n    full_train_dataset = get_dataset(dataset_cfg, split=\"train\", smoke_test=smoke_test)\n\n    # Validation split\n    val_split = 0.1 if not smoke_test else 0.2\n    val_size = int(len(full_train_dataset) * val_split)\n    train_size = len(full_train_dataset) - val_size\n    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size],\n                                              generator=torch.Generator().manual_seed(42))\n\n    test_dataset = get_dataset(dataset_cfg, split=\"test\", smoke_test=smoke_test)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                             num_workers=num_workers, pin_memory=True)\n\n    return train_loader, val_loader, test_loader",
    "model_py": "\"\"\"Model architectures used across experiments with placeholders for extensibility.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, Dict\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as tv_models\n\n\nclass DummyNet(nn.Module):\n    \"\"\"A minimal CNN for dummy classification tasks.\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# --------------------------- Model Factory --------------------------- #\n\ndef get_model(name: str, num_classes: int, model_cfg: Dict[str, Any] | None = None) -> nn.Module:\n    \"\"\"Return a nn.Module given its name and configuration.\"\"\"\n    name = name.lower()\n    if name in {\"dummynet\", \"dummy\", \"baseline\"}:\n        return DummyNet(num_classes=num_classes)\n    if name == \"resnet18\":\n        model = tv_models.resnet18(weights=None)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        return model\n    # PLACEHOLDER: Will be replaced with specific model architectures\n    raise NotImplementedError(f\"Model '{name}' is not implemented in common foundation.\")",
    "main_py": "import argparse\nimport json\nimport os\nimport queue\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nimport yaml\n\n# --------------------------- GPU Management --------------------------- #\n\ndef discover_gpus() -> List[int]:\n    \"\"\"Return list of visible GPU indices.\"\"\"\n    try:\n        return list(range(torch.cuda.device_count()))\n    except AssertionError:\n        return []\n\n\n# --------------------------- Sub-process Streaming Helpers --------------------------- #\n\ndef _stream_output(pipe, log_file_handle, stream):\n    \"\"\"Read bytes from pipe and write to both log file and given stream.\"\"\"\n    for line in iter(pipe.readline, b\"\"):\n        decoded = line.decode(\"utf-8\", errors=\"replace\")\n        log_file_handle.write(decoded)\n        log_file_handle.flush()\n        stream.write(decoded)\n        stream.flush()\n    pipe.close()\n\n\n# --------------------------- Experiment Runner --------------------------- #\n\ndef launch_experiment(exp_cfg: Dict, run_dir: Path, gpu_id: int, results_root: Path) -> subprocess.Popen:\n    \"\"\"Launch src.train as a subprocess assigned to a specific GPU.\"\"\"\n    config_path = run_dir / \"config.yaml\"\n    with open(config_path, \"w\") as fh:\n        yaml.safe_dump(exp_cfg, fh)\n\n    cmd = [sys.executable, \"-m\", \"src.train\", \"--config-path\", str(config_path), \"--results-dir\", str(results_root)]\n\n    env = os.environ.copy()\n    env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n\n    stdout_log_path = run_dir / \"stdout.log\"\n    stderr_log_path = run_dir / \"stderr.log\"\n    stdout_fh = open(stdout_log_path, \"w\")\n    stderr_fh = open(stderr_log_path, \"w\")\n\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, bufsize=1)\n\n    # Start streaming threads\n    threading.Thread(target=_stream_output, args=(process.stdout, stdout_fh, sys.stdout), daemon=True).start()\n    threading.Thread(target=_stream_output, args=(process.stderr, stderr_fh, sys.stderr), daemon=True).start()\n\n    # Attach log file handles for later closing\n    process._stdout_fh = stdout_fh  # type: ignore\n    process._stderr_fh = stderr_fh  # type: ignore\n    return process\n\n\n# --------------------------- Main Orchestrator --------------------------- #\n\ndef run_all_experiments(config_file: Path, results_dir: Path):\n    with open(config_file, \"r\") as fh:\n        experiments_config = yaml.safe_load(fh)\n\n    experiments: List[Dict] = experiments_config.get(\"experiments\", [])\n    if not experiments:\n        raise ValueError(\"No experiments defined in config file.\")\n\n    gpu_pool: queue.Queue[int] = queue.Queue()\n    for g in discover_gpus():\n        gpu_pool.put(g)\n    if gpu_pool.empty():\n        print(\"[WARN] No GPUs detected, experiments will run on CPU sequentially.\")\n\n    running_processes: List[subprocess.Popen] = []\n\n    try:\n        exp_iter = iter(experiments)\n        while True:\n            # Launch new experiments if GPUs free\n            while not gpu_pool.empty():\n                try:\n                    exp_cfg = next(exp_iter)\n                except StopIteration:\n                    break  # No more experiments to launch\n\n                run_id = exp_cfg[\"run_id\"]\n                run_dir = results_dir / run_id\n                run_dir.mkdir(parents=True, exist_ok=True)\n                gpu_id = gpu_pool.get()\n                proc = launch_experiment(exp_cfg, run_dir, gpu_id, results_dir)\n                proc._gpu_id = gpu_id  # type: ignore\n                running_processes.append(proc)\n                print(f\"[INFO] Launched {run_id} on GPU {gpu_id} (PID={proc.pid})\")\n\n            # Check running processes for completion\n            still_running: List[subprocess.Popen] = []\n            for p in running_processes:\n                if p.poll() is None:\n                    still_running.append(p)\n                else:\n                    # Release GPU and close logs\n                    gpu_pool.put(p._gpu_id)  # type: ignore\n                    p._stdout_fh.close()  # type: ignore\n                    p._stderr_fh.close()  # type: ignore\n                    print(f\"[INFO] Run with PID {p.pid} finished. GPU {p._gpu_id} freed.\")\n            running_processes = still_running\n\n            # Break when all experiments launched and completed\n            if not running_processes and all(gpu_pool.qsize() == torch.cuda.device_count() for _ in [0]):\n                try:\n                    _ = next(exp_iter)\n                except StopIteration:\n                    break\n\n            time.sleep(5)\n    finally:\n        # Ensure all child procs terminated on error\n        for p in running_processes:\n            p.terminate()\n        for p in running_processes:\n            p.wait()\n\n    # After all runs complete: evaluation\n    evaluate_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(evaluate_cmd, check=True)\n\n\n# --------------------------- CLI --------------------------- #\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for experiments.\")\n    mode = parser.add_mutually_exclusive_group(required=True)\n    mode.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml experiments.\")\n    mode.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml experiments.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store all results.\")\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    root = Path(__file__).resolve().parent.parent\n    config_path = root / \"config\" / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n    run_all_experiments(config_path, results_root)",
    "pyproject_toml": "[project]\nname = \"ohgw-foundation\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments.\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"",
    "smoke_test_yaml": "# Smoke-test configuration – uses minimal dummy dataset for quick execution\nexperiments:\n  - run_id: dummy_baseline\n    description: \"Smoke test baseline run on Dummy dataset.\"\n    dataset:\n      name: DummyClassification            # PLACEHOLDER: Replace with actual dataset in full study\n      num_samples: 512\n      num_classes: 10\n    model:\n      name: DummyNet                       # PLACEHOLDER: Replace with specific model\n    training:\n      epochs: 2\n      batch_size: 32\n      lr: 0.01\n      optimizer: Adam\n      seed: 42\n    evaluation:\n      metrics: [\"accuracy\"]\n    smoke_test: true\n\n  - run_id: dummy_variant\n    description: \"Second dummy run with altered LR to verify multiple runs.\"\n    dataset:\n      name: DummyClassification\n      num_samples: 512\n      num_classes: 10\n    model:\n      name: DummyNet\n    training:\n      epochs: 2\n      batch_size: 32\n      lr: 0.005\n      optimizer: Adam\n      seed: 43\n    evaluation:\n      metrics: [\"accuracy\"]\n    smoke_test: true",
    "full_experiment_yaml": "# Template for full experimental suite. Replace placeholders with actual datasets/models.\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER_1\n    description: \"# PLACEHOLDER: Detailed description of experiment variant 1\"\n    dataset:\n      name: DATASET_PLACEHOLDER            # PLACEHOLDER: Specific dataset loader name\n      root: /path/to/dataset_root          # PLACEHOLDER\n      num_classes: NUM_CLASSES_PLACEHOLDER # PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER              # PLACEHOLDER: e.g., ResNet18, Transformer-LM\n    training:\n      epochs: EPOCHS_PLACEHOLDER           # PLACEHOLDER\n      batch_size: BATCHSIZE_PLACEHOLDER    # PLACEHOLDER\n      lr: LR_PLACEHOLDER                   # PLACEHOLDER\n      optimizer: OPTIMIZER_PLACEHOLDER     # PLACEHOLDER\n      seed: SEED_PLACEHOLDER               # PLACEHOLDER\n    evaluation:\n      metrics: [\"accuracy\"]\n\n  # Add more experiment entries following the schema above as needed.\n"
}
