
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- To prevent labels, titles, and legends from overlapping, use `plt.tight_layout()` before saving the figure.
- All figures must be saved to `{results_dir}/images/` directory in .pdf format (e.g., using `plt.savefig(os.path.join(results_dir, "images", "filename.pdf"), bbox_inches="tight")`).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback




# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "\"\"\"\nsrc/train.py\n-------------\nRun exactly ONE experimental variation.  This script is launched as a subprocess\nby main.py and is therefore completely self-contained: it must read a single\nYAML file that describes *one* run, carry out training, save results / figures\ninto the run-specific directory that main.py created, and finally print a\nJSON-formatted summary (ALWAYS containing the key \"run_id\") to STDOUT so that\nmain.py can capture it.\n\nThe core algorithm – including the One-Shot Hyper-Gradient Warm-Start (OHGW) –\nis implemented without placeholders.  Only the *choice* of dataset/model can be\nplaceholder-controlled via the YAML file.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Relative imports – allowed because main.py runs with \"python -m src.train\"\nfrom .preprocess import Preprocessor\nfrom .model import ModelFactory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Make run fully deterministic where possible.\"\"\"\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef ohgw_warm_start(model: nn.Module,\n                    hyperparams: Dict[str, torch.Tensor],\n                    criterion: nn.Module,\n                    batch: Tuple[torch.Tensor, torch.Tensor],\n                    device: torch.device,\n                    eta_h: float) -> None:\n    \"\"\"Perform OHGW: one forward/backward to obtain hyper-gradients and update ψ.\n\n    Currently supports continuous hyper-parameters that *enter the loss\n    additively*, e.g. weight_decay:  L_total = L_ce + ψ * Σ‖w‖².\n    \"\"\"\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    # Enable gradient on every hyper-parameter tensor\n    for tensor in hyperparams.values():\n        tensor.requires_grad_(True)\n\n    model.train()\n    outputs = model(inputs)\n    loss_natural = criterion(outputs, targets)\n\n    # Regularisation term – this is where weight_decay hyper-param enters\n    reg_term = torch.zeros(1, device=device)\n    if \"weight_decay\" in hyperparams:\n        for p in model.parameters():\n            reg_term = reg_term + p.pow(2).sum()\n    total_loss = loss_natural + hyperparams.get(\"weight_decay\", torch.tensor(0., device=device)) * reg_term\n\n    # Second call to autograd to obtain ∂L / ∂ψ  (keep graph only once)\n    grads = torch.autograd.grad(total_loss, list(hyperparams.values()), retain_graph=False)\n\n    with torch.no_grad():\n        for (name, tensor), g in zip(hyperparams.items(), grads):\n            tensor -= eta_h * g\n            tensor.requires_grad_(False)  # Freeze afterwards – OHGW is *one* step\n            hyperparams[name] = tensor\n\n\ndef train_one_epoch(model: nn.Module,\n                    data_loader: DataLoader,\n                    optimizer: optim.Optimizer,\n                    criterion: nn.Module,\n                    device: torch.device,\n                    hyperparams: Dict[str, torch.Tensor]) -> Tuple[float, float]:\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for inputs, targets in data_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        ce_loss = criterion(outputs, targets)\n\n        reg_term = torch.zeros(1, device=device)\n        if \"weight_decay\" in hyperparams:\n            for p in model.parameters():\n                reg_term = reg_term + p.pow(2).sum()\n        loss = ce_loss + hyperparams.get(\"weight_decay\", torch.tensor(0., device=device)) * reg_term\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module,\n             data_loader: DataLoader,\n             criterion: nn.Module,\n             device: torch.device,\n             hyperparams: Dict[str, torch.Tensor]) -> Tuple[float, float]:\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            ce_loss = criterion(outputs, targets)\n\n            reg_term = torch.zeros(1, device=device)\n            if \"weight_decay\" in hyperparams:\n                for p in model.parameters():\n                    reg_term = reg_term + p.pow(2).sum()\n            loss = ce_loss + hyperparams.get(\"weight_decay\", torch.tensor(0., device=device)) * reg_term\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    loss = running_loss / total\n    acc = correct / total\n    return loss, acc\n\n\ndef plot_curves(history: List[Dict[str, Any]], results_dir: Path, run_id: str) -> None:\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    epochs = [h[\"epoch\"] for h in history]\n    train_loss = [h[\"train_loss\"] for h in history]\n    val_loss = [h[\"val_loss\"] for h in history]\n    val_acc = [h[\"val_acc\"] for h in history]\n\n    # Loss curves\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"train_loss\")\n    plt.plot(epochs, val_loss, label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training & Val Loss – {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{val_loss[-1]:.3f}\", xy=(epochs[-1], val_loss[-1]))\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Accuracy curve\n    plt.figure()\n    plt.plot(epochs, val_acc, label=\"val_accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(f\"Validation Accuracy – {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{val_acc[-1]*100:.2f}%\", xy=(epochs[-1], val_acc[-1]))\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"accuracy_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Run a single OHGW experiment variation\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the single-run YAML config file\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be stored\")\n    args = parser.parse_args()\n\n    import yaml  # local import to keep global namespace clean\n\n    cfg: Dict[str, Any]\n    with open(args.config, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    run_id: str = cfg[\"run_id\"]\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # 1.  Reproduce / environment setup\n    # ------------------------------------------------------------------\n    global_seed = cfg.get(\"seed\", 42)\n    set_seed(global_seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # 2.  Data\n    # ------------------------------------------------------------------\n    preprocess = Preprocessor(cfg[\"dataset\"])\n    train_loader, val_loader, input_shape, num_classes = preprocess.get_dataloaders()\n\n    # ------------------------------------------------------------------\n    # 3.  Model\n    # ------------------------------------------------------------------\n    model = ModelFactory.create(cfg[\"model\"].get(\"name\"),\n                                num_classes=num_classes,\n                                input_shape=input_shape,\n                                **cfg[\"model\"].get(\"params\", {}))\n    model.to(device)\n\n    # ------------------------------------------------------------------\n    # 4.  Training components\n    # ------------------------------------------------------------------\n    training_cfg = cfg[\"training\"]\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer_name = training_cfg.get(\"optimizer\", \"sgd\").lower()\n    lr = training_cfg.get(\"lr\", 0.1)\n\n    if optimizer_name == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=training_cfg.get(\"momentum\", 0.9))\n    elif optimizer_name == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n\n    # Hyper-parameters to be possibly OHGW-updated\n    hyperparams: Dict[str, torch.Tensor] = {}\n    for name, value in cfg.get(\"hyperparameters\", {}).items():\n        tensor = torch.tensor(float(value), device=device, requires_grad=False)\n        hyperparams[name] = tensor\n\n    # ------------------------------------------------------------------\n    # 5.  OHGW warm-start (optional)\n    # ------------------------------------------------------------------\n    if cfg.get(\"ohgw\", {}).get(\"enabled\", False):\n        warm_batch = next(iter(train_loader))\n        eta_h = float(cfg[\"ohgw\"].get(\"eta_h\", 1e-3))\n        ohgw_warm_start(model, hyperparams, criterion, warm_batch, device, eta_h)\n\n    # ------------------------------------------------------------------\n    # 6.  Training loop\n    # ------------------------------------------------------------------\n    epochs = int(training_cfg.get(\"epochs\", 50))\n    history: List[Dict[str, Any]] = []\n\n    # Timing for threshold metric – default val_acc ≥ 0.90\n    threshold_metric = cfg.get(\"threshold_metric\", \"val_acc\")\n    threshold_value = float(cfg.get(\"threshold_value\", 0.9))\n    time_to_threshold = None\n    start_time = time.time()\n\n    for epoch in range(1, epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, hyperparams)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device, hyperparams)\n\n        now = time.time()\n        history.append({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"timestamp\": now - start_time\n        })\n\n        # Time-to-threshold bookkeeping\n        if time_to_threshold is None and threshold_metric == \"val_acc\" and val_acc >= threshold_value:\n            time_to_threshold = now - start_time\n\n    total_time = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # 7.  Logging & persistence\n    # ------------------------------------------------------------------\n    # 7a.  Figures first\n    plot_curves(history, results_dir, run_id)\n\n    # 7b.  Structured results\n    final_metrics = {\n        \"run_id\": run_id,\n        \"final_val_acc\": history[-1][\"val_acc\"],\n        \"final_val_loss\": history[-1][\"val_loss\"],\n        \"total_time_sec\": total_time,\n        \"time_to_threshold_sec\": time_to_threshold,\n        \"threshold_metric\": threshold_metric,\n        \"threshold_value\": threshold_value,\n    }\n\n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump({\"config\": cfg, \"history\": history, \"summary\": final_metrics}, f, indent=2)\n\n    # ------------------------------------------------------------------\n    # 8.  STDOUT – experiment description then JSON\n    # ------------------------------------------------------------------\n    description = (\n        f\"Experiment {run_id}:\\n\"\n        f\"  Dataset   : {cfg['dataset']['name']}\\n\"\n        f\"  Model     : {cfg['model']['name']}\\n\"\n        f\"  OHGW      : {'ENABLED' if cfg.get('ohgw', {}).get('enabled', False) else 'disabled'}\\n\"\n        f\"  Epochs    : {epochs}\\n\"\n        f\"  Optimizer : {optimizer_name} (lr={lr})\\n\"\n    )\n    print(description, file=sys.stdout)\n    print(json.dumps(final_metrics), file=sys.stdout)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"\nsrc/evaluate.py\n-----------------\nAggregate *all* run-specific result files in a given results directory,\ncompute comparison metrics, and generate publication-quality figures under\n<results_dir>/images/.  The script prints a JSON-formatted high-level summary\nso that CI logs still carry numeric evidence even when figures are ignored.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\nFIG_NAMING_NOTE = \"Figures saved as .pdf in <results_dir>/images/ per naming rules.\"\n\n\ndef load_all_results(results_dir: Path) -> pd.DataFrame:\n    records: List[Dict[str, Any]] = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        results_file = run_dir / \"results.json\"\n        if results_file.exists():\n            with open(results_file, \"r\") as f:\n                obj = json.load(f)\n            summary = obj[\"summary\"]\n            records.append(summary)\n    return pd.DataFrame.from_records(records)\n\n\ndef plot_bar(df: pd.DataFrame, metric: str, ylabel: str, results_dir: Path) -> None:\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    plt.figure(figsize=(6, 4))\n    sns.barplot(data=df, x=\"run_id\", y=metric)\n    plt.ylabel(ylabel)\n    plt.xlabel(\"Run\")\n    for idx, row in df.iterrows():\n        plt.text(idx, row[metric], f\"{row[metric]:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{ylabel} across runs\")\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"{metric}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Aggregate and visualise OHGW experiments\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory that contains individual run sub-folders\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    df = load_all_results(results_dir)\n\n    if df.empty:\n        print(\"{} – no results found\".format(results_dir))\n        return\n\n    # ------------------------------------------------------------------\n    # 1.  Figures\n    # ------------------------------------------------------------------\n    plot_bar(df, metric=\"final_val_acc\", ylabel=\"Final Validation Accuracy\", results_dir=results_dir)\n    if \"time_to_threshold_sec\" in df.columns:\n        plot_bar(df, metric=\"time_to_threshold_sec\", ylabel=\"Time to threshold (s)\", results_dir=results_dir)\n\n    # ------------------------------------------------------------------\n    # 2.  Summary JSON\n    # ------------------------------------------------------------------\n    best_idx = df[\"final_val_acc\"].idxmax()\n    summary = {\n        \"best_run\": df.loc[best_idx, \"run_id\"],\n        \"best_final_val_acc\": df.loc[best_idx, \"final_val_acc\"],\n        \"mean_final_val_acc\": df[\"final_val_acc\"].mean(),\n        \"std_final_val_acc\": df[\"final_val_acc\"].std(),\n        \"figure_note\": FIG_NAMING_NOTE\n    }\n    print(\"Evaluation summary:\\n\" + json.dumps(summary, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"\nsrc/preprocess.py\n------------------\nCommon preprocessing pipeline with plug-in architecture.  Only the dataset-\nspecific loading logic is subject to placeholders; *everything else* – batch\nsplitting, DataLoader construction, deterministic splitting – is fully\nimplemented.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as T\nimport torchvision.datasets as dsets\n\n# Synthetic dataset implementation is fully functional and used by smoke tests\nclass SyntheticClassificationDataset(Dataset):\n    def __init__(self, num_samples: int = 1024, num_classes: int = 10, image_size=(3, 32, 32)) -> None:\n        self.num_samples = num_samples\n        self.num_classes = num_classes\n        self.image_size = image_size\n        self.data = torch.randn(num_samples, *image_size)\n        self.targets = torch.randint(0, num_classes, (num_samples,))\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n    def __getitem__(self, idx: int):\n        return self.data[idx], self.targets[idx]\n\n\nclass Preprocessor:\n    \"\"\"Create PyTorch DataLoaders for train/val from a dataset config dict.\"\"\"\n\n    def __init__(self, dataset_cfg):\n        self.cfg = dataset_cfg\n        self.batch_size = int(dataset_cfg.get(\"batch_size\", 128))\n        self.num_workers = int(dataset_cfg.get(\"num_workers\", 4))\n        self.pin_memory = bool(dataset_cfg.get(\"pin_memory\", True))\n\n    # ------------------------------------------------------------------\n    # Public API\n    # ------------------------------------------------------------------\n    def get_dataloaders(self) -> Tuple[DataLoader, DataLoader, Tuple[int, ...], int]:\n        dataset_name = self.cfg.get(\"name\")\n        if dataset_name == \"synthetic_classification\":\n            dataset = SyntheticClassificationDataset(\n                num_samples=self.cfg.get(\"num_samples\", 1024),\n                num_classes=self.cfg.get(\"num_classes\", 10),\n                image_size=tuple(self.cfg.get(\"image_size\", (3, 32, 32)))\n            )\n            input_shape = tuple(self.cfg.get(\"image_size\", (3, 32, 32)))\n            num_classes = self.cfg.get(\"num_classes\", 10)\n        elif dataset_name == \"cifar10\":\n            # Example fully-implemented real dataset (for quick prototyping).  In\n            # later specific experiments this clause can be replaced entirely\n            # without touching the rest of the pipeline.\n            transform = T.Compose([\n                T.ToTensor(),\n                T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ])\n            dataset = dsets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n            input_shape = (3, 32, 32)\n            num_classes = 10\n        elif dataset_name == \"DATASET_PLACEHOLDER\":\n            raise NotImplementedError(\n                \"PLACEHOLDER: Will be replaced with specific dataset loading logic in derived experiments.\")\n        else:\n            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n\n        # Deterministic split\n        val_fraction = float(self.cfg.get(\"val_fraction\", 0.1))\n        val_size = int(len(dataset) * val_fraction)\n        train_size = len(dataset) - val_size\n        train_set, val_set = random_split(dataset, [train_size, val_size],\n                                          generator=torch.Generator().manual_seed(42))\n\n        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True,\n                                  num_workers=self.num_workers, pin_memory=self.pin_memory)\n        val_loader = DataLoader(val_set, batch_size=self.batch_size, shuffle=False,\n                                num_workers=self.num_workers, pin_memory=self.pin_memory)\n        return train_loader, val_loader, input_shape, num_classes\n",
    "model_py": "\"\"\"\nsrc/model.py\n-------------\nLibrary of model architectures.  The factory can be extended by simply adding\nanother elif-branch.  Only *unknown* models are marked with placeholders – the\n*factory and SimpleCNN* are fully implemented.\n\"\"\"\nfrom __future__ import annotations\n\nimport torch.nn as nn\nfrom typing import Tuple\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Very small CNN suitable for synthetic 32×32 images (smoke tests).\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 16×16\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 8×8\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\nclass ModelFactory:\n    \"\"\"Create model instances by name.\"\"\"\n\n    @staticmethod\n    def create(model_name: str, num_classes: int, input_shape: Tuple[int, ...], **kwargs):\n        if model_name == \"simple_cnn\":\n            return SimpleCNN(num_classes=num_classes)\n        elif model_name == \"MODEL_PLACEHOLDER\":\n            raise NotImplementedError(\n                \"PLACEHOLDER: Will be replaced with specific model implementation in derived experiments.\")\n        else:\n            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "main_py": "\"\"\"\nsrc/main.py\n-----------\nOrchestrator:  reads a YAML file that enumerates run variations, spawns\nsrc/train.py as a subprocess for each, captures stdout/stderr to tee-logs, and\nfinally calls src/evaluate.py once all runs are finished.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nimport yaml\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nTHIS_PY_DIR = Path(__file__).resolve().parent\n\n\n# --------------------------- Tee helper ---------------------------------------\n\ndef tee_stream(stream, logfile_path: Path):\n    \"\"\"Forward a subprocess stream to *both* the console and a log file.\"\"\"\n    with open(logfile_path, \"w\") as f:\n        for line in iter(stream.readline, \"\"):\n            sys.stdout.write(line)\n            sys.stdout.flush()\n            f.write(line)\n            f.flush()\n    stream.close()\n\n\n# --------------------------- Main orchestration -------------------------------\n\ndef run_train_subprocess(run_cfg: Dict[str, Any], run_results_dir: Path):\n    run_cfg_path = run_results_dir / \"run_config.yaml\"\n    with open(run_cfg_path, \"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path),\n           \"--results-dir\", str(run_results_dir)]\n\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                               bufsize=1, universal_newlines=True)\n\n    stdout_thread = threading.Thread(target=tee_stream, args=(process.stdout, run_results_dir / \"stdout.log\"))\n    stderr_thread = threading.Thread(target=tee_stream, args=(process.stderr, run_results_dir / \"stderr.log\"))\n    stdout_thread.start()\n    stderr_thread.start()\n    process.wait()\n    stdout_thread.join()\n    stderr_thread.join()\n\n    if process.returncode != 0:\n        raise RuntimeError(f\"Subprocess for run {run_cfg['run_id']} failed with return code {process.returncode}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OHGW experiment orchestrator\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    group.add_argument(\"--config\", type=str, help=\"Path to custom experiment YAML file\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory for all results\")\n    args = parser.parse_args()\n\n    if args.smoke_test:\n        cfg_file = THIS_PY_DIR.parent / \"config\" / \"smoke_test.yaml\"\n    elif args.full_experiment:\n        cfg_file = THIS_PY_DIR.parent / \"config\" / \"full_experiment.yaml\"\n    else:\n        cfg_file = Path(args.config)\n\n    with open(cfg_file, \"r\") as f:\n        exp_cfg = yaml.safe_load(f)\n\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    # Save a copy of the master config for provenance\n    with open(results_root / \"master_config.yaml\", \"w\") as f:\n        yaml.safe_dump(exp_cfg, f)\n\n    for run_cfg in exp_cfg[\"runs\"]:\n        run_id = run_cfg[\"run_id\"]\n        run_results_dir = results_root / run_id\n        run_results_dir.mkdir(parents=True, exist_ok=True)\n        run_cfg[\"seed\"] = exp_cfg.get(\"global\", {}).get(\"seed\", 42)\n        run_cfg[\"threshold_metric\"] = exp_cfg.get(\"global\", {}).get(\"threshold_metric\", \"val_acc\")\n        run_cfg[\"threshold_value\"] = exp_cfg.get(\"global\", {}).get(\"threshold_value\", 0.9)\n        try:\n            run_train_subprocess(run_cfg, run_results_dir)\n        except RuntimeError as e:\n            print(str(e), file=sys.stderr)\n            sys.exit(1)\n\n    # --------------------  All runs finished  ------------------------------\n    # Trigger evaluation script\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch>=2.0\",\n    \"torchvision>=0.15\",\n    \"pyyaml>=6.0\",\n    \"tqdm>=4.65\",\n    \"matplotlib>=3.6\",\n    \"seaborn>=0.12\",\n    \"pandas>=1.5\",\n]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# -----------------------------------------------------------------------------\n# Lightweight configuration that exercises *every* part of the infrastructure\n# without heavy compute.  Uses a synthetic dataset and a toy CNN so that CI can\n# finish within seconds.\n# -----------------------------------------------------------------------------\nglobal:\n  seed: 7\n  threshold_metric: val_acc\n  threshold_value: 0.50\n\nruns:\n  - run_id: baseline_smoke\n    dataset:\n      name: synthetic_classification\n      num_samples: 512\n      num_classes: 4\n      image_size: [3, 32, 32]\n      batch_size: 64\n      val_fraction: 0.2\n    model:\n      name: simple_cnn\n      params: {}\n    training:\n      optimizer: sgd\n      lr: 0.01\n      epochs: 3\n      batch_size: 64\n      momentum: 0.0\n    ohgw:\n      enabled: false\n    hyperparameters:\n      weight_decay: 0.0\n\n  - run_id: ohgw_smoke\n    dataset:\n      name: synthetic_classification\n      num_samples: 512\n      num_classes: 4\n      image_size: [3, 32, 32]\n      batch_size: 64\n      val_fraction: 0.2\n    model:\n      name: simple_cnn\n      params: {}\n    training:\n      optimizer: sgd\n      lr: 0.01\n      epochs: 3\n      batch_size: 64\n      momentum: 0.0\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n    hyperparameters:\n      weight_decay: 0.01\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# -----------------------------------------------------------------------------\n# Template for the *real* experiments.  PLACEHOLDER strings will be replaced in\n# the next automation step so that the concrete data/model/scheduler settings\n# can be injected without touching the common core.\n# -----------------------------------------------------------------------------\nglobal:\n  seed: 42\n  threshold_metric: val_acc\n  threshold_value: 0.93\n\nruns:\n  # ---------------------------------------------------------------------------\n  # BASELINE – vanilla scheduler (placeholder)\n  # ---------------------------------------------------------------------------\n  - run_id: BASELINE_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER          # PLACEHOLDER: specific dataset name\n      params: SPECIFIC_CONFIG_PLACEHOLDER # PLACEHOLDER: dataset-specific params\n    model:\n      name: MODEL_PLACEHOLDER            # PLACEHOLDER: model architecture\n      params: SPECIFIC_CONFIG_PLACEHOLDER # PLACEHOLDER: model params\n    training:\n      optimizer: sgd\n      lr: 0.1\n      epochs: 50\n      batch_size: 128\n      momentum: 0.9\n    ohgw:\n      enabled: false\n    hyperparameters:\n      weight_decay: 0.0005\n\n  # ---------------------------------------------------------------------------\n  # PROPOSED – same scheduler + OHGW\n  # ---------------------------------------------------------------------------\n  - run_id: OHGW_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER          # PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER            # PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      optimizer: sgd\n      lr: 0.1\n      epochs: 50\n      batch_size: 128\n      momentum: 0.9\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n    hyperparameters:\n      weight_decay: 0.0005\n\n  # Additional ablations / variants can be appended below using the same schema\n  # e.g., RANDOM_WARMSTART_PLACEHOLDER, MULTISTEP_OHGW_PLACEHOLDER, etc.\n  # ---------------------------------------------------------------------------\n  \n  # - run_id: RANDOM_WARMSTART_PLACEHOLDER\n  #   ...\n  # ---------------------------------------------------------------------------\n\n# End of full_experiment.yaml\n\n"
}
