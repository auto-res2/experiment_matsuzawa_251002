
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback




# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport random\nfrom datetime import datetime\nfrom typing import Dict, Any, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import confusion_matrix\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\n\nfrom .preprocess import get_data_loaders\nfrom .model import build_model\n\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Utility to set RNG seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef compute_metrics(outputs: torch.Tensor, targets: torch.Tensor, num_classes: int) -> Dict[str, Any]:\n    \"\"\"Compute classification metrics given logits and targets.\"\"\"\n    with torch.no_grad():\n        preds = torch.argmax(outputs, dim=1)\n        correct = (preds == targets).sum().item()\n        total = targets.size(0)\n        accuracy = correct / total\n    return {\n        \"correct\": correct,\n        \"total\": total,\n        \"accuracy\": accuracy,\n    }\n\n\ndef ohgw_warm_start(model: nn.Module,\n                    hyper_params: List[torch.Tensor],\n                    data_iter: DataLoader,\n                    criterion: nn.Module,\n                    eta_h: float,\n                    device: torch.device) -> None:\n    \"\"\"\n    One-Shot Hyper-Gradient Warm-Start (OHGW).\n    Runs a *single* forward/backward pass on one mini-batch to obtain hyper-gradients\n    w.r.t. the provided ``hyper_params`` and applies a single update step with step-size ``eta_h``.\n\n    This function is generic: To receive gradients, the hyper-parameters must **appear in the computational graph**.\n    Concretely, the loss should depend on them (e.g. an L2 regularisation coefficient, label-smoothing factor …).\n    \"\"\"\n    model.train()\n    try:\n        batch = next(iter(data_iter))\n    except StopIteration:\n        return  # should not happen for non-empty loaders\n\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    # Zero gradients on model parameters and hyper-parameters\n    for p in model.parameters():\n        if p.grad is not None:\n            p.grad.zero_()\n    for hp in hyper_params:\n        if hp.grad is not None:\n            hp.grad.zero_()\n\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    grads = torch.autograd.grad(loss, hyper_params, retain_graph=False, create_graph=False, allow_unused=True)\n\n    # In case some hyper-parameter did not receive a gradient, use zero-tensor as fallback\n    for hp, g in zip(hyper_params, grads):\n        if g is None:\n            continue\n        with torch.no_grad():\n            hp -= eta_h * g\n\n\nclass SmoothedCrossEntropy(nn.Module):\n    \"\"\"Cross-Entropy with label smoothing (hyper-parameter `label_smoothing`).\"\"\"\n\n    def __init__(self, smoothing: torch.Tensor):\n        super().__init__()\n        self.smoothing = smoothing  # torch Tensor requiring grad if used for OHGW\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        n_classes = logits.size(1)\n        log_probs = F.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing.item() / (n_classes - 1))\n            true_dist.scatter_(1, targets.data.unsqueeze(1), 1.0 - self.smoothing.item())\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ndef train_run(cfg: Dict[str, Any], results_dir: str) -> Dict[str, Any]:\n    \"\"\"Execute one training run and return collected metrics.\"\"\"\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Data -----------------------------------------------------------------\n    train_loader, val_loader, num_classes, input_shape = get_data_loaders(cfg[\"dataset\"], cfg[\"training\"])\n\n    # 2. Model ----------------------------------------------------------------\n    model = build_model(cfg[\"model\"], num_classes=num_classes, input_shape=input_shape)\n    model.to(device)\n\n    # 3. Hyper-parameters that will receive OHGW --------------------------------\n    hp_configs = cfg.get(\"hyperparameters\", [])\n    hyper_params = []  # type: List[torch.Tensor]\n    hp_dict = {}  # name -> torch.Tensor (to report final values)\n\n    for hp in hp_configs:\n        # Store as 1-element tensor to make it differentiable\n        t = torch.tensor(hp[\"value\"], dtype=torch.float32, requires_grad=True, device=device)\n        hyper_params.append(t)\n        hp_dict[hp[\"name\"]] = t\n\n    # 4. Loss & Optimizer ------------------------------------------------------\n    # Label smoothing handling: if specified in hyper params, use SmoothedCrossEntropy\n    ls_hp = next((hp for hp in hp_configs if hp[\"name\"] == \"label_smoothing\"), None)\n    if ls_hp is not None:\n        smoothing_tensor = hp_dict[\"label_smoothing\"]\n        criterion = SmoothedCrossEntropy(smoothing=smoothing_tensor)\n    else:\n        criterion = nn.CrossEntropyLoss()\n\n    # Weight decay hyper-parameter handling\n    wd_hp = next((hp for hp in hp_configs if hp[\"name\"] == \"l2_coeff\"), None)\n    weight_decay = 0.0 if wd_hp is None else hp_dict[\"l2_coeff\"].item()\n\n    optimizer_cfg = cfg.get(\"optimizer\", {\"name\": \"sgd\", \"lr\": 1e-2})\n    if optimizer_cfg[\"name\"].lower() == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=optimizer_cfg[\"lr\"], weight_decay=weight_decay)\n    else:  # default to SGD\n        optimizer = optim.SGD(model.parameters(), lr=optimizer_cfg[\"lr\"], momentum=optimizer_cfg.get(\"momentum\", 0.0), weight_decay=weight_decay)\n\n    # Scheduler placeholder – simple MultiStepLR for now --------------------------------\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=optimizer_cfg.get(\"milestones\", []), gamma=optimizer_cfg.get(\"gamma\", 0.1))\n\n    # 5. OHGW ------------------------------------------------------------------\n    if cfg.get(\"hypergradient\", {}).get(\"enabled\", False):\n        eta_h = cfg[\"hypergradient\"].get(\"eta_h\", 1e-3)\n        ohgw_warm_start(model, hyper_params, train_loader, criterion, eta_h, device)\n\n    # 6. Training Loop ---------------------------------------------------------\n    epochs = cfg[\"training\"].get(\"epochs\", 1)\n    log_interval = cfg[\"training\"].get(\"log_interval\", 10)\n\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_accuracy\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        # --- Train -----------------------------------------------------------\n        model.train()\n        running_loss = 0.0\n        for batch_idx, (inputs, targets) in enumerate(train_loader, start=1):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            # L2 regularisation via differentiable hyper-parameter (if used)\n            if wd_hp is not None:\n                l2_term = 0.0\n                for p in model.parameters():\n                    l2_term = l2_term + torch.sum(p ** 2)\n                loss = loss + hp_dict[\"l2_coeff\"] * 0.5 * l2_term\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            if batch_idx % log_interval == 0:\n                avg = running_loss / log_interval\n                print(json.dumps({\"run_id\": run_id, \"event\": \"train_iter\", \"epoch\": epoch, \"batch\": batch_idx, \"loss\": avg}))\n                running_loss = 0.0\n\n        scheduler.step()\n\n        # --- Validation ------------------------------------------------------\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        all_preds = []\n        all_targets = []\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss += loss.item() * targets.size(0)\n\n                preds = torch.argmax(outputs, dim=1)\n                correct += (preds == targets).sum().item()\n                total += targets.size(0)\n\n                all_preds.append(preds.cpu())\n                all_targets.append(targets.cpu())\n\n        val_loss /= total\n        val_accuracy = correct / total\n\n        history[\"train_loss\"].append(running_loss)  # final mini-batch loss after epoch\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_accuracy\"].append(val_accuracy)\n\n        print(json.dumps({\"run_id\": run_id, \"event\": \"epoch_end\", \"epoch\": epoch, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy}))\n\n    # 7. Post training metrics & figures --------------------------------------\n    # Confusion matrix\n    confmat = confusion_matrix(torch.cat(all_targets), torch.cat(all_preds))\n\n    # Save results\n    results = {\n        \"run_id\": run_id,\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"config\": cfg,\n        \"history\": history,\n        \"final_val_accuracy\": history[\"val_accuracy\"][-1],\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"hyperparameters\": {k: v.item() for k, v in hp_dict.items()},\n    }\n    results_path = os.path.join(results_dir, \"results.json\")\n    with open(results_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # --- Figures -------------------------------------------------------------\n    epochs_axis = list(range(1, epochs + 1))\n\n    # Training loss & Val loss in one plot\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs_axis, history[\"val_loss\"], label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Validation Loss – {run_id}\")\n    plt.xticks(epochs_axis)\n    final_loss = history[\"val_loss\"][-1]\n    plt.annotate(f\"{final_loss:.4f}\", xy=(epochs_axis[-1], final_loss))\n    plt.legend()\n    loss_fig_path = os.path.join(results_dir, f\"training_loss_{run_id}.pdf\")\n    plt.savefig(loss_fig_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # Validation accuracy curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs_axis, history[\"val_accuracy\"], label=\"Val Acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.ylim(0, 1)\n    plt.title(f\"Validation Accuracy – {run_id}\")\n    plt.xticks(epochs_axis)\n    best_acc = max(history[\"val_accuracy\"])\n    best_epoch = history[\"val_accuracy\"].index(best_acc) + 1\n    plt.annotate(f\"{best_acc:.2%}\", xy=(best_epoch, best_acc))\n    plt.legend()\n    acc_fig_path = os.path.join(results_dir, f\"accuracy_{run_id}.pdf\")\n    plt.savefig(acc_fig_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # Confusion matrix heatmap\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(confmat, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix – {run_id}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    cm_fig_path = os.path.join(results_dir, f\"confusion_matrix_{run_id}.pdf\")\n    plt.savefig(cm_fig_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # Experiment description & figure names to stdout as required\n    print(\"Experiment description:\")\n    print(json.dumps(cfg, indent=2))\n    print(\"Figures produced:\")\n    print(json.dumps([os.path.basename(loss_fig_path), os.path.basename(acc_fig_path), os.path.basename(cm_fig_path)]))\n\n    return results\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation (TRAIN).\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to JSON file containing the run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to save artifacts (metrics, figs, logs).\")\n\n    args = parser.parse_args()\n\n    os.makedirs(args.results_dir, exist_ok=True)\n\n    with open(args.run_config, \"r\") as f:\n        cfg = json.load(f)\n\n    try:\n        train_run(cfg, args.results_dir)\n    except Exception as e:\n        # Ensure that exceptions are visible in CI logs\n        print(json.dumps({\"run_id\": cfg.get(\"run_id\", \"unknown\"), \"event\": \"exception\", \"error\": str(e)}))\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n\ndef gather_results(results_dir: Path) -> List[Dict]:\n    \"\"\"Collect results.json files from each run directory.\"\"\"\n    result_files = list(results_dir.glob(\"*/results.json\"))\n    aggregated = []\n    for f in result_files:\n        with open(f, \"r\") as fd:\n            aggregated.append(json.load(fd))\n    return aggregated\n\n\ndef create_accuracy_comparison(results: List[Dict], save_path: Path) -> None:\n    \"\"\"Generate bar plot comparing final validation accuracy across runs.\"\"\"\n    df = pd.DataFrame({\n        \"run_id\": [r[\"run_id\"] for r in results],\n        \"final_val_accuracy\": [r[\"final_val_accuracy\"] for r in results],\n    })\n    plt.figure(figsize=(8, 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"final_val_accuracy\", palette=\"viridis\")\n    plt.ylim(0, 1)\n    plt.ylabel(\"Final Validation Accuracy\")\n    plt.xlabel(\"\")\n    plt.title(\"Accuracy Comparison Across Runs\")\n    # annotate each bar\n    for idx, acc in enumerate(df[\"final_val_accuracy\"]):\n        plt.text(idx, acc + 0.01, f\"{acc:.2%}\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(save_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Aggregate and compare experiment results.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory containing sub-directories for each run.\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    results = gather_results(results_dir)\n\n    if not results:\n        print(json.dumps({\"event\": \"evaluate\", \"status\": \"no_results_found\"}))\n        return\n\n    # Create comparison figure\n    acc_fig_name = \"accuracy_comparison.pdf\"\n    acc_fig_path = results_dir / acc_fig_name\n    create_accuracy_comparison(results, acc_fig_path)\n\n    # Output aggregated metrics to stdout\n    summary = {\n        \"event\": \"evaluate_summary\",\n        \"runs\": [r[\"run_id\"] for r in results],\n        \"final_val_accuracy\": {r[\"run_id\"]: r[\"final_val_accuracy\"] for r in results},\n        \"figure\": acc_fig_name,\n    }\n    print(json.dumps(summary, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"\nCommon preprocessing and data-loading utilities.\nAll domain-specific datasets should be added here in the future.\nFor now we provide a random classification dataset that acts as a minimal placeholder\nso that smoke tests can execute end-to-end without external data.\n\"\"\"\n\nimport math\nimport os\nimport random\nfrom typing import Tuple, Dict, Any\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n\nclass RandomClassificationDataset(Dataset):\n    \"\"\"Synthetic dataset for quick smoke tests.\"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, ...], num_classes: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = torch.randn(self.input_shape).float()\n        y = torch.randint(0, self.num_classes, (1,)).item()\n        return x, y\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef get_data_loaders(dataset_cfg: Dict[str, Any], training_cfg: Dict[str, Any]):\n    \"\"\"Return train & validation DataLoaders along with dataset meta-data.\n\n    Parameters\n    ----------\n    dataset_cfg : Dict\n        Section of the run-config related to the dataset.\n    training_cfg : Dict\n        Section containing batch-size, num-workers, validation-split etc.\n    \"\"\"\n    dataset_name = dataset_cfg.get(\"name\", \"DATASET_PLACEHOLDER\")\n    batch_size = training_cfg.get(\"batch_size\", 32)\n    num_workers = training_cfg.get(\"num_workers\", 2)\n    val_split = training_cfg.get(\"val_split\", 0.2)\n\n    if dataset_name == \"DATASET_PLACEHOLDER\":\n        # fallback to random synthetic dataset so that CI can run\n        num_samples = dataset_cfg.get(\"num_samples\", 500)\n        num_classes = dataset_cfg.get(\"num_classes\", 10)\n        input_shape = tuple(dataset_cfg.get(\"input_shape\", (1, 28, 28)))  # (C,H,W)\n        full_ds = RandomClassificationDataset(num_samples, input_shape, num_classes)\n        val_size = int(math.ceil(num_samples * val_split))\n        train_size = num_samples - val_size\n        train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(f\"Dataset '{dataset_name}' is not implemented yet. Replace placeholder with actual loader.\")\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    return train_loader, val_loader, dataset_cfg.get(\"num_classes\", 10), dataset_cfg.get(\"input_shape\", (1, 28, 28))\n",
    "model_py": "\"\"\"\nModel architectures used across experiments.\nThis module already contains lightweight baseline architectures that can be swapped out later.\nFor domain-specific sophisticated models replace the placeholders with actual architectures (e.g. ResNet, Transformer …).\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\n\nimport torch\nfrom torch import nn\n\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_shape: Tuple[int, ...], num_classes: int, hidden_dim: int = 128):\n        super().__init__()\n        c, h, w = input_shape\n        flattened = c * h * w\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(flattened, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, input_shape: Tuple[int, ...], num_classes: int):\n        super().__init__()\n        c, h, w = input_shape\n        self.features = nn.Sequential(\n            nn.Conv2d(c, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        flattened = 64 * (h // 4) * (w // 4)\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(flattened, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef build_model(model_cfg: Dict[str, Any], num_classes: int, input_shape: Tuple[int, ...]) -> nn.Module:\n    \"\"\"Factory to build model specified by ``model_cfg``.\n\n    Parameters\n    ----------\n    model_cfg : Dict[str, Any]\n        Section of the run-config describing the model.\n    num_classes : int\n        Number of output classes (for classification).\n    input_shape : Tuple[int]\n        Shape of a single input sample (C,H,W).\n    \"\"\"\n    model_name = model_cfg.get(\"name\", \"MODEL_PLACEHOLDER\")\n\n    if model_name == \"mlp\" or model_name == \"MODEL_PLACEHOLDER\":\n        hidden_dim = model_cfg.get(\"hidden_dim\", 128)\n        return SimpleMLP(input_shape=input_shape, num_classes=num_classes, hidden_dim=hidden_dim)\n    elif model_name == \"cnn\":\n        return SimpleCNN(input_shape=input_shape, num_classes=num_classes)\n    else:\n        # PLACEHOLDER: Will be replaced with domain-specific model architectures\n        raise NotImplementedError(f\"Model '{model_name}' not implemented yet. Replace placeholder with actual architecture.\")\n",
    "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# Helper --------------------------------------------------------------\n\ndef tee_stream(stream, log_file_path):\n    \"\"\"Forward a stream to both stdout/stderr and a log file.\"\"\"\n    with open(log_file_path, \"w\") as log_file:\n        for line in iter(stream.readline, b\"\"):  # type: ignore[attr-defined]\n            sys.stdout.buffer.write(line)\n            log_file.buffer.write(line)\n            sys.stdout.flush()\n            log_file.flush()\n    stream.close()\n\n\ndef launch_run(run_cfg: Dict, results_dir: Path) -> None:\n    \"\"\"Launch a subprocess that executes a single run via src.train.\"\"\"\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write run-config JSON file for the child process\n    run_cfg_path = run_dir / \"run_config.json\"\n    with open(run_cfg_path, \"w\") as fp:\n        json.dump(run_cfg, fp, indent=2)\n\n    cmd = [sys.executable, \"-m\", \"src.train\", \"--run-config\", str(run_cfg_path), \"--results-dir\", str(run_dir)]\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Tee stdout/stderr\n    tee_stream(process.stdout, run_dir / \"stdout.log\")\n    tee_stream(process.stderr, run_dir / \"stderr.log\")\n\n    retcode = process.wait()\n    if retcode != 0:\n        raise RuntimeError(f\"Run '{run_id}' exited with code {retcode}\")\n\n\ndef run_experiments(config_path: Path, results_dir: Path):\n    \"\"\"Execute every variation listed in the YAML configuration file sequentially.\"\"\"\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    runs: List[Dict] = config.get(\"runs\", [])\n    if not runs:\n        raise ValueError(\"No 'runs' defined in configuration file.\")\n\n    print(json.dumps({\"event\": \"run_start\", \"num_runs\": len(runs), \"config_file\": str(config_path)}))\n\n    for run_cfg in runs:\n        print(json.dumps({\"event\": \"run_launch\", \"run_id\": run_cfg.get(\"run_id\")}))\n        launch_run(run_cfg, results_dir)\n\n    # After all runs, trigger evaluation\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.check_call(eval_cmd)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run lightweight smoke test.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in YAML config.\")\n\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store all outputs.\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    if args.smoke_test:\n        config_file = Path(\"config/smoke_test.yaml\")\n    else:\n        config_file = Path(\"config/full_experiment.yaml\")\n\n    run_experiments(config_file, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"ohgw-foundation\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for OHGW experiments\"\nauthors = [\n  { name = \"AI-Generated\", email = \"research@ohgw.org\" }\n]\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\n# torchvision is optional but handy for future datasets\n\"torchvision\" = { version = \"*\", optional = true }\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\nscikit-learn = \"*\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "smoke_test_yaml": "# Light-weight smoke test configuration\nruns:\n  - run_id: baseline_placeholder\n    description: \"Baseline training with placeholder dataset & model. No OHGW.\"\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with real dataset name\n      num_samples: 200\n      num_classes: 10\n      input_shape: [1, 28, 28]\n    model:\n      name: MODEL_PLACEHOLDER    # PLACEHOLDER: Will be replaced with real model\n      hidden_dim: 64\n    training:\n      epochs: 2\n      batch_size: 32\n      val_split: 0.2\n      log_interval: 10\n    optimizer:\n      name: sgd\n      lr: 0.01\n      momentum: 0.9\n    hypergradient:\n      enabled: false\n    hyperparameters: []\n\n  - run_id: ohgw_placeholder\n    description: \"Same as baseline but with OHGW warm-start on l2_coeff hyper-param.\"\n    dataset:\n      name: DATASET_PLACEHOLDER\n      num_samples: 200\n      num_classes: 10\n      input_shape: [1, 28, 28]\n    model:\n      name: MODEL_PLACEHOLDER\n      hidden_dim: 64\n    training:\n      epochs: 2\n      batch_size: 32\n      val_split: 0.2\n      log_interval: 10\n    optimizer:\n      name: sgd\n      lr: 0.01\n      momentum: 0.9\n    hypergradient:\n      enabled: true\n      eta_h: 0.001\n    hyperparameters:\n      - name: l2_coeff\n        value: 0.001\n      - name: label_smoothing\n        value: 0.05\n",
    "full_experiment_yaml": "# Full experiment placeholder configuration.\n# Replace DATASET_PLACEHOLDER / MODEL_PLACEHOLDER with actual components in the next phase.\nruns:\n  - run_id: BASELINE_DATASET_PLACEHOLDER_MODEL_PLACEHOLDER\n    description: \"Baseline scheduler without OHGW.\"\n    dataset:\n      name: DATASET_PLACEHOLDER  # e.g. cifar10\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER    # e.g. resnet20\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      val_split: 0.1\n    optimizer:\n      name: sgd\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      momentum: SPECIFIC_CONFIG_PLACEHOLDER\n    hypergradient:\n      enabled: false\n    hyperparameters: []\n\n  - run_id: OHGW_DATASET_PLACEHOLDER_MODEL_PLACEHOLDER\n    description: \"Scheduler variant with One-Shot Hyper-Gradient Warm-Starts.\"\n    dataset:\n      name: DATASET_PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      val_split: 0.1\n    optimizer:\n      name: sgd\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      momentum: SPECIFIC_CONFIG_PLACEHOLDER\n    hypergradient:\n      enabled: true\n      eta_h: 0.001\n    hyperparameters: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # Additional ablations / controls can be added here following the same schema.\n\n# NOTE: Concrete values will be injected in the next phase of experimentation.\n"
}
