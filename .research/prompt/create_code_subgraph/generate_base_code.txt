
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: Core requirement 1 is not fully satisfied: the base code does not implement a model saving/loading mechanism. train.py trains the model but never saves weights to disk, and evaluate.py therefore cannot load or compare checkpoints. All other criteria (placeholder strategy, 8-file structure, CLI orchestration, visualization, PyTorch usage, no premature specialization) are met, but lack of explicit model persistence breaks the ‘Complete Core Logic Implementation’ requirement.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "#!/usr/bin/env python\n\"\"\"\ntrain.py – run a single experiment variation\nThis module is invoked **only** by src/main.py.  It receives exactly two\ncommand-line arguments:\n    --config-file  : YAML or JSON file that contains *one* run configuration\n    --results-dir  : Directory to which all artefacts (checkpoints, metrics,\n                     figures) of this run must be written\ntrain.py is responsible for:\n  • end-to-end training (including the optional OHGW hyper-gradient warm-start)\n  • periodic validation\n  • structured logging of all epoch-wise metrics\n  • model checkpoint persistence (model.pt)\n  • generation of loss / accuracy curves as publication-quality PDF figures\n  • emitting a JSON summary (incl. run_id) to stdout so that the parent\n    process can capture the final metrics quickly\nThe file **must not** contain any placeholders for core logic.  Only\nexplicitly marked areas containing dataset/model specifics are allowed to be\nreplaced downstream.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # head-less backend for PDF generation\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n# --------------------------------------------------------------------------------------\n# Utility helpers\n# --------------------------------------------------------------------------------------\n\ndef set_reproducibility(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Run single experiment variation\")\n    parser.add_argument(\"--config-file\", required=True, type=str,\n                        help=\"Path to the YAML/JSON file holding one run configuration.\")\n    parser.add_argument(\"--results-dir\", required=True, type=str,\n                        help=\"Directory where artefacts for this run are written.\")\n    return parser.parse_args()\n\n# --------------------------------------------------------------------------------------\n# One-Shot Hyper-Gradient Warm-Start (OHGW) implementation\n# --------------------------------------------------------------------------------------\n\ndef apply_hypergradient_warmstart(model: nn.Module, batch: Dict[str, torch.Tensor],\n                                  criterion: nn.Module, device: torch.device,\n                                  hyper_cfg: Dict, eta_h: float = 1e-3) -> Dict:\n    \"\"\"Performs a single stochastic hyper-gradient step on *continuous* hyper-parameters.\n    For this foundational implementation we demonstrate the idea on *weight_decay*.\n\n    The weight-decay hyper-parameter is introduced as a differentiable scalar that\n    participates in the loss ( L_total = L_ce + wd * \\sum ||w||^2 ).  The gradient\n    wrt wd is cheap to compute via autograd because we **retain** the graph from\n    the forward pass.\n\n    Parameters\n    ----------\n    model       : network before any training took place (randomly initialised)\n    batch       : one mini-batch (dict with keys \"inputs\" and \"targets\")\n    criterion   : classification loss (without reduction of regularisation yet)\n    device      : cuda / cpu device on which tensors live\n    hyper_cfg   : dictionary containing continuous hyper-parameters.  Currently\n                  only \"weight_decay\" is supported in the foundational code; the\n                  derived experiments can extend this easily.\n    eta_h       : learning-rate for the hyper-gradient update (Eq. 3 in method).\n\n    Returns\n    -------\n    hyper_cfg   : same dict with *in-place* updated values.\n    \"\"\"\n    model.train()\n\n    # Convert scalar hyper-parameters into differentiable torch.Tensors.\n    wd = torch.tensor(float(hyper_cfg.get(\"weight_decay\", 0.0)), dtype=torch.float32,\n                      requires_grad=True, device=device)\n\n    # Forward + loss (includes differentiable weight decay)\n    inputs, targets = batch[\"inputs\"].to(device), batch[\"targets\"].to(device)\n    outputs = model(inputs)\n    ce_loss = criterion(outputs, targets)\n    l2_norm = 0.5 * sum(p.pow(2).sum() for p in model.parameters())\n    loss = ce_loss + wd * l2_norm\n\n    # One autograd call to get dL/dwd (stochastic hyper-gradient)\n    (grad_wd,) = torch.autograd.grad(loss, wd, retain_graph=False, create_graph=False)\n\n    # Hyper-parameter update ( Eq. 4 )\n    with torch.no_grad():\n        wd -= eta_h * grad_wd\n\n    # Store updated value back into config dict (as python float for later use)\n    hyper_cfg[\"weight_decay\"] = float(wd.item())\n    return hyper_cfg\n\n# --------------------------------------------------------------------------------------\n# Training & validation loops\n# --------------------------------------------------------------------------------------\n\ndef train_one_epoch(model: nn.Module, loader: torch.utils.data.DataLoader,\n                    criterion: nn.Module, optimizer: optim.Optimizer,\n                    device: torch.device) -> Dict[str, float]:\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n    epoch_loss = running_loss / total\n    epoch_acc = 100.0 * correct / total\n    return {\"loss\": epoch_loss, \"acc\": epoch_acc}\n\n\ndef validate(model: nn.Module, loader: torch.utils.data.DataLoader,\n             criterion: nn.Module, device: torch.device) -> Dict[str, float]:\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    epoch_loss = running_loss / total\n    epoch_acc = 100.0 * correct / total\n    return {\"loss\": epoch_loss, \"acc\": epoch_acc}\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    args = parse_args()\n\n    # ------------------------------------------------------------------\n    # Load configuration\n    # ------------------------------------------------------------------\n    cfg_path = Path(args.config_file)\n    if cfg_path.suffix in {\".yaml\", \".yml\"}:\n        run_cfg = yaml.safe_load(cfg_path.read_text())\n    else:\n        run_cfg = json.loads(cfg_path.read_text())\n\n    run_id: str = run_cfg[\"run_id\"]\n    results_dir = Path(args.results_dir)\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save the *exact* config for maximum reproducibility\n    with open(run_dir / \"config.yaml\", \"w\") as fp:\n        yaml.safe_dump(run_cfg, fp)\n\n    # ------------------------------------------------------------------\n    # Reproducibility & device\n    # ------------------------------------------------------------------\n    seed = int(run_cfg.get(\"seed\", 42))\n    set_reproducibility(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data pipeline (placeholders replaced in derived step)\n    # ------------------------------------------------------------------\n    train_loader, val_loader, num_classes = get_dataloaders(run_cfg)\n\n    # ------------------------------------------------------------------\n    # Model & Optimiser\n    # ------------------------------------------------------------------\n    model = get_model(run_cfg, num_classes=num_classes).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    # Hyper-parameters (will potentially be updated by OHGW)\n    optim_cfg = run_cfg.get(\"optimizer\", {})\n    lr = float(optim_cfg.get(\"lr\", 0.001))\n    weight_decay = float(optim_cfg.get(\"weight_decay\", 0.0))\n\n    # Placeholder note: For derived experiments momentum, betas etc. can be added.\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n\n    # ------------------------------------------------------------------\n    # One-Shot Hyper-Gradient Warm-Start (optional)\n    # ------------------------------------------------------------------\n    if run_cfg.get(\"training\", {}).get(\"use_ohgw\", False):\n        # We need *one* mini-batch.  Rely on next(iter(loader)).\n        warm_batch_inputs, warm_batch_targets = next(iter(train_loader))\n        batch = {\n            \"inputs\": warm_batch_inputs,\n            \"targets\": warm_batch_targets,\n        }\n        hyper_cfg = {\"weight_decay\": weight_decay}\n        hyper_cfg = apply_hypergradient_warmstart(model, batch, criterion, device,\n                                                  hyper_cfg, eta_h=1e-3)\n        # Re-instantiate optimiser with updated weight_decay value\n        weight_decay = hyper_cfg[\"weight_decay\"]\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9,\n                              weight_decay=weight_decay)\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    epochs: int = int(run_cfg.get(\"training\", {}).get(\"epochs\", 1))\n    metrics_per_epoch: List[Dict] = []\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_metrics = validate(model, val_loader, criterion, device)\n\n        epoch_metrics = {\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_metrics[\"loss\"],\n            \"train_acc\": train_metrics[\"acc\"],\n            \"val_loss\": val_metrics[\"loss\"],\n            \"val_acc\": val_metrics[\"acc\"],\n            \"time_elapsed_sec\": time.time() - start_time,\n        }\n        metrics_per_epoch.append(epoch_metrics)\n\n        # Console log in compact JSON format so that tee can capture\n        print(json.dumps({\"run_id\": run_id, \"epoch\": epoch + 1,\n                          \"train_loss\": train_metrics[\"loss\"],\n                          \"val_acc\": val_metrics[\"acc\"]}))\n\n    total_time = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # Save artefacts\n    # ------------------------------------------------------------------\n    # 1. Model checkpoint\n    torch.save({\n        \"model_state_dict\": model.state_dict(),\n        \"num_classes\": num_classes,\n        \"run_cfg\": run_cfg,\n    }, run_dir / \"model.pt\")\n\n    # 2. Metrics\n    with open(run_dir / \"results.json\", \"w\") as fp:\n        json.dump({\n            \"run_id\": run_id,\n            \"metrics\": metrics_per_epoch,\n            \"total_time_sec\": total_time,\n            \"final_val_acc\": metrics_per_epoch[-1][\"val_acc\"],\n            \"final_val_loss\": metrics_per_epoch[-1][\"val_loss\"],\n            \"use_ohgw\": run_cfg.get(\"training\", {}).get(\"use_ohgw\", False),\n        }, fp, indent=2)\n\n    # 3. Figures --------------------------------------------------------------------\n    epochs_axis = [m[\"epoch\"] for m in metrics_per_epoch]\n    train_losses = [m[\"train_loss\"] for m in metrics_per_epoch]\n    val_losses = [m[\"val_loss\"] for m in metrics_per_epoch]\n    train_accs  = [m[\"train_acc\"] for m in metrics_per_epoch]\n    val_accs    = [m[\"val_acc\"] for m in metrics_per_epoch]\n\n    # Loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs_axis, train_losses, label=\"Train\")\n    plt.plot(epochs_axis, val_losses, label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training/Validation Loss – %s\" % run_id)\n    plt.legend()\n    # annotate final value\n    plt.annotate(f\"{val_losses[-1]:.3f}\",\n                 xy=(epochs_axis[-1], val_losses[-1]),\n                 xytext=(epochs_axis[-1], val_losses[-1] + 0.05))\n    plt.savefig(run_dir / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Accuracy curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs_axis, train_accs, label=\"Train\")\n    plt.plot(epochs_axis, val_accs, label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(\"Training/Validation Accuracy – %s\" % run_id)\n    plt.legend()\n    plt.annotate(f\"{val_accs[-1]:.2f}%\",\n                 xy=(epochs_axis[-1], val_accs[-1]),\n                 xytext=(epochs_axis[-1], val_accs[-1] + 2))\n    plt.savefig(run_dir / f\"accuracy_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------\n    # Final summary to stdout (captured by main.py)\n    # ------------------------------------------------------------------\n    summary = {\n        \"run_id\": run_id,\n        \"epochs\": epochs,\n        \"final_val_acc\": val_accs[-1],\n        \"final_val_loss\": val_losses[-1],\n        \"total_time_sec\": total_time,\n        \"use_ohgw\": run_cfg.get(\"training\", {}).get(\"use_ohgw\", False),\n        \"figures\": [\n            f\"training_loss_{run_id}.pdf\",\n            f\"accuracy_{run_id}.pdf\",\n        ],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "#!/usr/bin/env python\n\"\"\"\nevaluate.py – After all variations finished, this script consolidates results and\ncreates comparison figures that summarise the numerical data.  A detailed\ntextual description of the experiment is printed **before** any numerical\noutput, adhering to the requirements in the task description.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# --------------------------------------------------------------------------------------\n# CLI\n# --------------------------------------------------------------------------------------\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Evaluate & compare experiments\")\n    parser.add_argument(\"--results-dir\", required=True, type=str,\n                        help=\"Directory holding one sub-folder per run_id.\")\n    return parser.parse_args()\n\n# --------------------------------------------------------------------------------------\n# Helper\n# --------------------------------------------------------------------------------------\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    all_runs = []\n    for sub in sorted(results_dir.iterdir()):\n        if not sub.is_dir():\n            continue\n        res_file = sub / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as fp:\n                all_runs.append(json.load(fp))\n    return all_runs\n\n# --------------------------------------------------------------------------------------\n# Figure helpers\n# --------------------------------------------------------------------------------------\n\ndef bar_plot_final_accuracy(results: List[Dict], save_dir: Path) -> str:\n    run_ids = [r[\"run_id\"] for r in results]\n    final_accs = [r[\"final_val_acc\"] for r in results]\n\n    plt.figure(figsize=(max(6, len(run_ids) * 0.8), 4))\n    sns.barplot(x=run_ids, y=final_accs, palette=\"viridis\")\n    plt.ylabel(\"Final Validation Accuracy (%)\")\n    plt.xlabel(\"Run ID\")\n    plt.title(\"Final Accuracy Comparison\")\n\n    # annotate\n    for idx, val in enumerate(final_accs):\n        plt.text(idx, val + 0.5, f\"{val:.2f}\", ha='center', va='bottom')\n\n    fname = \"accuracy_comparison.pdf\"\n    plt.savefig(save_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\ndef bar_plot_time(results: List[Dict], save_dir: Path) -> str:\n    run_ids = [r[\"run_id\"] for r in results]\n    times = [r[\"total_time_sec\"] / 60.0 for r in results]  # minutes\n\n    plt.figure(figsize=(max(6, len(run_ids) * 0.8), 4))\n    sns.barplot(x=run_ids, y=times, palette=\"magma\")\n    plt.ylabel(\"Total Time (minutes)\")\n    plt.xlabel(\"Run ID\")\n    plt.title(\"Wall-Clock Time Comparison\")\n    for idx, val in enumerate(times):\n        plt.text(idx, val + 0.5, f\"{val:.1f}\", ha='center', va='bottom')\n\n    fname = \"time_comparison.pdf\"\n    plt.savefig(save_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    args = parse_args()\n    results_dir = Path(args.results_dir)\n\n    # ------------------------------------------------------------------\n    # Load results\n    # ------------------------------------------------------------------\n    results = load_results(results_dir)\n    if not results:\n        raise RuntimeError(f\"No results.json found in any sub-folder of {results_dir}\")\n\n    # ------------------------------------------------------------------\n    # Experiment Description (stdout BEFORE numerical data)\n    # ------------------------------------------------------------------\n    description = (\n        \"Evaluation Summary for experiment batch located at '%s'. \"\n        \"Each bar in the forthcoming figures corresponds to one run variation \"\n        \"defined in the YAML configuration.  Primary metrics compared are the \"\n        \"final validation accuracy and the total wall-clock time per run.  \"\n        \"OHGW-enabled runs can be visually distinguished via legend labels.\" % results_dir\n    )\n    print(description)\n\n    # ------------------------------------------------------------------\n    # Numerical data (stdout)\n    # ------------------------------------------------------------------\n    consolidated = {r[\"run_id\"]: {\n        \"final_val_acc\": r[\"final_val_acc\"],\n        \"total_time_sec\": r[\"total_time_sec\"],\n        \"use_ohgw\": r[\"use_ohgw\"],\n    } for r in results}\n    print(json.dumps(consolidated, indent=2))\n\n    # ------------------------------------------------------------------\n    # Figures\n    # ------------------------------------------------------------------\n    fig_names: List[str] = []\n    fig_names.append(bar_plot_final_accuracy(results, results_dir))\n    fig_names.append(bar_plot_time(results, results_dir))\n\n    # List figure files for transparency\n    print(json.dumps({\"generated_figures\": fig_names}))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "#!/usr/bin/env python\n\"\"\"\npreprocess.py – Common data pipeline with dataset placeholders.\nThe *only* location in the COMMON CORE FOUNDATION where dataset-specific logic\nis allowed to be replaced downstream.  The public API surfaces a single\nfunction get_dataloaders(config) => (train_loader, val_loader, num_classes).\n\nIf the config specifies the special placeholder name \"DATASET_PLACEHOLDER\",\nthis module fabricates a small synthetic classification dataset using\n`torchvision.datasets.FakeData` so that smoke tests can run without external\ndependencies or large downloads.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom typing import Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import FakeData\n\n# --------------------------------------------------------------------------------------\n# Constants\n# --------------------------------------------------------------------------------------\n_DEFAULT_BATCH_SIZE = 32\n_DEFAULT_NUM_WORKERS = 2\n\n# --------------------------------------------------------------------------------------\n# Public API\n# --------------------------------------------------------------------------------------\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader, int]:\n    \"\"\"Return train/val dataloaders + number of classes.\n\n    The function checks `config[\"dataset\"][\"name\"]`.  If it equals\n    \"DATASET_PLACEHOLDER\", a synthetic dataset is generated; otherwise the call is\n    delegated to `_get_dataset_specific_dataloaders`, which **will be replaced**\n    by real logic in the derived experiment stage.\n    \"\"\"\n    dataset_cfg = config.get(\"dataset\", {})\n    name = dataset_cfg.get(\"name\", \"DATASET_PLACEHOLDER\")\n\n    batch_size = int(config.get(\"training\", {}).get(\"batch_size\", _DEFAULT_BATCH_SIZE))\n    num_workers = int(config.get(\"training\", {}).get(\"num_workers\", _DEFAULT_NUM_WORKERS))\n\n    if name == \"DATASET_PLACEHOLDER\":\n        # --------------------------- synthetic CIFAR-10-like data --------------------\n        transform = T.Compose([\n            T.ToTensor(),\n        ])\n        fake_dataset = FakeData(size=1000, image_size=(3, 32, 32), num_classes=10,\n                                transform=transform)\n        train_len = int(0.8 * len(fake_dataset))\n        val_len = len(fake_dataset) - train_len\n        train_set, val_set = random_split(fake_dataset, [train_len, val_len])\n    else:\n        train_set, val_set = _get_dataset_specific_datasets(dataset_cfg)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n\n    # Derive number of classes\n    if hasattr(train_set, \"classes\"):\n        num_classes = len(train_set.classes)\n    else:\n        # Fallback: try target range from first sample\n        _, tgt = train_set[0]\n        if isinstance(tgt, int):\n            num_classes = len(set(t for _, t in train_set))\n        else:\n            raise ValueError(\"Cannot infer number of classes from dataset placeholder\")\n\n    return train_loader, val_loader, num_classes\n\n# --------------------------------------------------------------------------------------\n# Private helper that *will* be replaced in subsequent step\n# --------------------------------------------------------------------------------------\n\ndef _get_dataset_specific_datasets(dataset_cfg):  # noqa: D401, pylint: disable=missing-docstring\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic in derived step\n    raise NotImplementedError(\n        \"Dataset '%s' not implemented in common foundation. \"\n        \"Replace this placeholder with real dataset loader in derived experiments.\" % dataset_cfg.get(\"name\", \"<unknown>\"))\n",
    "model_py": "#!/usr/bin/env python\n\"\"\"\nmodel.py – Common model definitions.  This file already includes two generic\nmodels (a small CNN for image data and a simple MLP) so that smoke tests run\nwithout modification.  Real experiments will *extend* this module with their\nspecific architectures by adding additional entries to MODEL_REGISTRY.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# --------------------------------------------------------------------------------------\n# Model definitions\n# --------------------------------------------------------------------------------------\n\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Minimal CNN resembling a tiny ResNet-like structure (for CIFAR-size inputs).\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):  # noqa: D401\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\nclass SimpleMLP(nn.Module):\n    \"\"\"Shallow MLP for tabular / flattened inputs.\"\"\"\n\n    def __init__(self, input_dim: int = 784, num_classes: int = 10):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x):  # noqa: D401\n        if x.ndim > 2:\n            x = torch.flatten(x, 1)\n        return self.model(x)\n\n# --------------------------------------------------------------------------------------\n# Model registry & factory\n# --------------------------------------------------------------------------------------\n\nMODEL_REGISTRY = {\n    \"MODEL_PLACEHOLDER\": SimpleCNN,  # default mapping until replaced\n    \"simple_cnn\": SimpleCNN,\n    \"simple_mlp\": SimpleMLP,\n}\n\n\ndef get_model(config: Dict, *, num_classes: int):\n    model_cfg = config.get(\"model\", {})\n    name = model_cfg.get(\"name\", \"MODEL_PLACEHOLDER\")\n    if name not in MODEL_REGISTRY:\n        raise KeyError(f\"Model '{name}' not found in registry. Available: {list(MODEL_REGISTRY)}\")\n\n    if name == \"simple_mlp\":\n        input_dim = model_cfg.get(\"input_dim\", 784)\n        return MODEL_REGISTRY[name](input_dim=input_dim, num_classes=num_classes)\n    else:\n        return MODEL_REGISTRY[name](num_classes=num_classes)\n",
    "main_py": "#!/usr/bin/env python\n\"\"\"\nmain.py – Orchestrator that reads a YAML file listing all run variations and\nlaunches each one sequentially via `python -m src.train`.  After all runs have\ncompleted it triggers `python -m src.evaluate`.\n\nKey responsibilities:\n  • CLI interface supporting --smoke-test  *or*  --full-experiment\n  • Subprocess management with tee-style logging\n  • Creation of per-run result folders under --results-dir\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\nTHIS_DIR = Path(__file__).resolve().parent\nCONFIG_DIR = THIS_DIR.parent / \"config\"\nSRC_ROOT = THIS_DIR.parent\nPYTHON_EXE = sys.executable  # current interpreter\n\n# --------------------------------------------------------------------------------------\n# CLI & config helpers\n# --------------------------------------------------------------------------------------\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"OHGW experiment orchestrator\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\",\n                       help=\"Run the lightweight smoke-test configuration.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\",\n                       help=\"Run the full experiment configuration.\")\n    parser.add_argument(\"--results-dir\", required=True, type=str,\n                        help=\"Directory where ALL outputs (figures, logs, metrics) are stored.\")\n    return parser.parse_args()\n\n\ndef load_config(smoke: bool) -> Dict:\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if smoke else \"full_experiment.yaml\")\n    with open(cfg_file, \"r\") as fp:\n        return yaml.safe_load(fp)\n\n\n# --------------------------------------------------------------------------------------\n# Subprocess with tee logging\n# --------------------------------------------------------------------------------------\n\ndef run_subprocess(cmd: List[str], log_dir: Path, run_id: str):\n    \"\"\"Run `cmd` while writing stdout & stderr to files and the console.\"\"\"\n    stdout_file = log_dir / \"stdout.log\"\n    stderr_file = log_dir / \"stderr.log\"\n\n    with open(stdout_file, \"w\") as stdout_f, open(stderr_file, \"w\") as stderr_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                                   text=True, bufsize=1)\n\n        # Non-blocking stream reading line by line\n        def _forward(stream, stream_file, is_stdout: bool):\n            for line in stream:\n                stream_file.write(line)\n                stream_file.flush()\n                (sys.stdout if is_stdout else sys.stderr).write(line)\n\n        import threading  # local import to avoid polluting global namespace\n        t_out = threading.Thread(target=_forward, args=(process.stdout, stdout_f, True))\n        t_err = threading.Thread(target=_forward, args=(process.stderr, stderr_f, False))\n        t_out.start(); t_err.start()\n        t_out.join(); t_err.join()\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n# --------------------------------------------------------------------------------------\n# Main orchestration\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    args = parse_args()\n    smoke = args.smoke_test\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Load YAML with list of experiments\n    # ------------------------------------------------------------------\n    exp_cfg = load_config(smoke)\n    experiments: List[Dict] = exp_cfg.get(\"experiments\", [])\n\n    if not experiments:\n        raise RuntimeError(\"No experiments found in configuration file.\")\n\n    # ------------------------------------------------------------------\n    # Sequential execution\n    # ------------------------------------------------------------------\n    start_all = time.time()\n    for run_cfg in experiments:\n        run_id = run_cfg[\"run_id\"]\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Dump this single run_cfg to temp YAML so that train.py can read it\n        with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".yaml\") as tmp:\n            yaml.safe_dump(run_cfg, tmp)\n            tmp_filepath = Path(tmp.name)\n\n        # Build command\n        cmd = [\n            PYTHON_EXE, \"-m\", \"src.train\",\n            \"--config-file\", str(tmp_filepath),\n            \"--results-dir\", str(results_dir),\n        ]\n\n        print(f\"\\n===== LAUNCHING {run_id} =====\\n\")\n        run_subprocess(cmd, run_dir, run_id)\n        print(f\"===== FINISHED {run_id} =====\\n\")\n\n        # Clean temp file\n        tmp_filepath.unlink(missing_ok=True)\n\n    total_time = time.time() - start_all\n    print(f\"All runs finished in {total_time/60.0:.1f} minutes.\")\n\n    # ------------------------------------------------------------------\n    # Evaluation (aggregates results across runs)\n    # ------------------------------------------------------------------\n    eval_cmd = [PYTHON_EXE, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, results_dir, run_id=\"evaluation\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"ohgw-foundation\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\nnumpy = \"*\"\n\n[tool.setuptools]\npackages = [\"src\"]\n",
    "smoke_test_yaml": "# Lightweight configuration to validate the entire pipeline end-to-end.\n# Replaces heavy datasets / models with synthetic stand-ins so that CI can\n# execute the full workflow under <60s.\n\nexperiments:\n  - run_id: smoke_baseline\n    seed: 1\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: will be replaced with real dataset\n    model:\n      name: MODEL_PLACEHOLDER    # PLACEHOLDER: will be replaced with real model\n    optimizer:\n      lr: 0.01\n      weight_decay: 0.0005\n    training:\n      epochs: 2\n      batch_size: 32\n      num_workers: 0\n      use_ohgw: false\n\n  - run_id: smoke_ohgw\n    seed: 2\n    dataset:\n      name: DATASET_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n    optimizer:\n      lr: 0.01\n      weight_decay: 0.0005\n    training:\n      epochs: 2\n      batch_size: 32\n      num_workers: 0\n      use_ohgw: true\n",
    "full_experiment_yaml": "# Template for the *full* experiment suite.  Placeholders should be replaced in\n# the subsequent derivation step.  The overall structure, however, must remain\n# unchanged so that src/main.py can parse it without modification.\n\nexperiments:\n  - run_id: EXPERIMENT_1_PLACEHOLDER\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER  # Will be replaced with e.g. CIFAR10\n      # Additional dataset-specific fields such as root/path, augmentations …\n    model:\n      name: MODEL_PLACEHOLDER    # Will be replaced with e.g. resnet20\n      # Additional model-specific hyper-parameters (depth, width …)\n    optimizer:\n      lr: 0.1\n      weight_decay: 0.0005\n    training:\n      epochs: 200\n      batch_size: 128\n      num_workers: 8\n      use_ohgw: false\n\n  - run_id: EXPERIMENT_2_PLACEHOLDER\n    seed: 456\n    dataset:\n      name: DATASET_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n    optimizer:\n      lr: 0.1\n      weight_decay: 0.0005\n    training:\n      epochs: 200\n      batch_size: 128\n      num_workers: 8\n      use_ohgw: true\n\n  # Additional experiments (ablation, baseline, variants) follow the same schema\n\n# NOTE: The top-level key MUST remain `experiments` so that main.py can parse it.\n#       All downstream scripts expect each list item to have at least `run_id`,\n#       `dataset`, `model`, `optimizer`, `training`.\n\n# PLACEHOLDER: Add more experiment variations here.\n\n\n# End of file\n"
}
