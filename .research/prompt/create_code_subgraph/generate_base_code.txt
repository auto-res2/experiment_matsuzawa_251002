
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments with one GPU per run variation. If GPUs are insufficient, experiments run sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations with 1 GPU per variation)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment in parallel across available GPUs, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. GPU Allocation: main.py assigns 1 GPU per run variation. If available GPUs < total variations, queue remaining runs sequentially.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: Criterion 1 violated – train.py never saves the trained model (e.g. via torch.save) and no loading utility exists, so the mandatory “model saving/loading mechanisms” are missing. All other core logic is present, but without persistence the base foundation is incomplete.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nTrain a single experimental variation.\nThis script is launched as a subprocess by src.main.  It must:\n1.  Load the experiment-specific YAML file passed via --config.\n2.  Build dataloaders (via src.preprocess), model (via src.model) and training infrastructure.\n3.  Run the full training loop, collecting epoch-wise metrics.\n4.  Persist artefacts (model checkpoint, raw metrics, figures) in --results-dir.\n5.  Print – to **stdout** – a human-readable experiment description followed by\n    JSON-formatted numeric results (one line!) so the orchestrator can parse it.\n\nCore training logic is COMPLETE.  Only the dataset/model creation depend on\nplaceholders handled in src.preprocess & src.model.\n\"\"\"\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports ---------------------------------------------------------------\nfrom src import preprocess as pp\nfrom src import model as mdl\n\n# -----------------------------------------------------------------------------\n# Helper utils\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    if seed is None:\n        return\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# -----------------------------------------------------------------------------\n# Main training routine\n# -----------------------------------------------------------------------------\n\ndef train_one_epoch(model: nn.Module, dataloader: DataLoader, criterion, optimizer, device):\n    model.train()\n    epoch_loss = 0.0\n    for batch in dataloader:\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * inputs.size(0)\n    return epoch_loss / len(dataloader.dataset)\n\n\ndef evaluate(model: nn.Module, dataloader: DataLoader, criterion, device):\n    model.eval()\n    loss = 0.0\n    correct = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            inputs, targets = batch\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            outputs = model(inputs)\n            batch_loss = criterion(outputs, targets)\n            loss += batch_loss.item() * inputs.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == targets).sum().item()\n    loss /= len(dataloader.dataset)\n    accuracy = correct / len(dataloader.dataset)\n    return loss, accuracy\n\n\n# -----------------------------------------------------------------------------\n# Plotting helpers\n# -----------------------------------------------------------------------------\n\ndef plot_curves(metrics: dict, run_id: str, out_dir: Path):\n    # Training loss -----------------------------------------------------------\n    plt.figure(figsize=(6, 4))\n    epochs = np.arange(1, len(metrics[\"train_loss\"]) + 1)\n    plt.plot(epochs, metrics[\"train_loss\"], label=\"Train Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    plt.legend()\n    # annotate final value\n    final_val = metrics[\"train_loss\"][-1]\n    plt.annotate(f\"{final_val:.4f}\", xy=(epochs[-1], final_val), xytext=(epochs[-1], final_val))\n    fname = out_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n    # Validation accuracy -----------------------------------------------------\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, metrics[\"val_accuracy\"], label=\"Val Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(f\"Validation Accuracy – {run_id}\")\n    plt.legend()\n    final_acc = metrics[\"val_accuracy\"][-1]\n    plt.annotate(f\"{final_acc:.4f}\", xy=(epochs[-1], final_acc), xytext=(epochs[-1], final_acc))\n    fname = out_dir / f\"accuracy_{run_id}.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Train a single experimental variation\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store all outputs for this run.\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Entrypoint\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------------------------------------------------------------\n    # Load config\n    # ---------------------------------------------------------------------\n    import yaml  # local import to avoid YAML requirement when not needed\n\n    with open(args.config, \"r\") as fh:\n        cfg = yaml.safe_load(fh)\n\n    run_id = cfg.get(\"run_id\", results_dir.name)\n    seed = cfg.get(\"seed\", 0)\n    set_seed(seed)\n\n    # ---------------------------------------------------------------------\n    # Device assignment – user controls visible GPU via CUDA_VISIBLE_DEVICES\n    # ---------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # Data\n    # ---------------------------------------------------------------------\n    dl_train, dl_val, dl_test = pp.get_dataloaders(cfg)\n\n    # ---------------------------------------------------------------------\n    # Model & optimisation\n    # ---------------------------------------------------------------------\n    model = mdl.get_model(cfg).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optim_cfg = cfg.get(\"optimizer\", {\"name\": \"Adam\", \"lr\": cfg.get(\"train\", {}).get(\"lr\", 1e-3)})\n    optimizer = torch.optim.Adam(model.parameters(), lr=optim_cfg[\"lr\"])\n\n    num_epochs = int(cfg.get(\"train\", {}).get(\"epochs\", 1))\n\n    # Containers for metrics\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_accuracy\": [],\n    }\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    start_time = time.time()\n    for epoch in range(1, num_epochs + 1):\n        ep_train_loss = train_one_epoch(model, dl_train, criterion, optimizer, device)\n        ep_val_loss, ep_val_acc = evaluate(model, dl_val, criterion, device)\n\n        history[\"train_loss\"].append(ep_train_loss)\n        history[\"val_loss\"].append(ep_val_loss)\n        history[\"val_accuracy\"].append(ep_val_acc)\n\n        # Minimal logging to console for progress visibility\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch,\n                    \"train_loss\": ep_train_loss,\n                    \"val_loss\": ep_val_loss,\n                    \"val_accuracy\": ep_val_acc,\n                }\n            )\n        )\n        sys.stdout.flush()\n\n    end_time = time.time()\n\n    # ---------------------------------------------------------------------\n    # Final evaluation on test set (if provided)\n    # ---------------------------------------------------------------------\n    test_loss, test_acc = (None, None)\n    if dl_test is not None:\n        test_loss, test_acc = evaluate(model, dl_test, criterion, device)\n\n    # ---------------------------------------------------------------------\n    # Save artefacts\n    # ---------------------------------------------------------------------\n    ckpt_path = results_dir / \"model.pth\"\n    mdl.save_model(model, ckpt_path)\n\n    # Store raw metrics -------------------------------------------------------\n    results = {\n        \"run_id\": run_id,\n        \"seed\": seed,\n        \"num_epochs\": num_epochs,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"final_val_accuracy\": history[\"val_accuracy\"][-1],\n        \"best_val_accuracy\": max(history[\"val_accuracy\"]),\n        \"epoch_of_best_accuracy\": int(np.argmax(history[\"val_accuracy\"]) + 1),\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc,\n        \"training_time_seconds\": end_time - start_time,\n        **history,\n    }\n    with open(results_dir / \"results.json\", \"w\") as fh:\n        json.dump(results, fh, indent=2)\n\n    # Figures ---------------------------------------------------------------\n    plot_curves(history, run_id, results_dir)\n\n    # ---------------------------------------------------------------------\n    # Experiment description (human-readable) then machine-readable metrics\n    # ---------------------------------------------------------------------\n    description_lines = [\n        f\"Run ID            : {run_id}\",\n        f\"Dataset           : {cfg['dataset']['name']}\",\n        f\"Model             : {cfg['model']['name']}\",\n        f\"Epochs            : {num_epochs}\",\n        f\"Batch size        : {cfg.get('train', {}).get('batch_size', 'N/A')}\",\n        f\"Optimizer / lr    : Adam / {optim_cfg['lr']}\",\n        f\"Seed              : {seed}\",\n        f\"Device            : {device}\",\n    ]\n    print(\"\\nEXPERIMENT DESCRIPTION:\")\n    print(\"\\n\".join(description_lines))\n\n    print(\"\\nEXPERIMENTAL RESULTS:\")\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAggregates the results of all experiment variations inside a results directory,\ncomputes comparison statistics, and produces publication-ready figures.\nThe evaluation logic is **identical** for every experiment family; therefore\nALL future studies will share the exact same comparison pipeline.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Compare experimental runs in a results dir.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory that contains one sub-folder per run (created by main.py)\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef load_results(result_dir: Path):\n    res_file = result_dir / \"results.json\"\n    if not res_file.exists():\n        raise FileNotFoundError(f\"Missing results.json in {result_dir}\")\n    with open(res_file, \"r\") as fh:\n        return json.load(fh)\n\n\n# -----------------------------------------------------------------------------\n# Figure helpers – consistent naming convention enforced globally\n# -----------------------------------------------------------------------------\n\ndef bar_plot(df: pd.DataFrame, metric: str, out_path: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(x=\"run_id\", y=metric, data=df, palette=\"viridis\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.4f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha='center', va='bottom')\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.title(metric.replace(\"_\", \" \").title())\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef time_vs_accuracy(df: pd.DataFrame, out_path: Path):\n    plt.figure(figsize=(8, 4))\n    sns.scatterplot(\n        x=\"training_time_seconds\", y=\"best_val_accuracy\", hue=\"run_id\", data=df, s=100)\n    for _, row in df.iterrows():\n        plt.annotate(row[\"run_id\"], (row[\"training_time_seconds\"], row[\"best_val_accuracy\"]))\n    plt.xlabel(\"Training Time (s)\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.title(\"Efficiency Comparison\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# Entrypoint\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n    results_root = Path(args.results_dir)\n    if not results_root.exists():\n        raise FileNotFoundError(results_root)\n\n    # ---------------------------------------------------------------------\n    # Load all results\n    # ---------------------------------------------------------------------\n    records = []\n    for sub in results_root.iterdir():\n        if (sub / \"results.json\").exists():\n            records.append(load_results(sub))\n    if not records:\n        raise RuntimeError(f\"No result files found in {results_root}\")\n\n    df = pd.DataFrame(records)\n\n    # ---------------------------------------------------------------------\n    # Basic comparisons (can be extended later with statistical tests)\n    # ---------------------------------------------------------------------\n    summary = {\n        \"n_runs\": len(df),\n        \"best_run_by_accuracy\": df.loc[df[\"final_val_accuracy\"].idxmax()][\"run_id\"],\n    }\n\n    # ---------------------------------------------------------------------\n    # Figures\n    # ---------------------------------------------------------------------\n    bar_plot(df, \"final_val_accuracy\", results_root / \"accuracy_comparison.pdf\")\n    bar_plot(df, \"training_time_seconds\", results_root / \"training_time_comparison.pdf\")\n    time_vs_accuracy(df, results_root / \"accuracy_vs_time.pdf\")\n\n    # ---------------------------------------------------------------------\n    # Console output – description + JSON\n    # ---------------------------------------------------------------------\n    print(\"\\nEVALUATION SUMMARY:\")\n    print(f\"Total runs analysed : {summary['n_runs']}\")\n    print(f\"Best run (accuracy) : {summary['best_run_by_accuracy']}\")\n\n    print(\"\\nCOMPARISON RESULTS (JSON):\")\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()",
    "preprocess_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nCommon data-loading and preprocessing utilities shared by every experiment.\nSpecific domains/datasets will replace the placeholders marked clearly below.\nFor generic smoke-tests and fallback behaviour, a light-weight random dataset\nis provided so that the core pipeline runs end-to-end even without a real\ndataset.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n\n# -----------------------------------------------------------------------------\n# Dummy dataset – allows CI smoke tests to pass quickly on any machine.\n# -----------------------------------------------------------------------------\n\nclass DummyClassificationDataset(Dataset):\n    \"\"\"Synthetic classification dataset (images flattened to vectors).\"\"\"\n\n    def __init__(self, num_samples: int = 1024, input_size: int = 32, num_classes: int = 10):\n        self.num_samples = num_samples\n        self.input_size = input_size\n        self.num_classes = num_classes\n        self.x = torch.randn(num_samples, input_size * input_size)\n        self.y = torch.randint(0, num_classes, (num_samples,))\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# -----------------------------------------------------------------------------\n# PLACEHOLDER hooks – will be replaced with real dataset loaders.\n# -----------------------------------------------------------------------------\n\ndef _load_specific_dataset(cfg: dict) -> Dataset:\n    \"\"\"PLACEHOLDER: Will be replaced with specific dataset loading logic.\n    The function must return a torch.utils.data.Dataset object ready for\n    consumption by DataLoader.\n    \"\"\"\n    raise NotImplementedError(\n        \"Dataset '{}' not implemented. Replace _load_specific_dataset() in preprocess.py or \"\n        \"specify dataset 'DUMMY' for smoke tests.\".format(cfg[\"dataset\"][\"name\"])\n    )\n\n\n# -----------------------------------------------------------------------------\n# Public API – consumed by train.py.\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg: dict, val_split: float = 0.2) -> Tuple[DataLoader, DataLoader, Optional[DataLoader]]:\n    \"\"\"Return (train, val, test) dataloaders in a device-agnostic way.\"\"\"\n\n    batch_size = int(cfg.get(\"train\", {}).get(\"batch_size\", 32))\n    dataset_cfg = cfg[\"dataset\"]\n    name = dataset_cfg[\"name\"].upper()\n\n    if name == \"DUMMY\":\n        dataset = DummyClassificationDataset(\n            num_samples=int(dataset_cfg.get(\"num_samples\", 1024)),\n            input_size=int(dataset_cfg.get(\"input_size\", 32)),\n            num_classes=int(dataset_cfg.get(\"num_classes\", 10)),\n        )\n    else:\n        dataset = _load_specific_dataset(cfg)  # ← placeholder call\n\n    # ---------------------------------------------------------------------\n    # Split into train / val ( / test )\n    # ---------------------------------------------------------------------\n    val_size = int(len(dataset) * val_split)\n    test_size = int(len(dataset) * val_split) if cfg.get(\"create_test_split\", True) else 0\n    train_size = len(dataset) - val_size - test_size\n    splits = random_split(dataset, [train_size, val_size, test_size]) if test_size else random_split(dataset, [train_size, val_size])\n    dl_train = DataLoader(splits[0], batch_size=batch_size, shuffle=True)\n    dl_val = DataLoader(splits[1], batch_size=batch_size, shuffle=False)\n    dl_test = DataLoader(splits[2], batch_size=batch_size, shuffle=False) if test_size else None\n\n    return dl_train, dl_val, dl_test",
    "model_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nModel zoo shared by all experiments.  Fully functional baseline / dummy models\nare provided so that the common core works end-to-end; placeholders mark the\nspots that will be extended with domain-specific architectures later.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n\n# -----------------------------------------------------------------------------\n# Dummy model – small MLP for synthetic dataset & smoke tests.\n# -----------------------------------------------------------------------------\n\nclass DummyMLP(nn.Module):\n    def __init__(self, input_size: int = 32, hidden_size: int = 64, num_classes: int = 10):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        in_dim = input_size * input_size\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        return self.net(x)\n\n\n# -----------------------------------------------------------------------------\n# PLACEHOLDER model registry – to be populated with real architectures.\n# -----------------------------------------------------------------------------\n\n_MODEL_REGISTRY = {\n    \"DUMMY\": DummyMLP,\n    # PLACEHOLDER: Add specific model classes here, e.g. \"RESNET20\": ResNet20,\n}\n\n\n# -----------------------------------------------------------------------------\n# Public helpers\n# -----------------------------------------------------------------------------\n\ndef get_model(cfg: Dict) -> nn.Module:\n    \"\"\"Factory that returns an instantiated model based on cfg['model']['name']\"\"\"\n    model_cfg = cfg[\"model\"]\n    name = model_cfg[\"name\"].upper()\n    if name not in _MODEL_REGISTRY:\n        raise NotImplementedError(\n            f\"Model '{name}' not implemented. Extend _MODEL_REGISTRY in src/model.py.\"\n        )\n\n    if name == \"DUMMY\":\n        return _MODEL_REGISTRY[name](\n            input_size=int(cfg[\"dataset\"].get(\"input_size\", 32)),\n            hidden_size=int(model_cfg.get(\"hidden_size\", 64)),\n            num_classes=int(cfg[\"dataset\"].get(\"num_classes\", 10)),\n        )\n    else:\n        # PLACEHOLDER: Other models may consume extra kwargs from model_cfg\n        return _MODEL_REGISTRY[name](**model_cfg)\n\n\ndef save_model(model: nn.Module, path):\n    path = str(path)\n    torch.save(model.state_dict(), path)\n\n\ndef load_model(path, cfg, device=None):\n    device = device or (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    model = get_model(cfg)\n    state = torch.load(str(path), map_location=device)\n    model.load_state_dict(state)\n    model.to(device)\n    return model\n",
    "main_py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nOrchestrator: reads YAML config that defines multiple experimental variations,\nlaunches src.train (one GPU per variation), collects logs, and finally calls\nsrc.evaluate for a consistent comparison.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport queue\nimport shutil\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nimport yaml\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Run smoke or full experiment suite.\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run the lightweight smoke-test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full experiment suite defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where all experiment outputs go.\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Subprocess management helpers\n# -----------------------------------------------------------------------------\n\ndef _stream_process_output(proc: subprocess.Popen, log_path: Path):\n    \"\"\"Mirror subprocess stdout/stderr to main stdout and to a log file.\"\"\"\n    with log_path.open(\"w\") as fh:\n        for line in iter(proc.stdout.readline, b\"\"):  # type: ignore\n            decoded = line.decode(errors=\"replace\")\n            fh.write(decoded)\n            fh.flush()\n            print(decoded, end=\"\")\n    proc.stdout.close()\n\n\n# -----------------------------------------------------------------------------\n# GPU scheduling logic\n# -----------------------------------------------------------------------------\n\ndef get_available_gpus() -> List[int]:\n    return list(range(torch.cuda.device_count()))\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n\n    # ---------------------------------------------------------------------\n    # Config path resolution\n    # ---------------------------------------------------------------------\n    root_dir = Path(__file__).resolve().parent.parent\n    if args.smoke_test:\n        cfg_path = root_dir / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_path = root_dir / \"config\" / \"full_experiment.yaml\"\n\n    with open(cfg_path, \"r\") as fh:\n        cfg = yaml.safe_load(fh)\n\n    experiments = cfg.get(\"experiments\", [])\n    if not experiments:\n        print(\"No experiments defined in config.\")\n        sys.exit(1)\n\n    results_root = Path(args.results_dir).resolve()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    available_gpus = get_available_gpus()\n    if not available_gpus:\n        print(\"No GPU detected – running on CPU sequentially.\")\n        available_gpus = [-1]  # sentinel for CPU\n\n    gpu_queue = queue.Queue()\n    for gid in available_gpus:\n        gpu_queue.put(gid)\n\n    running_procs = []  # list[(proc, gpu_id)]\n\n    def launch_experiment(exp_cfg: Dict):\n        run_id = exp_cfg[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Write experiment-specific config so the train subprocess can read it.\n        exp_cfg_path = run_dir / \"config.yaml\"\n        with open(exp_cfg_path, \"w\") as fh:\n            yaml.safe_dump(exp_cfg, fh)\n\n        gpu_id = gpu_queue.get()  # blocks until a GPU becomes free\n        env = os.environ.copy()\n        if gpu_id >= 0:\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n        else:\n            env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(exp_cfg_path), \"--results-dir\", str(run_dir)]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env, bufsize=1)\n\n        # Tee stdout/stderr\n        log_path = run_dir / \"stdout.log\"\n        t = threading.Thread(target=_stream_process_output, args=(proc, log_path))\n        t.daemon = True\n        t.start()\n\n        running_procs.append((proc, gpu_id))\n\n    # Launch all experiments (respecting GPU availability)\n    launch_idx = 0\n    while launch_idx < len(experiments) or running_procs:\n        # Launch new ones if GPUs free\n        while launch_idx < len(experiments) and not gpu_queue.empty():\n            exp_cfg = experiments[launch_idx]\n            launch_experiment(exp_cfg)\n            launch_idx += 1\n        # Check running processes\n        time.sleep(5)\n        for proc, gid in running_procs[:]:  # copy to allow removal\n            if proc.poll() is not None:  # finished\n                running_procs.remove((proc, gid))\n                gpu_queue.put(gid)\n\n    # ---------------------------------------------------------------------\n    # All experiments finished – run evaluation\n    # ---------------------------------------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"common-core-ohgw\"\nversion = \"0.1.0\"\ndescription = \"Common experimental foundation for OHGW studies\"\nauthors = [\n    { name = \"Research Automation\", email = \"no-reply@example.com\" }\n]\nrequires-python = \">=3.9\"\nlicense = {text = \"MIT\"}\ndependencies = [\n    \"torch>=2.0\",\n    \"torchvision>=0.15\",\n    \"pyyaml>=6.0\",\n    \"matplotlib>=3.8\",\n    \"seaborn>=0.13\",\n    \"pandas>=2.0\",\n    \"tqdm>=4.66\",\n    \"psutil>=5.9\"\n]\n\n[project.scripts]\n# Optional entrypoints (not strictly required by the orchestrator)\n",
    "smoke_test_yaml": "# Lightweight configuration to verify the whole stack in <30 seconds.\n# Uses a synthetic dataset and tiny model so CI can run on CPU if needed.\n\nexperiments:\n  - run_id: dummy_baseline\n    seed: 0\n    dataset:\n      name: DUMMY\n      input_size: 32\n      num_classes: 2\n      num_samples: 256\n    model:\n      name: DUMMY\n      hidden_size: 64\n    train:\n      epochs: 2\n      batch_size: 32\n      lr: 0.01\n    optimizer:\n      name: Adam\n      lr: 0.01\n",
    "full_experiment_yaml": "# Template for the complete experimental campaign.\n# PLACEHOLDERS will be replaced in the next pipeline stage.\n\nexperiments:\n  - run_id: PLACEHOLDER_RUN_ID_1  # PLACEHOLDER: unique identifier per variation\n    seed: 0\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: e.g. CIFAR10, IMAGENET\n      path: DATASET_PATH_PLACEHOLDER\n      input_size: SPECIFIC_CONFIG_PLACEHOLDER\n      num_classes: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER  # PLACEHOLDER: e.g. RESNET20, VIT_TINY\n      depth: SPECIFIC_CONFIG_PLACEHOLDER\n    train:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n    optimizer:\n      name: Adam\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n  \n  # Add more experiments here following the same schema.\n  # Each will be launched on one GPU (sequentially queued if GPUs < experiments).\n\n"
}
