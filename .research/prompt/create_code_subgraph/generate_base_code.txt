
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments with one GPU per run variation. If GPUs are insufficient, experiments run sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations with 1 GPU per variation)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment in parallel across available GPUs, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. GPU Allocation: main.py assigns 1 GPU per run variation. If available GPUs < total variations, queue remaining runs sequentially.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: Core requirement violated: model persistence is missing. train.py neither checkpoints the network during training nor provides a load routine, so reproducibility / later re-evaluation cannot be guaranteed. All other foundation elements are present, but absence of save/load logic means criterion 1 (complete core logic) is not fully met.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Any, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n# ----------------------------- Utility Functions -------------------------------- #\n\ndef seed_everything(seed: int) -> None:\n    \"\"\"Seed all major RNGs for reproducibility.\"\"\"\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef save_checkpoint(state: Dict[str, Any], checkpoint_dir: Path, is_best: bool = False) -> None:\n    \"\"\"Save model/optimizer/epoch state dictionary.\"\"\"\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    filename = checkpoint_dir / \"model_last.pth\"\n    torch.save(state, filename)\n    if is_best:\n        best_filename = checkpoint_dir / \"model_best.pth\"\n        torch.save(state, best_filename)\n\n\ndef train_one_epoch(model: nn.Module,\n                    dataloader: DataLoader,\n                    criterion: nn.Module,\n                    optimizer: optim.Optimizer,\n                    device: torch.device) -> Tuple[float, float]:\n    \"\"\"Train for exactly one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    running_correct = 0\n    total_samples = 0\n\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        running_correct += torch.sum(preds == targets).item()\n        total_samples += inputs.size(0)\n\n    epoch_loss = running_loss / total_samples\n    epoch_acc = running_correct / total_samples\n    return epoch_loss, epoch_acc\n\n\ndef validate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> Tuple[float, float]:\n    model.eval()\n    running_loss = 0.0\n    running_correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            running_correct += torch.sum(preds == targets).item()\n            total_samples += inputs.size(0)\n\n    epoch_loss = running_loss / total_samples\n    epoch_acc = running_correct / total_samples\n    return epoch_loss, epoch_acc\n\n\n# ----------------------------- Main Training Script ------------------------------ #\n\ndef run_experiment(config: Dict[str, Any], results_dir: Path, resume: bool = False) -> None:\n    run_id = config[\"run_id\"]\n    seed = config.get(\"seed\", 42)\n    seed_everything(seed)\n\n    # GPU handling: rely on CUDA_VISIBLE_DEVICES set by main.py\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # -------------------------------------------------------------------------\n    # 1. Data\n    # -------------------------------------------------------------------------\n    train_loader, val_loader, num_classes, input_shape = get_dataloaders(config[\"dataset\"],\n                                                                        config[\"training\"],\n                                                                        smoke_test=config.get(\"smoke_test\", False))\n    # -------------------------------------------------------------------------\n    # 2. Model\n    # -------------------------------------------------------------------------\n    model = get_model(config[\"model\"], num_classes=num_classes, input_shape=input_shape)\n    model.to(device)\n\n    # -------------------------------------------------------------------------\n    # 3. Optimizer & Loss\n    # -------------------------------------------------------------------------\n    criterion = nn.CrossEntropyLoss()\n    optimizer_cfg = config[\"optimizer\"]\n    opt_name = optimizer_cfg.get(\"name\", \"SGD\").lower()\n    lr = optimizer_cfg.get(\"lr\", 0.01)\n    weight_decay = optimizer_cfg.get(\"weight_decay\", 0.0)\n    momentum = optimizer_cfg.get(\"momentum\", 0.9)\n\n    if opt_name == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif opt_name == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {opt_name}\")\n\n    # Scheduler placeholder – can be disabled if not provided\n    scheduler = None\n    if \"lr_scheduler\" in config:\n        sched_cfg = config[\"lr_scheduler\"]\n        step_size = sched_cfg.get(\"step_size\", 30)\n        gamma = sched_cfg.get(\"gamma\", 0.1)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # -------------------------------------------------------------------------\n    # 4. Checkpoint (resume if asked)\n    # -------------------------------------------------------------------------\n    checkpoint_dir = results_dir / run_id / \"checkpoints\"\n    start_epoch = 0\n    best_val_acc = 0.\n    if resume:\n        ckpt_file = checkpoint_dir / \"model_last.pth\"\n        if ckpt_file.exists():\n            state = torch.load(ckpt_file, map_location=device)\n            model.load_state_dict(state[\"state_dict\"])\n            optimizer.load_state_dict(state[\"optimizer\"])\n            if scheduler and \"scheduler\" in state:\n                scheduler.load_state_dict(state[\"scheduler\"])\n            start_epoch = state[\"epoch\"] + 1\n            best_val_acc = state.get(\"best_val_acc\", 0.)\n            print(f\"Resumed from epoch {start_epoch}\")\n\n    # -------------------------------------------------------------------------\n    # 5. Training Loop\n    # -------------------------------------------------------------------------\n    n_epochs = config[\"training\"].get(\"epochs\", 100)\n    log_every = max(1, n_epochs // 10)\n\n    # Prepare metrics storage\n    metrics_path = results_dir / run_id / \"metrics.jsonl\"\n    metrics_path.parent.mkdir(parents=True, exist_ok=True)\n    metrics_file = metrics_path.open(\"a\" if resume else \"w\", buffering=1)\n\n    for epoch in range(start_epoch, n_epochs):\n        epoch_start = time.time()\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        if scheduler is not None:\n            scheduler.step()\n\n        epoch_time = time.time() - epoch_start\n        log_dict = {\n            \"run_id\": run_id,\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"epoch_time\": epoch_time,\n        }\n        print(json.dumps(log_dict))\n        metrics_file.write(json.dumps(log_dict) + \"\\n\")\n\n        # Checkpoint handling\n        is_best = val_acc > best_val_acc\n        best_val_acc = max(best_val_acc, val_acc)\n        save_checkpoint({\n            \"epoch\": epoch,\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"scheduler\": scheduler.state_dict() if scheduler else None,\n            \"best_val_acc\": best_val_acc,\n            \"config\": config,\n        }, checkpoint_dir, is_best=is_best)\n\n    metrics_file.close()\n\n    # -------------------------------------------------------------------------\n    # 6. Final test evaluation if test split given\n    # -------------------------------------------------------------------------\n    test_metrics = {}\n    if config[\"dataset\"].get(\"has_test_split\", False):\n        _, _, _, test_loader = get_dataloaders(config[\"dataset\"],\n                                               config[\"training\"],\n                                               smoke_test=config.get(\"smoke_test\", False),\n                                               include_test=True)\n        test_loss, test_acc = validate(model, test_loader, criterion, device)\n        test_metrics = {\"test_loss\": test_loss, \"test_acc\": test_acc}\n\n    # -------------------------------------------------------------------------\n    # 7. Save final aggregated results\n    # -------------------------------------------------------------------------\n    final_results = {\n        \"run_id\": run_id,\n        \"best_val_acc\": best_val_acc,\n        **test_metrics,\n    }\n    results_file = results_dir / run_id / \"results.json\"\n    with results_file.open(\"w\") as fp:\n        json.dump(final_results, fp, indent=2)\n\n\n# --------------------------------------------------------------------------------#\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Base directory to write results into.\")\n    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from last checkpoint if available\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    import yaml  # Local import to ensure dependency only when training\n\n    with open(args.config, \"r\") as fp:\n        config = yaml.safe_load(fp)\n\n    run_experiment(config, Path(args.results_dir), resume=args.resume)\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nsns.set(style=\"whitegrid\")\n\n\nFIGURE_NAMING = {\n    \"training_loss\": \"training_loss.pdf\",\n    \"accuracy\": \"accuracy.pdf\",\n}\n\n\ndef load_results(results_dir: Path) -> pd.DataFrame:\n    \"\"\"Load metrics.jsonl from all experiment variations.\"\"\"\n    frames: List[pd.DataFrame] = []\n    for run_dir in results_dir.iterdir():\n        metrics_file = run_dir / \"metrics.jsonl\"\n        if metrics_file.exists():\n            df = pd.read_json(metrics_file, lines=True)\n            frames.append(df)\n    if not frames:\n        raise RuntimeError(\"No result files found in results directory.\")\n    return pd.concat(frames, ignore_index=True)\n\n\ndef plot_training_loss(df: pd.DataFrame, save_path: Path):\n    plt.figure(figsize=(8, 5))\n    sns.lineplot(data=df, x=\"epoch\", y=\"train_loss\", hue=\"run_id\")\n    for run_id, sub in df.groupby(\"run_id\"):\n        end_point = sub.sort_values(\"epoch\").iloc[-1]\n        plt.text(end_point[\"epoch\"], end_point[\"train_loss\"], f\"{end_point['train_loss']:.3f}\")\n    plt.title(\"Training Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(title=\"Run\")\n    plt.tight_layout()\n    plt.savefig(str(save_path / FIGURE_NAMING[\"training_loss\"]), bbox_inches=\"tight\", format=\"pdf\")\n    plt.close()\n\n\ndef plot_accuracy(df: pd.DataFrame, save_path: Path):\n    plt.figure(figsize=(8, 5))\n    sns.lineplot(data=df, x=\"epoch\", y=\"val_acc\", hue=\"run_id\")\n    for run_id, sub in df.groupby(\"run_id\"):\n        end_point = sub.sort_values(\"epoch\").iloc[-1]\n        plt.text(end_point[\"epoch\"], end_point[\"val_acc\"], f\"{end_point['val_acc']:.3f}\")\n    plt.title(\"Validation Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(title=\"Run\")\n    plt.tight_layout()\n    plt.savefig(str(save_path / FIGURE_NAMING[\"accuracy\"]), bbox_inches=\"tight\", format=\"pdf\")\n    plt.close()\n\n\ndef aggregate_final_metrics(results_dir: Path) -> Dict[str, Dict[str, float]]:\n    aggregated = {}\n    for run_dir in results_dir.iterdir():\n        results_file = run_dir / \"results.json\"\n        if results_file.exists():\n            with open(results_file) as fp:\n                res = json.load(fp)\n            aggregated[run_dir.name] = res\n    return aggregated\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Aggregate and visualize results.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    print(\"Generating figures and aggregated metrics …\")\n    df = load_results(results_dir)\n    plot_training_loss(df, results_dir)\n    plot_accuracy(df, results_dir)\n\n    aggregated = aggregate_final_metrics(results_dir)\n\n    # Print comparison results to stdout in JSON format\n    print(json.dumps({\"comparison\": aggregated}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()",
    "preprocess_py": "\"\"\"Common data preprocessing utilities with dataset placeholders.\n\nThis module centralises all dataset-related handling such that swapping\nfrom one dataset to the next can be done by *only* editing the dataset\nconfiguration dictionary, without touching any other training or\nevaluation code.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom pathlib import Path\nfrom typing import Tuple, Dict, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\n\n\n# --------------------------- Synthetic Placeholder ------------------------------ #\nclass _SyntheticClassificationDataset(Dataset):\n    \"\"\"Very small synthetic dataset for smoke testing.\n\n    Generates random images and random labels on the fly; deterministic\n    given the global PyTorch RNG state. Assumes image classification\n    with C classes.\n    \"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, int, int], num_classes: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.input_shape = input_shape  # (C, H, W)\n        self.num_classes = num_classes\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        img = torch.randn(*self.input_shape)\n        label = torch.randint(0, self.num_classes, (1,)).item()\n        return img, label\n\n\n# --------------------------- Dataset Factory ------------------------------------ #\n\ndef _build_transforms(training: bool, dataset_cfg: Dict[str, Any]):\n    \"\"\"Return torchvision Transform pipeline depending on training/val.\"\"\"\n    # PLACEHOLDER: Replace or augment with dataset-specific transforms\n    if training:\n        return T.Compose([\n            T.RandomHorizontalFlip(),\n            T.Normalize(mean=dataset_cfg.get(\"mean\", (0.5, 0.5, 0.5)),\n                        std=dataset_cfg.get(\"std\", (0.5, 0.5, 0.5)))\n        ])\n    else:\n        return T.Compose([\n            T.Normalize(mean=dataset_cfg.get(\"mean\", (0.5, 0.5, 0.5)),\n                        std=dataset_cfg.get(\"std\", (0.5, 0.5, 0.5)))\n        ])\n\n\ndef _load_real_dataset(dataset_cfg: Dict[str, Any], split: str):\n    \"\"\"PLACEHOLDER: Will be replaced with specific dataset loading logic.\"\"\"\n    raise NotImplementedError(\"Real dataset loading not yet implemented in common core foundation.\")\n\n\n# --------------------------- Public API ----------------------------------------- #\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any],\n                    training_cfg: Dict[str, Any],\n                    smoke_test: bool = False,\n                    include_test: bool = False):\n    \"\"\"Return train/val(/test) dataloaders given configs.\n\n    Parameters\n    ----------\n    dataset_cfg: Dict[str, Any]\n        Dataset-specific configuration (root, name, etc.)\n    training_cfg: Dict[str, Any]\n        Training hyperparameters such as batch_size, num_workers.\n    smoke_test: bool\n        If True, use a tiny synthetic dataset regardless of configuration.\n    include_test: bool\n        If True, also returns test_loader; else None.\n    \"\"\"\n    batch_size = training_cfg.get(\"batch_size\", 64)\n    num_workers = training_cfg.get(\"num_workers\", 4)\n\n    if smoke_test:\n        input_shape = (3, 32, 32)\n        num_classes = dataset_cfg.get(\"num_classes\", 10)\n        num_train = training_cfg.get(\"smoke_num_train\", 256)\n        num_val = training_cfg.get(\"smoke_num_val\", 64)\n        train_dataset = _SyntheticClassificationDataset(num_train, input_shape, num_classes)\n        val_dataset = _SyntheticClassificationDataset(num_val, input_shape, num_classes)\n        test_dataset = _SyntheticClassificationDataset(num_val, input_shape, num_classes)\n    else:\n        # Load real datasets. To be replaced in subsequent steps.\n        train_dataset = _load_real_dataset(dataset_cfg, split=\"train\")\n        val_dataset = _load_real_dataset(dataset_cfg, split=\"val\")\n        test_dataset = _load_real_dataset(dataset_cfg, split=\"test\") if include_test else None\n\n        # For real datasets we could inspect a sample to get shape/num_classes\n        sample, _ = train_dataset[0]\n        input_shape = tuple(sample.shape)\n        # naive heuristic to guess num classes\n        label_set = set()\n        for _, lbl in (train_dataset[i] for i in range(min(len(train_dataset), 1024))):\n            label_set.add(int(lbl))\n        num_classes = len(label_set)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = None\n    if include_test and test_dataset is not None:\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, num_classes, input_shape if not include_test else (train_loader, val_loader, num_classes, input_shape, test_loader)",
    "model_py": "\"\"\"Model definitions and factory utilities.\n\nContains baseline, proposed and ablation architectures. Only dataset- or\nexperiment-specific details (like number of classes) are passed in as\nconstructor arguments so that the common core remains unchanged across\nexperimental variations.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# ---------------------------- Baseline Model ------------------------------------ #\nclass _SimpleCNN(nn.Module):\n    \"\"\"A minimal CNN that works for most 32×32 image classification tasks.\"\"\"\n\n    def __init__(self, num_classes: int = 10, input_channels: int = 3):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 16×16\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 8×8\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.flatten(1)\n        return self.classifier(x)\n\n\n# ---------------------------- Placeholder Models -------------------------------- #\nclass MODEL_PLACEHOLDER(nn.Module):\n    \"\"\"PLACEHOLDER: Will be replaced with specific model architecture.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        raise NotImplementedError(\"Model placeholder – to be replaced in derived experiments.\")\n\n\n# ---------------------------- Model Factory ------------------------------------- #\nMODEL_REGISTRY = {\n    \"simple_cnn\": _SimpleCNN,\n    # PLACEHOLDER: register additional models here.\n}\n\n\ndef get_model(model_cfg: dict | str, *, num_classes: int, input_shape: tuple[int, ...]):\n    \"\"\"Instantiate a model by name or full config.\n\n    Parameters\n    ----------\n    model_cfg: dict | str\n        Either a simple string name (e.g., \"simple_cnn\") or a dictionary\n        with keys {\"name\": <name>, **kwargs}\n    num_classes: int\n        Number of output classes for classification.\n    input_shape: tuple[int, ...]\n        Input tensor shape so that certain models can infer required\n        layer dimensions.\n    \"\"\"\n    if isinstance(model_cfg, str):\n        model_name = model_cfg\n        kwargs = {}\n    else:\n        model_name = model_cfg.get(\"name\")\n        kwargs = {k: v for k, v in model_cfg.items() if k != \"name\"}\n\n    model_name = model_name.lower()\n    if model_name not in MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model '{model_name}'. Available: {list(MODEL_REGISTRY.keys())}\")\n\n    # Automatically inject num_classes / input_channels if not supplied\n    kwargs.setdefault(\"num_classes\", num_classes)\n    kwargs.setdefault(\"input_channels\", input_shape[0])\n\n    return MODEL_REGISTRY[model_name](**kwargs)",
    "main_py": "import argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nimport queue\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport yaml\nimport torch\n\n\n# ------------------------- Helper Utilities ------------------------------------- #\n\ndef discover_gpus() -> List[int]:\n    \"\"\"Return list of available GPU indices.\"\"\"\n    num = torch.cuda.device_count()\n    return list(range(num))\n\n\ndef tee_stream(src, dst_file, prefix: str = \"\"):\n    \"\"\"Forward stream lines both to stdout and a log file.\"\"\"\n    for line in iter(src.readline, b\"\"):\n        decoded = line.decode(\"utf-8\", errors=\"replace\")\n        sys.stdout.write(prefix + decoded)\n        dst_file.write(decoded)\n        dst_file.flush()\n    src.close()\n\n\n# ------------------------- Orchestrator ----------------------------------------- #\nclass ExperimentRunner:\n    def __init__(self, config_path: Path, results_dir: Path):\n        self.config_path = config_path\n        self.results_dir = results_dir\n        self.gpus = discover_gpus()\n        if not self.gpus:\n            raise RuntimeError(\"No GPUs detected; at least one GPU is required.\")\n\n        with open(config_path, \"r\") as fp:\n            self.cfg = yaml.safe_load(fp)\n        self.experiments: List[Dict] = self.cfg[\"experiments\"]\n\n    def run_all(self):\n        pending = self.experiments.copy()\n        running: Dict[subprocess.Popen, int] = {}\n        gpu_queue = queue.Queue()\n        for g in self.gpus:\n            gpu_queue.put(g)\n\n        def launch(experiment: Dict, gpu_idx: int):\n            run_id = experiment[\"run_id\"]\n            run_dir = self.results_dir / run_id\n            run_dir.mkdir(parents=True, exist_ok=True)\n            # dump individual experiment config\n            exp_cfg_path = run_dir / \"config.yaml\"\n            with open(exp_cfg_path, \"w\") as fp:\n                yaml.safe_dump(experiment, fp)\n\n            env = os.environ.copy()\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_idx)\n\n            stdout_file = open(run_dir / \"stdout.log\", \"w\", buffering=1)\n            stderr_file = open(run_dir / \"stderr.log\", \"w\", buffering=1)\n\n            process = subprocess.Popen([\n                sys.executable, \"-m\", \"src.train\",\n                \"--config\", str(exp_cfg_path),\n                \"--results-dir\", str(self.results_dir)\n            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, bufsize=1)\n\n            # Launch tee threads\n            threading.Thread(target=tee_stream, args=(process.stdout, stdout_file, f\"[{run_id}] \"), daemon=True).start()\n            threading.Thread(target=tee_stream, args=(process.stderr, stderr_file, f\"[{run_id}][ERR] \"), daemon=True).start()\n            running[process] = gpu_idx\n\n        # Main scheduling loop\n        while pending or running:\n            # Launch experiments if GPUs available\n            while not gpu_queue.empty() and pending:\n                gpu_idx = gpu_queue.get()\n                experiment = pending.pop(0)\n                launch(experiment, gpu_idx)\n            # Poll running processes\n            finished = []\n            for proc in list(running.keys()):\n                ret = proc.poll()\n                if ret is not None:\n                    gpu_idx = running.pop(proc)\n                    if ret != 0:\n                        print(f\"Run with GPU {gpu_idx} exited with code {ret}\")\n                    gpu_queue.put(gpu_idx)\n                    finished.append(proc)\n            if not pending and not running:\n                break\n        print(\"All experiment variations completed.\")\n\n        # Trigger evaluation\n        subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(self.results_dir)])\n\n\n# ------------------------- CLI --------------------------------------------------- #\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for OHGW experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Where to save figures, logs and metrics.\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    config_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    runner = ExperimentRunner(config_path, results_dir)\n    runner.run_all()\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for One-Shot Hyper-Gradient Warm-Starts experiments\"\nrequires-python = \">=3.10\"\n\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15.0\",\n    \"PyYAML>=6.0\",\n    \"matplotlib>=3.7.0\",\n    \"seaborn>=0.12.2\",\n    \"tqdm>=4.65.0\",\n    \"pandas>=2.0.0\",\n    \"scikit-learn>=1.3.0\"\n]\n\n[build-system]\nrequires = [\"setuptools>=63.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"",
    "smoke_test_yaml": "# Lightweight configuration used by CI smoke test.\n# Only synthetic data is used so the test finishes within seconds.\n\nexperiments:\n  - run_id: smoke_baseline\n    smoke_test: true\n    dataset:\n      name: synthetic_classification  # DATASET_PLACEHOLDER\n      num_classes: 10\n      has_test_split: false\n    model:\n      name: simple_cnn  # MODEL_PLACEHOLDER default\n    optimizer:\n      name: sgd\n      lr: 0.01\n      momentum: 0.9\n      weight_decay: 0.0\n    training:\n      epochs: 2\n      batch_size: 32\n      num_workers: 2",
    "full_experiment_yaml": "# Full experiment template. Placeholders will be replaced in next stage.\n\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER  # Will be filled with actual dataset, e.g., cifar10\n      root: DATA_PATH_PLACEHOLDER\n      num_classes: NUM_CLASSES_PLACEHOLDER\n      has_test_split: true\n    model:\n      name: MODEL_PLACEHOLDER  # e.g., resnet20\n      depth: MODEL_DEPTH_PLACEHOLDER\n    optimizer:\n      name: OPTIMIZER_PLACEHOLDER  # e.g., sgd\n      lr: LR_PLACEHOLDER\n      momentum: MOMENTUM_PLACEHOLDER\n      weight_decay: WD_PLACEHOLDER\n    training:\n      epochs: EPOCHS_PLACEHOLDER\n      batch_size: BATCH_SIZE_PLACEHOLDER\n      num_workers: 8\n    lr_scheduler:\n      step_size: 60\n      gamma: 0.2\n  # Additional experiments can be added below following the same schema.\n  # {"
}
