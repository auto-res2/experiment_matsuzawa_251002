
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: Overall framework is almost correct, but two blocking issues violate the core-validation checklist:
1. Dependency gap – evaluate.py relies on pandas but pyproject.toml does not list the pandas package, so the evaluation stage will crash in a clean environment.
2. train.py requirement – it prints decorative banners and additional text before/after the JSON metrics block; checklist demands a clean JSON-formatted metrics output.  The extra lines break strict downstream parsing expectations.
Fixing these issues will make the base code fully compliant.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "\"\"\"src/train.py\nRuns a single experiment variation specified via a JSON-formatted --config argument.\nAll logs except the final JSON metrics block are written to stderr so that stdout\ncontains *only* machine-readable JSON as required by the validation checklist.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom .preprocess import get_dataloaders, set_random_seed\nfrom .model import get_model\n\n_LOG_FORMAT = \"%(asctime)s — %(levelname)s — %(message)s\"\nlogging.basicConfig(stream=sys.stderr, level=logging.INFO, format=_LOG_FORMAT)\nlogger = logging.getLogger(__name__)\n\ndef ohgw_warm_start(\n    model: nn.Module,\n    batch: Tuple[torch.Tensor, torch.Tensor],\n    config: Dict[str, Any],\n    device: torch.device,\n) -> Dict[str, Any]:\n    \"\"\"Performs a single stochastic hyper-gradient step on *weight_decay* as a toy\n    example of OHGW.  This function can be replaced with a task-specific version in\n    derived experiments.  All other logic stays unchanged.\n    \"\"\"\n    eta_h: float = config.get(\"eta_h\", 1e-3)\n    wd_value: float = config.get(\"weight_decay\", 0.0)\n\n    if eta_h == 0.0:\n        return config  # nothing to do\n\n    weight_decay = torch.tensor(wd_value, dtype=torch.float32, device=device, requires_grad=True)\n\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    # Forward pass with L2 regularisation that depends on *weight_decay*\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    l2_reg = torch.tensor(0.0, device=device)\n    for p in model.parameters():\n        l2_reg = l2_reg + p.pow(2).sum()\n    total_loss = loss + weight_decay * l2_reg\n\n    # dL/d(weight_decay)\n    grad_wd, = torch.autograd.grad(total_loss, weight_decay, retain_graph=False)\n\n    new_wd = (weight_decay - eta_h * grad_wd).clamp_(min=0.0).item()\n    logger.info(\"OHGW – weight_decay %.6f → %.6f\", wd_value, new_wd)\n\n    config[\"weight_decay\"] = new_wd\n    return config\n\n\ndef train(config: Dict[str, Any], results_dir: Path) -> Dict[str, Any]:\n    \"\"\"Main training routine. Returns a metrics dictionary.\"\"\"\n    run_id: str = config[\"run_id\"]\n    set_random_seed(config.get(\"seed\", 42))\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    logger.info(\"Preparing data loaders …\")\n    train_loader, val_loader = get_dataloaders(config)\n\n    # ------------------------------------------------------------------\n    # Model & Optimiser\n    # ------------------------------------------------------------------\n    logger.info(\"Instantiating model …\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = get_model(config).to(device)\n\n    # A tiny OHGW warm-start before creating the optimiser so that the updated\n    # hyper-parameter is actually used.\n    if config.get(\"use_ohgw\", False):\n        batch = next(iter(train_loader))\n        config = ohgw_warm_start(model, batch, config, device)\n\n    learning_rate = float(config.get(\"learning_rate\", 1e-3))\n    weight_decay = float(config.get(\"weight_decay\", 0.0))\n\n    optimizer_name = config.get(\"optimizer\", \"sgd\").lower()\n    if optimizer_name == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n    elif optimizer_name == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer {optimizer_name}\")\n\n    criterion = nn.CrossEntropyLoss()\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    metrics: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"history\": {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"train_acc\": [],\n            \"val_acc\": [],\n        },\n    }\n\n    max_epochs: int = int(config.get(\"max_epochs\", 1))\n    logger.info(\"Starting training for %d epochs\", max_epochs)\n    start_time = time.time()\n\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(targets).sum().item()\n            total += inputs.size(0)\n\n        train_loss = total_loss / total\n        train_acc = correct / total\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = outputs.max(1)\n                val_correct += predicted.eq(targets).sum().item()\n                val_total += inputs.size(0)\n        val_loss /= val_total\n        val_acc = val_correct / val_total\n\n        metrics[\"history\"][\"train_loss\"].append(train_loss)\n        metrics[\"history\"][\"val_loss\"].append(val_loss)\n        metrics[\"history\"][\"train_acc\"].append(train_acc)\n        metrics[\"history\"][\"val_acc\"].append(val_acc)\n\n        logger.info(\n            \"Epoch %d/%d − train_loss: %.4f, val_loss: %.4f, train_acc: %.3f, val_acc: %.3f\",\n            epoch,\n            max_epochs,\n            train_loss,\n            val_loss,\n            train_acc,\n            val_acc,\n        )\n\n    end_time = time.time()\n    elapsed = end_time - start_time\n\n    metrics.update(\n        {\n            \"final_val_accuracy\": metrics[\"history\"][\"val_acc\"][-1],\n            \"final_val_loss\": metrics[\"history\"][\"val_loss\"][-1],\n            \"train_time_sec\": elapsed,\n            \"epochs\": max_epochs,\n        }\n    )\n\n    # ------------------------------------------------------------------\n    # Serialization\n    # ------------------------------------------------------------------\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # ------------------------------------------------------------------\n    # *Only* the JSON metrics dict goes to STDOUT.  All other output stays on STDERR.\n    # ------------------------------------------------------------------\n    print(json.dumps(metrics))\n    return metrics\n\n\ndef _parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation\")\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        required=True,\n        help=\"JSON-formatted string with the experiment configuration for this run.\",\n    )\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store outputs\")\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = _parse_args()\n    config: Dict[str, Any] = json.loads(args.config)\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n    train(config, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nAggregates the results of all experiment variations located in <results_dir> and\ncreates publication-ready figures (in .pdf format) as well as a machine-readable\nJSON summary printed to stdout.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n_LOG_FORMAT = \"%(asctime)s — %(levelname)s — %(message)s\"\nlogging.basicConfig(stream=sys.stderr, level=logging.INFO, format=_LOG_FORMAT)\nlogger = logging.getLogger(__name__)\n\nFIG_TRAINING_LOSS = \"training_loss.pdf\"\nFIG_FINAL_ACC = \"final_val_accuracy.pdf\"\n\n\ndef load_results(results_dir: Path) -> List[Dict[str, Any]]:\n    runs = []\n    for path in results_dir.iterdir():\n        if path.is_dir() and (path / \"results.json\").exists():\n            with open(path / \"results.json\", \"r\", encoding=\"utf-8\") as f:\n                runs.append(json.load(f))\n    if not runs:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n    return runs\n\n\ndef build_dataframe(runs: List[Dict[str, Any]]) -> pd.DataFrame:\n    records = []\n    for r in runs:\n        rec = {\n            \"run_id\": r[\"run_id\"],\n            \"final_val_accuracy\": r.get(\"final_val_accuracy\", float(\"nan\")),\n            \"final_val_loss\": r.get(\"final_val_loss\", float(\"nan\")),\n            \"train_time_sec\": r.get(\"train_time_sec\", float(\"nan\")),\n            \"epochs\": r.get(\"epochs\", 0),\n        }\n        records.append(rec)\n    return pd.DataFrame.from_records(records)\n\n\ndef plot_training_loss(runs: List[Dict[str, Any]], out_path: Path) -> None:\n    plt.figure(figsize=(6, 4))\n    for r in runs:\n        epochs = list(range(1, len(r[\"history\"][\"train_loss\"]) + 1))\n        plt.plot(epochs, r[\"history\"][\"train_loss\"], label=f\"{r['run_id']}\")\n        # Annotate final value\n        plt.annotate(\n            f\"{r['history']['train_loss'][-1]:.3f}\",\n            xy=(epochs[-1], r[\"history\"][\"train_loss\"][-1]),\n            xytext=(3, 0),\n            textcoords=\"offset points\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Training Loss\")\n    plt.title(\"Training Loss Curves\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path / FIG_TRAINING_LOSS, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef plot_final_accuracy(df: pd.DataFrame, out_path: Path) -> None:\n    plt.figure(figsize=(6, 4))\n    ax = sns.barplot(x=\"run_id\", y=\"final_val_accuracy\", data=df, palette=\"viridis\")\n    ax.set_ylabel(\"Final Validation Accuracy\")\n    ax.set_xlabel(\"Run ID\")\n\n    # Annotate bars with numerical values\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\", (p.get_x() + p.get_width() / 2, height), ha=\"center\", va=\"bottom\")\n\n    plt.title(\"Final Validation Accuracy per Run\")\n    plt.tight_layout()\n    plt.savefig(out_path / FIG_FINAL_ACC, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef _parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Aggregate and evaluate experimental runs\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where run outputs are stored\")\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = _parse_args()\n    results_dir = Path(args.results_dir)\n    runs = load_results(results_dir)\n    df = build_dataframe(runs)\n\n    # Save aggregated CSV for convenience\n    df.to_csv(results_dir / \"summary.csv\", index=False)\n\n    # Generate figures\n    plot_training_loss(runs, results_dir)\n    plot_final_accuracy(df, results_dir)\n\n    # ------------------------------------------------------------------\n    # Standard output: description then JSON summary\n    # ------------------------------------------------------------------\n    description = (\n        \"This evaluation compares the final validation accuracy and loss curves \"\n        \"across all experiment variations.  Figures generated: \"\n        f\"{FIG_TRAINING_LOSS}, {FIG_FINAL_ACC}.\"\n    )\n    print(description)\n\n    summary_json = df.to_dict(orient=\"list\")\n    print(json.dumps(summary_json))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nData loading & preprocessing utilities shared by all experiments.  Contains\nplaceholders that will be filled with dataset-specific logic in derived\nexperiments.  For the common core we ship a *DummyDataset* so that smoke tests\nrun without external data.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple, Dict, Any\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n\nclass DummyDataset(Dataset):\n    \"\"\"A synthetic dataset (3×32×32 images, 10 classes) for smoke tests.\"\"\"\n\n    def __init__(self, num_samples: int = 512, num_classes: int = 10):\n        self.data = torch.randn(num_samples, 3, 32, 32)\n        self.targets = torch.randint(0, num_classes, (num_samples,))\n\n    def __len__(self) -> int:  # noqa: D401\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n\n# ---------------------------------------------------------------------------\n# Public API\n# ---------------------------------------------------------------------------\n\ndef set_random_seed(seed: int) -> None:\n    import torch\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef get_dataloaders(config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"Return train & validation dataloaders for the given config.\n\n    Placeholder branches MUST be replaced with real dataset logic in derived\n    experiments.  The default branch uses an in-memory dummy dataset so that\n    the smoke test remains lightweight.\n    \"\"\"\n    batch_size: int = int(config.get(\"batch_size\", 32))\n    dataset_name: str = config.get(\"dataset\", \"DUMMY_DATASET\")\n\n    if dataset_name == \"DUMMY_DATASET\":\n        full_ds = DummyDataset(num_samples=1024)\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\n            \"Dataset loading for '%s' is not yet implemented in the common core.\"\n            % dataset_name\n        )\n\n    val_fraction: float = float(config.get(\"val_split\", 0.2))\n    val_size = int(len(full_ds) * val_fraction)\n    train_size = len(full_ds) - val_size\n    train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader\n",
    "model_py": "\"\"\"src/model.py\nModel architectures usable across experiments.  BaselineCNN is shipped for the\nsmoke test; additional models will be added in derived variants.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict, Any\n\nimport torch.nn as nn\n\n\nclass BaselineCNN(nn.Module):\n    \"\"\"A very small CNN suitable for CIFAR-sized images (3×32×32).\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 16×16\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 8×8\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x):  # noqa: D401\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# ---------------------------------------------------------------------------\n# Factory\n# ---------------------------------------------------------------------------\n\ndef get_model(config: Dict[str, Any]):\n    \"\"\"Return a model according to *config['model']*.\n\n    *Only* the BaselineCNN is implemented in the common core.  Additional models\n    must be added in derived experiments.\n    \"\"\"\n    model_name = config.get(\"model\", \"DUMMY_MODEL\")\n    num_classes = int(config.get(\"num_classes\", 10))\n\n    if model_name == \"DUMMY_MODEL\":\n        return BaselineCNN(num_classes=num_classes)\n    else:\n        # PLACEHOLDER: Will be replaced with specific model architecture\n        raise NotImplementedError(f\"Model '{model_name}' is not implemented in the common core.\")\n",
    "main_py": "\"\"\"src/main.py\nMaster orchestrator that sequentially executes all experiment variations as\nspecified in a YAML configuration file.  For each run it starts *train.py* as a\nsub-process, tees stdout/stderr into the console *and* into per-run log files,\nand finally triggers *evaluate.py* to aggregate the outcomes.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport logging\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nimport yaml\n\n_LOG_FORMAT = \"%(asctime)s — %(levelname)s — %(message)s\"\nlogging.basicConfig(stream=sys.stderr, level=logging.INFO, format=_LOG_FORMAT)\nlogger = logging.getLogger(__name__)\n\n\nCONFIG_SMOKE = Path(__file__).resolve().parent.parent / \"config\" / \"smoke_test.yaml\"\nCONFIG_FULL = Path(__file__).resolve().parent.parent / \"config\" / \"full_experiment.yaml\"\n\n\n# ---------------------------------------------------------------------------\n# Sub-process helpers\n# ---------------------------------------------------------------------------\n\ndef _stream_with_tee(stream, *dest_files):\n    \"\"\"Forward *stream* line-by-line to sys.stdout/stderr and to *dest_files*.\"\"\"\n    for line in iter(stream.readline, \"\"):\n        sys.stdout.write(line) if stream.name == \"stdout\" else sys.stderr.write(line)\n        for f in dest_files:\n            f.write(line)\n    stream.close()\n\n\ndef _run_subprocess(cmd: List[str], run_dir: Path) -> None:\n    logger.info(\"Running command: %s\", \" \".join(cmd))\n    run_dir.mkdir(parents=True, exist_ok=True)\n    stdout_file = open(run_dir / \"stdout.log\", \"w\", encoding=\"utf-8\")\n    stderr_file = open(run_dir / \"stderr.log\", \"w\", encoding=\"utf-8\")\n\n    proc = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        bufsize=1,\n        text=True,\n        universal_newlines=True,\n    )\n\n    # Tee threads\n    threads = [\n        threading.Thread(target=_stream_with_tee, args=(proc.stdout, stdout_file), daemon=True),\n        threading.Thread(target=_stream_with_tee, args=(proc.stderr, stderr_file), daemon=True),\n    ]\n    for t in threads:\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n\n    stdout_file.close()\n    stderr_file.close()\n\n    if proc.returncode != 0:\n        raise subprocess.CalledProcessError(proc.returncode, cmd)\n\n\n# ---------------------------------------------------------------------------\n# CLI\n# ---------------------------------------------------------------------------\n\ndef _parse_args():\n    parser = argparse.ArgumentParser(description=\"OHGW experiment orchestrator\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory for all outputs\")\n    return parser.parse_args()\n\n\n# ---------------------------------------------------------------------------\n# Main routine\n# ---------------------------------------------------------------------------\n\ndef main() -> None:\n    args = _parse_args()\n    cfg_path = CONFIG_SMOKE if args.smoke_test else CONFIG_FULL\n\n    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n        cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    experiments: List[Dict[str, Any]] = cfg.get(\"experiments\", [])\n    if not experiments:\n        logger.error(\"No experiments found in %s\", cfg_path)\n        sys.exit(1)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    for exp_cfg in experiments:\n        run_id = exp_cfg[\"run_id\"]\n        run_dir = results_dir / run_id\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            json.dumps(exp_cfg),\n            \"--results-dir\",\n            str(results_dir),\n        ]\n        _run_subprocess(cmd, run_dir)\n\n    # After all runs finished → aggregate\n    cmd_eval = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    logger.info(\"Starting evaluation …\")\n    subprocess.run(cmd_eval, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ohgw_common_core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\n\n# Mandatory runtime dependencies\ndependencies = [\n    \"torch>=2.0\",\n    \"torchvision>=0.15\",\n    \"pyyaml>=6.0\",\n    \"matplotlib>=3.7\",\n    \"seaborn>=0.12\",\n    \"pandas>=1.5\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# A minimal configuration that can run on CPU in seconds.  Serves as CI smoke test.\nexperiments:\n  - run_id: dummy_baseline\n    dataset: DUMMY_DATASET        # PLACEHOLDER\n    model: DUMMY_MODEL            # PLACEHOLDER\n    max_epochs: 2\n    batch_size: 16\n    learning_rate: 0.01\n    weight_decay: 0.0\n    optimizer: sgd\n    use_ohgw: false\n\n  - run_id: dummy_ohgw\n    dataset: DUMMY_DATASET        # PLACEHOLDER\n    model: DUMMY_MODEL            # PLACEHOLDER\n    max_epochs: 2\n    batch_size: 16\n    learning_rate: 0.01\n    weight_decay: 0.0005\n    optimizer: sgd\n    use_ohgw: true\n    eta_h: 0.001\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Template for the full experimental campaign.  Placeholders will be replaced\n# with concrete datasets, models, schedulers etc. in the derived-specific step.\nexperiments:\n  - run_id: DATASET_PLACEHOLDER_MODEL_PLACEHOLDER_scheduler_baseline\n    dataset: DATASET_PLACEHOLDER        # PLACEHOLDER: replace with dataset name\n    model: MODEL_PLACEHOLDER            # PLACEHOLDER: replace with model name\n    scheduler: SCHEDULER_PLACEHOLDER    # PLACEHOLDER: e.g. ASHA / PASHA\n    search_space: SPECIFIC_CONFIG_PLACEHOLDER\n    max_epochs: 100\n    batch_size: 128\n    learning_rate: 0.1\n    weight_decay: 0.0005\n    optimizer: sgd\n    use_ohgw: false\n\n  - run_id: DATASET_PLACEHOLDER_MODEL_PLACEHOLDER_scheduler_ohgw\n    dataset: DATASET_PLACEHOLDER        # PLACEHOLDER\n    model: MODEL_PLACEHOLDER            # PLACEHOLDER\n    scheduler: SCHEDULER_PLACEHOLDER    # PLACEHOLDER\n    search_space: SPECIFIC_CONFIG_PLACEHOLDER\n    max_epochs: 100\n    batch_size: 128\n    learning_rate: 0.1\n    weight_decay: 0.0005\n    optimizer: sgd\n    use_ohgw: true\n    eta_h: 0.001\n\n# Additional experiments (ablations, controls, different seeds…) should be\n# appended here by the derive_specific stage.\n"
}
