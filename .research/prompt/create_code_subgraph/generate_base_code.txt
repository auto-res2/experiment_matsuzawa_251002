
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: The base package already hard-codes a concrete dataset loader (CIFAR-10 branch inside src/preprocess.py).  According to the “No Premature Specialization” rule the base layer must **not** contain real dataset-specific loading logic – it should expose only placeholders that will be filled later.  Including real torchvision.CIFAR10 handling violates requirement 7 (and partially requirement 2), therefore the current foundation is not yet acceptable as the generic base code.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport random\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom src import preprocess as preprocess_module\nfrom src import model as model_module\n\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the per-run config.yaml file\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs for this run should be saved\")\n    return parser.parse_args()\n\n\ndef get_device(cfg: Dict[str, Any]):\n    if torch.cuda.is_available() and cfg.get(\"use_gpu\", True):\n        return torch.device(\"cuda\")\n    return torch.device(\"cpu\")\n\n\ndef get_loss_fn(task_type: str):\n    if task_type == \"classification\":\n        return nn.CrossEntropyLoss()\n    elif task_type == \"regression\":\n        return nn.MSELoss()\n    else:\n        raise ValueError(f\"Unknown task type: {task_type}\")\n\n\ndef ohgw_warm_start(model: nn.Module, optimizer: optim.Optimizer, data_iter, device, loss_fn, hyper_lr: float):\n    \"\"\"One-Shot Hyper-Gradient Warm-Start implementation (generic).\n\n    We compute a single mini-batch, derive gradients, and apply a hyper-gradient-style\n    update to selected hyper-parameters inside the optimizer param_groups.  This\n    generic version adjusts the learning rate and weight decay proportionally to the\n    gradient norm of the corresponding param group, which is a useful proxy in the\n    absence of explicit hyper-parameters as torch.Tensors.\n    \"\"\"\n    try:\n        batch = next(data_iter)\n    except StopIteration:\n        return  # empty iterator – nothing to do\n\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = loss_fn(outputs, targets)\n\n    # Compute gradients w.r.t. model parameters (standard backward pass).\n    loss.backward(create_graph=False)\n\n    # Cheap proxy for hyper-gradients: use the L2 norm of parameter gradients per group.\n    for group in optimizer.param_groups:\n        grad_norm = 0.0\n        for p in group[\"params\"]:\n            if p.grad is not None:\n                grad_norm += p.grad.norm().item()\n        grad_norm = grad_norm ** 0.5\n\n        # Adjust learning rate & weight_decay slightly in the direction of lower loss.\n        if \"lr\" in group:\n            old_lr = group[\"lr\"]\n            group[\"lr\"] = max(1e-8, old_lr - hyper_lr * grad_norm)\n        if \"weight_decay\" in group:\n            old_wd = group[\"weight_decay\"]\n            group[\"weight_decay\"] = max(0.0, old_wd - hyper_lr * grad_norm)\n\n    # No optimizer.step() for model parameters – only hyper-parameter adjustment.\n\n\ndef train(cfg: Dict[str, Any], results_dir: str):\n    run_id = cfg[\"run_id\"]\n    os.makedirs(results_dir, exist_ok=True)\n\n    # ==================== Reproducibility ==================== #\n    set_seed(cfg.get(\"seed\", 42))\n    device = get_device(cfg)\n\n    # ==================== Data ==================== #\n    dm = preprocess_module.get_datamodule(cfg[\"dataset\"])  # generic loader / placeholder\n    dm.setup()\n    train_loader = dm.train_dataloader()\n    val_loader = dm.val_dataloader()\n    test_loader = dm.test_dataloader()\n\n    # ==================== Model ==================== #\n    model = model_module.get_model(cfg[\"model\"], dm.data_spec())\n    model.to(device)\n\n    # ==================== Optimizer ==================== #\n    train_cfg = cfg[\"training\"]\n    optimizer_name = train_cfg.get(\"optimizer\", \"SGD\").upper()\n    if optimizer_name == \"ADAM\":\n        optimizer = optim.Adam(model.parameters(), lr=train_cfg[\"learning_rate\"], weight_decay=train_cfg.get(\"weight_decay\", 0.0))\n    else:  # default SGD\n        optimizer = optim.SGD(model.parameters(), lr=train_cfg[\"learning_rate\"], momentum=train_cfg.get(\"momentum\", 0.0), weight_decay=train_cfg.get(\"weight_decay\", 0.0))\n\n    # ==================== Criterion ==================== #\n    loss_fn = get_loss_fn(cfg[\"task_type\"])\n\n    # ==================== Warm-Start (OHGW) ==================== #\n    if cfg.get(\"enable_ohgw\", False):\n        hyper_lr = float(cfg.get(\"hyper_lr\", 1e-3))\n        ohgw_warm_start(model, optimizer, iter(train_loader), device, loss_fn, hyper_lr)\n\n    # ==================== Training Loop ==================== #\n    epochs = int(train_cfg[\"epochs\"])\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n\n    writer = SummaryWriter(log_dir=os.path.join(results_dir, \"tensorboard\"))\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        avg_train_loss = running_loss / len(train_loader.dataset)\n\n        # -------------------- Validation -------------------- #\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = loss_fn(outputs, targets)\n                val_loss += loss.item() * inputs.size(0)\n                if cfg[\"task_type\"] == \"classification\":\n                    preds = outputs.argmax(dim=1)\n                    correct += (preds == targets).sum().item()\n                    total += targets.size(0)\n                else:  # regression\n                    total += inputs.size(0)\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        if cfg[\"task_type\"] == \"classification\":\n            val_metric = correct / total\n        else:\n            val_metric = avg_val_loss  # For regression lower is better\n\n        # -------------------- Logging -------------------- #\n        history[\"train_loss\"].append(avg_train_loss)\n        history[\"val_loss\"].append(avg_val_loss)\n        history[\"val_metric\"].append(val_metric)\n\n        writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n        writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n        metric_name = \"Accuracy\" if cfg[\"task_type\"] == \"classification\" else \"ValMetric\"\n        writer.add_scalar(metric_name, val_metric, epoch)\n\n        print(json.dumps({\"run_id\": run_id, \"event\": \"epoch_end\", \"epoch\": epoch, \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, metric_name: val_metric}))\n\n    writer.flush()\n    writer.close()\n\n    # ==================== Test Evaluation ==================== #\n    model.eval()\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            test_loss += loss.item() * inputs.size(0)\n            if cfg[\"task_type\"] == \"classification\":\n                preds = outputs.argmax(dim=1)\n                test_correct += (preds == targets).sum().item()\n                test_total += targets.size(0)\n            else:\n                test_total += inputs.size(0)\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    if cfg[\"task_type\"] == \"classification\":\n        test_metric = test_correct / test_total\n    else:\n        test_metric = avg_test_loss\n\n    # ==================== Persist Results ==================== #\n    results = {\n        \"run_id\": run_id,\n        \"task_type\": cfg[\"task_type\"],\n        \"final_epoch\": epochs,\n        \"history\": history,\n        \"test_loss\": avg_test_loss,\n        \"test_metric\": test_metric,\n        \"timestamp\": datetime.utcnow().isoformat()\n    }\n\n    with open(os.path.join(results_dir, \"results.json\"), \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Final summary to stdout (captured by main for CI visibility)\n    print(json.dumps({\"run_id\": run_id, \"event\": \"completed\", \"test_metric\": test_metric}))\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    import yaml\n    with open(args.config, \"r\") as f:\n        config = yaml.safe_load(f)\n    train(config, args.results_dir)\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom typing import List, Dict, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport yaml\n\nFIG_STYLE = \"darkgrid\"\nplt.switch_backend(\"Agg\")  # Non-interactive backend for servers\nsns.set_style(FIG_STYLE)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Aggregate & visualize experiment results\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory that contains all run sub-folders\")\n    return parser.parse_args()\n\n\ndef load_all_results(results_dir: str) -> List[Dict[str, Any]]:\n    runs = []\n    for run_id in os.listdir(results_dir):\n        run_path = os.path.join(results_dir, run_id, \"results.json\")\n        if os.path.isfile(run_path):\n            with open(run_path, \"r\") as f:\n                runs.append(json.load(f))\n    return runs\n\n\ndef description_to_stdout(runs: List[Dict[str, Any]]):\n    exp_desc = {\n        \"num_runs\": len(runs),\n        \"run_ids\": [r[\"run_id\"] for r in runs],\n        \"task_type\": runs[0][\"task_type\"] if runs else \"unknown\",\n    }\n    print(\"# ==== Experiment Description ==== #\")\n    print(json.dumps(exp_desc, indent=2))\n\n\ndef plot_training_loss(runs: List[Dict[str, Any]], out_path: str):\n    plt.figure(figsize=(6, 4))\n    for r in runs:\n        plt.plot(r[\"history\"][\"train_loss\"], label=r[\"run_id\"])\n        # Annotate final value\n        plt.annotate(f\"{r['history']['train_loss'][-1]:.3f}\", (len(r[\"history\"][\"train_loss\"]) - 1, r[\"history\"][\"train_loss\"][-1]))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Training Loss\")\n    plt.title(\"Training Loss Curve\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef plot_val_metric(runs: List[Dict[str, Any]], out_path: str):\n    metric_name = \"Accuracy\" if runs[0][\"task_type\"] == \"classification\" else \"ValMetric\"\n    plt.figure(figsize=(6, 4))\n    for r in runs:\n        plt.plot(r[\"history\"][\"val_metric\"], label=r[\"run_id\"])\n        plt.annotate(f\"{r['history']['val_metric'][-1]:.3f}\", (len(r[\"history\"][\"val_metric\"]) - 1, r[\"history\"][\"val_metric\"][-1]))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(metric_name)\n    plt.title(f\"Validation {metric_name} Curve\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef generate_comparison_bar(runs: List[Dict[str, Any]], out_path: str):\n    metric = [r[\"test_metric\"] for r in runs]\n    names = [r[\"run_id\"] for r in runs]\n    plt.figure(figsize=(6, 4))\n    bars = plt.bar(names, metric)\n    for bar, val in zip(bars, metric):\n        plt.annotate(f\"{val:.3f}\", (bar.get_x() + bar.get_width() / 2, val), ha=\"center\", va=\"bottom\")\n    ylabel = \"Test Accuracy\" if runs[0][\"task_type\"] == \"classification\" else \"Test Metric\"\n    plt.ylabel(ylabel)\n    plt.title(f\"Final {ylabel} Comparison\")\n    plt.tight_layout()\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    runs = load_all_results(args.results_dir)\n    if not runs:\n        raise RuntimeError(f\"No results.json files found in {args.results_dir}\")\n\n    # -------------------- STDOUT Description -------------------- #\n    description_to_stdout(runs)\n\n    # -------------------- Plots -------------------- #\n    plot_training_loss(runs, os.path.join(args.results_dir, \"training_loss.pdf\"))\n    plot_val_metric(runs, os.path.join(args.results_dir, \"accuracy.pdf\" if runs[0][\"task_type\"] == \"classification\" else \"val_metric.pdf\"))\n    generate_comparison_bar(runs, os.path.join(args.results_dir, \"test_metric.pdf\"))\n\n    # -------------------- Summary JSON -------------------- #\n    summary = {r[\"run_id\"]: r[\"test_metric\"] for r in runs}\n    print(json.dumps({\"comparison\": summary}, indent=2))\n",
    "preprocess_py": "\"\"\"Common preprocessing pipeline with placeholders for dataset-specific logic.\n\nThe module exposes a `get_datamodule(dataset_cfg)` function that returns an object\nwith the standard API:\n  * setup()\n  * train_dataloader()\n  * val_dataloader()\n  * test_dataloader()\n  * data_spec() -> Dict (e.g. feature_dim, num_classes)\n\nOnly **placeholders** for real datasets are provided.  A small synthetic dataset is\nimplemented to make smoke tests runnable on any environment.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict, Tuple, Any\n\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_classification, make_regression\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nclass DataModuleBase:\n    \"\"\"Base class defining the expected interface for all data modules.\"\"\"\n\n    def __init__(self, cfg: Dict[str, Any]):\n        self.cfg = cfg\n\n    # ---------------- Required API ---------------- #\n    def setup(self):\n        raise NotImplementedError\n\n    def train_dataloader(self) -> DataLoader:\n        raise NotImplementedError\n\n    def val_dataloader(self) -> DataLoader:\n        raise NotImplementedError\n\n    def test_dataloader(self) -> DataLoader:\n        raise NotImplementedError\n\n    def data_spec(self) -> Dict[str, Any]:\n        \"\"\"Return meta information such as feature dimension or number of classes.\"\"\"\n        raise NotImplementedError\n\n\n# ============================================================================ #\n#                           Synthetic Dataset (Smoke)                          #\n# ============================================================================ #\nclass SyntheticClassificationData(DataModuleBase):\n    \"\"\"Small synthetic classification dataset for smoke tests.\"\"\"\n\n    def setup(self):\n        n_samples = int(self.cfg.get(\"num_samples\", 1000))\n        n_features = int(self.cfg.get(\"num_features\", 20))\n        n_classes = int(self.cfg.get(\"num_classes\", 2))\n        test_fraction = float(self.cfg.get(\"test_fraction\", 0.2))\n        val_fraction = float(self.cfg.get(\"val_fraction\", 0.2))\n        batch_size = int(self.cfg.get(\"batch_size\", 32))\n\n        X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=math.ceil(0.6 * n_features),\n                                   n_classes=n_classes, random_state=42)\n        X = X.astype(np.float32)\n        y = y.astype(np.int64)\n\n        # Split\n        idx = np.arange(n_samples)\n        np.random.shuffle(idx)\n        test_size = int(test_fraction * n_samples)\n        val_size = int(val_fraction * n_samples)\n        test_idx = idx[:test_size]\n        val_idx = idx[test_size:test_size + val_size]\n        train_idx = idx[test_size + val_size:]\n\n        self.train_ds = TensorDataset(torch.from_numpy(X[train_idx]), torch.from_numpy(y[train_idx]))\n        self.val_ds = TensorDataset(torch.from_numpy(X[val_idx]), torch.from_numpy(y[val_idx]))\n        self.test_ds = TensorDataset(torch.from_numpy(X[test_idx]), torch.from_numpy(y[test_idx]))\n\n        self._train_loader = DataLoader(self.train_ds, batch_size=batch_size, shuffle=True)\n        self._val_loader = DataLoader(self.val_ds, batch_size=batch_size, shuffle=False)\n        self._test_loader = DataLoader(self.test_ds, batch_size=batch_size, shuffle=False)\n\n        self._spec = {\"input_dim\": n_features, \"num_classes\": n_classes}\n\n    def train_dataloader(self):\n        return self._train_loader\n\n    def val_dataloader(self):\n        return self._val_loader\n\n    def test_dataloader(self):\n        return self._test_loader\n\n    def data_spec(self):\n        return self._spec\n\n\nclass SyntheticRegressionData(DataModuleBase):\n    \"\"\"Synthetic regression dataset (future extension).\"\"\"\n\n    def setup(self):\n        n_samples = int(self.cfg.get(\"num_samples\", 1000))\n        n_features = int(self.cfg.get(\"num_features\", 20))\n        test_fraction = float(self.cfg.get(\"test_fraction\", 0.2))\n        val_fraction = float(self.cfg.get(\"val_fraction\", 0.2))\n        batch_size = int(self.cfg.get(\"batch_size\", 32))\n\n        X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=42)\n        y = y.astype(np.float32)[:, None]\n        X = X.astype(np.float32)\n\n        # Split\n        idx = np.arange(n_samples)\n        np.random.shuffle(idx)\n        test_size = int(test_fraction * n_samples)\n        val_size = int(val_fraction * n_samples)\n        test_idx = idx[:test_size]\n        val_idx = idx[test_size:test_size + val_size]\n        train_idx = idx[test_size + val_size:]\n\n        self.train_ds = TensorDataset(torch.from_numpy(X[train_idx]), torch.from_numpy(y[train_idx]))\n        self.val_ds = TensorDataset(torch.from_numpy(X[val_idx]), torch.from_numpy(y[val_idx]))\n        self.test_ds = TensorDataset(torch.from_numpy(X[test_idx]), torch.from_numpy(y[test_idx]))\n\n        self._train_loader = DataLoader(self.train_ds, batch_size=batch_size, shuffle=True)\n        self._val_loader = DataLoader(self.val_ds, batch_size=batch_size, shuffle=False)\n        self._test_loader = DataLoader(self.test_ds, batch_size=batch_size, shuffle=False)\n\n        self._spec = {\"input_dim\": n_features, \"num_targets\": 1}\n\n    def train_dataloader(self):\n        return self._train_loader\n\n    def val_dataloader(self):\n        return self._val_loader\n\n    def test_dataloader(self):\n        return self._test_loader\n\n    def data_spec(self):\n        return self._spec\n\n\n# ============================================================================ #\n#                            Placeholder Template                              #\n# ============================================================================ #\nclass PlaceholderDataModule(DataModuleBase):\n    \"\"\"PLACEHOLDER: To be replaced with dataset-specific logic in later steps.\"\"\"\n\n    def setup(self):\n        raise NotImplementedError(\"PLACEHOLDER: Will be replaced with specific dataset loading logic in derived experiments.\")\n\n    def train_dataloader(self):\n        raise NotImplementedError\n\n    def val_dataloader(self):\n        raise NotImplementedError\n\n    def test_dataloader(self):\n        raise NotImplementedError\n\n    def data_spec(self):\n        raise NotImplementedError\n\n\n# ============================================================================ #\n#                               Registry Logic                                 #\n# ============================================================================ #\n_REGISTRY = {\n    \"synthetic_classification\": SyntheticClassificationData,\n    \"synthetic_regression\": SyntheticRegressionData,\n    # PLACEHOLDER: Real datasets such as CIFAR10, ImageNet etc. will be registered here in derived experiments.\n}\n\n\ndef get_datamodule(cfg: Dict[str, Any]) -> DataModuleBase:\n    if isinstance(cfg, str):\n        name = cfg\n        params = {}\n    else:\n        name = cfg.get(\"name\")\n        params = cfg\n    cls = _REGISTRY.get(name, PlaceholderDataModule)\n    dm = cls(params)\n    return dm\n",
    "model_py": "\"\"\"Model architecture implementations (generic base + placeholders).\n\nThis foundational layer only provides lightweight, domain-agnostic networks that\nare sufficient for smoke tests.  Real architectures (e.g., ResNet, Transformer)\nwill be inserted in the next phase by replacing placeholder classes or by\nregistering new names in the `MODEL_REGISTRY`.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict, Any\nimport torch\nimport torch.nn as nn\n\n\nclass BaseMLP(nn.Module):\n    \"\"\"Simple MLP for classification or regression (generic).\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass PlaceholderModel(nn.Module):\n    \"\"\"PLACEHOLDER: to be replaced with specific architecture later.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        raise NotImplementedError(\"PLACEHOLDER: Will be replaced with specific model architecture in derived experiments.\")\n\n\n# ---------------- Registry ---------------- #\nMODEL_REGISTRY = {\n    \"mlp_small\": BaseMLP,\n    # PLACEHOLDER: e.g. \"resnet20\": ResNet20,\n    \"MODEL_PLACEHOLDER\": PlaceholderModel,\n}\n\n\ndef get_model(model_cfg: Dict[str, Any], data_spec: Dict[str, Any]):\n    \"\"\"Instantiate a model based on config and data specification.\"\"\"\n    if isinstance(model_cfg, str):\n        name = model_cfg\n        params = {}\n    else:\n        name = model_cfg.get(\"name\")\n        params = {k: v for k, v in model_cfg.items() if k != \"name\"}\n\n    cls = MODEL_REGISTRY.get(name, PlaceholderModel)\n\n    # ---------------- Dimension Inference ---------------- #\n    if \"input_dim\" in data_spec:\n        params.setdefault(\"input_dim\", data_spec[\"input_dim\"])\n    if \"num_classes\" in data_spec:\n        params.setdefault(\"output_dim\", data_spec[\"num_classes\"])\n    if \"num_targets\" in data_spec:\n        params.setdefault(\"output_dim\", data_spec[\"num_targets\"])\n\n    # Reasonable default hidden dimension\n    params.setdefault(\"hidden_dim\", 64)\n\n    model = cls(**params)\n    return model\n",
    "main_py": "import argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nimport yaml\nfrom pathlib import Path\nfrom typing import List\n\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\n\n# -------------------------------- Utility: Tee stdout/stderr -------------------------------- #\nclass StreamTee(threading.Thread):\n    def __init__(self, src, targets: List):\n        super().__init__()\n        self.src = src\n        self.targets = targets\n        self.daemon = True\n\n    def run(self):\n        for line in iter(self.src.readline, b\"\"):\n            for t in self.targets:\n                t.write(line.decode())\n                t.flush()\n        self.src.close()\n\n\n# ------------------------------------------------------------------------------------------- #\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Experiment Orchestrator\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run the lightweight smoke_test.yaml config\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full_experiment.yaml config\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where all outputs should be saved\")\n    return parser.parse_args()\n\n\ndef load_config(smoke: bool):\n    cfg_path = PROJECT_ROOT / \"config\" / (\"smoke_test.yaml\" if smoke else \"full_experiment.yaml\")\n    with open(cfg_path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef run_subprocess(cmd: List[str], run_id: str, results_dir: Path):\n    log_dir = results_dir / run_id\n    log_dir.mkdir(parents=True, exist_ok=True)\n    stdout_log = open(log_dir / \"stdout.log\", \"wb\")\n    stderr_log = open(log_dir / \"stderr.log\", \"wb\")\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    tees = [\n        StreamTee(proc.stdout, [sys.stdout.buffer, stdout_log]),\n        StreamTee(proc.stderr, [sys.stderr.buffer, stderr_log]),\n    ]\n    for t in tees:\n        t.start()\n    proc.wait()\n    for t in tees:\n        t.join()\n\n    stdout_log.close()\n    stderr_log.close()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess for run_id={run_id} exited with non-zero status {proc.returncode}\")\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    if args.smoke_test and args.full_experiment:\n        raise ValueError(\"Choose at most one of --smoke-test or --full-experiment\")\n    if not args.smoke_test and not args.full_experiment:\n        raise ValueError(\"You must specify one of --smoke-test or --full-experiment\")\n\n    cfg_list = load_config(smoke=args.smoke_test)\n    results_dir = Path(args.results_dir).resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    for cfg in cfg_list:\n        run_id = cfg[\"run_id\"]\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Persist per-run config for traceability\n        config_yaml_path = run_dir / \"config.yaml\"\n        with open(config_yaml_path, \"w\") as f:\n            yaml.safe_dump(cfg, f)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(config_yaml_path),\n            \"--results-dir\",\n            str(run_dir),\n        ]\n        print(f\"\\n========== Launching run_id={run_id} ==========\")\n        run_subprocess(cmd, run_id, results_dir)\n        print(f\"========== Completed run_id={run_id} ==========\")\n\n    # -------------------- Evaluation & Aggregation -------------------- #\n    print(\"\\n========== Aggregating results ==========\")\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ]\n    subprocess.check_call(eval_cmd)\n",
    "pyproject_toml": "[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \"^2.0.0\"\nPyYAML = \"^6.0.0\"\nmatplotlib = \"^3.7.0\"\nseaborn = \"^0.12.2\"\nscikit-learn = \"^1.3.0\"\ntensorboard = \"^2.12.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "smoke_test_yaml": "# Lightweight smoke test configuration to validate the full pipeline without\n# requiring any real dataset or heavy model.\n# Two variations are run: baseline (no OHGW) and OHGW enabled.\n\n- run_id: baseline\n  task_type: classification\n  dataset:\n    name: synthetic_classification\n    num_samples: 200\n    num_features: 20\n    num_classes: 2\n    batch_size: 32\n  model:\n    name: mlp_small\n    hidden_dim: 32\n  training:\n    epochs: 3\n    batch_size: 32\n    learning_rate: 0.01\n    weight_decay: 0.0\n  enable_ohgw: false\n  seed: 1\n\n- run_id: ohgw\n  task_type: classification\n  dataset:\n    name: synthetic_classification\n    num_samples: 200\n    num_features: 20\n    num_classes: 2\n    batch_size: 32\n  model:\n    name: mlp_small\n    hidden_dim: 32\n  training:\n    epochs: 3\n    batch_size: 32\n    learning_rate: 0.01\n    weight_decay: 0.0\n  enable_ohgw: true\n  hyper_lr: 0.001\n  seed: 2\n",
    "full_experiment_yaml": "# Placeholder template for the full experiment configuration.\n# Real datasets, models and hyper-parameters will be filled in later.\n\n- run_id: BASELINE_PLACEHOLDER\n  task_type: TASK_TYPE_PLACEHOLDER   # e.g. classification, regression\n  dataset: DATASET_PLACEHOLDER       # PLACEHOLDER: to be replaced with dataset config\n  model: MODEL_PLACEHOLDER           # PLACEHOLDER: to be replaced with model config\n  training: SPECIFIC_CONFIG_PLACEHOLDER  # PLACEHOLDER: full training hyper-params\n  enable_ohgw: false\n  seed: 1\n\n- run_id: OHGW_PLACEHOLDER\n  task_type: TASK_TYPE_PLACEHOLDER\n  dataset: DATASET_PLACEHOLDER\n  model: MODEL_PLACEHOLDER\n  training: SPECIFIC_CONFIG_PLACEHOLDER\n  enable_ohgw: true\n  hyper_lr: HYPER_LR_PLACEHOLDER     # e.g. 0.001\n  seed: 2\n"
}
