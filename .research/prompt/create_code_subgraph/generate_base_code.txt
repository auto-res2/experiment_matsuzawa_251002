
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments with one GPU per run variation. If GPUs are insufficient, experiments run sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations with 1 GPU per variation)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment in parallel across available GPUs, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. GPU Allocation: main.py assigns 1 GPU per run variation. If available GPUs < total variations, queue remaining runs sequentially.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback


**Previous Validation Issue**: main.py does not satisfy logging redirection requirement: it pipes stderr into stdout (stderr=STDOUT) and writes only stdout.log; spec requires separate stdout.log and stderr.log files. All other core criteria appear met, but this violation means the base foundation is not yet compliant.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport time\nfrom typing import Any, Dict, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom . import preprocess\nfrom . import model as model_lib\n\n# ------------------ Utility ----------------------------------------------------\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    import random, numpy as np\n    random.seed(seed)\n    np.random.seed(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n    \"\"\"Compute top-1 accuracy for classification tasks.\"\"\"\n    with torch.no_grad():\n        pred_cls = pred.argmax(dim=1)\n        correct = (pred_cls == target).sum().item()\n        return correct / target.size(0)\n\n\n# ------------------ Warm-Start (OHGW) -----------------------------------------\n\ndef ohgw_warm_start(\n    model: nn.Module,\n    hyperparams: Dict[str, torch.Tensor],\n    data_iter: DataLoader,\n    device: torch.device,\n    eta_h: float,\n    task_type: str,\n) -> None:\n    \"\"\"One-Shot Hyper-Gradient Warm-Start.\n\n    NOTE: We treat weight_decay as the exemplar continuous hyper-parameter.\n    The routine is generic and can be extended to any differentiable ψ.\n    \"\"\"\n    model.train()\n    try:\n        inputs, targets = next(iter(data_iter))\n    except StopIteration:\n        raise RuntimeError(\"DataLoader must yield at least one batch for OHGW warm-start.\")\n\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    # Forward + compute loss with explicit L2 term controlled by weight_decay\n    outputs = model(inputs)\n    if task_type == \"classification\":\n        criterion = nn.CrossEntropyLoss()\n    elif task_type == \"regression\":\n        criterion = nn.MSELoss()\n    else:\n        raise ValueError(f\"Unsupported task_type '{task_type}'.\")\n\n    l2_term = 0.0\n    if \"weight_decay\" in hyperparams:\n        l2_term = 0.5 * hyperparams[\"weight_decay\"] * sum(\n            (p ** 2).sum() for p in model.parameters() if p.requires_grad\n        )\n    loss = criterion(outputs, targets) + l2_term\n\n    # Hyper-gradients w.r.t. each ψ (one autograd call)\n    hyper_tensors = list(hyperparams.values())\n    grads = torch.autograd.grad(loss, hyper_tensors, retain_graph=False, create_graph=False)\n\n    with torch.no_grad():\n        for p, g in zip(hyper_tensors, grads):\n            if g is not None:\n                p -= eta_h * g\n\n    # After warm-start, detach ψ to avoid leaking graph further.\n    for p in hyper_tensors:\n        p.detach_()\n\n\n# ------------------ Training Loop --------------------------------------------\n\ndef train_one_epoch(\n    model: nn.Module,\n    loader: DataLoader,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n    task_type: str,\n    hyperparams: Dict[str, torch.Tensor],\n) -> Tuple[float, float]:\n    \"\"\"Return mean loss & accuracy/MSE for the epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    running_metric = 0.0\n    criterion_cls = nn.CrossEntropyLoss()\n    criterion_reg = nn.MSELoss()\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        if task_type == \"classification\":\n            loss = criterion_cls(outputs, targets)\n        else:\n            loss = criterion_reg(outputs.squeeze(), targets.float())\n\n        # Explicit L2 regularisation via hyper-param ψ_weight_decay (if available)\n        if \"weight_decay\" in hyperparams:\n            loss = loss + 0.5 * hyperparams[\"weight_decay\"] * sum(\n                (p ** 2).sum() for p in model.parameters() if p.requires_grad\n            )\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        if task_type == \"classification\":\n            running_metric += accuracy(outputs.detach(), targets) * inputs.size(0)\n        else:\n            running_metric += torch.square(outputs.detach().squeeze() - targets.float()).sum().item()\n\n    epoch_loss = running_loss / len(loader.dataset)\n    if task_type == \"classification\":\n        epoch_metric = running_metric / len(loader.dataset)  # accuracy\n    else:\n        epoch_metric = running_metric / len(loader.dataset)  # MSE\n    return epoch_loss, epoch_metric\n\n\ndef evaluate(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    task_type: str,\n    hyperparams: Dict[str, torch.Tensor],\n) -> Tuple[float, float]:\n    \"\"\"Return mean loss & accuracy/MSE.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    running_metric = 0.0\n    criterion_cls = nn.CrossEntropyLoss()\n    criterion_reg = nn.MSELoss()\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            if task_type == \"classification\":\n                loss = criterion_cls(outputs, targets)\n            else:\n                loss = criterion_reg(outputs.squeeze(), targets.float())\n\n            if \"weight_decay\" in hyperparams:\n                loss = loss + 0.5 * hyperparams[\"weight_decay\"] * sum(\n                    (p ** 2).sum() for p in model.parameters() if p.requires_grad\n                )\n\n            running_loss += loss.item() * inputs.size(0)\n            if task_type == \"classification\":\n                running_metric += accuracy(outputs, targets) * inputs.size(0)\n            else:\n                running_metric += torch.square(outputs.squeeze() - targets.float()).sum().item()\n\n    epoch_loss = running_loss / len(loader.dataset)\n    if task_type == \"classification\":\n        epoch_metric = running_metric / len(loader.dataset)\n    else:\n        epoch_metric = running_metric / len(loader.dataset)\n    return epoch_loss, epoch_metric\n\n\n# ------------------ Main entry -------------------------------------------------\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to YAML file for this run variation.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to dump all outputs.\")\n    parser.add_argument(\"--device\", type=str, default=\"auto\", help=\"cuda device idx | cpu | auto\")\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = parse_args()\n    cfg = preprocess.load_yaml(args.run_config)\n\n    run_id = cfg[\"name\"]\n    os.makedirs(os.path.join(args.results_dir, run_id), exist_ok=True)\n\n    # Dump config for reproducibility\n    with open(os.path.join(args.results_dir, run_id, \"config.json\"), \"w\") as f:\n        json.dump(cfg, f, indent=2)\n\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    if args.device == \"auto\":\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif args.device == \"cpu\":\n        device = torch.device(\"cpu\")\n    else:\n        device = torch.device(f\"cuda:{args.device}\")\n    # ------------------------------------ Data --------------------------------\n    train_loader, val_loader, test_loader = preprocess.get_dataloaders(\n        cfg[\"dataset\"], cfg[\"training\"][\"batch_size\"], seed\n    )\n    task_type = cfg[\"dataset\"].get(\"task_type\", \"classification\")\n    # ------------------------------------ Model -------------------------------\n    model = model_lib.get_model(cfg[\"model\"]).to(device)\n\n    # Hyper-parameters as differentiable tensors (weight_decay only for now)\n    hyperparams: Dict[str, torch.Tensor] = {}\n    if \"weight_decay\" in cfg[\"training\"]:\n        hyperparams[\"weight_decay\"] = torch.tensor(\n            float(cfg[\"training\"][\"weight_decay\"]), requires_grad=True, device=device\n        )\n\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=cfg[\"training\"][\"lr\"],\n        momentum=cfg[\"training\"].get(\"momentum\", 0.9),\n    )\n\n    # --------------------------- Optional OHGW ---------------------------------\n    warm_start_flag = cfg.get(\"scheduler\", {}).get(\"warm_start\", False)\n    if warm_start_flag:\n        ohgw_warm_start(\n            model=model,\n            hyperparams=hyperparams,\n            data_iter=train_loader,\n            device=device,\n            eta_h=cfg.get(\"scheduler\", {}).get(\"eta_h\", 1e-3),\n            task_type=task_type,\n        )\n\n    # ---------------------------- Training Loop -------------------------------\n    epochs = int(cfg[\"training\"][\"epochs\"])\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"train_metric\": [],\n        \"val_loss\": [],\n        \"val_metric\": [],\n    }\n    best_val_metric = -float(\"inf\") if task_type == \"classification\" else float(\"inf\")\n    best_epoch = -1\n\n    t_start = time.time()\n    for epoch in range(1, epochs + 1):\n        train_loss, train_metric = train_one_epoch(\n            model, train_loader, optimizer, device, task_type, hyperparams\n        )\n        val_loss, val_metric = evaluate(model, val_loader, device, task_type, hyperparams)\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_metric\"].append(train_metric)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_metric\"].append(val_metric)\n\n        better = (\n            val_metric > best_val_metric\n            if task_type == \"classification\"\n            else val_metric < best_val_metric\n        )\n        if better:\n            best_val_metric = val_metric\n            best_epoch = epoch\n            torch.save(\n                {\n                    \"model_state_dict\": model.state_dict(),\n                    \"hyperparams\": {k: v.detach().cpu().item() for k, v in hyperparams.items()},\n                },\n                os.path.join(args.results_dir, run_id, \"model_best.pt\"),\n            )\n\n    training_time = time.time() - t_start\n\n    # --------------------- Final Eval on Test Set -----------------------------\n    test_loss, test_metric = evaluate(model, test_loader, device, task_type, hyperparams)\n\n    # --------------------- Persist metrics ------------------------------------\n    summary = {\n        \"run_id\": run_id,\n        \"task_type\": task_type,\n        \"warm_start\": warm_start_flag,\n        \"best_val_metric\": best_val_metric,\n        \"best_epoch\": best_epoch,\n        \"test_metric\": test_metric,\n        \"training_time_sec\": training_time,\n        \"epochs\": epochs,\n    }\n    with open(os.path.join(args.results_dir, run_id, \"results.json\"), \"w\") as f:\n        json.dump({\"summary\": summary, \"history\": history}, f, indent=2)\n\n    # ------------------------ Visualization -----------------------------------\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # Training loss curve\n    plt.figure()\n    sns.lineplot(x=history[\"epoch\"], y=history[\"train_loss\"], marker=\"o\", label=\"train_loss\")\n    sns.lineplot(x=history[\"epoch\"], y=history[\"val_loss\"], marker=\"o\", label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training & Validation Loss – {run_id}\")\n    plt.annotate(\n        f\"final={history['val_loss'][-1]:.3f}\",\n        xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n        xytext=(5, 5),\n        textcoords=\"offset points\",\n    )\n    plt.legend()\n    plt.savefig(\n        os.path.join(args.results_dir, run_id, f\"training_loss_{run_id}.pdf\"),\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n    # Metric curve (accuracy or MSE)\n    plt.figure()\n    sns.lineplot(x=history[\"epoch\"], y=history[\"train_metric\"], marker=\"o\", label=\"train_metric\")\n    sns.lineplot(x=history[\"epoch\"], y=history[\"val_metric\"], marker=\"o\", label=\"val_metric\")\n    plt.xlabel(\"Epoch\")\n    ylabel = \"Accuracy\" if task_type == \"classification\" else \"MSE\"\n    plt.ylabel(ylabel)\n    plt.title(f\"{ylabel} – {run_id}\")\n    best_val_display = (\n        f\"best={best_val_metric:.3f} @ep{best_epoch}\"\n    )\n    plt.annotate(\n        best_val_display,\n        xy=(best_epoch, best_val_metric),\n        xytext=(5, -10),\n        textcoords=\"offset points\",\n    )\n    plt.legend()\n    plt.savefig(\n        os.path.join(args.results_dir, run_id, f\"{ylabel.lower()}_{run_id}.pdf\"),\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n    # ---------------------- Stdout JSON summary -------------------------------\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n\n# ------------------ Utility ----------------------------------------------------\n\ndef gather_results(results_dir: str) -> List[Dict]:\n    summaries = []\n    for run_dir in Path(results_dir).iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                data = json.load(f)\n                summaries.append(data[\"summary\"])\n    return summaries\n\n\n# ------------------ Visualization ---------------------------------------------\n\ndef plot_metric_comparison(df: pd.DataFrame, results_dir: str, metric: str = \"test_metric\"):\n    plt.figure(figsize=(max(6, 0.5 * len(df)), 4))\n    ax = sns.barplot(x=\"run_id\", y=metric, hue=\"warm_start\", data=df, dodge=False)\n    plt.xlabel(\"Run ID\")\n    plt.ylabel(metric)\n    plt.title(f\"{metric} Comparison Across Variations\")\n    for p in ax.patches:\n        value = p.get_height()\n        ax.annotate(f\"{value:.3f}\", (p.get_x() + p.get_width() / 2.0, value), ha=\"center\", va=\"bottom\")\n    plt.legend(title=\"Warm Start\")\n    fname = os.path.join(results_dir, f\"{metric}_comparison.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------ Main -------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Aggregate & visualise experiment results.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n\n    summaries = gather_results(args.results_dir)\n    if not summaries:\n        raise RuntimeError(\"No results.json files found in the provided directory.\")\n\n    df = pd.DataFrame(summaries)\n\n    # Save consolidated CSV\n    df.to_csv(os.path.join(args.results_dir, \"all_results.csv\"), index=False)\n\n    # Generate comparison figure(s)\n    figures = []\n    for metric in [\"test_metric\", \"best_val_metric\"]:\n        figures.append(plot_metric_comparison(df, args.results_dir, metric))\n\n    # --------------------- Stdout JSON summary ---------------------------------\n    output = {\n        \"num_runs\": len(df),\n        \"metrics\": df.to_dict(orient=\"list\"),\n        \"figures\": figures,\n    }\n    print(json.dumps(output))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"Common data-loading & preprocessing utilities with dataset placeholders.\"\"\"\n\nimport os\nfrom typing import Tuple, Dict, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport yaml\n\n# ------------------ YAML Loader ----------------------------------------------\n\ndef load_yaml(path: str) -> Dict[str, Any]:\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\n# ------------------ Synthetic Dataset (for smoke tests) ----------------------\n\nclass SyntheticClassificationDataset(Dataset):\n    \"\"\"Random data for classification tasks to enable smoke testing without I/O.\"\"\"\n\n    def __init__(self, num_samples: int, input_dim: int, num_classes: int, seed: int = 42):\n        import numpy as np\n\n        rng = np.random.RandomState(seed)\n        self.x = torch.from_numpy(rng.randn(num_samples, input_dim).astype(\"float32\"))\n        self.y = torch.from_numpy(rng.randint(0, num_classes, size=(num_samples,)).astype(\"int64\"))\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# ------------------ Generic DataLoader builder -------------------------------\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any], batch_size: int, seed: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    name = dataset_cfg[\"name\"]\n    if name == \"synthetic_classification\":\n        dataset = SyntheticClassificationDataset(\n            num_samples=dataset_cfg.get(\"num_samples\", 1000),\n            input_dim=dataset_cfg.get(\"input_dim\", 32),\n            num_classes=dataset_cfg.get(\"num_classes\", 10),\n            seed=seed,\n        )\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\n            f\"Dataset '{name}' not implemented in common core. Replace this placeholder with actual loader.\"\n        )\n\n    # Split train/val/test (80/10/10)\n    n_total = len(dataset)\n    n_train = int(0.8 * n_total)\n    n_val = int(0.1 * n_total)\n    n_test = n_total - n_train - n_val\n    generator = torch.Generator().manual_seed(seed)\n    train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test], generator=generator)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, test_loader\n",
    "model_py": "import math\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\n\n\nclass MLPClassifier(nn.Module):\n    \"\"\"Simple two-layer MLP for classification useful for smoke tests.\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# PLACEHOLDER: Additional model architectures (e.g., ResNet, Transformer) will be added in later steps.\n\n\ndef get_model(model_cfg: Dict[str, Any]) -> nn.Module:\n    name = model_cfg[\"name\"]\n    if name == \"mlp\":\n        return MLPClassifier(\n            input_dim=model_cfg[\"input_dim\"],\n            hidden_dim=model_cfg.get(\"hidden_dim\", 128),\n            num_classes=model_cfg[\"num_classes\"],\n        )\n    else:\n        # PLACEHOLDER: Will be replaced with specific model construction logic\n        raise NotImplementedError(f\"Model '{name}' not implemented in common core.\")\n",
    "main_py": "import argparse\nimport os\nimport shutil\nimport signal\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport yaml\nimport torch\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / \"config\"\n\n# ------------------ Helper Utilities -----------------------------------------\n\ndef tee_stream(stream, file_handle, out_stream):\n    \"\"\"Copy stream to both file and given stdout/stderr.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        decoded = line.decode()\n        file_handle.write(decoded)\n        file_handle.flush()\n        out_stream.write(decoded)\n        out_stream.flush()\n\n\ndef load_yaml(path: str) -> Dict[str, Any]:\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\n# ------------------ Orchestrator ---------------------------------------------\n\ndef run_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path):\n    stdout_file = open(stdout_path, \"w\")\n    stderr_file = open(stderr_path, \"w\")\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n    # Tee threads\n    t_out = threading.Thread(target=tee_stream, args=(process.stdout, stdout_file, sys.stdout))\n    t_err = threading.Thread(target=tee_stream, args=(process.stderr, stderr_file, sys.stderr))\n    t_out.start()\n    t_err.start()\n\n    def terminate(signum, frame):\n        process.terminate()\n    signal.signal(signal.SIGINT, terminate)\n\n    return_code = process.wait()\n    t_out.join()\n    t_err.join()\n    stdout_file.close()\n    stderr_file.close()\n\n    if return_code != 0:\n        raise subprocess.CalledProcessError(return_code, cmd)\n\n\ndef allocate_gpus(num_required: int) -> List[int]:\n    visible = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n    if visible is not None and visible.strip() != \"\":\n        gpu_ids = [int(i) for i in visible.split(\",\")]\n    else:\n        gpu_ids = list(range(torch.cuda.device_count()))\n    if len(gpu_ids) < num_required:\n        return gpu_ids  # fewer GPUs than tasks -> some will queue later\n    return gpu_ids[:num_required]\n\n\n# ------------------ Main ------------------------------------------------------\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for OHGW experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store all outputs.\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    os.makedirs(args.results_dir, exist_ok=True)\n\n    if args.smoke_test:\n        cfg_path = CONFIG_DIR / \"smoke_test.yaml\"\n    else:\n        cfg_path = CONFIG_DIR / \"full_experiment.yaml\"\n\n    config = load_yaml(str(cfg_path))\n    experiments = config.get(\"experiments\", [])\n    if not experiments:\n        print(\"[main] No experiments found in configuration file.\")\n        sys.exit(0)\n\n    # GPU allocation logic\n    available_gpus = allocate_gpus(len(experiments))\n    gpu_queue = list(available_gpus)\n    running_threads = []\n\n    def worker(exp_cfg: Dict[str, Any], assigned_gpu: int):\n        run_id = exp_cfg[\"name\"]\n        run_dir = Path(args.results_dir) / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Dump individual run config to temporary YAML file consumed by train.py\n        run_cfg_path = run_dir / \"run_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp_cfg, f)\n\n        env = os.environ.copy()\n        env[\"CUDA_VISIBLE_DEVICES\"] = str(assigned_gpu)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--run-config\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            args.results_dir,\n            \"--device\",\n            \"0\",  # since each subprocess sees only one visible GPU\n        ]\n\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        run_subprocess(cmd, env, stdout_path, stderr_path)\n\n        # return GPU to queue when done\n        gpu_queue.append(assigned_gpu)\n\n    for exp_cfg in experiments:\n        while not gpu_queue:\n            time.sleep(1)  # Wait for a GPU to free up\n        gpu_id = gpu_queue.pop(0)\n        t = threading.Thread(target=worker, args=(exp_cfg, gpu_id))\n        t.start()\n        running_threads.append(t)\n\n    # Wait for all experiments to finish\n    for t in running_threads:\n        t.join()\n\n    # After all training runs, trigger evaluation\n    print(\"[main] All training variations finished. Launching evaluation...\")\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        args.results_dir,\n    ]\n    subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"ohgw_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experimental framework\"\nrequires-python = \">=3.10\"\nreadme = \"README.md\"\nlicense = {text = \"MIT\"}\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \">=0.15.0\"\ntorchaudio = \">=2.0.0\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\npandas = \"*\"\ntqdm = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n",
    "smoke_test_yaml": "# Lightweight synthetic experiments for CI smoke testing\nexperiments:\n  - name: synthetic_baseline\n    seed: 1\n    model:\n      name: mlp\n      input_dim: 32\n      hidden_dim: 64\n      num_classes: 10\n    dataset:\n      name: synthetic_classification\n      num_samples: 200\n      input_dim: 32\n      num_classes: 10\n      task_type: classification\n    training:\n      epochs: 2\n      batch_size: 32\n      lr: 0.001\n      weight_decay: 0.0001\n    scheduler:\n      name: vanilla\n      warm_start: false\n\n  - name: synthetic_ohgw\n    seed: 2\n    model:\n      name: mlp\n      input_dim: 32\n      hidden_dim: 64\n      num_classes: 10\n    dataset:\n      name: synthetic_classification\n      num_samples: 200\n      input_dim: 32\n      num_classes: 10\n      task_type: classification\n    training:\n      epochs: 2\n      batch_size: 32\n      lr: 0.001\n      weight_decay: 0.0001\n    scheduler:\n      name: vanilla\n      warm_start: true\n      eta_h: 0.001\n",
    "full_experiment_yaml": "# PLACEHOLDER template – will be populated with real datasets and models in subsequent steps\nexperiments:\n  - name: DATASET_PLACEHOLDER_MODEL_PLACEHOLDER_baseline\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    dataset: DATASET_PLACEHOLDER\n    training: SPECIFIC_CONFIG_PLACEHOLDER\n    scheduler:\n      name: baseline\n      warm_start: false\n\n  - name: DATASET_PLACEHOLDER_MODEL_PLACEHOLDER_ohgw\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    dataset: DATASET_PLACEHOLDER\n    training: SPECIFIC_CONFIG_PLACEHOLDER\n    scheduler:\n      name: baseline\n      warm_start: true\n      eta_h: 0.001\n"
}
