
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['pasha-baseline', 'pasha-ohgw-vision-tuned-eta', 'pasha-ohgw-low-eta', 'pasha-ohgw-noisy-data']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Current Experiment (to generate code for)
- Experiment ID: exp-2
- Description: Objective / Hypothesis: Test zero-shot generalisation and robustness of OHGW in a different modality (language modelling) and under distribution shift.

Domain & Task: NLP – word-level language modelling on WikiText-103, with an additional OOD evaluation on WikiText-103-v1 (headlines first 5% shuffled – simulates domain drift).

Model: GPT2-small (124 M params, HuggingFace implementation) trained from scratch with BPE vocab 50 k.

Scheduler: PASHA (2023) – gradient-aware variation of Hyperband; ours wraps PASHA in the same way.

Search space (6 dims): log10-learning-rate, log10-weight-decay, attention-dropout, residual-dropout, label-smoothing, warmup-steps (continuous proxy by scaling factor).

Run variations:
• pasha-baseline – vanilla PASHA.
• pasha-ohgw-vision-tuned-eta – OHGW with η_h=1e-3 exactly copied from CIFAR experiment (tests transfer without retuning).
• pasha-ohgw-low-eta – OHGW with η_h=3e-4 (sensitivity extremum).
• pasha-ohgw-noisy-data – OHGW with η_h=1e-3 while 15% of training tokens are randomly replaced (robustness to noise).

Dataset processing: SentencePiece BPE (shared).  Sequence length 1024 tokens; dynamic batching up to 2M tokens/GPU.  Train/val/test split 238M / 8M / 8M tokens.  For OOD, evaluate perplexity on shuffled-headline subset (unseen ordering).

Training loop & budget: 50 training epochs (~250 k updates), AdamW, cosine LR.  PASHA minimum resource per config = 2 epochs, rungs ×2.  Total compute budget per replicate 16 GPU×hours (8 GPUs ×2 h).  24 replicates.

Evaluation metrics:
Primary – T@ppl=30 on validation set (wall-clock & GPU-h).
Secondary – best validation perplexity, best OOD perplexity, FLOPs overhead, peak VRAM, std-dev across seeds.
Calibration metric – ECE (expected calibration error) on top-k probabilities (k=10).

Data splitting: Configs use 90/10 split of train for inner-val to avoid test leakage; final report on held-out test.

Hyper-parameter analysis: log sweep of η_h done offline on 5 seeds; fit cubic spline to measure optimal region width (reports in appendix).

Robustness procedures:
1. Data noise: see pasha-ohgw-noisy-data.
2. Distribution shift: compute perplexity on OOD set after every rung, record Δppl.
3. Adversarial tokens: evaluate final models with TextFooler adversarial examples on 5 k sentences, report degradation.

Compute profiling: same toolkit as exp-1; additionally use PyTorch-CUDA-Profiler for kernel-level warm-start cost.

Example trial wrapper:
```python
loss = lm_model(input_ids, labels=input_ids).loss
hg = torch.autograd.grad(loss, cfg.continuous())
with torch.no_grad():
    for p,g in zip(cfg.continuous(), hg):
        p -= eta_h * g    # no extra unroll
pasha_scheduler.launch(cfg)
```

Statistical testing: Paired Wilcoxon on T@30 with Bonferroni correction (m=3 comparisons).  Significance if p<0.016.

Success criteria: vision-tuned η_h version achieves ≥15% speed-up vs. baseline on T@30 with ≤0.5 ppl regression on final test; effect still ≥10% under noisy data.  Extra compute overhead ≤5% of total FLOPs.
- Run Variations: ['pasha-baseline', 'pasha-ohgw-vision-tuned-eta', 'pasha-ohgw-low-eta', 'pasha-ohgw-noisy-data']

# Base Code
{'train_py': '"""src/train.py\nRuns a single experiment variation (one entry of the YAML config).\nSaves metrics, figures and model weights into the provided run directory\nand prints a JSON summary to stdout (required by the evaluation harness).\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport logging\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport torch\nimport torch.nn.functional as F\nimport yaml\nfrom matplotlib import pyplot as plt\nfrom torch import nn, optim\nfrom tqdm.auto import tqdm\n\nimport src.model as model_zoo\nimport src.preprocess as preprocess\n\nmatplotlib.use("Agg")  # do not require X11\n\n\n# ---------------- Utility helpers -------------------------------------------------\n\ndef set_random_seed(seed: int) -> None:\n    import random\n    import numpy as np\n\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n    """Top-1 accuracy."""\n    with torch.no_grad():\n        pred_labels = pred.argmax(dim=1)\n        return (pred_labels == target).float().mean().item() * 100.0\n\n\n# ---------------- Core OHGW implementation ---------------------------------------\n\ndef one_shot_hypergradient_warm_start(\n    model: nn.Module,\n    optimizer_cfg: Dict,\n    batch: List[torch.Tensor],\n    eta_h: float,\n    device: torch.device,\n) -> Dict:\n    """Apply a single stochastic hyper-gradient step on *weight_decay*.\n\n    For demonstration purposes the only continuous hyper-parameter we treat here\n    is the weight-decay "wd".  We keep the implementation generic so that the\n    subsequent specialised experiment step can swap in more elaborate\n    hyper-gradient logic without touching the surrounding framework.\n    """\n\n    # Extract current wd (defaults to 0.0) and wrap as differentiable tensor\n    wd_val: float = float(optimizer_cfg.get("weight_decay", 0.0))\n    wd = torch.tensor(wd_val, dtype=torch.float32, device=device, requires_grad=True)\n\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n    # Forward with weight-decay regularisation term that depends on *wd*\n    logits = model(inputs)\n    ce_loss = F.cross_entropy(logits, targets)\n    l2_term = 0.5 * wd * sum((p ** 2).sum() for p in model.parameters())\n    loss = ce_loss + l2_term\n\n    # Compute dL / d(wd)\n    grad_wd, = torch.autograd.grad(loss, wd, retain_graph=False, create_graph=False)\n\n    # One tiny hyper-parameter update\n    wd_new = (wd - eta_h * grad_wd).clamp(min=0.0).item()\n\n    # Return the modified optimiser configuration\n    new_optimizer_cfg = dict(optimizer_cfg)\n    new_optimizer_cfg["weight_decay"] = wd_new\n    return new_optimizer_cfg\n\n\n# ---------------- Training --------------------------------------------------------\n\ndef train_one_epoch(model, loader, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    total = 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = F.cross_entropy(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        batch_size = y.size(0)\n        running_loss += loss.item() * batch_size\n        running_acc += accuracy(logits, y) * batch_size\n        total += batch_size\n    return running_loss / total, running_acc / total\n\n\ndef evaluate(model, loader, device):\n    model.eval()\n    loss_sum = 0.0\n    acc_sum = 0.0\n    total = 0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            loss = F.cross_entropy(logits, y)\n            batch = y.size(0)\n            loss_sum += loss.item() * batch\n            acc_sum += accuracy(logits, y) * batch\n            total += batch\n    return loss_sum / total, acc_sum / total\n\n\n# ---------------- Main entry ------------------------------------------------------\n\ndef run_training(run_dir: Path, cfg: Dict, run_id: str):\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / "figures").mkdir(exist_ok=True, parents=True)\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    # Set seeds for reproducibility\n    seed = int(cfg.get("seed", 42))\n    set_random_seed(seed)\n\n    # ---------------- Dataset -----------------------------------------------------\n    d_cfg = cfg["dataset"]\n    t_cfg = cfg["training"]\n    train_loader, val_loader, test_loader = preprocess.build_dataloaders(\n        dataset_cfg=d_cfg,\n        batch_size=int(t_cfg["batch_size"]),\n        num_workers=int(t_cfg.get("num_workers", 2)),\n    )\n\n    # ---------------- Model -------------------------------------------------------\n    m_cfg = cfg["model"]\n    model = model_zoo.build_model(m_cfg).to(device)\n\n    # ---------------- Optimiser ---------------------------------------------------\n    opt_cfg = {\n        "lr": float(t_cfg["learning_rate"]),\n        "momentum": float(t_cfg.get("momentum", 0.0)),\n        "weight_decay": float(t_cfg.get("weight_decay", 0.0)),\n    }\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=opt_cfg["lr"],\n        momentum=opt_cfg["momentum"],\n        weight_decay=opt_cfg["weight_decay"],\n    )\n\n    # ---------------- OHGW warm-start -------------------------------------------\n    if cfg.get("ohgw", {}).get("enabled", False):\n        eta_h = float(cfg["ohgw"].get("eta_h", 1e-3))\n        batch_iter = iter(train_loader)\n        mini_batch = next(batch_iter)\n        opt_cfg = one_shot_hypergradient_warm_start(\n            model, optimizer_cfg=opt_cfg, batch=mini_batch, eta_h=eta_h, device=device\n        )\n        # Re-initialise optimiser with new hyper-params\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=opt_cfg["lr"],\n            momentum=opt_cfg["momentum"],\n            weight_decay=opt_cfg["weight_decay"],\n        )\n\n    # ---------------- Training loop ---------------------------------------------\n    n_epochs = int(t_cfg["epochs"])\n    history: List[Dict] = []\n    best_val_acc = -1.0\n    best_state_dict = None\n    start_time = time.time()\n\n    for epoch in range(1, n_epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, device)\n\n        history.append(\n            {\n                "epoch": epoch,\n                "train_loss": train_loss,\n                "train_acc": train_acc,\n                "val_loss": val_loss,\n                "val_acc": val_acc,\n            }\n        )\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n\n        logging.info(\n            f"Epoch {epoch:03d}/{n_epochs}: train_loss={train_loss:.4f} train_acc={train_acc:.2f} "\n            f"val_loss={val_loss:.4f} val_acc={val_acc:.2f}"\n        )\n\n    total_seconds = time.time() - start_time\n\n    # ---------------- Test evaluation ------------------------------------------\n    model.load_state_dict(best_state_dict)\n    test_loss, test_acc = evaluate(model, test_loader, device)\n\n    # ---------------- Persist artefacts ----------------------------------------\n    torch.save(best_state_dict, run_dir / "best_model.pt")\n    with open(run_dir / "history.json", "w") as f:\n        json.dump(history, f, indent=2)\n\n    # Figures: loss + accuracy curves -------------------------------------------\n    epochs = [h["epoch"] for h in history]\n    train_losses = [h["train_loss"] for h in history]\n    val_losses = [h["val_loss"] for h in history]\n    plt.figure()\n    plt.plot(epochs, train_losses, label="Train")\n    plt.plot(epochs, val_losses, label="Validation")\n    plt.xlabel("Epoch")\n    plt.ylabel("Cross-Entropy Loss")\n    plt.title(f"Training Loss – {run_id}")\n    for x, y in zip(epochs, val_losses):\n        plt.annotate(f"{y:.2f}", (x, y))\n    plt.legend()\n    fname_loss = run_dir / "figures" / f"training_loss_{run_id}.pdf"\n    plt.savefig(fname_loss, bbox_inches="tight")\n    plt.close()\n\n    train_accs = [h["train_acc"] for h in history]\n    val_accs = [h["val_acc"] for h in history]\n    plt.figure()\n    plt.plot(epochs, train_accs, label="Train")\n    plt.plot(epochs, val_accs, label="Validation")\n    plt.xlabel("Epoch")\n    plt.ylabel("Accuracy (%)")\n    plt.title(f"Accuracy – {run_id}")\n    plt.annotate(\n        f"Best={best_val_acc:.2f}", (epochs[-1], val_accs[-1]),\n        textcoords="offset points", xytext=(0, 10), ha=\'right\'\n    )\n    plt.legend()\n    fname_acc = run_dir / "figures" / f"accuracy_{run_id}.pdf"\n    plt.savefig(fname_acc, bbox_inches="tight")\n    plt.close()\n\n    # ---------------- Final results JSON ---------------------------------------\n    results = {\n        "run_id": run_id,\n        "seed": seed,\n        "dataset": d_cfg["name"],\n        "model": m_cfg["name"],\n        "ohgw_enabled": cfg.get("ohgw", {}).get("enabled", False),\n        "eta_h": cfg.get("ohgw", {}).get("eta_h", None),\n        "training_seconds": total_seconds,\n        "best_val_accuracy": best_val_acc,\n        "test_accuracy": test_acc,\n        "test_loss": test_loss,\n        "history": history,\n        "figures": [str(fname_loss.name), str(fname_acc.name)],\n    }\n    with open(run_dir / "results.json", "w") as f:\n        json.dump(results, f, indent=2)\n\n    # ---------------- Mandatory stdout prints ----------------------------------\n    # 1) Human-readable description\n    print(\n        f"\\n==== Experiment Description ({run_id}) ===="\n        f"\\nDataset: {d_cfg[\'name\']}\\nModel: {m_cfg[\'name\']}\\nOHGW: {results[\'ohgw_enabled\']} (eta_h={results[\'eta_h\']})"\n        f"\\nEpochs: {n_epochs}, Batch Size: {t_cfg[\'batch_size\']}, LR: {t_cfg[\'learning_rate\']}\\n"\n    )\n    # 2) Structured numerical data line (JSON)\n    print(json.dumps(results))\n\n\n# ---------------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description="Run one experiment variation")\n    p.add_argument("--run-id", required=True, type=str)\n    p.add_argument("--config-file", required=True, type=str)\n    p.add_argument("--run-dir", required=True, type=str)\n    return p.parse_args()\n\n\nif __name__ == "__main__":\n    args = parse_args()\n\n    # basic logging to stderr (main.py captures streams)\n    logging.basicConfig(\n        level=logging.INFO,\n        format="%(asctime)s | %(levelname)s | %(message)s",\n        datefmt="%H:%M:%S",\n    )\n\n    with open(args.config_file, "r") as f:\n        full_cfg = yaml.safe_load(f)\n\n    run_cfg = full_cfg["variations"][args.run_id]\n\n    run_training(Path(args.run_dir), run_cfg, args.run_id)\n', 'evaluate_py': '"""src/evaluate.py\nAggregates result files from all experiment variations and produces\ncomparison figures + JSON summary printed to stdout.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport pandas as pd\nimport yaml\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import wilcoxon  # Used if available in env\n\nmatplotlib.use("Agg")\n\n# ----------------------------------------------------------------------------------\n\n\ndef load_all_results(results_dir: Path) -> List[Dict]:\n    data = []\n    for run_dir in results_dir.iterdir():\n        if (run_dir / "results.json").exists():\n            with open(run_dir / "results.json") as f:\n                data.append(json.load(f))\n    return data\n\n\n# ----------------------------------------------------------------------------------\n\ndef create_barplot(df: pd.DataFrame, metric: str, ylabel: str, fname: Path):\n    plt.figure()\n    ax = plt.gca()\n    bars = ax.bar(df["run_id"], df[metric], color="skyblue")\n    ax.set_ylabel(ylabel)\n    ax.set_xlabel("Run Variation")\n    ax.set_title(f"{ylabel} Comparison")\n    ax.set_xticklabels(df["run_id"], rotation=45, ha="right")\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate(f"{height:.2f}", xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3), textcoords="offset points", ha=\'center\', va=\'bottom\')\n    plt.tight_layout()\n    plt.savefig(fname, bbox_inches="tight")\n    plt.close()\n\n\n# ----------------------------------------------------------------------------------\n\ndef aggregate_and_visualise(results_dir: Path):\n    records = load_all_results(results_dir)\n    if not records:\n        raise RuntimeError(f"No results.json found in {results_dir}.*")\n\n    df = pd.DataFrame(records)\n\n    # Basic statistics -----------------------------------------------------------\n    summary = {\n        "runs": df["run_id"].tolist(),\n        "best_val_accuracy": df["best_val_accuracy"].tolist(),\n        "test_accuracy": df["test_accuracy"].tolist(),\n        "training_seconds": df["training_seconds"].tolist(),\n    }\n\n    # Statistical test between first two runs (if >1)\n    p_value = None\n    if len(df) >= 2:\n        try:\n            stat, p_value = wilcoxon(df.iloc[:, 1], df.iloc[:, 2])  # type: ignore\n        except Exception:\n            pass\n    summary["wilcoxon_p_value"] = p_value\n\n    # Figures --------------------------------------------------------------------\n    create_barplot(\n        df, metric="test_accuracy", ylabel="Test Accuracy (%)",\n        fname=results_dir / "final_accuracy_comparison.pdf",\n    )\n    create_barplot(\n        df, metric="training_seconds", ylabel="Training Time (s)",\n        fname=results_dir / "training_time_comparison.pdf",\n    )\n\n    # stdout prints --------------------------------------------------------------\n    print("\\n==== Evaluation Summary ====")\n    print(json.dumps(summary))\n\n\n# ----------------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument("--results-dir", required=True, type=str)\n    return p.parse_args()\n\n\nif __name__ == "__main__":\n    logging.basicConfig(level=logging.INFO,\n                        format="%(asctime)s | %(levelname)s | %(message)s",\n                        datefmt="%H:%M:%S")\n    args = parse_args()\n    aggregate_and_visualise(Path(args.results_dir))\n', 'preprocess_py': '"""src/preprocess.py\nDataset loading & preprocessing pipeline with explicit placeholders for\nfuture dataset-specific logic.  Includes a fully functional FakeData\nfallback which enables smoke tests without external downloads.\n"""\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets\n\n\n# ----------------------------------------------------------------------------------\n\n\ndef _build_fakedata(params: dict, batch_size: int, num_workers: int):\n    transform = T.Compose([T.ToTensor()])\n    size = int(params.get("size", 1024))\n    image_size = tuple(params.get("image_size", (3, 32, 32)))\n    num_classes = int(params.get("num_classes", 10))\n\n    ds = datasets.FakeData(size=size, image_size=image_size, num_classes=num_classes, transform=transform)\n    n_val = n_test = size // 10  # 10 % each\n    n_train = size - n_val - n_test\n    train_set, val_set, test_set = random_split(ds, [n_train, n_val, n_test])\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    return train_loader, val_loader, test_loader\n\n\n# ----------------------------------------------------------------------------------\n\n\ndef build_dataloaders(dataset_cfg: dict, batch_size: int, num_workers: int = 2):\n    """Factory for train/val/test DataLoaders.\n\n    Parameters\n    ----------\n    dataset_cfg : dict\n        Must have field "name" specifying which dataset to load and optional\n        "params" carrying dataset-specific hyper-parameters.\n    """\n    name = dataset_cfg["name"]\n    params = dataset_cfg.get("params", {})\n\n    if name == "FakeData":\n        return _build_fakedata(params, batch_size, num_workers)\n\n    # -------------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    # Example:\n    # elif name == "CIFAR10":\n    #     return _build_cifar10(params, batch_size, num_workers)\n    # -------------------------------------------------------------------------\n\n    raise NotImplementedError(f"Dataset \'{name}\' not implemented ‑ placeholder needs replacement.")\n', 'model_py': '"""src/model.py\nModel definitions & factory.\nIncludes a lightweight SimpleCNN for image inputs that works out of the box\nfor the smoke test yet can be replaced by experiment-specific architectures.\n"""\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch.nn as nn\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# ----------------------------------------------------------------------------------\n\n\ndef build_model(model_cfg: Dict):\n    name = model_cfg["name"]\n    params = model_cfg.get("params", {})\n\n    if name == "SimpleCNN":\n        return SimpleCNN(**params)\n\n    # -------------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with specific model architectures\n    # e.g. if name == "ResNet20": return ResNet20(**params)\n    # -------------------------------------------------------------------------\n\n    raise NotImplementedError(f"Model \'{name}\' not implemented ‑ placeholder needs replacement.")\n', 'main_py': '"""src/main.py\nCentral orchestration script.\nSupports --smoke-test and --full-experiment flags and ensures that stdout and\nstderr of each subprocess are captured into *separate* files **and** streamed\nlive to the console, fully satisfying the logging requirement.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Dict\n\nimport yaml\n\n# ----------------------------------------------------------------------------------\n# Real-time tee helper ----------------------------------------------------------------\n\n\ndef _stream_tee(src, targets):\n    for line in iter(src.readline, ""):\n        for t in targets:\n            t.write(line)\n            t.flush()\n    src.close()\n\n\n# ----------------------------------------------------------------------------------\n\n\ndef run_subprocess(cmd, run_dir: Path):\n    stdout_file = open(run_dir / "stdout.log", "w", buffering=1)\n    stderr_file = open(run_dir / "stderr.log", "w", buffering=1)\n\n    proc = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        bufsize=1,\n    )\n\n    # Launch background threads that tee streams to both file and console\n    threads = [\n        threading.Thread(target=_stream_tee, args=(proc.stdout, [sys.stdout, stdout_file]), daemon=True),\n        threading.Thread(target=_stream_tee, args=(proc.stderr, [sys.stderr, stderr_file]), daemon=True),\n    ]\n    for t in threads:\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n    stdout_file.close()\n    stderr_file.close()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f"Subprocess {\' \'.join(cmd)} failed with code {proc.returncode}")\n\n\n# ----------------------------------------------------------------------------------\n\ndef orchestrate(config_path: Path, results_dir: Path):\n    with open(config_path, "r") as f:\n        cfg = yaml.safe_load(f)\n\n    variations: Dict = cfg["variations"]\n\n    for run_id, run_cfg in variations.items():\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist *individual* run config so the subprocess can load just that\n        single_cfg = {"variations": {run_id: run_cfg}}\n        cfg_file = run_dir / "config.yaml"\n        with open(cfg_file, "w") as f:\n            yaml.safe_dump(single_cfg, f)\n\n        cmd = [\n            sys.executable,\n            "-m",\n            "src.train",\n            "--run-id",\n            run_id,\n            "--config-file",\n            str(cfg_file),\n            "--run-dir",\n            str(run_dir),\n        ]\n\n        print(f"\\n===== Launching {run_id} =====")\n        run_subprocess(cmd, run_dir)\n\n    # After all runs launch evaluation\n    eval_cmd = [\n        sys.executable,\n        "-m",\n        "src.evaluate",\n        "--results-dir",\n        str(results_dir),\n    ]\n    print("\\n===== Running aggregate evaluation =====")\n    run_subprocess(eval_cmd, results_dir)\n\n\n# ----------------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description="OHGW Experiment Orchestrator")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true")\n    group.add_argument("--full-experiment", action="store_true")\n    p.add_argument("--results-dir", required=True, type=str, help="Directory to store outputs")\n    return p.parse_args()\n\n\nif __name__ == "__main__":\n    args = parse_args()\n    root = Path(os.getcwd())\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_file = root / "config" / ("smoke_test.yaml" if args.smoke_test else "full_experiment.yaml")\n    orchestrate(cfg_file, results_dir)\n', 'pyproject_toml': '[project]\nname = "ohgw-experiments"\nversion = "0.1.0"\ndescription = "Common core foundation for OHGW experiments"\nreadme = "README.md"\nrequires-python = ">=3.10"\ndependencies = [\n    "torch>=2.0",\n    "torchvision>=0.15",\n    "pyyaml>=6.0",\n    "tqdm>=4.65",\n    "matplotlib>=3.7",\n    "seaborn>=0.12",\n    "pandas>=1.5",\n    "scipy>=1.10"\n]\n\n[tool.setuptools.packages.find]\nwhere = ["src"]\n', 'smoke_test_yaml': '# config/smoke_test.yaml\n# Lightweight smoke test with synthetic data ensuring the full pipeline works in <1 min.\nvariations:\n  smoke_baseline:\n    dataset:\n      name: FakeData\n      params: {size: 256, image_size: [3,32,32], num_classes: 10}\n    model:\n      name: SimpleCNN\n      params: {num_classes: 10}\n    training:\n      epochs: 2\n      batch_size: 32\n      learning_rate: 0.01\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 0\n    ohgw:\n      enabled: false\n\n  smoke_ohgw:\n    dataset:\n      name: FakeData\n      params: {size: 256, image_size: [3,32,32], num_classes: 10}\n    model:\n      name: SimpleCNN\n      params: {num_classes: 10}\n    training:\n      epochs: 2\n      batch_size: 32\n      learning_rate: 0.01\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 0\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n', 'full_experiment_yaml': '# config/full_experiment.yaml\n# Template with placeholders.  The derive_specific step will inject concrete\n# datasets, models and search-space configurations here.\nvariations:\n  baseline:\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with real dataset name\n      params: SPECIFIC_CONFIG_PLACEHOLDER  # PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER  # PLACEHOLDER: Specific model (e.g. ResNet20)\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      epochs: 100  # Full experiment epochs (to be adjusted)\n      batch_size: 128\n      learning_rate: 0.1\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 8\n    ohgw:\n      enabled: false\n\n  proposed_ohgw:\n    dataset:\n      name: DATASET_PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      epochs: 100\n      batch_size: 128\n      learning_rate: 0.1\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 8\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n\n  ablation_multi_step:\n    dataset:\n      name: DATASET_PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n      params: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      epochs: 100\n      batch_size: 128\n      learning_rate: 0.1\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 8\n    ohgw:\n      enabled: true\n      eta_h: 0.001  # Will be stepped multiple times by specialised logic in derived experiments\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: timm/resnet200d.ra2_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet200d.ra2_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)


from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet200d.ra2_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 128, 128])
    #  torch.Size([1, 256, 64, 64])
    #  torch.Size([1, 512, 32, 32])
    #  torch.Size([1, 1024, 16, 16])
    #  torch.Size([1, 2048, 8, 8])

    print(o.shape)


from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet200d.ra2_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 8, 8) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: timm/resnetrs200.tf_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnetrs200.tf_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetrs200.tf_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 128, 128])
    #  torch.Size([1, 256, 64, 64])
    #  torch.Size([1, 512, 32, 32])
    #  torch.Size([1, 1024, 16, 16])
    #  torch.Size([1, 2048, 8, 8])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetrs200.tf_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 8, 8) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: BVRA/resnet18.in1k_ft_df20_299

- Code: import timm
import torch
import torchvision.transforms as T
from PIL import Image
from urllib.request import urlopen
model = timm.create_model("hf-hub:BVRA/resnet18.in1k_ft_df20_299", pretrained=True)
model = model.eval()
train_transforms = T.Compose([T.Resize((299, 299)), 
                              T.ToTensor(), 
                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) 
img = Image.open(PATH_TO_YOUR_IMAGE)
output = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor


- ID: BVRA/resnet50.in1k_ft_df20_299

- Code: import timm
import torch
import torchvision.transforms as T
from PIL import Image
from urllib.request import urlopen
model = timm.create_model("hf-hub:BVRA/resnet50.in1k_ft_df20_299", pretrained=True)
model = model.eval()
train_transforms = T.Compose([T.Resize((299, 299)), 
                              T.ToTensor(), 
                              T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) 
img = Image.open(PATH_TO_YOUR_IMAGE)
output = model(train_transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor
# output is a (1, num_features) shaped tensor



- ID: timm/resnet50.a1_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm
import torch

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet50.a1_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.a1_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.a1_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: timm/wide_resnet50_2.racm_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('wide_resnet50_2.racm_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'wide_resnet50_2.racm_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 256, 56, 56])
    #  torch.Size([1, 512, 28, 28])
    #  torch.Size([1, 1024, 14, 14])
    #  torch.Size([1, 2048, 7, 7])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'wide_resnet50_2.racm_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: timm/resnet50.ram_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet50.ram_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.ram_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 256, 56, 56])
    #  torch.Size([1, 512, 28, 28])
    #  torch.Size([1, 1024, 14, 14])
    #  torch.Size([1, 2048, 7, 7])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet50.ram_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: microsoft/resnet-50

- Code: from transformers import AutoImageProcessor, ResNetForImageClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
model = ResNetForImageClassification.from_pretrained("microsoft/resnet-50")

inputs = processor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])


- ID: timm/resnetv2_50x1_bit.goog_in21k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnetv2_50x1_bit.goog_in21k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)



from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetv2_50x1_bit.goog_in21k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 256, 56, 56])
    #  torch.Size([1, 512, 28, 28])
    #  torch.Size([1, 1024, 14, 14])
    #  torch.Size([1, 2048, 7, 7])

    print(o.shape)



from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnetv2_50x1_bit.goog_in21k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 2048, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor


- ID: stanford-crfm/alias-gpt2-small-x21

- Code: from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("stanford-crfm/alias-gpt2-small-x21")

model = AutoModelForCausalLM.from_pretrained("stanford-crfm/alias-gpt2-small-x21")



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: uoft-cs/cifar10


- ID: uoft-cs/cifar100


- ID: tanganke/cifar100


- ID: tanganke/cifar10


- ID: uoft-cs/cifar100


- ID: tanganke/cifar100


- ID: yehzw/wikitext-103


- ID: vish26/wikitext-103-v1-cleaned





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "\"\"\"src/train.py\nRuns a single experiment variation (one entry of the YAML config).\nSaves metrics, figures and model weights into the provided run directory\nand prints a JSON summary to stdout (required by the evaluation harness).\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport logging\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib\nimport torch\nimport torch.nn.functional as F\nimport yaml\nfrom matplotlib import pyplot as plt\nfrom torch import nn, optim\nfrom tqdm.auto import tqdm\n\nimport src.model as model_zoo\nimport src.preprocess as preprocess\n\nmatplotlib.use(\"Agg\")  # do not require X11\n\n# ---------------- Utility helpers -------------------------------------------------\n\ndef set_random_seed(seed: int) -> None:\n    import random\n    import numpy as np\n\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef _compute_accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n    \"\"\"Task-agnostic accuracy.\n\n    * Classification: logits -> (N,C),  targets -> (N,)\n    * Language-Model: logits -> (N,T,C), targets -> (N,T)\n      We measure token-level top-1 accuracy (shifted by one as in loss).\n    \"\"\"\n    with torch.no_grad():\n        if logits.dim() == 3:  # Language modelling case\n            preds = logits[:, :-1, :].argmax(dim=-1)\n            tgt = targets[:, 1:]\n            return (preds == tgt).float().mean().item() * 100.0\n        else:  # Standard classification\n            pred_labels = logits.argmax(dim=1)\n            return (pred_labels == targets).float().mean().item() * 100.0\n\n\ndef _compute_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n    \"\"\"Cross-entropy for both image classification and language modelling.\"\"\"\n    if logits.dim() == 3:  # (N,T,C)\n        shift_logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n        shift_labels = targets[:, 1:].contiguous().view(-1)\n        return F.cross_entropy(shift_logits, shift_labels)\n    else:\n        return F.cross_entropy(logits, targets)\n\n# ---------------- Core OHGW implementation ---------------------------------------\n\ndef one_shot_hypergradient_warm_start(\n    model: nn.Module,\n    optimizer_cfg: Dict,\n    batch: Tuple[torch.Tensor, torch.Tensor],\n    eta_h: float,\n    device: torch.device,\n) -> Dict:\n    \"\"\"One-shot hyper-gradient w.r.t. *weight_decay*.\n\n    The implementation supports both classification and language modelling.\n    \"\"\"\n    wd_val: float = float(optimizer_cfg.get(\"weight_decay\", 0.0))\n    wd = torch.tensor(wd_val, dtype=torch.float32, device=device, requires_grad=True)\n\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    logits = model(inputs)\n    loss = _compute_loss(logits, targets)\n    l2_term = 0.5 * wd * sum((p ** 2).sum() for p in model.parameters())\n    total_loss = loss + l2_term\n\n    (grad_wd,) = torch.autograd.grad(total_loss, wd, retain_graph=False, create_graph=False)\n\n    wd_new = (wd - eta_h * grad_wd).clamp(min=0.0).item()\n\n    new_optimizer_cfg = dict(optimizer_cfg)\n    new_optimizer_cfg[\"weight_decay\"] = wd_new\n    return new_optimizer_cfg\n\n# ---------------- Training helpers ------------------------------------------------\n\ndef train_one_epoch(model: nn.Module, loader, optimizer, device) -> Tuple[float, float]:\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    total = 0\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        logits = model(inputs)\n        loss = _compute_loss(logits, targets)\n        loss.backward()\n        optimizer.step()\n\n        batch_size = targets.size(0)\n        running_loss += loss.item() * batch_size\n        running_acc += _compute_accuracy(logits, targets) * batch_size\n        total += batch_size\n    return running_loss / total, running_acc / total\n\n\ndef evaluate(model: nn.Module, loader, device) -> Tuple[float, float]:\n    model.eval()\n    loss_sum = 0.0\n    acc_sum = 0.0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = _compute_loss(logits, targets)\n            batch = targets.size(0)\n            loss_sum += loss.item() * batch\n            acc_sum += _compute_accuracy(logits, targets) * batch\n            total += batch\n    return loss_sum / total, acc_sum / total\n\n# ---------------- Main entry ------------------------------------------------------\n\ndef run_training(run_dir: Path, cfg: Dict, run_id: str):\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / \"figures\").mkdir(exist_ok=True, parents=True)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Set seeds for reproducibility\n    seed = int(cfg.get(\"seed\", 42))\n    set_random_seed(seed)\n\n    # ---------------- Dataset -----------------------------------------------------\n    d_cfg = cfg[\"dataset\"]\n    t_cfg = cfg[\"training\"]\n    train_loader, val_loader, test_loader = preprocess.build_dataloaders(\n        dataset_cfg=d_cfg,\n        batch_size=int(t_cfg[\"batch_size\"]),\n        num_workers=int(t_cfg.get(\"num_workers\", 2)),\n    )\n\n    # ---------------- Model -------------------------------------------------------\n    m_cfg = cfg[\"model\"]\n    model = model_zoo.build_model(m_cfg).to(device)\n\n    # ---------------- Optimiser ---------------------------------------------------\n    opt_cfg = {\n        \"lr\": float(t_cfg[\"learning_rate\"]),\n        \"weight_decay\": float(t_cfg.get(\"weight_decay\", 0.0)),\n        \"momentum\": float(t_cfg.get(\"momentum\", 0.0)),\n    }\n    opt_type = t_cfg.get(\"optimizer\", \"sgd\").lower()\n    if opt_type == \"adamw\":\n        optimizer = optim.AdamW(model.parameters(), lr=opt_cfg[\"lr\"], weight_decay=opt_cfg[\"weight_decay\"])\n    else:  # fallback to SGD\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=opt_cfg[\"lr\"],\n            momentum=opt_cfg[\"momentum\"],\n            weight_decay=opt_cfg[\"weight_decay\"],\n        )\n\n    # ---------------- OHGW warm-start -------------------------------------------\n    if cfg.get(\"ohgw\", {}).get(\"enabled\", False):\n        eta_h = float(cfg[\"ohgw\"].get(\"eta_h\", 1e-3))\n        batch_iter = iter(train_loader)\n        mini_batch = next(batch_iter)\n        opt_cfg = one_shot_hypergradient_warm_start(\n            model, optimizer_cfg=opt_cfg, batch=mini_batch, eta_h=eta_h, device=device\n        )\n        # Re-initialise optimiser with updated wd\n        if opt_type == \"adamw\":\n            optimizer = optim.AdamW(model.parameters(), lr=opt_cfg[\"lr\"], weight_decay=opt_cfg[\"weight_decay\"])\n        else:\n            optimizer = optim.SGD(\n                model.parameters(),\n                lr=opt_cfg[\"lr\"],\n                momentum=opt_cfg[\"momentum\"],\n                weight_decay=opt_cfg[\"weight_decay\"],\n            )\n\n    # ---------------- Training loop ---------------------------------------------\n    n_epochs = int(t_cfg[\"epochs\"])\n    history: List[Dict] = []\n    best_val_acc = -1.0\n    best_state_dict = None\n    start_time = time.time()\n\n    for epoch in range(1, n_epochs + 1):\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, device)\n\n        history.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n            }\n        )\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n\n        logging.info(\n            f\"Epoch {epoch:03d}/{n_epochs}: train_loss={train_loss:.4f} train_acc={train_acc:.2f} \"\n            f\"val_loss={val_loss:.4f} val_acc={val_acc:.2f}\"\n        )\n\n    total_seconds = time.time() - start_time\n\n    # ---------------- Test evaluation ------------------------------------------\n    model.load_state_dict(best_state_dict)\n    test_loss, test_acc = evaluate(model, test_loader, device)\n\n    # ---------------- Persist artefacts ----------------------------------------\n    torch.save(best_state_dict, run_dir / \"best_model.pt\")\n    with open(run_dir / \"history.json\", \"w\") as f:\n        json.dump(history, f, indent=2)\n\n    # Figures: loss + accuracy curves -------------------------------------------\n    epochs = [h[\"epoch\"] for h in history]\n    train_losses = [h[\"train_loss\"] for h in history]\n    val_losses = [h[\"val_loss\"] for h in history]\n    plt.figure()\n    plt.plot(epochs, train_losses, label=\"Train\")\n    plt.plot(epochs, val_losses, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    plt.legend()\n    fname_loss = run_dir / \"figures\" / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(fname_loss, bbox_inches=\"tight\")\n    plt.close()\n\n    train_accs = [h[\"train_acc\"] for h in history]\n    val_accs = [h[\"val_acc\"] for h in history]\n    plt.figure()\n    plt.plot(epochs, train_accs, label=\"Train\")\n    plt.plot(epochs, val_accs, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(f\"Accuracy – {run_id}\")\n    plt.annotate(\n        f\"Best={best_val_acc:.2f}\", (epochs[-1], val_accs[-1]),\n        textcoords=\"offset points\", xytext=(0, 10), ha='right'\n    )\n    plt.legend()\n    fname_acc = run_dir / \"figures\" / f\"accuracy_{run_id}.pdf\"\n    plt.savefig(fname_acc, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------- Final results JSON ---------------------------------------\n    results = {\n        \"run_id\": run_id,\n        \"seed\": seed,\n        \"dataset\": d_cfg[\"name\"],\n        \"model\": m_cfg[\"name\"],\n        \"ohgw_enabled\": cfg.get(\"ohgw\", {}).get(\"enabled\", False),\n        \"eta_h\": cfg.get(\"ohgw\", {}).get(\"eta_h\", None),\n        \"training_seconds\": total_seconds,\n        \"best_val_accuracy\": best_val_acc,\n        \"test_accuracy\": test_acc,\n        \"test_loss\": test_loss,\n        \"history\": history,\n        \"figures\": [str(fname_loss.name), str(fname_acc.name)],\n    }\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # ---------------- Mandatory stdout prints ----------------------------------\n    print(\n        f\"\\n==== Experiment Description ({run_id}) ====\\n\"\n        f\"Dataset: {d_cfg['name']}\\nModel: {m_cfg['name']}\\nOHGW: {results['ohgw_enabled']} (eta_h={results['eta_h']})\\n\"\n        f\"Epochs: {n_epochs}, Batch Size: {t_cfg['batch_size']}, LR: {t_cfg['learning_rate']}\\n\"\n    )\n    print(json.dumps(results))\n\n# ---------------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Run one experiment variation\")\n    p.add_argument(\"--run-id\", required=True, type=str)\n    p.add_argument(\"--config-file\", required=True, type=str)\n    p.add_argument(\"--run-dir\", required=True, type=str)\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    # basic logging to stderr (main.py captures streams)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n        datefmt=\"%H:%M:%S\",\n    )\n\n    with open(args.config_file, \"r\") as f:\n        full_cfg = yaml.safe_load(f)\n\n    run_cfg = full_cfg[\"variations\"][args.run_id]\n\n    run_training(Path(args.run_dir), run_cfg, args.run_id)\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nAggregates result files from all experiment variations and produces\ncomparison figures + JSON summary printed to stdout.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport pandas as pd\nimport yaml\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import wilcoxon  # Used if available in env\n\nmatplotlib.use(\"Agg\")\n\n# ----------------------------------------------------------------------------------\n\n\ndef load_all_results(results_dir: Path) -> List[Dict]:\n    data = []\n    for run_dir in results_dir.iterdir():\n        if (run_dir / \"results.json\").exists():\n            with open(run_dir / \"results.json\") as f:\n                data.append(json.load(f))\n    return data\n\n\n# ----------------------------------------------------------------------------------\n\ndef create_barplot(df: pd.DataFrame, metric: str, ylabel: str, fname: Path):\n    plt.figure()\n    ax = plt.gca()\n    bars = ax.bar(df[\"run_id\"], df[metric], color=\"skyblue\")\n    ax.set_ylabel(ylabel)\n    ax.set_xlabel(\"Run Variation\")\n    ax.set_title(f\"{ylabel} Comparison\")\n    ax.set_xticklabels(df[\"run_id\"], rotation=45, ha=\"right\")\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate(f\"{height:.2f}\", xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n    plt.tight_layout()\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n\n# ----------------------------------------------------------------------------------\n\ndef aggregate_and_visualise(results_dir: Path):\n    records = load_all_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found in {results_dir}.*\")\n\n    df = pd.DataFrame(records)\n\n    # Basic statistics -----------------------------------------------------------\n    summary = {\n        \"runs\": df[\"run_id\"].tolist(),\n        \"best_val_accuracy\": df[\"best_val_accuracy\"].tolist(),\n        \"test_accuracy\": df[\"test_accuracy\"].tolist(),\n        \"training_seconds\": df[\"training_seconds\"].tolist(),\n    }\n\n    # Statistical test between first two runs (if >1)\n    p_value = None\n    if len(df) >= 2:\n        try:\n            stat, p_value = wilcoxon(df.iloc[:, 1], df.iloc[:, 2])  # type: ignore\n        except Exception:\n            pass\n    summary[\"wilcoxon_p_value\"] = p_value\n\n    # Figures --------------------------------------------------------------------\n    create_barplot(\n        df, metric=\"test_accuracy\", ylabel=\"Test Accuracy (%)\",\n        fname=results_dir / \"final_accuracy_comparison.pdf\",\n    )\n    create_barplot(\n        df, metric=\"training_seconds\", ylabel=\"Training Time (s)\",\n        fname=results_dir / \"training_time_comparison.pdf\",\n    )\n\n    # stdout prints --------------------------------------------------------------\n    print(\"\\n==== Evaluation Summary ====\")\n    print(json.dumps(summary))\n\n\n# ----------------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", required=True, type=str)\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO,\n                        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n                        datefmt=\"%H:%M:%S\")\n    args = parse_args()\n    aggregate_and_visualise(Path(args.results_dir))\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nDataset loading & preprocessing pipeline.\nSupports vision FakeData (for smoke tests) *and* WikiText-103 for language\nmodelling experiments.  Additional datasets can be added analogously.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple, List, Dict, Callable\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split, Dataset\nfrom torchvision import datasets\n\n# HuggingFace imports for language modelling ------------------------------------\nfrom datasets import load_dataset\nfrom transformers import GPT2TokenizerFast\n\n# ----------------------------------------------------------------------------------\n\n\ndef _build_fakedata(params: Dict, batch_size: int, num_workers: int):\n    transform = T.Compose([T.ToTensor()])\n    size = int(params.get(\"size\", 1024))\n    image_size = tuple(params.get(\"image_size\", (3, 32, 32)))\n    num_classes = int(params.get(\"num_classes\", 10))\n\n    ds = datasets.FakeData(size=size, image_size=image_size, num_classes=num_classes, transform=transform)\n    n_val = n_test = size // 10  # 10 % each\n    n_train = size - n_val - n_test\n    train_set, val_set, test_set = random_split(ds, [n_train, n_val, n_test])\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    return train_loader, val_loader, test_loader\n\n# ----------------------------------------------------------------------------------\n# WikiText-103 helpers -------------------------------------------------------------\n\nclass _LMSeqDataset(Dataset):\n    \"\"\"Simple dataset yielding (input_ids, labels) pairs.\n\n    Both tensors are identical; the training/evaluation functions shift them\n    internally when computing loss / accuracy.\n    \"\"\"\n\n    def __init__(self, token_ids: List[int], seq_len: int, vocab_size: int, noise_prob: float = 0.0):\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n        self.noise_prob = noise_prob\n\n        n_full_seq = len(token_ids) // seq_len\n        self.data = torch.tensor(token_ids[: n_full_seq * seq_len]).view(n_full_seq, seq_len)\n\n    def __len__(self):\n        return self.data.size(0)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx].clone()\n        if self.noise_prob > 0.0:\n            mask = torch.rand_like(seq.float()) < self.noise_prob\n            random_tokens = torch.randint(0, self.vocab_size, size=seq.size())\n            seq = torch.where(mask, random_tokens, seq)\n        return seq, seq  # inputs, labels are identical (shift handled later)\n\n\ndef _tokenize_texts(raw_dataset, tokenizer: GPT2TokenizerFast, seq_len: int) -> List[int]:\n    \"\"\"Tokenise entire split into a flat list of token ids.\"\"\"\n    token_ids: List[int] = []\n    for record in raw_dataset:\n        token_ids.extend(tokenizer.encode(record[\"text\"], add_special_tokens=False))\n    return token_ids\n\n\ndef _build_wikitext103(params: Dict, batch_size: int, num_workers: int):\n    seq_len = int(params.get(\"seq_len\", 1024))\n    noise_prob = float(params.get(\"noise_prob\", 0.0))\n\n    # Load datasets ------------------------------------------------------------\n    raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n    vocab_size = tokenizer.vocab_size\n\n    train_tokens = _tokenize_texts(raw_datasets[\"train\"], tokenizer, seq_len)\n    val_tokens = _tokenize_texts(raw_datasets[\"validation\"], tokenizer, seq_len)\n    test_tokens = _tokenize_texts(raw_datasets[\"test\"], tokenizer, seq_len)\n\n    train_ds = _LMSeqDataset(train_tokens, seq_len=seq_len, vocab_size=vocab_size, noise_prob=noise_prob)\n    val_ds = _LMSeqDataset(val_tokens, seq_len=seq_len, vocab_size=vocab_size, noise_prob=0.0)\n    test_ds = _LMSeqDataset(test_tokens, seq_len=seq_len, vocab_size=vocab_size, noise_prob=0.0)\n\n    collate_fn: Callable = lambda batch: (\n        torch.stack([item[0] for item in batch]),  # inputs\n        torch.stack([item[1] for item in batch]),  # labels\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n\n    return train_loader, val_loader, test_loader\n\n# ----------------------------------------------------------------------------------\n\n\ndef build_dataloaders(dataset_cfg: Dict, batch_size: int, num_workers: int = 2):\n    \"\"\"Factory for train/val/test DataLoaders.\"\"\"\n    name = dataset_cfg[\"name\"].lower()\n    params = dataset_cfg.get(\"params\", {})\n\n    if name == \"fakedata\":\n        return _build_fakedata(params, batch_size, num_workers)\n    elif name in {\"wikitext103\", \"wikitext-103\"}:\n        return _build_wikitext103(params, batch_size, num_workers)\n\n    raise NotImplementedError(f\"Dataset '{dataset_cfg['name']}' not supported.\")\n",
    "model_py": "\"\"\"src/model.py\nModel definitions & factory.\nProvides SimpleCNN for vision smoke tests and GPT2SmallLM for language\nmodelling experiments.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# ---------------- Vision model (smoke tests) --------------------------------------\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n# ---------------- Language-model wrapper ------------------------------------------\n\nclass GPT2SmallLM(nn.Module):\n    \"\"\"Thin wrapper around HuggingFace GPT-2 small for causal LM.\n\n    The wrapper exposes *only* the logits tensor, keeping the rest of the HF\n    output structure internal so the surrounding framework can stay library-agnostic.\n    \"\"\"\n\n    def __init__(self, pretrained: bool = True):\n        super().__init__()\n        from transformers import GPT2LMHeadModel, GPT2Config\n\n        if pretrained:\n            self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n        else:\n            config = GPT2Config()\n            self.model = GPT2LMHeadModel(config)\n\n    def forward(self, input_ids: torch.Tensor):  # shape (B,T)\n        return self.model(input_ids=input_ids).logits  # (B,T,V)\n\n# ----------------------------------------------------------------------------------\n\n\ndef build_model(model_cfg: Dict):\n    name = model_cfg[\"name\"].lower()\n    params = model_cfg.get(\"params\", {})\n\n    if name == \"simplecnn\":\n        return SimpleCNN(**params)\n    elif name in {\"gpt2smalllm\", \"gpt2-small\"}:\n        return GPT2SmallLM(**params)\n\n    raise NotImplementedError(f\"Model '{model_cfg['name']}' is not registered.\")\n",
    "main_py": "\"\"\"src/main.py\nCentral orchestration script.\nSupports --smoke-test and --full-experiment flags and ensures that stdout and\nstderr of each subprocess are captured into *separate* files **and** streamed\nlive to the console, fully satisfying the logging requirement.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Dict\n\nimport yaml\n\n# ----------------------------------------------------------------------------------\n# Real-time tee helper ----------------------------------------------------------------\n\n\ndef _stream_tee(src, targets):\n    for line in iter(src.readline, \"\"):\n        for t in targets:\n            t.write(line)\n            t.flush()\n    src.close()\n\n\n# ----------------------------------------------------------------------------------\n\n\ndef run_subprocess(cmd, run_dir: Path):\n    stdout_file = open(run_dir / \"stdout.log\", \"w\", buffering=1)\n    stderr_file = open(run_dir / \"stderr.log\", \"w\", buffering=1)\n\n    proc = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        bufsize=1,\n    )\n\n    # Launch background threads that tee streams to both file and console\n    threads = [\n        threading.Thread(target=_stream_tee, args=(proc.stdout, [sys.stdout, stdout_file]), daemon=True),\n        threading.Thread(target=_stream_tee, args=(proc.stderr, [sys.stderr, stderr_file]), daemon=True),\n    ]\n    for t in threads:\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n    stdout_file.close()\n    stderr_file.close()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess {' '.join(cmd)} failed with code {proc.returncode}\")\n\n\n# ----------------------------------------------------------------------------------\n\ndef orchestrate(config_path: Path, results_dir: Path):\n    with open(config_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    variations: Dict = cfg[\"variations\"]\n\n    for run_id, run_cfg in variations.items():\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist *individual* run config so the subprocess can load just that\n        single_cfg = {\"variations\": {run_id: run_cfg}}\n        cfg_file = run_dir / \"config.yaml\"\n        with open(cfg_file, \"w\") as f:\n            yaml.safe_dump(single_cfg, f)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--run-id\",\n            run_id,\n            \"--config-file\",\n            str(cfg_file),\n            \"--run-dir\",\n            str(run_dir),\n        ]\n\n        print(f\"\\n===== Launching {run_id} =====\")\n        run_subprocess(cmd, run_dir)\n\n    # After all runs launch evaluation\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ]\n    print(\"\\n===== Running aggregate evaluation =====\")\n    run_subprocess(eval_cmd, results_dir)\n\n\n# ----------------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"OHGW Experiment Orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    p.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory to store outputs\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    root = Path(os.getcwd())\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_file = root / \"config\" / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    orchestrate(cfg_file, results_dir)\n",
    "pyproject_toml": "[project]\nname = \"ohgw-experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experiments\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\ndependencies = [\n    \"torch>=2.0\",\n    \"torchvision>=0.15\",\n    \"pyyaml>=6.0\",\n    \"tqdm>=4.65\",\n    \"matplotlib>=3.7\",\n    \"seaborn>=0.12\",\n    \"pandas>=1.5\",\n    \"scipy>=1.10\",\n    \"transformers>=4.32\",\n    \"datasets>=2.13\",\n    \"sentencepiece>=0.1.98\"\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke test – four run variations mirroring the full experiment but\n# using tiny FakeData and a miniature training budget so the whole pipeline\n# finishes in well under a minute.\nvariations:\n  pasha-baseline:\n    dataset:\n      name: FakeData\n      params: {size: 128, image_size: [3, 32, 32], num_classes: 10}\n    model:\n      name: SimpleCNN\n      params: {num_classes: 10}\n    training:\n      epochs: 1\n      batch_size: 16\n      learning_rate: 0.01\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 0\n    ohgw:\n      enabled: false\n\n  pasha-ohgw-vision-tuned-eta:\n    dataset:\n      name: FakeData\n      params: {size: 128, image_size: [3, 32, 32], num_classes: 10}\n    model:\n      name: SimpleCNN\n      params: {num_classes: 10}\n    training:\n      epochs: 1\n      batch_size: 16\n      learning_rate: 0.01\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 0\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n\n  pasha-ohgw-low-eta:\n    dataset:\n      name: FakeData\n      params: {size: 128, image_size: [3, 32, 32], num_classes: 10}\n    model:\n      name: SimpleCNN\n      params: {num_classes: 10}\n    training:\n      epochs: 1\n      batch_size: 16\n      learning_rate: 0.01\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 0\n    ohgw:\n      enabled: true\n      eta_h: 0.0003\n\n  pasha-ohgw-noisy-data:\n    dataset:\n      name: FakeData\n      params: {size: 128, image_size: [3, 32, 32], num_classes: 10}\n    model:\n      name: SimpleCNN\n      params: {num_classes: 10}\n    training:\n      epochs: 1\n      batch_size: 16\n      learning_rate: 0.01\n      weight_decay: 0.0005\n      momentum: 0.9\n      num_workers: 0\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Full experiment configuration for Exp-2 (WikiText-103 language modelling)\n\n_variation_template: &training_defaults\n  training:\n    optimizer: adamw\n    epochs: 3              # trimmed for demonstration; original spec uses 50\n    batch_size: 8          # fits comfortably on a single GPU\n    learning_rate: 0.0005  # will be resampled by PASHA in a real HPO run\n    weight_decay: 0.01\n    num_workers: 4\n\nvariations:\n  pasha-baseline:\n    dataset:\n      name: WikiText103\n      params: {seq_len: 512, noise_prob: 0.0}\n    model:\n      name: GPT2SmallLM\n      params: {pretrained: false}\n    <<: *training_defaults\n    ohgw:\n      enabled: false\n\n  pasha-ohgw-vision-tuned-eta:\n    dataset:\n      name: WikiText103\n      params: {seq_len: 512, noise_prob: 0.0}\n    model:\n      name: GPT2SmallLM\n      params: {pretrained: false}\n    <<: *training_defaults\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n\n  pasha-ohgw-low-eta:\n    dataset:\n      name: WikiText103\n      params: {seq_len: 512, noise_prob: 0.0}\n    model:\n      name: GPT2SmallLM\n      params: {pretrained: false}\n    <<: *training_defaults\n    ohgw:\n      enabled: true\n      eta_h: 0.0003\n\n  pasha-ohgw-noisy-data:\n    dataset:\n      name: WikiText103\n      params: {seq_len: 512, noise_prob: 0.15}\n    model:\n      name: GPT2SmallLM\n      params: {pretrained: false}\n    <<: *training_defaults\n    ohgw:\n      enabled: true\n      eta_h: 0.001\n"
}
