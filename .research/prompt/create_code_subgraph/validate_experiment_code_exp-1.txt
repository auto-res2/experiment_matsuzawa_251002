
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating One-Shot Hyper-Gradient Warm-Starts (OHGW)

1. Core Hypotheses to Validate
   a. Efficiency: OHGW reduces wall-clock time and GPU hours needed by bandit-style schedulers to reach a preset performance threshold.
   b. Performance Preservation: OHGW does not hurt (and ideally preserves or slightly improves) the best final metric attainable by the underlying scheduler.
   c. Robustness & Variance: OHGW’s benefit is consistent across random seeds, search-space dimensionalities, data sets, model families and scheduler types.
   d. Generalization: The same one-line wrapper applies without retuning to tasks beyond image classification (e.g. language modelling, tabular, RL) and to both small- and large-scale training loops.
   e. Cost Overhead: Extra compute, memory and engineering overhead introduced by OHGW remain negligible (<5 % GPU-hours, <10 % peak-memory, ≤20 LoC integration).

2. Experiment Families (all experiments draw from one common pool of settings below)
   • Task Breadth: vision (CIFAR-10/100, ImageNet-1k), NLP (WikiText-103), tabular (UCI suite), RL (Atari).
   • Model Breadth: ResNet family, ViT, Transformer-LM, XGBoost, PPO-CNN.
   • Scheduler Breadth: ASHA, PASHA, DyHPO, Hyperband-BO, BOIL (if open-sourced).
   • Search-Space Breadth: 3–10 continuous hyper-parameters; mixed discrete+continuous cases to show neutrality to inapplicable params.
   • Scale Breadth: single-GPU up to 64-GPU distributed training (multi-node pools or simulated via concurrency on the 8×A100 machine).

3. Comparison Axes for Every Experiment
   • Baseline Scheduler (vanilla).
   • Baseline + Random Warm-Start in ∆ψ range (controls for mere perturbation).
   • Baseline + Multiple Hyper-Gradient Steps (ablation to check diminishing returns).
   • Scheduler-specific SoTA gradient-aware HPO if available (e.g. DyHPO, BOIL) to position OHGW competitively.

4. Metrics & Evaluation Protocol
   Primary quantitative metrics (reported as median ±IQR over ≥30 seeds):
      – T@τ: Wall-clock/GPU-hour to reach target score τ (task-specific; chosen so that vanilla reaches it within budget).
      – Best final validation/test score after fixed budget.
      – Compute Overhead: (Σ warm-start flop) ⁄ (total flop) and peak VRAM.
   Secondary diagnostics:
      – AUC of best-score-vs-time curve (overall sample efficiency).
      – Seed-wise variance of T@τ and final score.
      – Hyper-parameter trajectory statistics (mean shift & KL-divergence of posterior over ψ).
   Qualitative/visual:
      – Survival plots of promoted configurations per rung.
      – Heatmaps of hyper-gradient magnitude vs. eventual benefit.
   Statistical test: two-sided Wilcoxon signed-rank (α=0.05) on paired seeds.

5. Success Criteria (must hold in ≥75 % of experiment tuples)
   • ≥15 % median reduction in T@τ with p<0.05.
   • ≤0.2 % relative drop (or improvement) in best final score.
   • ≤5 % extra GPU-hours and ≤10 % extra peak VRAM.
   • Std-dev(T@τ) not inflated by more than 10 %.

6. Multi-Perspective Validation Plan
   • Efficiency: Use identical global budgets and identical seed lists; plot temporal efficiency curves and compute aggregated speed-up ratios.
   • Performance Preservation: Report final accuracy/F1/return and confidence intervals.
   • Robustness: Repeat each experiment block with different batch sizes, η_h values (10⁻⁴…10⁻²) and noisy labels; perform sensitivity analysis.
   • Generalization: Run a “zero-tuning transfer” study—apply the CIFAR-tuned η_h to NLP, RL and tabular tasks unchanged.
   • Ablations & Controls: Random perturbation, multistep hyper-gradient, frozen discrete params, partial gradient masking.
   • Scalability: Micro-benchmark warm-start time and memory for 1, 8, 64 GPUs using synthetic workloads.

7. Experimental Logistics
   • Central harness (Hydra + PyTorch Lightning) to register all trials, guarantee identical I/O pipelines and logging format (wandb/MLflow).
   • Dedicate 4×A100 per independent replicate to avoid resource contention; schedule via Slurm with cgroup accounting to record accurate GPU-hour usage.
   • Automated post-processing notebook generates unified tables, statistical tests and publication-ready plots.

8. Risk Mitigation & Contingency
   • If hyper-gradient extraction fails for exotic layers, fall back to finite-difference on ψ only (flag run but keep in aggregated stats).
   • If OHGW underperforms on discrete-heavy spaces, isolate continuous subset and document limitation.

This unified strategy guarantees that every forthcoming experiment—regardless of domain—collects commensurate evidence on efficiency, performance, robustness, generality and overhead, enabling a cohesive, multi-angle validation of OHGW’s claimed benefits.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all experimental runs stored in `results_dir/*/results.json`,\ncomputes summary statistics and generates cross-run comparison figures.\n\nUsage:\n    python -m src.evaluate --results-dir \u003cpath/to/experiments\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ----------------------------- Helper utilities ----------------------------- #\n\ndef load_run_results(results_path: str) -\u003e Dict[str, Any]:\n    with open(results_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\n\n# --------------------------------- Metrics ---------------------------------- #\n\ndef aggregate_metrics(results: List[Dict[str, Any]]) -\u003e Dict[str, Any]:\n    summary = {\n        \"num_runs\": len(results),\n        \"runs\": {r[\"run_id\"]: r[\"final_metrics\"] for r in results},\n    }\n    # Identify best run by validation accuracy\n    best_run = max(results, key=lambda r: r[\"final_metrics\"][\"val_acc\"])\n    summary[\"best_run_id\"] = best_run[\"run_id\"]\n    summary[\"best_val_acc\"] = best_run[\"final_metrics\"][\"val_acc\"]\n    return summary\n\n\n# -------------------------------- Figures ----------------------------------- #\n\ndef save_comparison_figures(results: List[Dict[str, Any]], results_dir: str) -\u003e None:\n    images_dir = Path(results_dir) / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Bar chart of final validation accuracy\n    run_ids = [r[\"run_id\"] for r in results]\n    val_accs = [r[\"final_metrics\"][\"val_acc\"] for r in results]\n\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=run_ids, y=val_accs, palette=\"viridis\")\n    for idx, val in enumerate(val_accs):\n        plt.text(idx, val + 0.005, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.ylabel(\"validation accuracy\")\n    plt.title(\"Final Validation Accuracy Across Runs\")\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy_comparison.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Line graph of validation accuracy over epochs\n    plt.figure(figsize=(6, 4))\n    for r in results:\n        epochs = [m[\"epoch\"] for m in r[\"epoch_metrics\"]]\n        val_accs_epoch = [m[\"val_acc\"] for m in r[\"epoch_metrics\"]]\n        plt.plot(epochs, val_accs_epoch, label=r[\"run_id\"])\n        plt.scatter(epochs[-1], val_accs_epoch[-1])\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"validation accuracy\")\n    plt.title(\"Validation Accuracy Trajectories\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy_trajectories.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# --------------------------------- Main ------------------------------------- #\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare experimental runs\")\n    parser.add_argument(\"--results-dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    # --------------------------------------------------------------------------------\n    # Load results.json from every run directory\n    # --------------------------------------------------------------------------------\n    result_files = glob(os.path.join(args.results_dir, \"*\", \"results.json\"))\n    if len(result_files) == 0:\n        raise FileNotFoundError(f\"No results.json files found under {args.results_dir}\")\n\n    results = [load_run_results(p) for p in sorted(result_files)]\n\n    # Aggregate and summarise\n    summary = aggregate_metrics(results)\n\n    # Generate figures\n    save_comparison_figures(results, args.results_dir)\n\n    # Final JSON summary to stdout\n    print(json.dumps(summary, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# Template for the complete experimental suite.  Concrete dataset / model /\n# scheduler definitions will be injected during the specialisation step.\n\nruns:\n  - run_id: EXPERIMENT_VARIATION_PLACEHOLDER_1  # will be replaced\n    seed: 0\n    dataset:\n      name: DATASET_PLACEHOLDER\n      # Additional dataset-specific parameters here\n    model:\n      name: MODEL_PLACEHOLDER\n      # Model-specific hyper-parameters here\n    optimizer:\n      type: SGD  # or Adam etc.\n      lr: SPECIFIC_LR_PLACEHOLDER\n    training:\n      batch_size: SPECIFIC_BATCHSIZE_PLACEHOLDER\n      epochs: SPECIFIC_EPOCHS_PLACEHOLDER\n\n  # Additional run variations can be added here\n\n", "main_py": "\"\"\"src/main.py\nCentral orchestrator that sequentially executes all run variations defined in\n`smoke_test.yaml` or `full_experiment.yaml`, captures their output \u0026 error\nstreams, and finally triggers evaluation.\n\nCLI:\n    python -m src.main --smoke-test --results-dir \u003cpath\u003e\n    python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport yaml\n\n# ------------------------- Subprocess streaming utils ----------------------- #\n\ndef _stream(pipe, tee_files):\n    \"\"\"Read lines from a pipe and write them to multiple file-like objects.\"\"\"\n    with pipe:\n        for line in iter(pipe.readline, b\"\"):\n            decoded = line.decode()\n            for f in tee_files:\n                f.write(decoded)\n                f.flush()\n            # Always mirror to main process\u0027 stdout / stderr\n            if tee_files[0].name.endswith(\"stdout.log\"):\n                sys.stdout.write(decoded)\n                sys.stdout.flush()\n            else:\n                sys.stderr.write(decoded)\n                sys.stderr.flush()\n\n\ndef _run_subprocess(cmd: list[str], stdout_log: Path, stderr_log: Path) -\u003e int:\n    \"\"\"Launch subprocess, tee stdout/err to provided log files *and* console.\"\"\"\n    stdout_log.parent.mkdir(parents=True, exist_ok=True)\n    stderr_log.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(stdout_log, \"w\", encoding=\"utf-8\") as fout, open(\n        stderr_log, \"w\", encoding=\"utf-8\") as ferr:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        t_out = threading.Thread(target=_stream, args=(proc.stdout, [fout]))\n        t_err = threading.Thread(target=_stream, args=(proc.stderr, [ferr]))\n        t_out.start()\n        t_err.start()\n        t_out.join()\n        t_err.join()\n        return proc.wait()\n\n\n# --------------------------------- Main flow -------------------------------- #\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Run experiment suite\")\n    grp = parser.add_mutually_exclusive_group(required=True)\n    grp.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke test\")\n    grp.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Output directory\")\n    args = parser.parse_args()\n\n    root_results_dir = Path(args.results_dir)\n    root_results_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = root_results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    config_path = (\n        Path(\"config/smoke_test.yaml\") if args.smoke_test else Path(\"config/full_experiment.yaml\")\n    )\n\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        suite_cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    runs = suite_cfg.get(\"runs\", [])\n    if len(runs) == 0:\n        raise ValueError(\"No runs found in configuration file\")\n\n    print(f\"Running {len(runs)} experiment variations defined in {config_path}\\n\", flush=True)\n\n    for run_cfg in runs:\n        run_id = run_cfg[\"run_id\"]\n        print(f\"========== Starting run: {run_id} ==========\")\n\n        run_dir = root_results_dir / run_id\n        run_dir.mkdir(exist_ok=True)\n\n        # Persist individual run config to file so that train.py can read it\n        run_cfg_path = run_dir / \"config.yaml\"\n        with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f_run:\n            yaml.safe_dump(run_cfg, f_run)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config-file\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(root_results_dir),\n        ]\n\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        exit_code = _run_subprocess(cmd, stdout_log, stderr_log)\n        if exit_code != 0:\n            raise RuntimeError(f\"Run {run_id} failed with exit code {exit_code}\")\n\n    # After all runs, perform evaluation \u0026 visualisation\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(root_results_dir),\n    ]\n    exit_code = subprocess.call(eval_cmd)\n    if exit_code != 0:\n        raise RuntimeError(\"Evaluation script failed\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel architecture registry.  Contains *baseline* implementation that works for\nany generic classification smoke test.  Real experiments will replace or extend\nthese models.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# ----------------------------- Base classifier ------------------------------ #\n\nclass BaseClassifier(nn.Module):\n    \"\"\"Very small CNN suitable for 1\u00d728\u00d728 images (MNIST-like).\"\"\"\n\n    def __init__(self, in_channels: int = 1, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 8, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:  # type: ignore[override]\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# ------------------------------- Registry ----------------------------------- #\n\n_MODEL_REGISTRY: Dict[str, nn.Module] = {\n    \"BASE_CLASSIFIER\": BaseClassifier,\n    # ---------------------------------------------------------------------\n    # PLACEHOLDER: Additional models will be registered here in later phase\n    # ---------------------------------------------------------------------\n}\n\n\ndef create_model(cfg: dict) -\u003e nn.Module:\n    mdl_cfg = cfg.get(\"model\", {})\n    name = mdl_cfg.get(\"name\", \"BASE_CLASSIFIER\").upper()\n    if name not in _MODEL_REGISTRY:\n        raise NotImplementedError(\n            f\"Model \u0027{name}\u0027 unknown to common core \u2013 must be provided in specialising step.\"\n        )\n    kwargs = mdl_cfg.get(\"kwargs\", {})\n    return _MODEL_REGISTRY[name](**kwargs)\n\n\ndef save_model(model: nn.Module, path: str) -\u003e None:\n    torch.save(model.state_dict(), path)\n\n\ndef load_model(model_class_name: str, path: str, device: torch.device) -\u003e nn.Module:\n    model_class_name = model_class_name.upper()\n    if model_class_name not in _MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model class \u0027{model_class_name}\u0027 for loading\")\n    model = _MODEL_REGISTRY[model_class_name]()\n    state_dict = torch.load(path, map_location=device)\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n    return model\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data loading and preprocessing pipeline with *dataset placeholders*.\nOnly the generic mechanics are fully implemented here.  Any concrete dataset\nintegration must be supplied during the dataset-specific derivation step.\n\nCurrent supported placeholder dataset:\n    RANDOM_PLACEHOLDER \u2013 synthetic data used for smoke tests.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Tuple, Any\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------------- Placeholders ------------------------------ #\n\nclass RandomClassificationDataset(Dataset):\n    \"\"\"Synthetic dataset that produces random tensors for classification.\n\n    This is *only* intended for smoke tests; real experiments must provide\n    a concrete dataset loader by replacing the placeholder in config.\n    \"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, ...], num_classes: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n\n    def __len__(self) -\u003e int:\n        return self.num_samples\n\n    def __getitem__(self, idx: int) -\u003e Tuple[torch.Tensor, torch.Tensor]:\n        x = torch.randn(self.input_shape)\n        y = torch.randint(0, self.num_classes, (1,)).long().squeeze()\n        return x, y\n\n\n# -------------------------- Dataloader factory ------------------------------ #\n\ndef _create_placeholder_dataset(cfg: dict, split: str):\n    ds_cfg = cfg[\"dataset\"]\n    name = ds_cfg[\"name\"].upper()\n\n    if name == \"RANDOM_PLACEHOLDER\":\n        n_train = int(ds_cfg.get(\"num_samples\", 1024))\n        n_val = max(1, math.ceil(n_train * 0.2))\n        if split == \"train\":\n            dataset = RandomClassificationDataset(\n                num_samples=n_train,\n                input_shape=tuple(ds_cfg.get(\"input_shape\", [1, 28, 28])),\n                num_classes=int(ds_cfg.get(\"num_classes\", 10)),\n            )\n        else:\n            dataset = RandomClassificationDataset(\n                num_samples=n_val,\n                input_shape=tuple(ds_cfg.get(\"input_shape\", [1, 28, 28])),\n                num_classes=int(ds_cfg.get(\"num_classes\", 10)),\n            )\n        return dataset\n\n    # ---------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    # ---------------------------------------------------------------------\n    raise NotImplementedError(\n        f\"Dataset \u0027{name}\u0027 not implemented in common core \u2013 must be provided in specialising step.\"\n    )\n\n\ndef get_dataloader(cfg: dict, split: str = \"train\") -\u003e DataLoader:\n    \"\"\"Return a PyTorch DataLoader for requested split.\n\n    Parameters\n    ----------\n    cfg : dict\n        Run-level configuration dictionary.\n    split : str\n        One of {\"train\", \"val\"}.\n    \"\"\"\n    assert split in {\"train\", \"val\"}, \"split must be \u0027train\u0027 or \u0027val\u0027\"\n    ds = _create_placeholder_dataset(cfg, split)\n\n    batch_size = int(cfg.get(\"training\", {}).get(\"batch_size\", 32))\n    shuffle = split == \"train\"\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n", "pyproject_toml": "[project]\nname = \"ohgw-common-core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experimental suite\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n    \"torch\u003e=2.1.0\",\n    \"torchvision\u003e=0.16.0\",\n    \"pyyaml\u003e=6.0\",\n    \"numpy\u003e=1.23\",\n    \"matplotlib\u003e=3.8\",\n    \"seaborn\u003e=0.13\",\n    \"tqdm\u003e=4.66\",\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight configuration used by CI smoke test.  Uses synthetic data \u0026 a\n# minimal model to keep runtime and memory requirements negligible.\n\nruns:\n  - run_id: smoke_random_model\n    seed: 0\n    dataset:\n      name: RANDOM_PLACEHOLDER  # PLACEHOLDER: real dataset name will replace this\n      input_shape: [1, 28, 28]\n      num_classes: 10\n      num_samples: 256\n    model:\n      name: base_classifier  # Uses the built-in tiny CNN for smoke tests\n      kwargs: {}\n    optimizer:\n      type: SGD\n      lr: 0.01\n      momentum: 0.9\n    training:\n      batch_size: 32\n      epochs: 2\n", "train_py": "\"\"\"src/train.py\nCore training script for a single experimental run.\nImplements end-to-end training, validation, checkpointing, figure generation and\nmetrics logging.  ALL dataset / model specialisation must be supplied via the\nconfiguration handed in by main.py.\n\nUsage (called only by main.py):\n    python -m src.train \\\n        --config-file \u003cpath/to/config.yaml\u003e \\\n        --results-dir \u003cpath/to/results_dir\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport yaml\n\nfrom . import preprocess as pp  # type: ignore\nfrom . import model as mdl  # type: ignore\n\n# ----------------------------- Utility functions ----------------------------- #\n\ndef set_seed(seed: int) -\u003e None:\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Compute top-1 accuracy for classification.\"\"\"\n    pred_label = pred.argmax(dim=1)\n    correct = (pred_label == target).sum().item()\n    return correct / target.size(0)\n\n\n# --------------------------------- Training --------------------------------- #\n\ndef train_one_epoch(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n) -\u003e tuple[float, float]:\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        running_acc += accuracy(outputs.detach(), targets.detach()) * inputs.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = running_acc / len(dataloader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef validate(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    device: torch.device,\n) -\u003e tuple[float, float]:\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n            running_acc += accuracy(outputs, targets) * inputs.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = running_acc / len(dataloader.dataset)\n    return epoch_loss, epoch_acc\n\n\n# --------------------------------- Figures ---------------------------------- #\n\ndef save_training_curves(\n    metrics: list[dict[str, float]],\n    run_id: str,\n    results_dir: str,\n) -\u003e None:\n    images_dir = Path(results_dir) / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    epochs = [m[\"epoch\"] for m in metrics]\n    train_losses = [m[\"train_loss\"] for m in metrics]\n    val_losses = [m[\"val_loss\"] for m in metrics]\n    train_accs = [m[\"train_acc\"] for m in metrics]\n    val_accs = [m[\"val_acc\"] for m in metrics]\n\n    sns.set_style(\"whitegrid\")\n\n    # Loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, train_losses, label=\"train_loss\")\n    plt.plot(epochs, val_losses, label=\"val_loss\")\n    plt.scatter(epochs[-1], val_losses[-1], color=\"red\")\n    plt.text(\n        epochs[-1],\n        val_losses[-1],\n        f\"{val_losses[-1]:.3f}\",\n        fontsize=8,\n        verticalalignment=\"bottom\",\n    )\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.title(f\"Training/Validation Loss ({run_id})\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"training_loss_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Accuracy curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, train_accs, label=\"train_acc\")\n    plt.plot(epochs, val_accs, label=\"val_acc\")\n    plt.scatter(epochs[-1], val_accs[-1], color=\"red\")\n    plt.text(\n        epochs[-1],\n        val_accs[-1],\n        f\"{val_accs[-1]*100:.2f}%\",\n        fontsize=8,\n        verticalalignment=\"bottom\",\n    )\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"accuracy\")\n    plt.title(f\"Training/Validation Accuracy ({run_id})\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"accuracy_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# --------------------------------- Main ------------------------------------- #\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config-file\", required=True, type=str, help=\"Path to YAML config for this run\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Root directory for all experiment outputs\")\n    args = parser.parse_args()\n\n    # ---------------------------------------------------------------------\n    # Configuration \u0026 directories\n    # ---------------------------------------------------------------------\n    with open(args.config_file, \"r\", encoding=\"utf-8\") as f:\n        cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    run_id: str = cfg[\"run_id\"]\n    run_dir = Path(args.results_dir) / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    checkpoints_dir = run_dir / \"checkpoints\"\n    checkpoints_dir.mkdir(exist_ok=True)\n\n    # Persist a copy of the resolved configuration for reproducibility\n    with open(run_dir / \"config.yaml\", \"w\", encoding=\"utf-8\") as f_cfg_out:\n        yaml.safe_dump(cfg, f_cfg_out)\n\n    seed = int(cfg.get(\"seed\", 42))\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # Data pipeline\n    # ---------------------------------------------------------------------\n    train_loader = pp.get_dataloader(cfg, split=\"train\")\n    val_loader = pp.get_dataloader(cfg, split=\"val\")\n\n    # ---------------------------------------------------------------------\n    # Model \u0026 optimisation\n    # ---------------------------------------------------------------------\n    model = mdl.create_model(cfg).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    optim_cfg = cfg.get(\"optimizer\", {})\n    opt_type = optim_cfg.get(\"type\", \"SGD\").upper()\n    lr = float(optim_cfg.get(\"lr\", 0.01))\n    momentum = float(optim_cfg.get(\"momentum\", 0.9))\n    weight_decay = float(optim_cfg.get(\"weight_decay\", 0.0))\n\n    if opt_type == \"ADAM\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:  # Default to SGD\n        optimizer = optim.SGD(\n            model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n        )\n\n    epochs: int = int(cfg.get(\"training\", {}).get(\"epochs\", 10))\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    epoch_metrics: list[dict[str, float]] = []\n    best_val_acc = -1.0\n    start_time = time.time()\n\n    for epoch in range(1, epochs + 1):\n        train_loss, train_acc = train_one_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n        # Book-keeping\n        metric_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }\n        epoch_metrics.append(metric_dict)\n\n        # Checkpointing\n        ckpt_path = checkpoints_dir / f\"epoch_{epoch}.pt\"\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            },\n            ckpt_path,\n        )\n\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), run_dir / \"best_model.pt\")\n\n        # Progress feedback (the CI system captures this)\n        print(json.dumps({\"run_id\": run_id, \"epoch\": epoch, **metric_dict}), flush=True)\n\n    elapsed = time.time() - start_time\n\n    # Final serialization of metrics\n    final_metrics = epoch_metrics[-1]\n    results_summary: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"final_metrics\": final_metrics,\n        \"epoch_metrics\": epoch_metrics,\n        \"training_time_sec\": elapsed,\n    }\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as f_res:\n        json.dump(results_summary, f_res, indent=2)\n\n    # Generate publication-ready figures\n    save_training_curves(epoch_metrics, run_id, args.results_dir)\n\n    # Print the summary to stdout last \u2013 required by evaluation harness\n    print(json.dumps(results_summary), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-1
- Description: Objective / Hypothesis: Quantify the raw efficiency gains of One-Shot Hyper-Gradient Warm-Starts (OHGW) for classical image-classification hyper-parameter optimisation and disentangle the effect of merely perturbing the hyper-parameters from using the true hyper-gradient.

Domain & Task: Vision – CIFAR-10 image classification.

Models: ResNet-20 (main), ResNet-50 (sanity-check on a deeper network – evaluated only for the best scheduler variant after selection).

Schedulers under test: ASHA (official implementation in Ray-Tune v2.6).

Search space (5-dim continuous): log10-learning-rate, log10-weight-decay, momentum, RandAugment-magnitude, label-smoothing.

Run variations:
• asha-baseline – vanilla ASHA.
• asha-random-warm – perform one random Gaussian step (σ=0.01) in ψ instead of a hyper-gradient step (control).* 
• asha-ohgw-1step – proposed method (η_h=1e-3, one step).
• asha-ohgw-3step – same but apply three successive hyper-gradient steps (ablation for diminishing returns).

Datasets & Pre-processing: CIFAR-10, standard train/val/test split = 45k/5k/10k.  Per-image Z-score normalisation, RandAugment (m searchable), Cutout(16), label-smoothing (ε searchable).  Validation set is stratified.

Training loop: 200 epochs (budget for full training), Cosine LR, batch size 128, SGD-M.  Weight initialisation He-normal.  Mixed precision (AMP).  Early-stopping OFF (to reveal full curve).

Evaluation metrics:
Primary – T@93%: wall-clock hours & GPU hours needed to first reach 93% val accuracy.
Secondary – best test accuracy after 200 epochs; AUC(score-vs-time); overhead-FLOPs ratio; peak VRAM; seed-wise variance.

Protocol: 32 independent HPO replicates (seed-paired across variations).  Each replicate is allowed 12 A100-GPU hours (4 GPUs×3 h) exactly.  Statistics: median±IQR, Wilcoxon signed-rank (α=0.05) on paired T@93%.

Hyper-parameter sensitivity: inside asha-ohgw-1step, sweep η_h ∈ {1e-4, 3e-4, 1e-3, 3e-3} on an extra 8 seeds; fit LOESS to quantify curvature.

Robustness: inject 10% symmetric label noise on a duplicate run; compare % speed-up retained.

Compute efficiency: PyTorch-Profiler to record FLOPs; NVML + psutil for VRAM/RAM; timestamps logged by Lightning Callbacks.

Example code fragment (within Ray trial):
```python
# one-shot HG warm-start
loss = model(train_mb)                    # forward
hyper_grads = torch.autograd.grad(loss, cfg.continuous())
with torch.no_grad():
    for p, g in zip(cfg.continuous(), hyper_grads):
        p -= 1e-3 * g                     # η_h
# hand control back to ASHA
train_full(cfg)
```

Computational budget on cluster: 8 concurrent trials * 4 GPUs each = 32 GPUs (< half of node).  Wall-clock expected 18h incl. repeats.

Success criteria: ≥15% lower median T@93% for asha-ohgw-1step than asha-baseline with p<0.05 and ≤0.2% drop in final test accuracy.

Footnote *: random-warm uses identical σ distribution as the empirical magnitude of a hyper-gradient step, ensuring fair energy injection.
- Run Variations: ['asha-baseline', 'asha-random-warm', 'asha-ohgw-1step', 'asha-ohgw-3step']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all experimental runs stored in `results_dir/*/results.json`,\ncomputes summary statistics and generates cross-run comparison figures.\n\nUsage:\n    python -m src.evaluate --results-dir \u003cpath/to/experiments\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ----------------------------- Helper utilities ----------------------------- #\n\ndef load_run_results(results_path: str) -\u003e Dict[str, Any]:\n    with open(results_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\n\n# --------------------------------- Metrics ---------------------------------- #\n\ndef aggregate_metrics(results: List[Dict[str, Any]]) -\u003e Dict[str, Any]:\n    summary = {\n        \"num_runs\": len(results),\n        \"runs\": {r[\"run_id\"]: r[\"final_metrics\"] for r in results},\n    }\n    # Identify best run by validation accuracy\n    best_run = max(results, key=lambda r: r[\"final_metrics\"][\"val_acc\"])\n    summary[\"best_run_id\"] = best_run[\"run_id\"]\n    summary[\"best_val_acc\"] = best_run[\"final_metrics\"][\"val_acc\"]\n    return summary\n\n\n# -------------------------------- Figures ----------------------------------- #\n\ndef save_comparison_figures(results: List[Dict[str, Any]], results_dir: str) -\u003e None:\n    images_dir = Path(results_dir) / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Bar chart of final validation accuracy\n    run_ids = [r[\"run_id\"] for r in results]\n    val_accs = [r[\"final_metrics\"][\"val_acc\"] for r in results]\n\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=run_ids, y=val_accs, palette=\"viridis\")\n    for idx, val in enumerate(val_accs):\n        plt.text(idx, val + 0.005, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.ylabel(\"validation accuracy\")\n    plt.title(\"Final Validation Accuracy Across Runs\")\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy_comparison.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Line graph of validation accuracy over epochs\n    plt.figure(figsize=(6, 4))\n    for r in results:\n        epochs = [m[\"epoch\"] for m in r[\"epoch_metrics\"]]\n        val_accs_epoch = [m[\"val_acc\"] for m in r[\"epoch_metrics\"]]\n        plt.plot(epochs, val_accs_epoch, label=r[\"run_id\"])\n        plt.scatter(epochs[-1], val_accs_epoch[-1])\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"validation accuracy\")\n    plt.title(\"Validation Accuracy Trajectories\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy_trajectories.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# --------------------------------- Main ------------------------------------- #\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare experimental runs\")\n    parser.add_argument(\"--results-dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    # --------------------------------------------------------------------------------\n    # Load results.json from every run directory\n    # --------------------------------------------------------------------------------\n    result_files = glob(os.path.join(args.results_dir, \"*\", \"results.json\"))\n    if len(result_files) == 0:\n        raise FileNotFoundError(f\"No results.json files found under {args.results_dir}\")\n\n    results = [load_run_results(p) for p in sorted(result_files)]\n\n    # Aggregate and summarise\n    summary = aggregate_metrics(results)\n\n    # Generate figures\n    save_comparison_figures(results, args.results_dir)\n\n    # Final JSON summary to stdout\n    print(json.dumps(summary, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete configuration for the CIFAR-10 OHGW study.  Four run variations are\n# defined: baseline, random warm-start control, and hyper-gradient warm-start\n# with 1-step and 3-step variants.\n\nruns:\n  - run_id: asha-baseline\n    seed: 0\n    dataset:\n      name: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout_size: 16\n      label_smoothing: 0.1\n    model:\n      name: resnet20\n      kwargs:\n        num_classes: 10\n    optimizer:\n      type: SGD\n      lr: 0.1\n      momentum: 0.9\n      weight_decay: 0.0005\n    training:\n      batch_size: 128\n      epochs: 200\n\n  - run_id: asha-random-warm\n    seed: 0\n    dataset:\n      name: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout_size: 16\n      label_smoothing: 0.1\n    model:\n      name: resnet20\n      kwargs:\n        num_classes: 10\n    optimizer:\n      type: SGD\n      lr: 0.1\n      momentum: 0.9\n      weight_decay: 0.0005\n    training:\n      batch_size: 128\n      epochs: 200\n    meta:\n      warm_start: random\n      warm_steps: 1\n\n  - run_id: asha-ohgw-1step\n    seed: 0\n    dataset:\n      name: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout_size: 16\n      label_smoothing: 0.1\n    model:\n      name: resnet20\n      kwargs:\n        num_classes: 10\n    optimizer:\n      type: SGD\n      lr: 0.1\n      momentum: 0.9\n      weight_decay: 0.0005\n    training:\n      batch_size: 128\n      epochs: 200\n    meta:\n      warm_start: hypergrad\n      warm_steps: 1\n      eta_h: 0.001\n\n  - run_id: asha-ohgw-3step\n    seed: 0\n    dataset:\n      name: CIFAR10\n      root: ./data\n      randaugment_magnitude: 9\n      cutout_size: 16\n      label_smoothing: 0.1\n    model:\n      name: resnet20\n      kwargs:\n        num_classes: 10\n    optimizer:\n      type: SGD\n      lr: 0.1\n      momentum: 0.9\n      weight_decay: 0.0005\n    training:\n      batch_size: 128\n      epochs: 200\n    meta:\n      warm_start: hypergrad\n      warm_steps: 3\n      eta_h: 0.001\n", "main_py": "\"\"\"src/main.py\nCentral orchestrator that sequentially executes all run variations defined in\n`smoke_test.yaml` or `full_experiment.yaml`, captures their output \u0026 error\nstreams, and finally triggers evaluation.\n\nCLI:\n    python -m src.main --smoke-test --results-dir \u003cpath\u003e\n    python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport yaml\n\n# ------------------------- Subprocess streaming utils ----------------------- #\n\ndef _stream(pipe, tee_files):\n    \"\"\"Read lines from a pipe and write them to multiple file-like objects.\"\"\"\n    with pipe:\n        for line in iter(pipe.readline, b\"\"):\n            decoded = line.decode()\n            for f in tee_files:\n                f.write(decoded)\n                f.flush()\n            # Always mirror to main process\u0027 stdout / stderr\n            if tee_files[0].name.endswith(\"stdout.log\"):\n                sys.stdout.write(decoded)\n                sys.stdout.flush()\n            else:\n                sys.stderr.write(decoded)\n                sys.stderr.flush()\n\n\ndef _run_subprocess(cmd: list[str], stdout_log: Path, stderr_log: Path) -\u003e int:\n    \"\"\"Launch subprocess, tee stdout/err to provided log files *and* console.\"\"\"\n    stdout_log.parent.mkdir(parents=True, exist_ok=True)\n    stderr_log.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(stdout_log, \"w\", encoding=\"utf-8\") as fout, open(\n        stderr_log, \"w\", encoding=\"utf-8\") as ferr:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        t_out = threading.Thread(target=_stream, args=(proc.stdout, [fout]))\n        t_err = threading.Thread(target=_stream, args=(proc.stderr, [ferr]))\n        t_out.start()\n        t_err.start()\n        t_out.join()\n        t_err.join()\n        return proc.wait()\n\n\n# --------------------------------- Main flow -------------------------------- #\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Run experiment suite\")\n    grp = parser.add_mutually_exclusive_group(required=True)\n    grp.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke test\")\n    grp.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Output directory\")\n    args = parser.parse_args()\n\n    root_results_dir = Path(args.results_dir)\n    root_results_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = root_results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    config_path = (\n        Path(\"config/smoke_test.yaml\") if args.smoke_test else Path(\"config/full_experiment.yaml\")\n    )\n\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        suite_cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    runs = suite_cfg.get(\"runs\", [])\n    if len(runs) == 0:\n        raise ValueError(\"No runs found in configuration file\")\n\n    print(f\"Running {len(runs)} experiment variations defined in {config_path}\\n\", flush=True)\n\n    for run_cfg in runs:\n        run_id = run_cfg[\"run_id\"]\n        print(f\"========== Starting run: {run_id} ==========\")\n\n        run_dir = root_results_dir / run_id\n        run_dir.mkdir(exist_ok=True)\n\n        # Persist individual run config to file so that train.py can read it\n        run_cfg_path = run_dir / \"config.yaml\"\n        with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f_run:\n            yaml.safe_dump(run_cfg, f_run)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config-file\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(root_results_dir),\n        ]\n\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        exit_code = _run_subprocess(cmd, stdout_log, stderr_log)\n        if exit_code != 0:\n            raise RuntimeError(f\"Run {run_id} failed with exit code {exit_code}\")\n\n    # After all runs, perform evaluation \u0026 visualisation\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(root_results_dir),\n    ]\n    exit_code = subprocess.call(eval_cmd)\n    if exit_code != 0:\n        raise RuntimeError(\"Evaluation script failed\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel architecture registry.  Contains *baseline* implementation that works for\nany generic classification smoke test.  Real experiments will replace or extend\nthese models.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# ----------------------------- Base classifier ------------------------------ #\n\nclass BaseClassifier(nn.Module):\n    \"\"\"Very small CNN suitable for 1\u00d728\u00d728 images (MNIST-like).\"\"\"\n\n    def __init__(self, in_channels: int = 1, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 8, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:  # type: ignore[override]\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# ------------------------------- Registry ----------------------------------- #\n\n_MODEL_REGISTRY: Dict[str, nn.Module] = {\n    \"BASE_CLASSIFIER\": BaseClassifier,\n    # ---------------------------------------------------------------------\n    # PLACEHOLDER: Additional models will be registered here in later phase\n    # ---------------------------------------------------------------------\n}\n\n\ndef create_model(cfg: dict) -\u003e nn.Module:\n    mdl_cfg = cfg.get(\"model\", {})\n    name = mdl_cfg.get(\"name\", \"BASE_CLASSIFIER\").upper()\n    if name not in _MODEL_REGISTRY:\n        raise NotImplementedError(\n            f\"Model \u0027{name}\u0027 unknown to common core \u2013 must be provided in specialising step.\"\n        )\n    kwargs = mdl_cfg.get(\"kwargs\", {})\n    return _MODEL_REGISTRY[name](**kwargs)\n\n\ndef save_model(model: nn.Module, path: str) -\u003e None:\n    torch.save(model.state_dict(), path)\n\n\ndef load_model(model_class_name: str, path: str, device: torch.device) -\u003e nn.Module:\n    model_class_name = model_class_name.upper()\n    if model_class_name not in _MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model class \u0027{model_class_name}\u0027 for loading\")\n    model = _MODEL_REGISTRY[model_class_name]()\n    state_dict = torch.load(path, map_location=device)\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n    return model\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data loading and preprocessing pipeline with *dataset placeholders*.\nOnly the generic mechanics are fully implemented here.  Any concrete dataset\nintegration must be supplied during the dataset-specific derivation step.\n\nCurrent supported placeholder dataset:\n    RANDOM_PLACEHOLDER \u2013 synthetic data used for smoke tests.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Tuple, Any\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------------- Placeholders ------------------------------ #\n\nclass RandomClassificationDataset(Dataset):\n    \"\"\"Synthetic dataset that produces random tensors for classification.\n\n    This is *only* intended for smoke tests; real experiments must provide\n    a concrete dataset loader by replacing the placeholder in config.\n    \"\"\"\n\n    def __init__(self, num_samples: int, input_shape: Tuple[int, ...], num_classes: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n\n    def __len__(self) -\u003e int:\n        return self.num_samples\n\n    def __getitem__(self, idx: int) -\u003e Tuple[torch.Tensor, torch.Tensor]:\n        x = torch.randn(self.input_shape)\n        y = torch.randint(0, self.num_classes, (1,)).long().squeeze()\n        return x, y\n\n\n# -------------------------- Dataloader factory ------------------------------ #\n\ndef _create_placeholder_dataset(cfg: dict, split: str):\n    ds_cfg = cfg[\"dataset\"]\n    name = ds_cfg[\"name\"].upper()\n\n    if name == \"RANDOM_PLACEHOLDER\":\n        n_train = int(ds_cfg.get(\"num_samples\", 1024))\n        n_val = max(1, math.ceil(n_train * 0.2))\n        if split == \"train\":\n            dataset = RandomClassificationDataset(\n                num_samples=n_train,\n                input_shape=tuple(ds_cfg.get(\"input_shape\", [1, 28, 28])),\n                num_classes=int(ds_cfg.get(\"num_classes\", 10)),\n            )\n        else:\n            dataset = RandomClassificationDataset(\n                num_samples=n_val,\n                input_shape=tuple(ds_cfg.get(\"input_shape\", [1, 28, 28])),\n                num_classes=int(ds_cfg.get(\"num_classes\", 10)),\n            )\n        return dataset\n\n    # ---------------------------------------------------------------------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    # ---------------------------------------------------------------------\n    raise NotImplementedError(\n        f\"Dataset \u0027{name}\u0027 not implemented in common core \u2013 must be provided in specialising step.\"\n    )\n\n\ndef get_dataloader(cfg: dict, split: str = \"train\") -\u003e DataLoader:\n    \"\"\"Return a PyTorch DataLoader for requested split.\n\n    Parameters\n    ----------\n    cfg : dict\n        Run-level configuration dictionary.\n    split : str\n        One of {\"train\", \"val\"}.\n    \"\"\"\n    assert split in {\"train\", \"val\"}, \"split must be \u0027train\u0027 or \u0027val\u0027\"\n    ds = _create_placeholder_dataset(cfg, split)\n\n    batch_size = int(cfg.get(\"training\", {}).get(\"batch_size\", 32))\n    shuffle = split == \"train\"\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n", "pyproject_toml": "[project]\nname = \"ohgw-common-core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for OHGW experimental suite\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n    \"torch\u003e=2.1.0\",\n    \"torchvision\u003e=0.16.0\",\n    \"pyyaml\u003e=6.0\",\n    \"numpy\u003e=1.23\",\n    \"matplotlib\u003e=3.8\",\n    \"seaborn\u003e=0.13\",\n    \"tqdm\u003e=4.66\",\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight configuration used by CI smoke test.  Uses synthetic data \u0026 a\n# minimal model to keep runtime and memory requirements negligible.\n\nruns:\n  - run_id: smoke_random_model\n    seed: 0\n    dataset:\n      name: RANDOM_PLACEHOLDER  # PLACEHOLDER: real dataset name will replace this\n      input_shape: [1, 28, 28]\n      num_classes: 10\n      num_samples: 256\n    model:\n      name: base_classifier  # Uses the built-in tiny CNN for smoke tests\n      kwargs: {}\n    optimizer:\n      type: SGD\n      lr: 0.01\n      momentum: 0.9\n    training:\n      batch_size: 32\n      epochs: 2\n", "train_py": "\"\"\"src/train.py\nCore training script for a single experimental run.\nImplements end-to-end training, validation, checkpointing, figure generation and\nmetrics logging.  All dataset / model specialisation is supplied via the YAML\nconfiguration handed in by main.py.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport yaml\n\nfrom . import preprocess as pp  # type: ignore\nfrom . import model as mdl  # type: ignore\n\n# --------------------------------------------------------------------------- #\n#                           Utility helpers                                   #\n# --------------------------------------------------------------------------- #\n\ndef set_seed(seed: int) -\u003e None:\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Compute top-1 accuracy for classification.\"\"\"\n    pred_label = pred.argmax(dim=1)\n    correct = (pred_label == target).sum().item()\n    return correct / target.size(0)\n\n\n# --------------------------------------------------------------------------- #\n#                           Training / Validation                             #\n# --------------------------------------------------------------------------- #\n\ndef train_one_epoch(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n    scaler: torch.cuda.amp.GradScaler,\n) -\u003e tuple[float, float]:\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast(enabled=device.type == \"cuda\"):\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * inputs.size(0)\n        running_acc += accuracy(outputs.detach(), targets.detach()) * inputs.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = running_acc / len(dataloader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef validate(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    device: torch.device,\n) -\u003e tuple[float, float]:\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=device.type == \"cuda\"):\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * inputs.size(0)\n            running_acc += accuracy(outputs, targets) * inputs.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = running_acc / len(dataloader.dataset)\n    return epoch_loss, epoch_acc\n\n\n# --------------------------------------------------------------------------- #\n#                              Plot helpers                                   #\n# --------------------------------------------------------------------------- #\n\ndef save_training_curves(\n    metrics: list[dict[str, float]],\n    run_id: str,\n    results_dir: str,\n) -\u003e None:\n    images_dir = Path(results_dir) / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    epochs = [m[\"epoch\"] for m in metrics]\n    train_losses = [m[\"train_loss\"] for m in metrics]\n    val_losses = [m[\"val_loss\"] for m in metrics]\n    train_accs = [m[\"train_acc\"] for m in metrics]\n    val_accs = [m[\"val_acc\"] for m in metrics]\n\n    sns.set_style(\"whitegrid\")\n\n    # Loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, train_losses, label=\"train_loss\")\n    plt.plot(epochs, val_losses, label=\"val_loss\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.title(f\"Loss ({run_id})\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"loss_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Accuracy curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, train_accs, label=\"train_acc\")\n    plt.plot(epochs, val_accs, label=\"val_acc\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"accuracy\")\n    plt.title(f\"Accuracy ({run_id})\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"accuracy_{run_id}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# --------------------------------------------------------------------------- #\n#                                  Main                                       #\n# --------------------------------------------------------------------------- #\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config-file\", required=True, type=str, help=\"Path to run-level YAML config\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Root directory for all outputs\")\n    args = parser.parse_args()\n\n    # ----------------------------- configuration --------------------------- #\n    with open(args.config_file, \"r\", encoding=\"utf-8\") as f:\n        cfg: Dict[str, Any] = yaml.safe_load(f)\n\n    run_id: str = cfg[\"run_id\"]\n    run_dir = Path(args.results_dir) / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    checkpoints_dir = run_dir / \"checkpoints\"\n    checkpoints_dir.mkdir(exist_ok=True)\n\n    # Persist copy of config for reproducibility\n    with open(run_dir / \"config.yaml\", \"w\", encoding=\"utf-8\") as f_cfg_out:\n        yaml.safe_dump(cfg, f_cfg_out)\n\n    seed = int(cfg.get(\"seed\", 42))\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------- Data pipeline ---------------------------- #\n    train_loader = pp.get_dataloader(cfg, split=\"train\")\n    val_loader = pp.get_dataloader(cfg, split=\"val\")\n\n    # ------------------------- Model \u0026 optim ------------------------------ #\n    model = mdl.create_model(cfg).to(device)\n\n    # label smoothing hyper-parameter can be placed in either dataset or training section\n    ls_val = float(\n        cfg.get(\"dataset\", {}).get(\"label_smoothing\", cfg.get(\"training\", {}).get(\"label_smoothing\", 0.0))\n    )\n    criterion = nn.CrossEntropyLoss(label_smoothing=ls_val)\n\n    optim_cfg = cfg.get(\"optimizer\", {})\n    opt_type = optim_cfg.get(\"type\", \"SGD\").upper()\n    lr = float(optim_cfg.get(\"lr\", 0.1))\n    momentum = float(optim_cfg.get(\"momentum\", 0.9))\n    weight_decay = float(optim_cfg.get(\"weight_decay\", 5e-4))\n\n    if opt_type == \"ADAM\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        optimizer = optim.SGD(\n            model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True\n        )\n\n    epochs: int = int(cfg.get(\"training\", {}).get(\"epochs\", 10))\n\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=device.type == \"cuda\")\n\n    # --------------------------- Training loop --------------------------- #\n    epoch_metrics: list[dict[str, float]] = []\n    best_val_acc = -1.0\n    start_time = time.time()\n\n    for epoch in range(1, epochs + 1):\n        train_loss, train_acc = train_one_epoch(\n            model, train_loader, criterion, optimizer, device, scaler\n        )\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        scheduler.step()\n\n        metric_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"lr\": scheduler.get_last_lr()[0],\n        }\n        epoch_metrics.append(metric_dict)\n\n        # Save checkpoint for each epoch (allows ASHA-style early stopping if desired)\n        ckpt_path = checkpoints_dir / f\"epoch_{epoch}.pt\"\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"scheduler_state\": scheduler.state_dict(),\n            },\n            ckpt_path,\n        )\n\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), run_dir / \"best_model.pt\")\n\n        # Lightweight logging so evaluation harness can scrape stdout easily\n        print(json.dumps({\"run_id\": run_id, **metric_dict}), flush=True)\n\n    elapsed = time.time() - start_time\n\n    results_summary: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"final_metrics\": epoch_metrics[-1],\n        \"epoch_metrics\": epoch_metrics,\n        \"best_val_acc\": best_val_acc,\n        \"training_time_sec\": elapsed,\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as f_res:\n        json.dump(results_summary, f_res, indent=2)\n\n    save_training_curves(epoch_metrics, run_id, args.results_dir)\n\n    # Print summary last \u2013 used by evaluation harness\n    print(json.dumps(results_summary), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "Derived code still contains unresolved placeholders and is not executable for the new CIFAR-10 OHGW experiment:\n1. Dataset: preprocess.py only supports RANDOM_PLACEHOLDER; CIFAR10 specified in full_experiment.yaml will raise NotImplementedError.\n2. Models: model.py registry lacks ResNet20/ResNet50 required by run configs; create_model will fail.\n3. OHGW logic: train.py does not implement warm-start hyper-gradient or random perturbation behaviour defined in meta section; core experimental novelty missing.\n4. Placeholder comments remain in model.py and preprocess.py and have not been fully replaced.\nTherefore important functionality is omitted and code cannot be executed as is."
}
