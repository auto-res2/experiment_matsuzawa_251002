
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

One-Shot Hyper-Gradient Warm-Starts for Bandit-Style Hyperparameter Optimisation

---

---
Section: abstract

Bandit-style multi-fidelity schedulers such as ASHA and PASHA are the work-horses of practical hyperparameter optimisation, yet they still waste substantial compute on configurations that could have been flagged as poor before real training even begins. The root cause is that every trial is treated as a black box: none of the gradients already computed inside the training loop are exploited by the scheduler. We close this gap with One-Shot Hyper-Gradient Warm-Starts (OHGW). For each freshly sampled configuration we run exactly one mini-batch, obtain stochastic hyper-gradients ∂L⁄∂ψ for all continuous hyperparameters at almost zero extra cost via automatic differentiation, apply a single tiny update ψ ← ψ − ηₕ∂L⁄∂ψ, and hand the nudged configuration back to the unmodified scheduler. OHGW therefore preserves exploration while biasing every candidate toward lower-loss regions at negligible overhead and with no change to promotion or stopping logic. On CIFAR-10 with ResNet-20 under ASHA and on WikiText-103 with GPT2-small under PASHA, OHGW cuts median wall-clock time to a preset quality threshold by roughly twenty percent, adds under four percent floating-point operations, and leaves final accuracy and perplexity unchanged. Random perturbations provide almost no benefit and taking more than one hyper-step shows diminishing returns. These findings demonstrate that a single noisy hyper-gradient obtained before expensive training commences can reclaim a significant share of wasted computation in grey-box hyperparameter optimisation.

---

---
Section: introduction

Hyperparameter optimisation (HPO) is indispensable for obtaining robust performance in modern machine-learning systems, yet even the most popular grey-box schedulers squander a sizable fraction of their budget on clearly sub-optimal configurations. Successive-Halving variants such as Hyperband, ASHA and PASHA prune weak contenders early by evaluating them on progressively larger budgets \cite{bohdal-2022-pasha}. Grey-box Bayesian schemes like DyHPO refine this idea through learning-curve modelling and dynamic promotion rules \cite{wistuba-2022-supervising}. Despite these advances, almost all schedulers regard the training process itself as opaque: internal gradients that are already computed for parameter updates are ignored during the search.  Hyper-gradient methods have shown that gradients with respect to hyperparameters can be extracted cheaply via automatic differentiation \cite{chandra-2019-gradient} or implicit differentiation techniques that avoid expensive unrolling \cite{bertrand-2020-implicit}. Unfortunately these approaches typically assume full control over the optimisation routine and therefore clash with production HPO systems whose scheduling logic is complex and battle-tested. The open question, then, is how to inject very cheap but noisy hyper-gradient information into existing bandit-style frameworks without having to rewrite their core.  We address this question with One-Shot Hyper-Gradient Warm-Starts (OHGW). Whenever the scheduler samples a configuration x=(θ₀,ψ) consisting of model parameters θ (usually random initialisation) and continuous hyperparameters ψ, the training script performs exactly one forward-and-backward pass on a single mini-batch, collects the stochastic hyper-gradient gψ=∂L⁄∂ψ, and applies a microscopic update ψ ← ψ − ηₕgψ with ηₕ = 10⁻³. Promotion rules, budgets and stopping criteria remain untouched; from the scheduler’s perspective nothing has changed except that the candidate starts from a slightly more promising point.  Two practical challenges arise. First, a gradient measured on a single mini-batch is extremely noisy, so the step must be sufficiently small to prevent biasing the search or harming exploration. Second, adoption hinges on a minimal engineering footprint—ideally a few lines of code that do not depend on the internals of the scheduler. OHGW meets both constraints: the extra cost is one forward and one backward pass per trial (<4 % FLOPs in our experiments) and integration is a five-line wrapper around trial creation.  We validate OHGW in two contrasting settings—vision (CIFAR-10, ResNet-20, ASHA) and language modelling (WikiText-103, GPT2-small, PASHA)—using 56 paired random seeds and equal GPU budgets. Metrics include time-to-target quality, best final score, compute overhead, variance, and hyperparameter distribution shift. OHGW consistently shortens time-to-target by about twenty percent while preserving ultimate performance and introducing negligible bias. Ablations confirm that gradient directionality, not random perturbation, drives the gain, and that repeating the warm-start step gives only marginal additional savings.  Contributions  • We introduce OHGW, a scheduler-agnostic, single-step hyper-gradient warm-start that improves efficiency without altering bandit logic.  • We provide a practical recipe for extracting hyper-gradients of continuous hyperparameters at negligible cost.  • Extensive experiments across vision and language reduce median wall-clock time to target quality by roughly twenty percent with under four percent compute overhead.  • Ablation, sensitivity and robustness studies show that gradient direction matters, benefits saturate quickly, and variance or bias are not inflated.  Looking forward, we plan to extend OHGW to mixed discrete–continuous spaces, integrate warm-start signals into surrogate-based selection \cite{khazi-2023-deep} and adaptive-fidelity frameworks \cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \cite{panda-2022-new,khodak-2021-federated}.

---

---
Section: related_work

Multi-fidelity schedulers Successive-Halving, Hyperband and ASHA progressively allocate resources; PASHA adds an adaptive cap on maximum fidelity \cite{bohdal-2022-pasha}. DyHPO supervises the race among configurations with a deep-kernel Gaussian Process that embeds learning-curve dynamics \cite{wistuba-2022-supervising}. All these methods leverage intermediate metrics yet still initialise every configuration blindly. OHGW is complementary: it keeps the scheduling logic intact and instead improves the starting point of each trial.  Grey-box Bayesian optimisation BOIL explicitly models iterative progress to balance cost and benefit \cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.  Gradient-based HPO Early work showed how to compute hyper-gradients by augmenting backpropagation \cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.  Data and fidelity efficiency AUTOMATA speeds up HPO by selecting informative data subsets \cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.  Constrained settings Federated HPO faces communication bottlenecks \cite{khodak-2021-federated}; differentially-private HPO must account for privacy budgets \cite{panda-2022-new,wang-2023-hypo}. OHGW’s one-shot nature and tiny overhead make it attractive in such resource-sensitive regimes.  In summary, earlier work either improves resource allocation, builds sophisticated surrogates, or performs full-fledged hyper-gradient optimisation. OHGW is unique in exploiting a single, virtually free gradient to warm-start any candidate before scheduling commences.

---

---
Section: background

Problem setting Let θ denote neural-network parameters and ψ∈ℝᵈ a vector of continuous hyperparameters (log learning rate, log weight decay, momentum, augmentation magnitude, label smoothing). For a mini-batch b the loss is L(θ,ψ; b). Successive-Halving style schedulers repeatedly sample configurations x=(θ₀,ψ), train for a small budget, and promote or discard contenders based on early validation metrics.  Untapped signal Deep-learning frameworks already compute ∂L⁄∂θ; obtaining ∂L⁄∂ψ requires little additional work as long as ψ influences the forward computation \cite{chandra-2019-gradient}. Although these hyper-gradients are noisy when estimated on a single mini-batch, they still indicate how the loss would change if ψ were perturbed.  Aim and constraints We aim to inject this cheap signal into existing schedulers without touching their allocation policies. Constraints are: overhead ≤ 5 % FLOPs and ≤ 10 % VRAM; zero changes to promotion logic; ability to operate in mixed search spaces (only continuous ψ are updated); preservation of exploration diversity.  Prior art typically computes hyper-gradients throughout training, unrolls optimisation steps, or solves auxiliary linear systems \cite{bertrand-2020-implicit,immer-2023-stochastic}. OHGW avoids all of these by taking exactly one hyper-step before heavy training begins.  Assumptions Continuous hyperparameters appear differentiably in the loss for at least one mini-batch; discrete ones remain fixed. A small hyper-learning-rate ηₕ ensures stability; the scheduler interacts with the training script only via process boundaries, so warm-starting must happen inside the trial before any metric is reported.

---

---
Section: method

OHGW augments trial initialisation with four simple steps.  1 Configuration sampling The scheduler outputs a candidate x containing initial parameters θ₀ and hyperparameters ψ.  2 Single-batch pass The training script draws one mini-batch (size 128), computes the loss L(θ₀,ψ), back-propagates, and retains the computation graph once to obtain both parameter gradients and the hyper-gradient gψ = ∂L⁄∂ψ.  3 One hyper-step Within a no-grad context the script applies ψ ← ψ − ηₕgψ with ηₕ = 10⁻³. No higher-order terms are considered and θ is left untouched.  4 Scheduler resumes The adjusted configuration x′ is trained for the first-rung budget exactly as in the original algorithm; promotion, stopping and resource accounting remain unchanged.  Design choices Differentiable hyperparameters are wrapped as tensors that influence the forward computation (e.g. learning rate scales the optimiser update, label smoothing alters target distributions). A small ηₕ prevents excessive bias; we sweep ηₕ∈{1e-4, 3e-4, 1e-3, 3e-3} in Section Results. Because only one extra backward pass is added, empirical overhead stays below four percent FLOPs and one percent VRAM.  Pseudocode  for cfg in scheduler.sample():
    model = build_model(cfg)
    data  = next(train_loader)            # one mini-batch
    loss  = forward_loss(model, data)
    grads = autograd.grad(loss, cfg.continuous_params())
    with no_grad():
        for p, g in zip(cfg.continuous_params(), grads):
            p -= eta_h * g                # single hyper-step
    scheduler.launch(cfg)                 # continue unchanged  Relation to prior work OHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \cite{khazi-2023-deep}.

---

---
Section: experimental_setup

Benchmarks (1) CIFAR-10 with ResNet-20 and a five-dimensional continuous search space {log learning rate, log weight decay, momentum, augmentation magnitude, label smoothing}. (2) WikiText-103 with GPT2-small.  Schedulers We employ the public implementations of ASHA, PASHA and DyHPO \cite{bohdal-2022-pasha,wistuba-2022-supervising} unmodified. Variants suffixed “+OHGW” wrap trial creation with the procedure described above.  Warm-start parameters Each configuration is warmed using exactly one mini-batch (batch size 128); ηₕ = 1e-3 unless specified; PyTorch autograd computes first-order gradients only. Discrete hyperparameters, if any, are unaffected.  Budgets and replication The CIFAR-10 study uses 32 paired seeds on 4 × V100 GPUs for 12 hours; the WikiText-103 study uses 24 paired seeds under the same budget.  Metrics Primary metrics are (i) T@τ: wall-clock time and GPU-hours to reach 93 % validation accuracy (vision) or validation perplexity 30 (language); (ii) best final test metric after exhausting the budget. Secondary diagnostics include area under the best-score-vs-time curve, compute overhead (warm-start FLOPs ⁄ total), peak VRAM, variance across seeds, and KL divergence between final ψ distributions. Significance is assessed via paired two-sided Wilcoxon signed-rank tests (α = 0.05).  Controls and ablations • Random warm-start of the same step magnitude but isotropic direction. • Three-step hyper-gradient warm-start to check diminishing returns. • ηₕ sweep 1e-4 … 3e-3. • Robustness under 15 % label (vision) or token (language) noise.  Implementation details All experiments are executed within a Hydra-based harness; Slurm cgroup accounting records precise GPU-hour usage. The OHGW wrapper consists of five additional lines of code, demonstrating negligible engineering burden.

---

---
Section: results

Results are organised by domain, followed by ablation, overhead and robustness analyses.  Vision – CIFAR-10 + ASHA Baseline reaches 93 % validation accuracy in 11.4 h ± 1.1. Random warm-start improves this marginally to 11.2 h ± 1.0 (−1.8 %). OHGW (one step) lowers time-to-target to 9.1 h ± 1.0 (−20.2 %, p = 3.1 × 10⁻⁶). Three steps reduce time further to 8.9 h ± 1.3 (−21.9 %) but raise overhead to 6 % FLOPs. Final test accuracy is 94.73 % ± 0.12 (baseline) versus 94.81 % ± 0.10 (OHGW), difference not significant. Warm-start overhead is 2.7 % FLOPs and <0.1 % VRAM.  Language – WikiText-103 + PASHA Baseline reaches validation perplexity 30 in 6.9 h ± 0.8. OHGW with ηₕ = 1e-3 needs 5.6 h ± 0.7 (−18.8 %, p = 7.5 × 10⁻⁵). Lowering ηₕ to 3e-4 produces 5.8 h (−16.3 %). Under 15 % token noise OHGW still gains 11.6 %. Final validation perplexity improves slightly from 24.8 ± 0.3 to 24.6 ± 0.3; out-of-domain perplexity drops from 32.1 to 31.7. Overhead is 3.4 % FLOPs and 1.2 % VRAM.  Figures   Figure 1: Validation accuracy over time for ASHA baseline; higher values indicate better performance (filename: accuracy_asha-baseline.pdf)    Figure 2: Validation accuracy over time for ASHA + OHGW (one step); higher is better (filename: accuracy_asha-ohgw-1step.pdf)    Figure 3: Validation accuracy for ASHA + OHGW (three steps); higher is better (filename: accuracy_asha-ohgw-3step.pdf)    Figure 4: Validation accuracy for ASHA with random warm-start; higher is better (filename: accuracy_asha-random-warm.pdf)    Figure 5: Accuracy comparison across all ASHA variants; higher is better (filename: accuracy_comparison.pdf)    Figure 6: Accuracy trajectories across 32 seeds; higher is better (filename: accuracy_trajectories.pdf)    Figure 7: Training loss over time for ASHA baseline; lower values indicate better performance (filename: training_loss_asha-baseline.pdf)    Figure 8: Training loss for ASHA + OHGW (one step); lower is better (filename: training_loss_asha-ohgw-1step.pdf)    Figure 9: Training loss for ASHA + OHGW (three steps); lower is better (filename: training_loss_asha-ohgw-3step.pdf)    Figure 10: Training loss for ASHA random warm-start; lower is better (filename: training_loss_asha-random-warm.pdf)    Ablation insights Random warm-start yields <2 % improvement, confirming that gradient direction drives efficiency. Additional hyper-steps offer diminishing returns relative to their overhead.  Variance and bias Standard deviation of T@τ rises by 5 % (vision) and 3 % (language), well below the 10 % inflation budget. KL divergence between final ψ distributions is 0.012 (vision) and 0.018 (language), signalling negligible bias.  Aggregate outcome Across 56 paired seeds, OHGW reduces median time-to-target by 19.5 %, preserves or slightly improves final task performance, incurs <4 % extra compute, and does not inflate variance—meeting all pre-registered success criteria.

---

---
Section: conclusion

We introduced One-Shot Hyper-Gradient Warm-Starts, a drop-in augmentation for Successive-Halving schedulers that leverages a single, almost-free hyper-gradient to nudge each new configuration before expensive training begins. Without modifying promotion logic or surrogate models, OHGW reduces median time-to-quality by roughly twenty percent on both vision and language benchmarks, adds less than four percent computational overhead, and leaves final metrics unchanged. Ablations demonstrate that the efficiency gain stems from the informative direction of the gradient, not random perturbation, and that additional hyper-steps yield diminishing returns.  Practitioners can adopt OHGW via a five-line wrapper, immediately reclaiming a significant share of wasted GPU hours in existing HPO pipelines. Future work will extend the idea to mixed discrete–continuous spaces, integrate warm-start signals into surrogate-based candidate selection and adaptive-fidelity frameworks \cite{jiang-2024-efficient,khazi-2023-deep}, and explore privacy-aware or federated settings where the one-shot, low-overhead characteristic of OHGW is particularly advantageous \cite{panda-2022-new,khodak-2021-federated}. By showing that even a noisy, single-batch hyper-gradient can materially accelerate grey-box optimisation, this work opens the door to deeper synergies between internal training-loop signals and external scheduling strategies.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "One-Shot Hyper-Gradient Warm-Starts for Bandit-Style Hyperparameter Optimisation",
    "abstract": "Bandit-style multi-fidelity schedulers such as ASHA and PASHA are the work-horses of practical hyperparameter optimisation, yet they still waste substantial compute on configurations that could have been flagged as poor before real training even begins. The root cause is that every trial is treated as a black box: none of the gradients already computed inside the training loop are exploited by the scheduler. We close this gap with One-Shot Hyper-Gradient Warm-Starts (OHGW). For each freshly sampled configuration we run exactly one mini-batch, obtain stochastic hyper-gradients \\(\\frac{\\partial L}{\\partial \\psi}\\) for all continuous hyperparameters at almost zero extra cost via automatic differentiation, apply a single tiny update \\(\\psi \\leftarrow \\psi - \\eta_h \\tfrac{\\partial L}{\\partial \\psi}\\), and hand the nudged configuration back to the unmodified scheduler. OHGW therefore preserves exploration while biasing every candidate toward lower-loss regions at negligible overhead and with no change to promotion or stopping logic. On CIFAR-10 with ResNet-20 under ASHA and on WikiText-103 with GPT2-small under PASHA, OHGW cuts median wall-clock time to a preset quality threshold by roughly twenty percent, adds under four percent floating-point operations, and leaves final accuracy and perplexity unchanged. Random perturbations provide almost no benefit and taking more than one hyper-step shows diminishing returns. These findings demonstrate that a single noisy hyper-gradient obtained before expensive training commences can reclaim a significant share of wasted computation in grey-box hyperparameter optimisation.",
    "introduction": "Hyperparameter optimisation (HPO) is indispensable for obtaining robust performance in modern machine-learning systems, yet even the most popular grey-box schedulers squander a sizable fraction of their budget on clearly sub-optimal configurations. Successive-Halving variants such as Hyperband, ASHA and PASHA prune weak contenders early by evaluating them on progressively larger budgets \\cite{bohdal-2022-pasha}. Grey-box Bayesian schemes like DyHPO refine this idea through learning-curve modelling and dynamic promotion rules \\cite{wistuba-2022-supervising}. Despite these advances, almost all schedulers regard the training process itself as opaque: internal gradients that are already computed for parameter updates are ignored during the search.\n\nHyper-gradient methods have shown that gradients with respect to hyperparameters can be extracted cheaply via automatic differentiation \\cite{chandra-2019-gradient} or implicit differentiation techniques that avoid expensive unrolling \\cite{bertrand-2020-implicit}. Unfortunately these approaches typically assume full control over the optimisation routine and therefore clash with production HPO systems whose scheduling logic is complex and battle-tested. The open question, then, is how to inject very cheap but noisy hyper-gradient information into existing bandit-style frameworks without having to rewrite their core.\n\nWe address this question with One-Shot Hyper-Gradient Warm-Starts (OHGW). Whenever the scheduler samples a configuration \\(x=(\\theta_0,\\psi)\\) consisting of model parameters \\(\\theta\\) (usually random initialisation) and continuous hyperparameters \\(\\psi\\), the training script performs exactly one forward-and-backward pass on a single mini-batch, collects the stochastic hyper-gradient \\(g_{\\psi}=\\tfrac{\\partial L}{\\partial \\psi}\\), and applies a microscopic update \\(\\psi \\leftarrow \\psi - \\eta_h g_{\\psi}\\) with \\(\\eta_h = 10^{-3}\\). Promotion rules, budgets and stopping criteria remain untouched; from the scheduler\u0019s perspective nothing has changed except that the candidate starts from a slightly more promising point.\n\nTwo practical challenges arise. First, a gradient measured on a single mini-batch is extremely noisy, so the step must be sufficiently small to prevent biasing the search or harming exploration. Second, adoption hinges on a minimal engineering footprint\u0019ideally a few lines of code that do not depend on the internals of the scheduler. OHGW meets both constraints: the extra cost is one forward and one backward pass per trial (\\(<4 \\%\\) FLOPs in our experiments) and integration is a five-line wrapper around trial creation.\n\nWe validate OHGW in two contrasting settings\u0019vision (CIFAR-10, ResNet-20, ASHA) and language modelling (WikiText-103, GPT2-small, PASHA)\u0019using 56 paired random seeds and equal GPU budgets. Metrics include time-to-target quality, best final score, compute overhead, variance, and hyperparameter distribution shift. OHGW consistently shortens time-to-target by about twenty percent while preserving ultimate performance and introducing negligible bias. Ablations confirm that gradient directionality, not random perturbation, drives the gain, and that repeating the warm-start step gives only marginal additional savings.\n\n\\subsection{Contributions}\n\\begin{itemize}\n    \\item \\textbf{Scheduler-agnostic warm-start:} We introduce OHGW, a single-step hyper-gradient warm-start that improves efficiency without altering bandit logic.\n    \\item \\textbf{Practical hyper-gradient extraction:} We provide a recipe for extracting hyper-gradients of continuous hyperparameters at negligible cost.\n    \\item \\textbf{Consistent efficiency gains:} Experiments across vision and language reduce median wall-clock time to target quality by roughly twenty percent with under four percent compute overhead.\n    \\item \\textbf{Robustness and ablations:} Gradient direction matters, benefits saturate quickly, and variance or bias are not inflated.\n\\end{itemize}\n\nLooking forward, we plan to extend OHGW to mixed discrete\u0013continuous spaces, integrate warm-start signals into surrogate-based selection \\cite{khazi-2023-deep} and adaptive-fidelity frameworks \\cite{jiang-2024-efficient}, and explore privacy-aware or federated scenarios where one-shot, low-overhead interventions are especially attractive \\cite{panda-2022-new,khodak-2021-federated}.",
    "related_work": "\\subsection{Multi-fidelity schedulers}\nSuccessive-Halving, Hyperband and ASHA progressively allocate resources; PASHA adds an adaptive cap on maximum fidelity \\cite{bohdal-2022-pasha}. DyHPO supervises the race among configurations with a deep-kernel Gaussian Process that embeds learning-curve dynamics \\cite{wistuba-2022-supervising}. All these methods leverage intermediate metrics yet still initialise every configuration blindly. OHGW is complementary: it keeps the scheduling logic intact and instead improves the starting point of each trial.\n\n\\subsection{Grey-box Bayesian optimisation}\nBOIL explicitly models iterative progress to balance cost and benefit \\cite{nguyen-2019-bayesian}. Deep Power Laws exploits power-law learning curves to decide when to pause training \\cite{kadra-2023-scaling}. Deep Ranking Ensembles meta-learn surrogates that optimise ranking metrics \\cite{khazi-2023-deep}. Differentiable EHVI accelerates multi-objective acquisition optimisation with exact gradients \\cite{daulton-2020-differentiable}. These approaches rely on surrogate modelling and acquisition optimisation, whereas OHGW exploits native gradients already available in the training loop.\n\n\\subsection{Gradient-based HPO}\nEarly work showed how to compute hyper-gradients by augmenting backpropagation \\cite{chandra-2019-gradient}; implicit differentiation scales to non-smooth penalties \\cite{bertrand-2020-implicit}; stochastic marginal-likelihood gradients further reduce cost \\cite{immer-2023-stochastic}. These techniques operate throughout training or require unrolling, imposing memory and engineering overhead. OHGW applies a single pre-training step, trading precision for immediacy.\n\n\\subsection{Data and fidelity efficiency}\nAUTOMATA speeds up HPO by selecting informative data subsets \\cite{killamsetty-2022-automata}; FastBO adaptively chooses fidelities per configuration \\cite{jiang-2024-efficient}; DNN-MFBO and BMBO-DARN model cross-fidelity correlations \\cite{li-2020-multi,li-2021-batch}. OHGW is orthogonal and can be layered on top of any of these strategies.\n\n\\subsection{Constrained settings}\nFederated HPO faces communication bottlenecks \\cite{khodak-2021-federated}; differentially-private HPO must account for privacy budgets \\cite{panda-2022-new,wang-2023-hypo}. OHGW\u0019s one-shot nature and tiny overhead make it attractive in such resource-sensitive regimes.\n\nIn summary, earlier work either improves resource allocation, builds sophisticated surrogates, or performs full-fledged hyper-gradient optimisation. OHGW is unique in exploiting a single, virtually free gradient to warm-start any candidate before scheduling commences.",
    "background": "\\subsection{Problem setting}\nLet \\(\\theta\\) denote neural-network parameters and \\(\\psi\\in\\mathbb{R}^d\\) a vector of continuous hyperparameters (log learning rate, log weight decay, momentum, augmentation magnitude, label smoothing). For a mini-batch \\(b\\) the loss is \\(L(\\theta,\\psi; b)\\). Successive-Halving style schedulers repeatedly sample configurations \\(x=(\\theta_0,\\psi)\\), train for a small budget, and promote or discard contenders based on early validation metrics.\n\n\\subsection{Untapped signal}\nDeep-learning frameworks already compute \\(\\tfrac{\\partial L}{\\partial \\theta}\\); obtaining \\(\\tfrac{\\partial L}{\\partial \\psi}\\) requires little additional work as long as \\(\\psi\\) influences the forward computation \\cite{chandra-2019-gradient}. Although these hyper-gradients are noisy when estimated on a single mini-batch, they still indicate how the loss would change if \\(\\psi\\) were perturbed.\n\n\\subsection{Aim and constraints}\nWe aim to inject this cheap signal into existing schedulers without touching their allocation policies. Constraints are: overhead \\(\\leq 5 \\%\\) FLOPs and \\(\\leq 10 \\%\\) VRAM; zero changes to promotion logic; ability to operate in mixed search spaces (only continuous \\(\\psi\\) are updated); preservation of exploration diversity. Prior art typically computes hyper-gradients throughout training, unrolls optimisation steps, or solves auxiliary linear systems \\cite{bertrand-2020-implicit,immer-2023-stochastic}. OHGW avoids all of these by taking exactly one hyper-step before heavy training begins.\n\n\\subsection{Assumptions}\nContinuous hyperparameters appear differentiably in the loss for at least one mini-batch; discrete ones remain fixed. A small hyper-learning-rate \\(\\eta_h\\) ensures stability; the scheduler interacts with the training script only via process boundaries, so warm-starting must happen inside the trial before any metric is reported.",
    "method": "\\subsection{Procedure}\nOHGW augments trial initialisation with four simple steps.\n\n1. Configuration sampling: The scheduler outputs a candidate \\(x\\) containing initial parameters \\(\\theta_0\\) and hyperparameters \\(\\psi\\).\n\n2. Single-batch pass: The training script draws one mini-batch (size 128), computes the loss \\(L(\\theta_0,\\psi)\\), back-propagates, and retains the computation graph once to obtain both parameter gradients and the hyper-gradient \\(g_{\\psi} = \\tfrac{\\partial L}{\\partial \\psi}\\).\n\n3. One hyper-step: Within a no-grad context the script applies \\(\\psi \\leftarrow \\psi - \\eta_h g_{\\psi}\\) with \\(\\eta_h = 10^{-3}\\). No higher-order terms are considered and \\(\\theta\\) is left untouched.\n\n4. Scheduler resumes: The adjusted configuration \\(x'\\) is trained for the first-rung budget exactly as in the original algorithm; promotion, stopping and resource accounting remain unchanged.\n\n\\subsection{Design choices}\nDifferentiable hyperparameters are wrapped as tensors that influence the forward computation (e.g., learning rate scales the optimiser update, label smoothing alters target distributions). A small \\(\\eta_h\\) prevents excessive bias; we sweep \\(\\eta_h\\in\\{10^{-4}, 3\\cdot10^{-4}, 10^{-3}, 3\\cdot10^{-3}\\}\\) in the results. Because only one extra backward pass is added, empirical overhead stays below four percent FLOPs and one percent VRAM.\n\n\\subsection{Pseudocode}\n\\begin{algorithm}[H]\n\\caption{One-Shot Hyper-Gradient Warm-Start (OHGW)}\n\\begin{algorithmic}\n    \\State \\textbf{Input:} Scheduler producing configurations \\(x=(\\theta_0,\\psi)\\); hyper-step size \\(\\eta_h\\); training data loader\n    \\While{scheduler has pending trials}\n        \\State \\(x\\gets\\) scheduler.sample()\n        \\State model \\(\\gets\\) build\\_model with initial parameters \\(\\theta_0\\)\n        \\State data \\(\\gets\\) next mini-batch from loader\n        \\State Compute loss: \\(\\ell \\gets L(\\theta_0, \\psi; \\text{data})\\)\n        \\State Compute hyper-gradient: \\(g_{\\psi} \\gets \\nabla_{\\psi} \\, \\ell\\) via autograd\n        \\State Update hyperparameters: \\(\\psi \\gets \\psi - \\eta_h \\, g_{\\psi}\\)\n        \\State Launch unmodified training of \\(x'=(\\theta_0,\\psi)\\) under the scheduler (budgets, promotion, and stopping unchanged)\n    \\EndWhile\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Relation to prior work}\nOHGW borrows the concept of hyper-gradients but applies it once, avoiding the memory footprint of unrolling \\cite{bertrand-2020-implicit} and the complexity of surrogate-guided selection \\cite{nguyen-2019-bayesian}. It is orthogonal to adaptive-fidelity scheduling \\cite{jiang-2024-efficient} and can coexist with surrogate-based candidate ranking \\cite{khazi-2023-deep}.",
    "experimental_setup": "\\subsection{Benchmarks}\n(1) CIFAR-10 with ResNet-20 and a five-dimensional continuous search space \\{log learning rate, log weight decay, momentum, augmentation magnitude, label smoothing\\}. (2) WikiText-103 with GPT2-small.\n\n\\subsection{Schedulers}\nWe employ the public implementations of ASHA, PASHA and DyHPO \\cite{bohdal-2022-pasha,wistuba-2022-supervising} unmodified. Variants suffixed ``+OHGW'' wrap trial creation with the procedure described above.\n\n\\subsection{Warm-start parameters}\nEach configuration is warmed using exactly one mini-batch (batch size 128); \\(\\eta_h = 10^{-3}\\) unless specified; PyTorch autograd computes first-order gradients only. Discrete hyperparameters, if any, are unaffected.\n\n\\subsection{Budgets and replication}\nThe CIFAR-10 study uses 32 paired seeds on 4 \\(\\times\\) V100 GPUs for 12 hours; the WikiText-103 study uses 24 paired seeds under the same budget.\n\n\\subsection{Metrics}\nPrimary metrics are (i) \\(T@\\tau\\): wall-clock time and GPU-hours to reach 93 \\% validation accuracy (vision) or validation perplexity 30 (language); (ii) best final test metric after exhausting the budget. Secondary diagnostics include area under the best-score-vs-time curve, compute overhead (warm-start FLOPs \\(/\\) total), peak VRAM, variance across seeds, and KL divergence between final \\(\\psi\\) distributions. Significance is assessed via paired two-sided Wilcoxon signed-rank tests (\\(\\alpha = 0.05\\)).\n\n\\subsection{Controls and ablations}\n\\begin{itemize}\n    \\item \\textbf{Random warm-start:} Same step magnitude but isotropic direction.\n    \\item \\textbf{Multiple hyper-steps:} Three-step hyper-gradient warm-start to check diminishing returns.\n    \\item \\textbf{Step-size sweep:} \\(\\eta_h\\) sweep from \\(10^{-4}\\) to \\(3\\cdot10^{-3}\\).\n    \\item \\textbf{Robustness:} Performance under 15 \\% label (vision) or token (language) noise.\n\\end{itemize}\n\n\\subsection{Implementation details}\nAll experiments are executed within a Hydra-based harness; Slurm cgroup accounting records precise GPU-hour usage. The OHGW wrapper consists of five additional lines of code, demonstrating negligible engineering burden.",
    "results": "Results are organised by domain, followed by ablation, overhead and robustness analyses.\n\n\\subsection{Vision - CIFAR-10 + ASHA}\nBaseline reaches 93 \\% validation accuracy in \\(11.4\\,\\mathrm{h} \\pm 1.1\\). Random warm-start improves this marginally to \\(11.2\\,\\mathrm{h} \\pm 1.0\\) (\\(-1.8 \\%\\)). OHGW (one step) lowers time-to-target to \\(9.1\\,\\mathrm{h} \\pm 1.0\\) (\\(-20.2 \\%\\), \\(p = 3.1 \\times 10^{-6}\\)). Three steps reduce time further to \\(8.9\\,\\mathrm{h} \\pm 1.3\\) (\\(-21.9 \\%\\)) but raise overhead to 6 \\% FLOPs. Final test accuracy is \\(94.73 \\% \\pm 0.12\\) (baseline) versus \\(94.81 \\% \\pm 0.10\\) (OHGW), difference not significant. Warm-start overhead is 2.7 \\% FLOPs and \\(<0.1 \\%\\) VRAM.\n\n\\subsection{Language - WikiText-103 + PASHA}\nBaseline reaches validation perplexity 30 in \\(6.9\\,\\mathrm{h} \\pm 0.8\\). OHGW with \\(\\eta_h = 10^{-3}\\) needs \\(5.6\\,\\mathrm{h} \\pm 0.7\\) (\\(-18.8 \\%\\), \\(p = 7.5 \\times 10^{-5}\\)). Lowering \\(\\eta_h\\) to \\(3\\cdot10^{-4}\\) produces \\(5.8\\,\\mathrm{h}\\) (\\(-16.3 \\%\\)). Under 15 \\% token noise OHGW still gains 11.6 \\%. Final validation perplexity improves slightly from \\(24.8 \\pm 0.3\\) to \\(24.6 \\pm 0.3\\); out-of-domain perplexity drops from 32.1 to 31.7. Overhead is 3.4 \\% FLOPs and 1.2 \\% VRAM.\n\n\\subsection{Figures}\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_asha-baseline.pdf }\n    \\caption{Validation accuracy over time for ASHA baseline; higher values indicate better performance.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_asha-ohgw-1step.pdf }\n    \\caption{Validation accuracy over time for ASHA + OHGW (one step); higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_asha-ohgw-3step.pdf }\n    \\caption{Validation accuracy for ASHA + OHGW (three steps); higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_asha-random-warm.pdf }\n    \\caption{Validation accuracy for ASHA with random warm-start; higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_comparison.pdf }\n    \\caption{Accuracy comparison across all ASHA variants; higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_trajectories.pdf }\n    \\caption{Accuracy trajectories across 32 seeds; higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/training\\_loss\\_asha-baseline.pdf }\n    \\caption{Training loss over time for ASHA baseline; lower values indicate better performance.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/training\\_loss\\_asha-ohgw-1step.pdf }\n    \\caption{Training loss for ASHA + OHGW (one step); lower is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/training\\_loss\\_asha-ohgw-3step.pdf }\n    \\caption{Training loss for ASHA + OHGW (three steps); lower is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{ images/training\\_loss\\_asha-random-warm.pdf }\n    \\caption{Training loss for ASHA random warm-start; lower is better.}\n\\end{figure}\n\n\\subsection{Ablation insights}\nRandom warm-start yields \\(<2 \\%\\) improvement, confirming that gradient direction drives efficiency. Additional hyper-steps offer diminishing returns relative to their overhead.\n\n\\subsection{Variance and bias}\nStandard deviation of \\(T@\\tau\\) rises by 5 \\% (vision) and 3 \\% (language), well below the 10 \\% inflation budget. KL divergence between final \\(\\psi\\) distributions is 0.012 (vision) and 0.018 (language), signalling negligible bias.\n\n\\subsection{Aggregate outcome}\nAcross 56 paired seeds, OHGW reduces median time-to-target by 19.5 \\%, preserves or slightly improves final task performance, incurs \\(<4 \\%\\) extra compute, and does not inflate variance\u0019meeting all pre-registered success criteria.",
    "conclusion": "We introduced One-Shot Hyper-Gradient Warm-Starts, a drop-in augmentation for Successive-Halving schedulers that leverages a single, almost-free hyper-gradient to nudge each new configuration before expensive training begins. Without modifying promotion logic or surrogate models, OHGW reduces median time-to-quality by roughly twenty percent on both vision and language benchmarks, adds less than four percent computational overhead, and leaves final metrics unchanged. Ablations demonstrate that the efficiency gain stems from the informative direction of the gradient, not random perturbation, and that additional hyper-steps yield diminishing returns.\n\nPractitioners can adopt OHGW via a five-line wrapper, immediately reclaiming a significant share of wasted GPU hours in existing HPO pipelines. Future work will extend the idea to mixed discrete\u0013continuous spaces, integrate warm-start signals into surrogate-based candidate selection and adaptive-fidelity frameworks \\cite{jiang-2024-efficient,khazi-2023-deep}, and explore privacy-aware or federated settings where the one-shot, low-overhead characteristic of OHGW is particularly advantageous \\cite{panda-2022-new,khodak-2021-federated}. By showing that even a noisy, single-batch hyper-gradient can materially accelerate grey-box optimisation, this work opens the door to deeper synergies between internal training-loop signals and external scheduling strategies."
}
