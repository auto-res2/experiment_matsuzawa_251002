
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
Improving efficiency of hyperparameter optimization

# New Method
{
    "Open Problems": "Even the fastest gray–box and multi-fidelity HPO methods (ASHA, PASHA, DyHPO, BOIL) still waste computation on obviously bad configurations because every trial is treated as a black box; none of the information that is already available inside the training loop – most notably the stochastic hyper-gradient obtained at almost zero cost with automatic differentiation – is used to steer the search. The open problem is: how can we inject very cheap, noisy hyper-gradient signals into existing bandit-style schedulers without redesigning their core logic?",
    "Methods": "We propose ‘One-Shot Hyper-Gradient Warm-Starts’ (OHGW), a drop-in modification for any Successive-Halving style scheduler (Hyperband / ASHA / PASHA).  1. When a new configuration x is sampled it is run for only one **mini-batch** (≈10-2% of a normal epoch).  2. In this first forward / backward pass we keep the compute graph and call automatic differentiation once more to obtain a single stochastic hyper-gradient ∂L/∂ψ for every continuous hyperparameter ψ (learning-rate, weight-decay, momentum …) exactly as in implicit hyper-gradient papers, but **without unrolling** (cost <1.2× normal mini-batch).  3. We apply one hyper-parameter update ψ←ψ−η_h ∂L/∂ψ (η_h is a fixed tiny step such as 10-3).  4. The adjusted configuration x′ – which differs from x by at most one gradient step in each hyper-parameter – is what the scheduler subsequently evaluates for its first rung (e.g. 1 epoch).  5. Everything else (promotion rules, budget doubling, stopping) is untouched.  In effect the scheduler still explores the same region, but every candidate is lightly nudged towards a valley before costly training starts.",
    "Experimental Setup": "Benchmark: CIFAR-10 with ResNet-20 and 5-dim continuous search space {log-lr, log-wd, momentum, augment-magnitude, label-smoothing}.  Scheduler baselines: ASHA, PASHA, DyHPO (their public implementations).  Our variants: ASHA+OHGW, PASHA+OHGW, DyHPO+OHGW (one-line wrapper around trial creation).  Mini-batch for warm-start: 128 images.  Hyper-gradient lr η_h=1e-3, computed with PyTorch autograd; no higher-order terms.  Each method is given the same overall GPU budget (4×V100 for 12 hours) and 50 seeds.  Metrics: (i) best test accuracy reached vs. wall-clock, (ii) total GPU hours until 93% accuracy, (iii) distribution of final hyper-parameters to check bias.",
    "Experimental Code": "# pseudo-code\nfor cfg in scheduler.sample():\n    model = build_model(cfg)\n    data = next(train_loader)            # one mini-batch\n    loss  = forward_loss(model, data)\n    grads = torch.autograd.grad(loss, cfg.continuous_params())\n    with torch.no_grad():               # one hyper step\n        for p,g in zip(cfg.continuous_params(), grads):\n            p -= eta_h * g\n    scheduler.launch(cfg)               # continue as usual",
    "Expected Result": "Across all schedules OHGW cuts the median time-to-93%-accuracy by ≈20% (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while keeping the same final accuracy. The added warm-start costs <3% extra compute. Hyper-parameter distributions remain similar, showing no harmful bias.",
    "Expected Conclusion": "A single stochastic hyper-gradient step collected before the first rung is enough to noticeably reduce wasted resources in bandit-style HPO. Because OHGW requires only two extra autograd calls and no change to the scheduler logic, it can be retro-fitted to almost any existing gray-box optimizer, offering an attractive efficiency boost with negligible engineering effort."
}

# Related Works
{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper introduces Bayesian Optimization for Iterative Learning (BOIL) to efficiently tune hyperparameters for iterative learning algorithms like Deep Reinforcement Learning (DRL) and Convolutional Neural Networks (CNNs). Key contributions include compressing the entire learning curve into a single numeric score that considers both training success and stability, learning this compression function directly from data, and employing a selective data augmentation technique to improve sample-efficiency and prevent Gaussian Process (GP) covariance matrix conditioning issues. BOIL significantly outperforms existing baselines in identifying optimal hyperparameters in less wall-clock time.",
    "Methodology": "BOIL models the cost-sensitive objective function f(x,t) using a Gaussian Process (GP) with a product kernel over hyperparameters x and training iterations t. The training time cost c(x,t) is approximated by a linear regressor. The algorithm selects the next evaluation point by maximizing a cost-aware acquisition function (a modified Expected Improvement criterion) that balances utility, exploration, and computational cost. Learning curves are compressed into a numeric score using a Sigmoid preference function, whose growth and middle point parameters (g0, m0) are learned by maximizing the GP's log marginal likelihood. To augment data efficiently and avoid GP covariance matrix ill-conditioning, a subset of points from the observed learning curve is actively selected at regions of maximum GP predictive uncertainty, with the number of augmented points adaptively controlled by a condition number threshold.",
    "Experimental Setup": "The algorithm's efficiency was demonstrated by tuning hyperparameters for Dueling DQN on the CartPole-v0 environment, and Advantage Actor Critic (A2C) agents on the InvertedPendulum-v2 and Reacher-v2 environments. Additionally, convolutional neural networks were tuned on the SVHN and CIFAR10 datasets. All experiments were conducted on an NVIDIA 1080 GTX GPU using the tensorflow-gpu Python package, with results averaged over 20 independent runs with different random seeds. Square-exponential kernels were used for the GP. Baselines for comparison included Hyperband and Continuous Multi-task/Multi-fidelity BO (CM-T/F-BO). Hyperparameter search ranges for DRL agents and CNNs, along with best-found parameters, were detailed in the appendix.",
    "Limitations": "A primary limitation is the potential for Gaussian Process covariance matrix ill-conditioning if a naive, full-curve data augmentation approach is used, which BOIL addresses with its selective augmentation strategy. Existing early stopping criteria for Bayesian Optimization, such as the exponential decay assumption in Freeze-Thaw BO, were found unsuitable for the unpredictable and noisy fluctuations typical of DRL reward curves. The cost function is approximated by a linear regressor for simplicity, which may not capture more complex dependencies if they exist. A broader impact consideration is that increased automation in ML model training could distance humans from the modeling process, potentially making critical failure detection more challenging.",
    "Future Research Directions": "The framework is highlighted for its broad applicability, extending beyond machine learning to any process exhibiting an iterative structure that can be exploited, such as the optimization of manufacturing pipelines. The work is also viewed as a step towards fully automated machine learning model training and deployment pipelines, suggesting further research in integrating BOIL into such systems. Future directions also include rigorous analysis of final training outcomes using interpretability methods to address the growing opacity of machine learning models and mitigate potential biases.",
    "Experiment Code": "class ProductGaussianProcess(object):\n    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')\n    \n    \n    #def __init__ (self,param):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']\n#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        \"\"\"\n        Fit Gaussian Process model\n\n        Input Parameters\n        ----------\n        x: the observed points \n        t: time or number of episode\n        y: the outcome y=f(x)\n        \n        \"\"\" \n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n        #self.Y_curves=Y_curves[myidx]\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta\n    \n        Euc_dist_t=euclidean_distances(T,T)\n        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       \n    \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)\n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        \"\"\"\n        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter\n        \"\"\"\n\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            # compute K\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            # transform Y_curve to Y_original, then to Y\n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\\\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                # if the matrix is too large, we randomly select a part of the data for fast computation\n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                #Wi, LW, LWi, W_logdet = pdinv(KK)\n                #sign,W_logdet2=np.linalg.slogdet(KK)\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A\n    \n                #second_term=-0.5*W_logdet2\n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            #print(lengthscale, lengthscale_t,midpoint,growth,\"logmarginal:\",logmarginal)\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n#    def optimize_lengthscale_SE_maximizing(self,previous_theta,noise_delta):\n#        \"\"\"\n#        Optimize to select the optimal lengthscale parameter\n#        \"\"\"\n#                \n#        # define a bound on the lengthscale\n#        SearchSpace_lengthscale_min=0.01\n#        SearchSpace_lengthscale_max=0.5\n#        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n#        \n#        mySearchSpace=np.asarray([[SearchSpace_lengthscale_min,SearchSpace_lengthscale_max],\\\n#                             [10*SearchSpace_lengthscale_min,2*SearchSpace_lengthscale_max]])\n#        \n#        # Concatenate new random points to possible existing\n#        # points from self.explore method.           \n#        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, mySearchSpace.shape[0]))\n\n#        #print lengthscale_tries\n\n#        # evaluate\n#        self.flagOptimizeHyperFirst=0 # for efficiency\n\n#        logmarginal_tries=self.log_marginal_lengthscale(lengthscale_tries,noise_delta)\n#        #print logmarginal_tries\n\n#        #find x optimal for init\n#        idx_max=np.argmax(logmarginal_tries)\n#        lengthscale_init_max=lengthscale_tries[idx_max]\n#        #print lengthscale_init_max\n#        \n#        myopts ={'maxiter':20*self.dim,'maxfun':20*self.dim}\n\n#        x_max=[]\n#        max_log_marginal=None\n#        \n#        res = minimize(lambda x: -self.log_marginal_lengthscale(x,noise_delta),lengthscale_init_max,\n#                       SearchSpace=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n#        if 'x' not in res:\n#            val=self.log_marginal_lengthscale(res,noise_delta)    \n#        else:\n#            val=self.log_marginal_lengthscale(res.x,noise_delta)  \n#        \n#        # Store it if better than previous minimum(maximum).\n#        if max_log_marginal is None or val >= max_log_marginal:\n#            if 'x' not in res:\n#                x_max = res\n#            else:\n#                x_max = res.x\n#            max_log_marginal = val\n#            #print res.x\n\n#        return x_max\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        \"\"\"\n        Optimize to select the optimal lengthscale parameter\n        \"\"\"\n        \n        # define a bound on the lengthscale\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        # evaluate\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        #find x optimal for init\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        #print lengthscale_init_max\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        # Store it if better than previous minimum(maximum).\n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n            #print res.x\n\n        return x_max\n\n\n#    def optimize_lengthscale(self,previous_theta_x, previous_theta_t,noise_delta):\n#\n#        prev_theta=[previous_theta_x,previous_theta_t]\n#        newlengthscale,newlengthscale_t=self.optimize_lengthscale_SE_maximizing(prev_theta,noise_delta)\n#        self.hyper['lengthscale_x']=newlengthscale\n#        self.hyper['lengthscale_t']=newlengthscale_t\n#        \n#        # refit the model\n#        temp=np.hstack((self.X,self.T))\n#        ur = unique_rows(temp)\n#        \n#        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n#        \n#        return newlengthscale,newlengthscale_t\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        # optimize both GP lengthscale and logistic hyperparameter\n\n            \n        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        # refit the model\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        # update Y here\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        #\n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        \"\"\"\n        compute variance given X and xTest\n        \n        Input Parameters\n        ----------\n        X: the observed points\n        xTest: the testing points \n        \n        Returns\n        -------\n        diag(var)\n        \"\"\" \n        \n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            #Euc_dist=euclidean_distances(xTest,xTest)\n            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta\n            #ur = unique_rows(X)\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      \n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    \n        \n    def predict(self,xTest, eval_MSE=True):\n        \"\"\"\n        compute predictive mean and variance\n        Input Parameters\n        ----------\n        xTest: the testing points \n        \n        Returns\n        -------\n        mean, var\n        \"\"\"    \n\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        # prevent singular matrix\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        #Exp_dist_test_train_x*Exp_dist_test_train_t\n  \n        # using Cholesky update\n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n    def posterior(self,x):\n        # compute mean function and covariance function\n        return self.predict(self,x)\n        \n    \n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \n        gp_params:                  GP parameters\n        gp_params.theta:            to compute the kernel\n        gp_params.delta:            to compute the kernel\n        \n        func_params:                function to optimize\n        func_params.init bound:     initial SearchSpace for parameters\n        func_params.SearchSpace:        SearchSpace on parameters        \n        func_params.func:           a function to be optimized\n        \n        \n        acq_params:            acquisition function, \n        acq_params.acq_func['name']=['ei','ucb','poi']\n        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))\n\n\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    # this is the Logistic transformation, used in the paper\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    #print(MaxEpisode)\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\ndef return_logistic_curve(midpoint, growth, MaxEpoch=1000):\n    # given the growth, midpoint and npoint, return the Logistic curve for visualization\n    \n    def logistic_func(x):\n        #alpha=32\n        if len(x)==1:\n            return 1.0/(1+np.exp(-growth*(x-midpoint)))\n        else:\n            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]\n        \n    my_xrange_scaled=np.linspace(-6,6, MaxEpoch)\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n    \n    return my_logistic_value_scaled\n\n\ndef transform_logistic_marginal(curves,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    def transform_one_logistic_marginal(curves,MaxEpisode):\n        # curve is a vector [1 x MaxIter]\n    \n        midpoint_list=[-3,-2,-1,0,1]\n        growth_list=[0.1,1,2,3]\n        \n        temp_Y_value=[0]*(len(midpoint_list)*len(growth_list))\n        for idx, (val1, val2) in enumerate(itertools.product(midpoint_list,growth_list)):\n            temp_Y_value[idx]=apply_one_transform_logistic(curves,val1, val2,MaxEpisode)\n                \n        temp_Y_value=np.asarray(temp_Y_value)\n        \n        Y=np.mean(temp_Y_value,axis=0)\n        return Y\n    if len(curves)==1:\n        output=transform_one_logistic_marginal(curves[0],MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=transform_one_logistic_marginal(curve,MaxEpisode)\n    return output    \n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n    \n\n\nclass AcquisitionFunction(object):\n    \"\"\"\n    An object to compute the acquisition functions.\n    \"\"\"\n\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max'] # this is for ei_mu acquisition function\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max'                          ]\n        \n        # check valid acquisition function\n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        #if  not in acq_name:\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        \n        #if type(meta) is dict and 'y_max' in meta.keys():\n        #   y_max=meta['y_max']\n        y_max=np.max(gp.Y)\n        #print self.kind\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max': # using max mu(x) as incumbent\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    def utility_plot(self, x, gp, y_max):\n        if np.any(np.isnan(x)):\n            return 0\n        if self.acq_name == 'ei':\n            return self._ei_plot(x, gp, y_max)\n  \n   \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _lcb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n\n        return mean - np.sqrt(beta_t) * np.sqrt(var) \n        \n    \n    @staticmethod\n    def _ucb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T                \n        \n        # Linear in D, log in t https://github.com/kirthevasank/add-gp-bandits/blob/master/BOLibkky/getUCBUtility.m\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n  \n        #beta=300*0.1*np.log(5*len(gp.Y))# delta=0.2, gamma_t=0.1\n        return mean + np.sqrt(beta_t) * np.sqrt(var) \n    \n    \n    @staticmethod\n    def _ucb_pe(x, gp, kappa, maxlcb):\n        mean, var = gp.predict_bucb(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n\n        value=mean + kappa * np.sqrt(var)        \n        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]\n        var[myidx]=0        \n        return var\n    \n   \n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n        \n   \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)        \n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \n        out[var2<1e-10]=0\n        return out\n \n \n    @staticmethod      \n    def _poi(x, gp,y_max): # run Predictive Entropy Search using Spearmint\n        mean, var = gp.predict(x, eval_MSE=True)    \n        # Avoid points with zero variance\n        var = np.maximum(var, 1e-9 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var)        \n        return norm.cdf(z)\n\n   \ndef unique_rows(a):\n    \"\"\"\n    A functions to trim repeated rows that may appear when optimizing.\n    This is necessary to avoid the sklearn GP object from breaking\n\n    :param a: array to trim repeated rows from\n\n    :return: mask of unique rows\n    \"\"\"\n\n    # Sort array and kep track of where things should go back to\n    order = np.lexsort(a.T)\n    reorder = np.argsort(order)\n\n    a = a[order]\n    diff = np.diff(a, axis=0)\n    ui = np.ones(len(a), 'bool')\n    ui[1:] = (diff != 0).any(axis=1)\n\n    return ui[reorder]\n\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    dim=SearchSpace.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n    #sobol_sequence=generate_sobol_seq(dim=dim,nSobol=500*dim)\n\n    # multi start\n    for i in range(3*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        #x_tries=sobol_sequence\n    \n        # evaluate\n        y_tries=myfunc(x_tries,**kwargs)\n        \n        #find x optimal for init\n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        # Store it if better than previous minimum(maximum).\n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n            #print max_acq\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n    \ndef acq_max_scipy(ac, gp, y_max, bounds):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n\n    # multi start\n    for i in range(1*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        # evaluate\n        y_tries=ac(x_tries,gp=gp)\n        #print \"elapse evaluate={:.5f}\".format(end_eval-start_eval)\n        \n        #find x optimal for init\n        idx_max=np.argmax(y_tries)\n        #print \"max y_tries {:.5f} y_max={:.3f}\".format(np.max(y_tries),y_max)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n\n        \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n            #print max_acq\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])\n        #print max_acq\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n    \n    # COBYLA -> x_max[0]\n    # L-BFGS-B -> x_max",
    "Experiment Result": "BOIL (Bayesian Optimization with Informed Learning) is implemented using a `ProductGaussianProcess` for the objective function and a `linear_model.LinearRegression` for the training time cost. The default acquisition function is 'ei_mu_max' (Expected Improvement using the maximum of the GP mean as incumbent).\n\n**GP Settings (`ProductGaussianProcess`):**\n*   Initial noise variance (`noise_delta`): 5e-4\n*   Initial lengthscale for hyperparameters `x` (`lengthscale_x`): 0.02\n*   Initial lengthscale for training iterations `t` (`lengthscale_t`): 0.2\n*   Hyperparameter optimization: Performed every `2 * dim` iterations.\n    *   Optimization bounds for lengthscales: `x` from 0.03 to 0.3; `t` from `10 * 0.03` to `2 * 0.3` (i.e., 0.3 to 0.6).\n    *   Number of random initial points for multi-start optimization: 20.\n    *   Optimizer options: `maxiter`: `30 * dim`, `maxfun`: `30 * dim`.\n\n**Learning Curve Compression (Sigmoid Preference Function):**\n*   Initial sigmoid midpoint (`midpoint`): 0.0\n*   Initial sigmoid growth (`growth`): 1.0\n*   Optimization bounds for sigmoid parameters: `midpoint` from -2 to 3; `growth` from 0.5 to 2.\n*   Learning method: Maximizing the GP's log marginal likelihood.\n*   Marginalization for final evaluation: The final score is evaluated by marginalizing across predefined lists of sigmoid `midpoint_list` ([-3, -2, -1, 0, 1]) and `growth_list` ([0.1, 1, 2, 3]).\n\n**Acquisition Function Maximization:**\n*   Optimization method: SciPy's L-BFGS-B (`minimize` function).\n*   Multi-start optimization: `1 * dim` restarts for maximization (`acq_max_scipy`), `3 * dim` restarts for minimization (`acq_min_scipy_kwargs`).\n*   Random initial points for optimization: `50 * dim` points for `acq_max_scipy`, `100 * dim` points for `acq_min_scipy_kwargs`.\n*   Optimizer options: `maxiter`: `10 * dim`, `maxfun`: `20 * dim`.\n\n**Data Augmentation (Virtual Observations):**\n*   Mechanism: Subsets of points from observed learning curves are actively selected at regions of maximum GP predictive uncertainty.\n*   Maximum number of augmentations per real observation (`max_n_augmentation`): 10.\n*   Adaptive control: Augmentation stops if the log of the GP covariance matrix's condition number (`log_cond`) exceeds a threshold of 15 (`threshold_cond`) or if the predictive variance is very small (`< 1e-3` after adding noise `1e-3` which is effectively `noise_delta`).\n\n**Initialization:**\n*   Number of initial points (`n_init_points`): 3 (default).\n*   Random seed for initialization (`seed`): 1 (default)."
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "The paper introduces Partitioned Neural Networks, a novel and efficient method for hyperparameter optimization that operates within a single training run and requires no validation data. Inspired by the marginal likelihood, this approach partitions both the training data into K shards and the neural network model's parameters into K partitions. Each parameter partition is optimized on specific data shards, and the 'out-of-training-sample' loss on data shards unseen by a subnetwork is used as the objective for hyperparameter optimization. This method is shown to be significantly computationally cheaper than alternative marginal likelihood-based approaches for neural networks and is particularly beneficial for complex tasks like learning input masks, data augmentations, feature extractors, and in resource-constrained environments such as Federated Learning, where retraining and cross-validation are challenging.",
    "Methodology": "The core methodology involves optimizing an approximation to the lower-bound of the marginal likelihood, LML. The neural network's parameters 'w' are divided into 'C' partitions (w1, ..., wC), and 'C' subnetworks are constructed where the k-th subnetwork w(k)s uses parameters (w1, ..., wk) and sets remaining parameters to default values (ˆwk+1, ..., ˆwC). A partitioned training scheme interleaves stochastic gradient updates for both parameter partitions 'wk' and hyperparameters 'ψ'. Parameter updates for wk are computed using loss on data shards D1:k, while hyperparameter updates for ψ are computed using the sum of out-of-sample losses on shards Dk for subnetworks w(k-1)s. This ensures that a subnetwork only 'sees' data from its designated partitions. The most common partitioning scheme used in experiments is 'random weight partitioning', where a fixed proportion of weights in each layer is randomly assigned to a partition. For Federated Learning, clients are assigned to data chunks, and local updates involve optimizing relevant parameter partitions while only communicating changes to the server, thereby reducing upload costs.",
    "Experimental Setup": "The method was evaluated across various tasks and datasets. For input selection, a synthetic dataset with informative and spurious features was used with fully connected neural networks. For learning invariances through data augmentations, MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet) were used with CNN and FixupResNet architectures (FixupResNet-8, FixupResNet-14, ResNet-50 with GroupNorm). Experiments compared partitioned networks against baselines like standard training, Augerino, Differentiable Laplace, and Last-layer ML. Validation methods included test accuracy and log-likelihood. A differentiable input selection task used a learnable mask with a concrete Bernoulli distribution. Comparisons to traditional train/validation splits for learning data augmentations on CIFAR10 subsets and learning feature extractors on CIFAR10 (Wide ResNet-20) were performed. Federated Learning experiments on non-i.i.d. partitioned MNIST and CIFAR10 (with label-skew and rotation-skew) evaluated performance against FedAvg and FedAvg + Augerino, optimizing data augmentation and dropout rates using CNN and ResNet-9 models. Training details specified Adam or SGD optimizers, varying learning rates, batch sizes, weight decay (scaled for earlier partitions), and specific iteration counts.",
    "Limitations": "The proposed method has several limitations. It inherently requires an additional forward-backward pass for updating hyperparameters, which, despite being significantly less costly than existing methods, introduces extra computational overhead. Empirically, partitioned networks also tend to require more training iterations to converge. Furthermore, partitioning the network introduces a capacity constraint, which can lead to a slight loss in performance compared to a fully optimized, non-partitioned network with ideal hyperparameters. Lastly, the method introduces new hyperparameters related to the partitioning strategy itself (e.g., number of chunks, relative proportions of data and parameters per chunk), which may require additional tuning for optimal performance, although experiments suggest a reasonable level of robustness to these choices.",
    "Future Research Directions": "Future research could explore dynamic partitioning of network parameters during training, allowing for more adaptive and potentially optimal allocation of resources. Investigating alternative partitioning schemes beyond the current random weight or node partitioning, such as those that automatically learn partitioning or are more amenable to single-batch multi-partition updates, is another promising direction. Efforts to alleviate the inherent performance loss due to network partitioning could involve adjusting training rounds or strategically increasing network capacity. Additionally, exploring different sequential updating schemes for partitions in Federated Learning and investigating methods to reduce computational overhead, such as accumulating gradients from different chunks or less frequent hyperparameter updates, are areas for further study.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
    "Main Contributions": "The paper introduces an efficient implicit differentiation algorithm for hyperparameter optimization of Lasso-type models, addressing the challenges of setting regularization parameters for non-smooth objectives. It proposes a novel approach that avoids matrix inversion and scales to high-dimensional data by leveraging solution sparsity. Key contributions include demonstrating that forward iterative differentiation of block coordinate descent (BCD) converges to the true gradient, and presenting an algorithm that decouples regression coefficient and Jacobian computation, thus not requiring the solution of potentially ill-conditioned linear systems. Experiments show that the proposed method outperforms numerous standard hyperparameter optimization techniques for optimizing held-out error or the Stein Unbiased Risk Estimator (SURE).",
    "Methodology": "The methodology casts hyperparameter optimization as a bi-level optimization problem, where the inner problem is a Lasso-type estimator (Lasso or weighted Lasso) and the outer problem optimizes a criterion like held-out loss or SURE. The core is an efficient algorithm for estimating the gradient with respect to hyperparameters (hypergradient) using implicit differentiation. Unlike traditional implicit differentiation, which relies on smooth loss functions, this method is tailored for non-smooth Lasso-type problems. It leverages the fixed-point iteration of proximal BCD algorithms, differentiating the soft-thresholding step to derive a closed-form solution for the Jacobian. The proposed 'Implicit Forward Iterative Differentiation' algorithm decouples the computation of regression coefficients (computed using any state-of-the-art convex solver) from the Jacobian. The Jacobian is then computed by applying forward differentiation recursion steps restricted to the identified support, leading to faster and more stable computation compared to solving a linear system. Hyperparameters are parametrized as e^λ to avoid positivity constraints.",
    "Experimental Setup": "The Python implementation, utilizing Numba for critical BCD loops, is released as an open-source package. For fair comparison, all methods used the same vanilla BCD algorithm (Algorithm 5) with a stopping tolerance of 10^-5. Hypergradient-based methods employed a line-search strategy. Lasso models were initialized with λ = λmax - log(10). Competitors included Implicit, Forward Iterative Differentiation, Grid-search, Random-search, Lattice Hypercube sampling, and Bayesian optimization. Evaluation used held-out loss on real, high-dimensional datasets (rcv1, 20news, finance, split into train/validation/test) and the Stein Unbiased Risk Estimator (SURE) with its differentiable approximation (dof_FDMC) on synthetic data. Synthetic data (n=100, p varied from 200 to 10,000, 5 non-zero coefficients, SNR=3) was used to measure normalized Mean Squared Error (MSE) over 50 repetitions. The method was also applied to a non-convex MCP estimator on rcv1 and 20news.",
    "Limitations": "The theoretical guarantees of the proposed method, particularly regarding Jacobian convergence, rely on the assumption of a unique solution for the inner optimization problem (Lasso or weighted Lasso). While non-unique solution cases are typically rare, the theory does not cover such pathological settings or cases where the solution path is non-continuous. Furthermore, the theory explicitly does not cover non-convex penalty functions like MCP, although the method numerically behaves properly in such scenarios. The overall hyperparameter optimization problem is non-convex, meaning convergence to a global minimum is not guaranteed and relies on proper initialization. Backward iterative differentiation was found to be computationally expensive and memory-intensive for the target problems, leading to its exclusion from most benchmarks. The SURE criterion requires prior knowledge of the noise variance.",
    "Future Research Directions": "Future work could focus on extending the theoretical guarantees to handle non-convex penalty formulations, such as the Minimax Concave Penalty (MCP), which were shown to behave properly numerically but are not covered by the current theory. Another direction is to further explore the integration of the proposed two-step approach with more advanced state-of-the-art Lasso solvers that utilize techniques like active sets or screening rules. This could involve addressing the discontinuities that such algorithms might introduce. Additionally, investigating the behavior and potential adaptations of the algorithm for rare cases where the Lasso solution is non-unique and the solution path is non-continuous could be a fruitful area, possibly leading to methods beyond gradient-based optimization.",
    "Experiment Code": "import numpy as np\nfrom scipy.sparse import issparse\nfrom numba import njit\nfrom numpy.linalg import norm\nimport scipy.sparse.linalg as slinalg\nfrom sparse_ho.utils import init_dbeta0_new, ST\n\n# --- Core helper from sparse_ho/algo/forward.py ---\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    alpha = np.exp(log_alpha)\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    for i in range(max_iter):\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n        # Simplified stopping criterion check\n        if use_stop_crit and i > 0 and (model._get_pobj(dual_var, X, beta, alphas, y) - model._get_pobj(dual_var, X, beta, alphas, y)) <= model._get_pobj0(dual_var, beta, alphas, y) * tol:\n            break\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask) if compute_jac else None\n    \n    # Store dual variables for warm-start if model has dual attribute\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n\n    return mask, dense, jac\n\n\n# --- Implicit Forward Iterative Differentiation from sparse_ho/algo/implicit_forward.py ---\n\nclass ImplicitForward():\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    # Step 1: Compute regression coefficients beta using inner solver\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, model, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n\n    # Step 2: Initialize Jacobian (dbeta) on the identified support\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    # Initialize dual variables for Jacobian computation\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n\n    # Step 3: Compute Jacobian using forward recursion steps restricted to the identified support\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    # Initialize dual Jacobian (ddual_var)\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    residual_norm = [] # Used for stopping criterion\n\n    for i in range(niter_jac):\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        \n        # Stopping criterion for Jacobian computation\n        if use_stop_crit:\n            current_res_norm = model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var, ddual_var, alpha)\n            residual_norm.append(current_res_norm)\n            if i > 1:\n                rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n                if (rel_tol < np.abs(residual_norm[-1]) * tol_jac or residual_norm[-1] < 1e-10):\n                    break\n\n    return dbeta\n\n# --- Example of differentiating the soft-thresholding step (from sparse_ho/models/lasso.py) ---\n\nclass Lasso(): # Simplified Lasso model class for illustration\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    # Placeholder for other required BaseModel methods (e.g., get_L, _init_beta_dual_var, etc.)\n    def get_L(self, X):\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros(n_features)\n        if jac0 is None or not compute_jac:\n            ddual_var = np.zeros(n_samples)\n        else:\n            dbeta[mask0] = jac0.copy()\n            ddual_var = - X[:, mask0] @ jac0.copy()\n        return dbeta, ddual_var\n\n    def _get_pobj0(self, dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    def _get_pobj(self, dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) +\n            np.abs(alphas * beta).sum())\n\n    def _get_jac(self, dbeta, mask):\n        return dbeta[mask]\n\n    def _init_dbeta0(self, mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is not None:\n            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)\n        else:\n            dbeta0_new = np.zeros(size_mat)\n        return dbeta0_new\n\n    def _init_dbeta(self, n_features):\n        dbeta = np.zeros(n_features)\n        return dbeta\n\n    def _init_ddual_var(self, dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    def sign(self, x, log_alpha):\n        return np.sign(x)\n\n    def reduce_X(self, X, mask):\n        return X[:, mask]\n\n    def reduce_y(self, y, mask):\n        return y\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta, dbeta, dual_var, ddual_var, alpha):\n        return norm(ddual_var.T @ ddual_var + n_samples * alpha * sign_beta @ dbeta)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            # Primal update: zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j]) # Soft-thresholding step\n            if compute_jac:\n                # Jacobian update for the soft-thresholding step\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj # Differentiating soft-thresholding\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j] # Gradient w.r.t hyperparameter\n                # update residuals\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,\n                         L, alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features,\n            dbeta, dual_var, ddual_var, L, alpha, sign_beta):\n        for j in range(n_features):\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n",
    "Experiment Result": "The methodology casts hyperparameter optimization as a bi-level optimization problem. The inner problem involves Lasso-type estimators (Lasso, Elastic Net, Sparse Logistic Regression, Weighted Lasso, SVM, SVR, Simplex SVR). The outer problem optimizes a criterion like held-out loss (MSE, Logistic Loss, Smoothed Hinge), Cross-Validation (MSE, Logistic Loss), or the Finite-Difference Monte-Carlo Stein Unbiased Risk Estimator (SURE).\n\n**1. Inner Problem Models and Solvers:**\n-   **Lasso**: `sparse_ho.models.Lasso` wrapping `celer.Lasso(fit_intercept=False, warm_start=True, max_iter=50-10000)`. Hyperparameter: `alpha` (L1 penalty).\n-   **ElasticNet**: `sparse_ho.models.ElasticNet` wrapping `celer.ElasticNet(fit_intercept=False, warm_start=True, max_iter=50-10000)`. Hyperparameters: `alpha_l1`, `alpha_l2`.\n-   **Sparse Logistic Regression**: `sparse_ho.models.SparseLogreg` wrapping `sklearn.linear_model.LogisticRegression(penalty='l1', fit_intercept=False, solver='saga', max_iter=100-10000)`. Hyperparameter: `alpha` (L1 penalty).\n-   **Weighted Lasso**: `sparse_ho.models.WeightedLasso` wrapping `celer.Lasso(fit_intercept=False, warm_start=True, tol=1e-3)`. Hyperparameters: an array of `n_features` `alpha_i`.\n\n**2. Hypergradient Estimation (Implicit Forward Iterative Differentiation):**\n-   **Algorithm**: `sparse_ho.ImplicitForward`.\n-   **Parameters**: `tol_jac` (tolerance for Jacobian computation, e.g., `1e-3`, `1e-8`), `n_iter_jac` (maximum iterations for Jacobian, e.g., `100`, `1000`). Inner solver `max_iter` (`100` to `100_000`) and `tol` (`1e-5` to `1e-8`) are passed to `compute_beta_grad`.\n\n**3. Outer Optimization Criteria:**\n-   **Held-out Mean Squared Error**: `sparse_ho.criterion.HeldOutMSE`.\n-   **Held-out Logistic Loss**: `sparse_ho.criterion.HeldOutLogistic`.\n-   **Held-out Smoothed Hinge Loss**: `sparse_ho.criterion.HeldOutSmoothedHinge`.\n-   **Cross-Validation**: `sparse_ho.criterion.CrossVal` (wraps Held-out MSE or Logistic Loss) with `cv=KFold(n_splits=5, shuffle=True, random_state=42)`.\n-   **Finite-Difference Monte-Carlo SURE**: `sparse_ho.criterion.FiniteDiffMonteCarloSure` with `sigma` (noise level, e.g., `1/np.sqrt(nave)` for MEG), `finite_difference_step` (often heuristic based on `sigma`), and `random_state`.\n-   **Multiclass Logistic Regression**: `sparse_ho.criterion.LogisticMulticlass`.\n\n**4. Outer Optimizers:**\n-   **LineSearch**: `sparse_ho.optimizers.LineSearch(n_outer=10-100, tol=1e-8, t_max=10000)`. Uses adaptive step size.\n-   **GradientDescent**: `sparse_ho.optimizers.GradientDescent(n_outer=10-100, step_size=None or explicit value, p_grad_norm=1-1.9, verbose=True, tol=1e-8, t_max=10000)`. Uses a heuristic adaptive step size.\n-   **Adam**: `sparse_ho.optimizers.Adam(n_outer=10, lr=0.01-0.11, beta_1=0.9, beta_2=0.999, epsilon=1e-3, verbose=False, tol=1e-5, t_max=10000)`.\n\n**5. Datasets:**\n-   **Real-world**: `rcv1.binary`, `rcv1`, `news20`, `real-sim`, `leukemia`, `finance`, `MEG data` (sample audvis). Used via `libsvmdata.datasets.fetch_libsvm`.\n-   **Synthetic (`simu`)**: `sklearn.datasets.make_classification(n_samples=100, n_features=1_000, random_state=42, flip_y=0.02)` or `celer.datasets.make_correlated_data(n_samples=200, n_features=400, snr=5, random_state=0)`.\n\n**6. Hyperparameter Ranges and Initialization:**\n-   **Alpha max (`alpha_max`)**: Empirically determined from training data, e.g., `np.max(np.abs(X[idx_train, :].T.dot(y[idx_train]))) / len(idx_train)`. Specific models might have scaling factors (e.g., Logistic Regression divided by 2 or 4).\n-   **Initial alpha (`alpha0`)**: Typically `0.1 * alpha_max`, `alpha_max / 10`, or `0.9 * alpha_max`. For multi-hyperparameter models (ElasticNet, WeightedLasso), it's an array of initial values.\n-   **Parametrization**: Hyperparameters are optimized in log-space (`log_alpha = np.log(alpha)`) to ensure positivity and facilitate unconstrained optimization. Internal `proj_hyperparam` function clips `log_alpha` to a valid range, e.g., `[log_alpha_max - 12, log_alpha_max + np.log(0.9)]` for Lasso."
}{
    "Title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
    "Main Contributions": "The paper addresses the open problem of hyperparameter optimization (HPO) in differentially private deep learning, which is computationally expensive and complicates privacy accounting. It proposes a novel adaptive HPO method based on a 'new linear scaling rule.' This method uses low-privacy-cost trials (small epsilon) to estimate optimal hyperparameters and then scales them linearly to larger privacy budgets. This approach significantly reduces the computation and privacy cost of HPO, achieves state-of-the-art performance on 22 benchmark tasks across computer vision and natural language processing, and accounts for the privacy cost of HPO, outperforming prior methods.",
    "Methodology": "The core methodology is the 'new linear scaling rule,' which simplifies HPO by reducing the search for optimal learning rate (η) and number of steps (T) to a single scalar variable, r = η × T. The method estimates r for small privacy budgets (ε) using random search to find two empirical points (ε1, r(ε1)) and (ε2, r(ε2)). It then fits a linear approximation to these points to estimate the optimal r for any desired target εf. The theoretical intuition is motivated by analyzing the excess empirical risk of private Gradient Descent (GD) and bounding the 'noisy radius,' the distance between noisy and non-noisy iterates, which is shown to be proportional to η × T. The method incorporates specific design choices to maximize the signal-to-noise ratio and accelerate training: full batch gradient computation (DP-GD), unit clipping (C=1), zero initialization of linear classifiers, SGD with momentum (ρ=0.9), and Privacy Loss Variable (PLV) accounting, which offers tighter privacy guarantees than RDP.",
    "Experimental Setup": "The method was evaluated on 22 benchmark tasks spanning computer vision (ImageNet, CIFAR10/100, FashionMNIST, STL10, EMNIST) and natural language processing (SQuAD, GLUE benchmark tasks: SST-2, QNLI, QQP, MNLI, PersonaChat, WikiText-2, Enron Emails). It also included distribution shift datasets (CIFAR10 -> STL, CIFAR10p1, CIFAR10C, CIFAR100 -> CIFAR100C, Waterbirds, FMoW, Camelyon17). Models evaluated included ResNets and various Transformer architectures (beit, beitv2, convnext, ViT-L, GPT-2, RoBERTa-base), covering both pretraining and fine-tuning scenarios. The privacy budgets (ε) ranged widely from 0.01 to 8.0. Performance was compared against random search, grid search (without accounting for HPO privacy cost), and several prior private HPO methods, including those based on Rényi DP (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023), DPAdamWOSM, and methods by Mehta et al. (2023b). Evaluation metrics included accuracy for classification tasks and perplexity for language modeling.",
    "Limitations": "The theoretical assumptions (e.g., strong convexity, smoothness, bounded gradients) do not universally hold for training complex neural networks, though empirical results support the method's success. The HPO method requires more runtime than simple random search (e.g., 7 GPU hours compared to 1 for random search) and exhibits worse parallelization than grid search due to its sequential adaptive nature. For extremely small privacy budgets (e.g., ε < 0.5 on challenging datasets like ImageNet), the privacy cost of HPO itself can be too high, preventing cheap trials from yielding useful information for the final run. While the method primarily optimizes learning rate (η) and number of steps (T), incorporating additional hyperparameters (e.g., batch size, clipping threshold) can degrade performance. The paper does not focus on tuning the hyperparameters of the HPO method itself (e.g., number of points for the linear approximation).",
    "Future Research Directions": "Future work could extend the theoretical analysis to incorporate additional factors like momentum acceleration and the bias introduced by gradient clipping, as well as generalize it to more complex neural network settings. Investigating Rényi Differential Privacy (RDP) analysis for the adaptive method is another potential direction. Applying the proposed HPO method to alternate threat models, such as scenarios involving small fractions of private training data combined with public data, could be explored. The paper also suggests privatizing other non-private HPO methods and using this HPO approach to tune the number of epochs in parameter-free optimizers like DPAdamWOSM. While the current work primarily uses linear approximation, exploring higher-degree polynomial approximations for the relationship between r and ε could be considered, albeit with increased privacy cost.",
    "Experiment Code": "import collections\nimport logging\nimport math\nimport types\nfrom typing import Callable, Dict, Optional, Sequence, Union\n\nimport torch\nfrom ml_swissknife import utils\nfrom torch import nn\n\nfrom . import autograd_grad_sample, transformers_support\nfrom .accounting import accounting_manager\nfrom .settings import AccountingMode, BackwardHookMode, ClippingMode, SUPPORTED_TRANSFORMERS\n\nlogger = logging.get_logger(__name__)\n\n\nclass PrivacyEngine(object):\n    \"\"\"Differentially-private optimization engine that works gracefully with Hugging Face transformers.\n\n    Supports ghost clipping as described in\n        Li, X., Tramèr, F., Liang, P., & Hashimoto, T. (2021).\n        Large Language Models Can Be Strong Differentially Private Learners.\n        arXiv preprint arXiv:2110.05679.\n\n    Implicitly assumes inputs are in batch first format.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: nn.Module,\n        *,\n        batch_size: int,\n        sample_size: int,\n        max_grad_norm: float,\n        epochs: Optional[Union[int, float]] = None,\n        noise_multiplier: Optional[float] = None,\n        target_epsilon: Optional[float] = None,\n        target_delta: Optional[float] = None,\n        alphas: Sequence[float] = accounting_manager.DEFAULT_ALPHAS,\n        record_snr: bool = True,\n        named_params: Optional[Sequence] = None,\n        numerical_stability_constant=1e-6,\n        clipping_mode=ClippingMode.default,\n        accounting_mode=\"rdp\",\n        eps_error=0.05,\n        skip_checks=False,\n        **unused_kwargs,\n    ):\n        \"\"\"Initialize the engine.\n\n        Args:\n            module: The PyTorch module for which per-sample gradient is required.\n                Setting the `requires_grad` attribute of a parameter to False\n                disables the per-sample gradient accumulation.\n            batch_size: The expected size of Poisson-sampled batch, i.e., the lot size.\n            sample_size: Size of dataset.\n            max_grad_norm: The maximum 2-norm for gradient clipping.\n            epochs: The number of epochs for training.\n            noise_multiplier: The extra multiplier for DP-SGD noise.\n            target_epsilon: The target privacy spending.\n                Only used to estimate the `noise_multiplier` if it is not set.\n            target_delta: The target failure probability.\n                Defaults to sample_size ** -1.1 if not set.\n            alphas: The RDP orders for (ε, δ)-DP conversion. Useless if not accounting in RDP.\n            record_snr: Record and report the signal-to-noise ratio --\n                ratio between norm of summed clipped gradient and norm of noise vector.\n            named_params: Specifies which parameters need gradients;s\n                defaults to use parameters which require grad in module.\n            numerical_stability_constant: Small constant to avoid division by 0 when clipping.\n            clipping_mode: The clipping mode to use. One of 'default', 'ghost', 'per_layer', 'per_layer_percentile'.\n            accounting_mode: The method of accounting privacy. One of (`rdp`, `glw`, `all`).\n                Meanings of shorthands:\n                    - rdp: Account loss with RDP but perform conversion to approx-DP with a procedure defined in\n                        \"The Discrete Gaussian for Differential Privacy\". https://arxiv.org/abs/2004.00010\n                    - glw: Account loss by numerically composing tradeoff functions in f-DP; defined in\n                        \"Numerical composition of differential privacy\". https://arxiv.org/abs/2106.02848\n                    - all: Report loss with all methods listed above.\n            eps_error: Error threshold for upper and lower bound in the GLW accounting procedure.\n            skip_checks: Skips the model type validation test if True.\n        \"\"\"\n        utils.handle_unused_kwargs(unused_kwargs)\n        del unused_kwargs\n        super(PrivacyEngine, self).__init__()\n\n        if clipping_mode not in ClippingMode.all():\n            raise ValueError(f\"Unknown clipping mode {clipping_mode}. Expected one of {ClippingMode.all()}.\")\n        if accounting_mode not in AccountingMode.all():\n            raise ValueError(f\"Unknown accounting mode: {accounting_mode}. Expected one of {AccountingMode.all()}.\")\n        if epochs <= 0.0:\n            raise ValueError(f\"Number of training epochs cannot be non-positive, but found epochs={epochs}\")\n\n        # Privacy parameters.\n        sample_rate = batch_size / sample_size\n        if target_delta is None:\n            target_delta = sample_size ** -1.1\n        if noise_multiplier is None:\n            if target_epsilon is None or epochs is None:\n                raise ValueError(\n                    f\"`target_epsilon` and `epochs` must be specified when `noise_multiplier` is `None`.\"\n                )\n            if accounting_mode in (\"rdp\", \"all\"):\n                manager = accounting_manager.RDPManager(alphas=alphas)\n            else:  # \"glw\"\n                manager = accounting_manager.GLWManager(eps_error=eps_error)\n            noise_multiplier = manager.compute_sigma(\n                target_epsilon=target_epsilon, target_delta=target_delta, sample_rate=sample_rate, epochs=epochs,\n            )\n\n        self.batch_size = batch_size\n        self.sample_size = sample_size\n        self.sample_rate = sample_rate\n        self.max_grad_norm = max_grad_norm\n\n        self.epochs = epochs\n        self.noise_multiplier = noise_multiplier\n        self.effective_noise_multiplier = noise_multiplier / batch_size\n        self.target_epsilon = target_epsilon\n        self.target_delta = target_delta\n        self.alphas = alphas\n        self.eps_error = eps_error\n        self.accounting_mode = accounting_mode\n        self.record_snr = record_snr\n\n        # Internals.\n        self.steps = 0  # Tracks privacy spending.\n\n        # Recording.\n        self.max_clip = None\n        self.min_clip = None\n        self.med_clip = None\n        self.signal = None\n        self.noise = None\n        self.snr = None\n        self.noise_limit = None\n\n        # Record parameters.\n        self.module = module\n        if named_params is None:\n            self.named_params = tuple(\n                (name, param) for (name, param) in module.named_parameters() if param.requires_grad\n            )\n        else:\n            self.named_params = named_params\n        self.num_params = sum(param.numel() for _, param in self.named_params)\n\n        self._locked = False  # Lock the part where noisy gradients is created (in `self.step`) if True.\n        self.numerical_stability_constant = numerical_stability_constant\n        self.clipping_mode = clipping_mode\n        if clipping_mode == ClippingMode.ghost:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)  # Prepare for first backward.\n        else:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.default)  # Extra guard.\n\n        if not isinstance(module, SUPPORTED_TRANSFORMERS) and not skip_checks:\n            raise ValueError(\n                f\"Model type {type(module)} is not supported. Please file an issue if you want this model to be added.\\n\"\n                f\"Currently supported transformers are: {SUPPORTED_TRANSFORMERS}\"\n            )\n        transformers_support.forward_swapper(module=module)  # Fix the position embeddings broadcast issue.\n\n    def lock(self):\n        \"\"\"Run this after noisy clipped gradient is created to prevent tampering with it before parameter update.\"\"\"\n        self._locked = True\n\n    def unlock(self):\n        \"\"\"Run this after parameter update to allow creation of noisy gradient for next step\"\"\"\n        self._locked = False\n\n    def attach(self, optimizer):\n        # `loss_reduction=\"sum\"` super important.\n        autograd_grad_sample.add_hooks(model=self.module, loss_reduction=\"sum\")\n\n        # Override zero grad.\n        def dp_zero_grad(_self, *args, **kwargs):\n            _self.privacy_engine.zero_grad()\n\n        # Override step.\n        def dp_step(_self, **kwargs):\n            closure = kwargs.pop(\"closure\", None)\n\n            _self.privacy_engine.step(**kwargs)\n            _self.original_step(closure=closure)\n            _self.privacy_engine.unlock()  # Only enable creating new grads once parameters are updated.\n            _self.privacy_engine.steps += 1\n\n        def virtual_step(_self, **kwargs):\n            _self.privacy_engine.virtual_step(**kwargs)\n\n        def get_privacy_spent(_self, **kwargs):\n            return _self.privacy_engine.get_privacy_spent(**kwargs)\n\n        def get_training_stats(_self, **kwargs):\n            return _self.privacy_engine.get_training_stats(**kwargs)\n\n        optimizer.privacy_engine = self\n\n        optimizer.original_step = optimizer.step\n        optimizer.step = types.MethodType(dp_step, optimizer)\n\n        optimizer.original_zero_grad = optimizer.zero_grad\n        optimizer.zero_grad = types.MethodType(dp_zero_grad, optimizer)\n\n        optimizer.virtual_step = types.MethodType(virtual_step, optimizer)\n\n        # Make getting info easier.\n        optimizer.get_privacy_spent = types.MethodType(get_privacy_spent, optimizer)\n        optimizer.get_training_stats = types.MethodType(get_training_stats, optimizer)\n\n        self.module.privacy_engine = self\n\n        # Just to be safe, we also override `zero_grad` for module.\n        self.module.original_zero_grad = self.module.zero_grad\n        self.module.zero_grad = types.MethodType(dp_zero_grad, self.module)\n\n        # For easy detaching.\n        self.optimizer = optimizer\n\n    def detach(self):\n        optimizer = self.optimizer\n        optimizer.step = optimizer.original_step\n        optimizer.zero_grad = optimizer.original_zero_grad\n        delattr(optimizer, \"privacy_engine\")\n        delattr(optimizer, \"original_step\")\n        delattr(optimizer, \"original_zero_grad\")\n        delattr(optimizer, \"virtual_step\")\n        delattr(optimizer, \"get_privacy_spent\")\n        delattr(optimizer, \"get_training_stats\")\n\n        module = self.module\n        autograd_grad_sample.remove_hooks(module)\n        autograd_grad_sample.set_hooks_mode(\"default\")  # This is super important when there are multiple attaches!\n        module.zero_grad(skip_grad=True)  # noqa\n        module.zero_grad = module.original_zero_grad\n        delattr(module, \"original_zero_grad\")\n\n    @torch.no_grad()\n    def step(\n        self,\n        loss: torch.Tensor,\n        scale=1.,\n        # Function that takes in named_params and does something.\n        # This option was included to help with another spectrum analysis project.\n        callback: Optional[Callable] = None,\n    ):\n        if loss.dim() != 1:\n            raise ValueError(\n                f\"Expected `loss` to be the per-example loss 1-D tensor, but got a tensor with dims={loss.dim()}.\n            )\n\n        if self.clipping_mode == ClippingMode.ghost:\n            if callback is not None:\n                raise ValueError(\"Ghost clipping does not support `callback` in `optimizer.step`.\")\n            if scale != 1.:\n                raise ValueError(\"Ghost clipping does not support mixed-precision training.\")\n            self._ghost_step(loss=loss)\n        else:\n            self._step(loss=loss, scale=scale, callback=callback)\n\n    @torch.no_grad()\n    def virtual_step(self, loss: torch.Tensor, scale=1.):\n        \"\"\"Virtual step function when there's gradient accumulation.\"\"\"\n        if self.clipping_mode == ClippingMode.ghost:\n            self._ghost_virtual_step(loss=loss)\n        else:\n            self._virtual_step(loss=loss, scale=scale)\n\n    def zero_grad(self, skip_grad=False):\n        for name, param in self.named_params:\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"summed_grad\"):\n                del param.summed_grad\n            if not skip_grad:\n                if hasattr(param, \"grad\"):\n                    del param.grad\n\n    def _create_noisy_clipped_gradient(self):\n        \"\"\"Create noisy clipped gradient for `optimizer.step`.\n\n        Add noise and scale by inverse batch size.\n\n        Notes:\n            In ghost clipping, `summed_grad` stores previous micro-batches; `grad` stores current micro-batch.\n            In default clipping, `summed_grad` stores summed clipped gradients for all micro-batches.\n        \"\"\"\n\n        signals, noises = [], []\n        for name, param in self.named_params:\n            assert hasattr(param, 'summed_grad'), (\n                f\"Internal error: PrivacyEngine should not reach here; \"\n                f\"this means either \"\n                f\"1) there is parameter which requires gradient, but was not used in the computational graph, \"\n                f\"or 2) the backward hook registry failed to find the corresponding module to register.\"\n            )\n            param.grad = param.summed_grad  # Ultra important to override `.grad`.\n\n            if self.record_snr:\n                signals.append(param.grad.reshape(-1).norm(2))\n\n            if self.noise_multiplier > 0 and self.max_grad_norm > 0:\n                noise = torch.normal(\n                    mean=0,\n                    std=self.noise_multiplier * self.max_grad_norm,\n                    size=param.size(),\n                    device=param.device,\n                    dtype=param.dtype,\n                )\n                param.grad += noise\n                if self.record_snr:\n                    noises.append(noise.reshape(-1).norm(2))\n                del noise\n\n            param.grad /= self.batch_size\n\n        if self.record_snr and len(noises) > 0:\n            self.signal, self.noise = tuple(torch.stack(lst).norm(2).item() for lst in (signals, noises))\n            self.noise_limit = math.sqrt(self.num_params) * self.noise_multiplier * self.max_grad_norm\n            self.snr = self.signal / self.noise\n        else:\n            self.snr = math.inf  # Undefined!\n\n        self.lock()  # Make creating new gradients impossible, unless optimizer.step is called.\n\n    # --- ghost clipping ---\n    def _ghost_step(self, loss: torch.Tensor):\n        \"\"\"Run double-backward on per-example loss, then sum up all gradients and noise it.\"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        self._ghost_virtual_step(loss)\n        self._create_noisy_clipped_gradient()\n\n    @torch.no_grad()\n    def _ghost_virtual_step(self, loss: torch.Tensor):\n        \"\"\"Backward twice to accumulate summed clipped gradients in `.summed_grad`.\n\n        We accumulate gradients in `.summed_grad` for micro-batching.\n        All of this copying actually creates a new 2x memory overhead.\n        \"\"\"\n        self._double_backward(loss)\n\n        for name, param in self.named_params:\n            if hasattr(param, 'summed_grad'):\n                param.summed_grad += param.grad\n            else:\n                param.summed_grad = param.grad\n\n            if hasattr(param, \"grad\"):\n                del param.grad\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n\n    @torch.enable_grad()\n    def _double_backward(self, loss: torch.Tensor):\n        \"\"\"Given per-example losses, backward twice to accumulate summed clipped gradients in `.grad`.\"\"\"\n        first_loss = loss.sum()\n        first_loss.backward(retain_graph=True)\n\n        # Prepare for second backward.\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_grad)\n\n        # The first backward might have accumulated things we don't need into `.grad`;\n        # remove it before the second pass to avoid accumulating garbage.\n        for name, param in self.named_params:\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        coef_sample = self.get_coef_sample()\n        second_loss = (coef_sample * loss).sum(dim=0)\n        second_loss.backward()\n\n        # Prepare for first backward (in the next round).\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)\n\n    def get_coef_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient scaling factor for clipping.\"\"\"\n        norm_sample = self.get_norm_sample()\n        return torch.clamp_max(self.max_grad_norm / (norm_sample + self.numerical_stability_constant), 1.)\n\n    def get_norm_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient norms.\"\"\"\n        norm_sample = torch.stack([param.norm_sample for name, param in self.named_params], dim=0).norm(2, dim=0)\n        return norm_sample\n\n    # --- default clipping ---\n    def _step(\n        self,\n        loss,\n        scale,\n        callback,\n    ):\n        \"\"\"Create noisy gradients.\n\n        Should be run right before you call `optimizer.step`.\n\n        This function does 3 things:\n            1) call `loss.backward()`\n            2) clip the current `.grad_sample` and add that to `.summed_grad`\n            3) noise the gradients\n        In mixed-precision training (with amp), the last two steps require knowing the loss scaling factor.\n\n        Args:\n            loss: The per-example loss; a 1-D tensor.\n            scale: The loss up-scaling factor in amp. In full precision, this arg isn't useful.\n        \"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        norm_sample, coef_sample = self._accumulate_summed_grad(loss=loss, scale=scale)\n        # Collect stats for debugging.\n        self.max_clip = coef_sample.max().item()\n        self.min_clip = coef_sample.min().item()\n        self.med_clip = coef_sample.median().item()\n\n        if callback is not None:\n            callback(self)\n        self._create_noisy_clipped_gradient()\n\n    def _virtual_step(self, loss, scale):\n        self._accumulate_summed_grad(loss=loss, scale=scale)\n\n    @torch.no_grad()\n    def _accumulate_summed_grad(self, loss, scale):\n        \"\"\"Accumulate signal by summing clipped gradients.\n\n        Removes `.grad_sample` and `.grad` for each variable that requires grad at the end.\n        \"\"\"\n        with torch.enable_grad():\n            loss.sum(dim=0).backward()\n\n        norm_sample = []\n        for name, param in self.named_params:\n            try:\n                batch_size = param.grad_sample.size(0)\n            except AttributeError as error:\n                args = error.args\n                extra_msg = f\"\\n *** {name} parameter has no grad_sample attribute ***\"\n                error.args = (args[0] + extra_msg, *args[1:])\n                raise error\n            norm = param.grad_sample.reshape(batch_size, -1).norm(2, dim=1)\n            norm_sample.append(norm)\n\n        # The stack operation here is prone to error, thus clarify where the error is.\n        try:\n            norm_sample = torch.stack(norm_sample, dim=0).norm(2, dim=0)\n        except RuntimeError as runtime_error:\n            args = runtime_error.args\n\n            # Get the major shape.\n            shapes = collections.defaultdict(int)\n            for tensor in norm_sample:\n                shapes[tensor.size()] += 1\n\n            # Get the shape that most tensors have.\n            major_shape, major_count = max(shapes.items(), key=lambda x: x[1])\n\n            # Check which tensors don't have the major shape!\n            extra_msg = f\" \\n*** Major shape: {major_shape}\"\n            for (name, param), tensor in zip(list(self.named_params), norm_sample):\n                if tensor.size() != major_shape:\n                    extra_msg += f\", {name} wrong shape: {tensor.size()}\"\n            extra_msg += \" ***\"\n\n            runtime_error.args = (args[0] + extra_msg, *args[1:])\n            raise runtime_error\n\n        coef_sample = torch.clamp_max(\n            self.max_grad_norm * scale / (norm_sample + self.numerical_stability_constant), 1.\n        )\n        for name, param in self.named_params:\n            if not hasattr(param, 'summed_grad'):\n                param.summed_grad = 0.\n            current_device = param.grad_sample.device\n            param.summed_grad += torch.einsum(\"i,i...->...\", coef_sample.to(current_device), param.grad_sample)\n\n            # Aggressive memory saving -- delete everything except `.summed_grad` to save memory!\n            if hasattr(param, \"grad_sample\"):\n                # This must be deleted due to how `privacy_utils::supported_layers_grad_samplers.py` works!\n                #   When a parameter with `.grad_sample` is reused, the per-sample gradients are accumulated!\n                del param.grad_sample\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        return norm_sample, coef_sample\n\n    def get_privacy_spent(\n        self,\n        steps: Optional[int] = None,\n        accounting_mode: Optional[str] = None,\n        lenient=False\n    ) -> Dict:\n        if steps is None:\n            steps = self.steps\n        if accounting_mode is None:\n            accounting_mode = self.accounting_mode\n\n        privacy_results = {}  # Contains stats from all modes.\n        if accounting_mode in (AccountingMode.all_, AccountingMode.rdp):\n            try:\n                manager = accounting_manager.RDPManager(alphas=self.alphas)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps,\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\"RDP accounting failed! Double check privacy parameters.\")\n                if not lenient:\n                    raise err\n\n        if accounting_mode in (AccountingMode.all_, AccountingMode.glw):\n            try:\n                manager = accounting_manager.GLWManager(eps_error=self.eps_error)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\n                    \"Numerical composition of tradeoff functions failed! Double check privacy parameters.\"\n                )\n                if not lenient:\n                    raise err\n\n        return privacy_results\n\n    def get_training_stats():\n        \"\"\"Get the clipping, signal, and noise statistics.\"\"\"\n        return {\n            \"med_clip\": self.med_clip,\n            \"max_clip\": self.max_clip,\n            \"min_clip\": self.min_clip,\n            \"snr\": self.snr,\n            \"signal\": self.signal,\n            \"noise\": self.noise,\n            \"noise_limit\": self.noise_limit,\n        }\n\n    def __repr__(self):\n        return (\n            f\"PrivacyEngine(\\n\"\n            f\"  target_epsilon={self.target_epsilon:.6f}, \\n\"\n            f\"  target_delta={self.target_delta:.6f}, \\n\"\n            f\"  noise_multiplier={self.noise_multiplier:.6f}, \\n\"\n            f\"  effective_noise_multiplier={self.effective_noise_multiplier:.6f}, \\n\"\n            f\"  epochs={self.epochs}, \\n\"\n            f\"  max_grad_norm={self.max_grad_norm}, \\n\"\n            f\"  sample_rate={self.sample_rate}, \\n\"\n            f\"  batch_size={self.batch_size}, \\n\"\n            f\"  accounting_mode={self.accounting_mode}, \\n\"\n            f\"  clipping_mode={self.clipping_mode}\\n\"\n            f\")\"\n        )\n\n\n# File: private_transformers/accounting/accounting_manager.py\nimport abc\nimport math\nfrom typing import Dict, Optional, Union\n\nfrom . import rdp_accounting\n\nDEFAULT_ALPHAS = tuple(1 + x / 10.0 for x in range(1, 100)) + tuple(range(12, 64))  # RDP.\n\n\nclass AccountingManager(abc.ABC):\n    def _get_sigma_with_target_epsilon(\n        self,\n        target_epsilon,\n        target_delta,\n        sample_rate,\n        steps,\n        threshold,\n        sigma_hi_init,\n        sigma_lo_init,\n    ):\n        \"\"\"Binary search σ given ε and δ.\"\"\"\n        if sigma_lo_init > sigma_hi_init:\n            raise ValueError(\"`sigma_lo` should be smaller than `sigma_hi`.\")\n\n        # Find an appropriate region for binary search.\n        sigma_hi = sigma_hi_init\n        sigma_lo = sigma_lo_init\n\n        # Ensure sigma_hi isn't too small.\n        while True:\n            eps = self._compute_epsilon_from_sigma(sigma_hi, sample_rate, target_delta, steps)\n            if eps < target_epsilon:\n                break\n            sigma_hi *= 2\n\n        # Ensure sigma_lo isn't too large.\n        while True:\n            eps = self._compute_epsilon_from_sigma(sigma_lo, sample_rate, target_delta, steps)\n            if eps > target_epsilon:\n                break\n            sigma_lo /= 2\n\n        # Binary search.\n        while sigma_hi - sigma_lo > threshold:\n            sigma = (sigma_hi + sigma_lo) / 2\n            eps = self._compute_epsilon_from_sigma(sigma, sample_rate, target_delta, steps)\n            if eps < target_epsilon:\n                sigma_hi = sigma\n            else:\n                sigma_lo = sigma\n\n        # Conservative estimate.\n        return sigma_hi\n\n    @abc.abstractmethod\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        \"\"\"Override for reporting results.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps) -> float:\n        \"\"\"Override for binary sigma search.\"\"\"\n        raise NotImplementedError\n\n    def compute_sigma(\n        self,\n        target_epsilon: float,\n        target_delta: float,\n        sample_rate: float,\n        epochs: Optional[Union[float, int]] = None,\n        steps=None,\n        threshold=1e-3,\n        sigma_hi_init=4,\n        sigma_lo_init=0.1,\n    ) -> float:\n        if steps is None:\n            if epochs is None:\n                raise ValueError(\"Epochs and steps cannot both be None.\")\n            steps = math.ceil(epochs / sample_rate)\n        return self._get_sigma_with_target_epsilon(\n            target_epsilon=target_epsilon,\n            target_delta=target_delta,\n            sample_rate=sample_rate,\n            steps=steps,\n            threshold=threshold,\n            sigma_hi_init=sigma_hi_init,\n            sigma_lo_init=sigma_lo_init,\n        )\n\n\nclass RDPManager(AccountingManager):\n    def __init__(self, alphas):\n        super(RDPManager, self).__init__()\n        self._alphas = alphas\n\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):\n        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)[\"eps_rdp\"]\n\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        \"\"\"Compute RDP as usual, but convert to (ε, δ)-DP based on the result by Canonne, Kamath, Steinke.\"\"\"\n        rdp = rdp_accounting.compute_rdp(q=sample_rate, noise_multiplier=sigma, steps=steps, orders=self._alphas)\n        eps, alpha = rdp_accounting.get_privacy_spent(orders=self._alphas, rdp=rdp, delta=target_delta)\n        return dict(eps_rdp=eps, alpha_rdp=alpha)\n\n\nclass GLWManager(AccountingManager):\n    def __init__(self, eps_error=0.05):\n        super(GLWManager, self).__init__()\n        self._eps_error = eps_error\n\n    def _compute_epsilon_from_sigma(self, sigma, sample_rate, target_delta, steps):\n        return self.compute_epsilon(sigma, sample_rate, target_delta, steps)[\"eps_upper\"]  # Be conservative.\n\n    def compute_epsilon(self, sigma, sample_rate, target_delta, steps) -> Dict:\n        if steps == 0:\n            return dict(eps_low=None, eps_estimate=None, eps_upper=None)\n\n        from prv_accountant import Accountant\n        accountant = Accountant(\n            noise_multiplier=sigma,\n            sampling_probability=sample_rate,\n            delta=target_delta,\n            eps_error=self._eps_error,\n            max_compositions=steps\n        )\n        eps_low, eps_estimate, eps_upper = accountant.compute_epsilon(num_compositions=steps)\n        return dict(eps_low=eps_low, eps_estimate=eps_estimate, eps_upper=eps_upper)\n\n\n# File: private_transformers/autograd_grad_sample.py\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .settings import BackwardHookMode\nfrom .supported_layers_grad_samplers import _supported_layers_grad_samplers\n\n_hooks_disabled: bool = False\n_hooks_mode = BackwardHookMode.default\n\n\ndef set_hooks_mode(mode):\n    if mode not in BackwardHookMode.all():\n        raise ValueError(f\"Unknown mode for hooks: {mode}; expected one of {BackwardHookMode.all()}.\")\n\n    global _hooks_mode\n    _hooks_mode = mode  # Set mode.\n\n    if _hooks_mode == BackwardHookMode.ghost_grad:  # Second backward pass of ghost clipping doesn't need hooks.\n        disable_hooks()\n    elif _hooks_mode == BackwardHookMode.ghost_norm:  # First backward pass of ghost clipping needs to accumulate norms.\n        enable_hooks()\n\n\ndef get_hooks_mode():\n    global _hooks_mode\n    return _hooks_mode\n\n\ndef requires_grad(module: nn.Module, recurse: bool = False) -> bool:\n    \"\"\"\n    Checks if any parameters in a specified module require gradients.\n\n    Args:\n        module: PyTorch module whose parameters are examined\n        recurse: Flag specifying if the gradient requirement check should\n            be applied recursively to sub-modules of the specified module\n\n    Returns:\n        Flag indicate if any parameters require gradients\n    \"\"\"\n    return any(p.requires_grad for p in module.parameters(recurse))\n\n\ndef add_hooks(model: nn.Module, loss_reduction: str = \"mean\"):\n    r\"\"\"\n    Adds hooks to model to save activations and backprop values.\n    The hooks will\n\n    1. save activations into ``param.activations`` during forward pass.\n    2. compute per-sample gradients and save them in ``param.grad_sample`` during backward pass.\n\n    Args:\n        model: Model to which hooks are added.\n        loss_reduction: Indicates if the loss reduction (for aggregating the gradients) is a sum or a mean operation.\n            Can take values ``sum`` or ``mean``.\n    \"\"\"\n    if hasattr(model, \"autograd_grad_sample_hooks\"):\n        raise ValueError(\"Trying to add hooks twice to the same model\")\n\n    enable_hooks()\n\n    handles = []\n    for name, layer in model.named_modules():\n        if type(layer) in _supported_layers_grad_samplers:\n            if requires_grad(layer, recurse=False):\n                handles.append(layer.register_forward_hook(_capture_activations))\n\n                def this_backward(this_layer, grad_input, grad_output):\n                    return _capture_backprops(this_layer, grad_input, grad_output, loss_reduction)\n\n                # Starting with 1.8.0, use `register_full_backward_hook`.\n                handles.append(layer.register_backward_hook(this_backward))\n\n    model.__dict__.setdefault(\"autograd_grad_sample_hooks\", []).extend(handles)\n\n\ndef remove_hooks(model: nn.Module):\n    \"\"\"Removes hooks added by `add_hooks()`.\"\"\"\n    if not hasattr(model, \"autograd_grad_sample_hooks\"):\n        raise ValueError(\"Asked to remove hooks, but no hooks found\")\n    else:\n        for handle in model.autograd_grad_sample_hooks:\n            handle.remove()\n        del model.autograd_grad_sample_hooks\n\n\ndef disable_hooks():\n    \"\"\"Globally disables all hooks installed by this library.\"\"\"\n    global _hooks_disabled\n    _hooks_disabled = True\n\n\ndef enable_hooks():\n    \"\"\"Globally enables all hooks installed by this library.\"\"\"\n    global _hooks_disabled\n    _hooks_disabled = False\n\n\ndef _capture_activations(layer: nn.Module, inputs: Tuple, outputs: Tuple):\n    \"\"\"Forward hook handler captures and saves activations.\"\"\"\n    if not requires_grad(layer) or not layer.training or _hooks_disabled:\n        return\n\n    if not hasattr(layer, \"activations\"):\n        layer.activations = []\n\n    # This improves on original Opacus and supports additional arguments on top of the (first) activation tensor.\n    stored_inputs = tuple(input_i.detach() if torch.is_tensor(input_i) else input_i for input_i in inputs)\n    layer.activations.append(stored_inputs)\n\n\ndef _capture_backprops(\n    layer: nn.Module,\n    inputs: Tuple[torch.Tensor],\n    outputs: Tuple[torch.Tensor],\n    loss_reduction: str\n):\n    \"\"\"Backward hook handler captures grad_outputs.\"\"\"\n    # This improves on the original Opacus codebase and supports multiple outputs.\n    backprops = tuple(output_i.detach() if torch.is_tensor(output_i) else output_i for output_i in outputs)\n    _compute_grad_sample(layer, backprops, loss_reduction)\n\n\ndef _compute_grad_sample(layer: nn.Module, backprops: Tuple, loss_reduction: str):\n    \"\"\"Computes per-sample gradients with respect to the parameters.\"\"\"\n    if not requires_grad(layer) or not layer.training or _hooks_disabled:\n        return\n\n    if not hasattr(layer, \"activations\"):\n        raise ValueError(f\"No activations detected for {type(layer)}, run forward after add_hooks(model)\")\n\n    # Outside of the LSTM there is \"batch_first\" but not for the Linear inside the LSTM\n    if isinstance(layer.activations, list):\n        A = layer.activations.pop()\n    else:\n        A = layer.activations\n\n    if not hasattr(layer, \"max_batch_len\"):\n        assert torch.is_tensor(A[0]), f\"Internal error: first input of the following layer isn't a Tensor. \\n{layer}\"\n        layer.max_batch_len = _get_batch_size(layer, A[0])\n\n    n = layer.max_batch_len\n    if loss_reduction == \"mean\":\n        B = tuple(B_i * n if torch.is_tensor(B_i) else B_i for B_i in backprops)\n    elif loss_reduction == \"sum\":\n        B = backprops\n    else:\n        raise ValueError(f\"loss_reduction = {loss_reduction}. Only 'sum' and 'mean' losses are supported\")\n\n    # compute grad sample for individual layers\n    compute_layer_grad_sample = _supported_layers_grad_samplers.get(type(layer))\n    compute_layer_grad_sample(layer, A, B)\n\n    if (not isinstance(layer.activations, list) or len(layer.activations) == 0) and hasattr(layer, \"max_batch_len\"):\n        del layer.max_batch_len\n\n\ndef _get_batch_size(layer: nn.Module, grad_sample: torch.Tensor) -> int:\n    r\"\"\"\n    Computes and returns the maximum batch size which is the maximum of the dimension values\n    along 'batch_dim' axis over layer.activations + [grad_sample], where layer.activations is\n    a list. If layer.activations is a not a list, then return grad_sample.shape[batch_dim].\n    \"\"\"\n\n    batch_dim = 0\n    max_batch_len = 0\n    if isinstance(layer.activations, list):\n        for out in layer.activations:\n            assert torch.is_tensor(out[0]), (\n                f\"Internal error: first input of the following layer isn't a Tensor. \\n{layer}\"\n            )\n            if out[0].shape[batch_dim] > max_batch_len:\n                max_batch_len = out[0].shape[batch_dim]\n\n    max_batch_len = max(max_batch_len, grad_sample.shape[batch_dim])\n    return max_batch_len\n\n\n# File: examples/classification/run_classification.py\nimport collections\nimport copy\nimport dataclasses\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport gc\nimport json\nimport logging\nimport os\nfrom typing import Callable, Dict, Optional\n\nfrom filelock import FileLock\nimport numpy as np\nfrom ml_swissknife import utils\nimport torch\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction\nfrom transformers import GlueDataTrainingArguments as DataTrainingArguments\nfrom transformers import GlueDataset\nfrom transformers import HfArgumentParser, set_seed\n\nfrom private_transformers import PrivacyEngine\nfrom .src.common import true_tags\nfrom .src.compiled_args import PrivacyArguments, TrainingArguments, AuxiliaryArguments\nfrom .src.dataset import FewShotDataset\nfrom .src.models import (\n    BertForPromptFinetuning, RobertaForPromptFinetuning, AlbertForPromptFinetuning, DistilBertForPromptFinetuning,\n    resize_token_type_embeddings\n)\nfrom .src.processors import num_labels_mapping, output_modes_mapping, compute_metrics_mapping, bound_mapping\nfrom .src.trainer import Trainer\n\nlogger = logging.getLogger(__name__)\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n    )\n    # Few-shot type\n    #   - finetune: standard fine-tuning\n    #   - prompt: prompt-based fine-tuning\n    #   - prompt-demo: prompt-based fine-tuning with demonstrations\n    few_shot_type: str = field(\n        default='prompt-demo',\n        metadata={\"help\": \"Few-shot learning model type. Choice: finetune, prompt, prompt-demo\"}\n    )\n\n    # Only for BERT-type model\n    random_segment: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to reinitialize the token type embeddings (only for BERT).\"}\n    )\n\n    static_embedding: str = field(\n        default=\"no\"\n    )\n    static_lm_head: str = field(\n        default=\"no\"\n    )\n    attention_only: str = field(\n        default=\"no\"\n    )\n\n    randomly_initialize: str = field(\n        default=\"no\",\n        metadata={\"help\": \"Randomly initialize the model; useful only for ablation studies.\"}\n    )\n\n    def __post_init__(self):\n        self.static_embedding = self.static_embedding.lower() in true_tags  # noqa\n        self.static_lm_head = self.static_lm_head.lower() in true_tags  # noqa\n        self.attention_only = self.attention_only.lower() in true_tags  # noqa\n        self.randomly_initialize = self.randomly_initialize.lower() in true_tags  # noqa\n\n\n@dataclass\nclass DynamicDataTrainingArguments(DataTrainingArguments):\n    \"\"\"\n    Arguments for dynamic training.\n    \"\"\"\n    num_k: Optional[int] = field(\n        default=16,\n        metadata={\"help\": \"Number of training instances per class\"}\n    )\n\n    num_sample: Optional[int] = field(\n        default=16,\n        metadata={\"help\": \"Number of samples (for inference) in fine-tuning with demonstrations\"}\n    )\n\n    num_demo: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"Number of demonstrations from each class\"}\n    )\n\n    auto_demo: bool = field(\n        default=True,\n        metadata={\"help\": \"Automatically generate template for using demonstrations\"}\n    )\n\n    # For prompting\n    template: str = field(\n        default=None,\n        metadata={\"help\": \"Template\"}\n    )\n\n    mapping: str = field(\n        default=None,\n        metadata={\"help\": \"Label word mapping\"}\n    )\n\n    template_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to a txt file that stores all the templates, one per line. Do not set this when prompt_path \"\n                    \"is used\"}\n    )\n\n    mapping_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to a txt file that stores all the label word mappings, one per line. Do not set this when \"\n                    \"prompt_path is used\"}\n    )\n\n    prompt_path: str = field(\n        default=None,\n        metadata={\"help\": \"Path to a txt file that stores all the prompts (templates and mappings), one per line\"}\n    )\n\n    template_id: int = field(\n        default=None,\n        metadata={\"help\": \"Template id if using template_path\"}\n    )\n\n    mapping_id: int = field(\n        default=None,\n        metadata={\"help\": \"Mapping id if using template_path\"}\n    )\n\n    prompt_id: int = field(\n        default=None,\n        metadata={\"help\": \"Prompt id if using prompt_path\"}\n    )\n\n    top_n_template: int = field(\n        default=None,\n        metadata={\"help\": \"Use top-n template in the template path\"}\n    )\n\n    # For logging\n    tag: str = field(\n        default='',\n        metadata={\"help\": \"Set the tag and find the result easier in the log.\"}\n    )\n\n    # For filtering when using demonstrations\n    demo_filter: bool = field(\n        default=False,\n        metadata={\"help\": \"Only use similar instances in demonstrations\"}\n    )\n\n    demo_filter_rate: float = field(\n        default=0.5,\n        metadata={\"help\": \"Only use top-x\\% similar instances in demonstrations\"}\n    )\n\n    demo_filter_model: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Model name for demonstration filter embeddings. Will load embeddings based on the model name.\"}\n    )\n\n    debug_mode: bool = field(\n        default=False,\n        metadata={\"help\": \"Debug mode\"}\n    )\n\n    # For max length\n    double_demo: bool = field(\n        default=False,\n        metadata={\"help\": \"Use double length for using demonstrations\"}\n    )\n\n    first_sent_limit: int = field(\n        default=None,\n        metadata={\"help\": \"Limit the length of the first sentence (i.e., sent_0)\"}\n    )\n\n    other_sent_limit: int = field(\n        default=None,\n        metadata={\"help\": \"Limit the length of sentences other than the first sentence\"}\n    )\n\n    use_full_length: bool = field(\n        default=None,\n        metadata={\"help\": \"Use the full length (512)\"}\n    )\n\n    # GPT-3's in-context learning\n    gpt3_in_context_head: bool = field(\n        default=False,\n        metadata={\"help\": \"GPT-3's in-context learning (context at the beginning)\"}\n    )\n\n    gpt3_in_context_tail: bool = field(\n        default=False,\n        metadata={\"help\": \"GPT-3's in-context learning (context at the end)\"}\n    )\n\n    gpt3_in_context_num: int = field(\n        default=32,\n        metadata={\"help\": \"Number of context examples\"}\n    )\n\n    truncate_head: bool = field(\n        default=False,\n        metadata={\"help\": \"When exceeding the maximum length, truncate the head instead of the tail.\"}\n    )\n\n    # Do not set up the following fields. They are set up automatically.\n    prompt: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use prompt-based fine-tuning\"}\n    )\n    template_list: tuple = field(\n        default=None,\n        metadata={\"help\": \"(DO NOT List of templates (only initialized after the program starts.\"}\n    )\n\n    inference_time_demo: bool = field(\n        default=False,\n        metadata={\"help\": \"Do not use demonstrations during inference time; \"\n                          \"the original paper attaches to each test example a few training examples as demo -- \"\n                          \"apparently this breaks privacy. We turn this off by default here.\"}\n    )\n\n\n@dataclass\nclass DynamicTrainingArguments(TrainingArguments):\n    # For ensemble\n    array_id: int = field(\n        default=-1,\n        metadata={\"help\": \"Array ID (contains seed and hyper-paramter search) to idenfity the model\"}\n    )\n\n    model_id: int = field(\n        default=-1,\n        metadata={\"help\": \"Model ID (contains template information) to identify the model\"}\n    )\n\n    save_logit: bool = field(\n        default=False,\n        metadata={\"help\": \"Save test file logit with name $TASK-$MODEL_ID-$ARRAY_ID.npy\"}\n    )\n\n    save_logit_dir: str = field(\n        default=None,\n        metadata={\"help\": \"Where to save the prediction result\"}\n    )\n\n    # Regularization\n    fix_layers: int = field(\n        default=0,\n        metadata={\"help\": \"Fix bottom-n layers when optimizing\"}\n    )\n\n    # Training\n    save_at_last: bool = field(\n        default=False,\n        metadata={\"help\": \"Instead of saving the best (dev performance) checkpoint, save the last checkpoint\"}\n    )\n\n    # Turn off train/test\n    no_train: bool = field(\n        default=False,\n        metadata={\"help\": \"No training\"}\n    )\n    no_predict: bool = field(\n        default=False,\n        metadata={\"help\": \"No test\"}\n    )\n\n    evaluate_after_training: bool = field(\n        default=True, metadata={\"help\": \"Always run evaluation after training ends.\"}\n    )\n\n    def __post_init__(self):\n        super(DynamicTrainingArguments, self).__post_init__()\n\n\ndef main():\n    parser = HfArgumentParser(\n        (ModelArguments, DynamicDataTrainingArguments, DynamicTrainingArguments, PrivacyArguments, AuxiliaryArguments)\n    )\n    model_args, data_args, training_args, privacy_args, auxiliary_args = parser.parse_args_into_dataclasses()\n\n    if not os.path.exists(training_args.output_dir):\n        print(f\"output_dir doesn't exists, mkdir now: {training_args.output_dir}\")\n        os.makedirs(training_args.output_dir)\n\n    if 'prompt' in model_args.few_shot_type:\n        data_args.prompt = True\n\n    if training_args.no_train:\n        training_args.do_train = False\n    if training_args.no_predict:\n        training_args.do_predict = False\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n    )\n\n    # TODO: Hacky mapping creation. Refactor this in the future.\n    #  Currently gets replace if mapping_id and mapping_path is set.\n    if data_args.task_name == \"sst-2\":\n        data_args.mapping = \"{'0':'terrible','1':'great'}\"\n    elif data_args.task_name == \"mnli\":\n        data_args.mapping = \"{'contradiction': 'no', 'entailment': 'yes', 'neutral': 'maybe'}\"\n    elif data_args.task_name == \"qnli\":\n        data_args.mapping = \"{'not_entailment': 'no', 'entailment': 'yes'}\"\n    elif data_args.task_name == \"qqp\":\n        data_args.mapping = \"{'1': 'yes', '0': 'no'}\"  # 1 -- equivalent, 0 -- not equivalent.\n    else:\n        raise ValueError(f\"Unknown task: {data_args.task_name}\")\n\n    # Load prompt/template/mapping file\n    if data_args.prompt:\n        if data_args.prompt_path is not None:\n            assert data_args.prompt_id is not None\n            prompt_list = []\n            with open(data_args.prompt_path) as f:\n                for line in f:\n                    line = line.strip()\n                    template, mapping = line.split('\\t')\n                    prompt_list.append((template, mapping))\n\n            data_args.template, data_args.mapping = prompt_list[data_args.prompt_id]\n            logger.info(\n                \"Specify load the %d-th prompt: %s | %s\" % (data_args.prompt_id, data_args.template, data_args.mapping))\n        else:\n            if data_args.template_path is not None:\n                with open(data_args.template_path) as f:\n                    data_args.template_list = []\n                    for line in f:\n                        line = line.strip()\n                        if len(line) > 0:\n                            data_args.template_list.append(line)\n\n                # Load top-n templates\n                if data_args.top_n_template is not None:\n                    data_args.template_list = data_args.template_list[:data_args.top_n_template]\n                logger.info(\"Load top-%d templates from %s\" % (len(data_args.template_list), data_args.template_path))\n\n                # ... or load i-th template\n                if data_args.template_id is not None:\n                    data_args.template = data_args.template_list[data_args.template_id]\n                    data_args.template_list = None\n                    logger.info(\"Specify load the %d-th template: %s\" % (data_args.template_id, data_args.template))\n\n            if data_args.mapping_path is not None:\n                assert data_args.mapping_id is not None  # Only can use one label word mapping\n                with open(data_args.mapping_path) as f:\n                    mapping_list = []\n                    for line in f:\n                        line = line.strip()\n                        mapping_list.append(line)\n\n                data_args.mapping = mapping_list[data_args.mapping_id]\n                logger.info(\"Specify using the %d-th mapping: %s\" % (data_args.mapping_id, data_args.mapping))\n\n    # Check save path\n    if (\n        os.path.exists(training_args.output_dir)\n        and os.listdir(training_args.output_dir)\n        and training_args.do_train\n        and not training_args.overwrite_output_dir\n    ):\n        raise ValueError(f\"Output directory ({training_args.output_dir}) already exists.\")\n\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        training_args.local_rank,\n        training_args.device,\n        training_args.n_gpu,\n        bool(training_args.local_rank != -1),\n        training_args.fp16,\n    )\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n\n    # Set seed\n    set_seed(training_args.seed)\n\n    try:\n        num_labels = num_labels_mapping[data_args.task_name]\n        output_mode = output_modes_mapping[data_args.task_name]\n        logger.info(\n            \"Task name: {}, number of labels: {}, output mode: {}\".format(data_args.task_name, num_labels, output_mode))\n    except KeyError:\n        raise ValueError(\"Task not found: %s\" % (data_args.task_name))\n\n    # Automatically generate template for using demonstrations\n    if data_args.auto_demo and model_args.few_shot_type == 'prompt-demo':\n        # GPT-3's in-context learning\n        if data_args.gpt3_in_context_head or data_args.gpt3_in_context_tail:\n            logger.info(\"Automatically convert the template to GPT-3's in-context learning.\")\n            assert data_args.template_list is None\n\n            old_template = data_args.template\n            new_template = old_template + ''\n            old_template = old_template.replace('*cls*', '')\n            # Single sentence or sentence pair?\n            sent_num = 1\n            if \"_1\" in old_template:\n                sent_num = 2\n            for instance_id in range(data_args.gpt3_in_context_num):\n                sub_template = old_template + ''\n                # Replace sent_id\n                for sent_id in range(sent_num):\n                    sub_template = sub_template.replace(\"_{}*\".format(sent_id),\n                                                        \"_{}*\".format(sent_num + sent_num * instance_id + sent_id))\n                # Replace mask\n                sub_template = sub_template.replace(\"*mask*\", \"*labelx_{}*\".format(instance_id))\n                if data_args.gpt3_in_context_tail:\n                    new_template = new_template + sub_template  # Put context at the end\n                else:\n                    new_template = sub_template + new_template  # Put context at the beginning\n            logger.info(\"| {} => {}\".format(data_args.template, new_template))\n            data_args.template = new_template\n        else:\n            logger.info(\"Automatically convert the template to using demonstrations.\")\n            if data_args.template_list is not None:\n                for i in range(len(data_args.template_list)):\n                    old_template = data_args.template_list[i]\n                    new_template = old_template + ''\n                    old_template = old_template.replace('*cls*', '')\n                    # Single sentence or sentence pair?\n                    sent_num = 1\n                    if \"_1\" in old_template:\n                        sent_num = 2\n                    for label_id in range(num_labels):\n                        sub_template = old_template + ''\n                        # Replace sent id\n                        for sent_id in range(sent_num):\n                            sub_template = sub_template.replace(\"_{}*\".format(sent_id),\n                                                                \"_{}*\".format(sent_num + sent_num * label_id + sent_id))\n                        # Replace mask\n                        sub_template = sub_template.replace(\"*mask*\", \"*label_{}*\".format(label_id))\n                        new_template = new_template + sub_template\n                    logger.info(\"| {} => {}\".format(data_args.template_list[i], new_template))\n                    data_args.template_list[i] = new_template\n            else:\n                old_template = data_args.template\n                new_template = old_template + ''\n                old_template = old_template.replace('*cls*', '')\n                # Single sentence or sentence pair?\n                sent_num = 1\n                if \"_1\" in old_template:\n                    sent_num = 2\n                for label_id in range(num_labels):\n                    sub_template = old_template + ''\n                    # Replace sent id\n                    for sent_id in range(sent_num):\n                        sub_template = sub_template.replace(\"_{}\".format(sent_id),\n                                                            \"_{}\".format(sent_num + sent_num * label_id + sent_id))\n                    # Replace mask\n                    sub_template = sub_template.replace(\"*mask*\", \"*label_{}*\".format(label_id))\n                    new_template = new_template + sub_template\n                logger.info(\"| {} => {}\".format(data_args.template, new_template))\n                data_args.template = new_template\n\n    # Create config\n    config = AutoConfig.from_pretrained(\n        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n        num_labels=num_labels,\n        finetuning_task=data_args.task_name,\n        cache_dir=model_args.cache_dir,\n    )\n\n    if 'prompt' in model_args.few_shot_type:\n        if config.model_type == 'roberta':\n            model_fn = RobertaForPromptFinetuning\n        elif config.model_type == 'bert':\n            model_fn = BertForPromptFinetuning\n        elif config.model_type == 'albert':\n            model_fn = AlbertForPromptFinetuning\n        elif config.model_type == 'distilbert':\n            model_fn = DistilBertForPromptFinetuning\n        else:\n            raise NotImplementedError\n    elif model_args.few_shot_type == 'finetune':\n        model_fn = AutoModelForSequenceClassification\n    else:\n        raise NotImplementedError\n    special_tokens = []\n\n    # Create tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n        additional_special_tokens=special_tokens,\n        cache_dir=model_args.cache_dir, use_fast=False\n    )\n    print(f' | tokenizer: {tokenizer}, size: {len(tokenizer)} \\n\\n\\n')\n\n    # Get our special datasets.\n    if model_args.few_shot_type == \"finetune\":\n        assert data_args.num_sample == 1\n        train_dataset = GlueDataset(data_args, tokenizer, mode=\"train\")\n        eval_dataset = (\n            GlueDataset(data_args, tokenizer, mode=\"dev\")\n            if training_args.do_eval else None\n        )\n        test_dataset = (\n            GlueDataset(data_args, tokenizer, mode=\"test\")\n            if training_args.do_predict or training_args.evaluate_test_split\n            else None\n        )\n\n        if eval_dataset is not None:\n            eval_dataset.num_sample = 1\n        if test_dataset is not None:\n            test_dataset.num_sample = 1\n    else:\n        use_demo = \"demo\" in model_args.few_shot_type\n        train_dataset = FewShotDataset(data_args, tokenizer=tokenizer, mode=\"train\", use_demo=use_demo)\n        eval_dataset = (\n            FewShotDataset(data_args, tokenizer=tokenizer, mode=\"dev\", use_demo=use_demo)\n            if training_args.do_eval else None\n        )\n        test_dataset = (\n            FewShotDataset(data_args, tokenizer=tokenizer, mode=\"test\", use_demo=use_demo)\n            if training_args.do_predict or training_args.evaluate_test_split else None\n        )\n    print(f\" *** dataset sizes: \")\n    for _tag, _ds in zip((\"train\", \"valid\", \"test\"), (train_dataset, eval_dataset, test_dataset)):\n        if _ds is not None:\n            print(f'{_tag}: {len(_ds)}')\n    print(f\" ***\")\n\n    set_seed(training_args.seed)\n\n    model = model_fn.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n    )\n    print(\" | model type: \")\n    print(type(model))\n\n    if model_args.attention_only:\n        model.requires_grad_(False)\n        for name, param in model.named_parameters():\n            if 'query' in name or 'value' in name or 'classifier' in name or 'lm_head' in name:\n                param.requires_grad_(True)\n        if model_args.static_lm_head and hasattr(model, 'lm_head'):\n            model.lm_head.requires_grad_(False)\n    else:\n        model.requires_grad_(True)\n        if model_args.static_embedding:\n            model.get_input_embeddings().requires_grad_(False)\n\n    if model_args.randomly_initialize:\n        # Only reinit the params which require gradients.\n        model_old = copy.deepcopy(model)  # Copy pretrained model.\n        model.init_weights()\n\n        params = tuple(model.parameters())\n        params_old = tuple(model_old.parameters())\n        for param, param_old in utils.zip_(params, params_old):\n            if not param.requires_grad:\n                param.data.copy_(param_old.data)\n\n        del model_old\n        gc.collect()\n        torch.cuda.empty_cache()\n    print(f\"attention_only: {model_args.attention_only} | randomly_initialize: {model_args.randomly_initialize}\")\n\n    named_params = [(name, param) for name, param in model.named_parameters() if param.requires_grad]\n    print('Params to update: ')\n    print(json.dumps([name for name, param in named_params], indent=4))\n    num_differentiable_params = utils.count_parameters(model, only_differentiable=True)\n    print(f'Number of differentiable params: {num_differentiable_params / 1e6:.3f} million')\n\n    # For BERT, increase the size of the segment (token type) embeddings\n    if config.model_type == 'bert':\n        model.resize_token_embeddings(len(tokenizer))\n        resize_token_type_embeddings(model, new_num_types=10, random_segment=model_args.random_segment)\n\n    # Pass dataset and argument information to the model\n    if data_args.prompt:\n        model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()\n        print(f\" | Classification label_word_list: {model.label_word_list}\")\n        print(f\"   converted words: {tokenizer.convert_ids_to_tokens(model.label_word_list)}\")\n    if output_modes_mapping[data_args.task_name] == 'regression':\n        # lower / upper bounds\n        model.lb, model.ub = bound_mapping[data_args.task_name]\n        print(f\" | Regression lb: {model.lb}, ub: {model.ub}\")\n    model.model_args = model_args\n    model.data_args = data_args\n    model.tokenizer = tokenizer\n\n    # Build metric\n    def build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:\n        def compute_metrics_fn(p: EvalPrediction):\n            # Note: the eval dataloader is sequential, so the examples are in order.\n            # We average the logits over each sample for using demonstrations.\n            predictions = p.predictions\n            num_logits = predictions.shape[-1]\n            logits = predictions.reshape([eval_dataset.num_sample, -1, num_logits])\n            logits = logits.mean(axis=0)\n\n            if num_logits == 1:\n                preds = np.squeeze(logits)\n            else:\n                preds = np.argmax(logits, axis=1)\n\n            # Just for sanity, assert label ids are the same.\n            label_ids = p.label_ids.reshape([eval_dataset.num_sample, -1])\n            label_ids_avg = label_ids.mean(axis=0)\n            label_ids_avg = label_ids_avg.astype(p.label_ids.dtype)\n            assert (label_ids_avg - label_ids[0]).mean() < 1e-2\n            label_ids = label_ids[0]\n\n            return compute_metrics_mapping[task_name](task_name, preds, label_ids)\n\n        return compute_metrics_fn\n\n    # Initialize our Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        model_args=model_args,\n        privacy_args=privacy_args,\n        auxiliary_args=auxiliary_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=build_compute_metrics_fn(data_args.task_name)\n    )\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)],\n         'weight_decay': training_args.weight_decay},\n        {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = trainer.optimizer = torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=training_args.learning_rate,\n        betas=(training_args.adam_beta1, training_args.adam_beta2),\n        eps=training_args.adam_epsilon,\n    )\n    if training_args.lr_decay:  # Default linear decay.\n        training_setup = trainer.get_training_setup()\n        t_total = training_setup[\"t_total\"]\n        # `trainer.optimizer` is not None here, so no optimizer is created.\n        trainer.create_optimizer_and_scheduler(num_training_steps=t_total)\n    else:\n        trainer.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(trainer.optimizer, lambda _: 1.)\n\n    if privacy_args.non_private:\n        privacy_args.noise_multiplier = 0.\n        privacy_args.per_example_max_grad_norm = None\n    else:\n        total_train_batch_size = training_args.gradient_accumulation_steps * training_args.per_device_train_batch_size\n        privacy_engine = PrivacyEngine(\n            module=model,\n            batch_size=total_train_batch_size,\n            sample_size=len(train_dataset),\n            epochs=training_args.num_train_epochs,\n            max_grad_norm=privacy_args.per_example_max_grad_norm,\n            noise_multiplier=privacy_args.noise_multiplier,\n            target_epsilon=privacy_args.target_epsilon,\n            target_delta=privacy_args.target_delta,\n            accounting_mode=privacy_args.accounting_mode,\n            clipping_mode=privacy_args.clipping_mode,\n            skip_checks=True,\n        )\n        # Originally, it could have been null.\n        privacy_args.noise_multiplier = privacy_engine.noise_multiplier\n        privacy_args.target_delta = privacy_engine.target_delta\n\n        print('privacy_args: ')\n        print(json.dumps(privacy_args.__dict__, indent=4))\n        privacy_engine.attach(optimizer)\n\n    # Training\n    if training_args.do_train:\n        # Write argparse.\n        utils.jdump(\n            {**training_args.__dict__, **model_args.__dict__, **data_args.__dict__, **privacy_args.__dict__},\n            os.path.join(training_args.output_dir, 'argparse.json'),\n            default=lambda x: str(x),\n        )\n        print(data_args.mapping)\n        print(data_args.template)\n\n        # Don't reload.\n        trainer.train(model_path=None)\n        # Use the early stop, so do not save the model in the end (unless specify save_at_last)\n        if training_args.save_at_last:\n            trainer.save_model(training_args.output_dir)\n\n        if trainer.is_world_process_zero():\n            tokenizer.save_pretrained(training_args.output_dir)\n            torch.save(model_args, os.path.join(training_args.output_dir, \"model_args.bin\"))\n            torch.save(data_args, os.path.join(training_args.output_dir, \"data_args.bin\"))\n\n    if training_args.do_eval or training_args.do_predict:\n        # Reload the best checkpoint (for eval or predict).\n        logger.info(\"*** Loading best checkpoint ***\")\n        model = model_fn.from_pretrained(training_args.output_dir)\n        model = model.to(training_args.device)\n        trainer.model = model\n        if data_args.prompt:\n            model.label_word_list = torch.tensor(train_dataset.label_word_list).long().cuda()\n        if output_modes_mapping[data_args.task_name] == 'regression':\n            # lower / upper bounds\n            model.lb, model.ub = bound_mapping[data_args.task_name]\n        model.model_args = model_args\n        model.data_args = data_args\n        model.tokenizer = tokenizer\n\n    # Evaluation\n    final_result = {'time': str(datetime.today())}\n\n    eval_results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Validate ***\")\n\n        eval_datasets = []\n        eval_task_names = []\n        eval_splits = []\n        for split, dataset in zip(('dev', 'test'), (eval_dataset, test_dataset)):\n            if split == \"test\" and not training_args.evaluate_test_split:\n                continue\n\n            eval_datasets.append(dataset)\n            eval_task_names.append(data_args.task_name)\n            eval_splits.append(split)\n\n            # --- lxuechen: This block depends on `split`.\n            if data_args.task_name == \"mnli\":\n                mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n                eval_task_names.append(mnli_mm_data_args.task_name)\n                eval_splits.append(split)\n                if model_args.few_shot_type == \"finetune\":\n                    mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode=split)\n                    mnli_mm_dataset.num_sample = 1\n                    eval_datasets.append(mnli_mm_dataset)\n                else:\n                    eval_datasets.append(\n                        FewShotDataset(\n                            mnli_mm_data_args, tokenizer=tokenizer, mode=split,\n                            use_demo=('demo' in model_args.few_shot_type),\n                        )\n                    )\n            # ---\n\n        results_json = collections.defaultdict(dict)\n        for eval_dataset, eval_task_name, eval_split in zip(eval_datasets, eval_task_names, eval_splits):\n            trainer.compute_metrics = build_compute_metrics_fn(eval_dataset.args.task_name)\n            output = trainer.evaluate(eval_dataset=eval_dataset)\n            eval_result = output.metrics\n\n            # --- lxuechen: My evaluation procedure.\n            if eval_result is not None:\n                if not privacy_args.non_private:\n                    privacy_spent = privacy_engine.get_privacy_spent(accounting_mode=\"all\", lenient=True)\n                    to_record_dict = {**eval_result, **privacy_spent}\n                else:\n                    to_record_dict = eval_result\n\n                if training_args.evaluate_test_split:\n                    results_json[eval_split][eval_task_name] = to_record_dict\n                else:\n                    results_json[eval_task_name] = to_record_dict\n            # ---\n\n        output_path = os.path.join(training_args.output_dir, \"final_results.json\")\n        utils.jdump(results_json, output_path)\n\n    test_results = {}\n    if training_args.do_predict:\n        logging.info(\"*** Test ***\")\n\n        test_datasets = [test_dataset]\n        if data_args.task_name == \"mnli\":\n            mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n            if model_args.few_shot_type == \"finetune\":\n                mnli_mm_dataset = GlueDataset(mnli_mm_data_args, tokenizer, mode=\"test\")\n                mnli_mm_dataset.num_sample = 1\n                test_datasets.append(mnli_mm_dataset)\n            else:\n                test_datasets.append(\n                    FewShotDataset(\n                        mnli_mm_data_args,\n                        tokenizer=tokenizer, mode=\"test\", use_demo=('demo' in model_args.few_shot_type)\n                    )\n                )\n\n        for test_dataset in test_datasets:\n            trainer.compute_metrics = build_compute_metrics_fn(test_dataset.args.task_name)\n            output = trainer.evaluate(eval_dataset=test_dataset)\n            test_result = output.metrics\n\n            output_test_file = os.path.join(\n                training_args.output_dir, f\"test_results_{test_dataset.args.task_name}.txt\"\n            )\n            if trainer.is_world_process_zero():\n                with open(output_test_file, \"w\") as writer:\n                    logger.info(\"***** Test results {} *****\".format(test_dataset.args.task_name))\n                    for key, value in test_result.items():\n                        logger.info(\"  %s = %s\", key, value)\n                        writer.write(\"%s = %s\\n\" % (key, value))\n                        final_result[test_dataset.args.task_name + '_test_' + key] = value\n\n                if training_args.save_logit:\n                    predictions = output.predictions\n                    num_logits = predictions.shape[-1]\n                    logits = predictions.reshape([test_dataset.num_sample, -1, num_logits]).mean(axis=0)\n                    np.save(os.path.join(training_args.save_logit_dir,\n                                         \"{}-{}-{}.npy\".format(test_dataset.task_name, training_args.model_id,\n                                                               training_args.array_id)), logits)\n\n            test_results.update(test_result)\n\n    with FileLock('log.lock'):\n        with open('log', 'a') as f:\n            final_result.update(vars(model_args))\n            final_result.update(vars(training_args))\n            final_result.update(vars(data_args))\n            if 'evaluation_strategy' in final_result:\n                final_result.pop('evaluation_strategy')\n            f.write(str(final_result) + '\\n')\n\n    return eval_results\n\n\nif __name__ == \"__main__\":\n    main()\n\n",
    "Experiment Result": "- **Hyperparameter Optimization (HPO) for learning rate (η) and number of steps (T) via `r = η × T`**: The `PrivacyEngine` is designed to calculate the `noise_multiplier` (sigma) given a target epsilon and number of steps/epochs (`steps = math.ceil(epochs / sample_rate)`), which serves as the building block for the proposed linear scaling rule. The method describes an HPO strategy involving fitting a linear approximation to empirical points (ε, r(ε)), which is a high-level conceptual strategy not directly implemented as a single function within the provided code, but facilitated by the `PrivacyEngine`'s ability to calculate `sigma` based on target `epsilon`.\n- **Full batch gradient computation (DP-GD)**: Achieved through gradient accumulation. The total effective batch size for one update (`batch_size`) is determined by a base batch size scaled by a factor related to the task (`batch_size = int(base_batch_size * factor)`). `gradient_accumulation_steps` are calculated as `batch_size // per_device_train_batch_size`.\n- **Unit clipping (C=1)**: The maximum per-example gradient 2-norm, `per_example_max_grad_norm`, is set to `0.1` in `examples/classification/run_wrapper.py`, not 1.\n- **Zero initialization of linear classifiers**: The code typically loads pre-trained models. For language models, the `lm_head` is explicitly initialized by *cloning* the embedding weights, not zeroing them. For classification tasks, the new classifier head might be randomly initialized by `from_pretrained`.\n- **SGD with momentum (ρ=0.9)**: The optimization is performed using `torch.optim.AdamW`. The `adam_beta1` parameter, which controls the momentum, defaults to `0.9` in `transformers.TrainingArguments`, aligning with the specified momentum.\n- **Privacy Loss Variable (PLV) accounting**: Implemented using `accounting_mode=\"glw\"` (Generalized L-W, also known as f-DP accounting), configurable via `PrivacyArguments`. A `target_epsilon` of `8` and a `target_delta` of `sample_size ** -1.1` (default if not explicitly set) are used.\n- **Noise Multiplier**: Automatically computed via a binary search in `PrivacyEngine`'s `compute_sigma` method if a `target_epsilon` is provided.\n- **Clipping Mode**: The `clipping_mode` can be configured as \"ghost\" or \"default\". \"ghost\" clipping is utilized for improved efficiency.\n- **Learning Rate Schedule**: A linear learning rate decay (`lr_decay=\"yes\"`) is applied by default."
}{
    "Title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
    "Main Contributions": "FastBO proposes a novel multi-fidelity Bayesian optimization (BO) method for Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS). Its main contribution is adaptively identifying the appropriate fidelity for each hyperparameter configuration to fit the surrogate model, addressing a key challenge in existing multi-fidelity methods. This is achieved through the introduction of \"efficient point\" and \"saturation point\" concepts. FastBO's adaptive fidelity identification strategy also provides a generalizable way to extend any single-fidelity method to a multi-fidelity setting, offering strong anytime performance.",
    "Methodology": "FastBO's methodology centers on the concepts of efficient point and saturation point. An 'efficient point' for a configuration is defined as the minimum resource level where doubling resources yields performance improvement below a small threshold (delta1), indicating optimal resource-to-performance balance. A 'saturation point' is defined as the minimum resource level beyond which performance no longer exhibits notable variations (variation below delta2), signifying performance stabilization. The FastBO process involves a warm-up stage to gather early observations, learning curve modeling to estimate these points, evaluating configurations up to their efficient points to update the surrogate model, and a post-processing stage where a small set of promising configurations are resumed to their saturation points to obtain optimal results. This adaptive fidelity identification is adaptable to other single-fidelity methods by evaluating configurations at their efficient points.",
    "Experimental Setup": "FastBO's performance was experimentally evaluated against a range of methods including Random Search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The evaluation was conducted on three established benchmarks: LCBench, NAS-Bench-201, and FCNet. The validation method focused on comparing 'anytime performance,' demonstrating FastBO's ability to gain an advantage earlier and rapidly converge to the global optimum after the initial phase.",
    "Limitations": "The paper does not explicitly detail inherent limitations of FastBO in a dedicated section. However, the future research directions implicitly suggest current constraints or areas for improvement, indicating that FastBO could be refined and expanded to handle larger search spaces and distributed computing systems more effectively, implying potential limitations in its current applicability and scalability within these complex environments.",
    "Future Research Directions": "Future research directions for FastBO include refining and expanding the method to handle larger search spaces and integrating it with distributed computing systems. These extensions aim to significantly improve FastBO's applicability and scalability in more complex and resource-intensive HPO and NAS scenarios.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper addresses the challenge of expensive hyperparameter tuning for iterative deep learning (DL) and deep reinforcement learning (DRL) systems, where traditional Bayesian Optimization (BO) methods ignore valuable intermediate training information. The main contributions include: 1) proposing an algorithm (BOIL) to optimize the learning curve of an ML algorithm using training curve compression, rather than just final averaged performance; 2) introducing an approach to learn the optimal compression curve parameters from data and a data augmentation technique for increased sample-efficiency; and 3) demonstrating BOIL's effectiveness in tuning hyperparameters for DRL agents and convolutional neural networks, outperforming existing baselines in identifying optimal hyperparameters in minimal wall-clock time.",
    "Methodology": "The proposed Bayesian Optimization for Iterative Learning (BOIL) approach exploits the iterative structure of learning algorithms. It models the cost-sensitive black-box function, which takes both hyperparameters (x) and training iterations (t) as input, using a Gaussian Process (GP) with a product kernel. The core idea is to compress the entire learning curve into a single numeric score using a user-preferred Sigmoid (Logistic) function, parameterized by a growth parameter (g0) and a middle point (m0). These parameters (g*0, m*0) are learned dynamically by maximizing the GP's log marginal likelihood. The training data is augmented by selectively including intermediate observations from the learning curve, chosen to maximize GP predictive uncertainty while ensuring the GP covariance matrix remains well-conditioned (condition number below a threshold). The cost function for evaluations is approximated by a linear regressor. The acquisition function selects the next hyperparameter and iteration count by balancing expected improvement (utility) against estimated computation cost.",
    "Experimental Setup": "The algorithm was evaluated on two deep reinforcement learning (DRL) agents and a convolutional neural network (CNN). For DRL, experiments included a Dueling DQN (DDQN) agent on the CartPole-v0 environment and Advantage Actor Critic (A2C) agents on the InvertedPendulum-v2 and Reacher-v2 environments (from OpenAI gym and Mujoco). For DL, hyperparameters for a CNN were tuned on the SVHN dataset and CIFAR10 dataset. All experimental results were averaged over 20 independent runs with different random seeds. The evaluations were conducted on NVIDIA 1080 GTX GPUs using the TensorFlow-GPU Python package. Baselines for comparison included Hyperband and Continuous Multi-Task/Multi-Fidelity BO (CM-T/F-BO). Square-exponential kernels were used for the GP, with parameters estimated by maximizing marginal likelihood. A maximum of 15 augmented points were used, with a natural log of GP condition number threshold of 20.",
    "Limitations": "A primary limitation, though effectively managed by the proposed method, is the potential for redundancy and numerical instability (ill-conditioning of the GP covariance matrix) when naively augmenting training data with the entire learning curve. While BOIL actively selects a subset of points to mitigate this, it highlights a constraint in data handling. Additionally, the cost function is approximated by a linear regressor; for scenarios with more complex cost dependencies on hyperparameters and iterations, a more sophisticated model (e.g., a second GP) might be more appropriate. The inherent noisiness and unpredictable fluctuations of DRL reward curves also pose a general challenge for hyperparameter optimization, which BOIL aims to address more robustly than prior methods like Freeze-Thaw BO.",
    "Future Research Directions": "The authors suggest that their BOIL framework is not limited to machine learning algorithms and can be applied more generally to any process exhibiting an iterative structure that can be exploited. A specific example given is the optimization of manufacturing pipelines, where factory settings are adjusted to increase productivity. Further research could also explore the use of various acquisition functions and kernel choices within the BOIL framework, as these were explicitly omitted from comparison in the current study but could easily be integrated.",
    "Experiment Code": "import numpy as np\nfrom bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import ProductGaussianProcess\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ncounter = 0\n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \n        gp_params:                  GP parameters\n        gp_params.theta:            to compute the kernel\n        gp_params.delta:            to compute the kernel\n        \n        func_params:                function to optimize\n        func_params.init bound:     initial SearchSpace for parameters\n        func_params.SearchSpace:        SearchSpace on parameters        \n        func_params.func:           a function to be optimized\n        \n        \n        acq_params:            acquisition function, \n        acq_params.acq_func['name']=['ei','ucb','poi']\n        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.S\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
    "Experiment Result": "BOIL (Bayesian Optimization for Iterative Learning) settings:\n- Gaussian Process (GP) with Product Kernel (RBF for hyperparameters and time):\n  - Initial GP hyperparameters (`gp.hyper`):\n    - `var`: 1 (standardized data)\n    - `lengthscale_x`: 0.02 (for hyperparameters)\n    - `lengthscale_t`: 0.2 (for time/iterations)\n  - GP noise parameter (`noise_delta`): 5e-4\n  - Upper bound for GP noise (`noise_upperbound`): 1e-2\n  - GP hyperparameters (`lengthscale_x`, `lengthscale_t`) and Logistic hyperparameters (`midpoint`, `growth`) are optimized dynamically by maximizing the GP's log marginal likelihood. Optimization is performed every `2 * dim` iterations.\n  - Bounds for lengthscales during optimization: `[0.03, 0.3]` for `lengthscale_x`, `[0.3, 0.6]` for `lengthscale_t` (derived from `[10*SearchSpace_l_min,2*SearchSpace_l_max]` for `lengthscale_t` and `SearchSpace_l_min=0.03, SearchSpace_l_max=0.3` in `ProductGaussianProcess.optimize_lengthscale_SE_logistic_hyper`).\n- Learning Curve Compression (Sigmoid/Logistic function):\n  - Initial Logistic hyperparameters (`gp.logistic_hyper`):\n    - `midpoint`: 0.0\n    - `growth`: 1.0\n  - Bounds for Logistic hyperparameters during optimization: `midpoint` `[-2, 3]`, `growth` `[0.5, 2]`.\n  - The Logistic function is `1.0 / (1 + np.exp(-growth * (x - midpoint)))`.\n- Acquisition Function (Cost-sensitive Expected Improvement):\n  - Type: `ei_mu_max` (Expected Improvement using the maximum of the GP mean function as incumbent).\n  - Balances expected improvement (`np.log(utility)`) against estimated computation cost (`np.log(mean_cost)`).\n  - The acquisition value is calculated as `np.log(utility) - np.log(mean_cost)` when using 'ei' based acquisition, or `np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))` otherwise. The objective function is then minimized (multiplied by -1).\n  - Acquisition function maximization uses `L-BFGS-B` optimizer with `maxiter=30*dim` and `maxfun=30*dim` (derived from `optimize_lengthscale_SE_logistic_hyper` which uses `myopts`). The search space is `self.scaleSearchSpace`.\n- Data Augmentation (Virtual Observations):\n  - Intermediate observations from learning curves are selectively included.\n  - Selection criteria: Maximize GP predictive uncertainty (`pure_exploration` acquisition function).\n  - Constraint: GP covariance matrix condition number must be below a threshold (`threshold_cond=15`). Augmentation stops if `log(condition_number) > threshold_cond` or predictive variance is too low (`pred_var_value < (self.gp.noise_delta + 1e-3)`).\n  - Maximum number of augmentations (`max_n_augmentation`): 10 times the current iteration count `t_max` (normalized).\n  - Cost of virtual observations is interpolated linearly based on the real observation's cost and the scaled iteration count.\n- Cost Model:\n  - Approximated by a `sklearn.linear_model.LinearRegression` on the combined input (`X`, `T`).\n  - Cost values are normalized to `[0,1]` scale before fitting the linear regressor.\n- Input Scaling: Hyperparameters (X) and time (T) are scaled to `[0,1]` using `MinMaxScaler`."
}{
    "Title": "Scaling Laws for Hyperparameter Optimization",
    "Main Contributions": "The paper introduces Deep Power Laws (DPL), a novel multi-fidelity Hyperparameter Optimization (HPO) method that leverages the power-law nature of learning curves to efficiently tune hyperparameters for Deep Learning (DL) models. DPL proposes a probabilistic surrogate based on an ensemble of deep power law functions, which, when combined with Bayesian optimization, dynamically decides which configurations to pause and train incrementally. The method achieves state-of-the-art results in HPO for DL across diverse modalities and architectures, making HPO for DL more feasible.",
    "Methodology": "DPL models learning curves as power law functions: `f̂(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`, where `g` is a parametric neural network mapping hyperparameter configurations `λ` to the power law coefficients `α, β, γ`, and `b` is the budget (epochs). An ensemble of K such neural networks is trained to provide probabilistic predictions (mean `μ` and variance `σ²`). Bayesian Optimization (BO) with an Expected Improvement (EI) acquisition function is used to recommend the next hyperparameter configuration `λN+1`. Instead of running `λN+1` to full convergence, a multi-fidelity strategy incrementally advances it by a small budget `b_step` (e.g., one epoch). The network is a 2-layer feedforward neural network with 128 units per layer, Leaky ReLU, and GLU non-linearity on `β` and `γ` output units, trained with L1 loss and Adam.",
    "Experimental Setup": "DPL was evaluated against 7 state-of-the-art HPO baselines (Random Search, Hyperband, ASHA, BOHB, DEHB, SMAC, BOCA from Dragonfly Library) on 3 benchmarks covering 59 diverse tasks: LCBench (2,000 configurations, 7 numerical hyperparameters, 35 datasets from AutoML benchmark for feedforward neural networks), PD1 (deep learning benchmark with Transformers, ResNeXt on large vision, statistical modeling, and protein sequence datasets, varying learning curve lengths and 807-2807 configurations), and TaskSet (12 RNN text classification tasks from Adam8p search space with 1000 configurations and 8 continuous hyperparameters). Additionally, experiments were conducted on nanoGPT-Bench for Large Language Models (GPT-2 on OpenWebText, tuning learning rates and warmup steps across 7 model sizes (0.3M to 30M parameters) using embedding size as a fidelity dimension, evaluated on an NVIDIA RTX 2080 GPU, then transferring optimal HPs to a full-scale transformer). A continuous search space experiment on CIFAR10 with EfficientNetV2 was also performed, optimizing learning rate and weight decay.",
    "Limitations": "The uncertainty estimation derived from the Deep Ensemble approach was found to be suboptimal compared to standard BO surrogates like Gaussian Processes. Additionally, training an ensemble incurs extra computational costs due to the necessity of training multiple power law models.",
    "Future Research Directions": "Future work includes investigating the combination of power laws with Gaussian Processes to improve uncertainty estimation, and exploring additional types of fidelities beyond training epochs and model size.",
    "Experiment Code": "import torch\nimport torch.nn as nn\n\n\nclass ConditionedPowerLaw(nn.Module):\n\n    def __init__(\n        self,\n        nr_initial_features=10,\n        nr_units=200,\n        nr_layers=3,\n        use_learning_curve: bool = True,\n        kernel_size: int = 3,\n        nr_filters: int = 4,\n        nr_cnn_layers: int = 2,\n    ):\n        \"\"\"\n        Args:\n            nr_initial_features: int\n                The number of features per example.\n            nr_units: int\n                The number of units for every layer.\n            nr_layers: int\n                The number of layers for the neural network.\n            use_learning_curve: bool\n                If the learning curve should be use in the network.\n            kernel_size: int\n                The size of the kernel that is applied in the cnn layer.\n            nr_filters: int\n                The number of filters that are used in the cnn layers.\n            nr_cnn_layers: int\n                The number of cnn layers to be used.\n        \"\"\"\n        super(ConditionedPowerLaw, self).__init__()\n\n        self.use_learning_curve = use_learning_curve\n        self.kernel_size = kernel_size\n        self.nr_filters = nr_filters\n        self.nr_cnn_layers = nr_cnn_layers\n\n        self.act_func = torch.nn.LeakyReLU()\n        self.last_act_func = torch.nn.GLU()\n        self.tan_func = torch.nn.Tanh()\n        self.batch_norm = torch.nn.BatchNorm1d\n\n        layers = []\n        # adding one since we concatenate the features with the budget\n        nr_initial_features = nr_initial_features\n        if self.use_learning_curve:\n            nr_initial_features = nr_initial_features + nr_filters\n\n        layers.append(nn.Linear(nr_initial_features, nr_units))\n        layers.append(self.act_func)\n\n        for i in range(2, nr_layers + 1):\n            layers.append(nn.Linear(nr_units, nr_units))\n            layers.append(self.act_func)\n\n        last_layer = nn.Linear(nr_units, 3)\n        layers.append(last_layer)\n\n        self.layers = torch.nn.Sequential(*layers)\n\n        cnn_part = []\n        if use_learning_curve:\n            cnn_part.append(\n                nn.Conv1d(\n                    in_channels=2,\n                    kernel_size=(self.kernel_size,),\n                    out_channels=self.nr_filters,\n                ),\n            )\n            for i in range(1, self.nr_cnn_layers):\n                cnn_part.append(self.act_func)\n                cnn_part.append(\n                    nn.Conv1d(\n                        in_channels=self.nr_filters,\n                        kernel_size=(self.kernel_size,),\n                        out_channels=self.nr_filters,\n                    ),\n                ),\n            cnn_part.append(nn.AdaptiveAvgPool1d(1))\n\n        self.cnn = nn.Sequential(*cnn_part)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        predict_budgets: torch.Tensor,\n        evaluated_budgets: torch.Tensor,\n        learning_curves: torch.Tensor,\n    ):\n        \"\"\"\n        Args:\n            x: torch.Tensor\n                The examples.\n            predict_budgets: torch.Tensor\n                The budgets for which the performance will be predicted for the\n                hyperparameter configurations.\n            evaluated_budgets: torch.Tensor\n                The budgets for which the hyperparameter configurations have been\n                evaluated so far.\n            learning_curves: torch.Tensor\n                The learning curves for the hyperparameter configurations.\n        \"\"\"\n        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)\n        if self.use_learning_curve:\n            lc_features = self.cnn(learning_curves)\n            # revert the output from the cnn into nr_rows x nr_kernels.\n            lc_features = torch.squeeze(lc_features, 2)\n            x = torch.cat((x, lc_features), dim=1)\n\n        x = self.layers(x)\n        alphas = x[:, 0]\n        betas = x[:, 1]\n        gammas = x[:, 2]\n\n        output = torch.add(\n            alphas,\n            torch.mul(\n                self.last_act_func(torch.cat((betas, betas))),\n                torch.pow(\n                    predict_budgets,\n                    torch.mul(self.last_act_func(torch.cat((gammas, gammas))), -1)\n                )\n            ),\n        )\n\n        return output\n\nimport logging\nimport os\nimport time\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom scipy.stats import norm\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom data_loader.tabular_data_loader import WrappedDataLoader\nfrom dataset.tabular_dataset import TabularDataset\n\n# Assuming ConditionedPowerLaw is imported from models.conditioned_power_law\n# from models.conditioned_power_law import ConditionedPowerLaw\n\nclass PowerLawSurrogate:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        surrogate_configs: dict = None,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        ensemble_size: int = 5,\n        nr_epochs: int = 250,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 1000,\n        device: str = None,\n        output_path: str = '.',\n        dataset_name: str = 'unknown',\n        pretrain: bool = False,\n        backbone: str = 'power_law',\n        max_value: float = 100,\n        min_value: float = 0,\n        fill_value: str = 'zero',\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for a given dataset.\n            surrogate_configs: dict\n                The model configurations for the surrogate.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            ensemble_size: int\n                The number of members in the ensemble.\n            nr_epochs: int\n                Number of epochs for which the surrogate should be\n                trained.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If for the evaluation metric, the lower the value the better.\n            total_budget: int\n                The total budget given. Used to calculate the initialization\n                percentage.\n            device: str\n                The device where the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            pretrain: bool\n                If the surrogate will be pretrained before with a synthetic\n                curve.\n            backbone: str\n                The backbone, which can either be 'power_law' or 'nn'.\n            max_value: float\n                The maximal value for the dataset.\n            min_value: float\n                The minimal value for the dataset.\n            fill_value: str = 'zero',\n                The filling strategy for when learning curves are used.\n                Either 'zero' or 'last' where last represents the last value.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n        self.total_budget = total_budget\n        self.fill_value = fill_value\n        self.max_value = max_value\n        self.min_value = min_value\n        self.backbone = backbone\n\n        self.model_instances = [\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n        ]\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.learning_rate = 0.001\n        self.batch_size = 64\n        self.refine_batch_size = 64\n\n        self.criterion = torch.nn.L1Loss()\n        self.hp_candidates = hp_candidates\n\n        self.minimization = minimization\n        self.seed = seed\n\n        self.logger = logging.getLogger('power_law')\n        logging.basicConfig(\n            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',\n            level=logging.INFO,\n            force=True,\n        )\n\n        self.fraction_random_configs = 0.1\n        self.iteration_probabilities = np.random.rand(self.total_budget)\n\n        self.examples = dict()\n        self.performances = dict()\n\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.seeds = np.random.choice(100, ensemble_size, replace=False)\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.ensemble_size = ensemble_size\n        self.nr_epochs = nr_epochs\n        self.refine_nr_epochs = 20\n        self.fantasize_step = fantasize_step\n\n        self.pretrain = pretrain\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        init_budgets = [i for i in range(1, conf_individual_budget + 1)]\n\n        self.rand_init_conf_indices = []\n        self.rand_init_budgets = []\n\n        for config_index in init_conf_indices:\n            for config_budget in init_budgets:\n                self.rand_init_conf_indices.append(config_index)\n                self.rand_init_budgets.append(config_budget)\n\n        self.initial_random_index = 0\n\n        if surrogate_configs is None:\n\n            self.surrogate_configs = []\n            for i in range(0, self.ensemble_size):\n                self.surrogate_configs.append(\n                    {\n                        'nr_units': 128,\n                        'nr_layers': 2,\n                        'kernel_size': 3,\n                        'nr_filters': 4,\n                        'nr_cnn_layers': 2,\n                        'use_learning_curve': False,\n                    }\n                )\n        else:\n            self.surrogate_configs = surrogate_configs\n\n        self.nr_features = self.hp_candidates.shape[1]\n        self.best_value_observed = np.inf\n\n        self.diverged_configs = set()\n\n        self.models = []\n        self.last_point = None\n\n        self.initial_full_training_trials = 10\n\n        self.train = True\n\n        self.refine_counter = 0\n        self.iterations_counter = 0\n        self.info_dict = dict()\n\n        self.suggest_time_duration = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n    def _prepare_dataset(self) -> TabularDataset:\n        \"\"\"This method is called to prepare the necessary training dataset\n        for training a model.\n\n        Returns:\n            train_dataset: A dataset consisting of examples, labels, budgets\n                and learning curves.\n        \"\"\"\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_curves = self.prepare_training_curves(train_budgets, train_curves)\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_dataset = TabularDataset(\n            train_examples,\n            train_labels,\n            train_budgets,\n            train_curves,\n        )\n\n        return train_dataset\n\n    def _refine_surrogate(self):\n        \"\"\"Refine the surrogate model.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started refining model with index: {model_index}')\n            refined_model = self.train_pipeline(\n                model_index,\n                train_dataset,\n                nr_epochs=self.refine_nr_epochs,\n                refine=True,\n                weight_new_example=True,\n                batch_size=self.refine_batch_size,\n            )\n\n            self.models[model_index] = refined_model\n\n    def _train_surrogate(self, pretrain: bool = False):\n        \"\"\"Train the surrogate model.\n\n        Trains all the models of the ensemble\n        with different initializations and different\n        data orders.\n\n        Args:\n            pretrain: bool\n                If we have pretrained weights and we will just\n                refine the models.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started training model with index: {model_index}')\n\n            if pretrain:\n                # refine the models that were already pretrained\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.refine_nr_epochs,\n                    refine=True,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.refine_nr_epochs, # basically no early stopping\n                )\n                self.models[model_index] = trained_model\n            else:\n                # train the models for the first time\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.nr_epochs,\n                    refine=False,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.nr_epochs, # basically no early stopping\n                )\n                self.models.append(trained_model)\n\n    def train_pipeline(\n        self,\n        model_index: int,\n        train_dataset: TabularDataset,\n        nr_epochs: int,\n        refine: bool = False,\n        weight_new_example: bool = True,\n        batch_size: int = 64,\n        early_stopping_it: int = 10,\n        activate_early_stopping: bool = False,\n    ) -> torch.nn.Module:\n        \"\"\"Train an algorithm to predict the performance\n        of the hyperparameter configuration based on the budget.\n\n        Args:\n            model_index: int\n                The index of the model.\n            train_dataset: TabularDataset\n                The tabular dataset featuring the examples, labels,\n                budgets and curves.\n            nr_epochs: int\n                The number of epochs to train the model for.\n            refine: bool\n                If an existing model will be refined or if the training\n                will start from scratch.\n            weight_new_example: bool\n                If the last example that was added should be weighted more\n                by being included in every batch. This is only applicable\n                when refine is True.\n            batch_size: int\n                The batch size to be used for training.\n            early_stopping_it: int\n                The early stopping iteration patience.\n            activate_early_stopping: bool\n                Flag controlling the activation.\n\n        Returns:\n            model: torch.nn.Module\n                A trained model.\n        \"\"\"\n        if model_index == 0:\n            self.iterations_counter += 1\n            self.logger.info(f'Iteration number: {self.iterations_counter}')\n\n        surrogate_config = self.surrogate_configs[model_index]\n        seed = self.seeds[model_index]\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if refine:\n            model = self.models[model_index]\n        else:\n            model = self.model_instances[model_index](\n                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,\n                nr_units=surrogate_config['nr_units'],\n                nr_layers=surrogate_config['nr_layers'],\n                use_learning_curve=surrogate_config['use_learning_curve'],\n                kernel_size=surrogate_config['kernel_size'],\n                nr_filters=surrogate_config['nr_filters'],\n                nr_cnn_layers=surrogate_config['nr_cnn_layers'],\n            )\n            model.to(self.dev)\n\n        # make the training dataset here\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n        )\n        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)\n        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n\n        patience_rounds = 0\n        best_loss = np.inf\n        from copy import deepcopy\n        best_state = deepcopy(model.state_dict())\n\n        for epoch in range(0, nr_epochs):\n            running_loss = 0\n            model.train()\n\n            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:\n\n                nr_examples_batch = batch_examples.shape[0]\n                if nr_examples_batch == 1:\n                    continue\n\n                optimizer.zero_grad(set_to_none=True)\n\n                # in case we are refining, we add the new example to every\n                # batch to give it more importance.\n                if refine and weight_new_example:\n                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point\n                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)\n                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])\n                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs\n                    newp_performance = np.array([newp_performance], dtype=np.single)\n                    from copy import deepcopy\n                    modified_curve = deepcopy(newp_curve)\n\n                    difference = self.max_benchmark_epochs - len(modified_curve) - 1\n                    if difference > 0:\n                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)\n\n                    modified_curve = np.array([modified_curve], dtype=np.single)\n                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)\n\n                    # add depth dimension to the train_curves array and missing_value_matrix\n                    modified_curve = np.expand_dims(modified_curve, 1)\n                    newp_missing_values = np.expand_dims(newp_missing_values, 1)\n                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)\n\n                    new_example = torch.tensor(new_example, device=self.dev)\n                    newp_budget = torch.tensor(newp_budget, device=self.dev)\n                    newp_performance = torch.tensor(newp_performance, device=self.dev)\n                    modified_curve = torch.tensor(modified_curve, device=self.dev)\n\n                    batch_examples = torch.cat((batch_examples, new_example))\n                    batch_budgets = torch.cat((batch_budgets, newp_budget))\n                    batch_labels = torch.cat((batch_labels, newp_performance))\n                    batch_curves = torch.cat((batch_curves, modified_curve))\n\n                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)\n                loss = self.criterion(outputs, batch_labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n\n            running_loss = running_loss / len(train_dataloader)\n            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')\n\n            if activate_early_stopping:\n                if running_loss < best_loss:\n                    best_state = deepcopy(model.state_dict())\n                    best_loss = running_loss\n                    patience_rounds = 0\n                elif running_loss > best_loss:\n                    patience_rounds += 1\n                    if patience_rounds == early_stopping_it:\n                        model.load_state_dict(best_state)\n                        self.logger.info(f'Stopping training since validation loss is not improving')\n                        break\n\n        if activate_early_stopping:\n            model.load_state_dict(best_state)\n\n        return model\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the ensemble.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, real_budgets:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices and budgets.\n\n        \"\"\"\n        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()\n        # scale budgets to [0, 1]\n        budgets = np.array(budgets, dtype=np.single)\n        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)\n        budgets = budgets / self.max_benchmark_epochs\n        real_budgets = np.array(real_budgets, dtype=np.single)\n        configurations = np.array(configurations, dtype=np.single)\n\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n        hp_curves = torch.tensor(hp_curves)\n        hp_curves = hp_curves.to(device=self.dev)\n        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)\n        network_real_budgets.to(device=self.dev)\n        all_predictions = []\n\n        for model in self.models:\n            model = model.eval()\n            predictions = model(configurations, budgets, network_real_budgets, hp_curves)\n            all_predictions.append(predictions.detach().cpu().numpy())\n\n        mean_predictions = np.mean(all_predictions, axis=0)\n        std_predictions = np.std(all_predictions, axis=0)\n\n        return mean_predictions, std_predictions, hp_indices, real_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"Suggest a hyperparameter configuration and a budget\n        to evaluate.\n\n        Returns:\n            suggested_hp_index, budget: Tuple[int, int]\n                The index of the hyperparamter configuration to be evaluated\n                and the budget for what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n\n        if self.initial_random_index < len(self.rand_init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. \\n'\n                'Returning randomly sampled configuration'\n            )\n            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]\n            budget = self.rand_init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n        else:\n            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n            )\n            # actually do the mapping between the configuration indices and the best prediction\n            # index\n            suggested_hp_index = hp_indices[best_prediction_index]\n\n            if suggested_hp_index in self.examples:\n                evaluated_budgets = self.examples[suggested_hp_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        return suggested_hp_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        hp_curve: List[float],\n    ):\n        \"\"\"Receive information regarding the performance of a hyperparameter\n        configuration that was suggested.\n\n        Args:\n            hp_index: int\n                The index of the evaluated hyperparameter configuration.\n            b: int\n                The budget for which the hyperparameter configuration was evaluated.\n            hp_curve: List\n                The performance of the hyperparameter configuration.\n        \"\"\"\n        for index, curve_element in enumerate(hp_curve):\n            if np.isnan(curve_element):\n                self.diverged_configs.add(hp_index)\n                # only use the non-nan part of the curve and the corresponding\n                # budget to still have the information in the network\n                hp_curve = hp_curve[0:index + 1]\n                b = index\n                break\n\n        from copy import deepcopy\n        if not self.minimization:\n            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)\n            hp_curve = hp_curve.tolist()\n\n        best_curve_value = min(hp_curve)\n\n        self.examples[hp_index] = np.arange(1, b + 1)\n        self.performances[hp_index] = hp_curve\n\n        if self.best_value_observed > best_curve_value:\n            self.best_value_observed = best_curve_value\n            self.no_improvement_patience = 0\n            self.logger.info(f'New Incumbent value found '\n                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')\n        else:\n            self.no_improvement_patience += 1\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.train = True\n                self.no_improvement_patience = 0\n                self.logger.info(\n                    'No improvement in the incumbent value threshold reached, '\n                    'restarting training from scratch'\n                )\n\n        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0\n        if self.initial_random_index >= len(self.rand_init_conf_indices):\n            performance = self.performances[hp_index]\n            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])\n\n            if self.train:\n                # delete the previously stored models\n                self.models = []\n                if self.pretrain:\n                    # TODO Load the pregiven weights.\n                    pass\n\n                self._train_surrogate(pretrain=self.pretrain)\n\n                if self.iterations_counter <= self.initial_full_training_trials:\n                    self.train = True\n                else:\n                    self.train = False\n            else:\n                self.refine_counter += 1\n                self._refine_surrogate()\n\n    @staticmethod\n    def acq(\n        best_values: np.ndarray,\n        mean_predictions: np.ndarray,\n        std_predictions: np.ndarray,\n        explore_factor: float = 0.25,\n        acq_choice: str = 'ei',\n    ) -> np.ndarray:\n        \"\"\"\n        Calculate the acquisition function based on the network predictions.\n\n        Args:\n        -----\n        best_values: np.ndarray\n            An array with the best value for every configuration.\n            Depending on the implementation it can be different for every\n            configuration.\n        mean_predictions: np.ndarray\n            The mean values of the model predictions.\n        std_predictions: np.ndarray\n            The standard deviation values of the model predictions.\n        explore_factor: float\n            The explore factor, when ucb is used as an acquisition\n            function.\n        acq_choice: str\n            The choice for the acquisition function to use.\n\n        Returns\n        -------\n        acq_values: np.ndarray\n            The values of the acquisition function for every configuration.\n        \"\"\"\n        if acq_choice == 'ei':\n            z = (np.subtract(best_values, mean_predictions))\n            difference = deepcopy(z)\n            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]\n            zero_std_indicator = np.invert(not_zero_std_indicator)\n            z = np.divide(z, std_predictions, where=not_zero_std_indicator)\n            np.place(z, zero_std_indicator, 0)\n            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))\n        elif acq_choice == 'ucb':\n            # we are working with error rates so we multiply the mean with -1\n            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)\n        elif acq_choice == 'thompson':\n            acq_values = np.random.normal(mean_predictions, std_predictions)\n        else:\n            acq_values = mean_predictions\n\n        return acq_values\n\n    def find_suggested_config(\n            self,\n            mean_predictions: np.ndarray,\n            mean_stds: np.ndarray,\n    ) -> int:\n        \"\"\"Return the hyperparameter with the highest acq function value.\n\n        Given the mean predictions and mean standard deviations from the DPL\n        ensemble for every hyperparameter configuraiton, return the hyperparameter\n        configuration that has the highest acquisition function value.\n\n        Args:\n            mean_predictions: np.ndarray\n                The mean predictions of the ensemble for every hyperparameter\n                configuration.\n            mean_stds: np.ndarray\n                The standard deviation predictions of the ensemble for every\n                hyperparameter configuration.\n\n        Returns:\n            max_value_index: int\n                the index of the maximal value.\n\n        \"\"\"\n        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])\n        acq_func_values = self.acq(\n            best_values,\n            mean_predictions,\n            mean_stds,\n            acq_choice='ei',\n        )\n\n        max_value_index = np.argmax(acq_func_values)\n\n        return max_value_index\n\n    # other helper methods for PowerLawSurrogate are omitted for brevity",
    "Experiment Result": "DPL models learning curves as power law functions: `f̂(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`. The function `g(λ)` is implemented as a neural network named `ConditionedPowerLaw`.\n\n**Neural Network (`ConditionedPowerLaw`) Settings:**\n*   **Architecture:** 2-layer feedforward neural network.\n*   **Units per layer:** 128 units.\n*   **Activation Functions:** Leaky ReLU for hidden layers, GLU non-linearity applied to the `β` and `γ` output units (implicitly handled by `self.last_act_func(torch.cat((betas, betas)))` and `self.last_act_func(torch.cat((gammas, gammas)))` in the forward pass).\n*   **Output Units:** 3 outputs corresponding to `α`, `β`, and `γ` coefficients.\n\n**Ensemble Settings:**\n*   **Ensemble Size (K):** 5 neural networks.\n\n**Training Settings:**\n*   **Loss Function:** L1 loss (`torch.nn.L1Loss`).\n*   **Optimizer:** Adam optimizer (`torch.optim.Adam`).\n*   **Learning Rate:** 0.001.\n*   **Batch Size:** 64 for initial training, 64 for refinement.\n*   **Initial Training Epochs:** 250 epochs.\n*   **Refinement Epochs:** 20 epochs.\n\n**Bayesian Optimization (BO) and Multi-fidelity Strategy:**\n*   **Acquisition Function:** Expected Improvement (EI).\n*   **Budget Step (`b_step`):** 1 epoch. New hyperparameter configurations are incrementally advanced by 1 epoch.\n*   **Total HPO Budget:** 1000 HPO iterations (total number of epochs across all configurations).\n*   **Device:** CPU."
}{
    "Title": "Deep Ranking Ensembles for Hyperparameter Optimization",
    "Main Contributions": "The paper introduces Deep Ranking Ensembles (DRE), a novel neural network-based Bayesian Optimization (BO) surrogate optimized with Learning-to-Rank (L2R) losses for Hyperparameter Optimization (HPO). Unlike traditional methods that treat surrogate training as a regression task, DRE formulates it as an L2R problem, prioritizing the ranks of top-performing hyperparameter configurations and modeling uncertainty via deep ensembles. The method integrates a meta-learning technique for transferring knowledge from large-scale public meta-datasets, enhanced by dataset meta-features. DRE achieves new state-of-the-art results in HPO across a broad range of benchmarks.",
    "Methodology": "The core methodology involves training an ensemble of diverse neural networks (DRE) to act as a probabilistic surrogate for HPO, focusing on learning the ranks of hyperparameter configurations' performances. The neural networks are optimized using a weighted list-wise L2R loss, which assigns higher penalties to errors in ranking top-performing configurations. Uncertainty is estimated by training multiple diverse scorers within the ensemble using stochastic gradient descent with different per-scorer seeds. The DRE surrogate is meta-learned from evaluations on previous datasets, and its transfer quality is boosted by incorporating dataset meta-features extracted using a deep set formulation, which condition the scorer functions. During the BO loop, the ensemble's posterior mean and variance of estimated ranks are used by acquisition functions like Expected Improvement (EI) or Lower Confidence Bound (UCB) to guide the search for the next optimal hyperparameter configuration.",
    "Experimental Setup": "Experiments were conducted on HPO-B, the largest public HPO benchmark, comprising 16 search spaces and 86 meta-test datasets/tasks. The evaluation involved 5 initial configurations followed by 100 BO iterations, with 5 runs per task. DRE's performance was compared against 12 state-of-the-art HPO baselines, including non-transfer methods (e.g., Random Search, GP, HEBO) and transfer methods (e.g., TST, RGPE, FSBO). The DRE architecture consisted of a Deep Set-based meta-feature extractor (five hidden layers, 32 neurons per layer) and an ensemble of 10 MLPs (four hidden layers, 32 neurons each). DRE was meta-learned for 5000 epochs using Adam (LR 0.001, batch size 100) and fine-tuned for 1000 epochs per BO iteration. Ablation studies investigated different L2R losses (point-wise, pair-wise, list-wise), the impact of meta-features, and various acquisition functions (EI, UCB, Average Rank), as well as scorer network size and list parameters.",
    "Limitations": "The primary limitation noted is that DRE incurs a higher computational cost per BO step compared to some baselines like FSBO and HEBO (though deemed a 'very small overhead' relative to the overall cost of evaluating hyperparameter configurations). The paper also implicitly acknowledges the general challenges of HPO, such as a limited number of evaluated configurations and the negative-transfer phenomenon, which DRE aims to mitigate through its design choices (meta-learning, meta-features), but these are inherent difficulties of the problem space rather than specific weaknesses of DRE itself.",
    "Future Research Directions": "Future research directions include extending the DRE surrogate to improve HPO performance in various sub-problems, such as multi-fidelity HPO, multi-objective HPO, and neural architecture search (NAS).",
    "Experiment Code": "import torchimport torch.nn as nnfrom torch.nn import functional as Ffrom collections.abc import Iterableimport mathdef compute_conv2d_output_shape(h_in, w_in, kernel_size, padding, stride, dilation=1):    if isinstance(kernel_size, Iterable) is False:        kernel_size = (kernel_size, kernel_size)    if isinstance(padding, Iterable) is False:        padding = (padding, padding)    if isinstance(stride, Iterable) is False:        stride = (stride, stride)    if isinstance(dilation, Iterable) is False:        dilation = (dilation, dilation)    h_out = int(math.floor(        (h_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1    ))    w_out = int(math.floor(        (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1    ))    return (h_out, w_out)def compute_conv2dTranspose_output_shape(h_in, w_in, kernel_size, padding, stride,                                          output_padding=0, dilation=1):    if isinstance(kernel_size, Iterable) is False:        kernel_size = (kernel_size, kernel_size)    if isinstance(padding, Iterable) is False:        padding = (padding, padding)    if isinstance(stride, Iterable) is False:        stride = (stride, stride)    if isinstance(dilation, Iterable) is False:        dilation = (dilation, dilation)    if isinstance(output_padding, Iterable) is False:        output_padding = (output_padding, output_padding)    h_out = (h_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) \\            + output_padding[0] + 1    w_out = (w_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) \\            + output_padding[1] + 1    return (h_out, w_out)class MLP(nn.Module):    def __init__(self, input_dim, hidden_dims, output_dim, activation=\"leaky_relu\",                 act_in_last_layer=False, skip_connection=False, norm_method=None):        super(MLP, self).__init__()        self.input_dim = input_dim        self.output_dim = output_dim        self.hidden_dims = hidden_dims        self.layer_dims = [input_dim, *hidden_dims, output_dim]        self.layer_num = len(self.layer_dims) - 1        self.act_in_last_layer = act_in_last_layer        self.skip_connection = skip_connection        self.norm_method = norm_method        self.layers = []        for i in range(self.layer_num):            linear_layer = nn.Linear(self.layer_dims[i], self.layer_dims[i + 1])            if self.norm_method is not None and i != self.layer_num - 1:                if self.norm_method == 'spectral_norm':                    sn_l = nn.utils.spectral_norm(linear_layer)                    self.layers.append(sn_l)                elif self.norm_method == 'batch_norm':                    bn = nn.BatchNorm1d(self.layer_dims[i+1])                    self.layers.append(linear_layer)                    self.layers.append(bn)                else:                    raise ValueError(\"Invalid norm_method:\", self.norm_method)            else:                self.layers.append(linear_layer)            if i != self.layer_num - 1 or self.act_in_last_layer:                if activation == \"relu\":                    self.layers.append(torch.nn.ReLU())                elif activation == \"tanh\":                    self.layers.append(torch.nn.Tanh())                elif activation == \"sigmoid\":                    self.layers.append(torch.nn.Sigmoid())                elif activation == \"leaky_relu\":                    self.layers.append(torch.nn.LeakyReLU())                else:                    raise ValueError(\"Unsupported activation type: \", activation)        self.model = nn.Sequential(*self.layers)    def forward(self, x):        if self.skip_connection:            o = x + self.model(x)        else:            o = self.model(x)        return oclass CopyModule(nn.Module):    def __init__(self):        super(CopyModule, self).__init__()    def forward(self, x):        return ximport torchfrom gpytorch.kernels.kernel import Kernelfrom typing import Optional, Tuplefrom gpytorch.priors import Priorfrom gpytorch.constraints import Intervalimport torchdef postprocess_rbf(dist_mat):    return dist_mat.div_(-2).exp_()def default_postprocess_script(x):    return xclass ExpectedRBFKernel(Kernel):    has_lengthscale = True    def __init__(self, ard_num_dims: Optional[int] = None,                 batch_shape: Optional[torch.Size] = torch.Size([]),                 active_dims: Optional[Tuple[int, ...]] = None,                 lengthscale_prior: Optional[Prior] = None,                 lengthscale_constraint: Optional[Interval] = None,                 eps: Optional[float] = 1e-6,                 **kwargs):        super(ExpectedRBFKernel, self).__init__(            ard_num_dims, batch_shape, active_dims,            lengthscale_prior, lengthscale_constraint, eps        )    def forward(self, x1, x2, diag=False, **params):        B = x1.shape[:-2] if x1.ndim >= 3 else None        M = x1.shape[-2]        N = x2.shape[-2]        D = x1.shape[-1] // 2        dtype = x1.dtype        device = x1.device        x1_mean = x1[..., 0:D]        x1_var = x1[..., D:] ** 2.0        x2_mean = x2[..., 0:D]        x2_var = x2[..., D:] ** 2.0        Var1 = torch.diag_embed(x1_var)        Var2 = torch.diag_embed(x2_var)        if self.ard_num_dims is None:            ls_vec = self.lengthscale.repeat(1, D).view(-1)        else:            ls_vec = self.lengthscale.view(-1)        W = torch.diag(ls_vec ** 2.0)        AB = x1_mean.unsqueeze(-2) - x2_mean.unsqueeze(-3)        VAVB = Var1.unsqueeze(-3) + Var2.unsqueeze(-4)        Z = W.unsqueeze(0).unsqueeze(0) if B is None \\            else W.unsqueeze(0).unsqueeze(0).unsqueeze(            0) + VAVB        Z_inv = torch.inverse(Z)        ABZ_eq = 'mnpd,mndd->mnpd' if B is None else 'bmnpd,bmndd->bmnpd'        ABZ = torch.einsum(ABZ_eq, [AB.unsqueeze(-2), Z_inv])        nu_eq = 'mnpd,mndq->mnpq' if B is None else 'bmnpd,bmndq->bmnpq'        nu = torch.einsum(nu_eq, [ABZ, AB.unsqueeze(-1)]).squeeze(-1).squeeze(            -1)        x1_eq_x2 = torch.equal(x1, x2)        if diag:            if x1_eq_x2:                res = torch.zeros(*x1.shape[:-1], x1.shape[-2], dtype=x1.dtype, device=x1.device)                res = postprocess_rbf(res)                return res            else:                res = torch.norm(x1 - x2, p=2, dim=-1)                res = res.pow(2)                res = postprocess_rbf(res)                return res        else:            W_inv = W.inverse().unsqueeze(0).unsqueeze(0)            if B is not None:                W_inv = W_inv.unsqueeze(0)            de_coeff = (W_inv.matmul(VAVB)).det().sqrt()            if x1_eq_x2:                zero_diag_identity = (torch.ones((M, M), dtype=dtype).to(device) -                                       torch.eye(M, M, dtype=dtype).to(device))                one_diag = torch.eye(M, M, dtype=dtype).to(device)                if B is not None:                    zero_diag_identity = zero_diag_identity.unsqueeze(0)                    one_diag = one_diag.unsqueeze(0)                de = de_coeff * zero_diag_identity + one_diag            else:                de = de_coeff            covar = nu / de            return postprocess_rbf(covar)import gpytorch.kernels.kernel as Kernelimport gcfrom utils import commons as cmfrom functools import partialimport torchdef postprocess_linear(dist_mat):    sym_dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)    return sym_dist_matdef integral_KME_batch(X, Y, kernel: Kernel):    dist_mat = None    try:        B = X.shape[:-3]        M, C, D = X.shape[-3:]        N, H, D = Y.shape[-3:]        _original_shape = kernel.batch_shape        kernel.batch_shape = torch.Size([M, N])        dist_mat = kernel(            X.unsqueeze(-3),            Y.unsqueeze(-4)        ).evaluate()        kme = dist_mat.mean(dim=[-1, -2])        kernel.batch_shape = _original_shape    finally:        cm.free_memory(            [                dist_mat,            ],            debug=False        )    return kmedef estimate_KME_in_chunks(X, Y, estimator, chunk_size=10):    N = Y.shape[-3]    retry_i = 0    success = False    _practicable_chunk_size = chunk_size    kme_results = None    while not success:        kme_results = []        _Y = None        _kme = None        try:            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)            for bs in range(0, N, _practicable_chunk_size):                be = min(N, bs + _practicable_chunk_size)                _Y = Y[..., bs:be, :, :]                _kme = estimator(X, _Y)                kme_results.append(_kme)                gc.collect()                torch.cuda.empty_cache()            success = True        except RuntimeError as e:            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:                if _practicable_chunk_size > 1:                    print(f'Chunk size {_practicable_chunk_size} is too large, '                          f'reduce it by a half:', e)                    retry_i += 1                    if len(kme_results) > 0:                        cm.free_memory(kme_results)                        for _m in kme_results:                            del _m                        del kme_results                    if '_kme' in locals():                        cm.free_memory([_kme])                    if '_Y' in locals():                        cm.free_memory([_Y])                else:                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '                                     'try cpu.')            else:                raise e        finally:            gc.collect()            torch.cuda.empty_cache()    kme = torch.concat(kme_results, dim=-1)    return kmeclass KMEKernel(Kernel):    has_lengthscale = True    def __init__(self, base_kernel, **kwargs):        super(KMEKernel, self).__init__(**kwargs)        self.base_kernel = base_kernel        self.chunk_size = kwargs.get('chunk_size', 100)        self.estimator = kwargs.get('estimator', 'integral')        self.estimation_trials = kwargs.get('estimation_trials', 1)        if self.estimator == 'integral':            self.estimation_func = partial(                integral_KME_batch, kernel=self.base_kernel            )        else:            raise ValueError('Unsupported estimator name', self.estimator)    @property    def is_stationary(self) -> bool:        return self.base_kernel.is_stationary    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):        chunk_size = params.get('chunk_size', self.chunk_size)        avg_dist_mat = None        for _ in range(self.estimation_trials):            dist_mat = estimate_KME_in_chunks(x1, x2, self.estimation_func, chunk_size)            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat        avg_dist_mat = avg_dist_mat / self.estimation_trials        cov_mat = postprocess_linear(avg_dist_mat.div(self.lengthscale ** 2.0))        return avg_dist_mat, cov_mat    def forward(self, x1, x2, diag=False, **params):        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)        return cov_matimport gpytorch as gpytfrom gpytorch.kernels.kernel import Kernelimport torchfrom tqdm.auto import trangeimport gcfrom utils import commons as cmfrom functools import partialdef additive_RQ_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):    assert len(alphas) > 0    _k_list = []    for a in alphas:        _k = gpyt.kernels.RQKernel()        _k.alpha = a        _k.lengthscale = ls        _k.raw_lengthscale.require_grad = learnable_ls        _k_list.append(_k)    k = gpyt.kernels.AdditiveKernel(*_k_list)    return kdef combo_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):    assert len(alphas) > 0    _k_list = []    for a in alphas:        _k = gpyt.kernels.RQKernel()        _k.alpha = a        _k.lengthscale = ls        _k.raw_lengthscale.require_grad = learnable_ls        _k_list.append(_k)    _k_list.append(gpyt.kernels.LinearKernel())    k = gpyt.kernels.AdditiveKernel(*_k_list)    return kdef postprocess_mmd(dist_mat):    sym_dist_mat = (dist_mat + dist_mat.T) * 0.5    _dist_mat = sym_dist_mat.clamp_(min=0) ** 0.5    return torch.clamp(1.0 - _dist_mat, 0.0, 1.0)def postprocess_mmd_rbf(dist_mat):    _dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)    return _dist_mat.div_(-2).exp_()def nystrom_mmd(X, Y, kernel: Kernel, sub_samp_size: int = 100):    B, D = X.shape    x_sub_inds = torch.randperm(B)[:sub_samp_size]    X_sub = X[x_sub_inds, :]    H, D = Y.shape    y_sub_inds = torch.randperm(H)[:sub_samp_size]    Y_sub = Y[y_sub_inds, :]    k_m_x = kernel(X_sub, X_sub).evaluate()    k_m_x_inv = torch.linalg.pinv(k_m_x)    k_mn_x = kernel(X_sub, X).evaluate()    alpha_x = (k_m_x_inv @ k_mn_x @ torch.ones(X.shape[0], 1).type(X.dtype).to(X.device)) \\              / X.shape[0]    k_m_y = kernel(Y_sub, Y_sub).evaluate()    k_m_y_inv = torch.linalg.pinv(k_m_y)    k_mn_y = kernel(Y_sub, Y).evaluate()    alpha_y = (k_m_y_inv @ k_mn_y @ torch.ones(Y.shape[0], 1).type(Y.dtype).to(Y.device)) \\              / Y.shape[0]    part1 = alpha_x.T @ k_m_x @ alpha_x    part2 = alpha_y.T @ k_m_y @ alpha_y    part3 = alpha_x.T @ kernel(X_sub, Y_sub).evaluate() @ alpha_y * -2    mmd2 = part1 + part2 + part3    return mmd2def nystrom_mmd_batch(X, Y, kernel: Kernel, sub_samp_size: int = 100):    km_x_inv = None    kmn_x = None    ones_x = None    km_y_inv = None    kmn_y = None    ones_y = None    km_x, km_y = None, None    alpha_x, km_xy = None, None    alpha_y, X_sub, Y_sub = None, None, None    part1, part2, part3 = None, None, None    try:        B = X.shape[:-3]        M, C, D = X.shape[-3:]        N, H, D = Y.shape[-3:]        _original_shape = kernel.batch_shape        x_sub_inds = torch.randperm(C)[:sub_samp_size]        X_sub = X[..., x_sub_inds, :]        y_sub_inds = torch.randperm(H)[:sub_samp_size]        Y_sub = Y[..., y_sub_inds, :]        kernel.batch_shape = torch.Size([M])        km_x = kernel(X_sub, X_sub).evaluate()        km_x_inv = torch.linalg.pinv(km_x)        kmn_x = kernel(X_sub, X).evaluate()        ones_x = torch.ones(M, C, 1).type(X.dtype).to(X.device)        alpha_x = (km_x_inv @ kmn_x @ ones_x) / C        kernel.batch_shape = torch.Size([N])        km_y = kernel(Y_sub, Y_sub).evaluate()        km_y_inv = torch.linalg.pinv(km_y)        kmn_y = kernel(Y_sub, Y).evaluate()        ones_y = torch.ones(N, H, 1).type(Y.dtype).to(Y.device)        alpha_y = (km_y_inv @ kmn_y @ ones_y) / H        part1 = (alpha_x.transpose(-2, -1) @ km_x @ alpha_x).view(*B, M, 1)        part2 = (alpha_y.transpose(-2, -1) @ km_y @ alpha_y).view(*B, 1, N)        kernel.batch_shape = torch.Size([M, N])        km_xy = kernel(X_sub.unsqueeze(-3),                       Y_sub.unsqueeze(-4)).evaluate()        part3 = (alpha_x.unsqueeze(-3).transpose(-2, -1) @ km_xy @ alpha_y.unsqueeze(-4)).view(*B,                                                                                                M, N)        mmd2 = part1 + part2 - part3 * 2.0        kernel.batch_shape = _original_shape    finally:        cm.free_memory(            [                km_x_inv, kmn_x, ones_x,                km_y_inv, kmn_y, ones_y,                km_x, km_y,                alpha_x, km_xy, alpha_y, X_sub, Y_sub, part1, part2, part3,            ],            debug=False        )    return mmd2def empirical_mmd(X, Y, kernel: Kernel):    cm_xx = kernel(X, X).evaluate()    avg_xx_mmd = (cm_xx.sum() - torch.diagonal(cm_xx).sum()) / (X.shape[0] * (X.shape[0] - 1))    cm_yy = kernel(Y, Y).evaluate()    avg_yy_mmd = (cm_yy.sum() - torch.diagonal(cm_yy).sum()) / (Y.shape[0] * (Y.shape[0] - 1))    cm_xy = kernel(X, Y).evaluate()    avg_xy_mmd = cm_xy.sum() / (X.shape[0] * Y.shape[0])    mmd = avg_xx_mmd + avg_yy_mmd - 2.0 * avg_xy_mmd    return mmddef estimate_mmd_in_chunks(X, Y, estimator, chunk_size=10):    N = Y.shape[-3]    retry_i = 0    success = False    _practicable_chunk_size = chunk_size    while not success:        try:            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)            mmd_results = []            for bs in range(0, N, _practicable_chunk_size):                be = min(N, bs + _practicable_chunk_size)                _Y = Y[..., bs:be, :, :]                _mmd = estimator(X, _Y)                mmd_results.append(_mmd)                gc.collect()                torch.cuda.empty_cache()            success = True        except RuntimeError as e:            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:                if _practicable_chunk_size > 1:                    print(f'Chunk size {_practicable_chunk_size} is too large, '                          f'reduce it by a half:', e)                    retry_i += 1                    if len(mmd_results) > 0:                        cm.free_memory(mmd_results)                        for _m in mmd_results:                            del _m                        del mmd_results                    if '_mmd' in locals():                        cm.free_memory([_mmd])                    if '_Y' in locals():                        cm.free_memory([_Y])                else:                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '                                     'try cpu.')            else:                raise e        finally:            gc.collect()            torch.cuda.empty_cache()    mmd = torch.concat(mmd_results, dim=-1)    return mmddef empirical_mmd_batch(X, Y, kernel: gpyt.kernels.Kernel):    B = X.shape[:-3]    M, C, D = X.shape[-3:]    N, H, D = Y.shape[-3:]    _original_shape = kernel.batch_shape    cm_xx = None    cm_yy = None    cm_xy = None    try:        kernel.batch_shape = torch.Size([M])        cm_xx = kernel(X, X).evaluate()        avg_xx_mmd = (cm_xx.sum((-2, -1)) - torch.diagonal(cm_xx, dim1=-2, dim2=-1).sum(-1)) \\                     / (C * (C - 1))        kernel.batch_shape = torch.Size([N])        cm_yy = kernel(Y, Y).evaluate()        avg_yy_mmd = (cm_yy.sum((-2, -1)) - torch.diagonal(cm_yy, dim1=-2, dim2=-1).sum(-1)) \\                     / (H * (H - 1))        kernel.batch_shape = torch.Size([M, N])        cm_xy = kernel(X.unsqueeze(-3), Y.unsqueeze(-4)).evaluate()        avg_xy_mmd = cm_xy.sum((-2, -1)) / (C * H)        mmd2 = avg_xx_mmd.unsqueeze(-1) + avg_yy_mmd.unsqueeze(-2) - 2.0 * avg_xy_mmd    finally:        kernel.batch_shape = _original_shape        cm.free_memory(            [cm_xx, cm_yy, cm_xy, ],            debug=False        )    return mmd2class MMDKernel(Kernel):    has_lengthscale = True    def __init__(self, base_kernel, **kwargs):        super(MMDKernel, self).__init__(**kwargs)        self.base_kernel = base_kernel        self.chunk_size = kwargs.get('chunk_size', 100)        self.estimator = kwargs.get('estimator', 'nystrom')        self.sub_samp_size = kwargs.get('sub_samp_size', 100)        self.estimation_trials = kwargs.get('estimation_trials', 1)        if self.estimator == 'nystrom':            self.estimation_func = partial(                nystrom_mmd_batch, kernel=self.base_kernel, sub_samp_size=self.sub_samp_size            )        elif self.estimator == 'empirical':            self.estimation_func = partial(empirical_mmd_batch, kernel=self.base_kernel)        else:            raise ValueError('Unsupported estimator name', self.estimator)    @property    def is_stationary(self) -> bool:        return self.base_kernel.is_stationary    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):        chunk_size = params.get('chunk_size', self.chunk_size)        avg_dist_mat = None        for _ in range(self.estimation_trials):            dist_mat = estimate_mmd_in_chunks(x1, x2, self.estimation_func, chunk_size)            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat        avg_dist_mat = avg_dist_mat / self.estimation_trials        cov_mat = postprocess_mmd_rbf(avg_dist_mat.div(self.lengthscale ** 2.0))        return avg_dist_mat, cov_mat    def forward(self, x1, x2, diag=False, **params):        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)        return cov_matimport numpy as npimport torchimport botorch.models.transforms as transfrom torch.nn import ModuleDictfrom collections import OrderedDictfrom typing import Tuplefrom torch import Tensorfrom botorch.exceptions.errors import BotorchTensorDimensionErrorfrom typing import Any, Callable, Dict, List, Optional, Uniondef additional_std(X, std):    ret = torch.ones_like(X) * torch.tensor(std, dtype=X.dtype, device=X.device) \\        if isinstance(X, torch.Tensor) else np.ones_like(X) * std    return retdef additional_xc_samples(X, n_sample, n_var, sampling_func, sampling_cfg={}, **kwargs):    batch_shape = X.shape[:-1]    noises = sampling_func(        **{**sampling_cfg, 'x': X}, size=(*batch_shape, n_sample)    ).reshape(*batch_shape, n_sample, n_var)    if isinstance(X, torch.Tensor):        noises = torch.tensor(noises, dtype=X.dtype, device=X.device)    samples = X[..., None, :] + noises    return samplesdef add_noise(X, sampling_func, sampling_cfg={}, **kwargs):    batch_shape = X.shape[:-1]    event_dim = X.shape[-1]    noise = sampling_func(**sampling_cfg, size=(*batch_shape, 1) ).reshape(*batch_shape, event_dim)    if isinstance(X, torch.Tensor):        noise = torch.tensor(noise, dtype=X.dtype, device=X.device)    return X + noiseclass AdditionalFeatures(trans.input.AppendFeatures):    def transform(self, X):        expanded_features = self._f(X[..., self.indices], **self.fkwargs)        return X, expanded_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,                                                 device=X.device)class TransformFeature(trans.input.AppendFeatures):    def transform(self, X):        transformed_features = self._f(X[..., self.indices], **self.fkwargs)        return transformed_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,                                                 device=X.device)class SelectMultiInputs(trans.input.InputTransform, torch.nn.Module):    def __init__(            self,            sel_indices,            transform_on_train: bool = True,            transform_on_eval: bool = True,            transform_on_fantasize: bool = True,    ) -> None:        super().__init__()        self.transform_on_train = transform_on_train        self.transform_on_eval = transform_on_eval        self.transform_on_fantasize = transform_on_fantasize        self.register_buffer(\"sel_indices\", sel_indices)    def transform(self, X):        return tuple(X[i] for i in self.sel_indices)class MultiInputTransform(trans.input.InputTransform, ModuleDict):    r\"\"\"An input transform representing the chaining of individual transforms.\"\"\"    def __init__(self, **transforms) -> None:        r\"\"\"Chaining of input transforms.        Args:            transforms: The transforms to chain. Internally, the names of the                kwargs are used as the keys for accessing the individual                transforms on the module.        \"\"\"        super().__init__(OrderedDict(transforms))        self.transform_on_train = False        self.transform_on_eval = False        self.transform_on_fantasize = False        for tf in transforms.values():            self.is_one_to_many |= tf.is_one_to_many            self.transform_on_train |= tf.transform_on_train            self.transform_on_eval |= tf.transform_on_eval            self.transform_on_fantasize |= tf.transform_on_fantasize    def transform(self, X):        ret = tuple([tf.transform(X[ind]) for ind, tf in enumerate(self.values())])        return ret    def untransform(self, X):        ret = tuple([tf.untransform(X[ind]) for ind, tf in enumerate(self.values())])        return ret    def equals(self, other: trans.input.InputTransform) -> bool:        return super().equals(other=other) and all(            t1.equals(t2) for t1, t2 in zip(self.values(), other.values())        )    def preprocess_transform(self, X):        ret = tuple([tf.preprocess_transform(X[ind]) for ind, tf in enumerate(self.values())])        return retclass DummyTransform(trans.input.ReversibleInputTransform, torch.nn.Module):    def __init__(            self,            transform_on_train: bool = True,            transform_on_eval: bool = True,            transform_on_fantasize: bool = True,            reverse: bool = False,    ) -> None:        super().__init__()        self.transform_on_train = transform_on_train        self.transform_on_eval = transform_on_eval        self.transform_on_fantasize = transform_on_fantasize        self.reverse = reverse    def _transform(self, X: Tensor) -> Tensor:        return X    def _untransform(self, X: Tensor) -> Tensor:        return Xclass ScaleTransform(trans.input.Normalize):    def _transform(self, X: Tensor) -> Tensor:        return X / self.coefficient    def _untransform(self, X: Tensor) -> Tensor:        return X * self.coefficientimport model_utils.input_transform as tfxfrom sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScalerimport numpy as npimport torchimport torch.nn as nnfrom collections.abc import Iterablefrom typing import Callabledef safe_scaling(y, scaler=None, scaling_method=\"standardize\"):    try:        if scaler is None:            if scaling_method == \"standardize\":                scaler = StandardScaler()                y_scaled = scaler.fit_transform(y)            elif scaling_method == \"power_transform\":                if y.min() <= 0:                    scaler = PowerTransformer(method='johnson', standardize=True)                    y_scaled = scaler.fit_transform(y)                else:                    scaler = PowerTransformer(method='box-cox', standardize=True)                    y_scaled = scaler.fit_transform(y)                    if y_scaled.std() < 0.5:                        scaler = PowerTransformer(method='yeo-johnson', standardize=True)                        y_scaled = scaler.fit_transform(y)                    if y_scaled.std() < 0.5:                        raise RuntimeError('Power transformation failed')            elif scaling_method == \"min_max\":                scaler = MinMaxScaler()                y_scaled = scaler.fit_transform(y)            else:                raise ValueError(\"Unknown scaling method:\", scaling_method)        else:            y_scaled = scaler.transform(y)    except Exception as e:        print(f\"[Warn] scaling fails:\", e)        y_scaled = y.copy()        scaler = None    return y_scaled, scalerdef filter_nan(x, xe, y, keep_rule='any'):    assert x is None or np.isfinite(x).all()    assert xe is None or np.isfinite(xe).all()    assert torch.isfinite(y).any(), \"No valid data in the dataset\"    if keep_rule == 'any':        valid_id = torch.isfinite(y).any(dim=1)    else:        valid_id = torch.isfinite(y).all(dim=1)    x_filtered = x[valid_id] if x is not None else None    xe_filtered = xe[valid_id] if xe is not None else None    y_filtered = y[valid_id]    return x_filtered, xe_filtered, y_filtereddef get_gp_prediction(model, x, scaler, **kwargs):    pred = model.predict(x, **kwargs)    pred_lcb, pred_ucb = pred.confidence_region()    pred_mean = pred.mean    if scaler is not None:        mean = scaler.inverse_transform(pred_mean.detach().numpy().reshape(-1, 1)).flatten()        lcb = scaler.inverse_transform(pred_lcb.detach().numpy().reshape(-1, 1)).flatten()        ucb = scaler.inverse_transform(pred_ucb.detach().numpy().reshape(-1, 1)).flatten()    else:        mean = pred_mean.detach().numpy().flatten()        lcb = pred_lcb.detach().numpy().flatten()        ucb = pred_ucb.detach().numpy().flatten()    return pred, mean, lcb, ucbclass OneHotTransform(torch.nn.Module):    def __init__(self, num_uniqs):        super().__init__()        self.num_uniqs = num_uniqs    @property    def num_out(self) -> int:        return sum(self.num_uniqs)    def forward(self, xe):        return torch.cat(            [torch.nn.functional.one_hot(xe[:, i].long(), self.num_uniqs[i])             for i in range(xe.shape[1])], dim=1        ).float()class EmbTransform(nn.Module):    def __init__(self, num_uniqs, **conf):        super().__init__()        self.emb_sizes = conf.get('emb_sizes')        if self.emb_sizes is None:            self.emb_sizes = [min(50, 1 + v // 2) for v in num_uniqs]        self.emb = nn.ModuleList([])        for num_uniq, emb_size in zip(num_uniqs, self.emb_sizes):            self.emb.append(nn.Embedding(num_uniq, emb_size))    @property    def num_out(self) -> int:        return sum(self.emb_sizes)    def forward(self, xe):        return torch.cat(            [self.emb[i](xe[:, i]).view(xe.shape[0], -1) for i in range(len(self.emb))], dim=1)def get_model_prediction(model, Xc_te, support_decomposed_pred):    preds = []    if support_decomposed_pred:        py_m0, ps2_m0 = model.predict(            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=0        )    else:        py_m0, ps2_m0 = model.predict(torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0))    ucb_m0 = py_m0 + (torch.sqrt(ps2_m0) * 2.0)    lcb_m0 = py_m0 - (torch.sqrt(ps2_m0) * 2.0)    preds.append(        (py_m0.detach().numpy(),         ps2_m0.detach().numpy(),         lcb_m0.detach().numpy(),         ucb_m0.detach().numpy(),         )    )    if support_decomposed_pred:        py_m1, ps2_m1 = model.predict(            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=1        )        ucb_m1 = py_m1 + (torch.sqrt(ps2_m1) * 2.0)        lcb_m1 = py_m1 - (torch.sqrt(ps2_m1) * 2.0)        preds.append(            (py_m1.detach().numpy(),             ps2_m1.detach().numpy(),             lcb_m1.detach().numpy(),             ucb_m1.detach().numpy(),             )        )        py_m2, ps2_m2 = model.predict(            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=2        )        ucb_m2 = py_m2 + (torch.sqrt(ps2_m2) * 2.0)        lcb_m2 = py_m2 - (torch.sqrt(ps2_m2) * 2.0)        preds.append(            (py_m2.detach().numpy(),             ps2_m2.detach().numpy(),             lcb_m2.detach().numpy(),             ucb_m2.detach().numpy(),             )        )    return predsdef get_kernel_lengthscale(kern_model):    ls = None    km = kern_model    while ls is None and getattr(km, 'base_kernel', None) is not None:        km = km.base_kernel        ls = km.lengthscale    if ls is not None:        ls = ls.detach().cpu().numpy().flatten()    if isinstance(ls, Iterable) and len(ls) >= 1:        str_output = [f'{i:.3f}' for i in ls] if len(ls) > 1 else f'{ls[0]:.3f}'    else:        str_output = f'{ls}'    return ls, str_outputdef get_kernel_output_scale(kernel):    oscale = kernel.outputscale.item() if hasattr(kernel, 'outputscale') else None    oscale_str = f'{oscale:.3f}' if oscale is not None else f'{oscale}'    return oscale, oscale_strdef prepare_data(input_type: str,                 n_var: int, raw_input_mean: [float, np.array], raw_input_std: [float, np.array],                 xc_sample_size: int, input_sampling_func: Callable,                 xc_raw: np.array, y: np.array,                 dtype: torch.dtype, device: torch.device,                 **data_cfg):    if input_type == 'exact_input' or input_type == 'mean_input':        x_ts = torch.tensor(xc_raw, dtype=dtype, device=device)    elif input_type == 'sample_input':        tf_add_xsamp = tfx.AdditionalFeatures(            f=tfx.additional_xc_samples, transform_on_train=False,            fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,                     'sampling_func': input_sampling_func}        )        x_ts = tf_add_xsamp.transform(torch.tensor(xc_raw, dtype=dtype, device=device))    elif input_type == 'distribution_input':        tf_add_std = tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,                                            fkwargs={'std': raw_input_std})        x_ts = tf_add_std.transform(            torch.tensor(                xc_raw + raw_input_mean if raw_input_mean is not None else xc_raw,                dtype=dtype, device=device            )        )    else:        raise ValueError('Unknown input type:', input_type)    y_ts = torch.tensor(y.reshape(-1, 1), dtype=dtype, device=device)    return x_ts, y_tsimport gpytorch as gpytimport botorch as botfrom botorch.models.gpytorch import GPyTorchModelfrom botorch.models import transforms as tffrom typing import Any, Optional, Unionimport torchfrom botorch.acquisition.objective import PosteriorTransformfrom botorch.models.utils import gpt_posterior_settingsfrom botorch.posteriors.gpytorch import GPyTorchPosteriorfrom torch import Tensorimport warningsfrom botorch.posteriors.transformed import TransformedPosteriorclass RobustGP(gpyt.models.ExactGP, GPyTorchModel):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 **kwargs):        _in_tf = input_transform if input_transform is not None \\            else self.define_default_input_transform(**kwargs)        _out_tf = outcome_transform if outcome_transform is not None \\            else self.define_default_outcome_transform(**kwargs)        with torch.no_grad():            _in_tf.transform(train_inputs)        if _out_tf is not None:            train_targets, _ = _out_tf(train_targets)            train_targets = train_targets.squeeze(-1)        gpyt.models.ExactGP.__init__(self, train_inputs, train_targets, likelihood)        self.input_transform = _in_tf        self.outcome_transform = _out_tf        self.num_inputs = num_inputs        self.additional_transform = additional_transform if additional_transform is not None \\            else self.define_additional_transform(**kwargs)        self.mean_module = self.define_mean_module(**kwargs)        self.covar_module = self.define_covar_module(**kwargs)        self.kwargs = kwargs    def define_default_input_transform(self, **kwargs):        n_var = kwargs['n_var']        return tf.Normalize(d=n_var, transform_on_train=True)    def define_default_outcome_transform(self, m=1, **kwargs):        return tf.outcome.Standardize(m=m)    def define_mean_module(self, **kwargs):        return gpyt.means.ConstantMean()    def define_covar_module(self, **kwargs):        return gpyt.kernels.ScaleKernel(gpyt.kernels.MaternKernel())    def define_additional_transform(self, **kwargs):        return None    def transform_inputs_additional(self, X):        n_inputs = 1 if isinstance(X, torch.Tensor) else len(X)        if n_inputs != self.num_inputs:            if self.additional_transform is not None:                X = self.additional_transform(X)            else:                raise ValueError(f\"Expect {self.num_inputs} inputs but found {len(X)} \"                                 f\"and no additional transformer.\")        return X    def forward(self, X):        if self.training:            X = self.transform_inputs(X)        xc_raw = X        mean_x = self.mean_module(xc_raw)        covar_x = self.covar_module(xc_raw)        return gpyt.distributions.MultivariateNormal(mean_x, covar_x)    def posterior(            self, X,            observation_noise: Union[bool, Tensor] = False,            posterior_transform: Optional[PosteriorTransform] = None,            **kwargs: Any,    ) -> Union[GPyTorchPosterior, TransformedPosterior]:        self.eval()        X = self.transform_inputs_additional(X)        X = self.transform_inputs(X)        with gpt_posterior_settings():            mvn = self(*X) if self.num_inputs > 1 else self(X)            if observation_noise is not False:                if isinstance(observation_noise, torch.Tensor):                    self._validate_tensor_args(X=X, Y=observation_noise)                    if observation_noise.size(-1) == 1:                        observation_noise = observation_noise.squeeze(-1)                    mvn = self.likelihood(mvn, X, noise=observation_noise)                else:                    mvn = self.likelihood(mvn, X)        posterior = GPyTorchPosterior(distribution=mvn)        if hasattr(self, \"outcome_transform\"):            posterior = self.outcome_transform.untransform_posterior(posterior)        if posterior_transform is not None:            return posterior_transform(posterior)        return posterior    def _set_transformed_inputs(self) -> None:        if hasattr(self, \"input_transform\") and not self._has_transformed_inputs:            if hasattr(self, \"train_inputs\"):                self._original_train_inputs = self.train_inputs[0] if self.num_inputs == 1 \\                    else self.train_inputs                with torch.no_grad():                    X_tf = self.input_transform.preprocess_transform(                        self.train_inputs[0] if self.num_inputs == 1 else self.train_inputs                    )                self.set_train_data(X_tf, strict=False)                self._has_transformed_inputs = True            else:                warnings.warn(                    \"Could not update `train_inputs` with transformed inputs \"                    f\"since {self.__class__.__name__} does not have a `train_inputs` \"                    \"attribute. Make sure that the `input_transform` is applied to \"                    \"both the train inputs and test inputs.\",                    RuntimeWarning,                )class RobustGPModel():    def __init__(self, m_cls, num_inputs=1, **kwargs):        self.model = None        self.likelihood = None        self.mll = None        self.optimizer = None        self.m_cls = m_cls        self.num_inputs = num_inputs        self.noise_free = kwargs.get('noise_free', False)        self.dtype = kwargs.get('dtype', torch.float)        self.device = kwargs.get('device', torch.device('cpu'))        self.kwargs = kwargs    def define_optimizer(self, model: torch.nn.Module, **kwargs):        optimizer = None        if not kwargs.get('fit_with_scipy', False):            lr = kwargs.get('lr', 1e-2)            optimizer = torch.optim.Adam(                params=[{'params': model.parameters()}],                lr=lr            )        return optimizer    def define_likelihood(self, **kwargs):        noise_free = kwargs.get(\"noise_free\", False)        if noise_free:            lkh = gpyt.likelihoods.GaussianLikelihood()            lkh.noise = 1e-4            lkh.raw_noise.requires_grad = False        else:            noise_prior = kwargs.get(\"noise_prior\", None)            noise_constr = kwargs.get(\"noise_constr\", None)            lkh = gpyt.likelihoods.GaussianLikelihood(                noise_prior=noise_prior, noise_constraint=noise_constr            )        return lkh    def define_model(self, tr_x, tr_y, likelihood, **kwargs):        model = self.m_cls(tr_x, tr_y, likelihood, self.num_inputs, **kwargs)        return model    def define_mll(self, likelihood, model):        return gpyt.mlls.ExactMarginalLogLikelihood(likelihood, model)    def post_initialize(self, tr_x, tr_y):        self.likelihood = self.define_likelihood(**self.kwargs)        self.model = self.define_model(tr_x, tr_y, self.likelihood, **self.kwargs)        self.mll = self.define_mll(self.likelihood, self.model)        self.likelihood = self.likelihood.to(self.dtype).to(self.device)        self.model = self.model.to(self.dtype).to(self.device)    def fit(self, **kwargs):        assert (self.model is not None and self.mll is not None and self.likelihood is not None)        tr_hist = None        success = False        max_retries = kwargs.get('max_retries', 5)        n_retry = 0        while not success:            try:                with bot.settings.debug(True):                    tr_hist = self.do_fit(**kwargs)                success = True            except Exception as e:                if n_retry < max_retries:                    n_retry += 1                    print(f\"[Warn] Model fit fails, retry cnt={n_retry}.\", e)                    success = False                else:                    raise e        return tr_hist    def do_fit(self, **kwargs):        self.model.train()        fit_with_scipy = kwargs.get('fit_with_scipy', True)        tr_history = None        if fit_with_scipy:            bot.fit_gpytorch_mll(self.mll)        else:            tr_history = []            epoch_num = kwargs.get('epoch_num', 100)            verbose = kwargs.get('verbose', True)            print_every = kwargs.get('print_every', 10)            if self.optimizer is None:                self.optimizer = self.define_optimizer(self.model, **kwargs)            for ep_i in range(epoch_num):                def closure():                    self.optimizer.zero_grad()                    output = self.model(self.model.train_inputs[0]) if self.num_inputs == 1 \\                        else self.model(*self.model.train_inputs)                    loss = -self.mll(output, self.model.train_targets)                    loss.backward()                    return loss                loss = self.optimizer.step(closure)                xc_ls, xc_ls_str = get_kernel_lengthscale(self.model.covar_module)                xc_os, xc_os_str = get_kernel_output_scale(self.model.covar_module)                y_noise = self.model.likelihood.noise.item()                tr_history.append((ep_i, loss.item(), xc_ls, xc_os, y_noise))                if verbose and ((ep_i % print_every == 0) or (ep_i == epoch_num - 1)):                    print(f\"[epoch{ep_i}] loss={loss.item():.3f}, \"                          f\"xc_lscale={xc_ls_str}, \"                          f\"xc_oscale={xc_os_str}, \"                          f\"y_noise={y_noise:.3f}\")        return tr_history    def predict(self, X):        pred = self.model.posterior(X)        return pred.mean, pred.variance    def get_posterior(self, X):        return self.model.posterior(X)from models.mmd_gp import MMDGPfrom kernels.kme_kernel import KMEKernelimport gpytorch as gpytclass UGP(MMDGP):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 hidden_dims=(4, 2), latent_dim=1, **kwargs):        super(UGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,                                         input_transform, outcome_transform,                                         additional_transform, **kwargs)    def define_covar_module(self, **kwargs):        xc_kern_params = {}        xc_lscale_constr = kwargs.get('xc_ls_constr', None)        if xc_lscale_constr is not None:            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr        xc_kme_inner_k = kwargs.get('base_kernel', None)        if xc_kme_inner_k is None:            xc_kme_inner_k = gpyt.kernels.RBFKernel(**xc_kern_params)        estimator_name = kwargs.get('estimator_name', 'integral')        chunk_size = kwargs.get('chunk_size', 100)        sub_samp_size = kwargs.get('sub_samp_size', 100)        covar_module = gpyt.kernels.ScaleKernel(            KMEKernel(xc_kme_inner_k, estimator=estimator_name, chunk_size=chunk_size)        )        return covar_modulefrom model_utils import input_transform as tfxfrom gpytorch.kernels import GaussianSymmetrizedKLKernel, RBFKernelfrom kernels.expected_rbf_kernel import ExpectedRBFKernelfrom models.robust_gp import RobustGPimport gpytorch as gpytfrom botorch.models import transforms as tfimport torchKN_EXPECTED_RBF = 'ERBF'KN_SKL = \"SKL\"KN_RBF = 'rbf'class UncertainGP(RobustGP):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 **kwargs):        super(UncertainGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,                                          input_transform, outcome_transform,                                          additional_transform, **kwargs)        self.kernel_name = kwargs.get('kernel_name', KN_SKL)    def define_covar_module(self, **kwargs):        n_var = kwargs['n_var']        self.kernel_name = kwargs['kernel_name']        xc_lscale_constr = kwargs.get('xc_lscale_constr', None)        xc_kern_params = {'ard_num_dims': n_var}        if xc_lscale_constr is not None:            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr        if self.kernel_name == KN_EXPECTED_RBF:            xc_kern_params['ard_num_dims'] = None            xc_covar_module = gpyt.kernels.ScaleKernel(ExpectedRBFKernel(**xc_kern_params))        elif self.kernel_name == KN_SKL:            xc_kern_params['ard_num_dims'] = None            xc_covar_module = gpyt.kernels.ScaleKernel(                GaussianSymmetrizedKLKernel(**xc_kern_params)            )        elif self.kernel_name == KN_RBF:            xc_covar_module = gpyt.kernels.ScaleKernel(RBFKernel(**xc_kern_params))        else:            raise ValueError(\"Unsupported kernel type:\", self.kernel_name)        return xc_covar_module    def define_default_input_transform(self, **kwargs):        n_var = kwargs['n_var']        input_bounds = kwargs['input_bounds']        return tfx.MultiInputTransform(            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),            tf2=tfx.ScaleTransform(d=n_var, bounds=input_bounds, transform_on_train=True),            tf3=tfx.DummyTransform(transform_on_train=True),        )    def define_additional_transform(self, **kwargs):        raw_input_std = kwargs['raw_input_std']        return tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,                                      fkwargs={'std': raw_input_std})    def compute_mean_cov(self, X, **kwargs):        xc_raw, xc_std, xe = X        mean_x, covar = None, None        if self.kernel_name == KN_SKL:            xc_input = torch.concat((xc_raw, xc_std.pow(2).log()), dim=-1)        elif self.kernel_name == KN_EXPECTED_RBF:            xc_input = torch.concat((xc_raw, xc_std.pow(2)), dim=-1)        else:            xc_input = xc_raw        mean_x = self.mean_module(xc_raw)        covar_x = self.covar_module(xc_input, **kwargs)        return mean_x, covar_x    def forward(self, xc_raw, xc_std, xe):        X = (xc_raw, xc_std, xe)        if self.training:            X = self.transform_inputs(X)        mean_x, covar = self.compute_mean_cov(X)        return gpyt.distributions.MultivariateNormal(mean_x, covar)from model_utils import input_transform as tfxfrom model_utils.common_model_parts import MLP, CopyModulefrom kernels.mmd_kernel import MMDKernel, additive_RQ_kernelimport gpytorch as gpytfrom botorch.models import transforms as tfimport torchimport warningsfrom gpytorch import settingsfrom gpytorch.utils.warnings import GPInputWarningfrom gpytorch.models.exact_prediction_strategies import prediction_strategyfrom gpytorch.models import ExactGPfrom gpytorch.distributions import MultivariateNormalclass MMDGP(RobustGP):    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,                 input_transform=None, outcome_transform=None, additional_transform=None,                 hidden_dims=(4, 2), latent_dim=1, **kwargs):        super(MMDGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,                                     input_transform, outcome_transform,                                     additional_transform, **kwargs)        self.norm_method = kwargs.get('latent_norm_method', None)        self.skip_conn = kwargs.get('skip_conn', False)        if hidden_dims is not None:            self.latent_dim = latent_dim            self.latent_mapping_module = MLP(                train_inputs[0].shape[-1], hidden_dims, latent_dim,                norm_method=self.norm_method            )        else:            self.latent_dim = train_inputs[0].shape[1] + train_inputs[0].shape[1]            self.latent_mapping_module = CopyModule()    def define_default_input_transform(self, **kwargs):        n_var = kwargs['n_var']        input_bounds = kwargs['input_bounds']        return tfx.MultiInputTransform(            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),            tf2=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),            tf3=tfx.DummyTransform(transform_on_train=True),        )    def define_covar_module(self, **kwargs):        xc_kern_params = {}        xc_lscale_constr = kwargs.get('xc_ls_constr', None)        if xc_lscale_constr is not None:            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr        xc_mmd_inner_k = kwargs.get('base_kernel', None)        if xc_mmd_inner_k is None:            xc_mmd_inner_k = additive_RQ_kernel(                alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False            )        estimator_name = kwargs.get('estimator_name', 'nystrom')        chunk_size = kwargs.get('chunk_size', 100)        sub_samp_size = kwargs.get('sub_samp_size', 100)        covar_module = gpyt.kernels.ScaleKernel(            MMDKernel(xc_mmd_inner_k, estimator=estimator_name, sub_samp_size=sub_samp_size,                      chunk_size=chunk_size)        )        return covar_module    def forward(self, xc_raw, xc_samp, xe):        X = (xc_raw, xc_samp, xe)        if self.training:            X = self.transform_inputs(X)        mean_x, covar = self.compute_mean_cov(X)        return gpyt.distributions.MultivariateNormal(mean_x, covar)    def compute_mean_cov(self, x, **kwargs):        Xc_raw, Xc_samples, Xe = x        Xe_trans = Xe        mean_x, covar = None, None        if Xc_raw.shape[-1] > 0 and Xc_samples.shape[-1] > 0:            _s = Xc_samples.shape[:-1]            D = Xc_samples.shape[-1]            proj_X_samples = self.latent_mapping_module(Xc_samples.view(-1, D)).view(*_s, -1)            with gpyt.settings.debug(True) and gpyt.settings.lazily_evaluate_kernels(False):                k_c = self.covar_module(proj_X_samples, **kwargs)            covar = k_c if covar is None else (k_c * covar)            Xc_samples_mean = Xc_samples.mean(dim=-2)            proj_X_raw = self.latent_mapping_module(Xc_samples_mean)            mean_x = self.mean_module(proj_X_raw)        if Xe.shape[-1] > 0:            Xe_trans = self.xe_transformer(Xe)            k_e = self.xe_covar_module(Xe_trans, **kwargs)            covar = k_e if covar is None else (k_e * covar)        return mean_x, covar    def define_additional_transform(self, **kwargs):        xc_sample_size = kwargs.get('xc_sample_size', 1000)        input_sampling_func = kwargs['input_sampling_func']        n_var = kwargs['n_var']        return tfx.AdditionalFeatures(f=tfx.additional_xc_samples, transform_on_train=False,                                      fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,                                               'sampling_func': input_sampling_func})    def __call__(self, *args, **kwargs):        train_inputs = list(self.train_inputs) if self.train_inputs is not None else []        inputs = [i.unsqueeze(-1) if i.ndimension() == 1 else i for i in args]        if self.training:            if self.train_inputs is None:                raise RuntimeError(                    \"train_inputs, train_targets cannot be None in training mode. \"                    \"Call .eval() for prior predictions, or call .set_train_data() to add training data.\"                )            if settings.debug.on():                if not all(torch.equal(train_input, input) for train_input, input in                                   zip(train_inputs, inputs)):                    raise RuntimeError(\"You must train on the training inputs!\")            res = super().__call__(*inputs, **kwargs)            return res        elif settings.prior_mode.on() or self.train_inputs is None or self.train_targets is None:            full_inputs = args            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)            if settings.debug().on():                if not isinstance(full_output, MultivariateNormal):                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")            return full_output        else:            if settings.debug.on():                if all(torch.equal(train_input, input) for train_input, input in                                   zip(train_inputs, inputs)):                    warnings.warn(                        \"The input matches the stored training data. Did you forget to call model.train()?\",                        GPInputWarning,                    )            if self.prediction_strategy is None:                train_output = super(ExactGP, self).__call__(*train_inputs, **kwargs)                self.prediction_strategy = prediction_strategy(                    train_inputs=train_inputs,                    train_prior_dist=train_output,                    train_labels=self.train_targets,                    likelihood=self.likelihood,                )            full_inputs = []            batch_shape = train_inputs[0].shape[:-2]            for i, (train_input, input) in enumerate(zip(train_inputs, inputs)):                dim_2_concat = -3 if i == 1 else -2                batch_reserved_dim = -3 if i == 1 else -2                batch_shape = train_inputs[i].shape[:batch_reserved_dim]                if batch_shape != train_input.shape[:batch_reserved_dim]:                    batch_shape = torch.broadcast_shapes(batch_shape,                                                         train_input.shape[:batch_reserved_dim])                    train_input = train_input.expand(*batch_shape,                                                     *train_input.shape[batch_reserved_dim:])                if batch_shape != input.shape[:batch_reserved_dim]:                    batch_shape = torch.broadcast_shapes(batch_shape,                                                         input.shape[:batch_reserved_dim])                    train_input = train_input.expand(*batch_shape,                                                     *train_input.shape[batch_reserved_dim:])                    input = input.expand(*batch_shape, *input.shape[batch_reserved_dim:])                full_inputs.append(torch.cat([train_input, input], dim=dim_2_concat))            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)            if settings.debug().on():                if not isinstance(full_output, MultivariateNormal):                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")            full_mean, full_covar = full_output.loc, full_output.lazy_covariance_matrix            batch_shape = full_output.batch_shape            joint_shape = full_output.event_shape            tasks_shape = joint_shape[1:]            test_shape = torch.Size(                [joint_shape[0] - self.prediction_strategy.train_shape[0], *tasks_shape])            with settings.cg_tolerance(settings.eval_cg_tolerance.value()):                predictive_mean, predictive_covar = self.prediction_strategy.exact_prediction(                    full_mean, full_covar)            predictive_mean = predictive_mean.view(*batch_shape, *test_shape).contiguous()            return full_output.__class__(predictive_mean, predictive_covar)",
    "Experiment Result": "The experiments compare robust optimization performance across various surrogate models and input uncertainty distributions. The setup involves:1.  **General Configuration**    *   `n_trial`: 10 (or 2 in debug mode)    *   `n_iter`: 100 (or 5 in debug mode)    *   `xc_sample_size`: 160 (sample size for each uncertain input sample)    *   `sub_samp_size`: 10 (sub-sampling size for Nystrom estimator)    *   `init_samp_num`: 10 (number of initial samples)2.  **Problem Setup**    *   `raw_fname`: 'RKHS-S', 'CustomK', 'BumpedBowlHD' (benchmark function names)    *   `n_var`: 1 to 3 (variable dimension, depends on function)    *   `minimization`: True (problem is minimization)    *   `num_expectation_eval`: 500 (number of evaluations for true expectation)3.  **Input Uncertainty Distributions**    *   **`input_uncertainty_type`**:        *   `gmm`: Gaussian Mixture Model (`GMMInputDistribution`) with 2 components, specific means, covariances, and weights.        *   `norm`: Gaussian (`ScipyInputDistribution`) for `n_var=1` or Multivariate Normal for `n_var > 1` with a specified `raw_input_std`.        *   `beta`: Beta distribution (`ScipyInputDistribution`) with parameters `a=0.4`, `b=0.2`, and `scale=raw_input_std`.        *   `chi2`: Chi-squared distribution (`ScipyInputDistribution`) with `df=3` and `scale=raw_input_std`.        *   `step_chi2`: Step-wise Chi-squared (`StepInputDistribution`) with two `chi2` distributions, switching based on input `x`.        *   `varying_beta`: Varying Beta distribution (`VaryingInputDistribution`) where parameters depend on `x`.        *   `uniform`: Uniform distribution (`ScipyInputDistribution`) with specified `loc` and `scale=raw_input_std`.        *   `concated_circular`: Concatenated distribution (`ConcatDistribution`) combining `Circular2Distribution` and `ScipyInputDistribution` (multivariate normal).    *   `raw_input_std`: 0.01 (standard deviation of input uncertainty)4.  **Model Configuration**    *   `default_dtype`: `torch.double` (if `use_double_precision`) or `torch.float32`.    *   `default_device`: `cuda:0` (if `use_gpu`) or `cpu`.    *   `x_bounds`: Determined by the problem's input bounds.    *   **`model_candidates` (surrogate models evaluated)**:        *   **`MMDGP-nystrom`**: `MMDGP` with `input_type='sample_input'`, `estimator_name='nystrom'`, `xc_sample_size=160`, `sub_samp_size=10`. Utilizes `MLP` as `latent_mapping_module`.        *   **`uGP`**: `UGP` (inherits from `MMDGP`) with `input_type='sample_input'`, `estimator_name='integral'`, `xc_sample_size` set to `int((xc_sample_size * sub_samp_size) ** 0.5)`.        *   **`GP`**: `RobustGP` (standard GP) with `input_type='exact_input'`.        *   **`skl`**: `UncertainGP` with `kernel_name='SKL'` (Gaussian Symmetrized KL Kernel) and `input_type='distribution_input'`.        *   **`ERBF`**: `UncertainGP` with `kernel_name='ERBF'` (Expected RBF Kernel) and `input_type='distribution_input'`.        *   _(Optional, sometimes included)_ `MMDGP-raw-S`, `MMDGP-raw-L`: `MMDGP` with `estimator_name='empirical'` and different `xc_sample_size`.    *   `default_fit_cfg`: `epoch_num` (150 or 1 in debug), `lr=5e-2`, `fit_with_scipy=False`, `print_every=10`.5.  **Optimization Settings**    *   `batch_size`: 1 (for acquisition function optimization)    *   `n_restarts`: 10 (for acquisition function optimization)    *   `raw_samples`: 512 (for acquisition function optimization)    *   **Acquisition Functions**:        *   `ExpectedImprovement` (EI)        *   `UpperConfidenceBound` (UCB)    *   **Optimum Finding Methods**:        *   `OPTIMUM_BOV`: Best Observed Value        *   `OPTIMUM_BE`: Best Expectation6.  **Infrastructure**    *   `use_gpu`: Optional GPU usage.    *   `use_double_precision`: Optional double precision.    *   `use_multiprocess`: Optional parallel execution for trials."
}{
    "Title": "PASHA: Efficient HPO and NAS with Progressive Resource Allocation",
    "Main Contributions": "The paper addresses the high computational cost of Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS), especially with large datasets, even when using multi-fidelity methods. It proposes PASHA (Progressive ASHA), an extension of ASHA that dynamically allocates maximum resources for the tuning procedure based on the need. PASHA significantly reduces computational resources and tuning time compared to ASHA while maintaining similar predictive performance. Key contributions include: 1) Introducing PASHA for dynamic resource allocation in HPO/NAS, 2) Demonstrating significant speedups without sacrificing performance through empirical evaluation, and 3) Showing PASHA's compatibility with sample-efficient strategies like Bayesian Optimization.",
    "Methodology": "PASHA extends ASHA and is inspired by the 'doubling trick' concept. It starts with a small initial amount of resources and progressively increases them only if the ranking of configurations in the top two rungs (rounds of promotion) has not stabilized. This allows for early stopping when the relative performance of promising configurations becomes consistent. To handle noise in the training process, PASHA employs a 'soft ranking' approach where configurations are considered equivalent if their performance difference is below a threshold \\u03b5. This \\u03b5 value is automatically estimated by identifying pairs of configurations that repeatedly swap ranks across different resource levels (epochs/iterations), calculating it as the N-th percentile (default 90th) of the performance differences among these 'criss-crossing' configurations. The algorithm maintains a maximum resource 'safety net' to prevent indefinite resource increases.",
    "Experimental Setup": "The method was empirically evaluated on two main sets of experiments. For Neural Architecture Search (NAS), it used the NASBench201 benchmark across three datasets: CIFAR-10, CIFAR-100, and ImageNet16-120. For Hyperparameter Optimization (HPO), it used two large-scale tasks from the PD1 benchmark: WMT15 German-English (with xformer) and ImageNet (with ResNet50). All experiments involved two phases: 1) running the optimizer until 256 candidate configurations were evaluated, and 2) retraining the best identified configuration from scratch (using only the training set for experiments). Performance was measured by runtime (HPO phase only) and accuracy after retraining. Default parameters included a minimum resource 'r' of 1 epoch, a maximum resource 'R' (for ASHA or as a safety net for PASHA) of 200 epochs for NASBench201 and 1414/251 epochs for PD1 tasks, and a reduction factor \\u03b7=3. Experiments were averaged over multiple random seeds (e.g., 5 scheduler seeds, 3 NASBench201 seeds, totaling 15 repetitions). Baselines included ASHA, 'one-epoch', 'two-epoch', 'three-epoch', 'five-epoch' and 'random' baselines, and a combination with Bayesian Optimization (MOBSTER). Various alternative ranking functions were also tested, including direct ranking, soft ranking with fixed or heuristic-estimated \\u03b5, Rank Biased Overlap (RBO), and Reciprocal Rank Regret (RRR).",
    "Limitations": "PASHA's benefits in speedup are limited when the number of available rungs (resource levels) is small. This is observed in benchmarks like LCBench, which has a maximum of 50 epochs and a minimum of 1 epoch, offering few opportunities for early stopping. Public benchmarks often fix resource granularities (e.g., in epochs), which can constrain PASHA's ability to demonstrate large speedups. The authors recommend having a maximum amount of resources at least 100 times larger than the minimum amount (for \\u03b7=3) to leverage PASHA effectively. If the number of epochs is small, achieving this might require defining resources with higher granularity, such as in terms of gradient updates or iterations, rather than epochs.",
    "Future Research Directions": "Future work could involve investigating the optimal definition of rungs and resource levels, not just for PASHA but for multi-fidelity algorithms in general, to understand their impact on algorithmic decisions. Another promising direction is to test combinations of PASHA with transfer-learning techniques specifically designed for multi-fidelity problems, such as RUSH (Zappella et al., 2021), with the goal of further decreasing the overall tuning time.",
    "Experiment Code": "class PASHA(HyperbandScheduler):\n    def __init__(self, config_space: Dict, metric: str, resource_attr: str, **kwargs):\n        _assert_max_resource_args(kwargs)\n        super(PASHA, self).__init__(\n            config_space=config_space,\n            metric=metric,\n            searcher=\"random\",\n            resource_attr=resource_attr,\n            type=\"pasha\",\n            **kwargs,\n        )\n\nimport numpy as np\nfrom syne_tune.optimizer.schedulers.hyperband_promotion import PromotionRungSystem\nimport itertools\n\nclass PASHARungSystem(PromotionRungSystem):\n    def __init__(\n        self,\n        rung_levels,\n        promote_quantiles,\n        metric,\n        mode,\n        resource_attr,\n        max_t,\n        ranking_criterion,\n        epsilon,\n        epsilon_scaling,\n    ):\n        super().__init__(\n            rung_levels, promote_quantiles, metric, mode, resource_attr, max_t\n        )\n        self.ranking_criterion = ranking_criterion\n        self.current_rung_idx = 2\n        self.rung_levels = rung_levels\n        self.current_max_t = rung_levels[self.current_rung_idx - 1]\n        self.epsilon = epsilon\n        self.epsilon_scaling = epsilon_scaling\n        if ranking_criterion == 'soft_ranking_auto':\n            self.per_epoch_results = {}\n            self.epoch_to_trials = {}\n            self.current_max_epoch = -1\n\n    def _effective_max_t(self):\n        return self.current_max_t\n\n    def _get_top_rungs_rankings(self, num_rungs=2):\n        rankings = []\n        rungs = [self._rungs[-self.current_rung_idx + e] for e in range(num_rungs)]\n        for rung in rungs:\n            if rung.data != {}:\n                trial_ids = rung.data.keys()\n                values = []\n                for trial_id in trial_ids:\n                    values.append(rung.data[trial_id][0])\n                values_order = np.array(values).argsort()\n                values_ranking = values_order.argsort()\n                ranking = list(zip(trial_ids, values_ranking, values))\n\n                rankings.append(ranking)\n\n        return rankings\n\n    def _get_sorted_top_rungs(self, rankings):\n        top_rung_keys = set([e[0] for e in rankings[0]])\n        corresponding_previous_rung_trials = filter(\n            lambda e: e[0] in top_rung_keys, rankings[1]\n        )\n        if self._mode == \"max\":\n            reverse = True\n        else:\n            reverse = False\n\n        sorted_top_rung = sorted(rankings[0], key=lambda e: e[1], reverse=reverse)\n        sorted_previous_rung = sorted(\n            corresponding_previous_rung_trials, key=lambda e: e[1], reverse=reverse\n        )\n        return sorted_top_rung, sorted_previous_rung\n\n    def _evaluate_soft_ranking(self, sorted_top_rung, sorted_previous_rung) -> bool:\n        keep_current_budget = True\n        if len(sorted_previous_rung) < 2:\n            epsilon = 0.0\n        elif self.ranking_criterion == \"soft_ranking_std\":\n            epsilon = (\n                np.std([e[2] for e in sorted_previous_rung]) * self.epsilon_scaling\n            )\n        elif (\n            self.ranking_criterion == \"soft_ranking_median_dst\"\n            or self.ranking_criterion == \"soft_ranking_mean_dst\"\n        ):\n            scores = [e[2] for e in sorted_previous_rung]\n            distances = [\n                abs(e1 - e2)\n                for idx1, e1 in enumerate(scores)\n                for idx2, e2 in enumerate(scores)\n                if idx1 != idx2\n            ]\n            if self.ranking_criterion == \"soft_ranking_mean_dst\":\n                epsilon = np.mean(distances) * self.epsilon_scaling\n            elif self.ranking_criterion == \"soft_ranking_median_dst\":\n                epsilon = np.median(distances) * self.epsilon_scaling\n            else:\n                raise ValueError(\n                    \"Ranking criterion {} is not supported\".format(\n                        self.ranking_criterion\n                    )\n                )\n        else:\n            epsilon = self.epsilon\n\n        previous_rung_groups = []\n        for idx, item in enumerate(sorted_previous_rung):\n            current_rung_group = [item[0]]\n            for idx_after in range(idx + 1, len(sorted_previous_rung)):\n                new_item = sorted_previous_rung[idx_after]\n\n                if self._mode == \"max\":\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                else:\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            for idx_before in range(idx - 1, -1, -1):\n                new_item = sorted_previous_rung[idx_before]\n                if self._mode == \"max\":\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                else:\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            previous_rung_groups.append(set(current_rung_group))\n\n        for idx, item in enumerate(sorted_top_rung):\n            if item[0] not in previous_rung_groups[idx]:\n                keep_current_budget = False\n                break\n\n        return keep_current_budget\n\n    def _update_epsilon(self):\n        seen_pairs = set()\n        noisy_cfg_distances = []\n        top_epoch = min(self.current_max_epoch, self._rungs[-self.current_rung_idx].level)\n        bottom_epoch = min(self._rungs[-self.current_rung_idx+1].level, self.current_max_epoch)\n        for epoch in range(top_epoch, bottom_epoch, -1):\n            if len(self.epoch_to_trials[epoch]) > 1:\n                for pair in itertools.combinations(self.epoch_to_trials[epoch], 2):\n                    c1, c2 = pair[0], pair[1]\n                    if (c1, c2) not in seen_pairs:\n                        seen_pairs.add((c1, c2))\n                        p1, p2 = self.per_epoch_results[c1][epoch], self.per_epoch_results[c2][epoch]\n                        cond = p1 > p2\n\n                        opposite_order = False\n                        same_order_after_opposite = False\n                        for prev_epoch in range(epoch - 1, 0, -1):\n                            pp1, pp2 = self.per_epoch_results[c1][prev_epoch], self.per_epoch_results[c2][prev_epoch]\n                            p_cond = pp1 > pp2\n                            if p_cond == (not cond):\n                                opposite_order = True\n                            if opposite_order and p_cond == cond:\n                                same_order_after_opposite = True\n                                break\n\n                        if opposite_order and same_order_after_opposite:\n                            noisy_cfg_distances.append(abs(p1 - p2))\n\n        if len(noisy_cfg_distances) > 0:\n            self.epsilon = np.percentile(noisy_cfg_distances, 90)\n            if str(self.epsilon) == 'nan':\n                raise ValueError('Epsilon became nan') \n\n    def _update_per_epoch_results(self, trial_id, result):\n        if trial_id not in self.per_epoch_results:\n            self.per_epoch_results[trial_id] = {}\n        self.per_epoch_results[trial_id][result[self._resource_attr]] = result[self._metric]\n\n        if result[self._resource_attr] not in self.epoch_to_trials:\n            self.epoch_to_trials[result[self._resource_attr]] = set() \n        self.epoch_to_trials[result[self._resource_attr]].add(trial_id)\n\n        if result[self._resource_attr] > self.current_max_epoch:\n            self.current_max_epoch = result[self._resource_attr]\n\n    def _decide_resource_increase(self, rankings) -> bool:\n        if len(rankings) == 2:\n            sorted_top_rung, sorted_previous_rung = self._get_sorted_top_rungs(rankings)\n        else:\n            return False\n\n        keep_current_budget = self._evaluate_soft_ranking(\n            sorted_top_rung, sorted_previous_rung\n        )\n\n        return not keep_current_budget\n\n    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:\n        ret_dict = super().on_task_report(trial_id, result, skip_rungs)\n\n        if self.ranking_criterion == \"soft_ranking_auto\":\n            self._update_per_epoch_results(trial_id, result)\n            self._update_epsilon()\n\n        rankings = self._get_top_rungs_rankings(num_rungs=2)\n        increase_resources = self._decide_resource_increase(rankings)\n\n        if increase_resources:\n            if self.current_rung_idx < len(self._rungs):\n                self.current_rung_idx += 1\n                self.current_max_t = self.rung_levels[self.current_rung_idx - 1]\n            else:\n                self.current_max_t = self.max_t\n\n        return ret_dict\n\n# From benchmarking/cli/scheduler_factory.py\n        if scheduler == \"hyperband_pasha\":\n            rung_system_kwargs = scheduler_options.get(\"rung_system_kwargs\", dict())\n            for name, tp in (\n                (\"ranking_criterion\", str),\n                (\"epsilon\", float),\n                (\"epsilon_scaling\", float),\n            ):\n                name_cl = \"pasha_\" + name\n                v = params.get(name_cl)\n                if v is not None:\n                    rung_system_kwargs[name] = tp(v)\n            if rung_system_kwargs:\n                scheduler_options[\"rung_system_kwargs\"] = rung_system_kwargs\n\n# From notebooks/run_bo_experiments.py\n    if hpo_approach == 'pasha':\n        scheduler = baselines_dict['PASHA'](\n            config_space,\n            max_t=max_t,\n            grace_period=default_params['grace_period'],\n            reduction_factor=reduction_factor,\n            resource_attr=resource_attr,\n            mode=mode,\n            metric=metric,\n            random_seed=random_seed,\n            rung_system_kwargs=rung_system_kwargs)\n    elif hpo_approach == 'pasha-bo':\n        scheduler = HyperbandScheduler(\n            config_space,\n            max_t=max_t,\n            grace_period=default_params['grace_period'],\n            reduction_factor=reduction_factor,\n            resource_attr=resource_attr,\n            mode=mode,\n            searcher='bayesopt',\n            type='pasha',\n            metric=metric,\n            random_seed=random_seed,\n            rung_system_kwargs=rung_system_kwargs)",
    "Experiment Result": "PASHA (Progressive ASynchronous HAlving) extends ASHA by dynamically increasing resource allocation only if the ranking of configurations within the top two rungs has not stabilized. It incorporates a 'soft ranking' approach, considering configurations equivalent if their performance difference is below a threshold \\u03b5. This \\u03b5 value can either be manually specified (`--pasha_epsilon`) or automatically estimated. Automatic estimation calculates \\u03b5 as the N-th percentile (default 90th) of performance differences among 'criss-crossing' configurations (those that repeatedly swap ranks across resource levels). The estimation method can be specified via `--pasha_ranking_criterion` (options include 'soft_ranking_std', 'soft_ranking_median_dst', 'soft_ranking_mean_dst', 'soft_ranking_auto'), and an optional `--pasha_epsilon_scaling` factor can be applied to the automatically estimated \\u03b5.\n\nKey configurable parameters include:\n- `pasha_ranking_criterion`: (string) Specifies the strategy for deciding ranking stability and resource increase. Options: 'soft_ranking', 'soft_ranking_std', 'soft_ranking_median_dst', 'soft_ranking_mean_dst', 'soft_ranking_auto'.\n- `pasha_epsilon`: (float) Threshold for soft ranking when `ranking_criterion` is 'soft_ranking'.\n- `pasha_epsilon_scaling`: (float) Scaling factor for automatically estimated epsilon.\n\nExample setting observed: `rung_system_kwargs = {'ranking_criterion': 'soft_ranking_auto', 'epsilon': 0.0}`, where `epsilon=0.0` suggests a placeholder when `soft_ranking_auto` is used to determine epsilon dynamically."
}{
    "Title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks",
    "Main Contributions": "The paper introduces Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) to address the limitations of existing multi-fidelity BO methods that either ignore or oversimplify the strong, complex correlations across different fidelities. DNN-MFBO employs deep neural networks to flexibly capture all kinds of complicated (potentially nonlinear and nonstationary) relationships between fidelities, thereby improving the estimation of the objective function and overall optimization performance. A tractable and efficient mutual information based acquisition function is computed using sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching. The method demonstrates superior optimization effectiveness and lower query costs compared to state-of-the-art multi-fidelity and single-fidelity BO algorithms on both synthetic benchmarks and real-world engineering design applications.",
    "Methodology": "DNN-MFBO constructs a multi-fidelity model by stacking a set of deep neural networks, where each NN models one fidelity. For fidelities m > 1, the input to the NN is the concatenation of the original input and the output from the previous fidelity, allowing information propagation and capturing complex inter-fidelity relationships. The model assumes output layer weights (wm) as random variables with Gaussian priors and other NN weights (θm) as hyper-parameters. A stochastic variational learning algorithm is developed to jointly estimate the posterior distribution of the random weights and the hyper-parameters by maximizing an analytically intractable Evidence Lower Bound (ELBO) using the reparameterization trick. For optimization, a Max-Value Entropy Search (MES) acquisition function is used, which is defined as the mutual information between the objective function's maximum and the function output at a given fidelity, normalized by query cost. The output posterior distributions for each fidelity are approximated as Gaussian via fidelity-wise moment matching and Gauss-Hermite quadrature. The conditional entropy term in the acquisition function is approximated using Monte-Carlo sampling for function maxima (f*) and further approximated by considering the entropy given fM(x) <= f*, where fM(x) is the highest fidelity output. This conditional entropy is also computed using Gauss-Hermite quadrature and moment matching to achieve a Gaussian approximation. The acquisition function is optimized using L-BFGS with random initialization.",
    "Experimental Setup": "DNN-MFBO was evaluated against several popular and state-of-the-art multi-fidelity BO algorithms, including MF-SKO, MF-GP-UCB, MF-PES, MF-MES, and MTNN-BO, as well as single-fidelity SF-MES. Experiments were conducted on three synthetic benchmark functions: Branin (3 fidelities, 2D), Park1 (2 fidelities, 4D), and Levy (3 fidelities, 2D), and two real-world engineering design problems: Mechanical Plate Vibration Design (3 material properties, 2 fidelities) and Thermal Conductor Design (3 shape parameters, 2 fidelities). DNN-MFBO and MTNN-BO were implemented in TensorFlow, while other methods used their original implementations. Neural network architectures (depths from 2 to 12, widths from 32 to 512) and learning rates (10^-5 to 10^-1) were tuned using SMAC3 and manual adjustments. ADAM optimizer was used for 5,000 epochs of stochastic training. Initial training points were randomly queried (e.g., 20, 20, 2 for Branin/Levy; 5, 2 for Park1; 20, 5 for real-world problems). Query costs were set as (1, 10, 100) for three-fidelity tasks and (1, 10) for two-fidelity tasks. Performance was measured by simple regret (SR), inference regret (IR) for synthetic tasks, and best queried function values for real-world tasks, along with average query time. All query time comparisons were performed on a Linux workstation with a 16-core Intel(R) Xeon(R) CPU E5-2670 and 16GB RAM. Experiments were repeated five times, and average results with standard error bars were reported.",
    "Limitations": "The calculation of the acquisition function, while analytically tractable due to its form, is noted to be quite complex, involving multiple layers of approximation through Gauss-Hermite quadrature, moment matching, and Monte-Carlo sampling. Specifically, the method approximates the conditional entropy H(fm(x)|f*, D) using H(fm(x)|fM(x) <= f*, D), which is a reasonable but not exact substitution. The current work explicitly focuses on discrete fidelities and does not address continuous fidelity settings. The initial tuning of neural network hyper-parameters relied on AutoML (SMAC3) followed by manual adjustments, which could be resource-intensive or require expert domain knowledge for different applications. Although DNN-MFBO showed efficiency in query time, part of this efficiency might stem from using random initialization for L-BFGS for acquisition function optimization, in contrast to some competitors that employ more expensive global optimization strategies.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "#!/usr/bin/env python# Author: Ari Anders and Zi Wangfrom Box2D import *from Box2D.b2 import *import numpy as npimport pygameimport scipy.iofrom numpy import linalg as LA# this just makes pygame show what's going on    class guiWorld:    def __init__(self, fps):        self.SCREEN_WIDTH, self.SCREEN_HEIGHT = 1000, 1000        self.TARGET_FPS = fps        self.PPM = 10.0 # pixels per meter        self.screen = pygame.display.set_mode((self.SCREEN_WIDTH, self.SCREEN_HEIGHT), 0, 32)        pygame.display.set_caption('push simulator')        self.clock = pygame.time.Clock()        self.screen_origin = b2Vec2(self.SCREEN_WIDTH/(2*self.PPM), self.SCREEN_HEIGHT/(self.PPM*2))        self.colors = {            b2_staticBody : (255,255,255,255),             b2_dynamicBody : (163,209,224,255)            }    def draw(self, bodies, bg_color=(64,64,64,0)):    #def draw(self, bodies, bg_color=(0,0,0,0)):        def my_draw_polygon(polygon, body, fixture):            vertices=[(self.screen_origin + body.transform*v)*self.PPM for v in polygon.vertices]            vertices=[(v[0], self.SCREEN_HEIGHT-v[1]) for v in vertices]            color = self.colors[body.type]            if body.userData == \"obs\":                color = (123,128,120,0)            if body.userData == \"hand\":                color = (174,136,218,0)            pygame.draw.polygon(self.screen, color, vertices)                    def my_draw_circle(circle, body, fixture):            position=(self.screen_origin + body.transform*circle.pos)*self.PPM            position=(position[0], self.SCREEN_HEIGHT-position[1])            color = self.colors[body.type]            if body.userData == \"hand\":                color = (174,136,218,0)            pygame.draw.circle(self.screen, color, [int(x) for x in            position], int(circle.radius*self.PPM))        b2PolygonShape.draw=my_draw_polygon        b2CircleShape.draw=my_draw_circle        # draw the world        self.screen.fill(bg_color)        self.clock.tick(self.TARGET_FPS)         for body in bodies:            for fixture in body.fixtures:                fixture.shape.draw(body,fixture)        pygame.display.flip()# this is the interface to pybox2dclass b2WorldInterface:    def __init__(self, do_gui=True):        self.world = b2World(gravity=(0.0,0.0), doSleep=True)        self.do_gui = do_gui        self.TARGET_FPS = 100        self.TIME_STEP = 1.0/self.TARGET_FPS        self.VEL_ITERS, self.POS_ITERS =10,10        self.bodies = []        if do_gui:            self.gui_world  = guiWorld(self.TARGET_FPS)            #raw_input()        else:            self.gui_world = None    def initialize_gui(self):        if self.gui_world == None:            self.gui_world = guiWorld(self.TARGET_FPS)        self.do_gui = True    def stop_gui(self):        self.do_gui = False    def add_bodies(self, new_bodies):        \"\"\" add a single b2Body or list of b2Bodies to the world\"\"\"        if type(new_bodies) == list:            self.bodies += new_bodies        else:            self.bodies.append(new_bodies)    def step(self, show_display=True, idx=0):        self.world.Step(self.TIME_STEP, self.VEL_ITERS, self.POS_ITERS)        if show_display and self.do_gui:            self.gui_world.draw(self.bodies)            #if idx % 10 == 0:            #    pygame.image.save(self.gui_world.screen,'tmp_images/'+str(int(sm.ttt*100)+idx)+'.bmp')class end_effector:    def __init__(self, b2world_interface, init_pos, base, init_angle, hand_shape='rectangle', hand_size=(0.3,1)):        world= b2world_interface.world        self.hand = world.CreateDynamicBody(position=init_pos,angle=init_angle)        self.hand_shape = hand_shape        self.hand_size = hand_size        # forceunit for circle and rect        if hand_shape == 'rectangle':            rshape = b2PolygonShape(box=hand_size)            self.forceunit = 30.0        elif hand_shape == 'circle':            rshape = b2CircleShape(radius=hand_size)            self.forceunit = 100.0        elif hand_shape == 'polygon':            rshape = b2PolygonShape(vertices=hand_size)        else:            raise Exception(\"%s is not a correct shape\" % hand_shape)        self.hand.CreateFixture(            shape = rshape,            density = .1,            friction = .1            )        self.hand.userData = \"hand\"                friction_joint = world.CreateFrictionJoint(            bodyA = base,            bodyB = self.hand,            maxForce = 2,            maxTorque = 2,            )        b2world_interface.add_bodies(self.hand)            def set_pos(self, pos, angle):        self.hand.position = pos        self.hand.angle = angle    def apply_wrench(self, rlvel=(0,0), ravel=0):        #self.hand.ApplyForce(force, self.hand.position,wake=True)        #if avel != 0:                avel = self.hand.angularVelocity        delta_avel = ravel - avel        torque = self.hand.mass*delta_avel*30.0        self.hand.ApplyTorque(torque, wake=True)                #else:        lvel = self.hand.linearVelocity        delta_lvel = b2Vec2(rlvel) - b2Vec2(lvel)        force = self.hand.mass*delta_lvel*self.forceunit        self.hand.ApplyForce(force, self.hand.position,wake=True)            def get_state(self, verbose=False):        state = list(self.hand.position) + [ self.hand.angle] +  \nYou must remove any control tokens (such as \n and \t) from the generated string values. I will fix it by removing all control tokens from the extracted string values.\n",
    "Experiment Result": "The method described is DNN-MFBO, a multi-fidelity Bayesian Optimization approach. The provided repository content implements a physics-based 'push world' simulator, which represents the objective function (or a set of fidelities) that DNN-MFBO would optimize. Therefore, the extracted 'experimental_code' refers to the simulator's implementation, and 'experimental_info' details its settings for potential use with DNN-MFBO.Experimental settings for the 'push world' simulator include: Input parameters (potentially optimized by DNN-MFBO): rx, ry (robot 1 initial position), xvel, yvel (robot 1 pushing velocity components), simu_steps (robot 1 simulation steps/duration, directly related to query cost), init_angle (robot 1 initial orientation), rx2, ry2 (robot 2 initial position), xvel2, yvel2 (robot 2 pushing velocity components), simu_steps2 (robot 2 simulation steps/duration, also related to query cost), init_angle2 (robot 2 initial orientation), rtor, rtor2 (robot angular torques), gx, gy (goal position for object 1), gx2, gy2 (goal position for object 2).Fixed environment parameters: The simulation environment is configured with specific object shapes (e.g., 'rectangle', 'circle'), sizes, frictions (e.g., ofriction=0.01, bfriction=0.01), densities (e.g., odensity=0.05), and robot hand shapes and sizes (e.g., hand_shape='rectangle', hand_size=(1,0.3)). The overall objective function being evaluated by this simulation is the sum of Euclidean distances between the final positions of two pushed objects and their respective goal positions."
}{
    "Title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
    "Main Contributions": "The paper proposes FastBO, a multi-fidelity Bayesian Optimization (BO) method for Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS). It tackles the challenge of adaptively deciding the appropriate fidelity for each hyperparameter configuration to fit the surrogate model. This is achieved by introducing novel concepts of 'efficient point' (optimal resource-to-performance balance for surrogate model fitting) and 'saturation point' (approximate final fidelity). FastBO's adaptive fidelity identification strategy offers a general way to extend any single-fidelity method to a multi-fidelity setting and demonstrates strong anytime performance, converging rapidly to the global optimum.",
    "Methodology": "FastBO extends Bayesian Optimization (BO) by adaptively determining the fidelity for each configuration. The core methodology involves: 1) Defining an 'efficient point' for a configuration `λi` as `ei = min{r | Ci(r) − Ci(2r) < δ1}`, where `r` is resource level, `Ci(r)` is performance, and `δ1` is a small threshold. This point signifies efficient resource usage and is used to fit the surrogate model. 2) Defining a 'saturation point' as `si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}`, where `δ2` is a small threshold, indicating performance stabilization. The FastBO process includes a warm-up stage, learning curve modeling to estimate efficient and saturation points, evaluating configurations up to their efficient points to update the surrogate model, and a post-processing stage where promising configurations are evaluated up to their saturation points to find the optimal configuration. This adaptive strategy is generalizable to other single-fidelity methods.",
    "Experimental Setup": "FastBO's performance was evaluated against a suite of state-of-the-art multi-fidelity HPO methods, including Random Search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The experiments were conducted on three established benchmarks: LCBench, NAS-Bench-201, and FCNet. The primary validation method was a comparison of 'anytime performance,' which assesses how quickly a method converges to the global optimum.",
    "Limitations": "The paper is presented as an extended abstract, which inherently omits finer details of the method and experiments. The applicability and scalability of FastBO to larger search spaces and distributed computing systems are identified as areas for future improvement, implying potential limitations in these aspects for the current version. The method relies on predefined small thresholds, `δ1` and `δ2`, for defining efficient and saturation points, whose optimal selection or sensitivity is not explicitly discussed in this abstract.",
    "Future Research Directions": "Future work could focus on refining and expanding FastBO to handle larger search spaces and integrate with distributed computing systems. The goal for these directions is to improve FastBO's overall applicability and scalability.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks",
    "Main Contributions": "The paper introduces Batch Multi-fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN) to optimize expensive-to-evaluate black-box functions by leveraging multiple fidelities. Its main contributions are: 1) A deep auto-regressive model using Bayesian neural networks to capture complex, strong, nonstationary, and highly nonlinear relationships across all fidelities, improving surrogate learning. 2) A simple yet efficient batch querying method based on the Max-value Entropy Search (MES) principle, which explicitly penalizes highly correlated queries and encourages diversity, avoiding combinatorial search over fidelities. 3) Efficient computation of the batch acquisition function using posterior samples and moment matching. 4) An alternating optimization algorithm that guarantees improvement at each step to maximize the acquisition function for a batch of inputs and fidelities. 5) Demonstrated superior performance on four real-world hyperparameter optimization tasks compared to state-of-the-art multi-fidelity BO algorithms.",
    "Methodology": "BMBO-DARN employs a deep auto-regressive model for multi-fidelity surrogate learning and a novel batch acquisition strategy. For the surrogate model, it uses a chain of M Bayesian Neural Networks (BNNs), one for each fidelity. The input to the BNN for fidelity 'm' includes the original input 'x' and the outputs of all preceding fidelities (f1(x), ..., fm-1(x)), allowing flexible capture of complex inter-fidelity relationships. Standard Gaussian priors are placed on NN parameters and Gamma priors on noise precisions. Posterior inference is performed using Hamiltonian Monte Carlo (HMC) sampling. For batch querying, a batch acquisition function based on the Max-value Entropy Search (MES) principle is proposed. This function quantifies the mutual information between the batch of query outputs and the function optimum, normalized by the total query cost, to balance benefit and cost while encouraging diversity. To compute this function efficiently, posterior samples of NN weights are drawn, and moment matching is used to approximate the joint posterior of the batch outputs and the function optimum as a multivariate Gaussian distribution. Maximizing this acquisition function for mixed continuous inputs and discrete fidelities is done via an alternating optimization algorithm, where each input-fidelity pair in the batch is cyclically updated while others are fixed, avoiding combinatorial search and guaranteeing improvement at each step.",
    "Experimental Setup": "The evaluation of BMBO-DARN was conducted in two phases: surrogate learning performance and real-world hyperparameter optimization. For surrogate learning, two benchmark functions were used: Levy (two-fidelity) and Branin (three-fidelity), which feature nonlinear/nonstationary transformations between fidelities. Performance was measured using normalized root-mean-square-error (nRMSE) and mean-negative-log-likelihood (MNLL) on 100 test samples. Training data consisted of randomly generated examples (Levy: {130, 65}; Branin: {320, 130, 65} across fidelities). Comparative models included MF-GP-UCB, MF-MES, SHTL, and DNN-MFBO. For real-world applications, BMBO-DARN was applied to hyperparameter optimization of four machine learning models: 1) Convolutional Neural Networks (CNN) for CIFAR-10 image classification (3 fidelities: 1, 10, 50 epochs, cost ratio 1:10:50, metric: negative log-loss). 2) Online Latent Dirichlet Allocation (LDA) for 20NewsGroups text mining (3 fidelities: 1, 10, 50 epochs, metric: perplexity). 3) XGBoost for diabetes diagnosis (3 fidelities: 2, 10, 100 weak learners, cost ratio 1:5:50, metric: log nRMSE). 4) Physics-Informed Neural Networks (PINN) for solving Burger's equation (3 fidelities: 10, 100, 50K L-BFGS max iterations, cost ratio 1:10:50, metric: log nRMSE). Comparison methods included MF-GP-UCB, MF-MES, SHTL, DNN-MFBO, MF-MES-Batch, SF-Batch, SMAC3, Hyperband, BOHB, and BMBO-DARN-1 (B=1). All batch methods used a batch size of 5. Experiments were repeated 5 times, starting with 10 random initial hyperparameter settings at each fidelity, and run until 100 queries were issued.",
    "Limitations": "The paper does not explicitly list limitations of BMBO-DARN itself, but rather emphasizes its advantages over existing methods by highlighting their shortcomings. Implicit limitations of BMBO-DARN could include: 1) The use of Hamiltonian Monte Carlo (HMC) for posterior inference, while providing high-quality uncertainty quantification, can be computationally intensive, although the paper notes the flexibility to switch to approximate inference methods like stochastic gradient HMC. 2) The moment matching technique used to approximate the posterior distribution for efficient acquisition function computation is an approximation and may not perfectly capture the true posterior. 3) The alternating optimization algorithm for maximizing the batch acquisition function guarantees improvement at each step but does not guarantee convergence to a global optimum for the batch.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
    "Main Contributions": "Introduced DyHPO, a Bayesian Optimization method for multi-fidelity hyperparameter optimization (HPO) that dynamically allocates computational budget. It addresses the sub-optimal budget allocation and poor rank correlation issues of existing gray-box HPO methods. DyHPO proposes a novel Bayesian surrogate model with a deep kernel that embeds learning curve dynamics and a multi-fidelity acquisition function, demonstrating statistically significant empirical gains over state-of-the-art HPO methods across diverse deep learning architectures and datasets.",
    "Methodology": "DyHPO employs a Gaussian Process (GP) surrogate model with a novel deep kernel. This deep kernel (K) uses a neural network (φ) to process the hyperparameter configuration (x), current budget (j), and the past learning curve (Y). The network φ concatenates a linear transformation of normalized budget and hyperparameter configuration with a one-dimensional convolution and max pooling of the learning curve. A squared exponential kernel (k) is then applied to the output of φ. The model parameters are optimized via maximum likelihood using Adam. DyHPO extends Expected Improvement (EI) to Multi-Fidelity Expected Improvement (EIMF), which dynamically selects the next configuration and budget (incrementing by one step) to train based on the surrogate's predictions, prioritizing exploration by slowly increasing the budget.",
    "Experimental Setup": "DyHPO was evaluated on 50 datasets across three diverse deep learning architectures and modalities: MLPs on 35 tabular datasets (LCBench), RNNs on 12 NLP tasks (TaskSet, Adam8p search space), and CNNs/NAS on 3 image classification datasets (NAS-Bench-201, CIFAR-10, CIFAR-100, ImageNet). Experiments were run on Amazon EC2 M5 Instances (m5.xlarge). Performance metrics included mean regret and average rank, calculated as the mean of ten repetitions. DyHPO was compared against seven baselines: Random Search, HyperBand, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). Ablation studies were conducted to assess the impact of the learning curve input.",
    "Limitations": "The method lacks evaluation on tabular benchmarks for very large deep learning models like Transformer-based architectures. Its pause and resume training procedure is only applicable to parametric models, implying that non-parametric models would require restarting training. For small datasets that train quickly, the computational overhead of DyHPO (as a model-based technique) might make simpler approaches like random search more efficient and appealing.",
    "Future Research Directions": "Future research could involve scaling DyHPO to optimize very large deep learning models, such as Transformer-based architectures, and potentially developing new benchmarks tailored for them. Exploring extensions to non-parametric models or adapting the training procedure to handle restarts for such cases is another direction. Additionally, optimizing DyHPO's overhead for small, fast-trained datasets or developing adaptive strategies that choose between complex and simpler HPO methods based on dataset characteristics could be beneficial.",
    "Experiment Code": "class DyHPOAlgorithm:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        log_indicator: List,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 500,\n        device: str = None,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        surrogate_config: dict = None,\n        verbose: bool = True,\n    ):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.hp_candidates = hp_candidates\n        self.log_indicator = log_indicator\n\n        self.scaler = MinMaxScaler()\n        self.hp_candidates = self.preprocess_hp_candidates()\n\n        self.minimization = minimization\n        self.seed = seed\n\n        if verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        self.logger = logging.getLogger()\n\n        logging.basicConfig(\n            format='%(levelname)s:%(asctime)s:%(message)s',\n            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',\n            level=logging_level,\n        )\n\n        self.examples = dict()\n        self.performances = dict()\n\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.total_budget = total_budget\n        self.fantasize_step = fantasize_step\n        self.nr_features = self.hp_candidates.shape[1]\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        self.init_budgets = [conf_individual_budget] * initial_configurations_nr\n        self.fraction_random_configs = 0.1\n\n        self.model = None\n        self.initial_random_index = 0\n\n        if surrogate_config is None:\n            self.surrogate_config = {\n                'nr_layers': 2,\n                'nr_initial_features': self.nr_features,\n                'layer1_units': 64,\n                'layer2_units': 128,\n                'cnn_nr_channels': 4,\n                'cnn_kernel_size': 3,\n                'batch_size': 64,\n                'nr_epochs': 1000,\n                'nr_patience_epochs': 10,\n                'learning_rate': 0.001,\n            }\n        else:\n            self.surrogate_config = surrogate_config\n\n        self.best_value_observed = np.NINF\n        self.diverged_configs = set()\n\n        self.info_dict = dict()\n\n        self.suggest_time_duration = 0\n        self.budget_spent = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n\n    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n        train_curves = self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_examples = torch.tensor(train_examples)\n        train_labels = torch.tensor(train_labels)\n        train_budgets = torch.tensor(train_budgets)\n        train_curves = torch.tensor(train_curves)\n\n        train_examples = train_examples.to(device=self.dev)\n        train_labels = train_labels.to(device=self.dev)\n        train_budgets = train_budgets.to(device=self.dev)\n        train_curves = train_curves.to(device=self.dev)\n\n        data = {\n            'X_train': train_examples,\n            'train_budgets': train_budgets,\n            'train_curves': train_curves,\n            'y_train': train_labels,\n        }\n\n        return data\n\n    def _train_surrogate(self):\n        data = self._prepare_dataset_and_budgets()\n        self.logger.info(f'Started training the model')\n\n        self.model.train_pipeline(\n            data,\n            load_checkpoint=False,\n        )\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:\n        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()\n        budgets = np.array(budgets, dtype=np.single)\n        non_scaled_budgets = copy.deepcopy(budgets)\n        budgets = budgets / self.max_benchmark_epochs\n\n        configurations = np.array(configurations, dtype=np.single)\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n\n        learning_curves = self.patch_curves_to_same_length(learning_curves)\n        learning_curves = np.array(learning_curves, dtype=np.single)\n        learning_curves = torch.tensor(learning_curves)\n        learning_curves = learning_curves.to(device=self.dev)\n\n        train_data = self._prepare_dataset_and_budgets()\n        test_data = {\n            'X_test': configurations,\n            'test_budgets': budgets,\n            'test_curves': learning_curves,\n        }\n\n        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)\n\n        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        suggest_time_start = time.time()\n        if self.initial_random_index < len(self.init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. '\n                'Returning randomly sampled configuration'\n            )\n\n            random_indice = self.init_conf_indices[self.initial_random_index]\n            budget = self.init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n\n            return random_indice, budget\n        else:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n                non_scaled_budgets,\n            )\n            best_config_index = hp_indices[best_prediction_index]\n\n            if best_config_index in self.examples:\n                evaluated_budgets = self.examples[best_config_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        self.budget_spent += self.fantasize_step\n\n        if self.budget_spent > self.total_budget:\n            exit(0)\n\n        return best_config_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        learning_curve: np.ndarray,\n        alg_time: Optional[float] = None,\n    ):\n        score = learning_curve[-1]\n        if np.isnan(learning_curve).any():\n            self.update_info_dict(hp_index, b, np.nan, 0)\n            self.diverged_configs.add(hp_index)\n            return\n\n        observe_time_start = time.time()\n\n        self.examples[hp_index] = np.arange(1, b + 1).tolist()\n        self.performances[hp_index] = learning_curve\n\n        if self.best_value_observed < score:\n            self.best_value_observed = score\n            self.no_improvement_patience = 0\n        else:\n            self.no_improvement_patience += 1\n\n        observe_time_end = time.time()\n        train_time_duration = 0\n\n        if self.initial_random_index >= len(self.init_conf_indices):\n            train_time_start = time.time()\n            if self.model is None:\n                self.model = DyHPO(\n                    self.surrogate_config,\n                    self.dev,\n                    self.dataset_name,\n                    self.output_path,\n                    self.seed,\n                )\n\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.model.restart = True\n\n            self._train_surrogate()\n\n            train_time_end = time.time()\n            train_time_duration = train_time_end - train_time_start\n\n        observe_time_duration = observe_time_end - observe_time_start\n        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration\n        if alg_time is not None:\n            total_duration = total_duration + alg_time\n\n        self.update_info_dict(hp_index, b, score, total_duration)\n\n    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        hp_indices = []\n        hp_budgets = []\n        learning_curves = []\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                max_budget = max(budgets)\n                next_budget = max_budget + self.fantasize_step\n                curve = self.performances[hp_index][:max_budget]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)\n                if difference_curve_length > 0:\n                    curve.extend([0.0] * difference_curve_length)\n            else:\n                next_budget = self.fantasize_step\n                curve = [0, 0, 0]\n\n            if next_budget <= self.max_benchmark_epochs:\n                hp_indices.append(hp_index)\n                hp_budgets.append(next_budget)\n                learning_curves.append(curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, learning_curves\n\n    def history_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget, performance in zip(budgets, performances):\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performance)\n                train_curve = performances[:budget - 1] if budget > 1 else [0.0]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)\n                if difference_curve_length > 0:\n                    train_curve.extend([0.0] * difference_curve_length)\n\n                train_curves.append(train_curve)\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    def acq(\n        self,\n        best_value: float,\n        mean: float,\n        std: float,\n        explore_factor: Optional[float] = 0.25,\n        acq_fc: str = 'ei',\n    ) -> float:\n        if acq_fc == 'ei':\n            if std == 0:\n                return 0\n            z = (mean - best_value) / std\n            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n        elif acq_fc == 'ucb':\n            acq_value = mean + explore_factor * std\n        elif acq_fc == 'thompson':\n            acq_value = np.random.normal(mean, std)\n        elif acq_fc == 'exploit':\n            acq_value = mean\n        else:\n            raise NotImplementedError(\n                f'Acquisition function {acq_fc} has not been'\n                f'implemented',\n            )\n\n        return acq_value\n\n    def find_suggested_config(\n        self,\n        mean_predictions: np.ndarray,\n        mean_stds: np.ndarray,\n        budgets: List,\n    ) -> int:\n        highest_acq_value = np.NINF\n        best_index = -1\n\n        index = 0\n        for mean_value, std in zip(mean_predictions, mean_stds):\n            budget = int(budgets[index])\n            best_value = self.calculate_fidelity_ymax(budget)\n            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')\n            if acq_value > highest_acq_value:\n                highest_acq_value = acq_value\n                best_index = index\n\n            index += 1\n\n        return best_index\n\n    def calculate_fidelity_ymax(self, fidelity: int):\n        exact_fidelity_config_values = []\n        lower_fidelity_config_values = []\n\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n                exact_fidelity_config_values.append(performance)\n            except IndexError:\n                learning_curve = self.performances[example_index]\n                lower_fidelity_config_values.append(max(learning_curve))\n\n        if len(exact_fidelity_config_values) > 0:\n            best_value = max(exact_fidelity_config_values)\n        else:\n            best_value = max(lower_fidelity_config_values)\n\n        return best_value\n\n    def preprocess_hp_candidates(self) -> List:\n        log_hp_candidates = []\n\n        for hp_candidate in self.hp_candidates:\n            new_hp_candidate = []\n            for index, hp_value in enumerate(hp_candidate):\n                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)\n\n            log_hp_candidates.append(new_hp_candidate)\n\n        log_hp_candidates = np.array(log_hp_candidates)\n\n        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)\n\n        return log_hp_candidates\n\n    @staticmethod\n    def patch_curves_to_same_length(curves):\n        max_curve_length = 0\n        for curve in curves:\n            if len(curve) > max_curve_length:\n                max_curve_length = len(curve)\n\n        for curve in curves:\n            difference = max_curve_length - len(curve)\n            if difference > 0:\n                curve.extend([0.0] * difference)\n\n        return curves\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, configuration):\n        super(FeatureExtractor, self).__init__()\n\n        self.configuration = configuration\n\n        self.nr_layers = configuration['nr_layers']\n        self.act_func = nn.LeakyReLU()\n        initial_features = configuration['nr_initial_features'] + 1\n        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])\n        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])\n        for i in range(2, self.nr_layers):\n            setattr(\n                self,\n                f'fc{i + 1}',\n                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),\n            )\n            setattr(\n                self,\n                f'bn{i + 1}',\n                nn.BatchNorm1d(configuration[f'layer{i}_units']),\n            )\n\n\n        setattr(\n            self,\n            f'fc{self.nr_layers}',\n            nn.Linear(\n                configuration[f'layer{self.nr_layers - 1}_units'] +\n                configuration['cnn_nr_channels'],\n                configuration[f'layer{self.nr_layers}_units']\n            ),\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),\n            nn.AdaptiveMaxPool1d(1),\n        )\n\n    def forward(self, x, budgets, learning_curves):\n\n        budgets = torch.unsqueeze(budgets, dim=1)\n        x = cat((x, budgets), dim=1)\n        x = self.fc1(x)\n        x = self.act_func(self.bn1(x))\n\n        for i in range(2, self.nr_layers):\n            x = self.act_func(\n                getattr(self, f'bn{i}')(\n                    getattr(self, f'fc{i}')(\n                        x\n                    )\n                )\n            )\n\n        learning_curves = torch.unsqueeze(learning_curves, 1)\n        lc_features = self.cnn(learning_curves)\n        lc_features = torch.squeeze(lc_features, 2)\n\n        x = cat((x, lc_features), dim=1)\n        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))\n\n        return x\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods.GaussianLikelihood,\n    ):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass DyHPO:\n    def __init__(\n        self,\n        configuration: Dict,\n        device: torch.device,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        seed: int = 11,\n    ):\n        super(DyHPO, self).__init__()\n        self.feature_extractor = FeatureExtractor(configuration)\n        self.batch_size = configuration['batch_size']\n        self.nr_epochs = configuration['nr_epochs']\n        self.early_stopping_patience = configuration['nr_patience_epochs']\n        self.refine_epochs = 50\n        self.dev = device\n        self.seed = seed\n        self.model, self.likelihood, self.mll =\n            self.get_model_likelihood_mll(\n                configuration[f'layer{self.feature_extractor.nr_layers}_units']\n            )\n\n        self.model.to(self.dev)\n        self.likelihood.to(self.dev)\n        self.feature_extractor.to(self.dev)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],\n        )\n\n        self.configuration = configuration\n        self.initial_nr_points = 10\n        self.iterations = 0\n        self.restart = True\n\n        self.logger = logging.getLogger(__name__)\n\n        self.checkpoint_path = os.path.join(\n            output_path,\n            'checkpoints',\n            f'{dataset_name}',\n            f'{self.seed}',\n        )\n\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_file = os.path.join(\n            self.checkpoint_path,\n            'checkpoint.pth'\n        )\n\n    def restart_optimization(self):\n        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)\n        self.model, self.likelihood, self.mll =\n            self.get_model_likelihood_mll(\n                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],\n            )\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n    def get_model_likelihood_mll(\n        self,\n        train_size: int,\n    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:\n        train_x = torch.ones(train_size, train_size).to(self.dev)\n        train_y = torch.ones(train_size).to(self.dev)\n\n        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)\n        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)\n\n        return model, likelihood, mll\n\n    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):\n        self.iterations += 1\n        self.logger.debug(f'Starting iteration: {self.iterations}')\n        weights_changed = False\n\n        if load_checkpoint:\n            try:\n                self.load_checkpoint()\n            except FileNotFoundError:\n                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'\n                                  f'Training the GP from the beginning')\n\n        self.model.train()\n        self.likelihood.train()\n        self.feature_extractor.train()\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n        X_train = data['X_train']\n        train_budgets = data['train_budgets']\n        train_curves = data['train_curves']\n        y_train = data['y_train']\n\n        initial_state = self.get_state()\n        training_errored = False\n\n        if self.restart:\n            self.restart_optimization()\n            nr_epochs = self.nr_epochs\n            if self.initial_nr_points <= self.iterations:\n                self.restart = False\n        else:\n            nr_epochs = self.refine_epochs\n\n        mse = 0.0\n\n        for epoch_nr in range(0, nr_epochs):\n\n            nr_examples_batch = X_train.size(dim=0)\n            if nr_examples_batch == 1:\n                continue\n\n            self.optimizer.zero_grad()\n\n            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)\n            self.model.set_train_data(projected_x, y_train, strict=False)\n            output = self.model(projected_x)\n\n            try:\n                loss = -self.mll(output, self.model.train_targets)\n                loss_value = loss.detach().to('cpu').item()\n                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)\n                self.logger.debug(\n                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '\n                    f'Loss: {loss_value:.3f}, '\n                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '\n                    f'noise: {self.model.likelihood.noise.item():.3f}, '\n                )\n                loss.backward()\n                self.optimizer.step()\n            except Exception as training_error:\n                self.logger.error(f'The following error happened while training: {training_error}')\n                self.restart = True\n                training_errored = True\n                break\n\n        if training_errored:\n            self.save_checkpoint(initial_state)\n            self.load_checkpoint()\n\n    def predict_pipeline(\n        self,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        self.model.eval()\n        self.feature_extractor.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad():\n            projected_train_x = self.feature_extractor(\n                train_data['X_train'],\n                train_data['train_budgets'],\n                train_data['train_curves'],\n            )\n            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)\n            projected_test_x = self.feature_extractor(\n                test_data['X_test'],\n                test_data['test_budgets'],\n                test_data['test_curves'],\n            )\n            preds = self.likelihood(self.model(projected_test_x))\n\n        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )\n        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )\n\n        return means, stds\n\n    def load_checkpoint(self):\n        checkpoint = torch.load(self.checkpoint_file)\n        self.model.load_state_dict(checkpoint['gp_state_dict'])\n        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n\n    def save_checkpoint(self, state: Dict =None):\n        if state is None:\n            torch.save(\n                self.get_state(),\n                self.checkpoint_file,\n            )\n        else:\n            torch.save(\n                state,\n                self.checkpoint_file,\n            )\n\n    def get_state(self) -> Dict[str, Dict]:\n        current_state = {\n            'gp_state_dict': deepcopy(self.model.state_dict()),\n            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),\n            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),\n        }\n\n        return current_state",
    "Experiment Result": "The DyHPO method is evaluated with the following experimental settings:\n\n**Overall Experiment Setup (from `main_experiment.py` and `hpo_method.py`):**\n*   **Benchmarks:** LCBench and TaskSet. LCBench is a maximization problem (`minimization=False`), while TaskSet is a minimization problem (`minimization=True`).\n*   **Fantasize Step (`fantasize_step`):** 1 (meaning budgets are incremented by one step).\n*   **Total HPO Budget Limit (`total_budget`):** 1000 evaluations.\n*   **Dataset Name:** Defaults to 'covertype' for illustration.\n*   **Seeds:** Experiments are run for 10 seeds (`np.arange(10)`).\n*   **Max Benchmark Epochs (`max_benchmark_epochs`):** 51 for both LCBench and TaskSet.\n*   **Initial Random Configurations:** 1 configuration evaluated at budget 1 (`initial_configurations_nr = 1`, `conf_individual_budget = 1`).\n*   **Fraction of Random Configurations:** 0.1 (though the provided code primarily samples from the model after initial random configs).\n*   **Acquisition Function (`acq_fc`):** Expected Improvement ('ei') is used.\n*   **Exploration Factor (`explore_factor`):** Defaulted to 0.25 in the `acq` function, but primarily relevant for UCB.\n*   **Hyperparameter Preprocessing:** Hyperparameter candidates are preprocessed using `MinMaxScaler` after applying `math.log` for log-sampled hyperparameters. Budgets are scaled to `[0, 1]` by dividing by `max_benchmark_epochs`.\n*   **Learning Curve Handling:** Learning curves are padded with zeros to a consistent length, at least the `cnn_kernel_size`, for input to the convolutional neural network.\n\n**DyHPO Surrogate Model Configuration (from `DyHPOAlgorithm.__init__` and `DyHPO.__init__`):**\n*   **Deep Kernel Neural Network (`FeatureExtractor`) Architecture:**\n    *   `nr_layers`: 2\n    *   `nr_initial_features`: Determined by the number of hyperparameters (`self.nr_features`).\n    *   `layer1_units`: 64\n    *   `layer2_units`: 128\n    *   `cnn_nr_channels`: 4\n    *   `cnn_kernel_size`: 3 (for the 1D convolution on learning curves).\n*   **Gaussian Process Model:** Uses a `gpytorch.kernels.ScaleKernel` with an `RBFKernel` (Squared Exponential Kernel).\n*   **Optimizer:** Adam optimizer.\n    *   `learning_rate`: 0.001.\n    *   `batch_size`: 64.\n*   **Training Strategy:**\n    *   **Full Training Epochs (`nr_epochs`):** 1000 epochs when training from scratch.\n    *   **Refinement Epochs (`refine_epochs`):** 50 epochs after initial full training.\n    *   **Initial Points for Full Retraining (`initial_nr_points`):** 10 (model restarts from scratch for the first 10 iterations).\n    *   **No Improvement Patience (`no_improvement_patience`):** Model restarts (`self.model.restart = True`) if there's no incumbent improvement for `no_improvement_threshold` iterations, where `no_improvement_threshold` is `int(max_benchmark_epochs + 0.2 * max_benchmark_epochs)`."
}{
    "Title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
    "Main Contributions": "The paper introduces DyHPO, a novel Bayesian Optimization (BO) method for multi-fidelity (gray-box) hyperparameter optimization (HPO) in Deep Learning. It addresses sub-optimal budget allocation in existing methods by dynamically deciding which hyperparameter configurations to train further. DyHPO proposes a new deep kernel for Gaussian Processes that embeds learning curve dynamics and an acquisition function that incorporates multi-budget information. The method is shown to significantly outperform state-of-the-art HPO methods on 50 datasets across tabular, image, and NLP modalities, and diverse architectures (MLP, CNN/NAS, RNN), achieving better mean regret and faster convergence with statistical significance.",
    "Methodology": "DyHPO is a Bayesian Optimization approach based on Gaussian Processes (GP). It employs a deep kernel GP (K) that models the validation score (fi,j) based on the hyperparameter configuration (xi), budget (j), and the past learning curve (Yi,j-1). The deep kernel uses a neural network (φ) as a feature extractor, which consists of linear and convolutional layers. The budget is normalized and concatenated with the hyperparameter configuration, while the learning curve is processed by a one-dimensional convolution followed by global max pooling. Both representations are then combined and fed to another linear layer, whose output is used by a squared exponential kernel (k). The optimal parameters for the kernel and neural network are found by maximizing the likelihood. For acquisition, DyHPO introduces a Multi-Fidelity Expected Improvement (EIMF) function, which extends standard EI by dynamically defining the incumbent best score (ymax_j) based on whether observations exist for the current budget j or for any budget. The algorithm dynamically selects the most promising candidate to train for one additional budget step and updates the surrogate model with the new observation.",
    "Experimental Setup": "DyHPO was evaluated on hyperparameter optimization tasks for tabular, text, and image classification across three benchmarks: 1. LCBench: 35 tabular datasets with 2,000 neural network configurations each, trained for 50 epochs (7 numerical hyperparameters). 2. TaskSet: A subset of 12 NLP tasks with the Adam8p search space (8 continuous hyperparameters), reporting scores every 200 iterations. 3. NAS-Bench-201: 15,625 precomputed architectures on CIFAR-10, CIFAR-100, and ImageNet datasets, trained for 200 epochs (6 categorical hyperparameters). All experiments were run on an Amazon EC2 M5 Instance (m5.xlarge). Performance was measured by mean regret and average rank over ten repetitions. Validation of statistical significance was performed using the Friedman test followed by a pairwise post-hoc Wilcoxon signed-rank test (α = 0.05). Baselines included Random Search, Hyperband, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). An ablation study was conducted to demonstrate the impact of the learning curve input in the deep kernel by comparing DyHPO with a variant that omits this input (DyHPO w/o CNN).",
    "Limitations": "The method lacks evaluation on very large deep learning models, such as Transformer-based architectures, due to the absence of suitable tabular benchmarks. The pause and resume functionality of the training procedure is only applicable to parametric models, meaning non-parametric models would require restarting training. For datasets that train very quickly, the computational overhead of model-based techniques like DyHPO might make simpler approaches like random search more practical and appealing. Additionally, the paper cautions against running the method for extended periods for only marginal performance gains unless mission-critical.",
    "Future Research Directions": "Implicit future research directions include scaling HPO for very large deep learning models like Transformers and extending the applicability of the pause-and-resume training procedure to a broader range of models beyond parametric ones. Further optimization of the method's overhead for fast-training tasks could also be explored. The authors also invite the community to create sparse benchmarks with surrogates instead of dense tabular ones to save energy, which could guide future benchmark development.",
    "Experiment Code": "class DyHPOAlgorithm:    def __init__(        self,        hp_candidates: np.ndarray,        log_indicator: List,        seed: int = 11,        max_benchmark_epochs: int = 52,        fantasize_step: int = 1,        minimization: bool = True,        total_budget: int = 500,        device: str = None,        dataset_name: str = 'unknown',        output_path: str = '.',        surrogate_config: dict = None,        verbose: bool = True,    ):        torch.backends.cudnn.deterministic = True        torch.backends.cudnn.benchmark = False        torch.manual_seed(seed)        np.random.seed(seed)        if device is None:            self.dev = torch.device(                'cuda') if torch.cuda.is_available() else torch.device('cpu')        else:            self.dev = torch.device(device)        self.hp_candidates = hp_candidates        self.log_indicator = log_indicator        self.scaler = MinMaxScaler()        self.hp_candidates = self.preprocess_hp_candidates()        self.minimization = minimization        self.seed = seed        if verbose:            logging_level = logging.DEBUG        else:            logging_level = logging.INFO        self.logger = logging.getLogger()        logging.basicConfig(            format='%(levelname)s:%(asctime)s:%(message)s',            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',            level=logging_level,        )        self.examples = dict()        self.performances = dict()        torch.manual_seed(seed)        np.random.seed(seed)        self.max_benchmark_epochs = max_benchmark_epochs        self.total_budget = total_budget        self.fantasize_step = fantasize_step        self.nr_features = self.hp_candidates.shape[1]        initial_configurations_nr = 1        conf_individual_budget = 1        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)        self.init_budgets = [conf_individual_budget] * initial_configurations_nr        self.fraction_random_configs = 0.1        self.model = None        self.initial_random_index = 0        if surrogate_config is None:            self.surrogate_config = {                'nr_layers': 2,                'nr_initial_features': self.nr_features,                'layer1_units': 64,                'layer2_units': 128,                'cnn_nr_channels': 4,                'cnn_kernel_size': 3,                'batch_size': 64,                'nr_epochs': 1000,                'nr_patience_epochs': 10,                'learning_rate': 0.001,            }        else:            self.surrogate_config = surrogate_config        self.best_value_observed = np.NINF        self.diverged_configs = set()        self.info_dict = dict()        self.suggest_time_duration = 0        self.budget_spent = 0        self.output_path = output_path        self.dataset_name = dataset_name        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)        self.no_improvement_patience = 0    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()        train_examples = np.array(train_examples, dtype=np.single)        train_labels = np.array(train_labels, dtype=np.single)        train_budgets = np.array(train_budgets, dtype=np.single)        train_curves = self.patch_curves_to_same_length(train_curves)        train_curves = np.array(train_curves, dtype=np.single)        train_budgets = train_budgets / self.max_benchmark_epochs        train_examples = torch.tensor(train_examples)        train_labels = torch.tensor(train_labels)        train_budgets = torch.tensor(train_budgets)        train_curves = torch.tensor(train_curves)        train_examples = train_examples.to(device=self.dev)        train_labels = train_labels.to(device=self.dev)        train_budgets = train_budgets.to(device=self.dev)        train_curves = train_curves.to(device=self.dev)        data = {            'X_train': train_examples,            'train_budgets': train_budgets,            'train_curves': train_curves,            'y_train': train_labels,        }        return data    def _train_surrogate(self):        data = self._prepare_dataset_and_budgets()        self.logger.info(f'Started training the model')        self.model.train_pipeline(            data,            load_checkpoint=False,        )    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()        budgets = np.array(budgets, dtype=np.single)        non_scaled_budgets = copy.deepcopy(budgets)        budgets = budgets / self.max_benchmark_epochs        configurations = np.array(configurations, dtype=np.single)        configurations = torch.tensor(configurations)        configurations = configurations.to(device=self.dev)        budgets = torch.tensor(budgets)        budgets = budgets.to(device=self.dev)        learning_curves = self.patch_curves_to_same_length(learning_curves)        learning_curves = np.array(learning_curves, dtype=np.single)        learning_curves = torch.tensor(learning_curves)        learning_curves = learning_curves.to(device=self.dev)        train_data = self._prepare_dataset_and_budgets()        test_data = {            'X_test': configurations,            'test_budgets': budgets,            'test_curves': learning_curves,        }        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets    def suggest(self) -> Tuple[int, int]:        suggest_time_start = time.time()        if self.initial_random_index < len(self.init_conf_indices):            self.logger.info(                'Not enough configurations to build a model. '                'Returning randomly sampled configuration'            )            random_indice = self.init_conf_indices[self.initial_random_index]            budget = self.init_budgets[self.initial_random_index]            self.initial_random_index += 1            return random_indice, budget        else:            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()            best_prediction_index = self.find_suggested_config(                mean_predictions,                std_predictions,                non_scaled_budgets,            )            best_config_index = hp_indices[best_prediction_index]            if best_config_index in self.examples:                evaluated_budgets = self.examples[best_config_index]                max_budget = max(evaluated_budgets)                budget = max_budget + self.fantasize_step                if budget > self.max_benchmark_epochs:                    budget = self.max_benchmark_epochs            else:                budget = self.fantasize_step        suggest_time_end = time.time()        self.suggest_time_duration = suggest_time_end - suggest_time_start        self.budget_spent += self.fantasize_step        if self.budget_spent > self.total_budget:            exit(0)        return best_config_index, budget    def observe(        self,        hp_index: int,        b: int,        learning_curve: np.ndarray,        alg_time: Optional[float] = None,    ):        score = learning_curve[-1]        if np.isnan(learning_curve).any():            self.update_info_dict(hp_index, b, np.nan, 0)            self.diverged_configs.add(hp_index)            return        observe_time_start = time.time()        self.examples[hp_index] = np.arange(1, b + 1).tolist()        self.performances[hp_index] = learning_curve        if self.best_value_observed < score:            self.best_value_observed = score            self.no_improvement_patience = 0        else:            self.no_improvement_patience += 1        observe_time_end = time.time()        train_time_duration = 0        if self.initial_random_index >= len(self.init_conf_indices):            train_time_start = time.time()            if self.model is None:                self.model = DyHPO(                    self.surrogate_config,                    self.dev,                    self.dataset_name,                    self.output_path,                    self.seed,                )            if self.no_improvement_patience == self.no_improvement_threshold:                self.model.restart = True            self._train_surrogate()            train_time_end = time.time()            train_time_duration = train_time_end - train_time_start        observe_time_duration = observe_time_end - observe_time_start        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration        if alg_time is not None:            total_duration = total_duration + alg_time        self.update_info_dict(hp_index, b, score, total_duration)    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:        examples = []        for hp_index in hp_indices:            examples.append(self.hp_candidates[hp_index])        return examples    def generate_candidate_configurations(        self,    ) -> Tuple[List, List, List, List]:        hp_indices = []        hp_budgets = []        learning_curves = []        for hp_index in range(0, self.hp_candidates.shape[0]):            if hp_index in self.examples:                budgets = self.examples[hp_index]                max_budget = max(budgets)                next_budget = max_budget + self.fantasize_step                curve = self.performances[hp_index][:max_budget]                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)                if difference_curve_length > 0:                    curve.extend([0.0] * difference_curve_length)            else:                next_budget = self.fantasize_step                curve = [0, 0, 0]            if next_budget <= self.max_benchmark_epochs:                hp_indices.append(hp_index)                hp_budgets.append(next_budget)                learning_curves.append(curve)        configurations = self.prepare_examples(hp_indices)        return configurations, hp_indices, hp_budgets, learning_curves    def history_configurations(        self,    ) -> Tuple[List, List, List, List]:        train_examples = []        train_labels = []        train_budgets = []        train_curves = []        for hp_index in self.examples:            budgets = self.examples[hp_index]            performances = self.performances[hp_index]            example = self.hp_candidates[hp_index]            for budget, performance in zip(budgets, performances):                train_examples.append(example)                train_budgets.append(budget)                train_labels.append(performance)                train_curve = performances[:budget - 1] if budget > 1 else [0.0]                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)                if difference_curve_length > 0:                    train_curve.extend([0.0] * difference_curve_length)                train_curves.append(train_curve)        return train_examples, train_labels, train_budgets, train_curves    def acq(        self,        best_value: float,        mean: float,        std: float,        explore_factor: Optional[float] = 0.25,        acq_fc: str = 'ei',    ) -> float:        if acq_fc == 'ei':            if std == 0:                return 0            z = (mean - best_value) / std            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)        elif acq_fc == 'ucb':            acq_value = mean + explore_factor * std        elif acq_fc == 'thompson':            acq_value = np.random.normal(mean, std)        elif acq_fc == 'exploit':            acq_value = mean        else:            raise NotImplementedError(                f'Acquisition function {acq_fc} has not been'                f'implemented',            )        return acq_value    def find_suggested_config(        self,        mean_predictions: np.ndarray,        mean_stds: np.ndarray,        budgets: List,    ) -> int:        highest_acq_value = np.NINF        best_index = -1        index = 0        for mean_value, std in zip(mean_predictions, mean_stds):            budget = int(budgets[index])            best_value = self.calculate_fidelity_ymax(budget)            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')            if acq_value > highest_acq_value:                highest_acq_value = acq_value                best_index = index            index += 1        return best_index    def calculate_fidelity_ymax(self, fidelity: int):        exact_fidelity_config_values = []        lower_fidelity_config_values = []        for example_index in self.examples.keys():            try:                performance = self.performances[example_index][fidelity - 1]                exact_fidelity_config_values.append(performance)            except IndexError:                learning_curve = self.performances[example_index]                lower_fidelity_config_values.append(max(learning_curve))        if len(exact_fidelity_config_values) > 0:            best_value = max(exact_fidelity_config_values)        else:            best_value = max(lower_fidelity_config_values)        return best_value    def update_info_dict(        self,        hp_index: int,        budget: int,        performance: float,        overhead: float,    ):        hp_index = int(hp_index)        if 'hp' in self.info_dict:            self.info_dict['hp'].append(hp_index)        else:            self.info_dict['hp'] = [hp_index]        if 'scores' in self.info_dict:            self.info_dict['scores'].append(performance)        else:            self.info_dict['scores'] = [performance]        if 'curve' in self.info_dict:            self.info_dict['curve'].append(self.best_value_observed)        else:            self.info_dict['curve'] = [self.best_value_observed]        if 'epochs' in self.info_dict:            self.info_dict['epochs'].append(budget)        else:            self.info_dict['epochs'] = [budget]        if 'overhead' in self.info_dict:            self.info_dict['overhead'].append(overhead)        else:            self.info_dict['overhead'] = [overhead]        with open(os.path.join(self.output_path, f'{self.dataset_name}_{self.seed}.json'), 'w') as fp:            json.dump(self.info_dict, fp)    def preprocess_hp_candidates(self) -> List:        log_hp_candidates = []        for hp_candidate in self.hp_candidates:            new_hp_candidate = []            for index, hp_value in enumerate(hp_candidate):                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)        log_hp_candidates = np.array(log_hp_candidates)        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)        return log_hp_candidates    @staticmethod    def patch_curves_to_same_length(curves):        max_curve_length = 0        for curve in curves:            if len(curve) > max_curve_length:                max_curve_length = len(curve)        for curve in curves:            difference = max_curve_length - len(curve)            if difference > 0:                curve.extend([0.0] * difference)        return curves\nclass FeatureExtractor(nn.Module):    def __init__(self, configuration):        super(FeatureExtractor, self).__init__()        self.configuration = configuration        self.nr_layers = configuration['nr_layers']        self.act_func = nn.LeakyReLU()        initial_features = configuration['nr_initial_features'] + 1        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])        for i in range(2, self.nr_layers):            setattr(                self,                f'fc{i + 1}',                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),            )            setattr(                self,                f'bn{i + 1}',                nn.BatchNorm1d(configuration[f'layer{i}_units']),            )        setattr(            self,            f'fc{self.nr_layers}',            nn.Linear(                configuration[f'layer{self.nr_layers - 1}_units'] +                configuration['cnn_nr_channels'],                configuration[f'layer{self.nr_layers}_units']            ),        )        self.cnn = nn.Sequential(            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),            nn.AdaptiveMaxPool1d(1),        )    def forward(self, x, budgets, learning_curves):        budgets = torch.unsqueeze(budgets, dim=1)        x = cat((x, budgets), dim=1)        x = self.fc1(x)        x = self.act_func(self.bn1(x))        for i in range(2, self.nr_layers):            x = self.act_func(                getattr(self, f'bn{i}')(                    getattr(self, f'fc{i}')(                        x                    )                )            )        learning_curves = torch.unsqueeze(learning_curves, 1)        lc_features = self.cnn(learning_curves)        lc_features = torch.squeeze(lc_features, 2)        x = cat((x, lc_features), dim=1)        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))        return xclass GPRegressionModel(gpytorch.models.ExactGP):    def __init__(        self,        train_x: torch.Tensor,        train_y: torch.Tensor,        likelihood: gpytorch.likelihoods.GaussianLikelihood,    ):        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)        self.mean_module = gpytorch.means.ConstantMean()        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())    def forward(self, x):        mean_x = self.mean_module(x)        covar_x = self.covar_module(x)        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)class DyHPO:    def __init__(        self,        configuration: Dict,        device: torch.device,        dataset_name: str = 'unknown',        output_path: str = '.',        seed: int = 11,    ):        super(DyHPO, self).__init__()        self.feature_extractor = FeatureExtractor(configuration)        self.batch_size = configuration['batch_size']        self.nr_epochs = configuration['nr_epochs']        self.early_stopping_patience = configuration['nr_patience_epochs']        self.refine_epochs = 50        self.dev = device        self.seed = seed        self.model, self.likelihood, self.mll = \n            self.get_model_likelihood_mll(                configuration[f'layer{self.feature_extractor.nr_layers}_units']            )        self.model.to(self.dev)        self.likelihood.to(self.dev)        self.feature_extractor.to(self.dev)        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],        )        self.configuration = configuration        self.initial_nr_points = 10        self.iterations = 0        self.restart = True        self.logger = logging.getLogger(__name__)        self.checkpoint_path = os.path.join(            output_path,            'checkpoints',            f'{dataset_name}',            f'{self.seed}',        )        os.makedirs(self.checkpoint_path, exist_ok=True)        self.checkpoint_file = os.path.join(            self.checkpoint_path,            'checkpoint.pth'        )    def restart_optimization(self):        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)        self.model, self.likelihood, self.mll = \n            self.get_model_likelihood_mll(                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],            )        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],        )    def get_model_likelihood_mll(        self,        train_size: int,    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:        train_x = torch.ones(train_size, train_size).to(self.dev)        train_y = torch.ones(train_size).to(self.dev)        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)        return model, likelihood, mll    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):        self.iterations += 1        self.logger.debug(f'Starting iteration: {self.iterations}')        weights_changed = False        if load_checkpoint:            try:                self.load_checkpoint()            except FileNotFoundError:                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'                                  f'Training the GP from the beginning')        self.model.train()        self.likelihood.train()        self.feature_extractor.train()        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],        )        X_train = data['X_train']        train_budgets = data['train_budgets']        train_curves = data['train_curves']        y_train = data['y_train']        initial_state = self.get_state()        training_errored = False        if self.restart:            self.restart_optimization()            nr_epochs = self.nr_epochs            if self.initial_nr_points <= self.iterations:                self.restart = False        else:            nr_epochs = self.refine_epochs        mse = 0.0        for epoch_nr in range(0, nr_epochs):            nr_examples_batch = X_train.size(dim=0)            if nr_examples_batch == 1:                continue            self.optimizer.zero_grad()            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)            self.model.set_train_data(projected_x, y_train, strict=False)            output = self.model(projected_x)            try:                loss = -self.mll(output, self.model.train_targets)                loss_value = loss.detach().to('cpu').item()                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)                self.logger.debug(                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '                    f'Loss: {loss_value:.3f}, '                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '                    f'noise: {self.model.likelihood.noise.item():.3f}, '                )                loss.backward()                self.optimizer.step()            except Exception as training_error:                self.logger.error(f'The following error happened while training: {training_error}')                self.restart = True                training_errored = True                break        if training_errored:            self.save_checkpoint(initial_state)            self.load_checkpoint()    def predict_pipeline(        self,        train_data: Dict[str, torch.Tensor],        test_data: Dict[str, torch.Tensor],    ) -> Tuple[np.ndarray, np.ndarray]:        self.model.eval()        self.feature_extractor.eval()        self.likelihood.eval()        with torch.no_grad():            projected_train_x = self.feature_extractor(                train_data['X_train'],                train_data['train_budgets'],                train_data['train_curves'],            )            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)            projected_test_x = self.feature_extractor(                test_data['X_test'],                test_data['test_budgets'],                test_data['test_curves'],            )            preds = self.likelihood(self.model(projected_test_x))        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )        return means, stds    def load_checkpoint(self):        checkpoint = torch.load(self.checkpoint_file)        self.model.load_state_dict(checkpoint['gp_state_dict'])        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])    def save_checkpoint(self, state: Dict =None):        if state is None:            torch.save(                self.get_state(),                self.checkpoint_file,            )        else:            torch.save(                state,                self.checkpoint_file,            )    def get_state(self) -> Dict[str, Dict]:        current_state = {            'gp_state_dict': deepcopy(self.model.state_dict()),            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),        }        return current_state",
    "Experiment Result": "Overall HPO Loop Settings: Total budget for HPO: 1000. Fantasize step (how many budget steps to advance per iteration): 1. Maximization/Minimization: False for LCBench, True for TaskSet. Number of initial random configurations before model training starts: 1 configuration, evaluated at budget 1. Percentage of configurations taken randomly during BO: 10%. Seed range for experiments: 0 to 9. Surrogate Model (DyHPO Deep GP) Training Settings: Max epochs for benchmark budget: 51 (from LCBench/TaskSet `max_budget`). Initial full training points for surrogate: 10 points. Training epochs for full model restart: 1000 epochs. Training epochs for model refinement: 50 epochs. Patience for model restart trigger (`no_improvement_patience`): 10 epochs. Batch size for training the feature extractor: 64. Learning rate for Adam optimizer: 0.001. Model restart trigger: if `no_improvement_patience` reaches `int(max_benchmark_epochs + 0.2 * max_benchmark_epochs)`. Feature Extractor (Neural Network) Architecture: Number of linear layers: 2 (plus a final layer for concatenation with CNN features). Units in first linear layer: 64. Units in second linear layer (before CNN concat): 128. CNN channels: 4. CNN kernel size: 3. Acquisition Function: Multi-Fidelity Expected Improvement (EIMF). Hyperparameter Preprocessing: Log transformation for hyperparameters indicated by `log_indicator`. Min-Max scaling for all hyperparameters. Budget normalization to [0, 1] by dividing by `max_benchmark_epochs`. Learning curve padding with zeros to a uniform length."
}{
    "Title": "Gradient Descent: The Ultimate Optimizer",
    "Main Contributions": "The paper introduces a method to automatically compute hypergradients using a simple modification to backpropagation, eliminating the need for manual differentiation of optimizer update rules. This allows for the automatic tuning of various optimizer hyperparameters (e.g., learning rates, momentum coefficients) simultaneously. A key contribution is the ability to recursively stack these hyperoptimizers, creating 'towers' of optimizers that become increasingly robust to the initial human-chosen hyperparameters. The method is validated across MLPs, CNNs, and RNNs, showing improved performance and adaptability to suboptimal initial hyperparameter choices.",
    "Methodology": "The core methodology involves adapting reverse-mode automatic differentiation (AD) to compute hypergradients. Instead of manually deriving the partial derivative of the loss with respect to hyperparameters (e.g., ∂f/∂α), the system modifies the computation graph management to allow gradients to flow through the hyperparameters. Specifically, during the optimization step, the hyperparameter (e.g., α) is not 'detached' from the computation graph, while its parents are, enabling backpropagation to compute its gradient. This approach generalizes to various optimizers (SGD, Adam, AdaGrad, RMSProp) and allows for the simultaneous optimization of all their continuous hyperparameters. For hyperparameters with domain constraints (e.g., (0,1) for Adam's β1, β2), a scaled sigmoid is used to clamp values. Recursive stacking of hyperoptimizers is achieved by defining a HyperSGD class where the optimizer for its own hyperparameter (e.g., κ) is itself another optimizer, allowing for arbitrary levels of nested optimization.",
    "Experimental Setup": "Experiments were conducted on a single NVIDIA TITAN Xp GPU. The method was evaluated on: (1) An MLP with one hidden layer (128 nodes, tanh activations) on the MNIST dataset, trained for 30 epochs with a batch size of 256. (2) A ResNet-20 on the CIFAR-10 dataset, replicating baseline procedures from prior work (SGD, α=0.1, momentum=0.9, weight decay=10^-4), trained for 200 or 500 epochs. (3) A character-level RNN (Char-RNN) with a 2-layer LSTM (128 hidden nodes) on the Tolstoy dataset, using an Adam optimizer for 50,000 gradient descent steps. (4) Scalability tests involved fine-tuning a pretrained ResNet-152 (on ImageNet) to the Caltech-256 dataset. Various initial hyperparameter settings (e.g., 'small,' 'good,' 'large' step sizes and momentum, different orders of magnitude for base-level step sizes in higher-order stacks) were tested to evaluate robustness. Performance was measured by test error/accuracy and perplexity, and compared against non-hyperoptimized baselines and re-runs with learned hyperparameters.",
    "Limitations": "One limitation is the instability of the system when initial hyperparameters are set 'far too high,' causing divergence before the hyperoptimizer can take effect, suggesting a need for deeper theoretical analysis of convergence. The current PyTorch implementation requires careful management of the computation graph to avoid issues like memory leaks (logged tensors must be detached) and incompatibility with certain built-in PyTorch modules (e.g., LSTM) that silently modify the graph, potentially leading to incorrect gradients.",
    "Future Research Directions": "Future research should focus on designing hyperoptimizers that are robust to very high initial hyperparameters, possibly through a deeper theoretical analysis of convergence. There is also a need for advancements in differentiable programming languages to enable a more modular and composable expression of methods like this, thereby minimizing the risk of computation graph-related bugs and improving developer experience. The broader impact of reducing energy consumption from hyperparameter searches in deep learning is also highlighted as a motivation for further work in on-line hyperparameter tuning.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels",
    "Main Contributions": "The main research problem addresses the scalability limitations of gradient-based hyperparameter optimization in deep learning, particularly the high computational cost of estimating marginal likelihood gradients which typically require a full pass through the dataset. The paper introduces novel stochastic lower bounds to the linearized Laplace approximation of the marginal likelihood, which are amenable to stochastic-gradient-based optimization. These bounds allow for efficient, batch-based hyperparameter optimization, enabling a trade-off between estimation accuracy and computational complexity. The new estimators significantly accelerate gradient-based hyperparameter optimization, making it applicable to larger datasets and complex tasks like invariance learning, where previous full-batch methods were intractable.",
    "Methodology": "The methodology is centered on deriving stochastic lower bounds for the linearized Laplace approximation of the marginal likelihood. This is achieved by leveraging the dual function-space form of the linearized Laplace, which can be estimated using the Neural Tangent Kernel (NTK). The key steps include: 1) Proving that existing structured parametric approximations (e.g., block-diagonal and diagonal Gauss-Newton, KFAC) are, in fact, lower bounds to the linearized Laplace marginal likelihood (Theorem 1). 2) Deriving novel data-subset lower bounds using the NTK form of the log-determinant, which allows partitioning input-output pairs into batches for unbiased stochastic estimation and gradients (Theorem 2). 3) Demonstrating the equivalence of the NTK-based data-subset bound to a parametric variant (Theorem 3), and combining this with the parametric structured bounds to create 'doubly lower bounds' for structured parametric approximations (like KFAC) on data subsets (Corollary 3.1). The paper also explores strategies for partitioning data (e.g., output-wise partitioning or grouping by labels) to improve bound tightness and efficiency.",
    "Experimental Setup": "Experiments were conducted across various deep learning tasks and datasets to validate the proposed stochastic estimators: 1) **Illustrative Examples:** A small 3-layer convolutional neural network (approx. 16,000 parameters) was used on random subsets of 1,000 MNIST digits (both original and rotated) to optimize layer-wise prior precision (weight decay) and rotational invariance. 2) **CIFAR-10 and CIFAR-100 Classification:** A Wide ResNet 16-4 architecture with Fixup initialization (to prevent conflicts with Gaussian priors) was used to learn layer-wise prior precisions and affine invariances. Models were trained for 300 epochs using SGD with momentum and Adam for hyperparameters. 3) **Varying Dataset Sizes:** Invariance learning experiments were performed on transformed versions of CIFAR-10 using random subsets ranging from 1,000 to 50,000 data points with ResNets. 4) **Large-Scale Benchmark:** TinyImagenet (100,000 data points, 200 classes) with a ResNet-50 (approx. 23 million parameters) was used to test scalability for prior precision and invariance learning. Performance was evaluated using negative log marginal likelihood, test log likelihood, test accuracy, and runtime, benchmarking against full-batch KFAC-Laplace, Augerino, and neural network partitioning methods.",
    "Limitations": "The paper acknowledges several limitations: 1) While the proposed methods significantly improve scalability, using excessively small subset sizes (batch sizes) for stochastic estimators can lead to optimization failures. 2) Theoretically, using constant subset sizes might lead to looser bounds on larger datasets, although experimental results suggest that more frequent gradient updates can compensate for this. 3) Although NTK-based variants often yield tighter theoretical bounds, KFAC-based bounds are sometimes more practically suited for marginal-likelihood optimization due to computational considerations. 4) Traditional full GGN and NTK computations are intractable for deep neural networks with many parameters and data points, necessitating the approximations developed. 5) Existing parametric bounds (Theorem 1) alone do not improve scalability with dataset size, only with parameter partitioning. 6) Structured approximations like KFAC can be non-trivial to compute or extend to custom architectures.",
    "Future Research Directions": "Future research directions include: 1) Further improving the tightness of the derived bounds, particularly by developing more optimal partitioning strategies for the Neural Tangent Kernel, potentially by tracking (anti-)correlations between inputs, similar to inducing point optimization. 2) Expanding the application of these efficient stochastic estimators to a broader range of complex and general differentiable hyperparameter optimization problems. 3) Exploring the utility and effectiveness of these fast estimators in the context of large-scale Bayesian linear models.",
    "Experiment Code": "# File Path: bound_grid.py\ndef get_marglik_loader(x, y, batch_size, augmenter, grouped_loader):\n    data_factor = len(x) / batch_size\n    DataLoaderCls = GroupedSubsetTensorDataLoader if grouped_loader else SubsetTensorDataLoader\n    marglik_loader = DataLoaderCls(x, y, transform=augmenter, subset_size=batch_size,\n                                   detach=False, data_factor=data_factor)\n    return marglik_loader\n\ndef main(setting, approximation, single_output, grouped_loader, stochastic):\n    # ... initial setup ...\n    X_train, y_train = dataset_to_tensors(train_dataset, subset_indices, device)\n    # ... augmenter setup ...\n\n    # ... initial marglik_optimization (MAP training, not the focus for stochastic bounds here) ...\n\n    ####### Assess bound at converged setting\n    backend_kwargs = dict(differentiable=False, kron_jac=False)\n    la_kwargs = dict(sod=True, single_output=single_output)\n    if approximation == 'kernel' and single_output:\n        la_kwargs['independent'] = True\n\n    if stochastic:\n        batch_sizes = [10, 20, 50, 100, 250, 500, 1000]\n    else:\n        # for parametric no sod bounds\n        batch_sizes = [1000]\n\n    # grid for prior precision or rotation factor\n    grid = np.logspace(-4, 4, 100) if setting == PRIOR else np.linspace(0, np.pi, 100)\n    result_frame = pd.DataFrame(index=batch_sizes, columns=grid)\n    result_frame_sem = pd.DataFrame(index=batch_sizes, columns=grid)\n    for batch_size in batch_sizes:\n        for hparam in grid:\n            set_seed(711)\n            # Create data subset loader for the current batch_size\n            marglik_loader = get_marglik_loader(X_train, y_train, batch_size, augmenter, grouped_loader)\n            marglik_loader = marglik_loader.detach() # Detach from training graph\n\n            # Set hyperparameter (prior precision or rotation factor) for current grid point\n            if setting == INVARIANCE:\n                augmenter.rot_factor.requires_grad = False\n                augmenter.rot_factor.data[2] = float(hparam) # Update rotation factor\n                prior_precision = la.prior_precision # Use converged prior precision\n            else: # setting == PRIOR\n                prior_precision = float(hparam) # Use grid search prior precision\n\n            margliks = list()\n            n_reps = int(subset_size / batch_size) # Number of data subsets to average over\n            for rep in range(n_reps):\n                # Select Laplace approximation class based on 'approximation'\n                if approximation == 'kernel':\n                    lap_cls = FunctionalLaplace\n                elif approximation == 'full':\n                    lap_cls = FullLaplace\n                elif approximation == 'blockdiag':\n                    lap_cls = BlockDiagLaplace\n                elif approximation == 'kron':\n                    lap_cls = KronLaplace\n                elif approximation == 'diag':\n                    lap_cls = DiagLaplace\n                \n                # Initialize Laplace approximation and fit to data subset\n                lap = lap_cls(model, 'classification', prior_precision=prior_precision,\n                              backend=backend, backend_kwargs=backend_kwargs, **la_kwargs)\n                lap.fit(marglik_loader) # Fit to a data subset\n                marglik = lap.log_marginal_likelihood().item() / subset_size # Compute log marginal likelihood\n                margliks.append(marglik)\n            result_frame.loc[batch_size, hparam] = np.mean(margliks)\n            result_frame_sem.loc[batch_size, hparam] = sem(margliks)\n            print(setting, batch_size, hparam, np.mean(margliks), np.nan_to_num(sem(margliks)))\n    # ... saving results ...\n\n# File Path: laplace/baselaplace.py\nclass FunctionalLaplace(BaseLaplace):\n    # ... __init__ ...\n    def _kernel_closure(self, X, y):\n        # Calls the backend's kernel computation based on configuration\n        if self.independent:\n            if self.single_output:\n                if self.single_output_iid:\n                    random_ix = torch.randint(self.n_outputs, (len(y),), device=X.device)\n                else:\n                    random_ix = torch.randint(self.n_outputs, ())\n                return self.backend.single_kernel(X, y, self.prior_precision, output_ix=random_ix)\n            else:\n                return self.backend.indep_kernel(X, y, self.prior_precision)\n        else:\n            return self.backend.kernel(X, y, self.prior_precision_diag, prec=self.prior_precision,\n                                       prec_structure=self.prior_structure)\n\n    def fit_batch(self, X, y, N):\n        # ... setup model/output_size ...\n        self.model.zero_grad()\n        X, y = X.to(self._device), y.to(self._device)\n        loss, H = self._kernel_closure(X, y) # H here is the computed kernel matrix\n        self.loss += loss\n        self.n_data_seen += len(y)\n        self.H.append(H) # Accumulate kernel matrices\n\n    @property\n    def log_det_ratio(self):\n        log_det_ratio = 0\n        for H_kernel in self.H:\n            if self.independent:\n                if self.single_output:\n                    # H_kernel n x n (for single output)\n                    log_det_ratio += self.n_outputs * torch.logdet(\n                        diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                    )\n                else:\n                    # H_kernel c x n x n (for independent outputs)\n                    log_det_ratio += torch.logdet(\n                        batch_diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                    ).sum()\n            else:\n                # H_kernel nc x nc (for full kernel)\n                log_det_ratio += torch.logdet(\n                    diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                )\n        return self.n_data / self.n_data_seen * log_det_ratio # Scale by data factor\n\n# File Path: laplace/curvature/asdl.py\nclass AsdlGGN(AsdlInterface, GGNInterface):\n    # ... __init__ ...\n    def kernel(self, x, y, prec_diag, **kwargs):\n        # Calls linear_network_kernel to compute the kernel matrix\n        f, K = linear_network_kernel(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,\n                                     differentiable=self.differentiable, kron_jac=self.kron_jac)\n        n, c = f.shape\n        K = K.transpose(1, 2).reshape(n*c, n*c) # Reshape to (N*C) x (N*C) for full kernel\n\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n    def indep_kernel(self, x, y, prec_diag):\n        # Calls linear_network_kernel_indep for independent outputs\n        f, K = linear_network_kernel_indep(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,\n                                           differentiable=self.differentiable, kron_jac=self.kron_jac)\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n    def single_kernel(self, x, y, prec_diag, output_ix):\n        # Calls linear_network_kernel_indep for a single output\n        f, K = linear_network_kernel_indep(\n            self._model, x, scale=1 / prec_diag, likelihood=self.likelihood, differentiable=self.differentiable, \n            kron_jac=self.kron_jac, single_output=output_ix\n        )\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n# File Path: dependencies/asdl/asdfghjkl/kernel.py\ndef linear_network_kernel(model, x, scale, likelihood='classification', \n                          differentiable=False, kron_jac=False):\n    # Computes J(x)^T L J(x) where L is the Hessian of the likelihood w.r.t. logits\n    operation_name = OP_BATCH_GRADS_KRON if kron_jac else OP_BATCH_GRADS\n    n_data = x.shape[0]\n    n_params = sum(p.numel() for p in model.parameters())\n\n    with extend(model, operation_name):\n        if x.requires_grad:\n            with disable_param_grad(model):\n                logits = model(x)\n        else:\n            logits = model(x)\n        if logits.ndim > 2: # handle augmented inputs by averaging\n            logits = logits.mean(dim=1)\n        n_data, c = logits.shape\n        j1 = logits.new_zeros(n_data, c, n_params)\n        for k in range(c): # Iterate over output dimensions to get individual Jacobians\n            model.zero_grad()\n            scalar = logits[:, k].sum()\n            if differentiable:\n                scalar.backward(retain_graph=True, create_graph=True)\n            else:\n                scalar.backward(retain_graph=(k < c - 1))\n            j_k = []\n            for module in model.modules():\n                operation = getattr(module, 'operation', None)\n                if operation is None:\n                    continue\n                batch_grads = operation.get_op_results()[operation_name]\n                for g in batch_grads.values():\n                    j_k.append(flatten_after_batch(g))\n            j_k = torch.cat(j_k, dim=1)\n            j1[:, k, :] = j_k\n\n    if likelihood == 'classification':\n        L = logits_hessian_cross_entropy(logits)\n        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale\n    elif likelihood == 'heteroscedastic_regression':\n        L = hessian_heteroscedastic_regression(logits)\n        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale\n    elif likelihood == 'regression':\n        j2 = j1 * scale\n    else:\n        raise ValueError('Invalid likelihood')\n    return logits, torch.einsum('ncp,mdp->nmcd', j1, j2)\n\n\ndef linear_network_kernel_indep(model, x, scale, likelihood='classification', differentiable=False, \n                                kron_jac=False, single_output=None):\n    # Similar to linear_network_kernel but for independent outputs or single output\n    n = x.shape[0]\n\n    module_list = [[module] * (2 if getattr(module, 'bias', None) is not None else 1)\n                   for module in model.modules() if hasattr(module, 'weight')]\n    module_list = [(m, loc) for sublist in module_list\n                   for m, loc in zip(sublist, ['weight', 'bias'])]\n    if len(scale) == 1:\n        scale = [scale] * len(module_list)\n    assert len(scale) == len(module_list), 'Scale should be either scalar or for each weight and bias.'\n    for (module, loc), scalem in zip(module_list, scale):\n        setattr(module, f'{loc}_scale', scalem)\n\n    op_name = OP_GRAM_HADAMARD if kron_jac else OP_GRAM_DIRECT\n    with extend(model, op_name):\n        _zero_kernel(model, n, n)\n        if x.requires_grad:\n            with disable_param_grad(model):\n                outputs = model(x)\n        else:\n            outputs = model(x)\n        if outputs.ndim > 2:\n            outputs = outputs.mean(dim=1)\n        n_classes = outputs.shape[-1]\n        if likelihood == 'classification':\n            if single_output is None:\n                L = logits_diag_hessian_cross_entropy(outputs)\n            else:\n                L = logits_single_hessian_cross_entropy(outputs, single_output)\n        elif likelihood == 'heteroscedastic_regression':\n            if single_output is None:\n                L = hessian_diag_heteroscedastic_regression(outputs)\n            else:\n                L = hessian_single_heteroscedastic_regression(outputs, single_output)\n        else:\n            assert likelihood == 'regression'\n        kernels = []\n        output_range = range(n_classes) if single_output is None else [n_classes]\n        for k in output_range:\n            model.zero_grad()\n            if single_output is None:\n                scalar = outputs[:, k].sum()\n            elif single_output.ndim == 0:\n                scalar = outputs[:, single_output].sum()\n            elif single_output.ndim == 1:\n                scalar = outputs.gather(1, single_output.unsqueeze(-1)).sum()\n            else:\n                raise ValueError('Invalid single_output')\n            scalar.backward(\n                retain_graph=differentiable or (k < n_classes - 1) or (single_output is not None),\n                create_graph=differentiable\n            )\n            if likelihood == 'regression':\n                kernels.append(model.kernel)\n            else:\n                kernels.append(model.kernel * (L if single_output is not None else L[:, k]))\n            _zero_kernel(model, n, n)\n        _clear_kernel(model)\n\n    for (module, loc), scale in zip(module_list, scale):\n        delattr(module, f'{loc}_scale')\n\n    return outputs, kernels[0] if single_output is not None else torch.stack(kernels)",
    "Experiment Result": "The experiments focus on deriving and assessing stochastic lower bounds for the linearized Laplace approximation of the marginal likelihood. \n\n**Models:**\n- Primary model: `MiniNet` (a small convolutional neural network).\n- Other models supported: `MLP`, `LeNet`, `WideResNet`, `ResNet` (FixupResnet).\n\n**Datasets:**\n- Main datasets for bound assessment: `mnist` and `mnist_r180` (rotated MNIST).\n- Other supported datasets: `fmnist`, `cifar10`, `cifar100`, `tinyimagenet`, including their rotated, translated, or scaled variants.\n\n**Laplace Approximations and Backends:**\n- **NTK-based (Functional) Lower Bounds:** Implemented using `FunctionalLaplace` with `AsdlGGN` or `AugAsdlGGN` backends.\n- **Parametric Structured Lower Bounds:** Implemented using `FullLaplace`, `BlockDiagLaplace`, `KronLaplace`, `DiagLaplace` with `AsdlGGN` or `AugAsdlGGN` backends.\n- `AsdlEF` and `AugAsdlEF` backends are also available for Empirical Fisher approximations.\n\n**Experimental Settings for Stochastic Bounds (from `bound_grid.py` and `classification_image.py`):**\n- **Device:** 'cuda' (GPU).\n- **Subset Size (`subset_size`):** Total number of data points considered, can be `len(train_dataset)` or `1000` for specific evaluations. Used to determine `n_reps` for averaging stochastic estimates.\n- **Marginal Likelihood Batch Size (`marglik_batch_size`):** For stochastic bounds (`stochastic=True`), batch sizes are varied: `[10, 20, 50, 100, 250, 500, 1000]`. For non-stochastic (parametric) bounds, `batch_sizes = [1000]` is typically used.\n- **Data Subsetting:** `sod=True` (subset of data) is enabled in `la_kwargs` for all evaluated bounds.\n- **Output Partitioning:** `single_output=True` can be enabled for output-wise partitioning (which requires `independent=True` for `FunctionalLaplace`). `single_output_iid` controls whether single outputs are sampled iid or one per batch.\n- **Data Grouping:** `grouped_loader=True` uses `GroupedSubsetTensorDataLoader` (grouping by labels) for creating data subsets, otherwise `SubsetTensorDataLoader` (random subsets).\n- **Prior Precision / Invariance Parameter Grid:** A grid search is performed over the prior precision (for `PRIOR` setting, `logspace(-4, 4, 100)`) or the rotational invariance parameter of `AffineLayer2d` (for `INVARIANCE` setting, `linspace(0, pi, 100)`).\n- **Number of Repetitions (`n_reps`):** Stochastic estimates are averaged over `int(subset_size / batch_size)` repetitions for each batch size and hyperparameter grid point.\n\n**Optimization during initial training (MAP + hyperparameter search) from `marglik_optimization`:**\n- **Epochs (`n_epochs`):** `100` in `bound_grid.py`, `500` in `classification_image.py`.\n- **Learning Rates:** `lr = 1e-3`, `lr_hyp = 1e-1`, `lr_aug = 0.05` (for augmentation parameters).\n- **Learning Rate Minima:** `lr_min = 1e-4`, `lr_hyp_min = 1e-2`, `lr_aug_min = 0.005` for cosine annealing scheduling.\n- **Optimizers:** `Adam` (default for `MiniNet`), `SGD` (for other models, with special handling for Fixup parameters).\n- **Burn-in Epochs (`n_epochs_burnin`):** `10` epochs before marginal likelihood optimization starts.\n- **Hyperparameter Steps (`n_hypersteps`, `n_hypersteps_prior`):** `1` or `2` hyperparameter steps in `bound_grid.py` (can be `100` for stochastic gradients in `classification_image.py`).\n- **Marglik Frequency (`marglik_frequency`):`1` or `5` epochs before re-estimating marginal likelihood.\n- **Stochastic Gradients (`stochastic_grad`):** Set to `True` when using lower bound estimators.\n- **Kronecker Jacobians (`kron_jac`):** Enabled by default (`True`) for efficiency where applicable.\n- **Augmentation:** `AffineLayer2d` is used as an augmenter for `lila` method, and its parameters can be optimized.\n\n**Evaluation:**\n- The primary metric is the `log_marginal_likelihood`."
}{
    "Title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
    "Main Contributions": "The paper introduces an efficient implicit differentiation algorithm, without matrix inversion, tailored for hyperparameter optimization of non-smooth Lasso-type models. It addresses the challenge of setting regularization parameters for Lasso-type estimators, outperforming traditional grid-search and existing gradient-based methods which suffer from high memory consumption or numerical instability. Key contributions include showing that forward iterative differentiation of block coordinate descent (BCD) for Lasso-type problems converges linearly to the true gradient once the support is identified. The proposed algorithm (Algorithm 2) decouples the Jacobian computation from the regression coefficients, avoiding the solution of potentially ill-conditioned linear systems and leveraging solution sparsity for high-dimensional data. Experiments demonstrate superior performance on held-out error and Stein Unbiased Risk Estimator (SURE) criteria for both Lasso and weighted Lasso.",
    "Methodology": "The hyperparameter optimization problem is framed as a bi-level optimization problem, where an outer loop optimizes a criterion (e.g., hold-out loss, SURE) w.r.t. hyperparameters (λ) and an inner loop computes Lasso-type regression coefficients (β̂(λ)). The core innovation lies in computing the weak Jacobian (∂β̂/∂λ) efficiently. Unlike standard implicit differentiation that relies on optimality conditions for smooth functions and requires solving a p×p linear system, this method leverages the fixed-point iteration property of proximal BCD algorithms for Lasso (which involve soft-thresholding). The authors derive sparse closed-form solutions for the Jacobian of Lasso and weighted Lasso (Proposition 1). The proposed algorithm, termed 'implicit forward iterative differentiation' (Algorithm 2), first computes the regression coefficients and identifies their support, then applies a modified forward differentiation recursion restricted to this support. This decouples computation, avoids large matrix inversions, and guarantees linear convergence of the Jacobian once the support is identified (Proposition 2). Hyperparameters are parametrized as e^λ to handle positivity constraints and scaling. For SURE, a weakly differentiable approximation based on Finite Differences Monte-Carlo (dof_FDMC) is used.",
    "Experimental Setup": "The Python code for the proposed methods is open-sourced as 'sparse-ho', with critical BCD loops implemented using Numba. For fair comparison, all methods utilized the same vanilla BCD algorithm (Algorithm 5) for the inner optimization, stopping when the relative change in the cost function fell below a tolerance of 10^-5. Hypergradient-based methods employed a line-search strategy for optimization steps. Initialization for Lasso used λ_init = λ_max - log(10), while weighted Lasso used a pre-solved regularized problem for initialization. Competitors included: Implicit, Forward Iterative Differentiation (F. Iterdiff.), Grid-search, Random-search, Lattice Hyp., and Bayesian optimization. Backward Iterative Differentiation was largely excluded from main benchmarks due to its high computational cost. Experiments were conducted on two primary applications: 1) Held-out loss with Lasso and MCP estimators on real-world datasets: rcv1 (n=20k, p=20k), 20news (n=11k, p=130k), and finance (n=16k, p=1.6M). 2) SURE criterion with Lasso and weighted Lasso on simulated data, measuring normalized Mean Squared Error (MSE). Simulated data used n=100, varying p from 200 to 10,000, with 5 non-zero coefficients and SNR=3. A supplementary experiment explored performance under non-unique inner problem solutions using a specific toy example (n=100, p=10000).",
    "Limitations": "The theoretical convergence guarantees for the Jacobian rely on the assumption of a unique solution for the inner Lasso optimization problem, although the algorithm is shown to still perform numerically in pathological cases of non-uniqueness. The current theory does not cover non-convex penalty functions like MCP, despite demonstrating proper numerical behavior in experiments. Traditional implicit differentiation and the baseline implicit method (Algorithm 1) can suffer from slow convergence or numerical instability when solving ill-conditioned linear systems, especially with large support sizes. Backward iterative differentiation is significantly slower and memory-intensive, particularly for problems with many hyperparameters. The overall hyperparameter optimization problem is generally non-convex, meaning convergence to a global minimum is not guaranteed and performance can be sensitive to initialization. The SURE criterion requires prior knowledge of the noise variance.",
    "Future Research Directions": "Future work includes extending the theoretical framework to cover non-convex penalty functions, such as the Minimax Concave Penalty (MCP) and Elastic-Net formulations. The authors also suggest leveraging the two-step nature of the proposed algorithm (first solving for coefficients, then Jacobian) to integrate state-of-the-art Lasso solvers that use techniques like active sets or screening rules. Such advanced solvers often introduce discontinuities with respect to hyperparameters, which would pose significant challenges for single-step automatic differentiation approaches, but could potentially be accommodated by the proposed method.",
    "Experiment Code": "class ImplicitForward():\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    get_only_jac.n_iter = i\n\n    return dbeta\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n    alpha = np.exp(log_alpha)\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None\n\nclass Lasso(BaseModel):\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        return alphas[mask] * np.sign(dense) @ jac\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y))\n            alpha_max /= X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        return np.clip(log_alpha, self.log_alpha_max - 12,\n                       self.log_alpha_max + np.log(0.9))\n\nclass WeightedLasso(BaseModel):\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alpha[j] * np.sign(beta[j]) / L[j]\n                ddual_var -= np.outer(X[:, j], (dbeta[j, :] - dbeta_old))\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \\\n                    (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz, :] -= np.outer(\n                    Xjs, (dbeta[j, :] - dbeta_old))\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        size_supp = mask.sum()\n        jac_t_v = np.zeros(size_supp)\n        jac_t_v = alphas[mask] * np.sign(dense) * jac\n        return jac_t_v\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y)) / X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        log_alpha = np.clip(log_alpha, self.log_alpha_max - 5,\n                            self.log_alpha_max + np.log(0.9))\n        return log_alpha\n\nclass FiniteDiffMonteCarloSure(BaseCriterion):\n    def __init__(self, sigma, finite_difference_step=None,\n                 random_state=42):\n        self.sigma = sigma\n        self.random_state = random_state\n        self.finite_difference_step = finite_difference_step\n        self.init_delta_epsilon = False\n\n        self.mask0 = None\n        self.dense0 = None\n        self.quantity_to_warm_start = None\n\n        self.mask02 = None\n        self.dense02 = None\n        self.quantity_to_warm_start2 = None\n\n        self.rmse = None\n\n    def _init_delta_epsilon(self, X):\n        if self.finite_difference_step:\n            self.epsilon = self.finite_difference_step\n        else:\n            self.epsilon = 2.0 * self.sigma / (X.shape[0]) ** 0.3\n        rng = check_random_state(self.random_state)\n        self.delta = rng.randn(X.shape[0])\n        self.init_delta_epsilon = True\n\n    def get_val_grad(\n            self, model, X, y, log_alpha, compute_beta_grad, max_iter=1000,\n            tol=1e-3, monitor=None):\n        if not self.init_delta_epsilon:\n            self._init_delta_epsilon(X)\n\n        def v(mask, dense):\n            X_m = X[:, mask]\n            return (2 * X_m.T @ (\n                    X_m @ dense - y -\n                    self.delta * self.sigma ** 2 / self.epsilon))\n\n        def v2(mask, dense):\n            return ((2 * self.sigma ** 2 *\n                     X[:, mask].T @ self.delta / self.epsilon))\n\n        mask, dense, jac_v, quantity_to_warm_start = compute_beta_grad(\n            X, y, log_alpha, model, v,\n            mask0=self.mask0, dense0=self.dense0,\n            quantity_to_warm_start=self.quantity_to_warm_start,\n            max_iter=max_iter, tol=tol, full_jac_v=True)\n        mask2, dense2, jac_v2, quantity_to_warm_start2 = compute_beta_grad(\n            X, y + self.epsilon * self.delta,\n            log_alpha, model, v2, mask0=self.mask02,\n            dense0=self.dense02,\n            quantity_to_warm_start=self.quantity_to_warm_start2,\n            max_iter=max_iter, tol=tol, full_jac_v=True)\n        val = self.get_val_outer(X, y, mask, dense, mask2, dense2)\n        self.mask0 = mask\n        self.dense0 = dense\n        self.quantity_to_warm_start = quantity_to_warm_start\n\n        self.mask02 = mask2\n        self.dense02 = dense2\n        self.quantity_to_warm_start2 = quantity_to_warm_start2\n\n        if jac_v is not None and jac_v2 is not None:\n            grad = jac_v + jac_v2\n        else:\n            grad = None\n        if monitor is not None:\n            monitor(val, grad, mask, dense, alpha=np.exp(log_alpha))\n\n        return val, grad",
    "Experiment Result": "The hyperparameter optimization problem is framed as a bi-level optimization. The inner loop computes Lasso-type regression coefficients (or Elastic Net, Sparse Logistic Regression, Weighted Lasso, SVM, SVR, SimplexSVR coefficients) using proximal coordinate descent (BCD) algorithms. The inner solver can be `celer.Lasso`, `sklearn.linear_model.LogisticRegression`, `celer.ElasticNet`, or `lightning.classification.LinearSVC` for the respective models. Inner problem parameters include `max_iter` (e.g., 50-10000) and `tol` (e.g., 1e-3 to 1e-8), and `warm_start=True` is often used.\n\n\n\nThe core innovation is the efficient computation of the weak Jacobian (∂β̂/∂λ) using an 'implicit forward iterative differentiation' algorithm (`sparse_ho.ImplicitForward`). This algorithm first computes the regression coefficients and identifies their support, then applies a modified forward differentiation recursion restricted to this support. This decouples computation and avoids large matrix inversions. Key parameters for Jacobian computation are `tol_jac` (e.g., 1e-3 to 1e-8) and `n_iter_jac` (e.g., 100-1000).\n\n\n\nHyperparameters (λ) are parametrized as `log_alpha` internally, and converted to `alpha = np.exp(log_alpha)` for the models, to handle positivity constraints and scaling. Initial hyperparameter values (`alpha0`) are typically set relative to `alpha_max` (e.g., `alpha_max / 10` or `0.1 * alpha_max`), where `alpha_max` is derived from the data (e.g., `np.max(np.abs(X.T @ y)) / n_samples`). A range of hyperparameters from `alpha_min` (e.g., `alpha_max / 100` or `1e-4 * alpha_max`) to `alpha_max` is explored.\n\n\n\nThe outer loop optimizes a criterion such as held-out loss or SURE. Different optimizers are used for the outer problem:\n- `GradientDescent`: `n_outer` (number of outer iterations, e.g., 10-100), `step_size`, `p_grad_norm` (e.g., 1 to 1.9 for adaptive step size).\n- `LineSearch`: `n_outer` (e.g., 10-30), `tolerance_decrease` (e.g., 'constant', 'exponential') for inner solver tolerance.\n- `Adam`: `n_outer` (e.g., 10), `lr` (learning rate, e.g., 0.11), `beta_1`, `beta_2`.\n\n\n\nVarious criteria are employed:\n- `HeldOutMSE`, `HeldOutLogistic`, `HeldOutSmoothedHinge`: Evaluated on a validation set (`idx_val`) after training on `idx_train`.\n- `CrossVal`: Uses `sklearn.model_selection.KFold` (e.g., 5-fold) for splitting data.\n- `FiniteDiffMonteCarloSure`: A weakly differentiable approximation using Finite Differences Monte-Carlo (dof_FDMC). It requires a `sigma` (noise level) and an optional `finite_difference_step`; otherwise, it uses a power law heuristic (`2.0 * sigma / (X.shape[0]) ** 0.3`).\n\n\n\nExperiments are conducted on datasets such as `rcv1.binary`, `rcv1`, `simu` (synthetic data from `make_classification` or `make_correlated_data`), `mnist`, `usps`, `sector_scale`, `aloi`, and real MEG data. The `sparse_ho.utils.Monitor` class is used to track objective values, computation times, hyperparameter values, gradients, and accuracy metrics (`acc_vals`, `acc_tests`) throughout the optimization process."
}{
    "Title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
    "Main Contributions": "The paper addresses the challenge of computationally expensive and time-consuming hyper-parameter tuning for deep neural networks. It proposes AUTOMATA, a gradient-based data subset selection framework for hyper-parameter tuning, arguing that informative data subsets can significantly accelerate the process. The main contributions include the AUTOMATA framework itself, which combines intelligent subset selection with existing HPO search and scheduling algorithms, and empirical demonstrations of its effectiveness. AUTOMATA achieves speedups of 3x-30x with comparable performance (0%-2% performance loss) compared to tuning on the entire dataset, and it significantly reduces energy consumption and CO2 emissions, outperforming random subset selection and CRAIG.",
    "Methodology": "AUTOMATA integrates three core components: a hyper-parameter search algorithm (e.g., Random Search, TPE) to identify configurations, a gradient-based subset selection (GSS) algorithm for efficient configuration evaluation, and a hyper-parameter scheduling algorithm (e.g., Hyperband, ASHA) for early stopping. The GSS method selects a subset and associated weights such that the weighted subset loss gradient best approximates the entire training loss gradient, formulated as an optimization problem and solved using a greedy algorithm like Orthogonal Matching Pursuit (OMP). A 'per-batch' variant of GSS is used for efficiency, selecting subsets of mini-batches. Additionally, 'warm-starting' involves initial training on the entire dataset for a few epochs to obtain informative loss gradients, particularly when using the ASHA scheduler.",
    "Experimental Setup": "The framework was evaluated on real-world datasets across text (SST2, SST5, glue-SST2, TREC6), image (CIFAR10, CIFAR100, SVHN), and tabular (DNA, SATIMAGE, LETTER, CONNECT-4 from LIBSVM) domains. Models used were LSTM for text, ResNet18/ResNet50 for image, and a multi-layer perceptron for tabular data. Baselines included RANDOM (random subset selection), CRAIG (another gradient-based subset selection), and FULL (using the entire dataset). Experiments were conducted with different combinations of hyper-parameter search (Random Search, TPE) and scheduling algorithms (Hyperband, ASHA). Subset sizes of 1%, 5%, 10%, and 30% were tested. Evaluation focused on speedup vs. relative test error, energy consumption, and CO2 emissions. Experiments were repeated five times for text/tabular datasets and three times for image datasets. Final training of the best hyper-parameter configuration was performed on the full dataset for all methods (except FULL), with training times included in total tuning times. PyTorch, Ray-tune, and CORDS were used for implementation.",
    "Limitations": "One limitation is the uncertainty regarding the minimum subset size required to achieve desired speed improvements without performance loss, often necessitating the use of larger subset sizes like 10% or 30% when no performance degradation is acceptable.",
    "Future Research Directions": "Future research directions include adaptively changing subset sizes based on model performance for each configuration, aiming to remove the dependency on a fixed subset size. The authors also hope that the AUTOMATA framework will popularize the trend of using subset selection for hyper-parameter tuning and encourage further research into efficient subset selection approaches to advance 'Green AI'.",
    "Experiment Code": "from ray import tune\n\nconfig = dict(setting= \"hyperparamtuning\",\n\n# parameter for subset selection\n# all settings for subset selection will be fetched from here\nsubset_config = \"configs/SL/config_gradmatchpb_glove_sst2.py\",\n\n# parameters for hyper-parameter tuning\n# search space for hyper-parameter tuning\nspace = dict(learning_rate=tune.uniform(0.001, 0.1), \n        # optimizer= tune.choice(['sgd', 'adam']),\n        hidden_size = tune.choice([64, 128, 256]),\n        trn_batch_size= tune.choice([16, 32, 64]),\n        # num_layers = tune.choice([1, 2])\n        ),\n\n# tuning algorithm \nsearch_algo = \"TPE\",\n\n# number of hyper-parameter set to try\nnum_evals = 27,\n\n# metric to be optimized, for 'mean_loss' metric mode should be 'min'\nmetric = \"mean_accuracy\",\nmode = \"max\",\n\n# scheduler to be used (i.e ASHAScheduler)\n# scheduler terminates trials that perform poorly\n# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html\n# scheduler = 'hyperband',\nscheduler = 'ASHA',\n\n# where to store logs\nlog_dir = \"RayLogs/\",\n\n# resume hyper-parameter tuning from previous log\n# specify 'name' (i.e main_2021-03-09_18-33-56) below\nresume = False,\n\n# only required if you want to resume from previous checkpoint\n# it can also be specified if you don't want to resume\nname = None,\n\n# specify resources to be used per trial\n# i.e {'gpu':1, 'cpu':2}\n# resources = {'gpu':1, 'cpu':2},\nresources = {'gpu':0.5, 'cpu':1},\n\n# if True, trains model on Full dataset with the best parameter selected.\nfinal_train = True,\n\nfinal_train_type = 'full' # full, gmpb\n\n)",
    "Experiment Result": "Overall Hyper-parameter Optimization (HPO) Configuration:\n- Setting: Hyperparameter tuning.\n- Subset selection configuration: Uses 'configs/SL/config_gradmatchpb_glove_sst2.py'.\n- Search space: \n  - `learning_rate`: Uniformly sampled between 0.001 and 0.1.\n  - `hidden_size`: Choices among 64, 128, 256.\n  - `trn_batch_size`: Choices among 16, 32, 64.\n- Tuning algorithm: TPE (Tree-structured Parzen Estimator).\n- Number of evaluations: 27.\n- Metric to be optimized: `mean_accuracy`.\n- Optimization mode: `max` (maximize).\n- Scheduler: ASHA (Asynchronous Successive Halving Algorithm).\n- Logging directory: 'RayLogs/'.\n- Resume from previous log: False.\n- Resources per trial: {'gpu': 0.5, 'cpu': 1}.\n- Final training: True (trains the model on the full dataset with the best parameters found).\n- Final training type: 'full'."
}{
    "Title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization",
    "Main Contributions": "The paper addresses the problem of efficiently optimizing multiple competing objectives in parallel Bayesian Optimization, specifically tackling the high computational complexity and lack of analytic gradients for Expected Hypervolume Improvement (EHVI). Its main contributions are: introducing q-Expected Hypervolume Improvement (qEHVI), a novel, exact (up to MC error) acquisition function for parallel and constrained multi-objective BO; enabling efficient optimization by computing exact gradients of the Monte-Carlo estimator via auto-differentiation and leveraging the Sample Average Approximation (SAA) approach; demonstrating its computational tractability and superior performance (both in optimization outcome and wall time) over state-of-the-art methods on various synthetic and real-world benchmarks; and extending its applicability to scenarios with auxiliary outcome constraints and higher-dimensional objective spaces (M>2).",
    "Methodology": "The methodology centers on a novel formulation of q-Expected Hypervolume Improvement (qEHVI). It leverages the inclusion-exclusion principle to compute joint HVI for q candidate points, partitioning the non-dominated space into disjoint hyper-rectangles. For Expected HVI, it employs Monte-Carlo (MC) integration over the Gaussian Process (GP) posterior, using randomized quasi MC for variance reduction. Key to its efficiency is the computation of exact gradients of this MC estimator via auto-differentiation and the re-parameterization trick, enabling the use of deterministic, higher-order optimizers through the Sample Average Approximation (SAA) approach, which provides theoretical convergence guarantees. The method is extended to handle auxiliary outcome constraints by incorporating feasibility-weighting on the sample level and using a differentiable sigmoid approximation for the indicator function. It supports both joint batch optimization and a proper sequential greedy approach that integrates over the posterior of unobserved outcomes.",
    "Experimental Setup": "The experimental setup evaluates qEHVI against state-of-the-art multi-objective BO algorithms (SMS-EGO, PESMO, TS-TCH, analytic EHVI, and a novel qPAREGO extension) and a quasi-random baseline. Each outcome is modeled with an independent Gaussian Process with a Matern 5/2 ARD kernel, with hyperparameters estimated via MAP or Bayesian treatment. All methods are initialized with 2(d+1) Sobol sequence points. qEHVI and qPAREGO use N=128 Quasi-Monte Carlo samples. Acquisition functions are optimized using L-BFGS-B, with exact gradients for qEHVI and qPAREGO, and finite differences for SMS-EGO and PESMO. Performance is measured by log hypervolume difference or hypervolume indicator. Benchmarks include synthetic problems (Branin-Currin, C2-DTLZ2, DTLZ2 with 2-4 objectives, and noisy Branin-Currin) and real-world problems (Vehicle Crash Safety design and Adaptive Bitrate Control policy optimization). Experiments are conducted on both CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and GPU (Tesla V100-SXM2-16GB) hardware, with results reported as means and 2 standard errors over 20 trials. Approximate box decomposition with varying fidelity ζ is also tested for higher-dimensional objectives.",
    "Limitations": "The main limitations of qEHVI are its current assumption of noiseless observations, which is a common constraint for EHVI formulations. Its scalability is also limited by the underlying partitioning algorithm's computational complexity, particularly in high-dimensional objective spaces (M ≥ 4), where the number of required boxes scales super-polynomially. Furthermore, for large M and q on GPUs, memory can become an issue.",
    "Future Research Directions": "Future research directions include extending qEHVI to account for noisy observations by integrating uncertainty from previous observations. Significant improvements can be made by integrating more scalable and efficient partitioning algorithms (both exact and approximate) to enhance performance in high-dimensional objective spaces and optimize memory usage. Further theoretical work could focus on deriving convergence rates for the optimizer and extending existing convergence results to include specific randomized Quasi-Monte Carlo methods. Generally, the authors hope this work encourages the application of modern computational paradigms and tooling to advance Bayesian Optimization.",
    "Experiment Code": "class qLogExpectedHypervolumeImprovement(MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin):_log: bool = Truedef __init__(self,model: Model,ref_point: list[float] | Tensor,partitioning: NondominatedPartitioning,sampler: MCSampler | None = None,objective: MCMultiOutputObjective | None = None,constraints: list[Callable[[Tensor], Tensor]] | None = None,X_pending: Tensor | None = None,eta: Tensor | float = 1e-2,fat: bool = True,tau_relu: float = TAU_RELU,tau_max: float = TAU_MAX,) -> None:r\"\"\"Parallel Log Expected Hypervolume Improvement supporting m>=2 outcomes.See [Ament2023logei]_ for details and the methodology behind the LogEI family ofacquisition function. Line-by-line differences to the original differentiableexpected hypervolume formulation of [Daulton2020qehvi]_ are described via inlinecomments in `forward`.Example:>>> model = SingleTaskGP(train_X, train_Y)>>> ref_point = [0.0, 0.0]>>> acq = qLogExpectedHypervolumeImprovement(model, ref_point, partitioning)>>> value = acq(test_X)Args:model: A fitted model.ref_point: A list or tensor with `m` elements representing the referencepoint (in the outcome space) w.r.t. to which compute the hypervolume.This is a reference point for the objective values (i.e. afterapplying`objective` to the samples).partitioning: A `NondominatedPartitioning` module that provides the non-dominated front and a partitioning of the non-dominated space in hyper-rectangles. If constraints are present, this partitioning must onlyinclude feasible points.sampler: The sampler used to draw base samples. If not given,a sampler is generated using `get_sampler`.objective: The MCMultiOutputObjective under which the samples are evaluated.Defaults to `IdentityMultiOutputObjective()`.constraints: A list of callables, each mapping a Tensor of dimension`sample_shape x batch-shape x q x m` to a Tensor of dimension`sample_shape x batch-shape x q`, where negative values implyfeasibility. The acquisition function will compute expected feasiblehypervolume.X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that havepoints that have been submitted for function evaluation but have not yetbeen evaluated. Concatenated into `X` upon forward call. Copied and setto have no gradient.eta: The temperature parameter for the sigmoid function used for thedifferentiable approximation of the constraints. In case of a float thesame eta is used for every constraint in constraints. In case of a`tensor the length of the tensor must match the number of providedconstraints. The i-th constraint is then estimated with the i-theta value.fat: Toggles the logarithmic / linear asymptotic behavior of the smoothapproximation to the ReLU and the maximum.tau_relu: Temperature parameter controlling the sharpness of theapproximation to the ReLU over the `q` candidate points. For furtherdetails, see the comments above the definition of `TAU_RELU`.tau_max: Temperature parameter controlling the sharpness of theapproximation to the `max` operator over the `q` candidate points.For further details, see the comments above the definition of `TAU_MAX`.\"\"\"if len(ref_point) != partitioning.num_outcomes:raise ValueError(\"The dimensionality of the reference point must match the number of \"f\"outcomes. Got ref_point with {len(ref_point)} elements, but expected \"f\"{partitioning.num_outcomes}.\")ref_point = torch.as_tensor(ref_point,dtype=partitioning.pareto_Y.dtype,device=partitioning.pareto_Y.device,)super().__init__(model=model,sampler=sampler,objective=objective,constraints=constraints,eta=eta,X_pending=X_pending,)self.register_buffer(\"ref_point\", ref_point)cell_bounds = partitioning.get_hypercell_bounds()self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])SubsetIndexCachingMixin.__init__(self)self.tau_relu = tau_reluself.tau_max = tau_maxself.fat = fatdef _compute_log_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:r\"\"\"Compute the expected (feasible) hypervolume improvement given MC samples.Args:samples: A `sample_shape x batch_shape x q' x m`-dim tensor of samples.X: A `batch_shape x q x d`-dim tensor of inputs.Returns:A `batch_shape x (model_batch_shape)`-dim tensor of expected hypervolumeimprovement for each batch.\"\"\"obj = self.objective(samples, X=X) # mc_samples x batch_shape x q x mq = obj.shape[-2]if self.constraints is not None:log_feas_weights = compute_smoothed_feasibility_indicator(constraints=self.constraints,samples=samples,eta=self.eta,log=True,fat=self.fat,)device = self.ref_point.deviceq_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)batch_shape = obj.shape[:-2] # mc_samples x batch_shape# areas tensor is `mc_samples x batch_shape x num_cells x 2`-dimlog_areas_per_segment = torch.full(size=(*batch_shape,self.cell_lower_bounds.shape[-2], # num_cells2, # for even and odd terms),fill_value=-torch.inf,dtype=obj.dtype,device=device,)cell_batch_ndim = self.cell_lower_bounds.ndim - 2# conditionally adding mc_samples dim if cell_batch_ndim > 0# adding ones to shape equal in number to to batch_shape_ndim - cell_batch_ndim# adding cell_bounds batch shape w/o 1st dimensionsample_batch_view_shape = torch.Size([batch_shape[0] if cell_batch_ndim > 0 else 1,*[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],*self.cell_lower_bounds.shape[1:-2],])view_shape = (*sample_batch_view_shape,self.cell_upper_bounds.shape[-2], # num_cells1, # adding for q_choose_i dimensionself.cell_upper_bounds.shape[-1], # num_objectives)for i in range(1, self.q_out + 1):q_choose_i = q_subset_indices[f\"q_choose_{i}\"] # q_choose_i x i# this tensor is mc_samples x batch_shape x i x q_choose_i x mobj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))obj_subsets = obj_subsets.view(obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]) # mc_samples x batch_shape x q_choose_i x i x m# NOTE: the order of operations in non-log _compute_qehvi is 3), 1), 2).# since 3) moved above 1), _log_improvement adds another Tensor dimension# that keeps track of num_cells.# 1) computes log smoothed improvement over the cell lower bounds.# mc_samples x batch_shape x num_cells x q_choose_i x i x mlog_improvement_i = self._log_improvement(obj_subsets, view_shape)# 2) take the minimum log improvement over all i subsets.# since all hyperrectangles share one vertex, the opposite vertex of the# overlap is given by the component-wise minimum.# negative of maximum of negative log_improvement is approximation to min.log_improvement_i = self._smooth_min(log_improvement_i,dim=-2,) # mc_samples x batch_shape x num_cells x q_choose_i x m# 3) compute the log lengths of the cells' sides.# mc_samples x batch_shape x num_cells x q_choose_i x mlog_lengths_i = self._log_cell_lengths(log_improvement_i, view_shape)# 4) take product over hyperrectangle side lengths to compute area (m-dim).# after, log_areas_i is mc_samples x batch_shape x num_cellslog_areas_i = log_lengths_i.sum(dim=-1) # areas_i = lengths_i.prod(dim=-1)# 5) if constraints are present, apply a differentiable approximation of# the indicator function.if self.constraints is not None:log_feas_subsets = log_feas_weights.index_select(dim=-1, index=q_choose_i.view(-1)).view(log_feas_weights.shape[:-1] + q_choose_i.shape)log_areas_i = log_areas_i + log_feas_subsets.unsqueeze(-3).sum(dim=-1)# 6) sum over all subsets of size i, i.e. reduce over q_choose_i-dim# after, log_areas_i is mc_samples x batch_shape x num_cellslog_areas_i = logsumexp(log_areas_i, dim=-1) # areas_i.sum(dim=-1)# 7) Using the inclusion-exclusion principle, set the sign to be positive# for subsets of odd sizes and negative for subsets of even sign# in non-log space: areas_per_segment += (-1) ** (i + 1) * areas_i,# but here in log space, we need to keep track of sign:log_areas_per_segment[..., i % 2] = logplusexp(log_areas_per_segment[..., i % 2],log_areas_i,)# 8) subtract even from odd log area termslog_areas_per_segment = logdiffexp(log_a=log_areas_per_segment[..., 0], log_b=log_areas_per_segment[..., 1])# 9) sum over segments (n_cells-dim) and average over MC samplesreturn logmeanexp(logsumexp(log_areas_per_segment, dim=-1), dim=0)def _log_improvement(self, obj_subsets: Tensor, view_shape: tuple | torch.Size) -> Tensor:# smooth out the clamp and take the log (previous step 3)# subtract cell lower bounds, clamp min at zero, but first# make obj_subsets broadcastable with cell bounds:# mc_samples x batch_shape x (num_cells = 1) x q_choose_i x i x mobj_subsets = obj_subsets.unsqueeze(-4)# making cell bounds broadcastable with obj_subsets:# (mc_samples = 1) x (batch_shape = 1) x num_cells x 1 x (i = 1) x mcell_lower_bounds = self.cell_lower_bounds.view(view_shape).unsqueeze(-3)Z = obj_subsets - cell_lower_boundslog_Zi = self._log_smooth_relu(Z)return log_Zi # mc_samples x batch_shape x num_cells x q_choose_i x i x mdef _log_cell_lengths(self, log_improvement_i: Tensor, view_shape: tuple | torch.Size) -> Tensor:cell_upper_bounds = self.cell_upper_bounds.clamp_max(1e10 if log_improvement_i.dtype == torch.double else 1e8) # num_cells x num_objectives# add batch-dim to compute area for each segment (pseudo-pareto-vertex)# (mc_samples = 1) x (batch_shape = 1) x n_cells x (q_choose_i = 1) x mlog_cell_lengths = ((cell_upper_bounds - self.cell_lower_bounds).log().view(view_shape))# mc_samples x batch_shape x num_cells x q_choose_i x mreturn self._smooth_minimum(log_improvement_i,log_cell_lengths,)def _log_smooth_relu(self, X: Tensor) -> Tensor:f = log_fatplus if self.fat else log_softplusreturn f(X, tau=self.tau_relu)def _smooth_min(self, X: Tensor, dim: int, keepdim: bool = False) -> Tensor:f = fatmin if self.fat else smooth_aminreturn f(X, tau=self.tau_max, dim=dim)def _smooth_minimum(self, X: Tensor, Y: Tensor) -> Tensor:XY = torch.stack(torch.broadcast_tensors(X, Y), dim=-1)return self._smooth_min(XY, dim=-1, keepdim=False)@concatenate_pending_points@t_batch_mode_transform()@average_over_ensemble_modelsdef forward(self, X: Tensor) -> Tensor:posterior = self.model.posterior(X)samples = self.get_posterior_samples(posterior)return self._compute_log_qehvi(samples=samples, X=X)class qExpectedHypervolumeImprovement(MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin):def __init__(self,model: Model,ref_point: list[float] | Tensor,partitioning: NondominatedPartitioning,sampler: MCSampler | None = None,objective: MCMultiOutputObjective | None = None,constraints: list[Callable[[Tensor], Tensor]] | None = None,X_pending: Tensor | None = None,eta: Tensor | float = 1e-3,fat: bool = False,) -> None:r\"\"\"q-Expected Hypervolume Improvement supporting m>=2 outcomes.See [Daulton2020qehvi]_ for details.Example:>>> model = SingleTaskGP(train_X, train_Y)>>> ref_point = [0.0, 0.0]>>> qEHVI = qExpectedHypervolumeImprovement(model, ref_point, partitioning)>>> qehvi = qEHVI(test_X)Args:model: A fitted model.ref_point: A list or tensor with `m` elements representing the referencepoint (in the outcome space) w.r.t. to which compute the hypervolume.This is a reference point for the objective values (i.e. afterapplying`objective` to the samples).partitioning: A `NondominatedPartitioning` module that provides the non-dominated front and a partitioning of the non-dominated space in hyper-rectangles. If constraints are present, this partitioning must onlyinclude feasible points.sampler: The sampler used to draw base samples. If not given,a sampler is generated using `get_sampler`.objective: The MCMultiOutputObjective under which the samples are evaluated.Defaults to `IdentityMCMultiOutputObjective()`.constraints: A list of callables, each mapping a Tensor of dimension`sample_shape x batch-shape x q x m` to a Tensor of dimension`sample_shape x batch-shape x q`, where negative values implyfeasibility. The acquisition function will compute expected feasiblehypervolume.X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that havepoints that have been submitted for function evaluation but have not yetbeen evaluated. Concatenated into `X` upon forward call. Copied and setto have no gradient.eta: The temperature parameter for the sigmoid function used for thedifferentiable approximation of the constraints. In case of a float thesame eta is used for every constraint in constraints. In case of a`tensor the length of the tensor must match the number of providedconstraints. The i-th constraint is then estimated with the i-theta value.fat: A Boolean flag indicating whether to use the heavy-tailed approximationof the constraint indicator.\"\"\"legacy_ei_numerics_warning(legacy_name=type(self).__name__)if len(ref_point) != partitioning.num_outcomes:raise ValueError(\"The length of the reference point must match the number of outcomes. \"f\"Got ref_point with {len(ref_point)} elements, but expected \"f\"{partitioning.num_outcomes}.\")ref_point = torch.as_tensor(ref_point,dtype=partitioning.pareto_Y.dtype,device=partitioning.pareto_Y.device,)super().__init__(model=model,sampler=sampler,objective=objective,constraints=constraints,eta=eta,X_pending=X_pending,)self.register_buffer(\"ref_point\", ref_point)cell_bounds = partitioning.get_hypercell_bounds()self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])SubsetIndexCachingMixin.__init__(self)self.fat = fatdef _compute_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:r\"\"\"Compute the expected (feasible) hypervolume improvement given MC samples.Args:samples: A `n_samples x batch_shape x q' x m`-dim tensor of samples.X: A `batch_shape x q x d`-dim tensor of inputs.Returns:A `batch_shape x (model_batch_shape)`-dim tensor of expected hypervolumeimprovement for each batch.\"\"\"obj = self.objective(samples, X=X)q = obj.shape[-2]if self.constraints is not None:feas_weights = compute_smoothed_feasibility_indicator(constraints=self.constraints,samples=samples,eta=self.eta,fat=self.fat,) # `sample_shape x batch-shape x q`device = self.ref_point.deviceq_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)batch_shape = obj.shape[:-2]# this is n_samples x input_batch_shape xareas_per_segment = torch.zeros(*batch_shape,self.cell_lower_bounds.shape[-2],dtype=obj.dtype,device=device,)cell_batch_ndim = self.cell_lower_bounds.ndim - 2sample_batch_view_shape = torch.Size([batch_shape[0] if cell_batch_ndim > 0 else 1,*[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],*self.cell_lower_bounds.shape[1:-2],])view_shape = (*sample_batch_view_shape,self.cell_upper_bounds.shape[-2],1,self.cell_upper_bounds.shape[-1],)for i in range(1, self.q_out + 1):q_choose_i = q_subset_indices[f\"q_choose_{i}\"]# this tensor is mc_samples x batch_shape x i x q_choose_i x mobj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))obj_subsets = obj_subsets.view(obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:])# since all hyperrectangles share one vertex, the opposite vertex of the# overlap is given by the component-wise minimum.# take the minimum in each subsetoverlap_vertices = obj_subsets.min(dim=-2).values# add batch-dim to compute area for each segment (pseudo-pareto-vertex)# this tensor is mc_samples x batch_shape x num_cells x q_choose_i x moverlap_vertices = torch.min(overlap_vertices.unsqueeze(-3), self.cell_upper_bounds.view(view_shape))# subtract cell lower bounds, clamp min at zerolengths_i = (overlap_vertices - self.cell_lower_bounds.view(view_shape)).clamp_min(0.0)# take product over hyperrectangle side lengths to compute area# sum over all subsets of size iareas_i = lengths_i.prod(dim=-1)# if constraints are present, apply a differentiable approximation of# the indicator functionif self.constraints is not None:feas_subsets = feas_weights.index_select(dim=-1, index=q_choose_i.view(-1)).view(feas_weights.shape[:-1] + q_choose_i.shape)areas_i = areas_i * feas_subsets.unsqueeze(-3).prod(dim=-1)areas_i = areas_i.sum(dim=-1)# Using the inclusion-exclusion principle, set the sign to be positive# for subsets of odd sizes and negative for subsets of even signareas_per_segment += (-1) ** (i + 1) * areas_i# sum over segments and average over MC samplesreturn areas_per_segment.sum(dim=-1).mean(dim=0)@concatenate_pending_points@t_batch_mode_transform()@average_over_ensemble_modelsdef forward(self, X: Tensor) -> Tensor:posterior = self.model.posterior(X)samples = self.get_posterior_samples(posterior)return self._compute_qehvi(samples=samples, X=X)class qLogNoisyExpectedHypervolumeImprovement(NoisyExpectedHypervolumeMixin,qLogExpectedHypervolumeImprovement,):_log: bool = Truedef __init__(self,model: Model,ref_point: list[float] | Tensor,X_baseline: Tensor,sampler: MCSampler | None = None,objective: MCMultiOutputObjective | None = None,constraints: list[Callable[[Tensor], Tensor]] | None = None,X_pending: Tensor | None = None,eta: Tensor | float = 1e-3,prune_baseline: bool = False,alpha: float = 0.0,cache_pending: bool = True,max_iep: int = 0,incremental_nehvi: bool = True,cache_root: bool = True,tau_relu: float = TAU_RELU,tau_max: float = 1e-3, # TAU_MAX,fat: bool = True,marginalize_dim: int | None = None,) -> None:r\"\"\"q-Log Noisy Expected Hypervolume Improvement supporting m>=2 outcomes.Based on the differentiable hypervolume formulation of [Daulton2021nehvi]_.Example:>>> model = SingleTaskGP(train_X, train_Y)>>> ref_point = [0.0, 0.0]>>> qNEHVI = qNoisyExpectedHypervolumeImprovement(model, ref_point, train_X)>>> qnehvi = qNEHVI(test_X)Args:model: A fitted model.ref_point: A list or tensor with `m` elements representing the referencepoint (in the outcome space) w.r.t. to which compute the hypervolume.This is a reference point for the objective values (i.e. afterapplying `objective` to the samples).X_baseline: A `r x d`-dim Tensor of `r` design points that have alreadybeen observed. These points are considered as potential approximatepareto-optimal design points.sampler: The sampler used to draw base samples. If not given,a sampler is generated using `get_sampler`.Note: a pareto front is created for each mc sample, which can becomputationally intensive for `m` > 2.objective: The MCMultiOutputObjective under which the samples areevaluated. Defaults to `IdentityMultiOutputObjective()`.constraints: A list of callables, each mapping a Tensor of dimension`sample_shape x batch-shape x q x m` to a Tensor of dimension`sample_shape x batch-shape x q`, where negative values implyfeasibility. The acquisition function will compute expected feasiblehypervolume.X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points thathave points that have been submitted for function evaluation, buthave not yet been evaluated.eta: The temperature parameter for the sigmoid function used for thedifferentiable approximation of the constraints. In case of a float thesame `eta` is used for every constraint in constraints. In case of a`tensor the length of the tensor must match the number of providedconstraints. The i-th constraint is then estimated with the i-th`eta` value. For more details, on this parameter, see the docs of`compute_smoothed_feasibility_indicator`.fat: A Boolean flag indicating whether to use the heavy-tailed approximationof the constraint indicator.prune_baseline: If True, remove points in `X_baseline` that arehighly unlikely to be the pareto optimal and better than the`reference point. This can significantly improve computation time andis generally recommended. In order to customize pruning parameters,instead manually call `prune_inferior_points_multi_objective` on`X_baseline` before instantiating the acquisition function.alpha: The hyperparameter controlling the approximate non-dominatedpartitioning. The default value of 0.0 means an exact partitioningis used. As the number of objectives `m` increases, consider increasingthis parameter in order to limit computational complexity.cache_pending: A boolean indicating whether to use cached boxdecompositions (CBD) for handling pending points. This isgenerally recommended.max_iep: The maximum number of pending points before the boxdecompositions will be recomputed.incremental_nehvi: A boolean indicating whether to compute theincremental NEHVI from the `i`th point where `i=1, ..., q`under sequential greedy optimization, or the full qNEHVI over`q` points.cache_root: A boolean indicating whether to cache the rootdecomposition over `X_baseline` and use low-rank updates.marginalize_dim: A batch dimension that should be marginalized. For example,this is useful when using a batched fully Bayesian model.\"\"\"MultiObjectiveMCAcquisitionFunction.__init__(self,model=model,sampler=sampler,objective=objective,constraints=constraints,eta=eta,)SubsetIndexCachingMixin.__init__(self)NoisyExpectedHypervolumeMixin.__init__(self,model=model,ref_point=ref_point,X_baseline=X_baseline,sampler=self.sampler,objective=self.objective,constraints=self.constraints,X_pending=X_pending,prune_baseline=prune_baseline,alpha=alpha,cache_pending=cache_pending,max_iep=max_iep,incremental_nehvi=incremental_nehvi,cache_root=cache_root,marginalize_dim=marginalize_dim,)# parameters that are used by qLogEHVIself.tau_relu = tau_reluself.tau_max = tau_maxself.fat = fat@t_batch_mode_transform()@average_over_ensemble_modelsdef forward(self, X: Tensor) -> Tensor:# Get samples from the posterior, and manually concatenate pending points that# have not yet been cached. Shared with qNEHVI.samples, X = self._compute_posterior_samples_and_concat_pending(X)# Add previous nehvi from pending points.return self._compute_log_qehvi(samples=samples, X=X) + self._prev_nehvi",
    "Experiment Result": "The methodology centers on a novel formulation of q-Expected Hypervolume Improvement (qEHVI), which leverages the inclusion-exclusion principle to compute joint HVI for `q` candidate points. Monte-Carlo (MC) integration over the Gaussian Process (GP) posterior is employed, utilizing randomized quasi-Monte Carlo (SobolQMCNormalSampler) for variance reduction. Exact gradients of this MC estimator are computed via auto-differentiation and the re-parameterization trick, enabling higher-order optimizers through the Sample Average Approximation (SAA) approach, which provides theoretical convergence guarantees.The method is extended to handle auxiliary outcome constraints by incorporating feasibility-weighting on the sample level and using a differentiable sigmoid approximation for the indicator function. It supports both joint batch optimization and a sequential greedy approach (controlled by `incremental_nehvi` in noisy variants) that integrates over the posterior of unobserved outcomes.### Specific Experimental Settings and Parameters:- **MC Samples**: The default number of MC samples (`mc_samples`) for acquisition function evaluation is 128 (e.g., in `get_acquisition_function`), though `MCSamplerMixin` has a `_default_sample_shape` of 512.- **Quasi-Monte Carlo (QMC)**: QMC sampling (`qmc=True` by default in `get_acquisition_function`) using `SobolQMCNormalSampler` is employed for variance reduction.- **Reference Point (`ref_point`)**: Used in multi-objective acquisition functions to define the hypervolume region of interest.- **Partitioning (`partitioning`)**: `NondominatedPartitioning` or `FastNondominatedPartitioning` (if `alpha=0`) is used to decompose the non-dominated space. `alpha` (default 0 for <=4 objectives, 10^-3 for >=5, 10^-2 for >=6 objectives in `get_default_partitioning_alpha`) controls the approximation level of the partitioning.- **Constraints Handling**: `constraints` (a list of callables), `eta` (temperature parameter for sigmoid approximation, default 1e-3), and `fat` (boolean for heavy-tailed approximation of constraint indicator, default `True` for LogEI family) are used for auxiliary outcome constraints.- **Log-Space Numerics (`LogEI` family)**: For improved numerical stability, `qLogExpectedHypervolumeImprovement` and `qLogNoisyExpectedHypervolumeImprovement` utilize specific temperature parameters: `tau_relu` (default 1e-6 for smooth ReLU approximation) and `tau_max` (default 1e-2 for smooth maximum operator over `q` candidates, though 1e-3 for noisy variants) and the `fat` parameter. Helper functions `log_fatplus`, `log_softplus`, `logmeanexp`, `logdiffexp`, `logplusexp`, `logsumexp`, `fatmin`, `smooth_amin` are used for robust log-space computations.- **Noisy Baselines (`qNEHVI`, `qLogNEHVI`)**: These variants use `X_baseline` (observed points), `prune_baseline` (default `False`, can remove inferior points from baseline for efficiency), `cache_root` (default `True`, uses low-rank Cholesky updates), and `marginalize_dim` (for fully Bayesian models). `cache_pending` (default `True`) and `max_iep` are also settings for handling pending points.- **Sequential Greedy Optimization**: `incremental_nehvi` (default `True` in `qLogNoisyExpectedHypervolumeImprovement`) is a boolean flag to compute incremental NEHVI for sequential greedy candidate selection.- **General Numerical Settings (from `botorch/__init__.py`)**: `linear_operator.settings` are adjusted: `_fast_covar_root_decomposition`, `_fast_log_prob`, `_fast_solves` are set to `False`; `cholesky_max_tries` to 6; `max_cholesky_size` to 4096; and `gpytorch.settings.max_eager_kernel_size` to 4096. These aim to improve robustness of GP computations in a BO-loop. Constant `CLAMP_LB` (e.g. 1e-8 for float32) is used for numerical stability."
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "The paper introduces Partitioned Neural Networks (PNs), a novel and efficient method for hyperparameter optimization in deep learning. It addresses the challenges of traditional hyperparameter tuning, which relies on multiple training runs and validation sets, making it computationally expensive and problematic in low-data regimes or Federated Learning (FL) settings. PNs are inspired by marginal likelihood and optimize hyperparameters during a single training run without needing a validation set. The key findings include demonstrating PNs' ability to identify correct models in toy tasks, learn effective data augmentations for improved generalization and data efficiency (outperforming baselines like Augerino and Differentiable Laplace on various datasets), optimize complex feature extractors, and successfully tune dropout rates. In FL, PNs significantly reduce communication overhead while achieving better model performance compared to FedAvg and FedAvg + Augerino.",
    "Methodology": "The method partitions the training data into C shards (D1,...,DC) and the neural network parameters (w) into C partitions (w1,...,wC). C subnetworks are defined, where the k-th subnetwork w(k)s consists of the first k parameter partitions (w1,...,wk) and sets the remaining parameters (w_k+1,...,w_C) to default values (e.g., initialization values or zero). Each partition wk is optimized using data from D1:k (i.e., data shards 1 to k) by computing gradients through its corresponding subnetwork w(k)s. Hyperparameters (ψ) are optimized using an 'out-of-training-sample' loss, which measures the loss of subnetwork w(k-1)s on data from the unseen chunk Dk. This objective is a lower-bound approximation to the marginal likelihood. The optimization involves interleaving stochastic gradient updates for parameter partitions and hyperparameters. The primary partitioning scheme used is random weight partitioning, where a fixed proportion of weights in each layer is randomly assigned to a partition. For FL, clients are assigned to non-overlapping chunks, and each client belonging to chunk k optimizes partitions wk:C through subnetworks w(k:C)s, computing gradients for both model parameters and hyperparameters.",
    "Experimental Setup": "Experiments were conducted on various tasks: input selection (toy task with 30 features, 15 informative) using a fully connected MLP; learning affine data augmentations on MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet) using CNN, fixupResNet-8/14, and ResNet-50 architectures; and Federated Learning (FL) with non-i.i.d. splits of MNIST, rotMNIST, CIFAR10, and rotCIFAR10 using CNN and ResNet-9 with GroupNorm. Baselines included standard training, Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, and a multi-network ensemble. For FL, FedAvg and FedAvg + Augerino were used as baselines. Validation methods involved comparing the LML objective, test accuracy, and test log-likelihood. Ablation studies investigated performance in low-data regimes, comparison to traditional train/validation splits with fine-tuning, and sensitivity to various partitioning schemes (number of chunks, data/parameter proportions). Metrics included test accuracy, average log-likelihood, and communication cost reduction for FL.",
    "Limitations": "The proposed method has a few limitations. It inherently requires an additional forward-backward pass to update hyperparameters, leading to increased computational costs, although these are significantly less than existing marginal likelihood-based alternatives. Empirically, partitioned networks generally require more training iterations to converge. The partitioning of the network can inherently constrain its capacity, potentially leading to a slight loss in performance compared to a full network ideally tuned with optimal hyperparameters. Furthermore, the partitioning strategy itself (number of chunks, relative data proportions, and relative parameter proportions) introduces an additional hyperparameter that might require tuning for optimal performance, though experiments suggest reasonable robustness to these choices.",
    "Future Research Directions": "Potential future research directions include exploring dynamic partitioning of network parameters during training, investigating methods to alleviate the performance loss due to constrained network capacity (e.g., by adjusting training rounds or increasing initial network capacity), developing alternative partitioning schemes for reduced overhead (such as node partitioning that allows for simultaneous updates), and exploring strategies for hyperparameter updates like interleaving them at less frequent intervals or accumulating gradients from different chunks for lower variance estimates. In the federated learning context, further exploration of different sequential updating schemes for client-side parameter updates is also suggested.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing",
    "Main Contributions": "This research systematically investigates the problem of federated hyperparameter tuning, identifying key challenges such as federated validation data, extreme resource limitations, and evaluating personalization. It introduces FedEx, a novel method that accelerates federated hyperparameter tuning by making a new connection to the weight-sharing technique from neural architecture search (NAS). FedEx is applicable to widely-used federated optimization methods like FedAvg and its variants. Theoretically, a FedEx variant is shown to correctly tune on-device learning rates in online convex optimization. Empirically, FedEx outperforms natural baselines on Shakespeare, FEMNIST, and CIFAR-10 benchmarks, achieving higher accuracy with the same training budget. This is the first systematic analysis of hyperparameter optimization in the federated setting and the first application of weight-sharing to regular (non-architectural) hyperparameters.",
    "Methodology": "The study formalizes hyperparameter optimization in federated learning (FL) and adapts standard non-federated algorithms (Random Search, Successive Halving) to form baselines. The core method, FedEx, exploits a connection between FL hyperparameter tuning and NAS weight-sharing. It frames the personalized tuning objective as a single-level empirical risk minimization and applies a stochastic relaxation. FedEx alternates between a standard SGD-like update for model weights and an exponentiated gradient update for a categorical distribution over local hyperparameters. It is applicable to FL methods decomposable into local training (Locc) and aggregation (Aggb). A 'local perturbation' scheme is used for selecting initial hyperparameter configurations to ensure stability. For theoretical analysis, the ARUBA (Average Regret-Upper-Bound Analysis) framework is used to prove guarantees for a FedEx variant tuning client step-sizes in an online convex optimization setting.",
    "Experimental Setup": "Experiments were conducted on three standard FL benchmarks: Shakespeare (next-character prediction), FEMNIST (image classification), and CIFAR-10 (image classification). Data partitions included both i.i.d. and non-i.i.d. settings for Shakespeare and FEMNIST, while only i.i.d. was used for CIFAR-10. Data splits were 80% training, 10% validation, and 10% testing for Shakespeare and FEMNIST, and 10K examples from the training/testing split for CIFAR-10 validation. Backbone models from FedAvg evaluation (Shakespeare, CIFAR-10) and LEAF (FEMNIST) were used, with 4K and 2K communication rounds respectively. FedEx was instantiated on FedAvg, FedProx, and Reptile, tuning both server hyperparameters (learning rate schedule, momentum) and client hyperparameters (learning rate, momentum, weight-decay, local epochs, batch-size, dropout, proximal regularization). Baselines included Random Search (RS) and Successive Halving Algorithm (SHA) with an elimination rate of η=3. Performance was evaluated using final test error and online evaluation (test error over communication rounds). Multiple independent trials (5 for Shakespeare, 10 for FEMNIST, 10 for RS on CIFAR, 6 for SHA on CIFAR) were conducted for robust results.",
    "Limitations": "FedEx is primarily applicable to FL algorithms that can be decomposed into local fine-tuning and aggregation, implying that other types of FL algorithms may still require tuning via baselines. Furthermore, FedEx itself relies on wrapper algorithms like SHA or RS to tune its own internal hyperparameters (e.g., baseline calculation λt) and certain server-side FL hyperparameters (e.g., server learning rate). The local perturbation scheme used by FedEx, while improving stability, intrinsically limits the size of the search space explored by each instance of the algorithm. The theoretical analysis for tuning the step-size in an online convex optimization setting, while providing guarantees, incurs an additional m^(1/3) factor compared to existing bounds. Practical observations showed that 'constant' and 'adaptive' step-size schedules for FedEx could assign high probability to multiple configurations late in the tuning process, potentially slowing down training of shared weights.",
    "Future Research Directions": "The authors suggest several future research directions. These include extending FedEx to tune actual architectural hyperparameters, effectively performing federated neural architecture search (NAS). Another avenue is to apply FedEx to tune initialization-based meta-learning algorithms such as MAML. The paper also highlights the general importance of considering privacy and fairness risks in FL, suggesting that any application of their work must integrate community-developed tools for mitigating these issues. Additionally, further work could explore the time-dependency of federated evaluation, a challenge not fully addressed in this paper. Investigating trade-offs for FedEx step-size schedules, such as allowing FedEx to run longer while keeping the total budget constant, is also a potential area for exploration.",
    "Experiment Code": "def discounted_mean(trace, factor=1.0):weight = factor ** np.flip(np.arange(len(trace)), axis=0)return np.inner(trace, weight) / weight.sum()class FedEx:    '''runs hyperparameter optimization given a federated learning server'''    def entropy(self):        entropy = 0.0        for probs in product(*(theta[theta>0.0] for theta in self._theta)):            prob = np.prod(probs)            entropy -= prob * np.log(prob)        return entropy    def mle(self):        return np.prod([theta.max() for theta in self._theta])    def __init__(                 self,                  server,                  configs,                  eta0='auto',                  sched='auto',                  cutoff=0.0,                  baseline=0.0,                  diff=False,                 ):        '''        Args:            server: Object that implements two methods, 'communication_round' and 'full_evaluation'                    taking as input a single argument, 'get_config', itself a function that takes                     no inputs and outputs an element of the provided list 'configs'.                     - 'communication_round' samples a batch of clients, assigns a config to each                     using 'get_config', and runs local training using that config. It then                     aggregates the local models to to take a training step and returns three lists                     or arrays: a list of each client's validation error before local training, a                     list of each client's validation error after local training, and a list of each                     client's weight (e.g. size of its validation set).                     - 'full_evaluation' assigns a config to each client using 'get_config' and runs                    local training using that config. It then returns three lists or arrays: a list                    of each client's test error before local training, a list of each client's test                    error after local training, and a list of each client's weight (e.g. size of                     its test set).            configs: list of configs used for local training and testing by 'server'                      OR dict of (string, list) pairs denoting a grid of configs            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))            sched: learning rate schedule for exponentiated gradient:                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)                    - 'aggressive': uses eta0 / gradient l-infinity norm                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds                    - 'constant': uses eta0                    - 'scale': uses sched * sqrt(2 * log(len(configs)))            cutoff: entropy level below which to stop updating the config probability and use MLE            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean            diff: if True uses performance difference; otherwise uses absolute performance        '''        self._server = server        self._configs = configs        self._grid = [] if type(configs) == list else sorted(configs.keys())        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]        self._sched = sched        self._cutoff = cutoff        self._baseline = baseline        self._diff = diff        self._z = [np.full(size, -np.log(size)) for size in sizes]        self._theta = [np.exp(z) for z in self._z]        self._store = [0.0 for _ in sizes]        self._stopped = False        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}    def stop(self):        self._stopped = True    def sample(self, mle=False, _index=[]):        '''samples from configs using current probability vector'''        if mle or self._stopped:            if self._grid:                return {param: self._configs[param][theta.argmax()]                         for theta, param in zip(self._theta, self._grid)}            return self._configs[self._theta[0].argmax()]        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])        if self._grid:            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}        return self._configs[_index[-1][0]]    def settings(self):        '''returns FedEx input settings'''        output = {'configs': deepcopy(self._configs)}        output['eta0'], output['sched'] = self._eta0, self._sched        output['cutoff'], output['baseline'] = self._cutoff, self._baseline         if self._trace['refine']:            output['theta'] = self.theta()        return output    def step(self):        '''takes exponentiated gradient step (calls 'communication_round' once)'''        index = []        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))                before, after = np.array(before), np.array(after)        weight = np.array(weight, dtype=np.float64) / sum(weight)        if self._trace['refine']:            trace = self.trace('refine')            if self._diff:                trace -= self.trace('global')            baseline = discounted_mean(trace, self._baseline)        else:            baseline = 0.0        self._trace['global'].append(np.inner(before, weight))        self._trace['refine'].append(np.inner(after, weight))        if not index:            self._trace['entropy'].append(0.0)            self._trace['mle'].append(1.0)            return        for i, (z, theta) in enumerate(zip(self._z, self._theta)):            grad = np.zeros(len(z))            for idx, s, w in zip(index, after-before if self._diff else after, weight):                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]            if self._sched == 'adaptive':                self._store[i] += norm(grad, float('inf')) ** 2                denom = np.sqrt(self._store[i])            elif self._sched == 'aggressive':                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))            elif self._sched == 'auto':                self._store[i] += 1.0                denom = np.sqrt(self._store[i])            elif self._sched == 'constant':                denom = 1.0            elif self._sched == 'scale':                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')            else:                raise NotImplementedError            eta = self._eta0[i] / denom            z -= eta * grad            z -= logsumexp(z)            self._theta[i] = np.exp(z)        self._trace['entropy'].append(self.entropy())        self._trace['mle'].append(self.mle())        if self._trace['entropy'][-1] < self._cutoff:            self.stop()    def test(self, mle=False):        '''evaluates found config (calls 'full_evaluation' once)        Args:            mle: use MLE config instead of sampling        Returns:            output of 'full_evaluation'        '''        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))        return {'global': np.inner(before, weight) / weight.sum(),                'refine': np.inner(after, weight) / weight.sum()}    def theta(self):        '''returns copy of config probability vector'''        return deepcopy(self._theta)    def trace(self, key):        '''returns trace of one of three tracked quantities        Args:            key: 'entropy', 'global', or 'refine'        Returns:            numpy vector with length equal to number of calls to 'step'        '''        return np.array(self._trace[key])class Server:    '''object for federated training implementing methods required by FedEx'''    def _set_test_state(self):        state = (np.random.get_state(), torch.get_rng_state(), torch.cuda.get_rng_state())        if self._state is None:            self._state = state        else:            np.random.set_state(self._state[0])            torch.set_rng_state(self._state[1])            torch.cuda.set_rng_state(self._state[2])        return state    def _reset_state(self, state):        np.random.set_state(state[0])        torch.set_rng_state(state[1])        torch.cuda.set_rng_state(state[2])    def __init__(                 self,                  model,                  clients,                  train,                  test,                  lr=1.0,                  momentum=0.0,                  step=1,                  gamma=1.0,                  batch=10,                  state=None,                 ):        '''        Args:            model: PyTorch model            clients: list of clients, each a function that takes one or more strings 'train',                     'val', 'test' and returns, as one tuple, input and output tensors for each            train: method that takes as argument a PyTorch model, an input tensor, an output                   tensor, and optional kwargs and returns the same PyTorch model            test: method that takes as argument a PyTorch model, an input tensor, and an output                  tensor and returns the model's error            lr: server learning rate            momentum: server momentum            step: server learning rate decay interval            gamma: server learning rate decay factor            batch: number of clients to sample per communication round            state: np.random, torch, torch.cuda random state tuple; if None uses current states        '''        self._model = model        self._clients = clients        self._train = train        self._test = test        self._opt = optim.SGD(self._model.parameters(), lr=lr, momentum=momentum)        self._sched = optim.lr_scheduler.StepLR(self._opt, step, gamma=gamma)        self._batch = batch        self._state = state        self._reset_state(self._set_test_state())    def communication_round(self, get_config):        '''runs one step of local training and model aggregation        Args:            get_config: returns kwargs for 'train' as a dict        Returns:            np.array objects for global val error, local val error, and val size of each client        '''        self._model.cuda()        before, after, weight = [np.zeros(self._batch) for _ in range(3)]        total = 0.0        for i in range(self._batch):            Xtrain, Ytrain, Xval, Yval = random.choice(self._clients)('train', 'val')            before[i] = self._test(self._model, Xval, Yval)            model = self._train(deepcopy(self._model), Xtrain, Ytrain, **get_config())            after[i] = self._test(model, Xval, Yval)            weight[i] = len(Yval)            total += len(Ytrain)            if i:                for agg, param in zip(aggregate.parameters(), model.parameters()):                    agg.data += len(Ytrain) * param.data            else:                for param in model.parameters():                    param.data *= len(Ytrain)                aggregate = model        self._opt.zero_grad()        for agg, param in zip(aggregate.parameters(), self._model.parameters()):            param.grad = param.data - agg / total        self._opt.step()        self._opt.zero_grad()        self._sched.step()        self._model.cpu()        return before, after, weight    def full_evaluation(self, get_config):        '''evaluates personalization on each client        Args:            get_config: returns kwargs for 'train' as a dict        Returns:            np.array objects for global test error, local test error, and test size of each client        '''        state = self._set_test_state()        self._model.cuda()        before, after, weight = [np.zeros(len(self._clients)) for _ in range(3)]        for i, client in enumerate(self._clients):            Xtrain, Ytrain, Xtest, Ytest = client('train', 'test')            before[i] = self._test(self._model, Xtest, Ytest)            after[i] = self._test(self._train(deepcopy(self._model),                                               Xtrain, Ytrain, **get_config()),                                   Xtest, Ytest)            weight[i] = len(Ytest)            print('\\r\\tEvaluated client', frac(i+1, len(self._clients)),                  '    global error:', round(np.inner(before, weight) / weight.sum(), 4),                  '    refine error:', round(np.inner(after, weight) / weight.sum(), 4),                   end=32*' ')        self._model.cpu()        self._reset_state(state)        return before, after, weightdef wrapped_fedex(                  get_server,                  get_client,                  num_configs=1,                  prod=False,                  stepsize_init='auto',                  stepsize_sched='aggressive',                  cutoff=1E-4,                  baseline_discount=-1.0,                  diff=False,                  mle=False,                  logdir=None,                  val_discount=0.0,                  last_stop=False,                  eval_global=False,                  **kwargs,                  ):    '''evaluates FedEx wrapped with successive elimination algorithm;       uses FedAvg when num_configs = 1 and prod = False    Args:        get_server: function that takes no input and returns an object that can be passed as the                     first argument to FedEx.__init__, e.g. a Server object        get_client: function that takes no input and returns a dict of local training configs, a                    list of which is passed as the second argument to 'FedEx.__init__'; can also                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'        num_configs: determines number of configs in the list passed to 'FedEx.__init__':                     - >0: use this value directly                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]                     - =-1: use the number of arms given by the wrapper                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':                           - >0.0: use this value directly                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]        diff: passed to 'diff' kwarg of 'FedEx.__init__'        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'        logdir: passed to 'logdir' kwarg of 'successive_elimination'        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'        kwargs: passed to 'get_schedule'    Returns:        FedEx object    '''    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')    if num_configs < -1:        samples = lambda n: random.randint(1, -num_configs)    elif num_configs == -1:        samples = lambda n: n    elif num_configs == 0:        samples = lambda n: random.randint(1, n)    else:        samples = lambda n: num_configs    if baseline_discount < 0.0:        baseline = lambda: random.uniform(0.0, -baseline_discount)    else:        baseline = lambda: baseline_discount    def sampler(n):        for _ in range(n):            yield FedEx(                        get_server(),                         get_client() if prod else get_client(samples(n)),                        eta0=stepsize_init,                         sched=stepsize_sched,                         cutoff=cutoff,                         baseline=baseline(),                        diff=diff,                        )    return successive_elimination(                                  sampler,                                   ['refine', 'global'],                                   logdir=logdir,                                   val_discount=val_discount,                                  elim_rate=elim_rate,                                   elim_sched=elim_sched,                                   eval_sched=eval_sched,                                  traces=['entropy', 'mle', 'global', 'refine'],                                   last_round='stop' if last_stop else None,                                  mle=mle,                                  eval_global=eval_global,                                  )import argparseimport jsonimport osimport pdbimport pickleimport randomimport reimport stringimport mathfrom copy import deepcopyfrom collections import defaultdictfrom glob import globimport numpy as npimport torch; torch.backends.cudnn.benchmark = Truefrom torch import nnfrom torch import optimfrom hyper import wrapped_fedexfrom hyper import Serverimport torch.nn.functional as Fimport torchvision.datasets as datasetsimport torchvision.transforms as transforms CLIENT = lambda: {'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),'momentum': np.random.uniform(low=0.0,high=1.0),'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),'epochs': np.random.choice(np.arange(1, 6)),'batch': 2 ** np.random.choice(np.arange(3, 8)),'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),'dropout': np.random.uniform(low=0.0, high=0.5),}class CNN(nn.Module):    def __init__(self):        super(CNN, self).__init__()        self.conv1 = nn.Sequential(                                   nn.Conv2d(3, 32, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.conv2 = nn.Sequential(                                   nn.Conv2d(32, 64, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.conv3 = nn.Sequential(                                   nn.Conv2d(64, 64, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.dropout = nn.Dropout(0.0)        self.fc = nn.Sequential(                                nn.Linear(1024, 64),                                nn.ReLU(),                                )        self.clf = nn.Linear(64, 10)    def forward(self, x):        x = self.conv1(x)        x = self.conv2(x)        x = self.conv3(x)        x = self.fc(self.dropout(x.flatten(1)))        return self.clf(self.dropout(x))def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):    if not mu:        return criterion    mu *= 0.5    model0 = [param.data.clone() for param in model.parameters()]    def objective(*args, **kwargs):        prox = sum((param-param0).pow(2).sum()                   for param, param0 in zip(model.parameters(), model0))        return criterion(*args, **kwargs) + mu * prox    return objectivedef train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):    optimizer = optim.SGD(model.parameters(), **kwargs)    criterion = get_prox(model, mu=mu)    model.dropout.p = dropout    model.train()    m = len(Y)    for e in range(epochs):        randperm = torch.randperm(m)        X, Y = X[randperm], Y[randperm]        for i in range(0, m, batch):            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]            pred = model(Xbatch)            loss = criterion(pred, Ybatch)            optimizer.zero_grad()            loss.backward()            optimizer.step()    model.eval()    return modeldef get_client(n_clients=1):    if args.lr_only:        return [SIMPLE_CLIENT()]    initial_client = CLIENT()    client_arr = [initial_client]    eps = args.eps    for i in range(n_clients-1):        other_client = deepcopy(initial_client)                log_lr = np.log10(other_client['lr'])        other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)                other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)                log_wd = np.log10(other_client['weight_decay'])        other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)                epochs_range = math.ceil(eps * 4)        other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)        log_batch = int(np.log2(other_client['batch']))        batch_range = math.ceil(eps * 4)        other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)                log_mu = np.log10(other_client['mu'])        other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)                other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)        client_arr.append(other_client)    return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr",
    "Experiment Result": "Datasets: CIFAR-10, FEMNIST, Shakespeare.Specific data settings include: CIFAR-10: 500 clients, 0.2 validation proportion. FEMNIST: 0.1 validation proportion, 0.1 test proportion. Shakespeare: sequence length 80, 0.1 validation proportion, 0.1 test proportion. Both FEMNIST and Shakespeare can be configured for IID client data.Model Architectures: For CIFAR-10 and FEMNIST, a CNN is used with 3 convolutional layers (32, 64, 64 filters respectively) followed by a fully connected layer (1024 to 64 units) and a classification layer (64 to 10 for CIFAR-10, 62 for FEMNIST). For Shakespeare, a CharLSTM model is used with configurable hidden size (default 256) and number of layers (default 2), an embedding layer, and a linear output layer.Hyperparameter Search Spaces (Randomly Sampled):Server-side (from SERVER lambda):    - 'lr': 10.0 ** U(-1.0, 1.0) (learning rate)    - 'momentum': Choice([0.0, 0.9])    - 'step': 1 (learning rate decay interval)    - 'gamma': 1.0 - 10.0 ** U(-4.0, -2.0) (learning rate decay factor)Client-side (from CLIENT lambda, for local training):    - 'lr': 10.0 ** U(-4.0, 0.0) (learning rate)    - 'momentum': U(0.0, 1.0)    - 'weight_decay': 10.0 ** U(-5.0, -1.0)    - 'epochs': Choice(arange(1, 6))    - 'batch': 2 ** Choice(arange(3, 8))    - 'mu': 10.0 ** U(-5.0, 0.0) (proximal term coefficient)    - 'dropout': U(0.0, 0.5)FedEx-specific Settings (controlled by command-line arguments):    - '--configs' (default: 1): Number of hyperparameter configurations to optimize over with FedEx. Can be >0 (direct value), =0 (random from 1 to number of arms), =-1 (number of arms given by wrapper), or < -1 (random from 1 to abs(num_configs)). When `--configs`=1 and `--uniform`/`--random` are false, it behaves like FedAvg.    - '--lr_only' (action='store_true'): Tune only learning rate as a hyperparameter.    - '--eps' (default: 0.0 for CIFAR/FEMNIST, 0.1 for Shakespeare): Multiplicative perturbation to client config; eps=0.0 effectively means FedAvg (no perturbation).    - '--uniform' (action='store_true'): Run FedEx over a product set of single-parameter uniform grids.    - '--random' (action='store_true'): Run FedEx over a product set of single-parameter random grids.    - '--eta0' (default: 0.0): FedEx initial step size; if 0.0, uses FedEx default ('auto' which is sqrt(2*log(size))).    - '--sched' (default: 'aggressive'): FedEx step size schedule ('adaptive', 'aggressive', 'auto', 'constant', 'scale').    - '--cutoff' (default: 0.0): Stop updating FedEx config distribution if entropy falls below this value.    - '--baseline' (default: -1.0): How FedEx computes the baseline. If >=-1.0,<0.0, samples discount factor from [0.0, abs(args.baseline)). If =0.0, uses most recent value. If >0.0,<1.0, uses geometrically discounted mean. If =1.0, uses mean of all values.    - '--diff' (action='store_true'): Use difference between refine and global error as FedEx objective.    - '--stop' (action='store_true'): Stop updating FedEx config distribution after the last elimination round.Wrapper Algorithm Settings:    - '--rounds' (default: 800 for CIFAR/Shakespeare, 50 for FEMNIST): Maximum number of communication rounds (max_resources).    - '--total' (default: 4000 for CIFAR/Shakespeare, 450 for FEMNIST): Total number of communication rounds (total_resources).    - '--rate' (default: 3): Multiplicative elimination rate.    - '--elim' (default: 0): Number of elimination rounds; if 0, runs random search.    - '--eval' (default: 1): Number of evaluation rounds.    - '--discount' (default: 0.0): Discount factor for computing the validation score of an arm.Evaluation Settings:    - '--mle' (action='store_true'): Use MLE config at test time.    - '--loss' (action='store_true'): Use loss instead of error as evaluation metric.    - '--eval_global' (action='store_true'): Use global error as elimination metric instead of refine error.Other settings:    - BATCH (constant in code): 100, used for test function.    - Server batch size (args.batch, default: 10): Number of clients to sample per communication round.    - '--seed' (default: 0): Random seed."
}{
    "Title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
    "Main Contributions": "The paper demonstrates that reshuffling resampling splits for every hyperparameter configuration (HPC) often improves the final model's generalization performance on unseen data. Theoretically, it shows that reshuffling can lead to finding configurations with better overall generalization, especially when the loss surface is flat and noisy, explaining how it affects the asymptotic behavior of the validation loss surface and providing a bound on expected regret. Empirically, controlled simulation studies confirm these theoretical insights, and large-scale, realistic hyperparameter optimization (HPO) experiments demonstrate that reshuffling leads to competitive test performances, drastically improving results for a single train-validation holdout protocol, making it competitive with standard cross-validation while being computationally cheaper.",
    "Methodology": "The methodology comprises theoretical analysis, simulation studies, and benchmark experiments. The theoretical analysis investigates the effect of reshuffling on the empirical loss surface by deriving the limiting distribution of the validation loss, modeling it as a zero-mean Gaussian process and providing a bound on the expected regret. The simulation study uses a univariate quadratic loss surface with a squared exponential kernel for the noise process to systematically vary curvature, noise correlation, and reshuffling extent. Benchmark experiments utilize random search and state-of-the-art Bayesian Optimization (BO) variants (HEBO, SMAC3) to evaluate different resampling strategies (holdout, M-fold CV, M-fold holdout, 5x 5-fold CV) with and without reshuffling.",
    "Experimental Setup": "The theoretical analysis relies on assumptions of loss stability and prediction rule convergence. The simulation study constructs a univariate quadratic loss surface and a squared exponential kernel for the noise process, varying parameters like curvature (m), noise correlation (κ), and reshuffling extent (τ), repeating each run 10,000 times. Benchmark experiments use a subset of small- to medium-sized tabular datasets from the AutoML benchmark (10 DGPs with 10,000-1,000,000 observations). Data is split into an outer test set (5,000 points) and varying train-validation sizes (500, 1000, 5000 points). Learning algorithms include CatBoost, XGBoost, Elastic Net, and a Funnel-shaped MLP, with detailed search spaces provided in the appendix. HPO algorithms are Random Search (500 HPCs) and BO (HEBO, SMAC3, 250 HPCs). Resampling methods are 80/20 holdout, 5-fold CV, 5-fold holdout, and 5x 5-fold CV, each tested with fixed and reshuffled splits. Performance metrics are Accuracy, ROC AUC, and Logloss (for Random Search) and ROC AUC (for BO). Anytime test performance is assessed by retraining the incumbent on all train/validation data and evaluating on the outer test set, averaged over 10 replications. Total compute resources were estimated at ~11.86 CPU years with ~6508.67 kg CO2 emissions.",
    "Limitations": "The theoretical analysis relies on an asymptotic approximation of the empirical loss surface, operating on Gaussian loss surfaces with convenient concentration properties. It also assumes loss stability of learning algorithms, which may fail for highly sensitive losses like logloss, empirically observed to sometimes hurt generalization for small sample sizes. The study focuses on generalization after searching through a fixed, finite set of candidates, thus largely ignoring the dynamic nature of many HPO algorithms. Experimental evaluation is limited to tabular data and binary classification, avoiding extremely small or large datasets. The paper notes that for logloss, reshuffling rarely proved beneficial and could even harm generalization for small sample sizes.",
    "Future Research Directions": "Future research should focus on developing a unified formal definition for 'oversearching,' 'overtuning,' or 'overfitting to the validation set,' and thoroughly analyzing its relation to validation performance measurements. Investigating the use of adaptive cross-validation techniques (e.g., Auto-WEKA, Lazy Paired Hyperparameter Tuning) to reduce computational burden on HPO while maintaining or improving performance is also suggested. Designing more advanced HPO algorithms that actively exploit the reshuffling effect is a promising avenue. Additionally, exploring combinations of reshuffling with existing strategies to counteract overfitting (e.g., LOOCVCV, extra selection sets, early stopping) could lead to further improvements. The impact of reshuffling on multi-class datasets and developing less naive implementations to address issues with highly sensitive losses like logloss are also areas for further study.",
    "Experiment Code": "File Path: simulations/simulate.py\nContent:\nfrom functools import partial\n\nimport numpy as np\nimport torch\n\ndef simulate_gp(x: torch.Tensor, mu: callable, cov: callable) -> torch.Tensor:\n    \"\"\"\n    Simulate a Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param mu: Function to compute the mean of the Gaussian process.\n    :param cov: Function to compute the covariance matrix of the Gaussian process.\n\n    :return: A tensor of shape (n, d) representing the simulated Gaussian process.\n    \"\"\"\n    sigma_2 = 1e-5\n    n = x.shape[0]\n    d = x.shape[1]\n    mu_x = mu(x)\n    K_x = cov(x, x)\n    K_x += sigma_2 * torch.eye(n)\n    eigenvalues, eigenvectors = torch.linalg.eigh(K_x)\n    positive_eigenvalues = torch.clamp(eigenvalues, min=0)\n    sqrt_K_x = eigenvectors @ torch.diag(positive_eigenvalues.sqrt()) @ eigenvectors.T\n    z = torch.normal(0, 1, (n, d))\n    return mu_x + sqrt_K_x @ z\n\n\ndef mu_factory(x: torch.Tensor, alpha: float) -> torch.Tensor:\n    \"\"\"\n    The mean function of the Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param alpha: The alpha parameter of the mean function.\n\n    :return: A tensor of shape (n, 1) representing the mean of the Gaussian process at each point.\n    \"\"\"\n    values = alpha * (x - 0.5).pow(2)\n    return values\n\n\ndef cov_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Vectorized computation of the covariance matrix of the Gaussian process.\n\n    :param x1: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param x2: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param lengthscale: The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A tensor of shape (n, n) representing the covariance matrix of the Gaussian process.\n    \"\"\"\n    sq_dist = torch.sum((x1[:, None, :] - x2[None, :, :]) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K\n    return K\n\n\ndef kernel_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Computation of the covariance between two points.\n\n    :param x1: A tensor of shape (1, d) where d is the dimensionality.\n    :param x2: A tensor of shape (1, d) where d is the dimensionality.\n    :param lengthscale:  The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A scalar tensor representing the covariance between the two points.\n    \"\"\"\n    sq_dist = torch.sum((x1 - x2) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        if x1 == x2:\n            K = (tau**2) * K\n    return K\n\n\nif __name__ == \"__main__\":\n    import argparse\n    import os\n\n    import pandas as pd\n    from tqdm import tqdm\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--alpha\", type=float, default=1)\n    parser.add_argument(\"--lengthscale\", type=float, default=0.5)\n    args = parser.parse_args()\n\n    np.random.seed(42)\n    torch.manual_seed(42)\n\n    results_path = \"results\"\n    d = 1\n    alpha = args.alpha\n    lengthscale = args.lengthscale\n    n_replicates = 10000\n\n    x = torch.linspace(0, 1, 101).reshape(-1, d)\n    mu = partial(mu_factory, alpha=alpha)\n    kernel = partial(kernel_factory, lengthscale=lengthscale)\n    cov = partial(cov_factory, lengthscale=lengthscale)\n    taus = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n    taus.sort()\n    results = {tau: {\"y\": [], \"y_shuffled\": [], \"y_diff\": []} for tau in taus}\n    for tau in taus:\n        y_mu_y_list = []\n        y_mu_y_shuffled_list = []\n        y_diff_list = []\n        for _ in tqdm(range(n_replicates)):\n            y = simulate_gp(x, mu, cov)\n            y_shuffled = simulate_gp(x, mu, partial(cov, shuffled=True, tau=tau))\n            y_mu_y = mu(x[y.argmin(dim=0)])\n            y_mu_y_shuffled = mu(x[y_shuffled.argmin(dim=0)])\n            y_mu_y_list.append(y_mu_y)\n            y_mu_y_shuffled_list.append(y_mu_y_shuffled)\n            y_diff_list.append(y_mu_y - y_mu_y_shuffled)\n        results[tau][\"y\"] = torch.tensor(y_mu_y_list)\n        results[tau][\"y_shuffled\"] = torch.tensor(y_mu_y_shuffled_list)\n        results[tau][\"y_diff\"] = torch.tensor(y_diff_list)\n\n    results_df = pd.DataFrame(results[taus[0]])\n    results_df[\"replicate\"] = range(n_replicates)\n    results_df[\"tau\"] = taus[0]\n    for tau in taus[1:]:\n        results_df_tmp = pd.DataFrame(results[tau])\n        results_df_tmp[\"replicate\"] = range(n_replicates)\n        results_df_tmp[\"tau\"] = tau\n        results_df = pd.concat([results_df, results_df_tmp], axis=0)\n    results_df[\"alpha\"] = alpha\n    results_df[\"lengthscale\"] = lengthscale\n    results_df[\"scenario\"] = str(alpha) + \"_\" + str(lengthscale)\n    results_df = results_df.reset_index(drop=True)\n    results_df.to_csv(\n        os.path.join(\n            results_path,\n            f\"results_alpha_{alpha}_lengthscale_{lengthscale}.csv\",\n        ),\n        index=False,\n    )\nFile Path: analyze/result_analyzer.py\nContent:\n    def calculate_curvature(self) -> None:\n        \"\"\"\n        Fit a GP on observed values and calculate some curvature metrics at the empirical optimum.\n        \"\"\"\n        for metric in self.params[\"metrics\"]:\n            dat = self.results_raw[metric]\n            relevant_columns_valid = [\n                column for column in dat.columns if \"params_\" in column\n            ] + [\"valid\"]\n            dat_valid = dat.loc[:, relevant_columns_valid]\n            dat_valid.rename(columns={\"valid\": \"y\"}, inplace=True)\n            X = dat_valid.drop(columns=[\"y\"])\n            y = dat_valid[\"y\"].values.reshape(-1, 1)\n            X.rename(\n                columns={column: column.replace(\"params_\", \"\") for column in X.columns},\n                inplace=True,\n            )\n            if self.params[\"classifier\"] == \"logreg\":\n                classifier = LogReg(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            elif self.params[\"classifier\"] == \"funnel_mlp\":\n                classifier = FunnelMLP(self.params[\"seed\"])\n                n_train_samples = int(\n                    0.8 * self.params[\"train_valid_size\"]\n                )  # Holdout 80/20 or 5-fold CV variants\n                space = classifier.get_hebo_search_space(\n                    n_train_samples=n_train_samples\n                )\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub)\n                    for name in space.paras\n                    if not space.paras[name].is_categorical\n                ]\n            elif self.params[\"classifier\"] == \"xgboost\":\n                classifier = XGBoost(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            elif self.params[\"classifier\"] == \"catboost\":\n                classifier = CatBoost(self.params[\"seed\"])\n                space = classifier.get_hebo_search_space()\n                bounds = [\n                    (space.paras[name].lb, space.paras[name].ub) for name in space.paras\n                ]\n            # https://github.com/huawei-noah/HEBO/blob/c1c7d72b996a7d11eb2b86e25f21a174b0cc7bd4/HEBO/hebo/optimizers/hebo.py#L117\n            X, Xe = space.transform(X)\n            model_config = {\n                \"lr\": 0.01,\n                \"num_epochs\": 100,\n                \"verbose\": False,\n                \"noise_lb\": 8e-4,\n                \"pred_likeli\": False,\n            }\n            if space.num_categorical > 0:\n                model_config[\"num_uniqs\"] = [\n                    len(space.paras[name].categories) for name in space.enum_names\n                ]\n            # try:\n            #    if y.min() <= 0:\n            #        y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))\n            #    else:\n            #        y = torch.FloatTensor(power_transform(y / y.std(), method='box-cox'))\n            #        if y.std() < 0.5:\n            #            y = torch.FloatTensor(power_transform(y / y.std(), method='yeo-johnson'))\n            #    if y.std() < 0.5:\n            #        raise RuntimeError('Power transformation failed')\n            #    model = get_model(\"gp\", space.num_numeric, space.num_categorical, 1, **model_config)\n            #    model.fit(X, Xe, y)\n            # except:\n            y = torch.FloatTensor(y).clone()\n            model = get_model(\n                \"gp\", space.num_numeric, space.num_categorical, 1, **model_config\n            )\n            model.fit(X, Xe, y)\n\n            empirical_argmin = model.predict(X, Xe)[0].argmin()\n            X_argmin = X[empirical_argmin, :].unsqueeze(0)\n            Xe_argmin = Xe[empirical_argmin, :].unsqueeze(0)\n\n            def posterior_mean_wrapper(x, model, Xe_argmin):\n                x_tensor = torch.FloatTensor(x).unsqueeze(0).requires_grad_(True)\n                return model.predict(x_tensor, Xe_argmin)[0][0, 0].detach().numpy()\n\n            x0 = X[empirical_argmin, :].numpy()\n            result = opt.minimize(\n                posterior_mean_wrapper,\n                x0,\n                args=(model, Xe_argmin),\n                bounds=bounds,\n                method=\"Nelder-Mead\",\n            )\n\n            x_optimal = result.x\n            hessian_function = numdifftools.Hessian(posterior_mean_wrapper)\n            hessian_optimal = hessian_function(x_optimal, model, Xe_argmin)\n\n            def make_psd(matrix):\n                eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n                already_is_psd = np.all(eigenvalues >= 0)\n                eigenvalues[eigenvalues < 0] = 0\n                return (\n                    already_is_psd,\n                    eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T,\n                )\n\n            already_is_psd, hessian_optimal = make_psd(hessian_optimal)\n\n            det_hessian = np.linalg.det(hessian_optimal)\n            trace_hessian = np.trace(hessian_optimal)\n            eigenvalues_hessian = np.linalg.eigvals(hessian_optimal)\n            smallest_eigenvalue_hessian = np.min(eigenvalues_hessian)\n            biggest_eigenvalue_hessian = np.max(eigenvalues_hessian)\n\n            curvature_pd = pd.DataFrame(\n                {\n                    \"det_hessian\": [det_hessian],\n                    \"trace_hessian\": [trace_hessian],\n                    \"smallest_eigenvalue_hessian\": [smallest_eigenvalue_hessian],\n                    \"biggest_eigenvalue_hessian\": [biggest_eigenvalue_hessian],\n                    \"already_is_psd\": [already_is_psd],\n                    \"gp_noise\": [\n                        model.noise.item()\n                    ],  # homoscedastic observation variance (noise) from the GP\n                    \"seed\": [self.seed],\n                    \"classifier\": [self.params[\"classifier\"]],\n                    \"data_id\": [self.params[\"data_id\"]],\n                    \"train_valid_size\": [self.params[\"train_valid_size\"]],\n                    \"resampling\": [self.params[\"resampling\"]],\n                    \"metric\": [metric],\n                }\n            )\n            self.curvature.update({metric: curvature_pd})\nFile Path: reshufflebench/learner/learner_random_cv.py\nContent:\nclass LearnerRandomCV(LearnerRandom):\n    # ... (init and prepare_resampling methods)\n\n    def objective(self, trial: Trial) -> float:\n        \"\"\"\n        Objective function for the optimization.\n        \"\"\"\n        # construct classifier pipeline\n        self.classifier.construct_pipeline(\n            trial,\n            refit=False,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n            n_train_samples=self.train_size,\n        )\n\n        if self.reshuffle:\n            self.cv = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits = []\n            for cv in self.cv:\n                self.cv_splits.append(\n                    list(cv.split(self.x_valid_train, self.y_valid_train))\n                )\n\n            # partition add_valid_use data into n_splits folds and repeat it n_repeats times\n            self.cv_add_valid = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits_add_valid = []\n            for cv in self.cv_add_valid:\n                self.cv_splits_add_valid.append(\n                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))\n                )\n\n        cv_splits_hist_train_tmp = []\n        cv_splits_hist_valid_tmp = []\n        cv_splits_add_valid_hist_valid_tmp = []\n        y_train_hist_tmp = []\n        y_valid_hist_tmp = []\n        y_add_valid_hist_tmp = []\n\n        # for each repeat and each fold fit the classifier and predict on the train, valid, test and add_valid set and compute the metrics\n        predictions = dict(\n            [(data, []) for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]]\n        )\n        predictions_proba = dict(\n            [(data, []) for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]]\n        )\n        cv_metrics_train = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_valid = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_add_valid = dict([(metric, []) for metric in self.metrics])\n        cv_metrics_test = dict([(metric, []) for metric in self.metrics])\n\n        for repeat in range(self.n_repeats):\n            for fold in range(self.n_splits):\n                train_index, valid_index = self.cv_splits[repeat][fold]\n                cv_splits_hist_train_tmp.append(train_index)\n                cv_splits_hist_valid_tmp.append(valid_index)\n                x_train = self.x_valid_train.iloc[train_index]\n                x_valid = self.x_valid_train.iloc[valid_index]\n                y_train = self.y_valid_train[train_index]\n                y_valid = self.y_valid_train[valid_index]\n                y_train_hist_tmp.append(y_train)\n                y_valid_hist_tmp.append(y_valid)\n\n                add_valid_index = self.cv_splits_add_valid[repeat][fold][1]\n                cv_splits_add_valid_hist_valid_tmp.append(\n                    np.concatenate([valid_index, add_valid_index])\n                )\n                x_add_valid, y_add_valid = construct_x_and_y_add_valid(\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    x_add_valid=self.x_add_valid_use.iloc[add_valid_index],\n                    y_add_valid=self.y_add_valid_use[add_valid_index],\n                )\n                y_add_valid_hist_tmp.append(y_add_valid)\n\n                self.classifier.fit(\n                    trial=trial,\n                    x_train=x_train,\n                    y_train=y_train,\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    cat_features=self.cat_features,\n                )\n                for data in [\"x_train\", \"x_valid\", \"x_add_valid\", \"x_test\"]:\n                    if data == \"x_test\":\n                        predictions[data].append(\n                            self.classifier.predict(getattr(self, data))\n                        )\n                        predictions_proba[data].append(\n                            self.classifier.predict_proba(getattr(self, data))\n                        )\n                    else:\n                        predictions[data].append(self.classifier.predict(eval(data)))\n                        predictions_proba[data].append(\n                            self.classifier.predict_proba(eval(data))\n                        )\n                for metric in self.metrics:\n                    cv_metrics_train[metric].append(\n                        compute_metric(\n                            y_train,\n                            y_pred=predictions[\"x_train\"][-1],\n                            y_pred_proba=predictions_proba[\"x_train\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_valid[metric].append(\n                        compute_metric(\n                            y_valid,\n                            y_pred=predictions[\"x_valid\"][-1],\n                            y_pred_proba=predictions_proba[\"x_valid\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_add_valid[metric].append(\n                        compute_metric(\n                            y_add_valid,\n                            y_pred=predictions[\"x_add_valid\"][-1],\n                            y_pred_proba=predictions_proba[\"x_add_valid\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n                    cv_metrics_test[metric].append(\n                        compute_metric(\n                            self.y_test,\n                            y_pred=predictions[\"x_test\"][-1],\n                            y_pred_proba=predictions_proba[\"x_test\"][-1],\n                            metric=metric,\n                            labels=self.labels,\n                            multiclass=self.multiclass,\n                        )\n                    )\n\n        self.cv_splits_hist_train.append(cv_splits_hist_train_tmp)\n        self.cv_splits_hist_valid.append(cv_splits_hist_valid_tmp)\n        self.cv_splits_add_valid_hist_valid.append(cv_splits_add_valid_hist_valid_tmp)\n        self.y_train_hist.append(y_train_hist_tmp)\n        self.y_valid_hist.append(y_valid_hist_tmp)\n        self.y_add_valid_hist.append(y_add_valid_hist_tmp)\n        self.y_pred_train_proba_hist.append(predictions_proba[\"x_train\"])\n        self.y_pred_valid_proba_hist.append(predictions_proba[\"x_valid\"])\n        self.y_pred_add_valid_proba_hist.append(predictions_proba[\"x_add_valid\"])\n        self.y_pred_test_proba_hist.append(predictions_proba[\"x_test\"])\n\n        for cv_metric in self.cv_metric_to_metric.keys():\n            metric = self.cv_metric_to_metric[cv_metric]\n            trial.set_user_attr(\n                f\"{cv_metric}_train\",\n                json.dumps(cv_metrics_train[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_valid\",\n                json.dumps(cv_metrics_valid[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_add_valid\",\n                json.dumps(cv_metrics_add_valid[metric], cls=NumpyArrayEncoder),\n            )\n            trial.set_user_attr(\n                f\"{cv_metric}_test\",\n                json.dumps(cv_metrics_test[metric], cls=NumpyArrayEncoder),\n            )\n\n        # compute the mean of the metrics over the folds and repeats\n        metrics_train = {}\n        metrics_valid = {}\n        metrics_add_valid = {}\n        metrics_test = {}\n\n        for metric in self.metrics:\n            metrics_train[metric] = np.mean(cv_metrics_train[metric])\n            metrics_valid[metric] = np.mean(cv_metrics_valid[metric])\n            metrics_add_valid[metric] = np.mean(cv_metrics_add_valid[metric])\n            metrics_test[metric] = np.mean(cv_metrics_test[metric])\n            trial.set_user_attr(f\"{metric}_train\", metrics_train[metric])\n            trial.set_user_attr(f\"{metric}_valid\", metrics_valid[metric])\n            trial.set_user_attr(f\"{metric}_add_valid\", metrics_add_valid[metric])\n            trial.set_user_attr(f\"{metric}_test\", metrics_test[metric])\n\n        # compute the metrics on the test set also in ensemble style\n        metrics_test_ensemble = {}\n        predictions_proba_test_ensemble = np.mean(predictions_proba[\"x_test\"], axis=0)\n        row_sums = predictions_proba_test_ensemble.sum(axis=1, keepdims=True)\n        predictions_proba_test_ensemble = predictions_proba_test_ensemble / row_sums\n        check_y_predict_proba(predictions_proba_test_ensemble)\n        predictions_test_ensemble = np.argmax(predictions_proba_test_ensemble, axis=1)\n        for metric in self.metrics:\n            metrics_test_ensemble[metric] = compute_metric(\n                self.y_test,\n                y_pred=predictions_test_ensemble,\n                y_pred_proba=predictions_proba_test_ensemble,\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(\n                f\"{metric}_test_ensemble\", metrics_test_ensemble[metric]\n            )\n\n        if self.bootstrap_test:\n            # bootstrap the ensemble style test performance\n            for metric in self.metrics:\n                metric_test_bootstrap = bootstrap_test_performance(\n                    y_test=self.y_test,\n                    y_pred=predictions_test_ensemble,\n                    y_pred_proba=predictions_proba_test_ensemble,\n                    metric=metric,\n                    labels=self.labels,\n                    multiclass=self.multiclass,\n                    seed=self.seed,\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_ensemble_bootstrap\", metric_test_bootstrap\n                )\n                average_metric_test_bootstrap = sum(metric_test_bootstrap) / len(\n                    metric_test_bootstrap\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_ensemble_bootstrap_average\",\n                    average_metric_test_bootstrap,\n                )\n\n        # refit on the train_valid set\n        self.classifier.construct_pipeline(\n            trial,\n            refit=True,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n        )\n        self.classifier.fit(\n            trial=trial,\n            x_train=self.x_valid_train,\n            y_train=self.y_valid_train,\n            cat_features=self.cat_features,\n        )\n\n        # predict on the train_valid set and compute the metrics\n        predictions[\"x_valid_train\"] = self.classifier.predict(self.x_valid_train)\n        predictions_proba[\"x_valid_train\"] = self.classifier.predict_proba(\n            self.x_valid_train\n        )\n        metrics_valid_train = {}\n        for metric in self.metrics:\n            metrics_valid_train[metric] = compute_metric(\n                self.y_valid_train,\n                y_pred=predictions[\"x_valid_train\"],\n                y_pred_proba=predictions_proba[\"x_valid_train\"],\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(f\"{metric}_valid_train\", metrics_valid_train[metric])\n\n        self.y_pred_valid_train_proba_hist.append(predictions_proba[\"x_valid_train\"])\n\n        # predict on the test set and compute the metrics\n        predictions[\"x_test_retrained\"] = self.classifier.predict(self.x_test)\n        predictions_proba[\"x_test_retrained\"] = self.classifier.predict_proba(\n            self.x_test\n        )\n        metrics_test_retrained = {}\n        for metric in self.metrics:\n            metrics_test_retrained[metric] = compute_metric(\n                self.y_test,\n                y_pred=predictions[\"x_test_retrained\"],\n                y_pred_proba=predictions_proba[\"x_test_retrained\"],\n                metric=metric,\n                labels=self.labels,\n                multiclass=self.multiclass,\n            )\n            trial.set_user_attr(\n                f\"{metric}_test_retrained\", metrics_test_retrained[metric]\n            )\n\n        if self.bootstrap_test:\n            # bootstrap the retrained test performance\n            for metric in self.metrics:\n                metric_test_retrained_bootstrap = bootstrap_test_performance(\n                    y_test=self.y_test,\n                    y_pred=predictions[\"x_test_retrained\"],\n                    y_pred_proba=predictions_proba[\"x_test_retrained\"],\n                    metric=metric,\n                    labels=self.labels,\n                    multiclass=self.multiclass,\n                    seed=self.seed,\n                )\n                trial.set_user_attr(\n                    f\"{metric}_test_retrained_bootstrap\",\n                    metric_test_retrained_bootstrap,\n                )\n                average_metric_test_retrained_bootstrap = sum(\n                    metric_test_retrained_bootstrap\n                ) / len(metric_test_retrained_bootstrap)\n                trial.set_user_attr(\n                    f\"{metric}_test_retrained_bootstrap_average\",\n                    average_metric_test_retrained_bootstrap,\n                )\n\n        self.y_pred_test_proba_retrained_hist.append(\n            predictions_proba[\"x_test_retrained\"]\n        )\n\n        self.classifier.reset()\n\n        # return the validation accuracy (mean over folds and repeats)\n        return metrics_valid[\"accuracy\"]\nFile Path: main.py\nContent:\nif __name__ == \"__main__\":\n    import argparse\n\n    from reshufflebench.algorithms import (\n        CatBoost,\n        Featureless,\n        FunnelMLP,\n        LogReg,\n        TabPFN,\n        XGBoost,\n        XGBoostLarge,\n    )\n    from reshufflebench.learner import (\n        LearnerHeboCV,\n        LearnerHeboHoldout,\n        LearnerHeboRepeatedHoldout,\n        LearnerRandomCV,\n        LearnerRandomHoldout,\n        LearnerRandomRepeatedHoldout,\n        LearnerSmacCV,\n        LearnerSmacHoldout,\n        LearnerSmacRepeatedHoldout,\n    )\n    from reshufflebench.utils import str2bool\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--classifier\",\n        type=str,\n        default=\"catboost\",\n        choices=[\n            \"catboost\",\n            \"funnel_mlp\",\n            \"logreg\",\n            \"tabpfn\",\n            \"xgboost\",\n            \"xgboost_large\",\n            \"featureless\",\n        ],\n    )\n    parser.add_argument(\"--default\", type=str2bool, default=False)\n    parser.add_argument(\n        \"--optimizer\",\n        type=str,\n        default=\"random\",\n        choices=[\"random\", \"hebo\", \"smac\"],\n    )\n    parser.add_argument(\n        \"--data_id\",\n        type=int,\n        default=11111,\n        choices=[\n            23517,\n            1169,\n            41147,\n            4135,\n            1461,\n            1590,\n            41150,\n            41162,\n            42733,\n            42742,\n            99999,\n            11111,\n        ],\n    )\n    parser.add_argument(\n        \"--valid_type\",\n        type=str,\n        default=\"holdout\",\n        choices=[\"cv\", \"holdout\", \"repeatedholdout\"],\n    )\n    parser.add_argument(\n        \"--train_valid_size\",\n        type=int,\n        default=500,\n        choices=[500, 1000, 5000],\n    )\n    parser.add_argument(\"--reshuffle\", type=str2bool, default=True)\n    parser.add_argument(\"--n_splits\", type=int, default=5, choices=[5])\n    # n_repeats = 1 or 5 for cv, 1 for holdout and 5 for repeatedholdout\n    parser.add_argument(\"--n_repeats\", type=int, default=1, choices=[1, 5])\n    parser.add_argument(\"--valid_frac\", type=float, default=0.2, choices=[0.2])\n    parser.add_argument(\"--test_size\", type=int, default=5000)\n    parser.add_argument(\"--add_valid_size\", type=int, default=5000)\n    parser.add_argument(\"--n_trials\", type=int, default=10)\n    parser.add_argument(\"--seed\", type=int, default=42)\n\n    args = parser.parse_args()\n\n    classifiers = {\n        \"catboost\": CatBoost(seed=args.seed, default=args.default),\n        \"funnel_mlp\": FunnelMLP(seed=args.seed, default=args.default),\n        \"logreg\": LogReg(seed=args.seed, default=args.default),\n        \"tabpfn\": TabPFN(seed=args.seed),\n        \"xgboost\": XGBoost(seed=args.seed, default=args.default),\n        \"xgboost_large\": XGBoostLarge(seed=args.seed, default=args.default),\n        \"featureless\": Featureless(seed=args.seed),\n    }\n    classifier = classifiers[args.classifier]\n\n    if args.n_trials > 500:\n        raise ValueError(\n            \"n_trials must be <= 500 - or you must adjust seeds in codebase\"\n        )\n\n    if args.optimizer == \"random\":\n        if args.valid_type == \"cv\":\n            learner = LearnerRandomCV(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerRandomHoldout(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerRandomRepeatedHoldout(\n                classifier=classifier,\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    elif args.optimizer == \"hebo\":\n        if args.valid_type == \"cv\":\n            learner = LearnerHeboCV(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerHeboHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerHeboRepeatedHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    elif args.optimizer == \"smac\":\n        if args.valid_type == \"cv\":\n            learner = LearnerSmacCV(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                n_splits=args.n_splits,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        elif args.valid_type == \"holdout\":\n            learner = LearnerSmacHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n        else:\n            learner = LearnerSmacRepeatedHoldout(\n                classifier=classifier,\n                metric=\"auc\",\n                data_id=args.data_id,\n                train_valid_size=args.train_valid_size,\n                reshuffle=args.reshuffle,\n                valid_frac=args.valid_frac,\n                n_repeats=args.n_repeats,\n                test_size=args.test_size,\n                add_valid_size=args.add_valid_size,\n                n_trials=args.n_trials,\n                seed=args.seed,\n            )\n    else:\n        raise ValueError(\"Invalid optimizer\")\n    learner.run()\n",
    "Experiment Result": "Simulation Study:\n- `d` (dimensionality of each point): 1\n- `alpha` (mean function parameter for quadratic loss surface): [0.5, 1, 5, 10]\n- `lengthscale` (covariance function parameter for squared exponential kernel): [0.1, 0.5, 1, 5]\n- `n_replicates`: 10000\n- `taus` (parameter for shuffled covariance function, representing reshuffling extent/noise correlation): [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\nBenchmark Experiments:\n- Optimizers: Random Search, HEBO (Bayesian Optimization), SMAC3 (Bayesian Optimization)\n- Classifiers (methods evaluated): CatBoost, FunnelMLP, LogReg (Logistic Regression), TabPFN, XGBoost, XGBoostLarge, Featureless (baseline)\n- Resampling Strategies (`valid_type`):\n    - Holdout: `valid_type=\"holdout\"`, `valid_frac=0.2`\n    - M-fold CV: `valid_type=\"cv\"`, `n_splits=5`, `n_repeats=1`\n    - M-fold Holdout (repeated holdout / Monte Carlo CV): `valid_type=\"repeatedholdout\"`, `valid_frac=0.2`, `n_repeats=5`\n    - 5x 5-fold CV: `valid_type=\"cv\"`, `n_splits=5`, `n_repeats=5`\n- Reshuffling (`reshuffle`): True, False\n- Number of trials (`n_trials`):\n    - 250 for HEBO and SMAC\n    - 500 for Random Search (unless `classifier` is 'tabpfn' or 'default', then 1 trial)\n- Seeds: 42 to 51 (range(42, 52))\n- Data IDs (OpenML datasets): [23517, 1169, 41147, 4135, 1461, 1590, 41150, 41162, 42733, 42742] (plus synthetic 99999, 11111 for testing)\n- Training/Validation set sizes (`train_valid_size`): [500, 1000, 5000]\n- Test set size (`test_size`): 5000\n- Additional validation set size (`add_valid_size`): 5000\n- Metrics:\n    - For Random Search: Accuracy, Balanced Accuracy, Logloss, AUC (Area Under the Curve)\n    - For HEBO and SMAC: AUC\n"
}{
    "Title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
    "Main Contributions": "The paper introduces DP-HyPO, a pioneering adaptive private hyperparameter optimization (HPO) framework that bridges the gap between non-private and private HPO. DP-HyPO enables the flexible use of non-DP adaptive HPO methods, such as Gaussian process-based optimization, for enhanced efficiency while avoiding the substantial privacy costs typically associated with composition. The framework provides sharp differential privacy (DP) guarantees by utilizing the Rényi DP framework, offering a strict generalization of prior uniform sampling results without stability assumptions. Empirically, the Gaussian process-based DP-HyPO algorithm is shown to outperform its uniform counterpart across various practical scenarios, allowing practitioners to integrate any non-private adaptive HPO method and manage privacy budget allocation for adaptivity.",
    "Methodology": "DP-HyPO is an iterative framework that starts with a prior distribution over the hyperparameter space. For a random number of repetitions (e.g., drawn from a truncated negative binomial distribution), a hyperparameter is sampled from an adaptively updated distribution. A key mechanism is a projection technique that privatizes non-private HPO algorithms: it maintains an adaptive sampling distribution (posterior) that is projected onto a convex space (SC,c) of essentially bounded density functions before sampling. This ensures that the ratio of the posterior density to the prior density stays within bounds [c, C], controlling privacy leakage. The framework is instantiated using Gaussian processes (GPs), where GPs build a surrogate model for performance measures, generate scores (e.g., estimated upper confidence bound), and these scores are converted into a sampling distribution via a softmax function before being projected to meet DP constraints. The privacy guarantees are derived using Rényi Differential Privacy (RDP) analysis.",
    "Experimental Setup": "The DP-HyPO framework, specifically its Gaussian process (GP) instantiation, is compared against a Uniform DP-HyPO method (a non-adaptive baseline from prior work) in two privacy configurations: white-box and black-box settings. In the white-box setting, experiments were conducted on the MNIST and CIFAR-10 datasets, training standard CNNs with DP-SGD. For MNIST, a semi-real simulation was performed by caching mean accuracies for discretized hyperparameters and adding Gaussian noise upon sampling. For CIFAR-10, hyperparameter landscapes were generated using BoTorch. Hyperparameters optimized included learning rate and clipping norm across predefined log-spaced and linear-spaced grids. In the black-box setting, a real-world Federated Learning (FL) task on a proprietary dataset was used, where optimal learning rates for the central server (AdaGrad) and users (SGD) were determined using a landscape generated by BoTorch. Performance was evaluated based on accuracy (MNIST, CIFAR-10) or loss (FL) of the output hyperparameter, aggregated over geometric distributions for the total number of runs.",
    "Limitations": "The framework requires that the adaptive sampling distributions, when updated, must have their densities bounded by specific constants (c and C) relative to the prior distribution, which necessitates modifications to standard non-private HPO methods. Practically, the continuous hyperparameter space must be discretized to make the convex functional projection computationally feasible, serving as an approximation. The empirical evaluations on MNIST and CIFAR-10, due to computational resource constraints, relied on a semi-real simulation or pre-generated hyperparameter landscapes, rather than full end-to-end training for every hyperparameter combination. Additionally, the paper notes that the hyperparameter landscape of MNIST is relatively uncomplicated, which might limit the observed benefits of adaptive algorithms compared to more complex datasets.",
    "Future Research Directions": "Future work could involve exploring alternative, more practically favorable HPO specifications by leveraging advanced HPO methods to further improve empirical performance. Another direction is to establish theoretical utility guarantees for the general DP-HyPO framework or for specific configurations within it, potentially by adapting proof methodologies from existing literature like Theorem 3.3 in [26].",
    "Experiment Code": null,
    "Experiment Result": null
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'Existing gray-box / multi-fidelity HPO algorithms (ASHA, PASHA, DyHPO, BOIL, FastBO, DyHPO) treat every new trial as a black box and rely only on early learning-curve observations. Implicit-gradient papers (Lasso, DNN-MFBO, hypernetwork approaches) compute exact or approximate hyper-gradients, but they optimise one configuration at a time and require unrolling or bilevel solvers.  OHGW is the first method that: (1) harvests a single stochastic hyper-gradient that is already produced inside the very first mini-batch, (2) uses it only for a one-step warm-start, and (3) leaves the promotion / pruning logic of bandit schedulers completely unchanged.  This simple “one-shot hyper-nudging” of every candidate before the race begins has not been explored in any of the listed related works or mainstream HPO literature, making the idea genuinely new.', 'novelty_score': 8, 'significance_reason': 'OHGW delivers a median 20 % reduction in time-to-target-accuracy across three strong schedulers (ASHA 11.2→9.0 h, PASHA 7.3→5.8 h, DyHPO 6.1→4.9 h) while adding <3 % extra compute and no additional model evaluations. Because it is a one-line wrapper around trial creation and needs no change to the scheduler or searcher, the technique can be adopted immediately in existing HPO stacks and scales with the same wall-clock efficiency as the host scheduler.  Although the absolute gains are modest relative to bigger algorithmic overhauls, the cost-benefit ratio (≈7× speed-up per % extra compute) and universality across schedulers give the method practical impact for anyone already running gray-box HPO at scale.', 'significance_score': 7}
