
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
BOIL models the cost-sensitive objective function f(x,t) using a Gaussian Process (GP) with a product kernel over hyperparameters x and training iterations t. The training time cost c(x,t) is approximated by a linear regressor. The algorithm selects the next evaluation point by maximizing a cost-aware acquisition function (a modified Expected Improvement criterion) that balances utility, exploration, and computational cost. Learning curves are compressed into a numeric score using a Sigmoid preference function, whose growth and middle point parameters (g0, m0) are learned by maximizing the GP's log marginal likelihood. To augment data efficiently and avoid GP covariance matrix ill-conditioning, a subset of points from the observed learning curve is actively selected at regions of maximum GP predictive uncertainty, with the number of augmented points adaptively controlled by a condition number threshold.

# Repository Content
File Path: bayes_opt/__init__.py
Content:



from .gaussian_process import GaussianProcess
from .acquisition_functions import AcquisitionFunction
from .product_gaussian_process import ProductGaussianProcess
#from .product_gaussian_process_freeze_thaw import ProductGaussianProcess_FreezeThaw


__all__ = ["AcquisitionFunction","GaussianProcess","ProductGaussianProcess"]
File Path: bayes_opt/acquisition_functions.py
Content:
import numpy as np
from scipy.stats import norm


counter = 0


class AcquisitionFunction(object):
    """
    An object to compute the acquisition functions.
    """

    def __init__(self, acq):

        self.acq=acq
        acq_name=acq['name']
        
        if 'mu_max' in acq:
            self.mu_max=acq['mu_max'] # this is for ei_mu acquisition function
        
        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',
                 'pure_exploration','mu','lcb','ei_mu_max'                          ]
        
        # check valid acquisition function
        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]
        #if  not in acq_name:
        if  IsTrue == []:
            err = "The utility function " \
                  "{} has not been implemented, " \
                  "please choose one of ucb, ei, or poi.".format(acq_name)
            raise NotImplementedError(err)
        else:
            self.acq_name = acq_name
            
        self.dim=acq['dim']
        
        if 'scalebounds' not in acq:
            self.scalebounds=[0,1]*self.dim
            
        else:
            self.scalebounds=acq['scalebounds']
               

    def acq_kind(self, x, gp):
        
        #if type(meta) is dict and 'y_max' in meta.keys():
        #   y_max=meta['y_max']
        y_max=np.max(gp.Y)
        #print self.kind
        if np.any(np.isnan(x)):
            return 0
       
        if self.acq_name == 'ucb':
            return self._ucb(x, gp)
        if self.acq_name == 'lcb':
            return self._lcb(x, gp)
        if self.acq_name == 'ei':
            return self._ei(x, gp, y_max)
        if self.acq_name == 'ei_mu_max': # using max mu(x) as incumbent
            return self._ei(x, gp, self.mu_max)
        if self.acq_name == 'poi':
            return self._poi(x, gp, y_max)
        
        if self.acq_name == 'pure_exploration':
            return self._pure_exploration(x, gp) 
      
        if self.acq_name == 'mu':
            return self._mu(x, gp)
        
        if self.acq_name == 'ucb_pe':
            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])
       
            
    def utility_plot(self, x, gp, y_max):
        if np.any(np.isnan(x)):
            return 0
        if self.acq_name == 'ei':
            return self._ei_plot(x, gp, y_max)
  
   
    @staticmethod
    def _mu(x, gp):
        mean, var = gp.predict(x, eval_MSE=True)
        mean=np.atleast_2d(mean).T
        return mean
                
    @staticmethod
    def _lcb(x, gp):
        mean, var = gp.predict(x, eval_MSE=True)
        var.flags['WRITEABLE']=True
        #var=var.copy()
        var[var<1e-10]=0
        mean=np.atleast_2d(mean).T
        var=np.atleast_2d(var).T
        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))
        beta_t = 2 * np.log(len(gp.Y));

        return mean - np.sqrt(beta_t) * np.sqrt(var) 
        
    
    @staticmethod
    def _ucb(x, gp):
        mean, var = gp.predict(x, eval_MSE=True)
        var.flags['WRITEABLE']=True
        #var=var.copy()
        var[var<1e-10]=0
        mean=np.atleast_2d(mean).T
        var=np.atleast_2d(var).T                
        
        # Linear in D, log in t https://github.com/kirthevasank/add-gp-bandits/blob/master/BOLibkky/getUCBUtility.m
        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))
        beta_t = 2 * np.log(len(gp.Y));
  
        #beta=300*0.1*np.log(5*len(gp.Y))# delta=0.2, gamma_t=0.1
        return mean + np.sqrt(beta_t) * np.sqrt(var) 
    
    
    @staticmethod
    def _ucb_pe(x, gp, kappa, maxlcb):
        mean, var = gp.predict_bucb(x, eval_MSE=True)
        var.flags['WRITEABLE']=True
        var[var<1e-10]=0
        mean=np.atleast_2d(mean).T
        var=np.atleast_2d(var).T

        value=mean + kappa * np.sqrt(var)        
        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]
        var[myidx]=0        
        return var
    
   
    @staticmethod
    def _pure_exploration(x, gp):
        mean, var = gp.predict(x, eval_MSE=True)
        var.flags['WRITEABLE']=True
        #var=var.copy()
        var[var<1e-10]=0
        mean=np.atleast_2d(mean).T
        var=np.atleast_2d(var).T
        return np.sqrt(var)
        
   
    @staticmethod
    def _ei(x, gp, y_max):
        y_max=np.asscalar(y_max)
        mean, var = gp.predict(x, eval_MSE=True)
        var2 = np.maximum(var, 1e-10 + 0 * var)
        z = (mean - y_max)/np.sqrt(var2)        
        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)
        
        out[var2<1e-10]=0
        return out
 
 
    @staticmethod      
    def _poi(x, gp,y_max): # run Predictive Entropy Search using Spearmint
        mean, var = gp.predict(x, eval_MSE=True)    
        # Avoid points with zero variance
        var = np.maximum(var, 1e-9 + 0 * var)
        z = (mean - y_max)/np.sqrt(var)        
        return norm.cdf(z)

   
def unique_rows(a):
    """
    A functions to trim repeated rows that may appear when optimizing.
    This is necessary to avoid the sklearn GP object from breaking

    :param a: array to trim repeated rows from

    :return: mask of unique rows
    """

    # Sort array and kep track of where things should go back to
    order = np.lexsort(a.T)
    reorder = np.argsort(order)

    a = a[order]
    diff = np.diff(a, axis=0)
    ui = np.ones(len(a), 'bool')
    ui[1:] = (diff != 0).any(axis=1)

    return ui[reorder]



class BColours(object):
    BLUE = '\033[94m'
    CYAN = '\033[36m'
    GREEN = '\033[32m'
    MAGENTA = '\033[35m'
    RED = '\033[31m'
    ENDC = '\033[0m'

File Path: bayes_opt/acquisition_maximization.py
Content:
# -*- coding: utf-8 -*-

import numpy as np
from scipy.optimize import minimize
from bayes_opt.acquisition_functions import AcquisitionFunction
import sobol_seq

__author__ = 'Vu'


def acq_max_with_name(gp,scaleSearchSpace,acq_name="ei",IsReturnY=False,IsMax=True,fstar_scaled=None):
    acq={}
    acq['name']=acq_name
    acq['dim']=scaleSearchSpace.shape[0]
    acq['scaleSearchSpace']=scaleSearchSpace   
    if fstar_scaled:
        acq['fstar_scaled']=fstar_scaled   

    myacq=AcquisitionFunction(acq)
    if IsMax:
        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')
    else:
        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)
    if IsReturnY==True:
        y_max=myacq.acq_kind(x_max,gp=gp)
        return x_max,y_max
    return x_max

def acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):
    """
    A function to find the maximum of the acquisition function using
    the scipy python

    Input Parameters
    ----------
    ac: The acquisition function object that return its point-wise value.
    gp: A gaussian process fitted to the relevant data.
    y_max: The current maximum known value of the target function.
    bounds: The variables bounds to limit the search of the acq max.
    
    Returns
    -------
    x_max, The arg max of the acquisition function.
    """
    y_max=np.max(gp.Y)
  
    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)

    return x_max

def generate_sobol_seq(dim,nSobol):
    mysobol_seq = sobol_seq.i4_sobol_generate(dim, nSobol)
    return mysobol_seq
    

def acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):
    """
    A function to find the maximum of the acquisition function using
    the scipy python

    Input Parameters
    ----------
    ac: The acquisition function object that return its point-wise value.
    gp: A gaussian process fitted to the relevant data.
    y_max: The current maximum known value of the target function.
    bounds: The variables bounds to limit the search of the acq max.
    
    Returns
    -------
    x_max, The arg max of the acquisition function.
    """
    dim=SearchSpace.shape[0]
    # Start with the lower bound as the argmax
    x_max = SearchSpace[:, 0]
    min_acq = None

    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}
    myopts ={'maxiter':10*dim,'maxfun':20*dim}
    #myopts ={'maxiter':5*dim}

    #sobol_sequence=generate_sobol_seq(dim=dim,nSobol=500*dim)

    # multi start
    for i in range(3*dim):
        # Find the minimum of minus the acquisition function        
        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))
        
        #x_tries=sobol_sequence
    
        # evaluate
        y_tries=myfunc(x_tries,**kwargs)
        
        #find x optimal for init
        idx_min=np.argmin(y_tries)

        x_init_min=x_tries[idx_min]
    
        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,
                       method="L-BFGS-B",options=myopts)#L-BFGS-B

        if 'x' not in res:
            val=myfunc(res,**kwargs)        
        else:
            val=myfunc(res.x,**kwargs) 
        
        # Store it if better than previous minimum(maximum).
        if min_acq is None or val <= min_acq:
            if 'x' not in res:
                x_max = res
            else:
                x_max = res.x
            min_acq = val
            #print max_acq

    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])

    
def acq_min_scipy(ac, gp, bounds):
    """
    A function to find the maximum of the acquisition function using
    the scipy python

    Input Parameters
    ----------
    ac: The acquisition function object that return its point-wise value.
    gp: A gaussian process fitted to the relevant data.
    y_max: The current maximum known value of the target function.
    bounds: The variables bounds to limit the search of the acq max.
    
    Returns
    -------
    x_max, The arg max of the acquisition function.
    """

    dim=bounds.shape[0]
    # Start with the lower bound as the argmax
    x_max = bounds[:, 0]
    min_acq = None

    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}
    myopts ={'maxiter':10*dim,'maxfun':20*dim}
    #myopts ={'maxiter':5*dim}

    # multi start
    for i in range(3*dim):
        # Find the minimum of minus the acquisition function        
        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))
    
        # evaluate
        y_tries=ac(x_tries,gp=gp)
        
        #find x optimal for init
        idx_max=np.argmin(y_tries)

        x_init_max=x_tries[idx_max]
        
    
        res = minimize(lambda x: ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,
                       method="L-BFGS-B",options=myopts)#L-BFGS-B
      
        if 'x' not in res:
            val=ac(res,gp)        
        else:
            val=ac(res.x,gp) 
        
        # Store it if better than previous minimum(maximum).
        if min_acq is None or val <= min_acq:
            if 'x' not in res:
                x_max = res
            else:
                x_max = res.x
            min_acq = val
            #print max_acq

    return np.clip(x_max, bounds[:, 0], bounds[:, 1])
    
    
def acq_max_scipy(ac, gp, y_max, bounds):
    """
    A function to find the maximum of the acquisition function using
    the scipy python

    Input Parameters
    ----------
    ac: The acquisition function object that return its point-wise value.
    gp: A gaussian process fitted to the relevant data.
    y_max: The current maximum known value of the target function.
    bounds: The variables bounds to limit the search of the acq max.
    
    Returns
    -------
    x_max, The arg max of the acquisition function.
    """

    dim=bounds.shape[0]
    # Start with the lower bound as the argmax
    x_max = bounds[:, 0]
    max_acq = None

    myopts ={'maxiter':10*dim,'maxfun':20*dim}
    #myopts ={'maxiter':5*dim}


    # multi start
    for i in range(1*dim):
        # Find the minimum of minus the acquisition function        
        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))
    
        # evaluate
        y_tries=ac(x_tries,gp=gp)
        #print "elapse evaluate={:.5f}".format(end_eval-start_eval)
        
        #find x optimal for init
        idx_max=np.argmax(y_tries)
        #print "max y_tries {:.5f} y_max={:.3f}".format(np.max(y_tries),y_max)

        x_init_max=x_tries[idx_max]
        
    
        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,
                       method="L-BFGS-B",options=myopts)#L-BFGS-B


        
        if 'x' not in res:
            val=ac(res,gp)        
        else:
            val=ac(res.x,gp) 

        # Store it if better than previous minimum(maximum).
        if max_acq is None or val >= max_acq:
            if 'x' not in res:
                x_max = res
            else:
                x_max = res.x
            max_acq = val
            #print max_acq

    # Clip output to make sure it lies within the bounds. Due to floating
    # point technicalities this is not always the case.
    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])
        #print max_acq
    return np.clip(x_max, bounds[:, 0], bounds[:, 1])
    
    # COBYLA -> x_max[0]
    # L-BFGS-B -> x_max

    
def acq_max_with_init(ac, gp, y_max, bounds, init_location=[]):
    """
    A function to find the maximum of the acquisition function using
    the scipy python

    Input Parameters
    ----------
    ac: The acquisition function object that return its point-wise value.
    gp: A gaussian process fitted to the relevant data.
    y_max: The current maximum known value of the target function.
    bounds: The variables bounds to limit the search of the acq max.
    
    Returns
    -------
    x_max, The arg max of the acquisition function.
    """

    dim=bounds.shape[0]
    # Start with the lower bound as the argmax
    x_max = bounds[:, 0]
    max_acq = None

    #x_tries = np.array([ np.linspace(i,j,500) for i,j in zip( bounds[:, 0], bounds[:, 1])])
    #x_tries=x_tries.T

    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}
    myopts ={'maxiter':5*dim,'maxfun':10*dim}
    #myopts ={'maxiter':5*dim}


    # multi start
    #for i in xrange(5*dim):
    #for i in xrange(1*dim):
    for i in range(2*dim):
        # Find the minimum of minus the acquisition function 
        
        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(20*dim, dim))
        
        if init_location!=[]:
            x_tries=np.vstack((x_tries,init_location))
        
            
        y_tries=ac(x_tries,gp=gp)
        
        #find x optimal for init
        idx_max=np.argmax(y_tries)
        #print "max y_tries {:.5f} y_max={:.3f}".format(np.max(y_tries),y_max)

        x_init_max=x_tries[idx_max]
        
        start_opt=time.time()
    
        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,
                       method="L-BFGS-B",options=myopts)#L-BFGS-B


        #res = fmin_bfgs(lambda x: -ac(x.reshape(1, -1), gp=gp, y_max=y_max),x_init_max.reshape(1, -1),disp=False)#L-BFGS-B
        # value at the estimated point
        #val=ac(res.x,gp,y_max)        
        
        if 'x' not in res:
            val=ac(res,gp)        
        else:
            val=ac(res.x,gp) 

        
        end_opt=time.time()
        #print "elapse optimize={:.5f}".format(end_opt-start_opt)
        
        # Store it if better than previous minimum(maximum).
        if max_acq is None or val >= max_acq:
            if 'x' not in res:
                x_max = res
            else:
                x_max = res.x
            max_acq = val
            #print max_acq

    # Clip output to make sure it lies within the bounds. Due to floating
    # point technicalities this is not always the case.
    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])
        #print max_acq
    return np.clip(x_max, bounds[:, 0], bounds[:, 1])


File Path: bayes_opt/auxiliary_functions.py
Content:
# -*- coding: utf-8 -*-
import sys
sys.path.insert(0,'../../')
sys.path.insert(0,'..')


import matplotlib.pyplot as plt
import numpy as np
import time
from tqdm import tqdm


def run_seq_BO(func,func_input_bounds,acq='ei',n_init=3,niter=10,kernel='SE',stopping_condition=0,verbose=True,isPlot=False):
    # create an empty object for BO
        
    if isinstance(func_input_bounds,dict):
        # Get the name of the parameters
    
        bounds = []
        for key in list(func_input_bounds.keys()):
            #print(key)
            bounds.append(func_input_bounds[key])
            
        bounds = np.asarray(bounds)
    else:
        bounds=np.asarray(func_input_bounds)
        
    
    # create an empty object for BO
    acq_func={}
    #acq_func['name']='thompson'
    dim=len(bounds)
    
    acq_func['name']=acq # ei, ucb, random
    acq_func['dim']=dim
    
    #print(myfunction.bounds)
    
    myfunction={}
    myfunction['func']=func
    myfunction['bounds']=bounds
    
    func_params={}
    func_params['function']=myfunction
    
    
    acq_params={}
    acq_params['acq_func']=acq_func
    #acq_params['optimize_gp']=['marginal','loo','maximize']
    acq_params['optimize_gp']='maximize'
    acq_params['stopping']=stopping_condition

    #gp_params = {'kernel':'SE','lengthscale':0.1*dim,'noise_delta':1e-10,'flagIncremental':0}
    
    if kernel=='SE':
        gp_params = {'kernel':'SE','dim':dim,'lengthscale':0.1*dim,'noise_delta':1e-10}
    elif kernel=='ARD':
        gp_params = {'kernel':'ARD','dim':dim,'noise_delta':1e-10}
    else:
        print('Please select SE kernel or ARD kernel')
        return

    acq_params['opt_toolbox']='scipy'
    
    bo=CAI_Seq_BO(gp_params,func_params,acq_params,verbose)
    bo.init(gp_params,n_init_points=n_init)
    
    for index in range(0,niter):
        bo.suggest_nextpoint()
            
        if isPlot and dim<=2 and acq_func['name']!="random":
            visualization.plot_bo(bo)
        
    
    maxIdx=np.argmax(bo.Y_original)
    
    return bo.X_original[maxIdx],bo.Y_original[maxIdx],bo
       

def run_experiment(bo,gp_params,yoptimal=0,n_init=3,NN=10,runid=1):
    # create an empty object for BO
    
    start_time = time.time()
    bo.init(gp_params,n_init_points=n_init,seed=runid)
    
    # number of recommended parameters
    for index in tqdm(range(0,NN-1)):
        #print index
        bo.suggest_nextpoint()

    fxoptimal=bo.Y_original
    elapsed_time = time.time() - start_time

    return fxoptimal, elapsed_time

  
            
def yBest_Iteration(YY,BatchSzArray,IsPradaBO=1,Y_optimal=0,step=3,IsMax=-1):
    
    
    nRepeat=len(YY)
    YY=np.asarray(YY)
    #YY=np.vstack(YY.T)
    #YY=YY.T
    print(YY.shape, step)
    ##YY_mean=np.mean(YY,axis=0)
    #YY_std=np.std(YY,axis=0)
    
    mean_TT=[]
    
    mean_cum_TT=[]
    
    for idxtt,tt in enumerate(range(0,nRepeat)): # TT run
    
        if IsPradaBO==1:
            temp_mean=YY[idxtt,0:BatchSzArray[0]].max()
        else:
            temp_mean=YY[idxtt,0:BatchSzArray[0]].min()
        
        temp_mean_cum=YY[idxtt,0:BatchSzArray[0]].mean()

        start_point=0
        for idx,bz in enumerate(BatchSzArray): # batch
            if idx==len(BatchSzArray)-1:
                break
            bz=np.int(bz)

            #    get the average in this batch
            temp_mean_cum=np.vstack((temp_mean_cum,YY[idxtt,start_point:start_point+bz].mean()))
            
            # find maximum in each batch            
            if IsPradaBO==1:
                temp_mean=np.vstack((temp_mean,YY[idxtt,start_point:start_point+bz].max()))
            else:
                temp_mean=np.vstack((temp_mean,YY[idxtt,start_point:start_point+bz].min()))

            start_point=start_point+bz

        if IsPradaBO==1:
            myYbest=[temp_mean[:idx+1].max()*IsMax for idx,val in enumerate(temp_mean)]
            temp_mean_cum=temp_mean_cum*IsMax
            temp_mean=temp_mean*IsMax
        else:
            myYbest=[temp_mean[:idx+1].min() for idx,val in enumerate(temp_mean)]

        
        temp_regret=np.abs(temp_mean-Y_optimal)
        myYbest_cum=[np.mean(temp_regret[:idx+1]) for idx,val in enumerate(temp_regret)]


        if len(mean_TT)==0:
            mean_TT=myYbest
            mean_cum_TT=myYbest_cum
        else:
            #mean_TT.append(temp_mean)
            mean_TT=np.vstack((mean_TT,myYbest))
            mean_cum_TT=np.vstack((mean_cum_TT,myYbest_cum))
            
    mean_TT    =np.array(mean_TT)
    std_TT=np.std(mean_TT,axis=0)
    std_TT=np.array(std_TT).ravel()
    mean_TT=np.mean(mean_TT,axis=0)

    
    mean_cum_TT=np.array(mean_cum_TT)   
    std_cum_TT=np.std(mean_cum_TT,axis=0)
    std_cum_TT=np.array(std_cum_TT).ravel()
    mean_cum_TT=np.mean(mean_cum_TT,axis=0)
   
    #return mean_TT[::step],std_TT[::step]#,mean_cum_TT[::5],std_cum_TT[::5]
    
    return mean_TT[::step],std_TT[::step],mean_cum_TT[::step],std_cum_TT[::step]
    

File Path: bayes_opt/baselines.py
Content:
import numpy as np
import itertools
#import time
#import bayes_opt.utils
from bayes_opt.utility.basic_utility_functions import transform_logistic_marginal, transform_logistic

counter = 0

class RandomSearch(object):
    def __init__(self, func_params, verbose=True):
        """      
        Input parameters
        ----------
        
        
        func_params:                function to optimize
        func_params.init bound:     initial bounds for parameters
        func_params.bounds:         bounds on parameters        
        func_params.func:           a function to be optimized
        
        Returns
        -------
        dim:            dimension
        """

        # Find number of parameters
        self.verbose=verbose

        try:
            bounds=func_params['function']['bounds']
        except:
            bounds=func_params['function'].bounds

        self.dim = len(bounds)

        # Create an array with parameters bounds
        if isinstance(bounds,dict):
            # Get the name of the parameters
            self.keys = list(bounds.keys())
        
            self.bounds = []
            for key in self.keys:
                self.bounds.append(bounds[key])
            self.bounds = np.asarray(self.bounds)
        else:
            self.bounds=np.asarray(bounds)
 
        # create a scalebounds 0-1
        self.scalebounds = np.array([np.zeros(self.dim), np.ones(self.dim)]).T
        self.max_min_gap=self.bounds[:,1]-self.bounds[:,0]
        
        
        # Some function to be optimized
        try:
            self.f = func_params['function']['func']
        except:
            self.f = func_params['function'].func

            
        # store X in original scale
        self.X_original = None

        # store X in 0-1 scale
        self.X = None
        
        self.Y_curves=[]
        
        # store y=f(x)
        # (y - mean)/(max-min)
        self.Y = None
               
        # y original scale
        self.Y_original = None

        self.time_opt = 0
    
    """
    def init(self, n_init_points=3,seed=1):
        utils.set_seed(seed)
        """
        
    def suggest_nextpoint(self):
        """
        Main optimization method.

        Input parameters
        ----------
        gp_params: parameter for Gaussian Process

        Returns
        -------
        x: recommented point for evaluation
        """
        
        
        x_max = [np.random.uniform(x[0], x[1], size=1) for x in self.bounds]
        x_max = np.asarray(x_max).T
        
        if self.X_original is None:
            self.X_original = x_max[:,:-1]
            self.T_original = x_max[:,-1]

        else:
            self.X_original=np.vstack((self.X_original, x_max[:,:-1]))
            self.T_original=np.vstack((self.T_original, x_max[:,-1]))

       
        y_curves, y_cost=self.f(x_max)
        y_max=transform_logistic_marginal(y_curves,self.bounds[-1,1])

        self.Y_curves+=y_curves

        # evaluate Y using original X
        if self.Y_original is None:
            #self.Y_original = np.array([self.f(x_max)])
            self.Y_original = np.array([y_max])
            y_cost=np.atleast_2d(np.asarray(y_cost)).astype('Float64')
            self.Y_cost_original=y_cost
            self.Y_cost_original=np.reshape(self.Y_cost_original,(-1,1))
            
        
        else:
            #self.Y_original = np.append(self.Y_original, self.f(x_max))
            self.Y_original = np.append(self.Y_original, y_max)
            self.Y_cost_original = np.append(self.Y_cost_original, y_cost)

        # update Y after change Y_original
        self.Y=(self.Y_original - np.mean(self.Y_original))/np.std(self.Y_original)
        if self.verbose:
            print("x={} current y={:.4f}, ybest={:.4f}".format(self.X_original[-1],self.Y_original[-1], self.Y_original.max()))

        return

class GridSearch(object):
    def __init__(self, func_params, acq_params, verbose=True):
        """      
        Input parameters
        ----------
        
        
        func_params:                function to optimize
        func_params.init bound:     initial bounds for parameters
        func_params.bounds:         bounds on parameters        
        func_params.func:           a function to be optimized
        
        
        acq_params:                 acquisition function, 
        acq_params.steps:           number of gridsearch sweeps to be run
        acq_params.points:          either list of points per parameter or integer of number of points
        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'
                            
        Returns
        -------
        dim:            dimension
        bounds:         bounds on original scale
        scalebounds:    bounds on normalized scale of 0-1
        time_opt:       will record the time spent on grid search
        gp:             Gaussian Process object
        """

        # Find number of parameters
        self.verbose=verbose

        try:
            bounds=func_params['function']['bounds']
        except:
            bounds=func_params['function'].bounds

        self.dim = len(bounds)

        # Create an array with parameters bounds
        if isinstance(bounds,dict):
            # Get the name of the parameters
            self.keys = list(bounds.keys())
        
            self.bounds = []
            for key in list(bounds.keys()):
                self.bounds.append(bounds[key])
            self.bounds = np.asarray(self.bounds)
        else:
            self.bounds=np.asarray(bounds)
        
        self.grid = []
        self.grid_params = acq_params
        for i, param in enumerate(self.grid_params["points"]):
            if isinstance(param, int):
                self.grid.append([self.bounds[i, 0] + s * (self.bounds[i, 1] - self.bounds[i, 0]) / (param-1) for s in range(param)])
            else:
                if len(param.shape) > 1:
                    self.grid.append(params[0, :])
                else:
                    self.grid.append([self.bounds[i, 0] + s * (self.bounds[i, 1] - self.bounds[i, 0]) / (int(param[0])-1) for s in range(param[0])])
        self.points = list(itertools.product(*self.grid))
        self.grid_lvl = [0,0]
        # create a scalebounds 0-1
        scalebounds=np.array([np.zeros(self.dim), np.ones(self.dim)])
        self.scalebounds=scalebounds.T
        
        self.max_min_gap=self.bounds[:,1]-self.bounds[:,0]
        
        
        # Some function to be optimized
        try:
            self.f = func_params['function']['func']
        except:
            self.f = func_params['function'].func
    
        # store X in original scale
        self.X_original= None

        self.Y_curves=[]
        # store X in 0-1 scale
        self.X = None
        
        # store y=f(x)
        # (y - mean)/(max-min)
        self.Y = None
               
        # y original scale
        self.Y_original = None

        self.time_opt=0    
    
    def init(self, seed=1):

        np.random.seed(seed)
        
    def suggest_nextpoint(self):
        """
        Main optimization method.

        Input parameters
        ----------
        gp_params: parameter for Gaussian Process

        Returns
        -------
        x: recommented point for evaluation
        """

        if self.grid_lvl[0] >= len(self.points):
            if self.grid_lvl[1] == self.grid_params["steps"]:
                return
            self.grid_lvl[0] = 0
            self.grid_lvl[1] += 1
            
            cur_max = self.X_original[self.Y_original.argmax()]
            for i, param in enumerate(self.grid_params["points"]):
                max_i = self.grid[i].index(cur_max[i])
                b_min = self.grid[i][0] if max_i == 0 else (self.grid[i][max_i] + self.grid[i][max_i - 1]) / 2
                b_max = self.grid[i][-1] if max_i == len(self.grid[i]) - 1 else (self.grid[i][max_i] + self.grid[i][max_i + 1]) / 2
                if isinstance(param, int):
                    self.grid[i] = [b_min + s * (b_max - b_min) / (param-1) for s in range(param)]
                else:
                    if len(param.shape) > 1:
                        self.grid[i] = param[self.grid_lvl[1], :]
                    else:
                        intervals = param[self.grid_lvl[1]]
                        self.grid[i] = [b_min + s * (b_max - b_min) / (intervals-1) for s in range(intervals)]
            self.points = list(itertools.product(*self.grid))       
            print("\n")

        x_max = self.points[self.grid_lvl[0]]
        self.grid_lvl[0] += 1
        x_max = np.asarray(x_max).T
        if self.X_original is None:
            self.X_original = np.array([x_max])
        else:
            self.X_original=np.vstack((self.X_original, x_max))
        # evaluate Y using original X
        if self.Y_original is None:
            self.Y_original = np.array([self.f(x_max)])
        else:
            self.Y_original = np.append(self.Y_original, self.f(x_max))
        
        # update Y after change Y_original
        self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)
       
        if self.verbose:
            print("x={} current y={:.4f}, ybest={:.4f}".format(self.X_original[-1],self.Y_original[-1], self.Y_original.max()))

class HyperBand(object):
    def __init__(self, func_params, verbose=True):
        """      
        Input parameters
        ----------
        
        
        func_params:                function to optimize
        func_params.init bound:     initial bounds for parameters
        func_params.bounds:         bounds on parameters        
        func_params.func:           a function to be optimized
        
        Returns
        -------
        dim:            dimension
        """

        # Find number of parameters
        self.verbose=verbose

        try:
            bounds=func_params['function']['bounds']
        except:
            bounds=func_params['function'].bounds

        self.dim = len(bounds)

        # Create an array with parameters bounds
        if isinstance(bounds,dict):
            # Get the name of the parameters
            self.keys = list(bounds.keys())
        
            self.bounds = []
            for key in self.keys:
                self.bounds.append(bounds[key])
            self.bounds = np.asarray(self.bounds)
        else:
            self.bounds=np.asarray(bounds)
 
        # create a scalebounds 0-1
        self.scalebounds = np.array([np.zeros(self.dim), np.ones(self.dim)]).T
        self.max_min_gap=self.bounds[:,1]-self.bounds[:,0]
        
        
        # Some function to be optimized
        try:
            self.f = func_params['function']['func']
        except:
            self.f = func_params['function'].func

            
        # store X in original scale
        self.X_original = None

        # store X in 0-1 scale
        self.X = None
        
        self.Y_curves=[]
        
        # store y=f(x)
        # (y - mean)/(max-min)
        self.Y = None
               
        # y original scale
        self.Y_original = None

        self.time_opt = 0
        
        #hyperBanc setup
        self.max_iter = func_params['R']
        self.thinning = func_params['n']
        self.s_max = int(np.log(self.max_iter) / np.log(self.thinning))
        self.stepsPerBracket = (self.s_max + 1) * self.max_iter

        self.s = self.s_max
        self.setup_configs(self.s)
        
        self.done = False

    def setup_configs(self, s):
        # starts a new iteration of successive halving
        n = int(np.ceil(int(self.stepsPerBracket/self.max_iter/(s+1))*self.thinning**s))
        self.run_times = [int(self.max_iter*self.thinning**(-s)) + self.bounds[-1][0] for s in range(s, -1, -1)]
        self.configs = [[np.random.uniform(x[0], x[1], size=1) for x in self.bounds] for _ in range(n)]
        self.next_conf = 0

    def suggest_nextpoint(self):
        """
            Executes next simulation for HyperBand
        """
        x_max = self.configs[self.next_conf]
        x_max[-1] = np.array([self.run_times[0]])
        x_max = np.asarray(x_max).T

        if self.X_original is None:
            self.X_original = x_max[:,:-1]
            self.T_original = x_max[:,-1]

        else:
            self.X_original=np.vstack((self.X_original, x_max[:,:-1]))
            self.T_original=np.vstack((self.T_original, x_max[:,-1]))

       
        y_curves, y_cost=self.f(x_max)
        y_max=transform_logistic_marginal(y_curves,self.bounds[-1,1])

        self.Y_curves+=y_curves

        # evaluate Y using original X
        if self.Y_original is None:
            #self.Y_original = np.array([self.f(x_max)])
            self.Y_original = np.array([y_max])
            y_cost=np.atleast_2d(np.asarray(y_cost)).astype('Float64')
            self.Y_cost_original=y_cost
            self.Y_cost_original=np.reshape(self.Y_cost_original,(-1,1))
        else:
            #self.Y_original = np.append(self.Y_original, self.f(x_max))
            self.Y_original = np.append(self.Y_original, y_max)
            self.Y_cost_original = np.append(self.Y_cost_original, y_cost)

        # update Y after change Y_original
        self.Y=(self.Y_original - np.mean(self.Y_original))/np.std(self.Y_original)
        if self.verbose:
            print("x={} current y={:.4f}, ybest={:.4f}".format(self.X_original[-1],self.Y_original[-1], self.Y_original.max()))

        self.next_conf += 1
        if self.next_conf >= len(self.configs):
            n = len(self.configs)
            self.configs =  [ self.configs[i] for i in np.argsort(self.Y_original[-n:])[0:int( n/self.thinning )] ]
            self.next_conf = 0
            self.run_times.pop(0)
            if len(self.run_times) == 0:
                self.s -= 1
                if self.s >= 0:
                    self.setup_configs(self.s)
                else:
                    self.done = True
        return

File Path: bayes_opt/curve_compression.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 15 10:18:39 2020

@author: Vu
"""


import itertools
import numpy as np

def apply_one_transform_average(curve, midpoint=3, growth=1,MaxEpisode=1000):
    # averaging the reward curve into numeric score
            
    if isinstance(curve, (list,)):
        curve=curve[0]
 
    def linear_func(x): # this is a straightline
        if len(x)==1:
            return 1
        else:
            return [1 for u in x]
	
    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)
    my_logistic_value_scaled=linear_func(my_xrange_scaled)
    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)] # this is for visualisation purpose

    average=np.mean(curve)
    
    return average,my_logistic_value_scaled    


def return_logistic_curve(midpoint, growth, MaxEpoch=1000):
    # given the growth, midpoint and npoint, return the Logistic curve for visualization
    
    def logistic_func(x):
        #alpha=32
        if len(x)==1:
            return 1.0/(1+np.exp(-growth*(x-midpoint)))
        else:
            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]
        
    my_xrange_scaled=np.linspace(-6,6, MaxEpoch)
    my_logistic_value_scaled=logistic_func(my_xrange_scaled)
    
    return my_logistic_value_scaled



#def apply_one_transform_decreasing(curve, midpoint=3, growth=1,MaxEpisode=1000):
#            
#    if isinstance(curve, (list,)):
#        curve=curve[0]
#
# 
#    def ln_func(x):
#        #alpha=32
#        if len(x)==1:
#            return 1/x
#        else:
#            return [1/u for u in x]
#	
#    #my_xrange_scaled=np.linspace(-6,6, len(curve))
#    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)
#    my_logistic_value_scaled=ln_func(my_xrange_scaled)
#    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]
#
#
#    # if curve is negative, add a constant to make it positive
#    if np.max(curve)<=0 and np.min(curve)<=0:
#        curve=curve+500
#    
#    threshold=(midpoint+6-2)*len(curve)/(12)
#    threshold=np.int(threshold)
#    #print(threshold)
#    
#    
#    prod_func=curve*my_logistic_value_scaled
#    
#    #print(len(prod_func))
#    average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]
#
#    #print(average)
#    if np.isnan(average[-1]):
#        print('bug [curve]')
#    return average[-1],my_logistic_value_scaled    


#def apply_one_transform_linear(curve, midpoint=3, growth=1,MaxEpisode=1000):
#            
#    if isinstance(curve, (list,)):
#        curve=curve[0]
# 
#    def ln_func(x):
#        if len(x)==1:
#            return x
#        else:
#            return [u for u in x]
#	
#    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)
#    my_logistic_value_scaled=ln_func(my_xrange_scaled)
#    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]
#
#    # if curve is negative, add a constant to make it positive
#    if np.max(curve)<=0 and np.min(curve)<=0:
#        curve=curve+500
#     
#    threshold=(midpoint+6-2)*len(curve)/(12)
#    threshold=np.int(threshold)
#    
#    prod_func=curve*my_logistic_value_scaled
#    
#    average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]
#
#    return average[-1],my_logistic_value_scaled    

def apply_one_transform_ln(curve, midpoint=3, growth=1,MaxEpisode=1000):
    # this is the log transformation, used in the ablation study
    if isinstance(curve, (list,)):
        curve=curve[0]
 
    def ln_func(x):
        if len(x)==1:
            return 20+np.log(x)
        else:
            return [np.log(u) for u in x]
	
    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)
    my_logistic_value_scaled=ln_func(my_xrange_scaled)
    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]

    # if curve is negative, add a constant to make it positive
    if np.max(curve)<=0 and np.min(curve)<=0:
        curve=curve+500
    
    threshold=(midpoint+6-2)*len(curve)/(12)
    threshold=np.int(threshold)    
    
    prod_func=curve*my_logistic_value_scaled
    
    average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]

    if np.isnan(average[-1]):
        print('bug [curve]')
    return average[-1],my_logistic_value_scaled


def apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):
    # this is the Logistic transformation, used in the paper
    if isinstance(curve, (list,)):
        curve=curve[0]
        
    def logistic_func(x):
        return 1.0/(1+np.exp(-growth*(x-midpoint)))
	
    #print(MaxEpisode)
    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))

    my_logistic_value_scaled=logistic_func(my_xrange_scaled)

    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]

    # if curve is negative, add a constant to make it positive
    if np.max(curve)<=0 and np.min(curve)<=0:
        curve=curve+500
    
    threshold=(midpoint+6-2)*len(curve)/(12)
    threshold=np.int(threshold)
    
    prod_func=curve*my_logistic_value_scaled
    
    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]

    if IsReturnCurve==True:
        return average[-1],my_logistic_value_scaled
    else:
        return average[-1]


#def return_logistic_curve(midpoint, growth, npoint):
#           
#    def logistic_func(x):
#        #alpha=32
#        if len(x)==1:
#            return 1.0/(1+np.exp(-growth*(x-midpoint)))
#        else:
#            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]
#        
#    my_xrange_scaled=np.linspace(-6,6, npoint)
#    my_logistic_value_scaled=logistic_func(my_xrange_scaled)
#    
#    return my_logistic_value_scaled




    
def transform_logistic_marginal(curves,MaxEpisode=1000):
    # curve is a matrix [nParameter x MaxIter]
    # or curve is a vector [1 x MaxIter]

    def transform_one_logistic_marginal(curves,MaxEpisode):
        # curve is a vector [1 x MaxIter]
    
        midpoint_list=[-3,-2,-1,0,1]
        growth_list=[0.1,1,2,3]
        
        temp_Y_value=[0]*(len(midpoint_list)*len(growth_list))
        for idx, (val1, val2) in enumerate(itertools.product(midpoint_list,growth_list)):
            temp_Y_value[idx]=apply_one_transform_logistic(curves,val1, val2,MaxEpisode)
                
        temp_Y_value=np.asarray(temp_Y_value)
        
        Y=np.mean(temp_Y_value,axis=0)
        return Y
    if len(curves)==1:
        output=transform_one_logistic_marginal(curves[0],MaxEpisode)
    else:
        output=[0]*len(curves)
        for idx, curve in enumerate(curves):
            output[idx]=transform_one_logistic_marginal(curve,MaxEpisode)
    return output    


def transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):
    # curve is a matrix [nParameter x MaxIter]
    # or curve is a vector [1 x MaxIter]

    if len(curves)==1:
        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)
    else:
        output=[0]*len(curves)
        for idx, curve in enumerate(curves):
            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)
    return output
    

        
    
File Path: bayes_opt/gaussian_process.py
Content:
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
from scipy.optimize import minimize
from sklearn.preprocessing import MinMaxScaler
import scipy
#from sklearn.metrics import pairwise_distances
import matplotlib.pyplot as plt
#import matplotlib as mpl
import matplotlib.cm as cm

class GaussianProcess(object):
    def __init__ (self,SearchSpace,noise_delta=1e-8,verbose=0):
        self.noise_delta=noise_delta
        self.noise_upperbound=noise_delta
        self.mycov=self.cov_RBF
        self.SearchSpace=SearchSpace
        scaler = MinMaxScaler()
        scaler.fit(SearchSpace.T)
        self.Xscaler=scaler
        self.verbose=verbose
        self.dim=SearchSpace.shape[0]
        
        self.hyper={}
        self.hyper['var']=1 # standardise the data
        self.hyper['lengthscale']=0.04 #to be optimised
        self.noise_delta=noise_delta
        return None
        
    def set_optimum_value(self,fstar_scaled):
        self.fstar=fstar_scaled
        
    def fit(self,X,Y,IsOptimize=0):
        """
        Fit a Gaussian Process model
        X: input 2d array [N*d]
        Y: output 2d array [N*1]
        """       
        self.X_ori=X # this is the output in original scale
        #self.X= self.Xscaler.transform(X) #this is the normalised data [0-1] in each column
        self.X=X
        self.Y_ori=Y # this is the output in original scale
        self.Y=(Y-np.mean(Y))/np.std(Y) # this is the standardised output N(0,1)
        
        if IsOptimize:
            self.hyper['lengthscale']=self.optimise()         # optimise GP hyperparameters
            
        self.KK_x_x=self.mycov(self.X,self.X,self.hyper)+np.eye(len(X))*self.noise_delta     
        if np.isnan(self.KK_x_x).any(): #NaN
            print("nan in KK_x_x !")
      
        self.L=scipy.linalg.cholesky(self.KK_x_x,lower=True)
        temp=np.linalg.solve(self.L,self.Y)
        self.alpha=np.linalg.solve(self.L.T,temp)
        
    def cov_RBF(self,x1, x2,hyper):        
        """
        Radial Basic function kernel (or SE kernel)
        """
        variance=hyper['var']
        lengthscale=hyper['lengthscale']

        if x1.shape[1]!=x2.shape[1]:
            x1=np.reshape(x1,(-1,x2.shape[1]))
        Euc_dist=euclidean_distances(x1,x2)

        return variance*np.exp(-np.square(Euc_dist)/lengthscale)
    

    def log_llk(self,X,y,hyper_values):
        
        #print(hyper_values)
        hyper={}
        hyper['var']=1
        hyper['lengthscale']=hyper_values[0]
        noise_delta=self.noise_delta

        KK_x_x=self.mycov(X,X,hyper)+np.eye(len(X))*noise_delta     
        if np.isnan(KK_x_x).any(): #NaN
            print("nan in KK_x_x !")   

        try:
            L=scipy.linalg.cholesky(KK_x_x,lower=True)
            alpha=np.linalg.solve(KK_x_x,y)

        except: # singular
            return -np.inf
        try:
            first_term=-0.5*np.dot(self.Y.T,alpha)
            W_logdet=np.sum(np.log(np.diag(L)))
            second_term=-W_logdet

        except: # singular
            return -np.inf

        logmarginal=first_term+second_term-0.5*len(y)*np.log(2*3.14)
        
        #print(hyper_values,logmarginal)
        return np.asscalar(logmarginal)
    
    def set_ls(self,lengthscale):
        self.hyper['lengthscale']=lengthscale
        
    def optimise(self):
        """
        Optimise the GP kernel hyperparameters
        Returns
        x_t
        """
        opts ={'maxiter':200,'maxfun':200,'disp': False}

        # epsilon, ls, var, noise var
        #bounds=np.asarray([[9e-3,0.007],[1e-2,self.noise_upperbound]])
        bounds=np.asarray([[1e-3,1]])

        init_theta = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(10, 1))
        logllk=[0]*init_theta.shape[0]
        for ii,val in enumerate(init_theta):           
            logllk[ii]=self.log_llk(self.X,self.Y,hyper_values=val) #noise_delta=self.noise_delta
            
        x0=init_theta[np.argmax(logllk)]

        res = minimize(lambda x: -self.log_llk(self.X,self.Y,hyper_values=x),x0,
                                   bounds=bounds,method="L-BFGS-B",options=opts)#L-BFGS-B
        
        if self.verbose:
            print("estimated lengthscale",res.x)
            
        return res.x  
   
    def predict(self,Xtest,isOriScale=False):
        """
        ----------
        Xtest: the testing points  [N*d]
        Returns
        -------
        pred mean, pred var, pred mean original scale, pred var original scale
        """    
        
        if isOriScale:
            Xtest=self.Xscaler.transform(Xtest)
            
        if len(Xtest.shape)==1: # 1d
            Xtest=np.reshape(Xtest,(-1,self.X.shape[1]))
            
        if Xtest.shape[1] != self.X.shape[1]: # different dimension
            Xtest=np.reshape(Xtest,(-1,self.X.shape[1]))
       
        KK_xTest_xTest=self.mycov(Xtest,Xtest,self.hyper)+np.eye(Xtest.shape[0])*self.noise_delta
        KK_xTest_x=self.mycov(Xtest,self.X,self.hyper)

        mean=np.dot(KK_xTest_x,self.alpha)
        v=np.linalg.solve(self.L,KK_xTest_x.T)
        var=KK_xTest_xTest-np.dot(v.T,v)

        #mean_ori=mean*np.std(self.Y_ori)+np.mean(self.Y_ori)
        std=np.reshape(np.diag(var),(-1,1))
        
        #std_ori=std*np.std(self.Y_ori)#+np.mean(self.Y_ori)
        
        #return mean,std,mean_ori,std_ori
        return  np.reshape(mean,(-1,1)),std  

  

File Path: bayes_opt/product_gaussian_process.py
Content:
# -*- coding: utf-8 -*-
# define Gaussian Process class


import numpy as np
from bayes_opt.acquisition_functions import unique_rows
from sklearn.preprocessing import MinMaxScaler

from scipy.optimize import minimize
from sklearn.metrics.pairwise import euclidean_distances
import scipy.linalg as spla
from bayes_opt.curve_compression import apply_one_transform_logistic, transform_logistic


class ProductGaussianProcess(object):
    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')
    
    
    #def __init__ (self,param):
    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):
        self.noise_delta=5e-4
        self.noise_upperbound=1e-2
        self.mycov=self.cov_RBF_time
        self.SearchSpace=SearchSpace
        scaler = MinMaxScaler()
        scaler.fit(SearchSpace.T)
        self.Xscaler=scaler
        self.verbose=verbose
        self.dim=SearchSpace.shape[0]
        
        if gp_hyper is None:
            self.hyper={}
            self.hyper['var']=1 # standardise the data
            self.hyper['lengthscale_x']=0.02 #to be optimised
            self.hyper['lengthscale_t']=0.2 #to be optimised
        else:
            self.hyper=gp_hyper

        
        if logistic_hyper is None:
            self.logistic_hyper={}
            self.logistic_hyper['midpoint']=0.0
            self.logistic_hyper['growth']=1.0   
        else:
            self.logistic_hyper=logistic_hyper

        self.X=[]
        self.T=[]
        self.Y=[]
        self.Y_curves=None
#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']
#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t
        
        self.alpha=[] # for Cholesky update
        self.L=[] # for Cholesky update LL'=A
        
        self.MaxEpisode=0
        
        return None
       

    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):
        
        Euc_dist=euclidean_distances(x1,x2)
        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)
        
        Euc_dist=euclidean_distances(t1,t2)
        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)
        
        return exp_dist_x*exp_dist_t
                
    def fit(self,X,T,Y,Y_curves):
        """
        Fit Gaussian Process model

        Input Parameters
        ----------
        x: the observed points 
        t: time or number of episode
        y: the outcome y=f(x)
        
        """ 
        temp=np.hstack((X,T))
        ur = unique_rows(temp)
        
        T=T[ur]
        X=X[ur]
        Y=Y[ur]
        
        self.X=X
        self.Y=Y
        self.T=T
        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]
        
        for curves in self.Y_curves:
            self.MaxEpisode=max(len(curves),self.MaxEpisode)
        #self.Y_curves=Y_curves[myidx]
            
        Euc_dist_x=euclidean_distances(X,X)
        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta
    
        Euc_dist_t=euclidean_distances(T,T)
        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       
    
        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\
                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta
          
        if np.isnan(self.KK_x_x).any(): #NaN
            print("nan in KK_x_x")
        
        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)
        self.L=np.linalg.cholesky(self.KK_x_x)
        temp=np.linalg.solve(self.L,self.Y)
        self.alpha=np.linalg.solve(self.L.T,temp)
        self.cond_num=self.compute_condition_number()
        
    def compute_condition_number(self):
        cond_num=np.linalg.cond(self.KK_x_x)
        return cond_num
    

    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):
        """
        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter
        """

        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):
            # compute K
            temp=np.hstack((self.X,self.T))
            ur = unique_rows(temp)
            myX=self.X[ur]
            myT=self.T[ur]
            
            # transform Y_curve to Y_original, then to Y
            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)
            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)
            
            myY=myY[ur]
          
            self.Euc_dist_x=euclidean_distances(myX,myX)
            self.Euc_dist_t=euclidean_distances(myT,myT)
        
            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\
                +np.eye(len(myX))*noise_delta
                    
            
            try:
                temp_inv=np.linalg.solve(KK,myY)
            except: # singular
                return -np.inf
            
            try:
                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)
                first_term=-0.5*np.dot(myY.T,temp_inv)
                
                # if the matrix is too large, we randomly select a part of the data for fast computation
                if KK.shape[0]>200:
                    idx=np.random.permutation(KK.shape[0])
                    idx=idx[:200]
                    KK=KK[np.ix_(idx,idx)]
                #Wi, LW, LWi, W_logdet = pdinv(KK)
                #sign,W_logdet2=np.linalg.slogdet(KK)
                chol  = spla.cholesky(KK, lower=True)
                W_logdet=np.sum(np.log(np.diag(chol)))
                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A
    
                #second_term=-0.5*W_logdet2
                second_term=-W_logdet
            except: # singular
                return -np.inf
            

            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)
                
            if np.isnan(np.asscalar(logmarginal))==True:
                print("lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}".format(
                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))

            #print(lengthscale, lengthscale_t,midpoint,growth,"logmarginal:",logmarginal)
            return np.asscalar(logmarginal)
        
        logmarginal=0

        if not isinstance(hyper,list) and len(hyper.shape)==2:
            logmarginal=[0]*hyper.shape[0]
            growth=hyper[:,3]
            midpoint=hyper[:,2]
            lengthscale_t=hyper[:,1]
            lengthscale_x=hyper[:,0]
            for idx in range(hyper.shape[0]):
                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\
                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)
        else:
            lengthscale_x,lengthscale_t,midpoint,growth=hyper
            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\
                                                                 midpoint,growth,noise_delta)
        return logmarginal

#    def optimize_lengthscale_SE_maximizing(self,previous_theta,noise_delta):
#        """
#        Optimize to select the optimal lengthscale parameter
#        """
#                
#        # define a bound on the lengthscale
#        SearchSpace_lengthscale_min=0.01
#        SearchSpace_lengthscale_max=0.5
#        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]
#        
#        mySearchSpace=np.asarray([[SearchSpace_lengthscale_min,SearchSpace_lengthscale_max],\
#                             [10*SearchSpace_lengthscale_min,2*SearchSpace_lengthscale_max]])
#        
#        # Concatenate new random points to possible existing
#        # points from self.explore method.
#        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, mySearchSpace.shape[0]))
#
#        #print lengthscale_tries
#
#        # evaluate
#        self.flagOptimizeHyperFirst=0 # for efficiency
#
#        logmarginal_tries=self.log_marginal_lengthscale(lengthscale_tries,noise_delta)
#        #print logmarginal_tries
#
#        #find x optimal for init
#        idx_max=np.argmax(logmarginal_tries)
#        lengthscale_init_max=lengthscale_tries[idx_max]
#        #print lengthscale_init_max
#        
#        myopts ={'maxiter':20*self.dim,'maxfun':20*self.dim}
#
#        x_max=[]
#        max_log_marginal=None
#        
#        res = minimize(lambda x: -self.log_marginal_lengthscale(x,noise_delta),lengthscale_init_max,
#                       SearchSpace=mySearchSpace,method="L-BFGS-B",options=myopts)#L-BFGS-B
#        if 'x' not in res:
#            val=self.log_marginal_lengthscale(res,noise_delta)    
#        else:
#            val=self.log_marginal_lengthscale(res.x,noise_delta)  
#        
#        # Store it if better than previous minimum(maximum).
#        if max_log_marginal is None or val >= max_log_marginal:
#            if 'x' not in res:
#                x_max = res
#            else:
#                x_max = res.x
#            max_log_marginal = val
#            #print res.x
#
#        return x_max
    
    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):
        """
        Optimize to select the optimal lengthscale parameter
        """
        
        # define a bound on the lengthscale
        SearchSpace_l_min=0.03
        SearchSpace_l_max=0.3
        
        SearchSpace_midpoint_min=-2
        SearchSpace_midpoint_max=3
        
        SearchSpace_growth_min=0.5
        SearchSpace_growth_max=2
        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]
        
        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],
                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])
        
        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))

        # evaluate
        self.flagOptimizeHyperFirst=0 # for efficiency

        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)

        #find x optimal for init
        idx_max=np.argmax(logmarginal_tries)
        lengthscale_init_max=lengthscale_tries[idx_max]
        #print lengthscale_init_max
        
        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}

        x_max=[]
        max_log_marginal=None
        
        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,
                       bounds=mySearchSpace,method="L-BFGS-B",options=myopts)#L-BFGS-B
        if 'x' not in res:
            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    
        else:
            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  
        
        # Store it if better than previous minimum(maximum).
        if max_log_marginal is None or val >= max_log_marginal:
            if 'x' not in res:
                x_max = res
            else:
                x_max = res.x
            max_log_marginal = val
            #print res.x

        return x_max


#    def optimize_lengthscale(self,previous_theta_x, previous_theta_t,noise_delta):
#
#        prev_theta=[previous_theta_x,previous_theta_t]
#        newlengthscale,newlengthscale_t=self.optimize_lengthscale_SE_maximizing(prev_theta,noise_delta)
#        self.hyper['lengthscale_x']=newlengthscale
#        self.hyper['lengthscale_t']=newlengthscale_t
#        
#        # refit the model
#        temp=np.hstack((self.X,self.T))
#        ur = unique_rows(temp)
#        
#        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)
#        
#        return newlengthscale,newlengthscale_t
            
    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):
        # optimize both GP lengthscale and logistic hyperparameter

            
        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]
        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)
        self.hyper['lengthscale_x']=newlengthscale
        self.hyper['lengthscale_t']=newlengthscale_t
        
        # refit the model
        temp=np.hstack((self.X,self.T))
        ur = unique_rows(temp)

        # update Y here
        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])
        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)
        self.Y=Y
        #
        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)
        
        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth


    def compute_var(self,X,T,xTest,tTest):
        """
        compute variance given X and xTest
        
        Input Parameters
        ----------
        X: the observed points
        xTest: the testing points 
        
        Returns
        -------
        diag(var)
        """ 
        
        xTest=np.asarray(xTest)
        xTest=np.atleast_2d(xTest)
        
        tTest=np.asarray(tTest)
        tTest=np.atleast_2d(tTest)
        tTest=np.reshape(tTest,(-1,1))
        
        if self.kernel_name=='SE':
            #Euc_dist=euclidean_distances(xTest,xTest)
            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta
            #ur = unique_rows(X)
            myX=X
            myT=T
            
            Euc_dist_x=euclidean_distances(myX,myX)
            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta
        
            Euc_dist_t=euclidean_distances(myT,myT)
            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      
        
            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\
                +np.eye(len(myX))*self.noise_delta
                    
                 
            Euc_dist_test_train_x=euclidean_distances(xTest,X)
            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])
            
            Euc_dist_test_train_t=euclidean_distances(tTest,T)
            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])
            
            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])
                
        try:
            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)
        except:
            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)
            temp=temp[0]
            
        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)
        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)
        var=np.diag(var)
        var.flags['WRITEABLE']=True
        var[var<1e-100]=0
        return var 

    
        
    def predict(self,xTest, eval_MSE=True):
        """
        compute predictive mean and variance
        Input Parameters
        ----------
        xTest: the testing points 
        
        Returns
        -------
        mean, var
        """    

        if len(xTest.shape)==1: # 1d
            xTest=xTest.reshape((-1,self.X.shape[1]+1))
            
        tTest=xTest[:,-1]
        tTest=np.atleast_2d(tTest)
        tTest=np.reshape(tTest,(xTest.shape[0],-1))
        
        xTest=xTest[:,:-1]
        
        # prevent singular matrix
        temp=np.hstack((self.X,self.T))
        ur = unique_rows(temp)
        
        X=self.X[ur]
        T=self.T[ur]
                
        Euc_dist_x=euclidean_distances(xTest,xTest)
        Euc_dist_t=euclidean_distances(tTest,tTest)

        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\
            +np.eye(xTest.shape[0])*self.noise_delta
        
        Euc_dist_test_train_x=euclidean_distances(xTest,X)
        
        Euc_dist_test_train_t=euclidean_distances(tTest,T)
        
        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])
            
        #Exp_dist_test_train_x*Exp_dist_test_train_t
  
        # using Cholesky update
        mean=np.dot(KK_xTest_xTrain,self.alpha)
        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)
        var=KK_xTest_xTest-np.dot(v.T,v)
        

        return mean.ravel(),np.diag(var)  

    def posterior(self,x):
        # compute mean function and covariance function
        return self.predict(self,x)
        
    

File Path: bayes_opt/sequentialBO/__init__.py
Content:

File Path: bayes_opt/sequentialBO/bo_logistic.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Tue Mar 29 11:49:58 2016

"""


import numpy as np
from bayes_opt.acquisition_functions import AcquisitionFunction
from bayes_opt import GaussianProcess
from bayes_opt import ProductGaussianProcess

from bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs, acq_max
import time
from bayes_opt.utility.basic_utility_functions import transform_logistic
from sklearn.preprocessing import MinMaxScaler


#@author: Vu

#======================================================================================================
#======================================================================================================
#======================================================================================================
#======================================================================================================
counter = 0


class BO_L(object):

    def __init__(self, func, SearchSpace,acq_name="ei_mu_max",verbose=1):
        """      
        BO_L: we perform Bayes Opt using Logistic transformation at the MaxEpoch
        ----------
        
        func:                       a function to be optimized
        SearchSpace:                bounds on parameters        
        acq_name:                   acquisition function name, such as [ei, gp_ucb]
                           
        Returns
        -------
        dim:            dimension
        SearchSpace:         SearchSpace on original scale
        scaleSearchSpace:    SearchSpace on normalized scale of 0-1
        time_opt:       will record the time spent on optimization
        gp:             Gaussian Process object
        """

        self.method='bo_l'
        self.verbose=verbose
        if isinstance(SearchSpace,dict):
            # Get the name of the parameters
            self.keys = list(SearchSpace.keys())
            
            self.SearchSpace = []
            for key in list(SearchSpace.keys()):
                self.SearchSpace.append(SearchSpace[key])
            self.SearchSpace = np.asarray(self.SearchSpace)
        else:
            self.SearchSpace=np.asarray(SearchSpace)
            
        # we will performa BO at the Max Iteration, thus we set the SearchSpace of Epoch to the Max Value
        self.SearchSpace[-1,0]=self.SearchSpace[-1,1]
            
        self.dim = len(SearchSpace)

        scaler = MinMaxScaler()
        scaler.fit(self.SearchSpace.T)
        
        scalerT = MinMaxScaler()
        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T
        scalerT.fit(SearchSpace_T)

        self.Xscaler=scaler
        self.Tscaler=scalerT

        # create a scaleSearchSpace 0-1
        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T
                
        # function to be optimised
        self.f = func
    
        # store X in original scale
        self.X_ori= None

        # store X in 0-1 scale
        self.X = None
        
        # store y=f(x)
        # (y - mean)/(max-min)
        self.Y = None
               
        # y original scale
        self.Y_ori = None
        
        # store the number of episode
        self.T=None
        self.T_original=None
        
        # store the cost original scale
        self.Y_cost_original=None
        
        
        self.time_opt=0
         
        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]


        # acquisition function
        self.acq_name = acq_name
        self.logmarginal=0

        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=self.verbose)


#        # store the curves of performances
        self.Y_curves=[]

       
    # will be later used for visualization
    def posterior(self, Xnew):
        self.gp.fit(self.X, self.Y)
        mu, sigma2 = self.gp.predict(Xnew, eval_MSE=True)
        return mu, np.sqrt(sigma2)
    
        
    def init(self, n_init_points=3, seed=1):
        """      
        Input parameters
        ----------
        gp_params:            Gaussian Process structure      
        n_init_points:        # init points
        """
        np.random.seed(seed)

        # Generate random points
        SearchSpace=np.copy(self.SearchSpace)
        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter


        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace]        #l=[np.linspace(x[0],x[1],num=n_init_points) for x in self.init_SearchSpace]

        # Concatenate new random points to possible existing
        # points from self.explore method.
        temp=np.asarray(l)
        temp=temp.T
        init_X=list(temp.reshape((n_init_points,-1)))
        
        self.X_original = np.asarray(init_X)
        self.T_original=self.X_original[:,-1]
        self.T_original=np.reshape(self.T_original,(n_init_points,-1))
        
        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode
        self.X_original=np.reshape(self.X_original,(n_init_points,-1))

        # Evaluate target function at all initialization           
        y_init_curves, y_init_cost=self.f(init_X)
        #y_init_curves=self.f(init_X)
        #y_init_cost=y_init_curves
        
        
        self.Y_curves+=y_init_curves

        # we transform the y_init_curves as the average of [ curves * logistic ]
        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\
                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])
        #y_init=y_init_curves
        y_init=np.reshape(y_init,(n_init_points,1))
        
     
        self.Y_original = np.asarray(y_init)      
        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))

        self.Y_original_maxGP=np.asarray(y_init)      

        # convert it to scaleX
        self.X = self.Xscaler.transform(init_X)
        self.X=self.X[:,:-1]#remove the last dimension of MaxEpisode
        self.X=np.reshape(self.X,(n_init_points,-1))

        #temp=(self.T_original-self.SearchSpace[-1,0])/self.max_min_gap[-1]
        #self.T = np.asarray(temp)
        self.T = self.Tscaler.transform(self.T_original)

        self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)



        
        
    def suggest_nextpoint(self): # logistic, time-cost, virtual
        """
        Main optimization method.

        Input parameters
        ----------
        gp_params: parameter for Gaussian Process

        Returns
        -------
        x: recommented point for evaluation
        """
     
        
        if self.acq_name=='random':
            x_max = [np.random.uniform(x[0], x[1], size=1) for x in self.SearchSpace]
            x_max=np.asarray(x_max)
            x_max=x_max.T
            self.X_original=np.vstack((self.X_original, x_max))
            # evaluate Y using original X
            self.Y_original = np.append(self.Y_original, self.f(x_max))
            
            # update Y after change Y_original
            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)
            
            self.time_opt=np.hstack((self.time_opt,0))
            return

        # init a new Gaussian Process
        #self.gp=ProductGaussianProcess(self.gp_params)
        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=self.verbose)

        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)

        # optimize GP parameters after 3*d iterations
        if  len(self.Y)%(3*self.dim)==0:

            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \
                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]
            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)
            
            self.gp.hyper['lengthscale_x']=newlengthscale_x
            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']
            self.gp.logistic_hyper['midpoint']=new_midpoint
            self.gp.logistic_hyper['growth']=new_growth
            if self.verbose:
                print("estimated lengthscale_x={}, estimated lengthscale_t={}".format(
                    newlengthscale_x,newlengthscale_t))
   
        # Set acquisition function
        start_opt=time.time()

        # linear regression is used to fit the cost - alternatively we can use GP
        

        acq={}
        acq['name']=self.acq_name
        acq['dim']=self.scaleSearchSpace.shape[0]
        acq['scaleSearchSpace']=self.scaleSearchSpace   
        
        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent
            
            # optimie the GP predictive mean function to find the max of mu
            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)
            acq['mu_max']=  mu_max_val
        
        scaleSearchSpace=np.copy(self.scaleSearchSpace)
        scaleSearchSpace[-1,0]=scaleSearchSpace[-1,1] # last dimension, set it to MaxIter
        
        #x_max_temp=acq_max_with_name(gp=self.gp,scaleSearchSpace=scaleSearchSpace,acq_name=self.acq_name)
        
        
        myacq=AcquisitionFunction(acq)
        #x_max = acq_max(ac=self.acq_func.acq_kind,gp=self.gp,bounds=self.scaleSearchSpace,opt_toolbox=self.opt_toolbox,seeds=self.xstars)
        x_max_temp = acq_max(ac=myacq.acq_kind,gp=self.gp,bounds=self.scaleSearchSpace)

#        x_max_temp = acq_min_scipy_kwargs(myfunc=myacq.acq_kind,bounds=self.scaleSearchSpace,
#                        acq_func=myacq, isDebug=False)
        
        
        #x_max_temp=self.acq_utility_cost()
        x_max=x_max_temp[:-1]
        x_max_t=x_max_temp[-1]       
      
        
        # record the optimization time
        finished_opt=time.time()
        elapse_opt=finished_opt-start_opt
        self.time_opt=np.hstack((self.time_opt,elapse_opt))

        # store X                                     
        self.X = np.vstack((self.X, x_max.reshape((1, -1))))
        self.T = np.vstack((self.T, x_max_t.reshape((1, -1))))


        # compute X in original scale
        temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]
        self.X_original=np.vstack((self.X_original, temp_X_new_original))
        
        temp_T_new_original=x_max_t*self.max_min_gap[-1]+self.SearchSpace[-1,0]
        self.T_original=np.vstack((self.T_original, temp_T_new_original))

        # evaluate Y using original X
        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]

        y_original_curves, y_cost_original= self.f(x_original_to_test)
        
        #y_original_curves= self.f(x_original_to_test)
        #y_cost_original=y_original_curves
        
        y_original=transform_logistic(y_original_curves,self.gp.logistic_hyper['midpoint'],\
                                      self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])
        #y_original=y_original_curves
        self.Y_curves.append(y_original_curves)
      
        
        self.Y_original = np.append(self.Y_original,y_original)
        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)

        # update Y after change Y_original        
        self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)
            
        #self.Y_cost=(self.Y_cost_original-np.mean(self.Y_cost_original))/np.std(self.Y_cost_original)
        
        if self.verbose:
            print("x={} t={} current y={:.4f}, ybest={:.4f}".format(self.X_original[-1],self.T_original[-1],self.Y_original[-1],self.Y_original.max()))



File Path: bayes_opt/sequentialBO/boil.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Tue Mar 29 11:49:58 2016

"""


import numpy as np
from bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows
from bayes_opt import GaussianProcess
from bayes_opt import ProductGaussianProcess

from bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs
import time
from sklearn import linear_model
import copy
from bayes_opt.curve_compression import transform_logistic
from sklearn.preprocessing import MinMaxScaler


#======================================================================================================
#======================================================================================================
#======================================================================================================
#======================================================================================================
counter = 0


class BOIL(object):

    #def __init__(self, gp_params, func_params, acq_params, verbose=True):
    def __init__(self, func, SearchSpace,acq_name="ei_mu_max",verbose=1):

        """      
        Input parameters
        ----------
        
        gp_params:                  GP parameters
        gp_params.theta:            to compute the kernel
        gp_params.delta:            to compute the kernel
        
        func_params:                function to optimize
        func_params.init bound:     initial SearchSpace for parameters
        func_params.SearchSpace:        SearchSpace on parameters        
        func_params.func:           a function to be optimized
        
        
        acq_params:            acquisition function, 
        acq_params.acq_func['name']=['ei','ucb','poi']
        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'
                            
        Returns
        -------
        dim:            dimension
        SearchSpace:         SearchSpace on original scale
        scaleSearchSpace:    SearchSpace on normalized scale of 0-1
        time_opt:       will record the time spent on optimization
        gp:             Gaussian Process object
        """
        
        self.method='boil'
        self.verbose=verbose
        if isinstance(SearchSpace,dict):
            # Get the name of the parameters
            self.keys = list(SearchSpace.keys())
            
            self.SearchSpace = []
            for key in list(SearchSpace.keys()):
                self.SearchSpace.append(SearchSpace[key])
            self.SearchSpace = np.asarray(self.SearchSpace)
        else:
            self.SearchSpace=np.asarray(SearchSpace)
            
            
        self.dim = len(SearchSpace)

        scaler = MinMaxScaler()
        scaler.fit(self.SearchSpace[:-1,:].T)
        
        scalerT = MinMaxScaler()
        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T
        scalerT.fit(SearchSpace_T)

        self.Xscaler=scaler
        self.Tscaler=scalerT

        # create a scaleSearchSpace 0-1
        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T
                
        # function to be optimised
        self.f = func
    
        # store X in original scale
        self.X_ori= None

        # store X in 0-1 scale
        self.X = None
        
        # store y=f(x)
        # (y - mean)/(max-min)
        self.Y = None
               
        # y original scale
        self.Y_ori = None
        
        # store the number of episode
        self.T=None
        self.T_original=None
        
        # store the cost original scale
        self.Y_cost_original=None
        
        self.time_opt=0
         
        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]


        # acquisition function
        self.acq_name = acq_name
        self.logmarginal=0

        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)

        # store the curves of performances
        self.Y_curves=[]
        
        # store the cost original scale
        self.Y_cost_original=None
        
        self.time_opt=0
        
        # acquisition function
        self.acq_func = None
   
        self.logmarginal=0
        
        self.markVirtualObs=[]
        
        self.countVirtual=[]

        self.linear_regression = linear_model.LinearRegression()

        self.condition_number=[]
        
        # maximum number of augmentations
        self.max_n_augmentation=10
        self.threshold_cond=15
        
    def init(self, n_init_points=3, seed=1):
        """      
        Input parameters
        ----------
        n_init_points:        # init points
        """
        np.random.seed(seed)

        # Generate random points
        SearchSpace=np.copy(self.SearchSpace)
        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter

        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] 

        # Concatenate new random points to possible existing
        # points from self.explore method.
        temp=np.asarray(l)
        temp=temp.T
        init_X=list(temp.reshape((n_init_points,-1)))
        
        self.X_original = np.asarray(init_X)
        self.T_original=self.X_original[:,-1]
        self.T_original=np.reshape(self.T_original,(n_init_points,-1))
        
        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode
        self.X_original=np.reshape(self.X_original,(n_init_points,-1))

        # Evaluate target function at all initialization           
        y_init_curves, y_init_cost=self.f(init_X)

        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')

        self.Y_curves+=y_init_curves

        # we transform the y_init_curves as the average of [ curves * logistic ]
        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\
                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])
        #y_init=y_init_curves
        y_init=np.reshape(y_init,(n_init_points,1))
        
        # record keeping ========================================================
        self.Y_original = np.asarray(y_init)      
        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))

        # convert it to scaleX
        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode
        #self.X=self.X[:,:-1]
        self.X=np.reshape(self.X,(n_init_points,-1))

        self.T = self.Tscaler.transform(self.T_original)

        self.markVirtualObs+=[0]*n_init_points

        # generating virtual observations for each initial point
        for ii in range(n_init_points):
            self.generating_virtual_observations(self.X[ii,:],
                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)

        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))

        if np.std(self.Y_original)==0:
            self.Y=(self.Y_original-np.mean(self.Y_original))
        else:
            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)

       
    def utility_cost_evaluation(self,x,acq_func,isDebug=False):
        # this is a wrapper function to evaluate at multiple x(s)
        
        
        def utility_cost_evaluation_single(x,acq_func,isDebug=False):
            # given a location x, we will evaluate the utility and cost
            
            utility=acq_func.acq_kind(x,gp=self.gp)
            
            try:
                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))
                
            except:
                print(x)
                print("bug")
    
            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost
            
            #acquisition_function_value= utility_normalized/cost_normalized
            if 'ei' in acq_func.acq_name:
                acquisition_function_value= np.log(utility)-np.log(mean_cost)
            else:
                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))
    
            if isDebug==True:
                print("acq_func at the selected point \t utility:",np.round(utility,decimals=4),"\t cost:",mean_cost)
                if utility==0:
                    print("utility =0===============================================================================")
       
            return acquisition_function_value*(-1) # since we will minimize this acquisition function
        
        
        if len(x)==self.dim: # one observation
            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)
            if isDebug==True:
                return temp
            else:
                utility=np.mean(temp)
        
        else: # multiple observations
            utility=[0]*len(x)
            for idx,val in enumerate(x):
                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)
                                                     
                utility[idx]=np.mean(temp)
                
            utility=np.asarray(utility)    				               
        return utility   
    
        
    def acq_utility_cost(self):
        
        # generate a set of x* at T=MaxIter
        # instead of running optimization on the whole space, we will only operate on the region of interest
        # the region of interest in DRL is where the MaxEpisode
    
        # we find maximum of EI

        acq={}
        acq['name']=self.acq_name
        acq['dim']=self.scaleSearchSpace.shape[0]
        acq['scaleSearchSpace']=self.scaleSearchSpace   
    
        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent
            
            # optimie the GP predictive mean function to find the max of mu
            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)
            acq['mu_max']=  mu_max_val

        myacq=AcquisitionFunction(acq)
        
        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,
                        acq_func=myacq, isDebug=False)
        
        if self.verbose==True:
            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)
            print("selected point from acq func:",np.round(x_min,decimals=4),"acq val=log(Utility/Cost)=",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func
            if np.round(acq_val,decimals=4)==0:
                print("acq value =0")
            
        return x_min
    
    
    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):
        # this function will select a list of informative locations to place a virtual obs
        # x_max is the selected hyperparameter
        # t_max is the selected number of epochs to train
        
        
        SearchSpace=np.copy(self.scaleSearchSpace)
        for dd in range(self.dim-1):
            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]
            
        SearchSpace[-1,1]=t_max
        
        temp_X,temp_T=self.X.copy(),self.T.copy()
        temp_gp=copy.deepcopy(self.gp )
        
        temp_Y=np.random.random(size=(len(temp_T),1))
        
        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)
        
        new_batch_T=None

        pred_var_value=[0]*n_virtual_obs
        for ii in range(n_virtual_obs):
            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,
                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)
            
            # stop augmenting if the uncertainty is smaller than a threshold
            # or stop augmenting if the uncertainty is smaller than a threshold

            log_cond=np.log( temp_gp.compute_condition_number() )
            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):
                break
          
            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting
                break
            
            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x
            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t
            temp_gp.X,temp_gp.T=temp_X,temp_T
            temp_Y=np.random.random(size=(len(temp_T),1))
            
            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)

            if new_batch_T is None:
                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))
            else:
                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))
        
#        if self.verbose:
#            print("pred_var_value at the augmented points:",np.round( pred_var_value,decimals=4))

        if new_batch_T is None:
            return [],0

        else:
            output=np.sort(new_batch_T.ravel()).tolist()
            return output, len(output)

    
    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):
        
        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]
        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))

        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points
        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)
        if max_n_virtual_obs==0:
            self.countVirtual.append(0)
            return
        
        if IsRandom==True:# select informative locations by random uniform   
            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]
        else:
            # select informative locations by uncertainty as in the paper
            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        
            
        self.countVirtual.append(n_virtual_obs)
        
        if self.verbose:
            np.set_printoptions(suppress=True)
            print("Max #augmented points",max_n_virtual_obs, "\t #augmented points ",len(l),
                  "\t Augmented points: ",np.round(l,decimals=3))
            
        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]
        #l_original=[self.Tscaler.inverse_transform(val) for val in l]
                           
        virtual_obs_t_original=np.asarray(l_original).T
        virtual_obs_t=np.asarray(l).T
        
        # compute y_original for the virtual observations
        y_virtual_original=[0]*n_virtual_obs
        for ii in range(n_virtual_obs):
            
            idx=np.int(virtual_obs_t_original[ii])
            
            temp_curve=y_original_curves[0][:idx+1]
            self.markVirtualObs.append(1)

            y_virtual_original[ii]=transform_logistic([temp_curve],\
                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])
           
            self.X = np.vstack((self.X, x_max.reshape((1, -1))))
            self.X_original=np.vstack((self.X_original, temp_X_new_original))
        
            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))
            temp=np.asarray(virtual_obs_t_original[ii])
            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))


            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])
            self.Y_curves.append(temp_curve)
            
            # interpolating the cost for augmented observation
            y_cost_estimate=y_cost_original*virtual_obs_t[ii]
            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])
            
        
#        if self.verbose:
#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\
#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])
#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))
#            
        
    def suggest_nextpoint(self): # logistic, time-cost, virtual
        """
        Main optimization method.


        Returns
        -------
        x: recommented point for evaluation
        """
 
        # init a new Gaussian Process============================================
        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)
        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)
            
        # we store the condition number here=====================================
        self.condition_number.append(self.gp.cond_num)
        if self.verbose:
            print("ln of conditioning number of GP covariance matrix", np.round(np.log(self.gp.cond_num),decimals=1))

        # count number of real observations
        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)
        count=np.int(count)

        # optimize GP hyperparameters and Logistic hyper after 3*d iterations
        if  len(self.Y)%(2*self.dim)==0:

            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \
                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]
            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)
            
            self.gp.hyper['lengthscale_x']=newlengthscale_x
            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']
            self.gp.logistic_hyper['midpoint']=new_midpoint
            self.gp.logistic_hyper['growth']=new_growth
          
            if self.verbose:
                print("==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}".format(
                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))
                
        # Set acquisition function
        start_opt=time.time()

        # linear regression is used to fit the cost
        # fit X and T
        combine_input=np.hstack((self.X,self.T))
        self.linear_regression.fit(combine_input,self.Y_cost)
        
        # maximize the acquisition function to select the next point =================================
        x_max_temp=self.acq_utility_cost()
        x_max=x_max_temp[:-1]
        t_max=x_max_temp[-1]       
            
        # record keeping stuffs ====================================================
        # record the optimization time
        finished_opt=time.time()
        elapse_opt=finished_opt-start_opt
        self.time_opt=np.hstack((self.time_opt,elapse_opt))

        # this is for house keeping stuff        
        self.markVirtualObs.append(0)

        self.X = np.vstack((self.X, x_max.reshape((1, -1))))
        self.T = np.vstack((self.T, t_max.reshape((1, -1))))

        # compute X in original scale
        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))
        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]
        self.X_original=np.vstack((self.X_original, temp_X_new_original))
        
        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]
        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))
        self.T_original=np.vstack((self.T_original, temp_T_new_original))

        # evaluate Y using original X
        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]

        # evaluate the black-box function=================================================
        y_original_curves, y_cost_original= self.f(x_original_to_test)
        
        # compute the utility score by transformation
        y_original=transform_logistic(y_original_curves,\
              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])
        
        if len(y_original_curves)==1: # list
            self.Y_curves.append(y_original_curves[0])
        else:
            self.Y_curves.append(y_original_curves)

        
        self.Y_original = np.append(self.Y_original,y_original)
        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)

        # augmenting virtual observations =====================================================
        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])
        
        # update Y after change Y_original        
        if np.std(self.Y_original)==0:
            self.Y=(self.Y_original-np.mean(self.Y_original))
        else:
            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)
            
        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))
                    
        #if self.verbose:
        np.set_printoptions(suppress=True)

        print("[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}".format( np.round(self.X_original[-1],decimals=4),\
              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))



File Path: bayes_opt/test_functions/__init__.py
Content:
#from pes_acquisition_function import PES_AcquisitionFunction


#__all__ = ["PES_AcquisitionFunction"]
File Path: bayes_opt/test_functions/cnn/cifar-10-batches-py/process_cnn_cifar.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Sat Jan 11 10:40:31 2020

@author: Lenovo
"""
import numpy as np
import pickle


def unpickle(file):
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict


label=[]
data=[]

vu=unpickle("data_batch_1")
label=label+vu[b'labels']
data=vu[b'data']

"""
vu=unpickle("data_batch_2")
label=label+vu[b'labels']
data=np.vstack((data,vu[b'data']))

vu=unpickle("data_batch_3")
label=label+vu[b'labels']
data=np.vstack((data,vu[b'data']))

vu=unpickle("data_batch_4")
label=label+vu[b'labels']
data=np.vstack((data,vu[b'data']))

vu=unpickle("data_batch_5")
label=label+vu[b'labels']
data=np.vstack((data,vu[b'data']))
"""


indices = np.arange(len(label))
np.random.shuffle(indices)

data=np.reshape(data,(len(label),32,32,3))

label=np.asarray(label)
#label=np.reshape(label,(-1,1))


label=label[indices]
data=data[indices]

#label=label[:1000]
#data=data[:1000]

vu_test=unpickle("test_batch")

label_test=vu_test[b'labels']
data_test=vu_test[b'data']

data_test=np.reshape(data_test,(10000,32,32,3))


label_test=np.asarray(label_test)
#label_test=np.reshape(label_test,(-1,1))


with open('cifar10.pickle', 'wb') as f:
    pickle.dump([data, label,data_test,label_test], f)
    
with open('cifar10.pickle', 'rb') as f:
    X_train, y_train, X_test,y_test = pickle.load(f)
File Path: bayes_opt/test_functions/cnn/cnn_tf_cifar10_blackbox.py
Content:
# -*- coding: utf-8 -*-

from __future__ import print_function, division
from builtins import range
# Note: you may need to update your version of future
# sudo pip install -U future

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

#from datetime import datetime
#from scipy.signal import convolve2d
#from scipy.io import loadmat
from sklearn.utils import shuffle
import time
#from benchmark import get_data, error_rate
#import pickle


def error_rate(p, t):
    return np.mean(p != t)

def convpool(X, W, b):
    # just assume pool size is (2,2) because we need to augment it with 1s
    conv_out = tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding='SAME')
    conv_out = tf.nn.bias_add(conv_out, b)
    pool_out = tf.nn.max_pool(conv_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    return tf.nn.relu(pool_out)


def init_filter(shape, poolsz):
    # w = np.random.randn(*shape) * np.sqrt(2) / np.sqrt(np.prod(shape[:-1]) + shape[-1]*np.prod(shape[:-2]) / np.prod(poolsz))
    w = np.random.randn(*shape) * np.sqrt(2.0 / np.prod(shape[:-1]))
    return w.astype(np.float32)


def rearrange(X):
    # input is (32, 32, 3, N)
    # output is (N, 32, 32, 3)
    # N = X.shape[-1]
    # out = np.zeros((N, 32, 32, 3), dtype=np.float32)
    # for i in xrange(N):
    #     for j in xrange(3):
    #         out[i, :, :, j] = X[:, :, j, i]
    # return out / 255
    return (X.transpose(3, 0, 1, 2) / 255).astype(np.float32)


def run_cnn_evaluation_cifar(x,Xtrain,Ytrain,Xtest,Ytest,isReturnAcc=False):
    
    tf.reset_default_graph()
    starttime = time.time()

    
    filter_sz,pool_sz,batch_sz,lr,momentum,decay,max_iter=x
    filter_sz=np.int(filter_sz)
    pool_sz=np.int(pool_sz)
    batch_sz=np.int(batch_sz)
    max_iter=np.int(max_iter)
    #lr=0.0001
    #momentum=0.9
    #decay=0.99


    # gradient descent params
    #max_iter = 200
    print_period = 50
    N = Xtrain.shape[0]
    #batch_sz = 1000
    n_batches = N // batch_sz


    
    #with open('house_number_data.pkl','wb') as f:
        #pickle.dump([Xtrain,Ytrain,Xtest,Ytest], f)
    # print "Xtest.shape:", Xtest.shape
    # print "Ytest.shape:", Ytest.shape

    # initial weights
    M = 500
    K = 10
    #filter_sz=5
    #pool_sz=2
    poolsz=(pool_sz,pool_sz)
    #poolsz = (2, 2)

    W1_shape = (filter_sz, filter_sz, 3, 20) # (filter_width, filter_height, num_color_channels, num_feature_maps)
    W1_init = init_filter(W1_shape, poolsz)
    b1_init = np.zeros(W1_shape[-1], dtype=np.float32) # one bias per output feature map

    W2_shape = (filter_sz, filter_sz, 20, 50) # (filter_width, filter_height, old_num_feature_maps, num_feature_maps)
    W2_init = init_filter(W2_shape, poolsz)
    b2_init = np.zeros(W2_shape[-1], dtype=np.float32)

    # vanilla ANN weights
    W3_init = np.random.randn(W2_shape[-1]*8*8, M) / np.sqrt(W2_shape[-1]*8*8 + M)
    b3_init = np.zeros(M, dtype=np.float32)
    W4_init = np.random.randn(M, K) / np.sqrt(M + K)
    b4_init = np.zeros(K, dtype=np.float32)


    # define variables and expressions
    # using None as the first shape element takes up too much RAM unfortunately
    X = tf.placeholder(tf.float32, shape=(batch_sz, 32, 32, 3), name='X')
    T = tf.placeholder(tf.int32, shape=(batch_sz,), name='T')
    W1 = tf.Variable(W1_init.astype(np.float32))
    b1 = tf.Variable(b1_init.astype(np.float32))
    W2 = tf.Variable(W2_init.astype(np.float32))
    b2 = tf.Variable(b2_init.astype(np.float32))
    W3 = tf.Variable(W3_init.astype(np.float32))
    b3 = tf.Variable(b3_init.astype(np.float32))
    W4 = tf.Variable(W4_init.astype(np.float32))
    b4 = tf.Variable(b4_init.astype(np.float32))

    Z1 = convpool(X, W1, b1)
    Z2 = convpool(Z1, W2, b2)
    Z2_shape = Z2.get_shape().as_list()
    Z2r = tf.reshape(Z2, [Z2_shape[0], np.prod(Z2_shape[1:])])
    Z3 = tf.nn.relu( tf.matmul(Z2r, W3) + b3 )

    #Z3_dropout = tf.layers.dropout(
      #inputs=Z3, rate=dropout_rate, training=mode == tf.estimator.ModeKeys.TRAIN)

    Yish = tf.matmul(Z3, W4) + b4

    
    cost = tf.reduce_sum(
        tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=Yish,
            labels=T
        )
    )

    train_op = tf.train.RMSPropOptimizer(lr, decay, momentum).minimize(cost)

    # we'll use this to calculate the error rate
    predict_op = tf.argmax(Yish, 1)

    Accuracy_Curve,Cost_Curve=[],[]

    init = tf.global_variables_initializer()
        
    with tf.Session() as session:
        session.run(init)

        for i in range(max_iter):
            myacc,mycost=[],[]
            for j in range(n_batches):
			
                Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]
                Ybatch = Ytrain[j*batch_sz:(j*batch_sz + batch_sz),]

                if len(Xbatch) == batch_sz:
                    session.run(train_op, feed_dict={X: Xbatch, T: Ybatch})
                    if j % print_period == 0:
                        # due to RAM limitations we need to have a fixed size input
                        # so as a result, we have this ugly total cost and prediction computation
                        test_cost = 0
                        prediction = np.zeros(len(Xtest))
                        for k in range(len(Xtest) // batch_sz):
                            Xtestbatch = Xtest[k*batch_sz:(k*batch_sz + batch_sz),]
                            Ytestbatch = Ytest[k*batch_sz:(k*batch_sz + batch_sz),]
                            test_cost += session.run(cost, feed_dict={X: Xtestbatch, T: Ytestbatch})
                            prediction[k*batch_sz:(k*batch_sz + batch_sz)] = session.run(
                                predict_op, feed_dict={X: Xtestbatch})
                        err = error_rate(prediction, Ytest)
                        #print("Cost / err at iteration i=%d, j=%d: %.3f / %.3f" % (i, j, test_cost, err))
                        #LL.append(test_cost)
                        myacc.append(1-err)
                        mycost.append((-1)*test_cost)
			
            Accuracy_Curve.append(np.mean(myacc))
            Cost_Curve.append(np.mean(mycost))


    endtime=time.time()
    elapse=endtime-starttime

    if isReturnAcc==False:
        return np.asarray(Cost_Curve),elapse
    else:
        return np.asarray(Accuracy_Curve),elapse
            
        


if __name__ == '__main__':
    
	#filter_sz,pool_sz,batch_sz,lr,momentum,decay,max_iter=x

    import pickle
    with open('cifar-10-batches-py//cifar10.pickle','rb') as f:
        temp = pickle.load(f)
        
    [Xtrain,Ytrain,Xtest,Ytest]=temp
        
    
    x=[5,2,500,1e-5,0.9,0.9,150]
    curve,elapse=run_cnn_evaluation_cifar(x,Xtrain,Ytrain,Xtest,Ytest,isReturnAcc=True)
    print(elapse)
    plt.plot(curve)

File Path: bayes_opt/test_functions/cnn/cnn_tf_housenumber_blackbox.py
Content:
# -*- coding: utf-8 -*-

from __future__ import print_function, division
from builtins import range
# Note: you may need to update your version of future
# sudo pip install -U future

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

#from datetime import datetime
#from scipy.signal import convolve2d
#from scipy.io import loadmat
from sklearn.utils import shuffle
import time
#from benchmark import get_data, error_rate
#import pickle


def error_rate(p, t):
    return np.mean(p != t)

def convpool(X, W, b):
    # just assume pool size is (2,2) because we need to augment it with 1s
    conv_out = tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding='SAME')
    conv_out = tf.nn.bias_add(conv_out, b)
    pool_out = tf.nn.max_pool(conv_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    return tf.nn.relu(pool_out)


def init_filter(shape, poolsz):
    # w = np.random.randn(*shape) * np.sqrt(2) / np.sqrt(np.prod(shape[:-1]) + shape[-1]*np.prod(shape[:-2]) / np.prod(poolsz))
    w = np.random.randn(*shape) * np.sqrt(2.0 / np.prod(shape[:-1]))
    return w.astype(np.float32)


def rearrange(X):
    # input is (32, 32, 3, N)
    # output is (N, 32, 32, 3)
    # N = X.shape[-1]
    # out = np.zeros((N, 32, 32, 3), dtype=np.float32)
    # for i in xrange(N):
    #     for j in xrange(3):
    #         out[i, :, :, j] = X[:, :, j, i]
    # return out / 255
    return (X.transpose(3, 0, 1, 2) / 255).astype(np.float32)


def run_cnn_evaluation(x,Xtrain,Ytrain,Xtest,Ytest,isReturnAcc=False):
    
    tf.reset_default_graph()
    starttime = time.time()

    """
    with open('house_number_data.pkl','rb') as f:
        temp = pickle.load(f)
        
    [Xtrain,Ytrain,Xtest,Ytest]=temp
    """
    
    filter_sz,pool_sz,batch_sz,lr,momentum,decay,max_iter=x
    filter_sz=np.int(filter_sz)
    pool_sz=np.int(pool_sz)
    batch_sz=np.int(batch_sz)
    max_iter=np.int(max_iter)
    #lr=0.0001
    #momentum=0.9
    #decay=0.99


    # gradient descent params
    #max_iter = 200
    print_period = 50
    N = Xtrain.shape[0]
    #batch_sz = 1000
    n_batches = N // batch_sz


    
    #with open('house_number_data.pkl','wb') as f:
        #pickle.dump([Xtrain,Ytrain,Xtest,Ytest], f)
    # print "Xtest.shape:", Xtest.shape
    # print "Ytest.shape:", Ytest.shape

    # initial weights
    M = 500
    K = 10
    #filter_sz=5
    #pool_sz=2
    poolsz=(pool_sz,pool_sz)
    #poolsz = (2, 2)

    W1_shape = (filter_sz, filter_sz, 3, 20) # (filter_width, filter_height, num_color_channels, num_feature_maps)
    W1_init = init_filter(W1_shape, poolsz)
    b1_init = np.zeros(W1_shape[-1], dtype=np.float32) # one bias per output feature map

    W2_shape = (filter_sz, filter_sz, 20, 50) # (filter_width, filter_height, old_num_feature_maps, num_feature_maps)
    W2_init = init_filter(W2_shape, poolsz)
    b2_init = np.zeros(W2_shape[-1], dtype=np.float32)

    # vanilla ANN weights
    W3_init = np.random.randn(W2_shape[-1]*8*8, M) / np.sqrt(W2_shape[-1]*8*8 + M)
    b3_init = np.zeros(M, dtype=np.float32)
    W4_init = np.random.randn(M, K) / np.sqrt(M + K)
    b4_init = np.zeros(K, dtype=np.float32)


    # define variables and expressions
    # using None as the first shape element takes up too much RAM unfortunately
    X = tf.placeholder(tf.float32, shape=(batch_sz, 32, 32, 3), name='X')
    T = tf.placeholder(tf.int32, shape=(batch_sz,), name='T')
    W1 = tf.Variable(W1_init.astype(np.float32))
    b1 = tf.Variable(b1_init.astype(np.float32))
    W2 = tf.Variable(W2_init.astype(np.float32))
    b2 = tf.Variable(b2_init.astype(np.float32))
    W3 = tf.Variable(W3_init.astype(np.float32))
    b3 = tf.Variable(b3_init.astype(np.float32))
    W4 = tf.Variable(W4_init.astype(np.float32))
    b4 = tf.Variable(b4_init.astype(np.float32))

    Z1 = convpool(X, W1, b1)
    Z2 = convpool(Z1, W2, b2)
    Z2_shape = Z2.get_shape().as_list()
    Z2r = tf.reshape(Z2, [Z2_shape[0], np.prod(Z2_shape[1:])])
    Z3 = tf.nn.relu( tf.matmul(Z2r, W3) + b3 )

    #Z3_dropout = tf.layers.dropout(
      #inputs=Z3, rate=dropout_rate, training=mode == tf.estimator.ModeKeys.TRAIN)

    Yish = tf.matmul(Z3, W4) + b4

    
    cost = tf.reduce_sum(
        tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=Yish,
            labels=T
        )
    )

    train_op = tf.train.RMSPropOptimizer(lr, decay, momentum).minimize(cost)

    # we'll use this to calculate the error rate
    predict_op = tf.argmax(Yish, 1)

    Accuracy_Curve,Cost_Curve=[],[]

    init = tf.global_variables_initializer()
        
    with tf.Session() as session:
        session.run(init)

        for i in range(max_iter):
            myacc,mycost=[],[]
            for j in range(n_batches):
			
                Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]
                Ybatch = Ytrain[j*batch_sz:(j*batch_sz + batch_sz),]

                if len(Xbatch) == batch_sz:
                    session.run(train_op, feed_dict={X: Xbatch, T: Ybatch})
                    if j % print_period == 0:
                        # due to RAM limitations we need to have a fixed size input
                        # so as a result, we have this ugly total cost and prediction computation
                        test_cost = 0
                        prediction = np.zeros(len(Xtest))
                        for k in range(len(Xtest) // batch_sz):
                            Xtestbatch = Xtest[k*batch_sz:(k*batch_sz + batch_sz),]
                            Ytestbatch = Ytest[k*batch_sz:(k*batch_sz + batch_sz),]
                            test_cost += session.run(cost, feed_dict={X: Xtestbatch, T: Ytestbatch})
                            prediction[k*batch_sz:(k*batch_sz + batch_sz)] = session.run(
                                predict_op, feed_dict={X: Xtestbatch})
                        err = error_rate(prediction, Ytest)
                        #print("Cost / err at iteration i=%d, j=%d: %.3f / %.3f" % (i, j, test_cost, err))
                        #LL.append(test_cost)
                        myacc.append(1-err)
                        mycost.append((-1)*test_cost)
			
            Accuracy_Curve.append(np.mean(myacc))
            Cost_Curve.append(np.mean(mycost))


    endtime=time.time()
    elapse=endtime-starttime

    if isReturnAcc==False:
        return np.asarray(Cost_Curve),elapse
    else:
        return np.asarray(Accuracy_Curve),elapse
            
        


if __name__ == '__main__':
    
	#filter_sz,pool_sz,batch_sz,lr,momentum,decay,max_iter=x

    x=[5,2,500,1e-4,0.9,0.9,100]
    curve,elapse=run_cnn_evaluation(x)
    print(elapse)
    plt.plot(curve)

File Path: bayes_opt/test_functions/cnn_experiments.py
Content:
# -*- coding: utf-8 -*-

import numpy as np
from collections import OrderedDict
from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score,accuracy_score
from sklearn.model_selection import train_test_split
import pickle
from bayes_opt.test_functions.cnn.cnn_tf_housenumber_blackbox import run_cnn_evaluation

from bayes_opt.test_functions.cnn.cnn_tf_cifar10_blackbox import run_cnn_evaluation_cifar

import os
#import matlab.engine
#import matlab
#eng = matlab.engine.start_matlab()
from sklearn.metrics import f1_score 
        
def reshape(x,input_dim):
    '''
    Reshapes x into a matrix with input_dim columns

    '''
    x = np.array(x)
    if x.size ==input_dim:
        x = x.reshape((1,input_dim))
    return x
  
class CNN_HouseNumber:
    '''
    Tuning Convolutional Neural Network on Housing Number dataset
    '''
    def __init__(self,  bounds=None,sd=None):
        self.input_dim = 7
        
        if bounds == None: 
            self.bounds = OrderedDict([('filter_sz',(1,8)),('pool_sz',(1,5)),('batch_sz',(16,1000)),
                                       ('lr',(1e-6,1e-2)),
            ('momentum',(0.82,0.999)),('decay',(0.92,0.99)),('MaxIter',(60,130))])
        else: 
            self.bounds = bounds
        
        self.min = [(0.)*self.input_dim]
        self.fmin = 0
        self.ismax=-1
        self.name='CNN_HouseNumber'


        path = os.path.dirname(os.path.realpath(__file__))

        with open(os.path.join(path,'cnn/house_number_data.pkl'),'rb') as f:
            temp = pickle.load(f)
        
        [self.Xtrain,self.Ytrain,self.Xtest,self.Ytest]=temp

    def func_returnAcc(self,X):
        X=np.asarray(X)
        
        if len(X.shape)==1: # 1 data point
            output=run_cnn_evaluation(X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest,isReturnAcc=True)
            acc_curve=[output[0]]
            elapse=[output[1]]
        else:
            output=np.apply_along_axis(run_cnn_evaluation,1,X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest,isReturnAcc=True)
            acc_curve=output[:,0].tolist()
            elapse=output[:,1].tolist()

        # we return -1* error_curve
        return acc_curve,elapse
    
    def func(self,X):
        X=np.asarray(X)
        
        if len(X.shape)==1: # 1 data point
            output=run_cnn_evaluation(X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest)
            error_curve=[output[0]]
            elapse=[output[1]]
        else:
            output=np.apply_along_axis(run_cnn_evaluation,1,X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest)
            error_curve=output[:,0].tolist()
            elapse=output[:,1].tolist()

        # we return -1* error_curve
        return error_curve,elapse

    
class CNN_Cifar10:
    '''
    Tuning Convolutional Neural Network on Housing Number dataset
    '''
    def __init__(self,  bounds=None,sd=None):
        self.input_dim = 7
        
        if bounds == None: 
            self.bounds = OrderedDict([('filter_sz',(1,8)),('pool_sz',(1,5)),('batch_sz',(16,1000)),
                                       ('lr',(1e-6,1e-3)),
            ('momentum',(0.82,0.999)),('decay',(0.92,0.99)),('MaxIter',(60,130))])
        else: 
            self.bounds = bounds
        
        self.min = [(0.)*self.input_dim]
        self.fmin = 0
        self.ismax=-1
        self.name='CNN_Cifar10'


        path = os.path.dirname(os.path.realpath(__file__))

        with open(os.path.join(path,'cnn/cifar10.pickle'),'rb') as f:
            temp = pickle.load(f)
        
        [self.Xtrain,self.Ytrain,self.Xtest,self.Ytest]=temp

    def func_returnAcc(self,X):
        X=np.asarray(X)
        
        if len(X.shape)==1: # 1 data point
            output=run_cnn_evaluation_cifar(X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest,isReturnAcc=True)
            acc_curve=[output[0]]
            elapse=[output[1]]
        else:
            output=np.apply_along_axis(run_cnn_evaluation_cifar,1,X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest,isReturnAcc=True)
            acc_curve=output[:,0].tolist()
            elapse=output[:,1].tolist()

        # we return -1* error_curve
        return acc_curve,elapse
    
    def func(self,X):
        X=np.asarray(X)
        
        if len(X.shape)==1: # 1 data point
            output=run_cnn_evaluation_cifar(X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest)
            error_curve=[output[0]]
            elapse=[output[1]]
        else:
            output=np.apply_along_axis(run_cnn_evaluation_cifar,1,X,self.Xtrain,self.Ytrain,self.Xtest,self.Ytest)
            error_curve=output[:,0].tolist()
            elapse=output[:,1].tolist()

        # we return -1* error_curve
        return error_curve,elapse
File Path: bayes_opt/test_functions/drl/__init__.py
Content:
# -*- coding: utf-8 -*-

#import mujoco
#import mujoco_py

File Path: bayes_opt/test_functions/drl/agents.py
Content:
import numpy as np
import tensorflow_probability as tfp
import tensorflow.compat.v1 as tf
import time as time_mod

import gym.spaces
import matplotlib.pyplot as plt
from collections import OrderedDict
from collections import namedtuple

from bayes_opt.test_functions.drl import replay_buffer as bf
from bayes_opt.test_functions.drl.models import make_net, make_dueling_dqn, make_policy_net
tf.disable_v2_behavior()

Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))


class RL_alg:
    params = OrderedDict()
    bounds = []

    def initialise(self, state_space, action_space):
        # setups the place holders for state, action, reward and termination variables
        # in discrete settings we use a 1-hot encoding
        # in continuous settings, we normalise onto the interval [-1,1]
        if isinstance(state_space, gym.spaces.Discrete):
            self.in_shape = [state_space.n]
            self.norm_state = (lambda x: np.squeeze(np.eye(state_space.n)[x], axis=0))
        else:
            self.in_shape = state_space.shape
            upper = np.array(state_space.high)
            upper[upper > 100000] = 1.0
            lower = np.array(state_space.low)
            lower[lower < -100000] = -1.0
            self.state_scale = [(upper + lower) / 2, (upper - lower) / 2]
            self.norm_state = (lambda s:
                               (np.reshape(s, (1, *state_space.shape)) - self.state_scale[0]) / self.state_scale[1])
        self.state_ph = tf.placeholder(tf.float32, shape=(None, *self.in_shape), name='obs')
        
        if isinstance(action_space, gym.spaces.Discrete):
            self.A = action_space.n
            self.action_ph = tf.placeholder(tf.int32, (None, 1), name='a')
            self.norm_act = (lambda x: x)
            self.denorm_act = (lambda x: x)
        elif isinstance(action_space, gym.spaces.Box):
            self.A = int(np.prod(action_space.shape))
            self.action_ph = tf.placeholder(tf.float32, shape=(None, self.A), name='obs')
            self.action_scale = [(action_space.high + action_space.low) / 2, (action_space.high - action_space.low) / 2]
            self.norm_act = (lambda a: (a - self.action_scale[0]) / self.action_scale[1])
            self.denorm_act = (lambda a: np.clip(a, -1, 1) * self.action_scale[1] + self.action_scale[0])
           
        self.reward_ph = tf.placeholder(tf.float32, (None, 1), name='r')
        self.done_ph = tf.placeholder(tf.float32, (None, 1), name='done')

    def set_session(self, session):
        self.session = session

    def act(self, state, time):
        action = self._act(self.norm_state(np.atleast_2d(state)), time)
        return self.denorm_act(action)

    def _act(self, state, time):
        pass

    def train(self, experience, time):
        prev_obs, action, reward, observation, done = experience
        prev_obs = self.norm_state(np.atleast_2d(prev_obs))
        observation = self.norm_state(np.atleast_2d(observation))
        action = self.norm_act(np.atleast_2d(action))
        self._train(prev_obs, action, reward, observation, done, time)

    def _train(self, prev_obs, action, reward, observation, done, time):
        pass

    def nextEpisode(self, env, time, display=False):
        observation = env.reset()
        if display:
            from IPython import display as dsp
            plt.figure(100)
            img = plt.imshow(env.render(mode='rgb_array'))
        totalreward = 0
        last_step = time_mod.time()
        for t in range(time, time + self.params["maxSteps"]):
            action = self.act(observation, t)
            prev_obs = observation
            if time_mod.time() - last_step < 0.01:          # to avoid Mujoco error
                time_mod.sleep(0.01)
            observation, reward, done, _ = env.step(action)
            last_step = time_mod.time()
            totalreward += reward
            if display:
                img.set_data(env.render(mode='rgb_array'))  # just update the data
                dsp.display(plt.gcf())
                dsp.clear_output(wait=True)
            experience = (prev_obs, action, reward, observation, done)
            self.train(experience, t)
            if done:
                break
        return totalreward, t


def schedule(maxT, t, start, end):
    if maxT == 0:
        return start
    elif t >= maxT:
        return end
    else:
        return start + (end - start) * t/maxT


class DQN(RL_alg):
    """
        DQN and subclass implementations
    """
    params = OrderedDict([
        ("DDQN", True),                 # Enables double DQN computation
        ("Duelling", True),             # Enables duelling architecture
        ("PER", 1),                     # Enables prioritisation for experience replay
                                        # 0 - Uniform, 1 - PER (SegmentTree), 2 - PER (SegmentSumTree)
        ("gamma", 0.99),                # discount factor
        ("lr", 5e-4),                   # learning rate for gradient steps
        ("adam_beta1", 0.9),
        ("adam_beta2", 0.999),
        ("adam_epsilon", 1e-08),

        ("update_rate", 100),           # interval for copying parameters to target network
        ("maxSteps", 200),              # maximum length of a  single episode
        ("buffer_size", 50000),         # 
        ("batch_sz", 64),               #
        ("architecture", [32, 32]),     # 
        ("maxExp", 10000),              # exploration duration over which epsilon decays linearly
        ("eps_0", 1.0),                 # start epsilon
        ("eps_final", 0.05),            # final epsilon
        ("alpha", 0.4),                 # "amount" of prioritisation
        ("beta_time", 10000),           # linearly annealing of importance sampling correction for prioritisation
        ("beta_0", 0.6),                #
        ("beta_final", 1.0)])           #

    bounds = OrderedDict([
        ("gamma", (0.0, 1.0)),
        ("lr", (1e-5, 1e-2)),
        ("adam_beta1", (0.8, 1.0)),
        ("adam_beta2", (0.9, 1.0)),
        ("adam_epsilon", (0.0, 1.0)),
        ("buffer_size", (100, 100000)),
        ("batch_sz", (1, 512)),
        ("maxSteps", (50, 2000)),
        ("alpha", (0.0, 1.0)),
        ("update_rate", (1, 1000)),
        ("maxExp", (0, 1000000)),
        ("eps_0", (0.0, 1.0)),
        ("eps_final", (0.0, 1.0)),
        ("beta_time", (0, 1000000)),
        ("beta_0", (0.0, 1.0)),
        ("beta_final", (0.0, 1.0))])

    def initialise(self, state_space, action_space):
        assert isinstance(action_space, gym.spaces.Discrete), "DQN does not support continuous action spaces"
        super(DQN, self).initialise(state_space, action_space)
        if self.params["PER"] == 0:
            self.memory = bf.ReplayBuffer(self.params["buffer_size"], self.params["batch_sz"])
        elif self.params["PER"] == 1:
            self.memory = bf.SumTreePERBuffer(self.params["buffer_size"], self.params["batch_sz"], self.params["alpha"])
        else:
            self.memory = bf.SumSegmentTreePERBuffer(self.params["buffer_size"],
                                                     self.params["batch_sz"], self.params["alpha"])
        
        self.optimiser = tf.train.AdamOptimizer(self.params["lr"], beta1=self.params["adam_beta1"],
                                                beta2=self.params["adam_beta2"], epsilon=self.params["adam_epsilon"])

        self.next_state_ph = tf.placeholder(tf.float32, shape=(self.params["batch_sz"], *self.in_shape), name='obs')
        self.next_a_ph = tf.placeholder(tf.int32, [self.params["batch_sz"]], name='next_a')
        self.weight_ph = tf.placeholder(tf.float32, [self.params["batch_sz"]], name='loss_weight_ph')
        
        if self.params["Duelling"]:
            self.dqn_out, dqn_params = make_dueling_dqn(self.state_ph, self.params["architecture"], self.A)
            self.target_out, target_params = make_dueling_dqn(self.next_state_ph, self.params["architecture"], self.A)
        else:
            self.dqn_out, dqn_params = make_net(self.state_ph, self.params["architecture"], final=self.A)
            self.target_out, target_params = make_net(self.next_state_ph, self.params["architecture"], final=self.A)
        self.update_target_params_ops = [t.assign(s) for s, t in zip(dqn_params, target_params)]
        
        if self.params["DDQN"]:
            self.max_q = tf.reshape(tf.gather_nd(
                self.target_out, tf.stack((tf.range(self.params["batch_sz"]), self.next_a_ph), -1)), (-1, 1))
        else:
            self.max_q = tf.reshape(tf.reduce_max(self.target_out, axis=1), (-1, 1))
        
        self.target = self.reward_ph + (1.0 - self.done_ph) * self.params["gamma"] * self.max_q
        self.gathered_outputs = tf.gather_nd(
            self.dqn_out, tf.stack((tf.reshape(tf.range(self.params["batch_sz"]), (-1, 1)), self.action_ph), -1),
            name='gathered_outputs')
        # compute huber loss
        self.new_weights = tf.abs(self.gathered_outputs - self.target, name='abs_error')
        self.loss = tf.reduce_sum(
            self.weight_ph * tf.where(self.new_weights <= 1.0, x=0.5 * tf.square(self.new_weights),
                                      y=(self.new_weights - 0.5)))
        self.train_op = self.optimiser.minimize(self.loss, var_list=dqn_params)

    def correct_params(self):
        for p in ["update_rate", "maxSteps", "buffer_size", "batch_sz", "maxExp", "beta_time"]:
            self.params[p] = int(self.params[p])
        
    def _train(self, prev_obs, action, reward, observation, done, t):
        self.memory.add(prev_obs, action, reward, observation, done)
        idxes, weights, batch = self.memory.sample(
                beta=schedule(self.params["beta_time"], t, self.params["beta_0"], self.params["beta_final"]))
    
        batch = Transition(*zip(*batch))
        next_a = np.argmax(self.session.run(self.dqn_out,
                                            feed_dict={self.state_ph: np.vstack(batch.next_state)}), axis=1)

        _, new_weights, loss = self.session.run([self.train_op, self.new_weights, self.loss],
                                                feed_dict={
                                                self.state_ph: np.vstack(batch.state),
                                                self.next_state_ph: np.vstack(batch.next_state),
                                                self.action_ph: np.vstack(batch.action),
                                                self.next_a_ph: np.array(next_a),
                                                self.reward_ph: np.vstack(batch.reward),
                                                self.done_ph: np.vstack(batch.done),
                                                self.weight_ph: weights})
        self.memory.update_priorities(idxes, np.reshape(new_weights, (-1,)))
        if t % self.params["update_rate"] == 0:
            self.session.run(self.update_target_params_ops, feed_dict={})
    
    def _act(self, state, time):
        if np.random.random() < schedule(self.params["maxExp"], time, self.params["eps_0"], self.params["eps_final"]):
            return np.random.choice(self.A)
        else:
            return np.argmax(self.session.run(self.dqn_out, feed_dict={self.state_ph: state}))


class A2C(RL_alg):
    """
        A2C implementation and subclasses
    """
    params = OrderedDict([
        ("gamma", 0.99),                    # discount factor
        ("lr_actor", 5e-4),                 # learning rate for actor gradient steps
        ("actor_beta1", 0.9),
        ("actor_beta2", 0.999),
        ("actor_epsilon", 1e-08),
        ("lr_critic", 5e-4),                # learning rate for critic gradient steps
        ("critic_beta1", 0.9),
        ("critic_beta2", 0.999),
        ("critic_epsilon", 1e-08),
        ("maxSteps", 200),                  # maximum length of a  single episode
        ("batch_sz", 1),                    # dummy for now in case we want to add multistep updates
        ("val_architecture", [32, 32]),     # architecture for the value network
        ("pol_architecture", [32, 32]),     # architecture for the policy network
        ("ent_coef", 0.01)])                  # coefficient weighting the entropy penalisation

    bounds = OrderedDict([
        ("gamma", (0.0, 1.0)),
        ("lr_actor", (1e-5, 1e-2)),
        ("actor_beta1", (0.8, 1.0)),
        ("actor_beta2", (0.9, 1.0)),
        ("actor_epsilon", (0.0, 1.0)),
        ("lr_critic", (1e-5, 1e-2)),
        ("critic_beta1", (0.8, 1.0)),
        ("critic_beta2", (0.9, 1.0)),
        ("critic_epsilon", (0.0, 1.0)),
        ("ent_coef", (0, 0.1)),
        ("maxSteps", (50, 2000))])

    def initialise(self, state_space, action_space):
        super(A2C, self).initialise(state_space, action_space)
        self.training_steps = 0
        self.R = tf.placeholder(tf.float32, (None,), name='r')
        self.Adv = tf.placeholder(tf.float32, (None,), name='adv')
        # inputs and targets
        self.val_net, val_params = make_net(self.state_ph, self.params["val_architecture"], final=1)
 
        if isinstance(action_space, gym.spaces.Discrete):
            self.actor_net, actor_params = make_policy_net(self.state_ph, self.params["pol_architecture"], self.A)
            self.policy = tf.distributions.Categorical(tf.nn.softmax(self.actor_net))
            self.action = tf.squeeze(self.policy.sample(1))
        elif isinstance(action_space, gym.spaces.Box):
            self.actor_net, self.pol_var, actor_params = \
                make_policy_net(self.state_ph, self.params["pol_architecture"], self.A, continuous=True)
            self.policy = tfp.distributions.MultivariateNormalDiag(loc=self.actor_net, scale_diag=self.pol_var)
            self.action = self.policy.sample()
        else:
            raise(Exception("This type of actionspace is not supported"))

        entropy = self.policy.entropy()
        logprobs = self.policy.log_prob(self.action_ph)
        pg_loss = tf.reduce_mean(-logprobs * self.Adv - self.params["ent_coef"]*entropy)
        vf_loss = tf.reduce_mean(tf.squared_difference(self.val_net, self.R))

        policy_opt = tf.train.AdamOptimizer(learning_rate=self.params["lr_actor"],
                                            beta1=self.params["actor_beta1"],
                                            beta2=self.params["actor_beta2"],
                                            epsilon=self.params["actor_epsilon"])
        value_opt = tf.train.AdamOptimizer(learning_rate=self.params["lr_critic"],
                                           beta1=self.params["critic_beta1"],
                                           beta2=self.params["critic_beta2"],
                                           epsilon=self.params["critic_epsilon"])
        self.vtrain_op = value_opt.minimize(vf_loss, var_list=val_params)
        self.ptrain_op = policy_opt.minimize(pg_loss, var_list=actor_params)

    def _train(self, prev_obs, action, reward, observation, done, t):
        V = self.session.run(self.val_net, feed_dict={self.state_ph: prev_obs})[0]
        R = [reward] if done else reward + self.params["gamma"] *\
                                  self.session.run(self.val_net, feed_dict={self.state_ph: observation})[0]
        advs = R-V
        self.session.run([self.vtrain_op, self.ptrain_op],
                         feed_dict={self.state_ph: prev_obs, self.action_ph: action, self.Adv: advs, self.R: R})
        
    def correct_params(self):
        for p in ["maxSteps", "batch_sz"]:
            self.params[p] = int(self.params[p])

    def _act(self, state, t):
        return self.session.run(self.action, feed_dict={self.state_ph: state})

File Path: bayes_opt/test_functions/drl/models.py
Content:
import numpy as np
#import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() 

def make_net( in_layer, architecture, final = None, run_seed = None):
        params = []
        last_layer = in_layer
        #initializer = tf.initializers.glorot_uniform()
        for desc in architecture:
            if isinstance(desc, int) or isinstance(desc[0], int):
                #new_layer = tf.layers.Dense(desc, activation = tf.nn.tanh, kernel_initializer = tf.contrib.layers.xavier_initializer(seed=run_seed) )
                new_layer = tf.layers.Dense(desc, activation = tf.nn.tanh, kernel_initializer = tf.initializers.glorot_uniform(seed=run_seed) )
            else:
                new_layer = tf.layers.Conv2D(desc[1], desc[2], strides = desc[3], activation = tf.nn.tanh, kernel_initializer = tf.contrib.layers.xavier_initializer(seed=run_seed))
            last_layer = new_layer(last_layer)
            params.extend(new_layer.weights)

        if final is not None:
            if isinstance(final, int) or isinstance(final[0], int):
                #new_layer = tf.layers.Dense(final, kernel_initializer = tf.contrib.layers.xavier_initializer(seed=run_seed) )
                new_layer = tf.layers.Dense(final, kernel_initializer = tf.initializers.glorot_uniform(seed=run_seed) )

            else:
                new_layer = tf.layers.Conv2D(final[1], final[2], strides = final[3], kernel_initializer = tf.initializers.glorot_uniform(seed=run_seed))
            last_layer = new_layer(last_layer)
            params.extend(new_layer.weights)
        return last_layer, params


def make_dueling_dqn( in_layer, architecture, actions, run_seed = None):
        # calculate output and cost
        out, params = make_net(in_layer, architecture)
        out_shape = out.get_shape().as_list()[1]
        initializer = tf.initializers.glorot_uniform(seed=0)

        fcV_W = tf.Variable(initializer(shape = [out_shape, 512]))
        fcV_b = tf.Variable(tf.zeros([512], dtype=tf.float32), dtype=tf.float32)
        val_a = tf.nn.elu(tf.matmul(out, fcV_W) + fcV_b)

        fcV2_W = tf.Variable(initializer(shape = [512, 1]))
        fcV2_b = tf.Variable(tf.zeros([1], dtype=tf.float32))
        value_out = tf.matmul(val_a, fcV2_W) + fcV2_b


        fcA_W = tf.Variable(initializer(shape = [out_shape, 512]))
        fcA_b = tf.Variable(tf.zeros([512], dtype=tf.float32))
        adv_a = tf.nn.elu(tf.matmul(out, fcA_W) + fcA_b)

        fcA2_W = tf.Variable(initializer(shape = [512, actions]))
        fcA2_b = tf.Variable(tf.zeros([actions], dtype=tf.float32))
        adv_out = tf.matmul(adv_a, fcA2_W) + fcA2_b

        params += [fcV_W, fcV_b, fcV2_W, fcV2_b, fcA_W, fcA_b, fcA2_W, fcA2_b]
        output = value_out + adv_out - tf.reduce_mean(adv_out)
        return output, params

def make_policy_net( in_layer, architecture, actions, continuous = False, run_seed = None):
        # calculate output and cost
        out, params = make_net(in_layer, architecture)
        out_shape = out.get_shape().as_list()[1]
        initializer = tf.initializers.glorot_uniform(seed=0)

        fcPol_W = tf.Variable(initializer(shape = [out_shape, actions]))
        fcPol_b = tf.Variable(tf.zeros([actions], dtype=tf.float32))
        pol_out = tf.tanh(tf.matmul(out, fcPol_W) + fcPol_b)
        
        params += [fcPol_W, fcPol_b]

        if continuous:
            fcPol_W_var = tf.Variable(initializer(shape = [out_shape, actions]))
            fcPol_b_var = tf.Variable(tf.zeros([actions], dtype=tf.float32))
            pol_out_var = tf.nn.softplus(tf.matmul(out, fcPol_W_var) + fcPol_b_var)
            params += [fcPol_W_var, fcPol_b_var]      
            return pol_out, pol_out_var, params
        return pol_out, params


def make_ddpg_net(state_in, act_in, architecture, state_layer = 50, run_seed = None):
    initializer = tf.contrib.layers.xavier_initializer(seed=0)
    obs_dim = state_in.get_shape().as_list()[1]
    act_dim = act_in.get_shape().as_list()[1]

    if state_layer is not None:
        if isinstance(state_layer, int):
            inter_W = tf.Variable(initializer(shape = [obs_dim, state_layer]))
            inter_b = tf.Variable(tf.zeros([state_layer], dtype=tf.float32))
            intermediate = tf.nn.tanh(tf.matmul(state_in, inter_W) + inter_b)
            inter_dim = state_layer
            params = [inter_W, inter_b]
        else:
            raise(Exception("Not supported yet"))
    else: 
        intermediate = state_in
        inter_dim = obs_dim
        params = []

    state_W = tf.Variable(initializer(shape = [inter_dim, architecture[0]]))
    action_W = tf.Variable(initializer(shape = [act_dim, architecture[0]]))
    joint_b = tf.Variable(tf.zeros([architecture[0]], dtype=tf.float32))
    joint = tf.nn.tanh(tf.matmul(intermediate, state_W) + tf.matmul(act_in, action_W) + joint_b) 
    params += [state_W, action_W, joint_b]

    out, shared_params = make_net(joint, architecture[1:], final = 1)
    params += shared_params
    return out, params
File Path: bayes_opt/test_functions/drl/replay_buffer.py
Content:
import sys

from collections import namedtuple
import numpy as np
from bayes_opt.test_functions.drl.segment_tree import SumSegmentTree, MaxSegmentTree, SumTree

Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))


class ReplayBuffer(object):
    def __init__(self, capacity, batch):
        self.memory = []
        self._maxsize = capacity
        self._next_idx = 0
        self._batch_size = batch

    def __len__(self):
        return len(self.memory)

    def add(self, s, action, reward, s_n, done):
        if len(self.memory) < self._maxsize:
            self.memory.append(None)
        self.memory[self._next_idx] = Transition(s, action, reward, s_n, done)
        self._next_idx = (self._next_idx + 1) % self._maxsize

    def sample(self, beta = 0):
        idxes = list(np.random.randint( len(self), size=(self._batch_size,)))
        return (idxes, [1/self._batch_size] * self._batch_size, self._encode_samples(idxes))

    def _encode_samples(self, idx):
        return [self.memory[i] for i in idx]
        
    def update_priorities(self, idx, weights):
        pass

    def reset(self):
        self.memory = []
        self._next_idx = 0

class SumSegmentTreePERBuffer(ReplayBuffer):
    def __init__(self, size, batch, alpha, min_w = 1e-2):
        """Create Prioritized Replay buffer.

        Parameters
        ----------
        size: int
            Max number of transitions to store in the buffer. When the buffer
            overflows the old memories are dropped.
        alpha: float
            how much prioritization is used (0 - no prioritization, 1 - full prioritization)
        """
        super(SumSegmentTreePERBuffer, self).__init__(size, batch)
        assert alpha >= 0, alpha
        self._alpha = alpha

        it_capacity = 1
        while it_capacity < size:
            it_capacity *= 2
        self.min_w = min_w
        self._it_sum = SumSegmentTree(it_capacity)
        self._it_max = MaxSegmentTree(it_capacity)

    def add(self, *args, **kwargs):
        """See ReplayBuffer.store_effect"""
        idx = self._next_idx
        super().add(*args, **kwargs)
        self._it_sum.last_set = []
        self._it_sum[idx] = self._it_max.max() + self.min_w
        self._it_max[idx] = self._it_sum[idx]

    def _sample_proportional(self, batch_size):
        res = []
        p_total = self._it_sum.sum()
        for _ in range(batch_size):
            mass = np.random.rand() * p_total
            idx = self._it_sum.find_prefixsum_idx(mass)
            res.append(idx)
        return np.array(res)

    def sample(self, beta):
        """Sample a batch of experiences.

        Parameters
        ----------
        batch_size: int
            How many transitions to sample.
        beta: float
            To what degree to use importance weights (0 - no corrections, 1 - full correction)
        """
        assert beta > 0

        idxes = self._sample_proportional(self._batch_size)
        weights = (self._it_sum[idxes] / self._it_sum.sum() * len(self.memory) ) ** (-beta)
        try:
            return (idxes, weights, self._encode_samples(idxes))
        except e:
            print("bug")
            pass

    def update_priorities(self, idxes, priorities):
        """Update priorities of sampled transitions.

        sets priority of transition at index idxes[i] in buffer to priorities[i].

        Parameters
        ----------
        idxes: [int]
            List of idxes of sampled transitions
        priorities: [float]
            List of updated priorities corresponding to transitions at the sampled idxes denoted by variable `idxes`.
        """
        # assert len(idxes) == len(priorities)
        priorities = priorities ** self._alpha
        self._it_sum.last_set = []
        for idx, priority in zip(idxes, priorities):
            # assert (priority >= 0).all(), priority
            # assert (0 <= idx).all() and (idx < len(self.memory)).all()
            self._it_sum[idx] = priority + self.min_w
            self._it_max[idx] = priority + self.min_w

    def reset(self):
        super().reset()
        self._it_max.reset()
        self._it_sum.reset()

class SumTreePERBuffer(object):  # stored as ( s, a, r, s_ ) in SumTree
    """
    This SumTree code is modified version and the original code is from:
    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py
    """
    
    absolute_error_upper = 1.  # clipped abs error

    def __init__(self, capacity, batch, alpha, min_w= 0.01):
        # Making the tree 
        """
        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf
        And also a data array
        We don't use deque because it means that at each timestep our experiences change index by one.
        We prefer to use a simple array and to overwrite when the memory is full.
        """
        self.tree = SumTree(capacity)
        self.batch_size = batch
        self.PER_e = min_w  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
        self.PER_a = alpha  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
            
    """
    Store a new experience in our tree
    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)
    """
    def add(self, s, action, reward, s_n, done):
        # Find the max priority
        max_priority = self.tree.max_weight
        
        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected
        # So we use a minimum priority
        if max_priority == 0:
            max_priority = self.absolute_error_upper
        
        self.tree.add(max_priority, Transition(s, action, reward, s_n, done))   # set the max p for new p

        
    """
    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.
    - Then a value is uniformly sampled from each range
    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.
    - Then, we calculate IS weights for each minibatch element
    """
    def sample(self, beta):
        # Create a sample array that will contains the minibatch
        memory_b = []
        n = self.batch_size
        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n,), dtype=np.float32)
        
        # Calculate the priority segment
        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges
        priority_segment = self.tree.total_priority / n       # priority segment
    
        # Calculating the max_weight
        p_min = self.tree.min_weight / self.tree.total_priority
        max_weight = (p_min * n) ** (-beta)
        
        for i in range(n):
            """
            A value is uniformly sample from each range
            """
            a, b = priority_segment * i, priority_segment * (i + 1)            
            """
            Experience that correspond to each value is retrieved
            """
            index, priority, data = self.tree.get_leaf(a,b)
            
            #P(j)
            sampling_probabilities = priority / self.tree.total_priority
            
            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi
            b_ISWeights[i] = np.power(n * sampling_probabilities, -beta)/ max_weight
                                   
            b_idx[i]= index
            
            memory_b.append(data)
        return b_idx, b_ISWeights, memory_b
    
    """
    Update the priorities on the tree
    """
    def update_priorities(self, tree_idx, abs_errors):
        abs_errors += self.PER_e  # convert to abs and avoid 0
        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)
        ps = np.power(clipped_errors, self.PER_a)

        for ti, p in zip(tree_idx, ps):
            self.tree.update(ti, p)

File Path: bayes_opt/test_functions/drl/segment_tree.py
Content:
import operator
import numpy as np

class SumTree(object):
    """
    This SumTree code is modified version of Morvan Zhou: 
    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py
    """
    data_pointer = 0
    """
    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0
    """
    def __init__(self, capacity):
        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences
        self.it_capacity = 1
        while self.it_capacity < capacity:
            self.it_capacity *= 2
        # Generate the tree with all nodes values = 0
        # To understand this calculation (2 * capacity - 1) look at the schema above
        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)
        # Parent nodes = capacity - 1
        # Leaf nodes = capacity
        self.tree = np.zeros(2 * self.it_capacity)
        
        # Contains the experiences (so the size of data is capacity)
        self.data = np.zeros(capacity, dtype=object)
    
    
    """
    Here we add our priority score in the sumtree leaf and add the experience in data
    """
    def add(self, priority, data):
        # Look at what index we want to put the experience
        tree_index = self.data_pointer + self.it_capacity
        
        # Update data frame
        self.data[self.data_pointer] = data
        
        # Update the leaf
        self.update(tree_index, priority)
        
        # Add 1 to data_pointer
        self.data_pointer += 1
        
        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)
            self.data_pointer = 0
            
    
    """
    Update the leaf priority score and propagate the change through tree
    """
    def update(self, tree_index, priority):
        # Change = new priority score - former priority score
        change = priority - self.tree[tree_index]
        self.tree[tree_index] = priority
        
        # then propagate the change through tree
        while tree_index != 0:    # this method is faster than the recursive loop in the reference code
            
            """
            Here we want to access the line above
            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES
            
            If we are in leaf at index 6, we updated the priority score
            We need then to update index 2 node
            So tree_index = (tree_index - 1) // 2
            tree_index = (6-1)//2
            tree_index = 2 (because // round the result)
            """
            tree_index = (tree_index - 1) // 2
            self.tree[tree_index] += change
    
    
    """
    Here we get the leaf_index, priority value of that leaf and experience associated with that index
    """
    def get_leaf(self, a, b):
        v = np.random.uniform(a, b)
        parent_index = 0
        
        while True: # the while loop is faster than the method in the reference code
            left_child_index = 2 * parent_index + 1
            right_child_index = left_child_index + 1
            
            # If we reach bottom, end the search
            if left_child_index >= len(self.tree):
                leaf_index = parent_index
                break
            
            else: # downward search, always search for a higher priority node
                
                if v <= self.tree[left_child_index]:
                    parent_index = left_child_index
                    
                else:
                    v -= self.tree[left_child_index]
                    parent_index = right_child_index
            
        data_index = leaf_index - self.it_capacity
        if data_index >= self.capacity:
            return self.get_leaf(a,b)
        return leaf_index, self.tree[leaf_index], self.data[data_index]
    
    @property
    def total_priority(self):
        return self.tree[0] # Returns the root node

    @property
    def min_weight(self):
        valid = np.where(self.tree[self.it_capacity:self.it_capacity + self.capacity] > 0.0)
        return self.tree[self.it_capacity:self.it_capacity + self.capacity][valid].min()

    @property
    def max_weight(self):
        return np.max(self.tree[self.it_capacity:self.it_capacity + self.capacity])

class SumSegmentTree(object):
    def __init__(self, capacity):
        assert capacity > 0 and capacity & (capacity - 1) == 0, "capacity must be positive and a power of 2."
        self._capacity = capacity
        self._value = np.zeros((2 * self._capacity))

    def reset(self):
        self._value = np.zeros((2 * self._capacity))

    def sum(self):
        return self._value[1]

    def find_prefixsum_idx(self, prefixsum):
        #assert 0 <= prefixsum <= self.sum() + 1e-5
        idx = 1
        while idx < self._capacity:  # while non-leaf
            idx *= 2
            if self._value[ idx] < prefixsum:
                prefixsum -= self._value[idx]
                idx += 1
        return idx - self._capacity

    def __setitem__(self, idx, val):
        # index of the leaf
        idx += self._capacity
        diff = val - self._value[idx]
        self.last_set.append((idx, self._value[idx], val, diff))
        self._value[idx] = val
        idx //= 2
        while idx >= 1:
            self._value[idx] += diff
            idx //= 2

    def __getitem__(self, idx):
        #assert 0 <= idx < self._capacity
        return self._value[self._capacity + idx]


class MaxSegmentTree(object):
    def __init__(self, capacity):
        assert capacity > 0 and capacity & (capacity - 1) == 0, "capacity must be positive and a power of 2."
        self._capacity = capacity
        self._value = np.zeros((2 * self._capacity))

    def reset(self):
        self._value = np.zeros((2 * self._capacity))

    def max(self):
        return self._value[1]
    
    def __setitem__(self, idx, val):
        # index of the leaf
        idx += self._capacity
        self._value[idx] = val
        
        while idx > 1 and self._value[idx] == val:
            idx //= 2
            self._value[idx] = max(
                self._value[2 * idx],
                self._value[2 * idx + 1]
            )

    def __getitem__(self, idx):
        #assert 0 <= idx < self._capacity
        return self._value[self._capacity + idx]

def test():
    # s = SumSegmentTree(4)
    # s[0] = 0.1
    # s[1] = 0.3
    # s[2] = 1.0
    # s[3] = 0.6
    # print(s._value)
    # print("SUM ", s.sum())
    # print("Items ", s[np.arange(4)])
    # print("0.05", s.find_prefixsum_idx(0.05))
    # print("0.15", s.find_prefixsum_idx(0.15))
    # print("0.45", s.find_prefixsum_idx(0.45))
    # print("1.45", s.find_prefixsum_idx(1.45))
    # print("2.05", s.find_prefixsum_idx(2.05))

    # m = MaxSegmentTree(8)
    # m[0] = 0.1
    # m[1] = 0.3
    # m[2] = 1.0
    # m[3] = 0.6
    # m[4] = 2.0
    # m[5] = 0.5
    # m[6] = 0.2
    # m[7] = 0.6
    # print(m._value)
    s = SumTree(10)
    s.add(.5,"a")
    print(s.get_leaf(0.4))
if __name__ == '__main__':
    test()
File Path: bayes_opt/test_functions/drl/wrapper.py
Content:
import numpy as np
import os
from collections import deque
import gym
from gym import spaces
import pickle as pickle
from multiprocessing import Process, Pipe
import copy

class TaxiWrapper(gym.Wrapper):
    def __init__(self, env):
        gym.Wrapper.__init__(self, env)

    def step(self, action):
        ob, reward, done, info = self.env.step(action)
        done = (reward == 20)
        return ob, reward, done, info

class ParallelEnvExecutor(object):
    """
    Wraps multiple environments of the same kind and provides functionality to reset / step the environments
    in a vectorized manner. Thereby the environments are distributed among batch_size processes and
    executed in parallel.

    Args:
        env (meta_policy_search.envs.base.MetaEnv): meta environment object
        batch_size (int): number of parallel environments
    """

    def __init__(self, env, batch_size):
        self.n_envs = batch_size
        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(batch_size)])
        seeds = np.random.choice(range(10**6), size=batch_size, replace=False)

        self.ps = [
            Process(target=worker, args=(work_remote, remote, pickle.dumps(env), seed))
            for (work_remote, remote, seed) in zip(self.work_remotes, self.remotes, seeds)]

        for p in self.ps:
            p.daemon = True  # if the main process crashes, we should not cause things to hang
            p.start()
        for remote in self.work_remotes:
            remote.close()

    def step(self, actions):
        """
        Executes actions on each env

        Args:
            actions (list): list of actions of length batch_size

        Returns
            (tuple): a length 4 tuple of lists, containing obs (np.array), rewards (float), dones (bool), env_infos (dict)
                      each list is of length batch_size
        """
        assert len(actions) == self.num_envs

        # step remote environments
        for remote, action in zip(self.remotes, actions):
            remote.send(('step', action))

        results = [remote.recv() for remote in self.remotes]
        obs, rewards, dones, env_infos = zip(*results)
        return obs, rewards, dones, env_infos

    def reset(self):
        """
        Resets the environments of each worker

        Returns:
            (list): list of (np.ndarray) with the new initial observations.
        """
        for remote in self.remotes:
            remote.send(('reset', None))
        return [remote.recv() for remote in self.remotes]

    @property
    def num_envs(self):
        """
        Number of environments

        Returns:
            (int): number of environments
        """
        return self.n_envs


def worker(remote, parent_remote, env_pickle, seed):
    """
    Instantiation of a parallel worker for collecting samples. It loops continually checking the task that the remote
    sends to it.

    Args:
        remote (multiprocessing.Connection):
        parent_remote (multiprocessing.Connection):
        env_pickle (pkl): pickled environment
        seed (int): random seed for the worker
    """
    parent_remote.close()

    env = pickle.loads(env_pickle)
    np.random.seed(seed)
    while True:
        # receive command and data from the remote
        cmd, data = remote.recv()

        # do a step in each of the environment of the worker
        if cmd == 'step':
            obs, reward, done, info = env.step(data)
            if done:
                obs = env.reset()
            remote.send((obs, reward, done, info))

        # reset all the environments of the worker
        elif cmd == 'reset':
            obs = env.reset()
            remote.send(obs)

        # close the remote and stop the worker
        elif cmd == 'close':
            remote.close()
            break

        else:
            raise NotImplementedError
File Path: bayes_opt/test_functions/drl_experiments.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Sat Dec 22 21:52:22 2018

@author: VuNguyen
"""
import time, sys
import numpy as np
import tensorflow as tf
import gym
import tensorflow.compat.v1 as v1
# tf.disable_v2_behavior()

import copy
from bayes_opt.test_functions.drl import agents
from bayes_opt.test_functions.drl.wrapper import TaxiWrapper


def reshape(x, input_dim):
    """
    Reshapes x into a matrix with input_dim columns
    """
    x = np.array(x)
    if x.size == input_dim:
        x = x.reshape((1, input_dim))
    return x


algs = {"A2C": agents.A2C,
        "DQN": agents.DQN}


class DRL_experiment:
    def __init__(self, alg_name, env_name, varParams, fixParams={}, bounds=None, gpu_id=None):
        self.env = env_name
        self.alg_name = alg_name  # for printing purpose
        self.agent = algs[alg_name]
        if bounds is None:
            self.bounds = self.agent.bounds
        else:
            self.bounds = bounds
        self.varParams = varParams
        self.fixParams = fixParams
        self.input_dim = len(varParams)
        self.name = env_name
        self.ismax = 1
        self.gpu_id = gpu_id  # for multiple GPU machine: None, 0, 1

    def func(self, X):
        X = np.asarray(X)
        
        if len(X.shape) == 1:  # 1 data point
            output = self.evaluate(X)
            Reward = [output[0]]
            elapse = [output[1]]
        else:
            output = np.apply_along_axis(self.evaluate, 1, X)
            Reward = output[:, 0].tolist()
            elapse = output[:, 1].tolist()
        return Reward, elapse

    def evaluate(self, X, unwrap=False, display=False):
        # tf.reset_default_graph()
        tf.compat.v1.reset_default_graph()
        ag = self.agent()
        for i, val in enumerate(X):
            ag.params[self.varParams[i]] = val
        for k, v in self.fixParams.items():
            ag.params[k] = v

        start_time = time.time()
        run_seed = np.random.randint(0, 100000)
        # tf.reset_default_graph()
        env = gym.make(self.env)
        #env.seed(run_seed)
        if unwrap:
            env = env.unwrapped
        if self.env == "Taxi-v2":
            env = TaxiWrapper(env)
        tf.random.set_seed(run_seed)
        ag.initialise(copy.copy(env.observation_space), copy.copy(env.action_space))
        # tf.set_random_seed(run_seed)
        tf.random.set_seed(run_seed)

        init = v1.global_variables_initializer()
        
        if self.gpu_id is None:
            session = v1.InteractiveSession()
        else:
            gpu_options = tf.GPUOptions(visible_device_list=str(self.gpu_id))   # set a GPU ID for multiple GPU cards
            session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))

            # config = tf.ConfigProto(device_count = {'GPU': self.gpu_id})#gpu_id=0 or 1
            # session = tf.InteractiveSession(config=config)

        session.run(init)
        ag.set_session(session)
            
        N = int(ag.params['maxEpisodes'])
        totalrewards = np.empty(N)
        
        t = 0
        if display and issubclass(type(env.unwrapped),  gym.envs.mujoco.mujoco_env.MujocoEnv):
            print("Rendering this environment is not supported"
                  "- try to execute with display=False to get simulation outcomes")
            return

        for e in range(N):
            totalrewards[e], t = ag.nextEpisode(env, t, display=display)
        end_time = time.time()
        elapse = end_time-start_time
        env.close()
        session.close()
        return totalrewards, elapse

File Path: bayes_opt/utility/__init__.py
Content:

File Path: bayes_opt/utility/basic_utility_functions.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Tue Mar 01 21:37:03 2016

@author: Vu
"""

import itertools
import numpy as np
import tensorflow as tf
import random
from bayes_opt.curve_compression import transform_logistic_marginal
from tqdm import tqdm

def set_seed(seed):
	random.seed(seed)
	np.random.seed(seed)
	tf.random.set_seed(seed)
    
def generate_random_points(bounds,size=1):
    x_max = [np.random.uniform(x[0], x[1], size=size) for x in bounds]
    x_max=np.asarray(x_max)
    x_max=x_max.T
    return x_max


def best_output_to_best_input(output,X_input):
    # giving a list of output
    # return the corresponding input
    
    best_X=[0]*len(output)
    for ii in range(len(output)):
        best_X[ii]=[0]*len(output[ii])
        for jj in range(len(output[ii])):
            idxBest=np.argmax(output[ii][0:jj+1])
            best_X[ii][jj]=X_input[ii][idxBest]
            
    return best_X
    
def evaluating_the_final_utility(bo):
    # for a fair comparison, the final score is evaluated by marginalizing 
    # across different choices in the preference function (Logistic function)
    
    # ignore the intermediate curves
    idxReal=[idx for idx,val in enumerate(bo.markVirtualObs) if val==0]
    Y_curves=[val for idx,val in enumerate(bo.Y_curves) if idx in idxReal]  
            
    y_score=[0]*len(Y_curves)
    for ii,curve in enumerate(Y_curves):
        y_score[ii]=transform_logistic_marginal([curve],bo.SearchSpace[-1,1])
        
    return np.asarray(y_score)
        
def evaluating_final_util_MaxEpoch(bo):
    # for a fair comparison, the final score is evaluated by (1) training using MaxEpoch and 
    # (2) marginalizing across different choices in the preference function (Logistic function)
    
    # ignore the intermediate curves
    idxReal=[idx for idx,val in enumerate(bo.markVirtualObs) if val==0]
    Y_curves=[val for idx,val in enumerate(bo.Y_curves) if idx in idxReal]  
    T_original=[val for idx,val in enumerate(bo.T_original) if idx in idxReal]   
    Y_original=[val for idx,val in enumerate(bo.Y_original) if idx in idxReal]   
    X_original=[val for idx,val in enumerate(bo.X_original) if idx in idxReal]  
            
            
    y_score=[0]*len(Y_curves)
    for ii,curve in enumerate(Y_curves):
        y_score[ii]=transform_logistic_marginal([curve],bo.SearchSpace[-1,1])
        
    # identify the best input x for each run, then run it until the end

    # run these input until the max episode
    #T_max=50
    best_X=None
    Y_best_max_T=np.copy(y_score)
    T_max=bo.SearchSpace[-1,1]

    for jj in tqdm(range(1,len(y_score))):
        
        idxBest=np.argmax(y_score[0:jj+1])
        
        old_best_X=best_X
        best_X=X_original[idxBest]

        if T_max==T_original[idxBest]:
            Y_best_max_T[jj]=y_score[idxBest]
            continue

        if jj>0 and (old_best_X==best_X).all():
            Y_best_max_T[jj]=Y_best_max_T[jj-1]
        else:
            input_test=best_X.tolist()+[T_max]
            curve,time=bo.f(input_test)
            Y_best_max_T[jj]=transform_logistic_marginal(curve,T_max)
            #print(input_test,y_score[idxBest],Y_best_max_T[jj])
            
                
    return np.asarray(y_score)


        

File Path: bayes_opt/utility/export_results.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Tue Mar 01 21:37:03 2016

@author: Vu
"""

import sys
sys.path.insert(0,'../..')
sys.path.insert(0,'../')
import numpy as np
import pickle
import os
import sys

out_dir="pickle_storage"

def print_result(bo,myfunction,Score,mybatch_type,acq_type):

    print_result_sequential(bo,myfunction,Score,mybatch_type,acq_type)

def print_result_sequential(bo,myfunction,Score,method_type,acq_type):
    
    if 'ystars' in acq_type:
        acq_type['ystars']=[]
    if 'xstars' in acq_type:
        acq_type['xstars']=[]
        
    #Regret=Score["Regret"]
    ybest=Score["ybest"]
    MyTime=Score["MyTime"]
    
    print('{:s} {:d}'.format(myfunction.name,myfunction.input_dim))
  
    MaxFx=[val.max() for idx,val in enumerate(ybest)]

    
    if myfunction.ismax==1:
        print('MaxBest={:.4f}({:.2f})'.format(myfunction.ismax*np.mean(MaxFx),np.std(MaxFx)))    
    else:
        print('MinBest={:.4f}({:.2f})'.format(myfunction.ismax*np.mean(MaxFx),np.std(MaxFx)))
        
    
    if 'MyOptTime' in Score:
        MyOptTime=Score["MyOptTime"]

        print('OptTime/Iter={:.1f}({:.1f})'.format(np.mean(MyOptTime),np.std(MyOptTime)))
        
    try:
        strFile="{:s}_{:s}_{:d}_{:s}_{:s}.pickle".format(myfunction.name,myfunction.alg_name,myfunction.input_dim,method_type['name'],acq_type['name'])
    except:
        strFile="{:s}_{:d}_{:s}_{:s}.pickle".format(myfunction.name,myfunction.input_dim,method_type['name'],acq_type['name'])
    if sys.version_info[0] < 3:
        version=2
    else:
        version=3
        
    path=os.path.join(out_dir,strFile)
    
    if version==2:
        with open(path, 'wb') as f:
            pickle.dump([ybest, MyTime,bo[-1].bounds,MyOptTime], f)
    else:
        pickle.dump( [ybest, MyTime,bo,MyOptTime], open( path, "wb" ) )


       
def yBest_Iteration(YY,BatchSzArray,step=3):
    
    nRepeat=len(YY)
    
    result=[0]*nRepeat

    for ii,yy in enumerate(YY):
        result[ii]=[np.max(yy[:uu+1]) for uu in range(len(yy))]
        
    result=np.asarray(result)
    
    result_mean=np.mean(result,axis=0)
    result_mean=result_mean[BatchSzArray[0]-1:]
    result_std=np.std(result,axis=0)
    result_std=result_std[BatchSzArray[0]-1:]
    
    return result_mean[::step], result_std[::step], None, None


    std_cum_TT=np.std(mean_cum_simple_regret_TT,axis=0)
    std_cum_TT=np.array(std_cum_TT).ravel()
    mean_cum_simple_regret_TT=np.mean(mean_cum_simple_regret_TT,axis=0)
   
    #return mean_TT[::step],std_TT[::step]#,mean_cum_TT[::5],std_cum_TT[::5]
    #return mean_TT,std_TT,np.mean(mean_cum_simple_regret_TT),np.mea(std_cum_TT)
    
    #half_list_index=np.int(len(mean_cum_simple_regret_TT)*0.5)
    #return np.mean(mean_cum_simple_regret_TT[half_list_index:]),np.mean(std_cum_TT[half_list_index:])
    return np.mean(mean_cum_simple_regret_TT),np.mean(std_cum_TT)
File Path: bayes_opt/visualization/__init__.py
Content:

File Path: bayes_opt/visualization/visualization_utility_cost.py
Content:
# -*- coding: utf-8 -*-
"""
Created on Sat Feb 27 23:22:32 2016

@author: Vu
"""
from __future__ import division

import sys
sys.path.insert(0,'..')
import numpy as np
import matplotlib.pyplot as plt
from bayes_opt.acquisition_maximization import acq_max_with_name



from bayes_opt.acquisition_functions import AcquisitionFunction
import os

cdict = {'red': ((0.0, 0.0, 0.0),
                  (0.5, 1.0, 0.7),
                  (1.0, 1.0, 1.0)),
          'green': ((0.0, 0.0, 0.0),
                    (0.5, 1.0, 0.0),
                    (1.0, 1.0, 1.0)),
          'blue': ((0.0, 0.0, 0.0),
                   (0.5, 1.0, 0.0),
                   (1.0, 0.5, 1.0))}

my_cmap = plt.get_cmap('Blues')

        
counter = 0

out_dir="P:\\03.Research\\05.BayesianOptimization\\PradaBayesianOptimization\\pickle_storage"
out_dir=""



    
def show_optimization_progress(bo):

    fig=plt.figure(figsize=(6, 3))
    myYbest=[bo.Y_original[:idx+1].max() for idx,val in enumerate(bo.Y_original)]
    plt.plot(range(len(myYbest)),myYbest,linewidth=2,color='m',linestyle='-',marker='o')
    plt.title('Best Found Value')
    plt.xlabel('Iteration')
    plt.ylabel('f(x)')
       
    
def plot_bo_2d(bo):
    
    x1 = np.linspace(bo.scaleSearchSpace[0,0], bo.scaleSearchSpace[0,1], 70)
    x2 = np.linspace(bo.scaleSearchSpace[1,0], bo.scaleSearchSpace[1,1], 70)
    x1g,x2g=np.meshgrid(x1,x2)
    
    X=np.c_[x1g.flatten(), x2g.flatten()]
    
    x1_ori = np.linspace(bo.SearchSpace[0,0], bo.SearchSpace[0,1], 70)
    x2_ori = np.linspace(bo.SearchSpace[1,0], bo.SearchSpace[1,1], 70)    
    x1g_ori,x2g_ori=np.meshgrid(x1_ori,x2_ori)
    
    X_ori=np.c_[x1g_ori.flatten(), x2g_ori.flatten()]
  
    fig = plt.figure()
    
    #axis2d = fig.add_subplot(1, 2, 1)
    acq2d = fig.add_subplot(1, 1, 1)
    

    utility = bo.acq_func.acq_kind(X, bo.gp)
    #acq3d.plot_surface(x1g,x1g,utility.reshape(x1g.shape))
    
    CS_acq=acq2d.contourf(x1g_ori,x2g_ori,utility.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    idxBest=np.argmax(utility)
    
    acq2d.scatter(bo.X_original[:,0],bo.X_original[:,1],color='g',label='Data')  
    #acq2d.scatter(bo.X_original[-1,0],bo.X_original[-1,1],color='r',s=30,label='Previous Selection')
    acq2d.scatter(bo.X_original[-1,0],bo.X_original[-1,1],marker='*', color='green',s=140,label='Selected')
    acq2d.scatter(X_ori[idxBest,0],X_ori[idxBest,1],marker='s',color='r',s=30,label='Peak')

    acq2d.set_title('Acquisition Function',fontsize=16)
    acq2d.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    acq2d.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1])
    
    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    acq2d.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq, ax=acq2d, shrink=0.9)

 
def plot_2d_GPmean_var(bo,strFolderOut=None):
    
    global counter
    counter=counter+1
    
    x1 = np.linspace(bo.scaleSearchSpace[0,0], bo.scaleSearchSpace[0,1], 30)
    x2 = np.linspace(bo.scaleSearchSpace[1,0], bo.scaleSearchSpace[1,1], 30)
    x1g,x2g=np.meshgrid(x1,x2)
    
    X=np.c_[x1g.flatten(), x2g.flatten()]
    T=X[:,1]
    T=np.atleast_2d(T)
    T=T.T
    T-np.reshape(T,(900,-1))
    
    x1_ori = np.linspace(bo.SearchSpace[0,0], bo.SearchSpace[0,1], 30)
    x2_ori = np.linspace(bo.SearchSpace[1,0], bo.SearchSpace[1,1], 30)   
    x1g_ori,x2g_ori=np.meshgrid(x1_ori,x2_ori)
    
    X_ori=np.c_[x1g_ori.flatten(), x2g_ori.flatten()]
  
    fig = plt.figure(figsize=(8,2.5))
    
    axis2d_GPmean = fig.add_subplot(1, 2, 1)
    axis2d_GPvariance = fig.add_subplot(1, 2, 2)
    #axis2d_util = fig.add_subplot(1, 5, 3)
    #axis2d_cost = fig.add_subplot(1, 5, 4)

    #axis2d_acq = fig.add_subplot(1, 5, 5)

    
    gp_mean, sigma = bo.gp.predict(X,eval_MSE=True)
    gp_mean_original=gp_mean*np.std(bo.Y_original)+np.mean(bo.Y_original)
    # plot the utility

    CS_acq=axis2d_GPmean.contourf(x1g_ori,x2g_ori,gp_mean.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    try:
        idxVirtual=[idx for idx,val in enumerate(bo.markVirtualObs) if val==1]
        idxReal=[idx for idx,val in enumerate(bo.markVirtualObs) if val==0]
    
        nLastVirtual=len(bo.markVirtualObs)-idxReal[-1]
        idxVirtual_ExcludeLast=idxVirtual[:-nLastVirtual]
        idxReal_ExcludeLast=idxReal[:-1]
    except:
        idxVirtual=[]
        idxReal=list(range(len(bo.Y)))
        idxVirtual_ExcludeLast=[]
        idxReal_ExcludeLast=idxReal[:-1]
    #x_stars=np.asarray(bo.x_stars_original)   
    
    axis2d_GPmean.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  
    axis2d_GPmean.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  
    

    axis2d_GPmean.set_title('GP mean',fontsize=16)
    axis2d_GPmean.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_GPmean.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    axis2d_GPmean.set_ylabel('#Episode',fontsize=16)

    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    axis2d_GPmean.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq, ax=axis2d_GPmean, shrink=0.9)
    
    
    gp_var_original=sigma
    #gp_var_original=sigma*np.std(bo.Y_original)+np.mean(bo.Y_original)

    CS_acq=axis2d_GPvariance.contourf(x1g_ori,x2g_ori,gp_var_original.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    axis2d_GPvariance.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  
    axis2d_GPvariance.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  

    
    axis2d_GPvariance.set_yticks([])
    axis2d_GPvariance.set_title('GP variance',fontsize=16)
    axis2d_GPvariance.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_GPvariance.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    axis2d_GPvariance.set_xlabel(r'$x$',fontsize=16)

    fig.colorbar(CS_acq, ax=axis2d_GPvariance, shrink=0.9)
    

    if strFolderOut is not None:
        strFileName="{:d}_GP2d.pdf".format(counter)
        strFinalPath=os.path.join(strFolderOut,strFileName)
        fig.savefig(strFinalPath, bbox_inches='tight')
    
    
    
def plot_bo_2d_cost_utility_AF(bo,strFolderOut=None):
    
    global counter
    counter=counter+1
    
    x1 = np.linspace(bo.scaleSearchSpace[0,0], bo.scaleSearchSpace[0,1], 30)
    x2 = np.linspace(bo.scaleSearchSpace[1,0], bo.scaleSearchSpace[1,1], 30)
    x1g,x2g=np.meshgrid(x1,x2)
    
    X=np.c_[x1g.flatten(), x2g.flatten()]
    T=X[:,1]
    T=np.atleast_2d(T)
    T=T.T
    T-np.reshape(T,(900,-1))
    
    x1_ori = np.linspace(bo.SearchSpace[0,0], bo.SearchSpace[0,1], 30)
    x2_ori = np.linspace(bo.SearchSpace[1,0], bo.SearchSpace[1,1], 30)   
    x1g_ori,x2g_ori=np.meshgrid(x1_ori,x2_ori)
    
    X_ori=np.c_[x1g_ori.flatten(), x2g_ori.flatten()]
  
    fig = plt.figure(figsize=(18,3))
    
    axis2d_GPmean = fig.add_subplot(1, 5, 1)
    axis2d_GPvariance = fig.add_subplot(1, 5, 2)
    axis2d_util = fig.add_subplot(1, 5, 3)
    axis2d_cost = fig.add_subplot(1, 5, 4)

    axis2d_acq = fig.add_subplot(1, 5, 5)

    
    gp_mean, sigma = bo.gp.predict(X,eval_MSE=True)
    gp_mean_original=gp_mean*np.std(bo.Y_original)+np.mean(bo.Y_original)
    # plot the utility

    CS_acq=axis2d_GPmean.contourf(x1g_ori,x2g_ori,gp_mean.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    try:
        idxVirtual=[idx for idx,val in enumerate(bo.markVirtualObs) if val==1]
        idxReal=[idx for idx,val in enumerate(bo.markVirtualObs) if val==0]
    
        nLastVirtual=len(bo.markVirtualObs)-idxReal[-1]
        idxVirtual_ExcludeLast=idxVirtual[:-nLastVirtual]
        idxReal_ExcludeLast=idxReal[:-1]
    except:
        idxVirtual=[]
        idxReal=list(range(len(bo.Y)))
        idxVirtual_ExcludeLast=[]
        idxReal_ExcludeLast=idxReal[:-1]
    #x_stars=np.asarray(bo.x_stars_original)   
    
    axis2d_GPmean.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  
    axis2d_GPmean.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  
    

    axis2d_GPmean.set_title('GP mean',fontsize=16)
    axis2d_GPmean.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_GPmean.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    axis2d_GPmean.set_ylabel('#Episode',fontsize=16)

    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    axis2d_GPmean.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq, ax=axis2d_GPmean, shrink=0.9)
    
    
    gp_var_original=sigma
    #gp_var_original=sigma*np.std(bo.Y_original)+np.mean(bo.Y_original)

    CS_acq=axis2d_GPvariance.contourf(x1g_ori,x2g_ori,gp_var_original.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    axis2d_GPvariance.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  
    axis2d_GPvariance.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  

    
    axis2d_GPvariance.set_yticks([])
    axis2d_GPvariance.set_title('GP variance',fontsize=16)
    axis2d_GPvariance.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_GPvariance.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    axis2d_GPvariance.set_xlabel('gamma',fontsize=16)

    
    fig.colorbar(CS_acq, ax=axis2d_GPvariance, shrink=0.9)
    
    
    # plot the cost
    #gp_mean_cost, sigma = bo.gp_cost.predict(X)
    
    try:
        mean_cost=bo.linear_regression.predict(X)
    except:
        mean_cost=bo.linear_regression.predict(T)
        
    mean_cost=np.reshape(mean_cost,(-1,1))
    mean_cost[mean_cost<0]=0.001
    mean_cost=np.log(1+np.exp(mean_cost))
    #mean_cost[mean_cost<0]=0.001

    #gp_mean_cost=mean_cost
    gp_mean_cost_original=mean_cost*np.std(bo.Y_cost_original)+np.mean(bo.Y_cost_original)
    
    CS_acq_cost=axis2d_cost.contourf(x1g_ori,x2g_ori,mean_cost.reshape(x1g.shape),cmap=my_cmap,origin='lower')
   
    axis2d_cost.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  

    axis2d_cost.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  


    axis2d_cost.set_title(r'Cost $\mu_c$',fontsize=16)
    #axis2d_cost.set_xlabel('gamma',fontsize=16)
    #axis2d_cost.set_ylabel('#Episode',fontsize=16)

    axis2d_cost.set_yticks([])
    axis2d_cost.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_cost.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    
    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    axis2d_cost.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq_cost, ax=axis2d_cost, shrink=0.9)
    
    
  

    # optimie the GP predictive mean function to find the max of mu
    x_mu_max,mu_max_val=acq_max_with_name(gp=bo.gp,scaleSearchSpace=bo.scaleSearchSpace,acq_name='mu',IsReturnY=True)

    # plot acquisition function
    acq_func={}
    acq_func['name']=bo.acq_name
    acq_func['dim']=1
    acq_func['scaleSearchSpace']=bo.scaleSearchSpace
    acq_func['mu_max']=  mu_max_val


    myacq=AcquisitionFunction(acq_func)
    util_value = myacq.acq_kind(X, bo.gp)
    util_value=np.log(1+np.exp(util_value))
    util_value=np.reshape(util_value,(-1,1))
    
    
    CS_acq=axis2d_util.contourf(x1g_ori,x2g_ori,util_value.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    #axis2d_util.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Data')
    #axis2d_util.scatter(bo.X_original[idxReal[-1],0],bo.T_original[idxReal[-1],0],marker='s', color='yellow',s=80,label='Selected')
    axis2d_util.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  

    axis2d_util.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  


    axis2d_util.set_yticks([])

    axis2d_util.set_title(r'Acquisition $\alpha$',fontsize=16)
    axis2d_util.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_util.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    axis2d_util.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))

    
    fig.colorbar(CS_acq, ax=axis2d_util, shrink=0.9)


    # find the max of utility and max of cost to normalize
#    x_max_acq, utility_max=acq_max_with_name(gp=bo.gp,scaleSearchSpace=bo.scaleSearchSpace,
#                             acq_name=bo.acq_name,IsReturnY=True)
#    x_max_acq, utility_min=acq_max_with_name(gp=bo.gp,scaleSearchSpace=bo.scaleSearchSpace,
#                             acq_name=bo.acq_name,IsReturnY=True,IsMax=False)      
    
 
    
    util_min=np.min(util_value)
    cost_min=np.min(mean_cost)
    if util_min<0 or cost_min<0:
        print("bug")
    acq_value=util_value/(mean_cost)
    
    val=np.min(acq_value)
    if val<0:
        print("val<0")
    acq_value=np.reshape(acq_value,x1g.shape)
    idxMax=np.argmax(acq_value)
        
    CS_acq=axis2d_acq.contourf(x1g_ori,x2g_ori,acq_value.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    axis2d_acq.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r')  
    axis2d_acq.scatter(bo.X_original[idxReal,0],bo.T_original[idxReal,0],marker='s',color='g')  
    axis2d_acq.scatter(bo.X_original[idxReal[-1],0],bo.T_original[idxReal[-1],0],marker='s', color='k',s=80,label='Selected')
    #axis2d_acq.scatter(X_ori[idxMax,0],X_ori[idxMax,1],marker='^',s=100,color='k',label='Maximum')

    
    axis2d_acq.set_yticks([])

    axis2d_acq.set_title(r'Decision $\alpha$ / $\mu_c$',fontsize=16)
    axis2d_acq.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_acq.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    
    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    axis2d_acq.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq, ax=axis2d_acq, shrink=0.9)
    

    if strFolderOut is not None:
        strFileName="{:d}_GP2d_AF_5.pdf".format(counter)
        strFinalPath=os.path.join(strFolderOut,strFileName)
        fig.savefig(strFinalPath, bbox_inches='tight')
    
def plot_2d_Acq_Cost(bo,strFolderOut=None):
    
    global counter
    counter=counter+1
    
    x1 = np.linspace(bo.scaleSearchSpace[0,0], bo.scaleSearchSpace[0,1], 30)
    x2 = np.linspace(bo.scaleSearchSpace[1,0], bo.scaleSearchSpace[1,1], 30)
    x1g,x2g=np.meshgrid(x1,x2)
    
    X=np.c_[x1g.flatten(), x2g.flatten()]
    T=X[:,1]
    T=np.atleast_2d(T)
    T=T.T
    T-np.reshape(T,(900,-1))
    
    x1_ori = np.linspace(bo.SearchSpace[0,0], bo.SearchSpace[0,1], 30)
    x2_ori = np.linspace(bo.SearchSpace[1,0], bo.SearchSpace[1,1], 30)   
    x1g_ori,x2g_ori=np.meshgrid(x1_ori,x2_ori)
    
    X_ori=np.c_[x1g_ori.flatten(), x2g_ori.flatten()]
  
    fig = plt.figure(figsize=(14,2.5))
    
    
    axis2d_util = fig.add_subplot(1, 3, 1)
    axis2d_cost = fig.add_subplot(1, 3, 2)
    axis2d_acq = fig.add_subplot(1, 3, 3)

    
    gp_mean, sigma = bo.gp.predict(X,eval_MSE=True)
    # plot the utility

    try:
        idxVirtual=[idx for idx,val in enumerate(bo.markVirtualObs) if val==1]
        idxReal=[idx for idx,val in enumerate(bo.markVirtualObs) if val==0]
    
        nLastVirtual=len(bo.markVirtualObs)-idxReal[-1]
        idxVirtual_ExcludeLast=idxVirtual[:-nLastVirtual]
        idxReal_ExcludeLast=idxReal[:-1]
    except:
        idxVirtual=[]
        idxReal=list(range(len(bo.Y)))
        idxVirtual_ExcludeLast=[]
        idxReal_ExcludeLast=idxReal[:-1]
    #x_stars=np.asarray(bo.x_stars_original)   
    
    try:
        mean_cost=bo.linear_regression.predict(X)
    except:
        mean_cost=bo.linear_regression.predict(T)
        
    mean_cost=np.reshape(mean_cost,(-1,1))
    mean_cost[mean_cost<0]=0.001
    mean_cost=np.log(1+np.exp(mean_cost))
    #mean_cost[mean_cost<0]=0.001

    #gp_mean_cost=mean_cost
    gp_mean_cost_original=mean_cost*np.std(bo.Y_cost_original)+np.mean(bo.Y_cost_original)
    #acq3d.plot_surface(x1g,x1g,utility.reshape(x1g.shape))
    
    CS_acq_cost=axis2d_cost.contourf(x1g_ori,x2g_ori,mean_cost.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    #idxBest=np.argmax(gp_mean_cost)
    axis2d_cost.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  

    axis2d_cost.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  


    axis2d_cost.set_title(r'Cost $\mu_c$',fontsize=16)
    axis2d_cost.set_xlabel('$\gamma$',fontsize=16)
    #axis2d_cost.set_ylabel('#Episode',fontsize=16)

    axis2d_cost.set_yticks([])
    axis2d_cost.set_xlim(bo.SearchSpace[0,0]-0.003, bo.SearchSpace[0,1]+0.003)
    axis2d_cost.set_ylim(bo.SearchSpace[1,0]-5, bo.SearchSpace[1,1]+5)
    
    fig.colorbar(CS_acq_cost, ax=axis2d_cost, shrink=0.9)
    
    
    # plot acquisition function
    acq_func={}
    acq_func['name']=bo.acq['name']
    acq_func['dim']=1
    acq_func['scaleSearchSpace']=bo.scaleSearchSpace

    myacq=AcquisitionFunction(acq_func)
    util_value = myacq.acq_kind(X, bo.gp)
    util_value=np.log(1+np.exp(util_value))
    util_value=np.reshape(util_value,(-1,1))
    
    
    CS_acq=axis2d_util.contourf(x1g_ori,x2g_ori,util_value.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    #axis2d_util.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Data')
    #axis2d_util.scatter(bo.X_original[idxReal[-1],0],bo.T_original[idxReal[-1],0],marker='s', color='yellow',s=80,label='Selected')
    axis2d_util.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r',label='Augmented Obs')  
    axis2d_util.scatter(bo.X_original[idxReal_ExcludeLast,0],bo.T_original[idxReal_ExcludeLast,0],marker='s',color='g',label='Obs')  


    #axis2d_util.set_yticks([])
    axis2d_util.set_ylabel('#Episode',fontsize=16)


    axis2d_util.set_title(r'Acquisition $\alpha$',fontsize=16)
    axis2d_util.set_xlim(bo.SearchSpace[0,0]-0.003, bo.SearchSpace[0,1]+0.003)
    axis2d_util.set_ylim(bo.SearchSpace[1,0]-5, bo.SearchSpace[1,1]+5)
    axis2d_util.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))

    
    fig.colorbar(CS_acq, ax=axis2d_util, shrink=0.9)


    # find the max of utility and max of cost to normalize
    x_max_acq, utility_max=acq_max_with_name(gp=bo.gp,scaleSearchSpace=bo.scaleSearchSpace,
                             acq_name=bo.acq['name'],IsReturnY=True)
    x_max_acq, utility_min=acq_max_with_name(gp=bo.gp,scaleSearchSpace=bo.scaleSearchSpace,
                             acq_name=bo.acq['name'],IsReturnY=True,IsMax=False)      
    
   
    util_min=np.min(util_value)
    cost_min=np.min(mean_cost)
    if util_min<0 or cost_min<0:
        print("bug")
    acq_value=util_value/(mean_cost)
    
    val=np.min(acq_value)
    if val<0:
        print("val<0")
    acq_value=np.reshape(acq_value,x1g.shape)
    idxMax=np.argmax(acq_value)
        
    CS_acq=axis2d_acq.contourf(x1g_ori,x2g_ori,acq_value.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    axis2d_acq.scatter(bo.X_original[idxVirtual_ExcludeLast,0],bo.T_original[idxVirtual_ExcludeLast,0],color='r')  

    axis2d_acq.scatter(bo.X_original[idxReal,0],bo.T_original[idxReal,0],marker='s',color='g')  
    axis2d_acq.scatter(bo.X_original[idxReal[-1],0],bo.T_original[idxReal[-1],0],marker='s', color='k',s=80,label='Next Point')
    #axis2d_acq.scatter(X_ori[idxMax,0],X_ori[idxMax,1],marker='^',s=100,color='k',label='Maximum')

    
    axis2d_acq.set_yticks([])

    axis2d_acq.set_title(r'Decision $\alpha$ / $\mu_c$',fontsize=16)
    axis2d_acq.set_xlim(bo.SearchSpace[0,0]-0.003, bo.SearchSpace[0,1]+0.003)
    axis2d_acq.set_ylim(bo.SearchSpace[1,0]-5, bo.SearchSpace[1,1]+5)
    
    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    axis2d_acq.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq, ax=axis2d_acq, shrink=0.9)
    

    if strFolderOut is not None:
        strFileName="{:d}_GP2d_AF.pdf".format(counter)
        strFinalPath=os.path.join(strFolderOut,strFileName)
        fig.savefig(strFinalPath, bbox_inches='tight')
    
        
def plot_bo_2d_cost_utility(bo,strFolderOut=None):
    
    global counter
    counter=counter+1
    
    x1 = np.linspace(bo.scaleSearchSpace[0,0], bo.scaleSearchSpace[0,1], 30)
    x2 = np.linspace(bo.scaleSearchSpace[1,0], bo.scaleSearchSpace[1,1], 30)
    x1g,x2g=np.meshgrid(x1,x2)
    
    X=np.c_[x1g.flatten(), x2g.flatten()]
    T=X[:,1]
    T=np.atleast_2d(T)
    T=T.T
    T-np.reshape(T,(900,-1))
    
    x1_ori = np.linspace(bo.SearchSpace[0,0], bo.SearchSpace[0,1], 30)
    x2_ori = np.linspace(bo.SearchSpace[1,0], bo.SearchSpace[1,1], 30)   
    x1g_ori,x2g_ori=np.meshgrid(x1_ori,x2_ori)
    
    X_ori=np.c_[x1g_ori.flatten(), x2g_ori.flatten()]
  
    fig = plt.figure(figsize=(18,3.5))
    
    axis2d_utility = fig.add_subplot(1, 4, 1)
    axis2d_pvrs_variance = fig.add_subplot(1, 4, 2)
    axis2d_cost = fig.add_subplot(1, 4, 3)
    axis2d_acq = fig.add_subplot(1, 4, 4)
    
    gp_mean, sigma = bo.gp.predict(X,eval_MSE=True)
    gp_mean_original=gp_mean*np.std(bo.Y_original)+np.mean(bo.Y_original)
    # plot the utility

    #utility = bo.acq_func.acq_kind(X, bo.gp)
    #acq3d.plot_surface(x1g,x1g,utility.reshape(x1g.shape))
    
    CS_acq=axis2d_utility.contourf(x1g_ori,x2g_ori,gp_mean_original.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    idxBest=np.argmax(gp_mean)
    
    x_stars=np.asarray(bo.x_stars_original)
    
    axis2d_utility.scatter(bo.X_original[:,0],bo.T_original[:,0],color='g',label='Data')  
    #acq2d.scatter(bo.X_original[-1,0],bo.X_original[-1,1],color='r',s=30,label='Previous Selection')
    axis2d_utility.scatter(bo.X_original[-1,0],bo.T_original[-1,0],marker='s', color='yellow',s=80,label='Selected')
    #axis2d_utility.scatter(X_ori[idxBest,0],X_ori[idxBest,1],marker='s',color='r',s=30,label='Peak')
#    axis2d_utility.scatter(x_stars[:,0],x_stars[:,1],label='x*',marker='*',color='r',s=50)

    
    axis2d_utility.set_title('Utility Function',fontsize=16)
    axis2d_utility.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_utility.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+5)
    
    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    axis2d_utility.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq, ax=axis2d_utility, shrink=0.9)
    

    
    # plot the cost
    #gp_mean_cost, sigma = bo.gp_cost.predict(X)
    mean_cost=bo.linear_regression.predict(T)
    gp_mean_cost=mean_cost
    #gp_mean_cost_original=gp_mean_cost*np.std(bo.Y_cost_original)+np.mean(bo.Y_cost_original)
    #acq3d.plot_surface(x1g,x1g,utility.reshape(x1g.shape))
    
    CS_acq_cost=axis2d_cost.contourf(x1g_ori,x2g_ori,mean_cost.reshape(x1g.shape),cmap=my_cmap,origin='lower')
    #CS2_acq = plt.contour(CS_acq, levels=CS_acq.levels[::2],colors='r',origin='lower',hold='on')
    
    idxBest=np.argmax(gp_mean_cost)
    
    axis2d_cost.scatter(bo.X_original[:,0],bo.T_original[:,0],color='g',label='Data')  
    #acq2d.scatter(bo.X_original[-1,0],bo.X_original[-1,1],color='r',s=30,label='Previous Selection')
    axis2d_cost.scatter(bo.X_original[-1,0],bo.T_original[-1,0],marker='s', color='yellow',s=80,label='Selected')
    #axis2d_cost.scatter(X_ori[idxBest,0],X_ori[idxBest,1],marker='s',color='r',s=30,label='Peak')
    #axis2d_cost.scatter(x_stars[:,0],x_stars[:,1],label='x*',marker='*',color='r',s=50)

    axis2d_cost.set_title('Cost Function',fontsize=16)
    axis2d_cost.set_xlabel('gamma',fontsize=16)
    axis2d_cost.set_ylabel('#Episode',fontsize=16)


    axis2d_cost.set_xlim(bo.SearchSpace[0,0], bo.SearchSpace[0,1])
    axis2d_cost.set_ylim(bo.SearchSpace[1,0], bo.SearchSpace[1,1]+10)
    
    #acq2d.legend(loc=1, bbox_to_anchor=(1.01, 1), borderaxespad=0.)
    axis2d_cost.legend(loc='center left',ncol=3,bbox_to_anchor=(0, -0.2))
      
    fig.colorbar(CS_acq_cost, ax=axis2d_cost, shrink=0.9)
    
    
   
    if strFolderOut is not None:
        strFileName="{:d}_GP2d_AF.pdf".format(counter)
        strFinalPath=os.path.join(strFolderOut,strFileName)
        fig.savefig(strFinalPath, bbox_inches='tight')
    
    
def plot_original_function(myfunction):
    
    origin = 'lower'

    func=myfunction.func

    if myfunction.input_dim>2:
        print("Cannot plot function which dimension is >2")
        return

    if myfunction.input_dim==1:    
        x = np.linspace(myfunction.SearchSpace['x'][0], myfunction.SearchSpace['x'][1], 100)
        y = func(x)
    
        fig=plt.figure(figsize=(8, 5))
        plt.plot(x, y)
        strTitle="{:s}".format(myfunction.name)

        plt.title(strTitle)
    
    if myfunction.input_dim==2:    
        
        # Create an array with parameters SearchSpace
        if isinstance(myfunction.SearchSpace,dict):
            # Get the name of the parameters        
            SearchSpace = []
            for key in myfunction.SearchSpace.keys():
                SearchSpace.append(myfunction.SearchSpace[key])
            SearchSpace = np.asarray(SearchSpace)
        else:
            SearchSpace=np.asarray(myfunction.SearchSpace)
            
        x1 = np.linspace(SearchSpace[0][0], SearchSpace[0][1], 50)
        x2 = np.linspace(SearchSpace[1][0], SearchSpace[1][1], 50)
        x1g,x2g=np.meshgrid(x1,x2)
        X_plot=np.c_[x1g.flatten(), x2g.flatten()]
        Y = func(X_plot)
    
        #fig=plt.figure(figsize=(8, 5))
        
        #fig = plt.figure(figsize=(12, 3.5))
        fig = plt.figure(figsize=(14, 4))
        
        ax3d = fig.add_subplot(1, 2, 1, projection='3d')
        ax2d = fig.add_subplot(1, 2, 2)
        
        alpha = 0.7
        ax3d.plot_surface(x1g,x2g,Y.reshape(x1g.shape),cmap=my_cmap,alpha=alpha) 
        
        
        idxBest=np.argmax(Y)
        #idxBest=np.argmin(Y)
    
        ax3d.scatter(X_plot[idxBest,0],X_plot[idxBest,1],Y[idxBest],marker='*',color='r',s=200,label='Peak')
    

        strTitle="{:s}".format(myfunction.name)
        #print strTitle
        ax3d.set_title(strTitle)
        #ax3d.view_init(40, 130)

        
        idxBest=np.argmax(Y)
        CS=ax2d.contourf(x1g,x2g,Y.reshape(x1g.shape),cmap=my_cmap,origin=origin)   
       
        #CS2 = plt.contour(CS, levels=CS.levels[::2],colors='r',origin=origin,hold='on')
        ax2d.scatter(X_plot[idxBest,0],X_plot[idxBest,1],marker='*',color='r',s=300,label='Peak')
        plt.colorbar(CS, ax=ax2d, shrink=0.9)

        ax2d.set_title(strTitle)

        
    strFileName="{:s}.eps".format(myfunction.name)
    strPath=os.path.join(out_dir,strFileName)
    fig.savefig(strPath, bbox_inches='tight')
    
File Path: setup.py
Content:
from setuptools import setup, find_packages

setup(
    name='bayes_opt',
    version='1',
    packages=find_packages(),
    include_package_data = True,
    description='BOIL',
    install_requires=[
        "numpy == 1.23",
        "scipy >= 1.4.1",
        "scikit-learn >= 1.0.2",
        "tabulate>=0.8.7",
        "matplotlib>=3.1.0",
        "tensorflow==2.8.0",
        "gym>=0.5",
        "sobol-seq>=0.2.0",
        "tensorflow-probability==0.16.0",
        "tqdm>=4.64.0",
        "pygame>=2.1.0"
    ],
)

Output:
{
    "experimental_code": "class ProductGaussianProcess(object):\n    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')\n    \n    \n    #def __init__ (self,param):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']\n#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        \"\"\"\n        Fit Gaussian Process model\n\n        Input Parameters\n        ----------\n        x: the observed points \n        t: time or number of episode\n        y: the outcome y=f(x)\n        \n        \"\"\" \n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n        #self.Y_curves=Y_curves[myidx]\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta\n    \n        Euc_dist_t=euclidean_distances(T,T)\n        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       \n    \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)\n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        \"\"\"\n        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter\n        \"\"\"\n\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            # compute K\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            # transform Y_curve to Y_original, then to Y\n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\\\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                # if the matrix is too large, we randomly select a part of the data for fast computation\n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                #Wi, LW, LWi, W_logdet = pdinv(KK)\n                #sign,W_logdet2=np.linalg.slogdet(KK)\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A\n    \n                #second_term=-0.5*W_logdet2\n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            #print(lengthscale, lengthscale_t,midpoint,growth,\"logmarginal:\",logmarginal)\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n#    def optimize_lengthscale_SE_maximizing(self,previous_theta,noise_delta):\n#        \"\"\"\n#        Optimize to select the optimal lengthscale parameter\n#        \"\"\"\n#                \n#        # define a bound on the lengthscale\n#        SearchSpace_lengthscale_min=0.01\n#        SearchSpace_lengthscale_max=0.5\n#        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n#        \n#        mySearchSpace=np.asarray([[SearchSpace_lengthscale_min,SearchSpace_lengthscale_max],\\\n#                             [10*SearchSpace_lengthscale_min,2*SearchSpace_lengthscale_max]])\n#        \n#        # Concatenate new random points to possible existing\n#        # points from self.explore method.           \n#        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, mySearchSpace.shape[0]))\n\n#        #print lengthscale_tries\n\n#        # evaluate\n#        self.flagOptimizeHyperFirst=0 # for efficiency\n\n#        logmarginal_tries=self.log_marginal_lengthscale(lengthscale_tries,noise_delta)\n#        #print logmarginal_tries\n\n#        #find x optimal for init\n#        idx_max=np.argmax(logmarginal_tries)\n#        lengthscale_init_max=lengthscale_tries[idx_max]\n#        #print lengthscale_init_max\n#        \n#        myopts ={'maxiter':20*self.dim,'maxfun':20*self.dim}\n\n#        x_max=[]\n#        max_log_marginal=None\n#        \n#        res = minimize(lambda x: -self.log_marginal_lengthscale(x,noise_delta),lengthscale_init_max,\n#                       SearchSpace=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n#        if 'x' not in res:\n#            val=self.log_marginal_lengthscale(res,noise_delta)    \n#        else:\n#            val=self.log_marginal_lengthscale(res.x,noise_delta)  \n#        \n#        # Store it if better than previous minimum(maximum).\n#        if max_log_marginal is None or val >= max_log_marginal:\n#            if 'x' not in res:\n#                x_max = res\n#            else:\n#                x_max = res.x\n#            max_log_marginal = val\n#            #print res.x\n\n#        return x_max\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        \"\"\"\n        Optimize to select the optimal lengthscale parameter\n        \"\"\"\n        \n        # define a bound on the lengthscale\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        # evaluate\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        #find x optimal for init\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        #print lengthscale_init_max\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        # Store it if better than previous minimum(maximum).\n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n            #print res.x\n\n        return x_max\n\n\n#    def optimize_lengthscale(self,previous_theta_x, previous_theta_t,noise_delta):\n#\n#        prev_theta=[previous_theta_x,previous_theta_t]\n#        newlengthscale,newlengthscale_t=self.optimize_lengthscale_SE_maximizing(prev_theta,noise_delta)\n#        self.hyper['lengthscale_x']=newlengthscale\n#        self.hyper['lengthscale_t']=newlengthscale_t\n#        \n#        # refit the model\n#        temp=np.hstack((self.X,self.T))\n#        ur = unique_rows(temp)\n#        \n#        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n#        \n#        return newlengthscale,newlengthscale_t\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        # optimize both GP lengthscale and logistic hyperparameter\n\n            \n        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        # refit the model\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        # update Y here\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        #\n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        \"\"\"\n        compute variance given X and xTest\n        \n        Input Parameters\n        ----------\n        X: the observed points\n        xTest: the testing points \n        \n        Returns\n        -------\n        diag(var)\n        \"\"\" \n        \n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            #Euc_dist=euclidean_distances(xTest,xTest)\n            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta\n            #ur = unique_rows(X)\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      \n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    \n        \n    def predict(self,xTest, eval_MSE=True):\n        \"\"\"\n        compute predictive mean and variance\n        Input Parameters\n        ----------\n        xTest: the testing points \n        \n        Returns\n        -------\n        mean, var\n        \"\"\"    \n\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        # prevent singular matrix\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        #Exp_dist_test_train_x*Exp_dist_test_train_t\n  \n        # using Cholesky update\n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n    def posterior(self,x):\n        # compute mean function and covariance function\n        return self.predict(self,x)\n        \n    \n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \n        gp_params:                  GP parameters\n        gp_params.theta:            to compute the kernel\n        gp_params.delta:            to compute the kernel\n        \n        func_params:                function to optimize\n        func_params.init bound:     initial SearchSpace for parameters\n        func_params.SearchSpace:        SearchSpace on parameters        \n        func_params.func:           a function to be optimized\n        \n        \n        acq_params:            acquisition function, \n        acq_params.acq_func['name']=['ei','ucb','poi']\n        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))\n\n\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    # this is the Logistic transformation, used in the paper\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    #print(MaxEpisode)\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\ndef return_logistic_curve(midpoint, growth, MaxEpoch=1000):\n    # given the growth, midpoint and npoint, return the Logistic curve for visualization\n    \n    def logistic_func(x):\n        #alpha=32\n        if len(x)==1:\n            return 1.0/(1+np.exp(-growth*(x-midpoint)))\n        else:\n            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]\n        \n    my_xrange_scaled=np.linspace(-6,6, MaxEpoch)\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n    \n    return my_logistic_value_scaled\n\n\ndef transform_logistic_marginal(curves,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    def transform_one_logistic_marginal(curves,MaxEpisode):\n        # curve is a vector [1 x MaxIter]\n    \n        midpoint_list=[-3,-2,-1,0,1]\n        growth_list=[0.1,1,2,3]\n        \n        temp_Y_value=[0]*(len(midpoint_list)*len(growth_list))\n        for idx, (val1, val2) in enumerate(itertools.product(midpoint_list,growth_list)):\n            temp_Y_value[idx]=apply_one_transform_logistic(curves,val1, val2,MaxEpisode)\n                \n        temp_Y_value=np.asarray(temp_Y_value)\n        \n        Y=np.mean(temp_Y_value,axis=0)\n        return Y\n    if len(curves)==1:\n        output=transform_one_logistic_marginal(curves[0],MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=transform_one_logistic_marginal(curve,MaxEpisode)\n    return output    \n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n    \n\n\nclass AcquisitionFunction(object):\n    \"\"\"\n    An object to compute the acquisition functions.\n    \"\"\"\n\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max'] # this is for ei_mu acquisition function\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max'                          ]\n        \n        # check valid acquisition function\n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        #if  not in acq_name:\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        \n        #if type(meta) is dict and 'y_max' in meta.keys():\n        #   y_max=meta['y_max']\n        y_max=np.max(gp.Y)\n        #print self.kind\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max': # using max mu(x) as incumbent\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    def utility_plot(self, x, gp, y_max):\n        if np.any(np.isnan(x)):\n            return 0\n        if self.acq_name == 'ei':\n            return self._ei_plot(x, gp, y_max)\n  \n   \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _lcb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n\n        return mean - np.sqrt(beta_t) * np.sqrt(var) \n        \n    \n    @staticmethod\n    def _ucb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T                \n        \n        # Linear in D, log in t https://github.com/kirthevasank/add-gp-bandits/blob/master/BOLibkky/getUCBUtility.m\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n  \n        #beta=300*0.1*np.log(5*len(gp.Y))# delta=0.2, gamma_t=0.1\n        return mean + np.sqrt(beta_t) * np.sqrt(var) \n    \n    \n    @staticmethod\n    def _ucb_pe(x, gp, kappa, maxlcb):\n        mean, var = gp.predict_bucb(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n\n        value=mean + kappa * np.sqrt(var)        \n        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]\n        var[myidx]=0        \n        return var\n    \n   \n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n        \n   \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)        \n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \n        out[var2<1e-10]=0\n        return out\n \n \n    @staticmethod      \n    def _poi(x, gp,y_max): # run Predictive Entropy Search using Spearmint\n        mean, var = gp.predict(x, eval_MSE=True)    \n        # Avoid points with zero variance\n        var = np.maximum(var, 1e-9 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var)        \n        return norm.cdf(z)\n\n   \ndef unique_rows(a):\n    \"\"\"\n    A functions to trim repeated rows that may appear when optimizing.\n    This is necessary to avoid the sklearn GP object from breaking\n\n    :param a: array to trim repeated rows from\n\n    :return: mask of unique rows\n    \"\"\"\n\n    # Sort array and kep track of where things should go back to\n    order = np.lexsort(a.T)\n    reorder = np.argsort(order)\n\n    a = a[order]\n    diff = np.diff(a, axis=0)\n    ui = np.ones(len(a), 'bool')\n    ui[1:] = (diff != 0).any(axis=1)\n\n    return ui[reorder]\n\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    dim=SearchSpace.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n    #sobol_sequence=generate_sobol_seq(dim=dim,nSobol=500*dim)\n\n    # multi start\n    for i in range(3*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        #x_tries=sobol_sequence\n    \n        # evaluate\n        y_tries=myfunc(x_tries,**kwargs)\n        \n        #find x optimal for init\n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        # Store it if better than previous minimum(maximum).\n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n            #print max_acq\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n    \ndef acq_max_scipy(ac, gp, y_max, bounds):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n\n    # multi start\n    for i in range(1*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        # evaluate\n        y_tries=ac(x_tries,gp=gp)\n        #print \"elapse evaluate={:.5f}\".format(end_eval-start_eval)\n        \n        #find x optimal for init\n        idx_max=np.argmax(y_tries)\n        #print \"max y_tries {:.5f} y_max={:.3f}\".format(np.max(y_tries),y_max)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n\n        \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n            #print max_acq\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])\n        #print max_acq\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n    \n    # COBYLA -> x_max[0]\n    # L-BFGS-B -> x_max",
    "experimental_info": "BOIL (Bayesian Optimization with Informed Learning) is implemented using a `ProductGaussianProcess` for the objective function and a `linear_model.LinearRegression` for the training time cost. The default acquisition function is 'ei_mu_max' (Expected Improvement using the maximum of the GP mean as incumbent).\n\n**GP Settings (`ProductGaussianProcess`):**\n*   Initial noise variance (`noise_delta`): 5e-4\n*   Initial lengthscale for hyperparameters `x` (`lengthscale_x`): 0.02\n*   Initial lengthscale for training iterations `t` (`lengthscale_t`): 0.2\n*   Hyperparameter optimization: Performed every `2 * dim` iterations.\n    *   Optimization bounds for lengthscales: `x` from 0.03 to 0.3; `t` from `10 * 0.03` to `2 * 0.3` (i.e., 0.3 to 0.6).\n    *   Number of random initial points for multi-start optimization: 20.\n    *   Optimizer options: `maxiter`: `30 * dim`, `maxfun`: `30 * dim`.\n\n**Learning Curve Compression (Sigmoid Preference Function):**\n*   Initial sigmoid midpoint (`midpoint`): 0.0\n*   Initial sigmoid growth (`growth`): 1.0\n*   Optimization bounds for sigmoid parameters: `midpoint` from -2 to 3; `growth` from 0.5 to 2.\n*   Learning method: Maximizing the GP's log marginal likelihood.\n*   Marginalization for final evaluation: The final score is evaluated by marginalizing across predefined lists of sigmoid `midpoint_list` ([-3, -2, -1, 0, 1]) and `growth_list` ([0.1, 1, 2, 3]).\n\n**Acquisition Function Maximization:**\n*   Optimization method: SciPy's L-BFGS-B (`minimize` function).\n*   Multi-start optimization: `1 * dim` restarts for maximization (`acq_max_scipy`), `3 * dim` restarts for minimization (`acq_min_scipy_kwargs`).\n*   Random initial points for optimization: `50 * dim` points for `acq_max_scipy`, `100 * dim` points for `acq_min_scipy_kwargs`.\n*   Optimizer options: `maxiter`: `10 * dim`, `maxfun`: `20 * dim`.\n\n**Data Augmentation (Virtual Observations):**\n*   Mechanism: Subsets of points from observed learning curves are actively selected at regions of maximum GP predictive uncertainty.\n*   Maximum number of augmentations per real observation (`max_n_augmentation`): 10.\n*   Adaptive control: Augmentation stops if the log of the GP covariance matrix's condition number (`log_cond`) exceeds a threshold of 15 (`threshold_cond`) or if the predictive variance is very small (`< 1e-3` after adding noise `1e-3` which is effectively `noise_delta`).\n\n**Initialization:**\n*   Number of initial points (`n_init_points`): 3 (default).\n*   Random seed for initialization (`seed`): 1 (default)."
}
