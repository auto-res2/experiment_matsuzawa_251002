
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
DPL models learning curves as power law functions: `f̂(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`, where `g` is a parametric neural network mapping hyperparameter configurations `λ` to the power law coefficients `α, β, γ`, and `b` is the budget (epochs). An ensemble of K such neural networks is trained to provide probabilistic predictions (mean `μ` and variance `σ²`). Bayesian Optimization (BO) with an Expected Improvement (EI) acquisition function is used to recommend the next hyperparameter configuration `λN+1`. Instead of running `λN+1` to full convergence, a multi-fidelity strategy incrementally advances it by a small budget `b_step` (e.g., one epoch). The network is a 2-layer feedforward neural network with 128 units per layer, Leaky ReLU, and GLU non-linearity on `β` and `γ` output units, trained with L1 loss and Adam.

# Repository Content
File Path: benchmarks/__init__.py
Content:

File Path: benchmarks/benchmark.py
Content:
class BaseBenchmark:

    nr_hyperparameters = None
    max_budget = None
    log_indicator = None
    hp_names = None
    # if the best value corresponds to a lower value
    minimization_metric = True

    def __init__(self, path_to_json_file: str):

        self.path_to_json_file = path_to_json_file

    def _load_benchmark(self):

        raise NotImplementedError('Please implement the load_benchmark method')

    def load_dataset_names(self):
        raise NotImplementedError('Please implement the load_dataset_names method')

    def get_hyperparameter_candidates(self):

        raise NotImplementedError('Please extend the get_hyperparameter_candidates method')

    def get_performance(self, hp_index: int, budget: int):

        raise NotImplementedError('Please extend the get_performance method')

File Path: benchmarks/hyperbo.py
Content:
from collections import OrderedDict
import math

import numpy as np
import pandas as pd
from syne_tune.blackbox_repository import load_blackbox
from syne_tune.config_space import is_log_space

from benchmarks.benchmark import BaseBenchmark


class PD1(BaseBenchmark):

    def __init__(self, path_to_json_files: str, dataset_name: str, eta=3, number_of_brackets=3):

        super().__init__(path_to_json_files)
        self.dataset_name = dataset_name
        self.blackbox = self._load_benchmark()

        self.hp_names = None
        self.hp_candidates = self.blackbox[dataset_name].hyperparameters.to_numpy()

        # hyperparameter candidates, seed, fidelity, metrics
        # 0 is the validation train error
        self.validation_error_rate = self.blackbox[dataset_name].objectives_evaluations[:, :, :, 0]
        self.validation_error_rate = np.mean(self.validation_error_rate, axis=1)

        self.max_value = 0.0
        self.min_value = 1.0

        self.eta = eta
        self.number_of_brackets = number_of_brackets

        filtered_indices = self.filter_curves()
        self.max_budget = self.blackbox[dataset_name].num_fidelities
        # considering an eta of 3
        self.min_budget = int(self.max_budget / math.pow(self.eta, self.number_of_brackets))
        self.min_budget = self.min_budget if self.min_budget > 0 else 1
        self.log_indicator = self.get_log_indicator()
        self.categorical_indicator = [False] * self.hp_candidates[1]
        self.validation_error_rate = self.validation_error_rate[filtered_indices]
        self.hp_candidates = self.hp_candidates[filtered_indices]
        self.nr_hyperparameters = self.validation_error_rate.shape[0]

        hp_names = list(self.blackbox[dataset_name].hyperparameters.columns)

        self.param_space = OrderedDict(
            [
                (hp_names[i], [self.blackbox[dataset_name].hyperparameters[hp_names[i]].min(), self.blackbox[dataset_name].hyperparameters[hp_names[i]].max(), float, self.log_indicator[i]]) for i in range(len(hp_names))
            ]
        )

    def get_worst_performance(self):

        return np.amax(self.validation_error_rate)

    def _load_benchmark(self):

        return load_blackbox('pd1')

    @staticmethod
    def load_dataset_names():

        dataset_names = []
        enough_lc_points = 10

        blackbox = load_blackbox('pd1')
        for dataset_name in blackbox:
            if blackbox[dataset_name].num_fidelities > enough_lc_points:
                dataset_names.append(dataset_name)

        return dataset_names

    def get_log_indicator(self):

        log_indicator = [is_log_space(v) for v in self.blackbox[self.dataset_name].configuration_space.values()]

        return log_indicator

    def get_hyperparameter_candidates(self) -> np.ndarray:

        return self.hp_candidates

    def get_performance(self, hp_index: int, budget: int) -> float:

        budget = int(budget)
        val_performance = self.validation_error_rate[hp_index, budget - 1]

        return float(val_performance)

    def get_curve(self, hp_index: int, budget: int) -> float:

        budget = int(budget)
        val_curve = self.validation_error_rate[hp_index, 0:budget]
        return val_curve.tolist()

    def get_incumbent_curve(self):

        best_value = np.inf
        best_index = -1
        for index in range(0, self.validation_error_rate.shape[0]):
            val_error_curve = self.validation_error_rate[index, :]
            best_performance = min(val_error_curve)

            if best_performance < best_value:
                best_value = best_performance
                best_index = index

        return self.validation_error_rate[best_index]

    def get_gap_performance(self):

        incumbent_curve = self.get_incumbent_curve()
        best_value = min(incumbent_curve)
        worst_value = self.get_worst_performance()

        return worst_value - best_value

    def get_incumbent_config_index(self):

        best_value = np.inf
        best_index = -1
        for index in range(0, self.validation_error_rate.shape[0]):
            val_error_curve = self.validation_error_rate[index, :]
            best_performance = min(val_error_curve)

            if best_performance < best_value:
                best_value = best_performance
                best_index = index

        return best_index

    def filter_curves(self):

        validation_curves = pd.DataFrame(self.validation_error_rate)
        # TODO do a query for both values instead of going through the df twice
        non_nan_idx = validation_curves.notnull().all(axis=1)
        non_diverging_idx = (validation_curves < validation_curves.quantile(0.95).min()).all(axis=1)

        idx = non_nan_idx & non_diverging_idx

        return idx

File Path: benchmarks/lcbench.py
Content:
from collections import OrderedDict
from typing import List

import numpy as np

from benchmarks.benchmark import BaseBenchmark
from lc_bench.api import Benchmark


class LCBench(BaseBenchmark):

    nr_hyperparameters = 2000
    # Declaring the search space for LCBench
    param_space = OrderedDict([
        ('batch_size', [16, 512, int, True]),
        ('learning_rate', [0.0001, 0.1, float, True]),
        ('momentum', [0.1, 0.99, float, False]),
        ('weight_decay', [0.00001, 0.1, float, False]),
        ('num_layers', [1, 5, int, False]),
        ('max_units', [64, 1024, int, True]),
        ('max_dropout', [0.0, 1.0, float, False]),
    ])
    max_budget = 51
    min_budget = 1

    hp_names = list(param_space.keys())

    log_indicator = [True, True, False, False, False, True, False]

    # if the best value corresponds to a lower value
    minimization_metric = False

    def __init__(self, path_to_json_file: str, dataset_name: str):

        super().__init__(path_to_json_file)
        self.benchmark = self._load_benchmark()
        self.dataset_name = dataset_name
        self.dataset_names = self.load_dataset_names()
        self.categorical_indicator = [False] * len(self.param_space)
        self.max_value = 1.0
        self.min_value = 0.0

    def get_worst_performance(self):

        # since it is accuracy for LCBench
        min_value = 100
        for hp_index in range(0, LCBench.nr_hyperparameters):
            val_curve = self.benchmark.query(
                dataset_name=self.dataset_name,
                config_id=hp_index,
                tag='Train/val_balanced_accuracy',
            )
            val_curve = val_curve[1:]
            worst_performance_hp_curve = min(val_curve)
            if worst_performance_hp_curve < min_value:
                min_value = worst_performance_hp_curve

        return min_value

    def _load_benchmark(self):

        bench = Benchmark(
            data_dir=self.path_to_json_file,
        )

        return bench

    def load_dataset_names(self) -> List[str]:

        return self.benchmark.get_dataset_names()

    def get_hyperparameter_candidates(self) -> np.ndarray:

        hp_names = list(LCBench.param_space.keys())
        hp_configs = []
        for i in range(LCBench.nr_hyperparameters):
            hp_config = []
            config = self.benchmark.query(
                dataset_name=self.dataset_name,
                tag='config',
                config_id=i,
            )
            for hp_name in hp_names:
                hp_config.append(config[hp_name])
            hp_configs.append(hp_config)

        hp_configs = np.array(hp_configs)

        return hp_configs

    def get_performance(self, hp_index: int, budget: int) -> float:

        val_curve = self.benchmark.query(
            dataset_name=self.dataset_name,
            config_id=hp_index,
            tag='Train/val_balanced_accuracy',
        )
        val_curve = val_curve[1:]
        budget = int(budget)

        return val_curve[budget - 1]

    def get_curve(self, hp_index: int, budget: int) -> List[float]:

        val_curve = self.benchmark.query(
            dataset_name=self.dataset_name,
            config_id=hp_index,
            tag='Train/val_balanced_accuracy',
        )
        val_curve = val_curve[1:]
        budget = int(budget)

        return val_curve[0:budget]

    def get_incumbent_curve(self):

        inc_curve = self.benchmark.query_best(
            self.dataset_name,
            "Train/val_balanced_accuracy",
            "Train/val_balanced_accuracy",
            0,
        )
        inc_curve = inc_curve[1:]

        return inc_curve

    def get_max_value(self):

        return max(self.get_incumbent_curve())

    def get_incumbent_config_id(self):

        best_value = 0
        best_index = -1
        for index in range(0, LCBench.nr_hyperparameters):
            val_curve = self.benchmark.query(
                dataset_name=self.dataset_name,
                config_id=index,
                tag='Train/val_balanced_accuracy',
            )
            val_curve = val_curve[1:]
            max_value = max(val_curve)

            if max_value > best_value:
                best_value = max_value
                best_index = index

        return best_index

    def get_gap_performance(self):

        incumbent_curve = self.get_incumbent_curve()
        best_value = max(incumbent_curve)
        worst_value = self.get_worst_performance()

        return best_value - worst_value

    def get_step_cost(self, hp_index: int, budget: int):

        time_cost_curve = self.benchmark.query(
            dataset_name=self.dataset_name,
            config_id=hp_index,
            tag='time',
        )
        time_cost_curve = time_cost_curve[1:]
        budget = int(budget)
        if budget > 1:
            step_cost = time_cost_curve[budget - 1] - time_cost_curve[budget - 2]
        else:
            step_cost = time_cost_curve[budget - 1]

        return step_cost

    def set_dataset_name(self, dataset_name: str):

        self.dataset_name = dataset_name

File Path: benchmarks/taskset.py
Content:
import json
import os
from typing import List

import numpy as np
import pandas as pd

from benchmarks.benchmark import BaseBenchmark


class TaskSet(BaseBenchmark):

    nr_hyperparameters = 1000
    max_budget = 51

    hp_names = [
        'learning_rate',
        'beta1',
        'beta2',
        'epsilon',
        'l1',
        'l2',
        'linear_decay',
        'exponential_decay',
    ]

    log_indicator = [True, False, False, True, True, True, True, True]

    def __init__(self, path_to_json_files: str, dataset_name: str):

        super().__init__(path_to_json_files)
        self.dataset_name = dataset_name
        self.hp_candidates = []
        self.training_curves = []
        self.validation_curves = []
        self.test_curves = []

        self._load_benchmark()

        filtered_indices = self.filter_curves()
        self.validation_curves = np.array(self.validation_curves)
        self.validation_curves = self.validation_curves[filtered_indices]
        self.hp_candidates = np.array(self.hp_candidates)
        self.hp_candidates = self.hp_candidates[filtered_indices]

        self.categorical_indicator = [False] * self.hp_candidates[1]
        self.min_value = self.get_worst_performance()
        self.max_value = self.get_best_performance()

    def get_worst_performance(self):
        # for taskset we have loss, so the worst value possible value
        # is infinity
        worst_value = 0
        for hp_index in range(0, self.validation_curves.shape[0]):
            val_curve = self.validation_curves[hp_index]
            worst_performance_hp_curve = max(val_curve)
            if worst_performance_hp_curve > worst_value:
                worst_value = worst_performance_hp_curve

        return worst_value

    def get_best_performance(self):

        incumbent_curve = self.get_incumbent_curve()
        best_value = min(incumbent_curve)

        return best_value

    def _load_benchmark(self):

        dataset_file = os.path.join(self.path_to_json_file, f'{self.dataset_name}.json')

        with open(dataset_file, 'r') as fp:
            dataset_info = json.load(fp)

        for optimization_iteration in dataset_info:
            hp_configuration = optimization_iteration['hp']
            train_curve = optimization_iteration['train']['loss']
            validation_curve = optimization_iteration['valid']['loss']
            test_curve = optimization_iteration['test']['loss']

            # keep a fixed order for the hps and their values,
            # just in case
            new_hp_configuration = []
            for hp_name in self.hp_names:
                new_hp_configuration.append(hp_configuration[hp_name])

            self.hp_candidates.append(new_hp_configuration)
            self.training_curves.append(train_curve)
            self.validation_curves.append(validation_curve)
            self.test_curves.append(test_curve)

    def load_dataset_names(self) -> List[str]:

        dataset_file_names = [
            dataset_file_name[:-5] for dataset_file_name in os.listdir(self.path_to_json_file)
            if os.path.isfile(os.path.join(self.path_to_json_file, dataset_file_name))
        ]

        return dataset_file_names

    def get_hyperparameter_candidates(self) -> np.ndarray:

        return np.array(self.hp_candidates)

    def get_performance(self, hp_index: int, budget: int) -> float:

        val_curve = self.validation_curves[hp_index]

        budget = int(budget)

        return val_curve[budget - 1]

    def get_curve(self, hp_index: int, budget: int) -> float:

        val_curve = self.validation_curves[hp_index]

        budget = int(budget)

        return val_curve[0:budget].tolist()

    def get_incumbent_curve(self):

        best_value = np.inf
        best_index = -1
        for index in range(0, self.validation_curves.shape[0]):
            val_curve = self.validation_curves[index]
            min_loss = min(val_curve)

            if min_loss < best_value:
                best_value = min_loss
                best_index = index

        return self.validation_curves[best_index]

    def get_gap_performance(self):

        incumbent_curve = self.get_incumbent_curve()
        best_value = min(incumbent_curve)
        worst_value = self.get_worst_performance()

        return worst_value - best_value

    def get_incumbent_config_index(self):

        best_value = np.inf
        best_index = -1
        for index in range(0, self.validation_curves.shape[0]):
            val_curve = self.validation_curves[index]
            min_loss = min(val_curve)

            if min_loss < best_value:
                best_value = min_loss
                best_index = index

        return best_index

    def log_transform_labels(self):

        validation_curves = np.array(self.validation_curves).flatten()
        max_value = np.amax(validation_curves)
        min_value = np.amin(validation_curves)
        self.max_value = max_value
        self.min_value = min_value

        f = lambda x: (np.log(x) - np.log(min_value)) / (np.log(max_value) - np.log(min_value))

        log_transformed_values = f(self.validation_curves)

        return log_transformed_values.tolist()

    def filter_curves(self):

        validation_curves = np.array(self.validation_curves)
        validation_curves = pd.DataFrame(validation_curves)
        # TODO do a query for both values instead of going through the df twice
        non_nan_idx = validation_curves.notnull().all(axis=1)
        non_diverging_idx = (validation_curves < validation_curves.quantile(0.95).min()).all(axis=1)

        idx = non_nan_idx & non_diverging_idx

        return idx

File Path: data_loader/__init__.py
Content:

File Path: data_loader/tabular_data_loader.py
Content:
class WrappedDataLoader:
    def __init__(self, dl, dev):
        self.dl = dl
        self.device = dev

    def __len__(self):
        return len(self.dl)

    def __iter__(self):
        batches = iter(self.dl)
        for b in batches:
            yield b[0].to(self.device), b[1].to(self.device), b[2].to(self.device), b[3].to(self.device)

File Path: dataset/__init__.py
Content:

File Path: dataset/tabular_dataset.py
Content:
from torch.utils.data import Dataset


class TabularDataset(Dataset):

    def __init__(self, X, y, budgets, curves):
        self.X = X
        self.y = y
        self.budgets = budgets
        self.curves = curves

    def __len__(self):
        return self.y.size

    def __getitem__(self, idx):

        return self.X[idx], self.y[idx], self.budgets[idx], self.curves[idx]





File Path: framework.py
Content:
import argparse
import json
import os
import time

import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, OneHotEncoder

from benchmarks.lcbench import LCBench
from benchmarks.taskset import TaskSet
from benchmarks.hyperbo import PD1
from surrogate_models.power_law_surrogate import PowerLawSurrogate
from surrogate_models.asha import AHBOptimizer
from surrogate_models.dehb.interface import DEHBOptimizer
from surrogate_models.dragonfly import DragonFlyOptimizer
from surrogate_models.random_search import RandomOptimizer


class Framework:

    def __init__(
        self,
        args: argparse.Namespace,
        seed: int,
    ):
        """
        Args:
            args: Namespace
                Includes all the arguments given as variables to the main_experiment
                script.
            seed: int
                The seed for the experiment.
        """

        if args.benchmark_name == 'lcbench':
            benchmark_extension = os.path.join(
                'lc_bench',
                'results',
                'data_2k.json',
            )
        elif args.benchmark_name == 'taskset':
            benchmark_extension = os.path.join(
                'data',
                'taskset',
            )
        elif args.benchmark_name == 'pd1':
            benchmark_extension = 'pd1'
        else:
            raise ValueError(f'Benchmark {args.benchmark_name} not supported')

        benchmark_data_path = os.path.join(
            args.project_dir,
            benchmark_extension,
        )

        benchmark_types = {
            'lcbench': LCBench,
            'taskset': TaskSet,
            'pd1': PD1,
        }

        surrogate_types = {
            'power_law': PowerLawSurrogate,
            'asha': AHBOptimizer,
            'dehb': DEHBOptimizer,
            'dragonfly': DragonFlyOptimizer,
            'random': RandomOptimizer,
        }

        disable_preprocessing = {
            'dehb',
        }

        self.benchmark = benchmark_types[args.benchmark_name](benchmark_data_path, args.dataset_name)
        self.dataset_name = args.dataset_name
        self.seed = seed
        self.max_value = self.benchmark.max_value
        self.min_value = self.benchmark.min_value
        self.total_budget = args.budget_limit
        self.fantasize_step = args.fantasize_step

        self.categorical_indicator = self.benchmark.categorical_indicator
        self.log_indicator = self.benchmark.log_indicator
        self.hp_names = self.benchmark.hp_names
        self.minimization_metric = self.benchmark.minimization_metric
        self.info_dict = dict()
        self.result_dir = os.path.join(
            args.output_dir,
            args.benchmark_name,
            args.surrogate_name,
        )
        os.makedirs(self.result_dir, exist_ok=True)

        self.result_file = os.path.join(
            self.result_dir,
            f'{self.dataset_name}_{self.seed}.json',
        )

        if args.surrogate_name not in disable_preprocessing:
            self.hp_candidates = self.preprocess(self.benchmark.get_hyperparameter_candidates())
        else:
            self.hp_candidates = self.benchmark.get_hyperparameter_candidates()

        if args.surrogate_name == 'power_law':
            self.surrogate = surrogate_types[args.surrogate_name](
                self.hp_candidates,
                seed=seed,
                max_benchmark_epochs=self.benchmark.max_budget,
                ensemble_size=args.ensemble_size,
                nr_epochs=args.nr_epochs,
                fantasize_step=self.fantasize_step,
                minimization=self.minimization_metric,
                total_budget=args.budget_limit,
                device='cpu',
                dataset_name=args.dataset_name,
                output_path=self.result_dir,
                max_value=self.max_value,
                min_value=self.min_value,
            )
        else:
            self.surrogate = surrogate_types[args.surrogate_name](
                hyperparameter_candidates=self.hp_candidates,
                param_space=self.benchmark.param_space,
                min_budget=self.benchmark.min_budget,
                max_budget=self.benchmark.max_budget,
                eta=3,
                seed=seed,
                max_nr_trials=args.budget_limit,
                maximization=not self.benchmark.minimization_metric,
            )

    def run(self):

        evaluated_configs = dict()
        surrogate_budget = 0

        if self.benchmark.minimization_metric:
            best_value = np.inf
        else:
            best_value = 0

        while surrogate_budget < self.total_budget:

            start_time = time.time()
            hp_index, budget = self.surrogate.suggest()
            hp_curve = self.benchmark.get_curve(hp_index, budget)

            self.surrogate.observe(hp_index, budget, hp_curve)
            time_duration = time.time() - start_time

            if hp_index in evaluated_configs:
                previous_budget = evaluated_configs[hp_index]
            else:
                previous_budget = 0

            budget_cost = budget - previous_budget
            evaluated_configs[hp_index] = budget

            step_time_duration = time_duration / budget_cost

            for epoch in range(previous_budget + 1, budget + 1):
                epoch_performance = float(hp_curve[epoch - 1])
                if self.benchmark.minimization_metric:
                    if best_value > epoch_performance:
                        best_value = epoch_performance
                else:
                    if best_value < epoch_performance:
                        best_value = epoch_performance

                surrogate_budget += 1

                if surrogate_budget > self.total_budget:
                    exit(0)

                self.log_info(
                    int(hp_index),
                    epoch_performance,
                    epoch,
                    best_value,
                    step_time_duration,
                )

        exit(0)

    def preprocess(self, hp_candidates: np.ndarray) -> np.ndarray:
        """Preprocess the hyperparameter candidates.

        Performs min-max standardization for the numerical attributes and
        additionally one-hot encoding for the categorical attributes.

        Args:
            hp_candidates: np.ndarray
                The hyperparameter candidates in their raw form as taken
                from the benchmark.

        Returns:
            preprocessed_candidates: np.ndarray
                The transformed hyperparameter candidates after being
                preprocessed.
        """
        column_transformers = []
        numerical_columns = [
            col_index for col_index, category_indicator in enumerate(self.categorical_indicator)
            if not category_indicator
        ]
        categorical_columns = [
            col_index for col_index, category_indicator in enumerate(self.categorical_indicator)
            if category_indicator
        ]

        general_transformers = []

        if len(numerical_columns) > 0:

            if self.log_indicator is not None and any(self.log_indicator):
                log_columns = [col_index for col_index, log_indicator in enumerate(self.log_indicator) if log_indicator]
                log_transformer = FunctionTransformer(np.log)
                column_transformers.append(
                    (
                        'log_pre',
                        ColumnTransformer(
                            [('log', log_transformer, log_columns)],
                            remainder='passthrough'
                        )
                    )
                )

            general_transformers.append(('num', MinMaxScaler(), numerical_columns))

        if len(categorical_columns) > 0:

            general_transformers.append(
                (
                    'cat',
                    OneHotEncoder(
                        categories=[self.hp_names] * hp_candidates.shape[1],
                        sparse=False,
                    ),
                    categorical_columns,
                )
            )
        column_transformers.append(('feature_types_pre', ColumnTransformer(general_transformers)))

        preprocessor = Pipeline(
            column_transformers
        )
        # TODO log preprocessing will push numerical columns to the right
        # so a mapping has to happen for the feature_types_pre
        preprocessed_candidates = preprocessor.fit_transform(hp_candidates)

        return preprocessed_candidates

    def log_info(
            self,
            hp_index: int,
            performance: float,
            budget: int,
            best_value_observed: float,
            time_duration: float,
    ):
        """Log information after every HPO iteration.

        Args:
            hp_index: int
                The index of the suggested hyperparameter candidate.
            performance: float
                The performance of the hyperparameter candidate.
            budget: int
                The budget at which the hyperpararameter candidate has been evaluated so far.
            best_value_observed: float
                The incumbent value observed so far during the optimization.
            time_duration: float
                The time taken for the HPO iteration.
        """
        if 'hp' in self.info_dict:
            self.info_dict['hp'].append(hp_index)
        else:
            self.info_dict['hp'] = [hp_index]

        accuracy_performance = performance

        if 'scores' in self.info_dict:
            self.info_dict['scores'].append(accuracy_performance)
        else:
            self.info_dict['scores'] = [accuracy_performance]

        incumbent_acc_performance = best_value_observed

        if 'curve' in self.info_dict:
            self.info_dict['curve'].append(incumbent_acc_performance)
        else:
            self.info_dict['curve'] = [incumbent_acc_performance]

        if 'epochs' in self.info_dict:
            self.info_dict['epochs'].append(budget)
        else:
            self.info_dict['epochs'] = [budget]

        if 'overhead' in self.info_dict:
            self.info_dict['overhead'].append(time_duration)
        else:
            self.info_dict['overhead'] = [time_duration]

        with open(self.result_file, 'w') as fp:
            json.dump(self.info_dict, fp)

File Path: main_experiment.py
Content:
import argparse

import numpy as np

from framework import Framework

parser = argparse.ArgumentParser(
    description='DPL publication experiments.',
)
parser.add_argument(
    '--index',
    type=int,
    default=1,
    help='The worker index. Every worker runs the same experiment, however, with a different seed.',
)
parser.add_argument(
    '--fantasize_step',
    type=int,
    default=1,
    help='The step used in fantasizing the next learning curve value from the last'
         'observed one for a certain hyperparameter configuration.',
)
parser.add_argument(
    '--budget_limit',
    type=int,
    default=1000,
    help='The maximal number of HPO iterations.',
)
parser.add_argument(
    '--ensemble_size',
    type=int,
    default=5,
    help='The ensemble size for the DPL surrogate.',
)
parser.add_argument(
    '--nr_epochs',
    type=int,
    default=250,
    help='The number of epochs used to train (not refine) the HPO surrogate.',
)
parser.add_argument(
    '--dataset_name',
    type=str,
    default='credit-g',
    help='The name of the dataset used in the experiment.'
         'The dataset names must be matched with the benchmark they belong to.',
)
parser.add_argument(
    '--benchmark_name',
    type=str,
    default='lcbench',
    help='The name of the benchmark used in the experiment. '
         'Every benchmark offers its own distinctive datasets. Available options are lcbench, taskset and pd1.',
)
parser.add_argument(
    '--surrogate_name',
    type=str,
    default='power_law',
    help='The method that will be run.',
)
parser.add_argument(
    '--project_dir',
    type=str,
    default='.',
    help='The directory where the project files are located.',
)
parser.add_argument(
    '--output_dir',
    type=str,
    default='./output',
    help='The directory where the project output files will be stored.',
)

args = parser.parse_args()
seeds = np.arange(10)
seed = seeds[args.index - 1]

framework = Framework(args, seed)
framework.run()

File Path: models/__init__.py
Content:

File Path: models/breaking_power_law.py
Content:
import torch
import torch.nn as nn


class BreakingPowerLaw(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        use_learning_curve: bool = True,
        kernel_size: int = 3,
        nr_filters: int = 4,
        nr_cnn_layers: int = 2,
    ):
        """
        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            use_learning_curve: bool
                If the learning curve should be use in the network.
            kernel_size: int
                The size of the kernel that is applied in the cnn layer.
            nr_filters: int
                The number of filters that are used in the cnn layers.
            nr_cnn_layers: int
                The number of cnn layers to be used.
        """
        super(BreakingPowerLaw, self).__init__()

        self.use_learning_curve = use_learning_curve
        self.kernel_size = kernel_size
        self.nr_filters = nr_filters
        self.nr_cnn_layers = nr_cnn_layers

        self.act_func = torch.nn.LeakyReLU()
        self.last_act_func = torch.nn.GLU()
        self.tan_func = torch.nn.Tanh()
        self.sigmoid_func = torch.nn.Sigmoid()
        self.batch_norm = torch.nn.BatchNorm1d

        layers = []
        # adding one since we concatenate the features with the budget
        nr_initial_features = nr_initial_features
        if self.use_learning_curve:
            nr_initial_features = nr_initial_features + nr_filters

        layers.append(nn.Linear(nr_initial_features, nr_units))
        layers.append(self.act_func)

        for i in range(2, nr_layers + 1):
            layers.append(nn.Linear(nr_units, nr_units))
            layers.append(self.act_func)

        last_layer = nn.Linear(nr_units, 6)
        layers.append(last_layer)

        self.layers = torch.nn.Sequential(*layers)

        cnn_part = []
        if use_learning_curve:
            cnn_part.append(
                nn.Conv1d(
                    in_channels=2,
                    kernel_size=(self.kernel_size,),
                    out_channels=self.nr_filters,
                ),
            )
            for i in range(1, self.nr_cnn_layers):
                cnn_part.append(self.act_func)
                cnn_part.append(
                    nn.Conv1d(
                        in_channels=self.nr_filters,
                        kernel_size=(self.kernel_size,),
                        out_channels=self.nr_filters,
                    ),
                ),
            cnn_part.append(nn.AdaptiveAvgPool1d(1))

        self.cnn = nn.Sequential(*cnn_part)

    def forward(
        self,
        x: torch.Tensor,
        predict_budgets: torch.Tensor,
        evaluated_budgets: torch.Tensor,
        learning_curves: torch.Tensor,
    ):
        """
        Args:
            x: torch.Tensor
                The examples.
            predict_budgets: torch.Tensor
                The budgets for which the performance will be predicted for the
                hyperparameter configurations.
            evaluated_budgets: torch.Tensor
                The budgets for which the hyperparameter configurations have been
                evaluated so far.
            learning_curves: torch.Tensor
                The learning curves for the hyperparameter configurations.
        """
        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)
        if self.use_learning_curve:
            lc_features = self.cnn(learning_curves)
            # revert the output from the cnn into nr_rows x nr_kernels.
            lc_features = torch.squeeze(lc_features, 2)
            x = torch.cat((x, lc_features), dim=1)

        x = self.layers(x)
        a = x[:, 0]
        b = x[:, 1]
        c0 = x[:, 2]
        c1 = x[:, 3]
        #c2 = x[:, 4]
        d1 = x[:, 4]
        #d2 = x[:, 6]
        f1 = x[:, 5]
        #f2 = x[:, 8]

        output = torch.add(
            a,
            torch.mul(
            torch.mul(
            #torch.mul(
                self.last_act_func(torch.cat((b, b))),
                torch.pow(
                    predict_budgets,
                    torch.mul(self.last_act_func(torch.cat((c0, c0))), -1)
                )
            ),
            torch.pow(torch.add(1, torch.pow(torch.div(predict_budgets, self.sigmoid_func(d1)), torch.div(1, self.sigmoid_func(f1)))), torch.mul(-c1, self.sigmoid_func(f1)))
            ),
            #torch.pow(torch.add(1, torch.pow(torch.div(predict_budgets, self.sigmoid_func(d2)), torch.div(1, self.sigmoid_func(f2)))), torch.mul(-c2, self.sigmoid_func(f2)))
        )

        return output

File Path: models/conditioned_janoschek.py
Content:
import torch
import torch.nn as nn


class ConditionedJanoschek(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        use_learning_curve: bool = True,
        kernel_size: int = 3,
        nr_filters: int = 4,
        nr_cnn_layers: int = 2,
    ):
        """
        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            use_learning_curve: bool
                If the learning curve should be use in the network.
            kernel_size: int
                The size of the kernel that is applied in the cnn layer.
            nr_filters: int
                The number of filters that are used in the cnn layers.
            nr_cnn_layers: int
                The number of cnn layers to be used.
        """
        super(ConditionedJanoschek, self).__init__()

        self.use_learning_curve = use_learning_curve
        self.kernel_size = kernel_size
        self.nr_filters = nr_filters
        self.nr_cnn_layers = nr_cnn_layers

        self.act_func = torch.nn.LeakyReLU()
        self.last_act_func = torch.nn.GLU()
        self.tan_func = torch.nn.Tanh()
        self.batch_norm = torch.nn.BatchNorm1d

        layers = []
        # adding one since we concatenate the features with the budget
        nr_initial_features = nr_initial_features
        if self.use_learning_curve:
            nr_initial_features = nr_initial_features + nr_filters

        layers.append(nn.Linear(nr_initial_features, nr_units))
        layers.append(self.act_func)

        for i in range(2, nr_layers + 1):
            layers.append(nn.Linear(nr_units, nr_units))
            layers.append(self.act_func)

        last_layer = nn.Linear(nr_units, 4)
        layers.append(last_layer)

        self.layers = torch.nn.Sequential(*layers)

        cnn_part = []
        if use_learning_curve:
            cnn_part.append(
                nn.Conv1d(
                    in_channels=2,
                    kernel_size=(self.kernel_size,),
                    out_channels=self.nr_filters,
                ),
            )
            for i in range(1, self.nr_cnn_layers):
                cnn_part.append(self.act_func)
                cnn_part.append(
                    nn.Conv1d(
                        in_channels=self.nr_filters,
                        kernel_size=(self.kernel_size,),
                        out_channels=self.nr_filters,
                    ),
                ),
            cnn_part.append(nn.AdaptiveAvgPool1d(1))

        self.cnn = nn.Sequential(*cnn_part)

    def forward(
        self,
        x: torch.Tensor,
        predict_budgets: torch.Tensor,
        evaluated_budgets: torch.Tensor,
        learning_curves: torch.Tensor,
    ):
        """
        Args:
            x: torch.Tensor
                The examples.
            predict_budgets: torch.Tensor
                The budgets for which the performance will be predicted for the
                hyperparameter configurations.
            evaluated_budgets: torch.Tensor
                The budgets for which the hyperparameter configurations have been
                evaluated so far.
            learning_curves: torch.Tensor
                The learning curves for the hyperparameter configurations.
        """
        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)
        if self.use_learning_curve:
            lc_features = self.cnn(learning_curves)
            # revert the output from the cnn into nr_rows x nr_kernels.
            lc_features = torch.squeeze(lc_features, 2)
            x = torch.cat((x, lc_features), dim=1)

        x = self.layers(x)
        alpha = x[:, 0]
        beta = x[:, 1]
        k = x[:, 2]
        delta = x[:, 3]

        # alpha - (alpha - beta) * e^-(k * x^delta)
        output = torch.sub(
            alpha,
            torch.mul(
                torch.sub(
                    alpha,
                    beta,
                ),
                torch.exp(
                    torch.mul(
                        -k,
                        torch.pow(
                            predict_budgets,
                            delta,
                        ),
                    )
                )

            )
        )

        return output

File Path: models/conditioned_log_power.py
Content:
import torch
import torch.nn as nn


class ConditionedLogPower(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        use_learning_curve: bool = True,
        kernel_size: int = 3,
        nr_filters: int = 4,
        nr_cnn_layers: int = 2,
    ):
        """
        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            use_learning_curve: bool
                If the learning curve should be use in the network.
            kernel_size: int
                The size of the kernel that is applied in the cnn layer.
            nr_filters: int
                The number of filters that are used in the cnn layers.
            nr_cnn_layers: int
                The number of cnn layers to be used.
        """
        super(ConditionedLogPower, self).__init__()

        self.use_learning_curve = use_learning_curve
        self.kernel_size = kernel_size
        self.nr_filters = nr_filters
        self.nr_cnn_layers = nr_cnn_layers

        self.act_func = torch.nn.LeakyReLU()
        self.last_act_func = torch.nn.GLU()
        self.tan_func = torch.nn.Tanh()
        self.batch_norm = torch.nn.BatchNorm1d

        layers = []
        # adding one since we concatenate the features with the budget
        nr_initial_features = nr_initial_features
        if self.use_learning_curve:
            nr_initial_features = nr_initial_features + nr_filters

        layers.append(nn.Linear(nr_initial_features, nr_units))
        layers.append(self.act_func)
        for i in range(2, nr_layers + 1):
            layers.append(nn.Linear(nr_units, nr_units))
            layers.append(self.act_func)

        last_layer = nn.Linear(nr_units, 3)

        layers.append(last_layer)

        self.layers = torch.nn.Sequential(*layers)

        cnn_part = []
        if use_learning_curve:
            cnn_part.append(
                nn.Conv1d(
                    in_channels=2,
                    kernel_size=(self.kernel_size,),
                    out_channels=self.nr_filters,
                ),
            )
            for i in range(1, self.nr_cnn_layers):
                cnn_part.append(self.act_func)
                cnn_part.append(
                    nn.Conv1d(
                        in_channels=self.nr_filters,
                        kernel_size=(self.kernel_size,),
                        out_channels=self.nr_filters,
                    ),
                ),
            cnn_part.append(nn.AdaptiveAvgPool1d(1))

        self.cnn = nn.Sequential(*cnn_part)

    def forward(
        self,
        x: torch.Tensor,
        predict_budgets: torch.Tensor,
        evaluated_budgets: torch.Tensor,
        learning_curves: torch.Tensor,
    ):
        """
        Args:
            x: torch.Tensor
                The examples.
            predict_budgets: torch.Tensor
                The budgets for which the performance will be predicted for the
                hyperparameter configurations.
            evaluated_budgets: torch.Tensor
                The budgets for which the hyperparameter configurations have been
                evaluated so far.
            learning_curves: torch.Tensor
                The learning curves for the hyperparameter configurations.
        """
        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)
        if self.use_learning_curve:
            lc_features = self.cnn(learning_curves)
            # revert the output from the cnn into nr_rows x nr_kernels.
            lc_features = torch.squeeze(lc_features, 2)
            x = torch.cat((x, lc_features), dim=1)

        x = self.layers(x)
        a = x[:, 0]
        b = x[:, 1]
        c = x[:, 2]

        # a divided by (1 + (x / e^b)^c)
        output = torch.div(
            a,
            torch.add(
                1,
                torch.pow(
                    torch.div(
                        predict_budgets,
                        torch.exp(b)
                    ),
                    c,
                ),
            )
        )

        return output

    @staticmethod
    def num_flat_features(x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

    @staticmethod
    def init_weights(m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_normal_(m.weight)

File Path: models/conditioned_mmf.py
Content:
import torch
import torch.nn as nn


class ConditionedMMF(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        use_learning_curve: bool = True,
        kernel_size: int = 3,
        nr_filters: int = 4,
        nr_cnn_layers: int = 2,
    ):
        """
        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            use_learning_curve: bool
                If the learning curve should be use in the network.
            kernel_size: int
                The size of the kernel that is applied in the cnn layer.
            nr_filters: int
                The number of filters that are used in the cnn layers.
            nr_cnn_layers: int
                The number of cnn layers to be used.
        """
        super(ConditionedMMF, self).__init__()

        self.use_learning_curve = use_learning_curve
        self.kernel_size = kernel_size
        self.nr_filters = nr_filters
        self.nr_cnn_layers = nr_cnn_layers

        self.act_func = torch.nn.LeakyReLU()
        self.last_act_func = torch.nn.GLU()
        self.tan_func = torch.nn.Tanh()
        self.batch_norm = torch.nn.BatchNorm1d

        layers = []
        # adding one since we concatenate the features with the budget
        nr_initial_features = nr_initial_features
        if self.use_learning_curve:
            nr_initial_features = nr_initial_features + nr_filters

        layers.append(nn.Linear(nr_initial_features, nr_units))
        layers.append(self.act_func)

        for i in range(2, nr_layers + 1):
            layers.append(nn.Linear(nr_units, nr_units))
            layers.append(self.act_func)

        last_layer = nn.Linear(nr_units, 4)
        layers.append(last_layer)

        self.layers = torch.nn.Sequential(*layers)

        cnn_part = []
        if use_learning_curve:
            cnn_part.append(
                nn.Conv1d(
                    in_channels=2,
                    kernel_size=(self.kernel_size,),
                    out_channels=self.nr_filters,
                ),
            )
            for i in range(1, self.nr_cnn_layers):
                cnn_part.append(self.act_func)
                cnn_part.append(
                    nn.Conv1d(
                        in_channels=self.nr_filters,
                        kernel_size=(self.kernel_size,),
                        out_channels=self.nr_filters,
                    ),
                ),
            cnn_part.append(nn.AdaptiveAvgPool1d(1))

        self.cnn = nn.Sequential(*cnn_part)

    def forward(
        self,
        x: torch.Tensor,
        predict_budgets: torch.Tensor,
        evaluated_budgets: torch.Tensor,
        learning_curves: torch.Tensor,
    ):
        """
        Args:
            x: torch.Tensor
                The examples.
            predict_budgets: torch.Tensor
                The budgets for which the performance will be predicted for the
                hyperparameter configurations.
            evaluated_budgets: torch.Tensor
                The budgets for which the hyperparameter configurations have been
                evaluated so far.
            learning_curves: torch.Tensor
                The learning curves for the hyperparameter configurations.
        """
        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)
        if self.use_learning_curve:
            lc_features = self.cnn(learning_curves)
            # revert the output from the cnn into nr_rows x nr_kernels.
            lc_features = torch.squeeze(lc_features, 2)
            x = torch.cat((x, lc_features), dim=1)

        x = self.layers(x)
        alpha = x[:, 0]
        beta = x[:, 1]
        delta = x[:, 2]
        k = x[:, 3]

        # alpha - ((alpha - beta) / 1 + k * (x)^delta)
        output = torch.sub(
            alpha,
            torch.div(
                torch.sub(
                    alpha,
                    beta,
                ),
                torch.add(
                    1,
                    torch.mul(
                        torch.pow(
                            predict_budgets,
                            delta,
                        ),
                        k,
                    )
                )
            )
        )

        return output

File Path: models/conditioned_power_law.py
Content:
import torch
import torch.nn as nn


class ConditionedPowerLaw(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        use_learning_curve: bool = True,
        kernel_size: int = 3,
        nr_filters: int = 4,
        nr_cnn_layers: int = 2,
    ):
        """
        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            use_learning_curve: bool
                If the learning curve should be use in the network.
            kernel_size: int
                The size of the kernel that is applied in the cnn layer.
            nr_filters: int
                The number of filters that are used in the cnn layers.
            nr_cnn_layers: int
                The number of cnn layers to be used.
        """
        super(ConditionedPowerLaw, self).__init__()

        self.use_learning_curve = use_learning_curve
        self.kernel_size = kernel_size
        self.nr_filters = nr_filters
        self.nr_cnn_layers = nr_cnn_layers

        self.act_func = torch.nn.LeakyReLU()
        self.last_act_func = torch.nn.GLU()
        self.tan_func = torch.nn.Tanh()
        self.batch_norm = torch.nn.BatchNorm1d

        layers = []
        # adding one since we concatenate the features with the budget
        nr_initial_features = nr_initial_features
        if self.use_learning_curve:
            nr_initial_features = nr_initial_features + nr_filters

        layers.append(nn.Linear(nr_initial_features, nr_units))
        layers.append(self.act_func)

        for i in range(2, nr_layers + 1):
            layers.append(nn.Linear(nr_units, nr_units))
            layers.append(self.act_func)

        last_layer = nn.Linear(nr_units, 3)
        layers.append(last_layer)

        self.layers = torch.nn.Sequential(*layers)

        cnn_part = []
        if use_learning_curve:
            cnn_part.append(
                nn.Conv1d(
                    in_channels=2,
                    kernel_size=(self.kernel_size,),
                    out_channels=self.nr_filters,
                ),
            )
            for i in range(1, self.nr_cnn_layers):
                cnn_part.append(self.act_func)
                cnn_part.append(
                    nn.Conv1d(
                        in_channels=self.nr_filters,
                        kernel_size=(self.kernel_size,),
                        out_channels=self.nr_filters,
                    ),
                ),
            cnn_part.append(nn.AdaptiveAvgPool1d(1))

        self.cnn = nn.Sequential(*cnn_part)

    def forward(
        self,
        x: torch.Tensor,
        predict_budgets: torch.Tensor,
        evaluated_budgets: torch.Tensor,
        learning_curves: torch.Tensor,
    ):
        """
        Args:
            x: torch.Tensor
                The examples.
            predict_budgets: torch.Tensor
                The budgets for which the performance will be predicted for the
                hyperparameter configurations.
            evaluated_budgets: torch.Tensor
                The budgets for which the hyperparameter configurations have been
                evaluated so far.
            learning_curves: torch.Tensor
                The learning curves for the hyperparameter configurations.
        """
        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)
        if self.use_learning_curve:
            lc_features = self.cnn(learning_curves)
            # revert the output from the cnn into nr_rows x nr_kernels.
            lc_features = torch.squeeze(lc_features, 2)
            x = torch.cat((x, lc_features), dim=1)

        x = self.layers(x)
        alphas = x[:, 0]
        betas = x[:, 1]
        gammas = x[:, 2]

        output = torch.add(
            alphas,
            torch.mul(
                self.last_act_func(torch.cat((betas, betas))),
                torch.pow(
                    predict_budgets,
                    torch.mul(self.last_act_func(torch.cat((gammas, gammas))), -1)
                )
            ),
        )

        return output

File Path: models/conditioned_power_law_v1.py
Content:
import torch
import torch.nn as nn


class ConditionedPowerLawV1(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        use_learning_curve: bool = True,
        kernel_size: int = 3,
        nr_filters: int = 4,
        nr_cnn_layers: int = 2,
    ):
        """
        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            use_learning_curve: bool
                If the learning curve should be use in the network.
            kernel_size: int
                The size of the kernel that is applied in the cnn layer.
            nr_filters: int
                The number of filters that are used in the cnn layers.
            nr_cnn_layers: int
                The number of cnn layers to be used.
        """
        super(ConditionedPowerLaw, self).__init__()

        self.use_learning_curve = use_learning_curve
        self.kernel_size = kernel_size
        self.nr_filters = nr_filters
        self.nr_cnn_layers = nr_cnn_layers

        self.act_func = torch.nn.LeakyReLU()
        self.relu_func = torch.nn.ReLU()
        self.last_act_func = torch.nn.GLU()
        self.tan_func = torch.nn.Tanh()
        self.batch_norm = torch.nn.BatchNorm1d

        layers = []
        # adding one since we concatenate the features with the budget
        nr_initial_features = nr_initial_features
        if self.use_learning_curve:
            nr_initial_features = nr_initial_features + nr_filters

        layers.append(nn.Linear(nr_initial_features, nr_units))
        layers.append(self.act_func)

        for i in range(2, nr_layers + 1):
            layers.append(nn.Linear(nr_units, nr_units))
            layers.append(self.act_func)

        last_layer = nn.Linear(nr_units, 4)

        layers.append(last_layer)
        with torch.no_grad():
            last_layer.bias.data = torch.Tensor([0.1, 1, 0.2, 0.2])

        self.layers = torch.nn.Sequential(*layers)

        cnn_part = []
        if use_learning_curve:
            cnn_part.append(
                nn.Conv1d(
                    in_channels=2,
                    kernel_size=(self.kernel_size,),
                    out_channels=self.nr_filters,
                ),
            )
            for i in range(1, self.nr_cnn_layers):
                cnn_part.append(self.act_func)
                cnn_part.append(
                    nn.Conv1d(
                        in_channels=self.nr_filters,
                        kernel_size=(self.kernel_size,),
                        out_channels=self.nr_filters,
                    ),
                ),
            cnn_part.append(nn.AdaptiveAvgPool1d(1))

        self.cnn = nn.Sequential(*cnn_part)

    def forward(
        self,
        x: torch.Tensor,
        predict_budgets: torch.Tensor,
        evaluated_budgets: torch.Tensor,
        learning_curves: torch.Tensor,
    ):
        """
        Args:
            x: torch.Tensor
                The examples.
            predict_budgets: torch.Tensor
                The budgets for which the performance will be predicted for the
                hyperparameter configurations.
            evaluated_budgets: torch.Tensor
                The budgets for which the hyperparameter configurations have been
                evaluated so far.
            learning_curves: torch.Tensor
                The learning curves for the hyperparameter configurations.
        """
        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)
        if self.use_learning_curve:
            lc_features = self.cnn(learning_curves)
            # revert the output from the cnn into nr_rows x nr_kernels.
            lc_features = torch.squeeze(lc_features, 2)
            x = torch.cat((x, lc_features), dim=1)

        x = self.layers(x)
        a = x[:, 0]
        b = x[:, 1]
        c = x[:, 2]
        d = x[:, 3]

        first_part = self.relu_func(
            torch.add(
                predict_budgets,
                d,
            ),
        )

        output = torch.add(
            a,
            torch.mul(
                b,
                torch.pow(
                    first_part,
                    -c,
                ),
            ),
        )

        return output

File Path: models/conditioned_power_law_v2.py
Content:
import torch
import torch.nn as nn


class ConditionedPowerLawV2(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        use_learning_curve: bool = True,
        kernel_size: int = 3,
        nr_filters: int = 4,
        nr_cnn_layers: int = 2,
    ):
        """
        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            use_learning_curve: bool
                If the learning curve should be use in the network.
            kernel_size: int
                The size of the kernel that is applied in the cnn layer.
            nr_filters: int
                The number of filters that are used in the cnn layers.
            nr_cnn_layers: int
                The number of cnn layers to be used.
        """
        super(ConditionedPowerLaw, self).__init__()

        self.use_learning_curve = use_learning_curve
        self.kernel_size = kernel_size
        self.nr_filters = nr_filters
        self.nr_cnn_layers = nr_cnn_layers

        self.act_func = torch.nn.LeakyReLU()
        self.relu_func = torch.nn.ReLU()
        self.last_act_func = torch.nn.GLU()
        self.tan_func = torch.nn.Tanh()
        self.batch_norm = torch.nn.BatchNorm1d

        layers = []
        # adding one since we concatenate the features with the budget
        nr_initial_features = nr_initial_features
        if self.use_learning_curve:
            nr_initial_features = nr_initial_features + nr_filters

        layers.append(nn.Linear(nr_initial_features, nr_units))
        layers.append(self.act_func)

        for i in range(2, nr_layers + 1):
            layers.append(nn.Linear(nr_units, nr_units))
            layers.append(self.act_func)

        last_layer = nn.Linear(nr_units, 5)

        layers.append(last_layer)
        with torch.no_grad():
            last_layer.bias.data = torch.Tensor([0.1, 1, 0.2, 0.2, 0.4])

        self.layers = torch.nn.Sequential(*layers)

        cnn_part = []
        if use_learning_curve:
            cnn_part.append(
                nn.Conv1d(
                    in_channels=2,
                    kernel_size=(self.kernel_size,),
                    out_channels=self.nr_filters,
                ),
            )
            for i in range(1, self.nr_cnn_layers):
                cnn_part.append(self.act_func)
                cnn_part.append(
                    nn.Conv1d(
                        in_channels=self.nr_filters,
                        kernel_size=(self.kernel_size,),
                        out_channels=self.nr_filters,
                    ),
                ),
            cnn_part.append(nn.AdaptiveAvgPool1d(1))

        self.cnn = nn.Sequential(*cnn_part)

    def forward(
        self,
        x: torch.Tensor,
        predict_budgets: torch.Tensor,
        evaluated_budgets: torch.Tensor,
        learning_curves: torch.Tensor,
    ):
        """
        Args:
            x: torch.Tensor
                The examples.
            predict_budgets: torch.Tensor
                The budgets for which the performance will be predicted for the
                hyperparameter configurations.
            evaluated_budgets: torch.Tensor
                The budgets for which the hyperparameter configurations have been
                evaluated so far.
            learning_curves: torch.Tensor
                The learning curves for the hyperparameter configurations.
        """
        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)
        if self.use_learning_curve:
            lc_features = self.cnn(learning_curves)
            # revert the output from the cnn into nr_rows x nr_kernels.
            lc_features = torch.squeeze(lc_features, 2)
            x = torch.cat((x, lc_features), dim=1)

        x = self.layers(x)
        a = x[:, 0]
        b = x[:, 1]
        c = x[:, 2]
        d = x[:, 3]
        e = x[:, 4]

        first_part = self.relu_func(
            torch.add(
                torch.mul(
                    b,
                    predict_budgets,
                ),
                d,
            )
        )

        output = torch.add(
            a,
            torch.mul(
                e,
                torch.pow(
                    first_part,
                    -c,
                ),
            ),
        )

        return output

File Path: models/feed_forward_nn.py
Content:
import torch
import torch.nn as nn


class NN(nn.Module):

    def __init__(
        self,
        nr_initial_features=10,
        nr_units=200,
        nr_layers=3,
        dropout_fraction=0.2,
        nr_classes=1,
    ):
        """

        Args:
            nr_initial_features: int
                The number of features per example.
            nr_units: int
                The number of units for every layer.
            nr_layers: int
                The number of layers for the neural network.
            dropout_fraction: float
                The dropout fraction to be used through training.
            nr_classes: int
                The number of classes in the dataset.
        """
        super(NN, self).__init__()
        self.nr_layers = nr_layers
        self.fc1 = nn.Linear(nr_initial_features, nr_units)
        self.bn1 = nn.BatchNorm1d(nr_units)
        for i in range(2, nr_layers + 1):
            setattr(self, f'fc{i}', nn.Linear(nr_units, nr_units))
            setattr(self, f'bn{i}', nn.BatchNorm1d(nr_units))
        setattr(self, f'fc{nr_layers + 1}', nn.Linear(nr_units, nr_classes))

        self.dropout = nn.Dropout(p=dropout_fraction)
        self.last_act_func = torch.nn.LeakyReLU()


    def forward(self, x):

        x = x.view(-1, self.num_flat_features(x))

        x = self.last_act_func(self.bn1(self.fc1(x)))
        for i in range(2, self.nr_layers + 1):
            x = self.dropout(x)
            temp_layer = getattr(self, f'fc{i}')
            x = self.last_act_func(getattr(self, f'bn{i}')(temp_layer(x)))

        x = self.dropout(x)
        x = getattr(self, f'fc{self.nr_layers + 1}')(x)

        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

File Path: models/gaussian_processor.py
Content:
import scipy
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.utils.optimize import _check_optimize_result


class MyGPR(GaussianProcessRegressor):
    def __init__(self, *args, max_iter=5e05, gtol=1e-06, **kwargs):
        super().__init__(*args, **kwargs)
        self._max_iter = max_iter
        self._gtol = gtol

    def _constrained_optimization(self, obj_func, initial_theta, bounds):
        if self.optimizer == "fmin_l_bfgs_b":
            opt_res = scipy.optimize.minimize(obj_func, initial_theta, method="L-BFGS-B", jac=True, bounds=bounds, options={'maxiter':self._max_iter, 'gtol': self._gtol})
            _check_optimize_result("lbfgs", opt_res)
            theta_opt, func_min = opt_res.x, opt_res.fun
        elif callable(self.optimizer):
            theta_opt, func_min = self.optimizer(obj_func, initial_theta, bounds=bounds)
        else:
            raise ValueError("Unknown optimizer %s." % self.optimizer)
        return theta_opt, func_min

File Path: models/power_law.py
Content:
import torch


class PowerLaw(torch.nn.Module):
    def __init__(self):

        super().__init__()
        self.alpha = torch.nn.Parameter(torch.rand(()))
        self.beta = torch.nn.Parameter(torch.rand(()))
        self.gamma = torch.nn.Parameter(torch.rand(()))

        self.act_func = torch.nn.LeakyReLU()

    def forward(self, x):
        """
        In the forward function we accept a Tensor of input data and we must return
        a Tensor of output data. We can use Modules defined in the constructor as
        well as arbitrary operators on Tensors.
        """
        output = torch.add(
            self.act_func(self.alpha),
            torch.mul(
                self.act_func(self.beta),
                torch.pow(
                    x,
                    torch.mul(self.act_func(self.gamma), -1)
                )
            )
        )

        return output

File Path: models/preact_resnet.py
Content:
'''Pre-activation ResNet in PyTorch.

Reference:
[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
    Identity Mappings in Deep Residual Networks. arXiv:1603.05027
'''
import torch
import torch.nn as nn
import torch.nn.functional as F


class PreActBlock(nn.Module):
    '''Pre-activation version of the BasicBlock.'''
    expansion = 1

    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.2):
        super(PreActBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.dropout = nn.Dropout2d(p=dropout_rate)

        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)
            )

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.dropout(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out += shortcut
        return out


class PreActBottleneck(nn.Module):
    '''Pre-activation version of the original Bottleneck module.'''
    expansion = 4

    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.2):
        super(PreActBottleneck, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)
        self.dropout = nn.Dropout2d(p=dropout_rate)

        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)
            )

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.dropout(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out = self.dropout(out)
        out = self.conv3(F.relu(self.bn3(out)))
        out += shortcut
        return out


class PreActResNet(nn.Module):
    def __init__(self, block, num_blocks, drop_rate=0.2, num_classes=10):
        super(PreActResNet, self).__init__()
        self.in_planes = 64
        self.drop_rate = drop_rate
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride, self.drop_rate))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


def PreActResNet18(drop_rate: float):
    return PreActResNet(PreActBlock, [2,2,2,2], drop_rate)

def PreActResNet34():
    return PreActResNet(PreActBlock, [3,4,6,3])

def PreActResNet50():
    return PreActResNet(PreActBottleneck, [3,4,6,3])

def PreActResNet101():
    return PreActResNet(PreActBottleneck, [3,4,23,3])

def PreActResNet152():
    return PreActResNet(PreActBottleneck, [3,8,36,3])

File Path: plots/debugging.py
Content:
import json
import os
from typing import List

import matplotlib
import matplotlib.pyplot as plt
matplotlib.use('agg')  # no need for tk
import numpy as np
import seaborn as sns
import scipy
from scipy import stats

sns.set(
    rc={
        'figure.figsize': (11.7, 8.27),
        'font.size': 45,
        'axes.titlesize': 45,
        'axes.labelsize': 45,
        'xtick.labelsize': 45,
        'ytick.labelsize': 45,
        'legend.fontsize': 39,
    },
    style="white"
)


def gradients_and_parameters(
    parameters: List[List],
    parameter_gradients: List[List],
    parameter_names: List,
    final_predicted_curve: List,
    final_true_curve: List,
    loss_curve,
    hp_index: int,
    max_budget: int = 1000,
    curve_length: int = 25,
):
    # Create four subplots and unpack the output array immediately
    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)

    for parameter_name, parameter_values in zip(parameter_names, parameters):
        ax1.plot(np.arange(1, max_budget + 1), parameter_values, label=f'{parameter_name} value')
    #ax1.set_aspect('equal', 'box')
    ax1.legend()
    for parameter_name, parameter_values in zip(parameter_names, parameter_gradients):
        ax2.plot(np.arange(1, max_budget + 1), parameter_values, label=f'{parameter_name} gradients')
        ax2.set_ylim(-0.5, 0.5)
    #ax2.set_aspect('equal', 'box')
    ax2.legend()


    ax3.plot(np.arange(1, curve_length + 1), final_true_curve, label=f'True validation curve')
    ax3.plot(np.arange(1, curve_length + 1), final_predicted_curve, label=f'Predicted validation curve')
    #ax3.set_aspect('equal', 'box')
    ax3.legend()

    ax4.plot(np.arange(1, max_budget + 1), loss_curve, label=f'{parameter_name} gradients')


    f.tight_layout()
    plt.savefig(f'training_info_{hp_index}.pdf')


def plot_grad_flow(named_parameters, i):
    ave_grads = []
    layers = []
    for n, p in named_parameters:
        if p.requires_grad:
            layers.append(n)
            ave_grads.append(p.grad.abs().mean().cpu())
    plt.plot(ave_grads, alpha=0.3, color="b")
    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color="k" )
    plt.xticks(range(0, len(ave_grads), 1), layers, rotation="vertical")
    plt.xlim(xmin=0, xmax=len(ave_grads))
    plt.xlabel("Layers")
    plt.ylabel("Average gradient")
    plt.title("Gradient flow")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f'gradients_epoch{i}.pdf')


def plot_conditioned_surrogates(result_dir: str):

    models = [
        'conditioned_power_law',
        'conditioned_nn',
    ]

    method_names_to_pretty = {
        'conditioned_power_law': 'DPL',
        'conditioned_nn': 'Cond NN',
        'power_law': 'PL',
        'nn': 'NN',
        'gp': 'GP',
    }
    seed = 11
    val_fractions = [0.1, 0.2, 0.3, 0.4, 0.5]

    dataset_names = ['APSFailure', 'Amazon_employee_access', 'Australian', 'Fashion-MNIST', 'KDDCup09_appetency',
                     'MiniBooNE', 'adult', 'airlines', 'albert', 'bank-marketing', 'blood-transfusion-service-center',
                     'car', 'christine', 'cnae-9', 'connect-4', 'covertype', 'credit-g', 'dionis', 'fabert', 'helena',
                     'higgs', 'jannis', 'jasmine', 'jungle_chess_2pcs_raw_endgame_complete', 'kc1', 'kr-vs-kp',
                     'mfeat-factors', 'nomao', 'numerai28.6', 'phoneme', 'segment', 'shuttle', 'sylvine', 'vehicle',
                     'volkert']

    for model in models:
        model_correlation_means = []
        model_correlation_stds = []

        for val_fraction in val_fractions:

            run_information_folder = os.path.join(
                result_dir,
                f'{model}',
                f'{seed}',
                f'{val_fraction}',
            )

            dataset_correlations = []
            for dataset_name in dataset_names:
                # for dataset_name in dataset_names:
                information_file = os.path.join(run_information_folder, f'{dataset_name}.json')
                with open(information_file, 'r') as fp:
                    information = json.load(fp)
                dataset_correlation = information['correlation']
                if not np.isnan(dataset_correlation):
                    dataset_correlations.append(dataset_correlation)

            if model == 'conditioned_power_law':
                print(f'Validation fraction: {val_fraction}')
                print(f'Dataset correlations: {dataset_correlations}')

            model_correlation_means.append(np.mean(dataset_correlations))
            model_correlation_stds.append(np.std(dataset_correlations))

        plt.plot(val_fractions, model_correlation_means, label=method_names_to_pretty[model], marker='o', linestyle='--', linewidth=7)

    for unconditioned_model_name in ['power_law', 'nn', 'gp']:
        model_correlation_means = []
        model_correlation_stds = []

        for val_fraction in val_fractions:
            run_information_folder = os.path.join(
                result_dir,
                f'{unconditioned_model_name}',
                f'{seed}',
                f'{val_fraction}',
                'config_6' if unconditioned_model_name == 'nn' else 'config_1',
            )
            dataset_correlations = []
            for dataset_name in dataset_names:
                # for dataset_name in dataset_names:
                information_file = os.path.join(run_information_folder, f'{dataset_name}.json')
                with open(information_file, 'r') as fp:
                    information = json.load(fp)

                hp_true_performances = []
                hp_predicted_performances = []
                for hp_information in information:
                    hp_predicted_performances.append(hp_information['hp_predicted_performance'])
                    hp_true_performances.append(hp_information['hp_true_performance'])

                dataset_correlation, _ = scipy.stats.pearsonr(hp_predicted_performances, hp_true_performances)
                if not np.isnan(dataset_correlation):
                    dataset_correlations.append(dataset_correlation)

            model_correlation_means.append(np.mean(dataset_correlations))
            model_correlation_stds.append(np.std(dataset_correlations))

        plt.plot(val_fractions, model_correlation_means, label=method_names_to_pretty[unconditioned_model_name], marker='o', linestyle='--', linewidth=7)

    plt.xlabel('LC Length Fraction')
    plt.xticks(val_fractions, [f'{val_fraction}' for val_fraction in val_fractions])
    plt.ylabel('Correlation: Est. vs. True')
    plt.legend(bbox_to_anchor=(0.5, -0.42), loc='lower center', ncol=5)
    plt.savefig('conditioned_model_correlations.pdf', bbox_inches="tight")


def plot_uncertainty_estimation(mean_values, std_values, evaluated_configs, point_to_be_evaluated, hp_indices, counter):

    plt.figure()
    point_indices = np.arange(0, 2000, 20)
    point_indices = np.append(point_indices, list(evaluated_configs.keys()))
    point_indices = np.append(point_indices, point_to_be_evaluated)
    point_indices = np.sort(point_indices)
    hp_indices = np.array(hp_indices)
    mean_point_x = hp_indices[point_indices]
    mean_point_y = mean_values[point_indices]
    mean_point_std =  std_values[point_indices]
    plt.plot(mean_point_x, mean_point_y, color='red', label='Mean Surrogate Predictions')

    plt.fill_between(mean_point_x, np.add(mean_point_y, mean_point_std), np.subtract(mean_point_y, mean_point_std), color='red', alpha=0.2)
    plt.plot([hp_indices[point_to_be_evaluated]], mean_values[point_to_be_evaluated], marker="o", markersize=20, markeredgecolor="red", markerfacecolor="green", label='Chosen Point')

    already_evaluated_x = []
    already_evaluated_y = []
    for hp_index, mean_performance in zip(hp_indices, mean_values):
        if hp_index in evaluated_configs:
            already_evaluated_x.append(hp_index)
            already_evaluated_y.append(mean_performance)

    plt.scatter(already_evaluated_x, already_evaluated_y, color='black', label='Evaluated Points')
    plt.xlabel('Hyperparameter indices')
    plt.ylabel('Surrogate Prediction')
    plt.legend(loc=8, ncol=5)
    counter = int(counter)
    plt.savefig(f'surrogate_uncertainty_{counter}.pdf', bbox_inches="tight")


def plot_top_conditioned_surrogates(result_dir: str):

    models = [
        'conditioned_power_law',
    ]

    seed = 11
    val_fractions = [0.1, 0.2, 0.3, 0.4, 0.5]

    dataset_names = ['APSFailure', 'Amazon_employee_access', 'Australian', 'Fashion-MNIST', 'KDDCup09_appetency',
                     'MiniBooNE', 'adult', 'airlines', 'albert', 'bank-marketing', 'blood-transfusion-service-center',
                     'car', 'christine', 'cnae-9', 'connect-4', 'covertype', 'credit-g', 'dionis', 'fabert', 'helena',
                     'higgs', 'jannis', 'jasmine', 'jungle_chess_2pcs_raw_endgame_complete', 'kc1', 'kr-vs-kp',
                     'mfeat-factors', 'nomao', 'numerai28.6', 'phoneme', 'segment', 'shuttle', 'sylvine', 'vehicle',
                     'volkert']

    model_correlations = []
    for model in models:
        model_correlation_means = []
        model_correlation_stds = []
        model_mae = []

        collection_mae = []
        for val_fraction in val_fractions:

            run_information_folder = os.path.join(
                result_dir,
                f'{model}',
                f'{seed}',
                f'{val_fraction}',
            )

            dataset_correlations = []
            mean_absolute_relative_errors = []

            dataset_errors = []
            for dataset_name in dataset_names:
                # for dataset_name in dataset_names:
                information_file = os.path.join(run_information_folder, f'{dataset_name}.json')
                with open(information_file, 'r') as fp:
                    information = json.load(fp)
                real_labels = information['real_labels']
                predicted_labels = information['predicted_labels']
                info_dict = dict()
                for real_label, predicted_label in zip(real_labels, predicted_labels):
                    info_dict[real_label] = predicted_label
                real_labels.sort(reverse=True)
                real_top_labels = []
                predicted_top_labels = []
                config_errors = []
                for i in range(0, len(real_labels)):
                    example_label = real_labels[i]
                    real_top_labels.append(example_label)
                    predicted_label = info_dict[example_label]
                    predicted_top_labels.append(info_dict[example_label])
                    mae = abs((example_label - predicted_label)) / example_label
                    mean_absolute_relative_errors.append(mae)
                    config_errors.append(mae)

                dataset_correlation, _ = stats.pearsonr(real_top_labels, predicted_top_labels)
                dataset_errors.append(np.mean(config_errors))
                if not np.isnan(dataset_correlation):
                    dataset_correlations.append(dataset_correlation)

            model_correlations.append(dataset_correlations)
            model_correlation_means.append(np.mean(dataset_correlations))
            model_mae.append(dataset_errors)
            model_correlation_stds.append(np.std(dataset_correlations))
            collection_mae.append(mean_absolute_relative_errors)

    meanlineprops = dict(linewidth=4)
    whiskersprops = dict(linewidth=3)
    plt.boxplot(model_mae, positions=val_fractions, widths=0.02, showfliers=False, whis=0.5, medianprops=meanlineprops, capprops=whiskersprops, boxprops=whiskersprops, whiskerprops=whiskersprops)
    plt.xlabel('LC Length Fraction')
    plt.xlim(0, 0.6)
    plt.ylim(0, 0.4)
    plt.xticks(val_fractions, [f'{val_fraction}' for val_fraction in val_fractions])
    plt.ylabel('Absolute Relative Error')
    plt.savefig('pl_mae_distribution.pdf', bbox_inches="tight")

File Path: plots/normalized_regret.py
Content:
import json
import os
from typing import Tuple, List

import matplotlib.pyplot as plt
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)
import numpy as np
import pandas as pd
from scipy.stats import rankdata
import seaborn as sns

from benchmarks.benchmark import BaseBenchmark
from benchmarks.lcbench import LCBench
from benchmarks.taskset import TaskSet
from benchmarks.hyperbo import PD1


sns.set_style('white')

sns.set(
    rc={
        'figure.figsize': (11.7, 8.27),
        'font.size': 31,
        'axes.titlesize': 31,
        'axes.labelsize': 31,
        'xtick.labelsize': 31,
        'ytick.labelsize': 31,
        'legend.fontsize': 31,
    },
    style="white"
)


result_path = os.path.expanduser(
    os.path.join(
        '~',
        'Desktop',
        'results',
    )
)

project_folder = os.path.expanduser(
    os.path.join(
        '~',
        'Desktop',
        'PhD',
        'Projekte',
        'DeepRegret',
    )
)

method_names = [
    'power_law',
    'ahb',
    'bohb',
    'dehb',
    'dragonfly',
    'hyperband',
    'random',
    'smac',
]

pretty_names = {
    'power_law': 'DPL',
    'ahb': 'ASHA',
    'bohb': 'BOHB',
    'dehb': 'DEHB',
    'dragonfly': 'Dragonfly',
    'hyperband': 'Hyperband',
    'random': 'Random',
    'smac': 'SMAC',
}


def get_method_dataset_regret_time_performance(
        benchmark: BaseBenchmark,
        dataset_name: str,
        method_name: str,
        benchmark_surrogate_results: str,
        benchmark_name: str = 'lcbench',
) -> Tuple[List, np.ndarray]:
    """Retrieve the time taken and the incumbent curve
    of a method for a particular dataset.

    Retrieves the time taken and the incumbent curve over the
    HPO budget for a given method, for a particular dataset from
    a benchmark.

    Args:
        benchmark: BaseBenchmark
            The benchmark object.
        dataset_name: str
            The dataset for which the results will be generated.
        method_name: str
            The name of the method for which the results will be generated.
        benchmark_surrogate_results: str
            The path there the results are stored for all methods.
        benchmark_name: str
            The name of the benchmark.

    Returns:
        iterations_overhead, baseline_incumbent_curve: Tuple
            The time taken for every HPO step and the baseline incumbent curve over
            every HPO step.
    """
    gap_performance = benchmark.get_gap_performance()

    total_overhead = 0
    iterations_overhead = []
    baseline_incumbent_curves = []
    baseline_overhead = []
    configuration_eval_overhead = []

    repeat_nrs = [nr for nr in range(0, 10)]

    longest_run = 0

    for repeat_nr in repeat_nrs:

        dataset_result_file = os.path.join(benchmark_surrogate_results, method_name, f'{dataset_name}_{repeat_nr}.json')
        try:
            with open(dataset_result_file, 'r') as fp:
                result_info = json.load(fp)
        except FileNotFoundError:
            # no results for this repeat/seed, move on to the next
            continue

        baseline_incumbent_curve = result_info['curve']
        if len(baseline_incumbent_curve) < 1000:
            continue
        elif len(baseline_incumbent_curve) > 1000:
            baseline_incumbent_curve = baseline_incumbent_curve[0:1000]

        baseline_incumbent_curves.append(baseline_incumbent_curve)

        if 'overhead' in result_info:
            baseline_overhead.append(result_info['overhead'])

        evaluated_hps = result_info['hp']
        budgets_evaluated = result_info['epochs']

        config_times = []
        for hp_index, budget_eval in zip(evaluated_hps, budgets_evaluated):
            config_time = benchmark.get_step_cost(
                hp_index=hp_index,
                budget=budget_eval,
            )
            config_times.append(config_time)
        if len(config_times) > longest_run:
            longest_run = len(config_times)
        configuration_eval_overhead.append(config_times)

    if len(baseline_incumbent_curves) == 0:
        return [], []

    mean_config_eval_values = []

    for curve_point in range(0, len(baseline_incumbent_curves[0])):
        mean_config_point_values = []
        for curve_nr in range(0, len(configuration_eval_overhead)):
            config_curve = configuration_eval_overhead[curve_nr]
            if len(config_curve) > curve_point:
                mean_config_point_values.append(config_curve[curve_point])
            else:
                continue
        mean_config_eval_values.append(np.mean(mean_config_point_values))

    # take the mean value over all repeats
    baseline_incumbent_curve = np.mean(baseline_incumbent_curves, axis=0)
    best_incumbent_curve = benchmark.get_incumbent_curve()
    if benchmark_name == 'lcbench':
        incumbent_best_performance = max(best_incumbent_curve)
    else:
        incumbent_best_performance = min(best_incumbent_curve)

    worst_performance = benchmark.get_worst_performance()

    if benchmark_name == 'lcbench':
        # convert the incumbent iteration performance to the normalized regret for every iteration
        baseline_incumbent_curve = [(incumbent_best_performance - incumbent_it_performance) / gap_performance
                                    for incumbent_it_performance in baseline_incumbent_curve]
    elif benchmark_name == 'taskset':
        if method_name != 'power_law' and method_name != 'smac':
            baseline_incumbent_curve = [worst_performance - curve_element for curve_element in baseline_incumbent_curve]

        baseline_incumbent_curve = [(incumbent_it_performance - incumbent_best_performance) / gap_performance
                                    for incumbent_it_performance in baseline_incumbent_curve]
    elif benchmark_name == 'pd1':
        if method_name == 'dragonfly' or method_name == 'ahb':
            baseline_incumbent_curve = [1.0 - curve_element for curve_element in baseline_incumbent_curve]
            # convert the incumbent iteration performance to the normalized regret for every iteration
        baseline_incumbent_curve = [(incumbent_it_performance - incumbent_best_performance) / gap_performance
                                    for incumbent_it_performance in baseline_incumbent_curve]

    if len(baseline_overhead) > 0:

        mean_baseline_eval_values = []
        for curve_point in range(0, len(baseline_incumbent_curve)):
            mean_config_point_values = []
            for curve_nr in range(0, len(baseline_overhead)):
                config_curve = baseline_overhead[curve_nr]
                if len(config_curve) > curve_point:
                    mean_config_point_values.append(config_curve[curve_point])
                else:
                    continue
            mean_baseline_eval_values.append(np.mean(mean_config_point_values))

        for baseline_overhead_it, config_overhead in zip(mean_baseline_eval_values, mean_config_eval_values):
            total_overhead = total_overhead + baseline_overhead_it + config_overhead
            iterations_overhead.append(total_overhead)
    else:
        for config_overhead in mean_config_eval_values:
            total_overhead = total_overhead + config_overhead
            iterations_overhead.append(total_overhead)

    return iterations_overhead, baseline_incumbent_curve


def get_method_dataset_regret_epoch_performance(
        benchmark: BaseBenchmark,
        dataset_name: str,
        method_name: str,
        benchmark_surrogate_results: str,
        benchmark_name: str = 'lcbench',
) -> Tuple[List, np.ndarray, np.ndarray]:
    """Retrieve the epochs taken and the incumbent curve
    of a method for a particular dataset.

    Retrieves the epochs taken and the incumbent curve over the
    HPO budget for a given method, for a particular dataset from
    a benchmark.

    Args:
        benchmark: BaseBenchmark
            The benchmark object.
        dataset_name: str
            The dataset for which the results will be generated.
        method_name: str
            The name of the method for which the results will be generated.
        benchmark_surrogate_results: str
            The path there the results are stored for all methods.
        benchmark_name: str
            The name of the benchmark.

    Returns:
        iteration_cost, baseline_incumbent_curve, baseline_incumbent_std: Tuple
            The epochs taken for every HPO step, the baseline incumbent curve over
            every HPO step and the baseline incumbent std over the HPO steps.
    """
    gap_performance = benchmark.get_gap_performance()

    baseline_epochs = []
    baseline_incumbent_curves = []

    repeat_nrs = [nr for nr in range(0, 10)]

    for repeat_nr in repeat_nrs:
        repeat_epochs_cost = []
        configs_evaluated = dict()
        dataset_result_file = os.path.join(benchmark_surrogate_results, method_name, f'{dataset_name}_{repeat_nr}.json')
        try:
            with open(dataset_result_file, 'r') as fp:
                result_info = json.load(fp)
        except FileNotFoundError:
            # no results for this repeat/seed, move on to the next
            continue

        baseline_incumbent_curve = result_info['curve']

        if len(baseline_incumbent_curve) < 1000:
            continue
        elif len(baseline_incumbent_curve) > 1000:
            baseline_incumbent_curve = baseline_incumbent_curve[0:1000]
        baseline_incumbent_curves.append(baseline_incumbent_curve)

        evaluated_hps = result_info['hp']
        budgets_evaluated = result_info['epochs']

        for evaluated_hp, budget_evaluated in zip(evaluated_hps, budgets_evaluated):
            if evaluated_hp in configs_evaluated:
                budgets = configs_evaluated[evaluated_hp]
                max_budget = max(budgets)
                cost = budget_evaluated - max_budget
                repeat_epochs_cost.append(cost)
                configs_evaluated[evaluated_hp].append(budget_evaluated)
            else:
                repeat_epochs_cost.append(budget_evaluated)
                configs_evaluated[evaluated_hp] = [budget_evaluated]

        baseline_epochs.append(repeat_epochs_cost)

    if len(baseline_incumbent_curves) == 0:
        return [], []

    mean_cost_values = []
    try:
        for curve_point in range(0, len(baseline_incumbent_curves[0])):
            mean_config_point_values = []
            for curve_nr in range(0, len(baseline_epochs)):
                config_curve = baseline_epochs[curve_nr]
                if len(config_curve) > curve_point:
                    mean_config_point_values.append(config_curve[curve_point])
                else:
                    continue
            if len(mean_config_point_values) > 0:
                mean_cost_values.append(np.mean(mean_config_point_values))
    except Exception:
        return [], []

    if len(mean_cost_values) < len(baseline_incumbent_curves[0]):
        mean_cost_values = [1 for _ in range(1, len(baseline_incumbent_curves[0]) + 1)]
    iteration_cost = []

    total_iteration_cost = 0
    for iteration_cost_value in mean_cost_values:
        total_iteration_cost += iteration_cost_value
        iteration_cost.append(total_iteration_cost)

    # take the mean value over all repeats and the standard deviation
    baseline_incumbent_curve = np.mean(baseline_incumbent_curves, axis=0)
    baseline_incumbent_std = np.std(baseline_incumbent_curves, axis=0)

    incumbent_best_performance = benchmark.get_best_performance()

    worst_performance = benchmark.get_worst_performance()

    if benchmark_name == 'lcbench':
        # convert the incumbent iteration performance to the normalized regret for every iteration
        baseline_incumbent_curve = [incumbent_best_performance - incumbent_it_performance
                                    for incumbent_it_performance in baseline_incumbent_curve]
    elif benchmark_name == 'taskset':
        if method_name != 'power_law' and method_name != 'smac':
            baseline_incumbent_curve = [worst_performance - curve_element for curve_element in baseline_incumbent_curve]

        baseline_incumbent_curve = [(incumbent_it_performance - incumbent_best_performance)  # / gap_performance
                                    for incumbent_it_performance in baseline_incumbent_curve]
    elif benchmark_name == 'pd1':
        if method_name == 'dragonfly' or method_name == 'ahb':
            baseline_incumbent_curve = [1.0 - curve_element for curve_element in baseline_incumbent_curve]
            # convert the incumbent iteration performance to the normalized regret for every iteration
        baseline_incumbent_curve = [(incumbent_it_performance - incumbent_best_performance) / gap_performance
                                    for incumbent_it_performance in baseline_incumbent_curve]

    return iteration_cost, baseline_incumbent_curve, baseline_incumbent_std


def get_method_dataset_number_configs(
        benchmark: BaseBenchmark,
        dataset_name: str,
        method_name: str,
        benchmark_surrogate_results: str,
        benchmark_name: str = 'lcbench',
) -> float:
    """Calculate the number of unique configurations
    explored.

    Calculates the number of unique configurations that
    were explored during the HPO phase.

    Args:
        benchmark: BaseBenchmark
            The benchmark object.
        dataset_name: str
            The dataset for which the number of configurations will
            be calculated.
        method_name: str
            The method name.
        benchmark_surrogate_results: str
            The path where the results are located.
        benchmark_name: str
            The name of the benchmark

    Returns: float
        The number of configurations explored averaged over the repetitions.
    """
    repeat_nrs = [nr for nr in range(0, 10)]

    number_configs_repeats = []
    for repeat_nr in repeat_nrs:
        dataset_result_file = os.path.join(benchmark_surrogate_results, method_name, f'{dataset_name}_{repeat_nr}.json')
        try:
            with open(dataset_result_file, 'r') as fp:
                result_info = json.load(fp)
        except FileNotFoundError:
            # no results for this repeat/seed, move on to the next
            continue

        if benchmark_name == 'pd1':
            if benchmark.max_budget < 50:
                max_epochs = int(benchmark.max_budget * 20)
                result_info = result_info['hp']
                result_info = result_info[0:max_epochs]
            else:
                return -1
        else:
            result_info = result_info['hp']

        unique_configs = len(set(result_info))
        number_configs_repeats.append(unique_configs)

    return np.mean(number_configs_repeats)


def get_method_dataset_number_max_configs(
        benchmark: BaseBenchmark,
        dataset_name: str,
        method_name: str,
        benchmark_surrogate_results: str,
        benchmark_name: str = 'lcbench',
) -> float:
    """Return the number of unique configurations that
    were explored at the max budget.

    Return the number of unique configurations explored during
    the HPO phase until the end/max budget.

    Args:
        benchmark: BaseBenchmark
            The benchmark object.
        dataset_name: str
            The name of the dataset.
        method_name: str
            The method name.
        benchmark_surrogate_results: str
            The path where the results are located.
        benchmark_name: str
            The benchmark name.

    Returns: float
        The number of configurations explored maximally.
    """
    repeat_nrs = [nr for nr in range(0, 10)]

    number_configs_repeats = []
    for repeat_nr in repeat_nrs:
        dataset_result_file = os.path.join(benchmark_surrogate_results, method_name, f'{dataset_name}_{repeat_nr}.json')
        try:
            with open(dataset_result_file, 'r') as fp:
                result_info = json.load(fp)
        except FileNotFoundError:
            # no results for this repeat/seed, move on to the next
            continue

        if benchmark_name == 'pd1':
            if benchmark.max_budget < 60:
                max_epochs = int(benchmark.max_budget * 18)
                result_info = result_info['hp']
                result_budgets = result_info['epochs']
                result_info = result_info[0:max_epochs]
                result_budgets = result_budgets[0:max_epochs]
            else:
                return -1
        else:
            result_info = result_info['hp']
            result_budgets = result_info['epochs']

        config_ids = []
        for budget_index, budget in enumerate(result_budgets):
            if budget == benchmark.max_budget:
                config_ids.append(result_info[budget_index])

        unique_configs = len(set(result_info))
        number_configs_repeats.append(unique_configs)

    return np.mean(number_configs_repeats)


def generate_walltime_data(
        benchmark_data_path: str,
        surrogate_results_path: str,
        benchmark_name: str,
):
    benchmark_class = {
        'pd1': PD1,
    }

    benchmark_surrogate_results = os.path.join(surrogate_results_path, benchmark_name)
    if benchmark_name == 'lcbench':
        benchmark = LCBench(benchmark_data_path, 'credit_g')
    elif benchmark_name == 'taskset':
        benchmark = TaskSet(benchmark_data_path, 'FixedTextRNNClassification_imdb_patch32_GRU128_bs128')

    if benchmark_name == 'lcbench' or benchmark_name == 'taskset':
        dataset_names = benchmark.load_dataset_names()
    else:
        dataset_names = benchmark_class[benchmark_name].load_dataset_names()

    method_dataset_times = {}
    method_dataset_performances = {}
    updated_dataset_names = []
    success_datasets = dict()

    for method_name in method_names:
        dataset_iteration_overhead = []
        dataset_normalized_regret = []
        for dataset_name in dataset_names:
            if benchmark_name == 'lcbench':
                benchmark.set_dataset_name(dataset_name)
            elif benchmark_name == 'taskset':
                benchmark = TaskSet(benchmark_data_path, dataset_name)
            else:
                benchmark = benchmark_class[benchmark_name](benchmark_data_path, dataset_name)

            iterations_overhead, baseline_incumbent_performance = get_method_dataset_regret_time_performance(
                benchmark,
                dataset_name,
                method_name,
                benchmark_surrogate_results,
                benchmark_name=benchmark_name
            )
            if benchmark_name == 'pd1':
                if benchmark.max_budget < 50:
                    max_epochs = int(benchmark.max_budget * 20)
                    iterations_overhead = iterations_overhead[0: max_epochs]
                    baseline_incumbent_performance = baseline_incumbent_performance[0:max_epochs]
                    if method_name == 'random':
                        updated_dataset_names.append(dataset_name)
                else:
                    continue
            # if len(baseline_incumbent_performance) < 1000:
            #    continue

            if method_name in success_datasets:
                success_datasets[method_name].append(dataset_name)
            else:
                success_datasets[method_name] = [dataset_name]
            dataset_iteration_overhead.append(iterations_overhead)
            dataset_normalized_regret.append(baseline_incumbent_performance)

        method_dataset_times[method_name] = dataset_iteration_overhead
        method_dataset_performances[method_name] = dataset_normalized_regret

    random_times = method_dataset_times['random']

    for method_name in method_names:
        method_times = method_dataset_times[method_name]
        reconstructed_method_times = []
        dataset_names = success_datasets[method_name]
        for dataset_index, _ in enumerate(dataset_names):
            total_random_time = random_times[dataset_index][-1]
            dataset_times = method_times[dataset_index]

            dataset_times = np.array(dataset_times) / total_random_time
            reconstructed_method_times.append(dataset_times)
        method_dataset_times[method_name] = reconstructed_method_times

    datasets_mean_performances = []
    datasets_mean_times = []
    for method_name in method_names:
        method_times = method_dataset_times[method_name]
        method_performances = method_dataset_performances[method_name]

        if benchmark_name == 'pd1':
            dataset_index = 0
            for dataset_method_performance, dataset_method_time in zip(method_performances, method_times):
                dataset_time_curve = []
                dataset_performance_curve = []

                for dataset_index_point, dataset_time_point in enumerate(dataset_method_time):
                    if dataset_time_point <= 1.0:
                        dataset_performance_curve.append(dataset_method_performance[dataset_index_point])
                        dataset_time_curve.append(dataset_method_time[dataset_index_point])
                    elif dataset_time_point > 1:
                        method_performances[dataset_index] = dataset_performance_curve
                        method_times[dataset_index] = dataset_time_curve

                dataset_index += 1

        if benchmark_name == 'pd1':
            min_curve_length = 1000
            min_curve_index = -1

            for i in range(0, len(method_times)):
                curve_length = len(method_times[i])
                if curve_length < min_curve_length:
                    min_curve_length = curve_length
                    min_curve_index = i

            min_curve = method_times[min_curve_index]
            for i in range(0, len(method_times)):
                current_curve = method_times[i]
                current_performance_curve = method_performances[i]
                min_curve_point_index = 0
                transformed_curve = []
                transformed_performances = []
                for point_index, point in enumerate(current_curve):
                    curve_time = point
                    min_curve_time = min_curve[min_curve_point_index]
                    if curve_time >= min_curve_time:
                        min_curve_point_index += 1
                        if min_curve_point_index == len(min_curve):
                            break
                        transformed_curve.append(curve_time)
                        transformed_performances.append(current_performance_curve[point_index])

                method_times[i] = transformed_curve
                method_performances[i] = transformed_performances

            max_curve_length = -1
            max_curve_index = -1
            for i in range(0, len(method_times)):
                curve_length = len(method_times[i])
                if curve_length > max_curve_length:
                    max_curve_length = curve_length
                    max_curve_index = i

            for i in range(0, len(method_times)):
                if i == max_curve_index:
                    continue
                else:
                    method_dataset_performance = method_performances[i]
                    method_dataset_time = method_times[i]

                    current_curve_length = len(method_dataset_time)
                    difference = max_curve_length - current_curve_length
                    method_dataset_performance.extend([method_dataset_performance[-1]] * difference)
                    method_dataset_time.extend([method_dataset_time[-1]] * difference)
                    method_times[i] = method_dataset_time
                    method_performances[i] = method_dataset_performance

        dataset_iteration_overhead = np.mean(method_times, axis=0)
        dataset_normalized_regret = np.mean(method_performances, axis=0)
        cut_dataset_iteration_overhead = []
        cut_dataset_normalized_regret = []
        for time, regret in zip(dataset_iteration_overhead, dataset_normalized_regret):
            if time <= 1:
                cut_dataset_iteration_overhead.append(time)
                cut_dataset_normalized_regret.append(regret)

        datasets_mean_times.append(cut_dataset_iteration_overhead)
        datasets_mean_performances.append(cut_dataset_normalized_regret)

    return datasets_mean_times, datasets_mean_performances


def generate_epoch_performance_data(
        benchmark_data_path: str,
        surrogate_results_path: str,
        benchmark_name: str,
):
    benchmark_class = {
        'pd1': PD1,
    }

    benchmark_surrogate_results = os.path.join(surrogate_results_path, benchmark_name)
    if benchmark_name == 'lcbench':
        benchmark = LCBench(benchmark_data_path, 'credit_g')
    elif benchmark_name == 'taskset':
        benchmark = TaskSet(benchmark_data_path, 'FixedTextRNNClassification_imdb_patch32_GRU128_bs128')

    if benchmark_name == 'lcbench' or benchmark_name == 'taskset':
        dataset_names = benchmark.load_dataset_names()
    else:
        dataset_names = benchmark_class[benchmark_name].load_dataset_names()

    method_mean_regret_performance = []
    method_mean_epochs = []
    for method_name in method_names:
        method_epochs_taken = []
        dataset_normalized_regret = []
        dataset_normalized_std = []
        for dataset_name in dataset_names:
            if benchmark_name == 'lcbench':
                benchmark.set_dataset_name(dataset_name)
            elif benchmark_name == 'taskset':
                benchmark = TaskSet(benchmark_data_path, dataset_name)
            else:
                benchmark = benchmark_class[benchmark_name](benchmark_data_path, dataset_name)

            dataset_epochs_taken, baseline_incumbent_performance, baseline_incumbent_std = get_method_dataset_regret_epoch_performance(
                benchmark,
                dataset_name,
                method_name,
                benchmark_surrogate_results,
                benchmark_name,
            )

            if len(dataset_epochs_taken) < 1000:
                continue

            if benchmark_name == 'pd1':
                if benchmark.max_budget <= 50:
                    max_epochs = int(benchmark.max_budget * 20)
                    dataset_epochs_taken = dataset_epochs_taken[0: max_epochs]
                    baseline_incumbent_performance = baseline_incumbent_performance[0:max_epochs]
                    baseline_incumbent_std = baseline_incumbent_std[0:max_epochs]
                    dataset_epochs_taken = [point / max_epochs for point in dataset_epochs_taken]
                else:
                    continue

            method_epochs_taken.append(dataset_epochs_taken)
            dataset_normalized_regret.append(baseline_incumbent_performance)
            dataset_normalized_std.append(baseline_incumbent_std)

        if benchmark_name == 'pd1':
            min_curve_length = 1000
            min_curve_index = -1
            for i in range(0, len(method_epochs_taken)):
                curve_length = len(method_epochs_taken[i])
                if curve_length < min_curve_length:
                    min_curve_length = curve_length
                    min_curve_index = i

            min_curve = method_epochs_taken[min_curve_index]
            for i in range(0, len(method_epochs_taken)):
                if i == min_curve_index:
                    continue
                current_curve = method_epochs_taken[i]
                current_performance_curve = dataset_normalized_regret[i]
                current_std_curve = dataset_normalized_std[i]
                curve_epochs = 0
                min_curve_index = 0
                transformed_curve = []
                transformed_performances = []
                tranformed_stds = []
                for point_index, point in enumerate(current_curve):
                    curve_epochs = point
                    if curve_epochs > min_curve[min_curve_index]:
                        min_curve_index += 1
                        if min_curve_index == len(min_curve):
                            break
                        transformed_curve.append(curve_epochs)
                        transformed_performances.append(current_performance_curve[point_index])
                        tranformed_stds.append(current_std_curve[point_index])
                dataset_normalized_regret[i] = transformed_performances
                method_epochs_taken[i] = transformed_curve
                dataset_normalized_std[i] = tranformed_stds

        mean_epochs_taken_method = np.mean(method_epochs_taken, axis=0)
        dataset_normalized_regret = np.mean(dataset_normalized_regret, axis=0)
        dataset_normalized_std = np.mean(dataset_normalized_std, axis=0)

        method_mean_regret_performance.append(dataset_normalized_regret)
        method_mean_epochs.append(mean_epochs_taken_method)

    return method_mean_epochs, method_mean_regret_performance, dataset_normalized_std

def plot_rank_performance(
        benchmark_data_path: str,
        surrogate_results_path: str,
        benchmark_name: str,
):
    benchmark_surrogate_results = os.path.join(surrogate_results_path, benchmark_name)
    benchmark_class = {
        'pd1': PD1,
        'nasbench201': NASBench201,
    }
    # It loads all datasets in one go, so we cannot load it for every dataset like the
    # other ones.
    if benchmark_name == 'lcbench':
        benchmark = LCBench(benchmark_data_path, 'credit-g')
    elif benchmark_name == 'taskset':
        benchmark = TaskSet(benchmark_data_path, 'FixedTextRNNClassification_imdb_patch32_GRU128_bs128')

    if benchmark_name == 'lcbench' or benchmark_name == 'taskset':
        dataset_names = benchmark.load_dataset_names()
    else:
        dataset_names = benchmark_class[benchmark_name].load_dataset_names()

    plotting_method_ranks = dict()
    plotting_method_epochs = dict()

    for dataset_index, dataset_name in enumerate(dataset_names):
        if benchmark_name == 'lcbench':
            benchmark.set_dataset_name(dataset_name)
        elif benchmark_name == 'taskset':
            benchmark = TaskSet(benchmark_data_path, dataset_name)
        else:
            benchmark = benchmark_class[benchmark_name](benchmark_data_path, dataset_name)

        method_performances = []
        for method_name in method_names:
            dataset_epochs_taken, baseline_incumbent_performance, _ = get_method_dataset_regret_epoch_performance(
                benchmark,
                dataset_name,
                method_name,
                benchmark_surrogate_results,
                benchmark_name,
            )
            if len(baseline_incumbent_performance) < 1000:
                continue

            if method_name in plotting_method_epochs:
                plotting_method_epochs[method_name].append(dataset_epochs_taken)
            else:
                plotting_method_epochs[method_name] = [dataset_epochs_taken]

            method_performances.append(baseline_incumbent_performance)

        dataset_method_ranks = dict()
        for iteration_point in range(0, 1000):
            method_point_performances = []
            for method_index in range(0, len(method_names)):
                method_curve = method_performances[method_index]
                method_point_curve = method_curve[iteration_point]
                method_point_performances.append(method_point_curve)

            method_ranks = rankdata(method_point_performances, method='min')
            for method_name, method_rank in zip(method_names, method_ranks):
                if method_name in dataset_method_ranks:
                    dataset_method_ranks[method_name].append(method_rank)
                else:
                    dataset_method_ranks[method_name] = [method_rank]

        for method_name in method_names:
            if method_name in plotting_method_ranks:
                plotting_method_ranks[method_name].append(dataset_method_ranks[method_name])
            else:
                plotting_method_ranks[method_name] = [dataset_method_ranks[method_name]]

    all_data = []
    for method_name in method_names:
        plotting_method_ranks[method_name] = np.mean(plotting_method_ranks[method_name], axis=0)
        all_data.append(plotting_method_ranks[method_name])
        plotting_method_epochs[method_name] = np.mean(plotting_method_epochs[method_name], axis=0)

        plt.plot(plotting_method_epochs[method_name], plotting_method_ranks[method_name], label=method_name)

    # plt.violinplot(all_data)

    plt.ylabel('Rank')
    # set style for the axes
    plt.legend(bbox_to_anchor=(1.04, 1), loc="upper left")
    # plt.xticks([i for i in range(1, len(method_names) + 1)], method_names)
    plt.savefig(f'comparison_ranks_{benchmark_name}.pdf', bbox_inches="tight")


def build_cd_diagram(
        benchmark_data_path: str,
        surrogate_results_path: str,
        benchmark_name: str,
        half: bool,
) -> pd.DataFrame:
    table_results = {
        'Method': [],
        'Dataset Name': [],
        'Regret': [],
    }

    benchmark_class = {
        'pd1': PD1,
        'nasbench201': NASBench201,
    }

    benchmark_surrogate_results = os.path.join(surrogate_results_path, benchmark_name)
    if benchmark_name == 'lcbench':
        benchmark = LCBench(benchmark_data_path, 'credit_g')
    elif benchmark_name == 'taskset':
        benchmark = TaskSet(benchmark_data_path, 'FixedTextRNNClassification_imdb_patch32_GRU128_bs128')

    if benchmark_name == 'lcbench' or benchmark_name == 'taskset':
        dataset_names = benchmark.load_dataset_names()
    else:
        dataset_names = benchmark_class[benchmark_name].load_dataset_names()

    for method_name in method_names:
        for dataset_name in dataset_names:
            if benchmark_name == 'lcbench':
                benchmark.set_dataset_name(dataset_name)
            elif benchmark_name == 'taskset':
                benchmark = TaskSet(benchmark_data_path, dataset_name)
            else:
                benchmark = benchmark_class[benchmark_name](benchmark_data_path, dataset_name)

            _, baseline_incumbent_performance, _ = get_method_dataset_regret_epoch_performance(
                benchmark,
                dataset_name,
                method_name,
                benchmark_surrogate_results,
                benchmark_name,
                half,
            )

            if benchmark_name == 'pd1':
                if benchmark.max_budget * 20 > 1000:
                    continue
                baseline_incumbent_performance = baseline_incumbent_performance[0:int(benchmark.max_budget * 20)]
            if half:
                baseline_incumbent_performance = baseline_incumbent_performance[
                                                 0:int(len(baseline_incumbent_performance) / 2)]
            final_performance = baseline_incumbent_performance[-1]
            table_results['Method'].append(pretty_names[method_name])
            table_results['Dataset Name'].append(dataset_name)
            table_results['Regret'].append(final_performance)

    result_df = pd.DataFrame(data=table_results)
    if half:
        file_extension = 'half'
    else:
        file_extension = 'full'

    result_df.to_csv(f'{benchmark_name}_cdinfo_{file_extension}.csv', index=False)

    return result_df


def plot_config_distribution(
        benchmark_data_path: str,
        surrogate_results_path: str,
        benchmark_name: str,
):
    benchmark_class = {
        'pd1': PD1,
        'nasbench201': NASBench201,
    }

    config_info_dict = {
        'Method': [],
        'Dataset Name': [],
        'Number Of Configurations': [],
    }
    benchmark_surrogate_results = os.path.join(surrogate_results_path, benchmark_name)
    if benchmark_name == 'lcbench':
        benchmark = LCBench(benchmark_data_path, 'credit_g')
    elif benchmark_name == 'taskset':
        benchmark = TaskSet(benchmark_data_path, 'FixedTextRNNClassification_imdb_patch32_GRU128_bs128')

    if benchmark_name == 'lcbench' or benchmark_name == 'taskset':
        dataset_names = benchmark.load_dataset_names()
    else:
        dataset_names = benchmark_class[benchmark_name].load_dataset_names()

    # TODO remove hack
    method_names = ['power_law', 'ahb', 'dehb', 'dragonfly', 'random', 'bohb', 'smac', 'hyperband']  # 'nonlc_power_law'
    for method_name in method_names:
        for dataset_name in dataset_names:
            if benchmark_name == 'lcbench':
                benchmark.set_dataset_name(dataset_name)
            elif benchmark_name == 'taskset':
                benchmark = TaskSet(benchmark_data_path, dataset_name)
            else:
                benchmark = benchmark_class[benchmark_name](benchmark_data_path, dataset_name)

            dataset_nr_configs = get_method_dataset_number_configs(
                benchmark,
                dataset_name,
                method_name,
                benchmark_surrogate_results,
                benchmark_name,
            )
            if dataset_nr_configs == -1:
                continue

            config_info_dict['Method'].append(pretty_names[method_name])
            config_info_dict['Dataset Name'].append(dataset_name)
            config_info_dict['Number Of Configurations'].append(dataset_nr_configs)

    ax = sns.violinplot(y='Method', x='Number Of Configurations', data=config_info_dict, kind='violin')

    if benchmark_name == 'lcbench':
        plt.title('LCBench')
        # plt.ylabel('Average Normalized Regret')
    elif benchmark_name == 'taskset':
        plt.title('TaskSet')
    elif benchmark_name == 'pd1':
        plt.title('PD1')
        plt.legend(bbox_to_anchor=(1.04, 1), loc="upper left")

    plt.savefig(f'config_distribution_{benchmark_name}.pdf', bbox_inches="tight")


def plot_dataset_epoch_performance(
        benchmark_data_path: str,
        surrogate_results_path: str,
        benchmark_name: str,
):
    benchmark_class = {
        'pd1': PD1,
        'nasbench201': NASBench201,
    }

    benchmark_surrogate_results = os.path.join(surrogate_results_path, benchmark_name)
    if benchmark_name == 'lcbench':
        benchmark = LCBench(benchmark_data_path, 'credit_g')
    elif benchmark_name == 'taskset':
        benchmark = TaskSet(benchmark_data_path, 'FixedTextRNNClassification_imdb_patch32_GRU128_bs128')

    if benchmark_name == 'lcbench' or benchmark_name == 'taskset':
        dataset_names = benchmark.load_dataset_names()
    else:
        dataset_names = benchmark_class[benchmark_name].load_dataset_names()

    for dataset_index, dataset_name in enumerate(dataset_names):
        if benchmark_name == 'lcbench':
            benchmark.set_dataset_name(dataset_name)
        elif benchmark_name == 'taskset':
            benchmark = TaskSet(benchmark_data_path, dataset_name)
        else:
            benchmark = benchmark_class[benchmark_name](benchmark_data_path, dataset_name)

        fig = plt.figure()
        if benchmark_name == 'pd1':
            if benchmark.max_budget > 50:
                continue

        for method_name in method_names:

            dataset_epochs_taken, baseline_incumbent_mean, baseline_incumbent_std = get_method_dataset_regret_epoch_performance(
                benchmark,
                dataset_name,
                method_name,
                benchmark_surrogate_results,
                benchmark_name,
            )

            if len(dataset_epochs_taken) < 1000:
                continue

            if benchmark_name == 'pd1':
                if benchmark.max_budget <= 50:
                    max_epochs = int(benchmark.max_budget * 20)
                    dataset_epochs_taken = dataset_epochs_taken[0: max_epochs]
                    baseline_incumbent_mean = baseline_incumbent_mean[0:max_epochs]
                    baseline_incumbent_std = baseline_incumbent_std[0:max_epochs]
                    dataset_epochs_taken = [point / max_epochs for point in dataset_epochs_taken]
                else:
                    continue
            hpo_budget = dataset_epochs_taken / dataset_epochs_taken[-1]
            plt.plot(hpo_budget, baseline_incumbent_mean, label=pretty_names[method_name], linewidth=4)

        plt.ylabel('Average Regret')
        if benchmark_name == 'lcbench':
            plt.xlabel('HPO Budget')
            plt.title(f'{dataset_name}')
        elif benchmark_name == 'taskset':
            plt.xlabel('HPO Budget')
            plt.title(f'{dataset_name[0:26]}\n{dataset_name[27:-1]}')
        else:
            plt.xlabel('HPO Budget')
            plt.title(f'{dataset_name}')
        plt.tick_params(left=True, bottom=True)
        plt.legend(bbox_to_anchor=(0.5, -0.52), loc='lower center', ncol=3)
        plt.savefig(f'./{benchmark_name}/{dataset_name}_regret_epochs.pdf', bbox_inches='tight')
        plt.close(fig)


def plot_all_baselines_epoch_performance(project_folder, result_path):
    fig, ax = plt.subplots(3, 3, sharey=True)
    benchmark_names = ['lcbench', 'taskset', 'pd1']

    for axes in ax[1]:
        axes.set_visible(False)

    for axes in ax[2]:
        axes.set_visible(False)

    for axes, benchmark_name in zip(ax[0], benchmark_names):

        if benchmark_name == 'lcbench':
            benchmark_extension = os.path.join(
                'lc_bench',
                'results',
                'data_2k.json',
            )
        elif benchmark_name == 'taskset':
            benchmark_extension = os.path.join(
                'data',
                'taskset',
            )
        else:
            benchmark_extension = ''

        benchmark_data_path = os.path.join(
            project_folder,
            benchmark_extension,
        )

        methods_mean_epochs, methods_mean_regret, methods_mean_std = \
            generate_epoch_performance_data(
                benchmark_data_path,
                result_path,
                benchmark_name
            )
        for method_index, method_name in enumerate(method_names):
            axes.plot(
                methods_mean_epochs[method_index],
                methods_mean_regret[method_index],
                label=pretty_names[method_name],
                linewidth=4,
            )
            axes.fill_between(methods_mean_regret[method_index], np.add(methods_mean_regret[method_index], methods_mean_std[method_index]), np.subtract(methods_mean_regret[method_index], methods_mean_std[method_index]), alpha=0.1)

        if benchmark_name == 'lcbench':
            axes.set_title('LCBench')
            axes.set_xlabel('Epochs')
            axes.tick_params(left=True, bottom=True)
            axes.xaxis.set_major_locator(MultipleLocator(250))
            axes.xaxis.set_major_formatter('{x:.0f}')
            # For the minor ticks, use no labels; default NullFormatter.
            axes.xaxis.set_minor_locator(MultipleLocator(10))
            axes.set_ylabel('Average Normalized Regret')
        elif benchmark_name == 'taskset':
            axes.set_title('TaskSet')
            axes.set_xlabel('Steps')
            axes.tick_params(left=True, bottom=True)
            axes.xaxis.set_major_locator(MultipleLocator(250))
            axes.xaxis.set_major_formatter('{x:.0f}')
            # For the minor ticks, use no labels; default NullFormatter.
            axes.xaxis.set_minor_locator(MultipleLocator(100))
        elif benchmark_name == 'pd1':
            axes.set_title('PD1')
            axes.set_xlabel('Fraction of Iterations')
            axes.tick_params(left=True, bottom=True)
            axes.xaxis.set_major_locator(MultipleLocator(0.25))
            axes.xaxis.set_major_formatter('{x:.2f}')
            # For the minor ticks, use no labels; default NullFormatter.
            axes.xaxis.set_minor_locator(MultipleLocator(0.1))

        axes.set_yscale('log')
        handles, labels = axes.get_legend_handles_labels()

    fig.legend(handles, labels, bbox_to_anchor=(0.5, 0.52), loc='lower center', ncol=8)
    fig.savefig(f'comparison_epochs.pdf', bbox_inches="tight")


def plot_all_baselines_time_performance(project_folder, result_path):
    fig, ax = plt.subplots(2, 2, sharey=True)
    benchmark_names = ['lcbench', 'pd1']

    for axes in ax[1]:
        axes.set_visible(False)

    for axes, benchmark_name in zip(ax[0], benchmark_names):

        if benchmark_name == 'lcbench':
            benchmark_extension = os.path.join(
                'lc_bench',
                'results',
                'data_2k.json',
            )
        elif benchmark_name == 'taskset':
            benchmark_extension = os.path.join(
                'data',
                'taskset',
            )
        else:
            benchmark_extension = ''

        benchmark_data_path = os.path.join(
            project_folder,
            benchmark_extension,
        )

        methods_mean_epochs, methods_mean_regret = \
            generate_walltime_data(
                benchmark_data_path,
                result_path,
                benchmark_name,
            )

        for method_index, method_name in enumerate(method_names):
            axes.plot(
                methods_mean_epochs[method_index],
                methods_mean_regret[method_index],
                label=pretty_names[method_name],
                linewidth=4,
            )

        if benchmark_name == 'lcbench':
            axes.set_title('LCBench')
            axes.set_xlabel('Normalized Walltime')
            axes.tick_params(left=True, bottom=True)
            axes.xaxis.set_major_locator(MultipleLocator(0.2))
            axes.xaxis.set_major_formatter('{x:.2f}')
            # For the minor ticks, use no labels; default NullFormatter.
            axes.xaxis.set_minor_locator(MultipleLocator(0.1))
            axes.set_ylabel('Average Normalized Regret')
        elif benchmark_name == 'taskset':
            axes.set_title('TaskSet')
            axes.set_xlabel('Steps')
            axes.tick_params(left=True, bottom=True)
            axes.xaxis.set_major_locator(MultipleLocator(250))
            axes.xaxis.set_major_formatter('{x:.0f}')
            # For the minor ticks, use no labels; default NullFormatter.
            axes.xaxis.set_minor_locator(MultipleLocator(100))
        elif benchmark_name == 'pd1':
            axes.set_title('PD1')
            axes.set_xlabel('Normalized Walltime')
            axes.tick_params(left=True, bottom=True)
            axes.xaxis.set_major_locator(MultipleLocator(0.2))
            axes.xaxis.set_major_formatter('{x:.2f}')
            # For the minor ticks, use no labels; default NullFormatter.
            axes.xaxis.set_minor_locator(MultipleLocator(0.1))

        axes.set_yscale('log')
        handles, labels = axes.get_legend_handles_labels()

    fig.legend(handles, labels, bbox_to_anchor=(0.5, 0.4), loc='lower center', ncol=8)
    fig.savefig(f'comparison_time.pdf', bbox_inches="tight")

File Path: python_scripts/cifar10.py
Content:
from __future__ import print_function
import argparse
import json
import os

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

from models.preact_resnet import PreActResNet18


def main():

    # Training settings
    parser = argparse.ArgumentParser(description='CIFAR10 benchmarking')
    parser.add_argument('--train_batch_size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test_batch_size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--nr_epochs', type=int, default=50, metavar='N',
                        help='number of epochs to train (default: 50)')
    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
                        help='learning rate (default: 0.01)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--dry-run', action='store_true', default=False,
                        help='quickly check a single pass')
    parser.add_argument('--seed', type=int, default=11, metavar='S',
                        help='random seed (default: 11)')
    parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model')
    parser.add_argument('--output_path', type=str, default='./benchmarks/cifar',
                        help='Path where the results will be stored.')
    parser.add_argument('--index', type=int, default=1,
                        help='The index of the dropout rate to be used.')

    args = parser.parse_args()
    os.makedirs(args.output_path, exist_ok=True)
    file_path = os.path.join(
        args.output_path,
        f'hp_config{args.index}.log',
    )
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    drop_rate_possible_values = np.arange(0.05, 0.85, 0.05)
    drop_rate = drop_rate_possible_values[args.index]
    use_cuda = not args.no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    transform = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ]
    )

    train_set = datasets.CIFAR10(
        root='./data',
        train=True,
        download=False,
        transform=transform,
    )
    train_loader = torch.utils.data.DataLoader(
        train_set,
        batch_size=args.train_batch_size,
        shuffle=True,
        num_workers=1,
    )

    test_set = datasets.CIFAR10(
        root='./data',
        train=False,
        download=False,
        transform=transform,
    )
    test_loader = torch.utils.data.DataLoader(
        test_set,
        batch_size=args.test_batch_size,
        shuffle=False,
        num_workers=1,
    )

    criterion = nn.CrossEntropyLoss()
    model = PreActResNet18(drop_rate).to(device)
    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.nr_epochs)
    test_epoch_performances = []
    for epoch in range(args.nr_epochs):

        model.train()
        running_loss = 0.0

        for i, data in enumerate(train_loader, 0):

            inputs, labels = data
            # zero the parameter gradients
            optimizer.zero_grad()
            # forward + backward + optimize
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()
            if i % 100 == 99:  # print every 100 mini-batches
                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 99:.3f}')
                running_loss = 0.0

        model.eval()
        # total number of examples evaluated
        total = 0
        # number of correctly classified examples
        correct = 0
        with torch.no_grad():
            for data in test_loader:
                images, labels = data
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        fraction_correct_examples = correct / total
        test_epoch_performances.append(fraction_correct_examples)
        with open(file_path, 'w') as fp:
            json.dump({drop_rate: test_epoch_performances}, fp)

        scheduler.step(epoch + 1)

    if args.save_model:
        torch.save(
            model.state_dict(),
            os.path.join(args.output_path, f'{drop_rate}_cifar10_cnn.pt'),
        )

if __name__ == '__main__':
    main()

File Path: python_scripts/download_task_set_data.py
Content:
import argparse
import json
import os
import urllib

from concurrent import futures
import numpy as np
import tensorflow as tf # for gfile.
import tensorflow_io as tfio
import tqdm


parser = argparse.ArgumentParser(
    description='Prepare hyperparameter candidates from the taskset task',
)
parser.add_argument(
    '--task_id',
    help='The task index to retrieve from all the TaskSet tasks',
    type=int,
    default=0,
)
parser.add_argument(
    '--output_dir',
    help='The output directory where the validation curves and hyperparameter configurations will be saved',
    type=str,
    default='./taskset',
)

gfile = tf.io.gfile


def load_joint_cache(task, opt_set_name):
    """Loads the learning curves for the given task and opt_set_name."""
    base_dir = "gs://gresearch/task_set_data/"
    p = os.path.join(base_dir, task,
                     "%s_10000_replica5.npz" % (opt_set_name))
    cc = np.load(gfile.GFile(p, "rb"))
    return cc["optimizers"], cc["xs"], cc["ys"]


def threaded_tqdm_map(threads, func, data):
    """Helper that does a map on multiple threads."""
    future_list = []
    with futures.ThreadPoolExecutor(threads) as executor:
        for l in tqdm.tqdm(data, position=0):
            future_list.append(executor.submit(func, l))
        return [x.result() for x in tqdm.tqdm(future_list, position=0)]


def load_tasks(tasks):
    """Multi threaded loading of all data for each task.
    Args:
      tasks: list of task names
    Returns:
      A dictionary mapping taks name to tuples of:
      (optimizer names, x data points, and y data points)
    """

    def load_one(t):
        adam8p = load_joint_cache(t, "adam8p_wide_grid_1k")
        adam6p = load_joint_cache(t, "adam6p_wide_grid_1k")
        adam4p = load_joint_cache(t, "adam4p_wide_grid_1k")
        adam1p = load_joint_cache(t, "adam1p_wide_grid_1k")
        nadamw = load_joint_cache(t, "nadamw_grid_1k")
        return {
            "adam8p_wide_grid_1k": adam8p,
            "adam6p_wide_grid_1k": adam6p,
            "adam4p_wide_grid_1k": adam4p,
            "adam1p_wide_grid_1k": adam1p,
            "nadamw": nadamw,
        }

    results = threaded_tqdm_map(100, load_one, tasks)

    for k, v in zip(tasks, results):
        if v is None:
            print("No data found for task: %s" % k)

    return {k: v for k, v in zip(tasks, results) if v is not None}


def get_task_names():
    content = gfile.GFile("gs://gresearch/task_set_data/task_names.txt").read()
    return sorted(content.strip().split("\n"))


args = parser.parse_args()

task_id = args.task_id
task_names = get_task_names()

for task_name in task_names:
    if task_name.startswith('FixedTextRNNClassification'):

        results = load_tasks([task_name])

        # For each task, there is then a dictionary of optimizer families.
        optimizer_families = results[task_name].keys()
        # hardcode to only the search space with 8 hyperparameters now
        optimizer_name = 'adam8p_wide_grid_1k'
        optimizer_names, x, y = results[task_name][optimizer_name]

        nr_seeds = y.shape[1]
        nr_optimizations = y.shape[0]
        train_curves = []
        val_curves = []
        test_curves = []

        for hp_index in range(nr_optimizations):

            train_seed_curve = []
            valid_seed_curve = []
            test_seed_curve = []

            for seed_index in range(nr_seeds):
                train_seed_curve.append(y[hp_index, seed_index, :, 0])
                valid_seed_curve.append(y[hp_index, seed_index, :, 1])
                test_seed_curve.append(y[hp_index, seed_index, :, 2])

            train_seed_curve = np.mean(train_seed_curve, axis=0)
            valid_seed_curve = np.mean(valid_seed_curve, axis=0)
            test_seed_curve = np.mean(test_seed_curve, axis=0)

            train_curves.append(train_seed_curve.tolist())
            val_curves.append(valid_seed_curve.tolist())
            test_curves.append(test_seed_curve.tolist())

        os.makedirs(args.output_dir, exist_ok=True)

        path = "https://raw.githubusercontent.com/google-research/google-research/master/task_set/optimizers/configs/adam8p_wide_grid.json"
        configs = json.loads(urllib.request.urlopen(path).read())
        hparam_dicts = [configs[optname.decode("utf8")][0] for optname in optimizer_names]

        all_results = []

        for hp_index, hp_config in enumerate(hparam_dicts):

            hp_config_result = {
                'hp': hp_config,
                'train': {'loss': train_curves[hp_index]},
                'valid': {'loss': val_curves[hp_index]},
                'test': {'loss': test_curves[hp_index]},
            }

            all_results.append(hp_config_result)

        result_path = os.path.join(
            args.output_dir,
            f'{task_name}_0_{nr_optimizations}.json',
        )

        with open(result_path, 'w') as file_handle:
            json.dump(all_results, file_handle)

    else:
        continue
File Path: surrogate_models/__init__.py
Content:

File Path: surrogate_models/asha.py
Content:
from typing import Dict, List, OrderedDict, Tuple

import numpy as np

import optuna


class AHBOptimizer:

    def __init__(
        self,
        hyperparameter_candidates: np.ndarray,
        param_space: OrderedDict,
        min_budget: int,
        max_budget: int,
        eta: int,
        seed: int = 11,
        max_nr_trials: int = 1000,
        maximization: bool = True,
        **kwargs,
    ):
        """
        Wrapper for the Async Hyperband algorithm.

        Args:
        -----
        hyperparameter_candidates: np.ndarray
            2d array which contains all possible configurations which can be queried.
        param_space: OrderedDict
            The hyperparameter search-space, indicating the type and range of every
            hyperparameter.
        min_budget: int
            Minimum number of epochs available.
        max_budget: int
            Maximum number of epochs available.
        eta: int
            Halving factor
        seed: int
            Seed used to reproduce the experiments.
        max_nr_trials: int
            Maximum number of HPO trials.
        maximization: bool
            If the inner objective is to maximize or minimize.
        """
        self.maximization = maximization
        self.min_budget = min_budget
        self.max_budget = max_budget
        self.eta = eta
        self.max_nr_trials = max_nr_trials
        self.extra_arguments = kwargs

        self.param_space = param_space
        self.hyperparameter_candidates = hyperparameter_candidates
        self.hyperparameter_mapping = self.create_configuration_to_indices(
            hyperparameter_candidates,
        )
        self.transformed_hp_candidates = self.from_hp_value_to_unit_cube_values(
            hyperparameter_candidates,
        )

        self.distribution = self.get_optuna_search_space()

        # empty configuration, empty budget, empty information for config
        self.next_conf = None
        self.trial = None
        self.conf_budget = None
        self.conf_info = None
        self.fidelity_index = None
        self.rng = np.random.RandomState(seed)
        np.random.seed(seed)

        self.evaluated_configurations = dict()
        self.evaluated_hp_curves = dict()

        # define study with hyperband pruner.
        sampler = optuna.samplers.RandomSampler(seed=seed)
        self.study = optuna.create_study(
            sampler=sampler,
            direction='maximize' if self.maximization else 'minimize',
            pruner=optuna.pruners.HyperbandPruner(
                min_resource=self.min_budget,
                max_resource=self.max_budget,
                reduction_factor=self.eta,
            ),
        )

    def suggest(self) -> Tuple[int, int]:
        """
        Get information about the next configuration.

        Returns:
        ________
        next_conf, conf_budget: tuple
            A tuple that contains information about the next
            configuration (index in the hyperparameter_candidates it was
            given) and the budget for the hyperparameter to be evaluated
            on.
        """
        if self.next_conf is None:

            self.trial = self.study.ask(self.distribution)
            self.next_conf = self.get_hp_config_from_trial(self.conf_budget)
            self.conf_budget = 1

            # if the hyperparameter has been evaluated before
            while self.next_conf in self.evaluated_hp_curves:

                val_curve = self.evaluated_hp_curves[self.next_conf]
                # it was not evaluated as far as now, it can go to the framework
                if self.conf_budget > len(val_curve):
                    break
                else:
                    pruned_trial = False

                    score = val_curve[self.conf_budget - 1]
                    self.trial.report(score, self.conf_budget)

                    if self.trial.should_prune():
                        pruned_trial = True

                    if pruned_trial:
                        self.study.tell(self.trial, state=optuna.trial.TrialState.PRUNED)
                        # hyperparameter config was pruned, sample another one
                        self.trial = self.study.ask(self.distribution)
                        if self.conf_budget in self.evaluated_configurations:
                            self.evaluated_configurations[self.conf_budget].add(self.next_conf)
                        else:
                            self.evaluated_configurations[self.conf_budget] = set([self.next_conf])
                        self.conf_budget = 1
                        self.next_conf = self.get_hp_config_from_trial(self.conf_budget)

                    else:
                        if self.conf_budget == self.max_budget:
                            self.study.tell(self.trial, val_curve[-1])
                            self.trial = self.study.ask(self.distribution)
                            if self.conf_budget in self.evaluated_configurations:
                                self.evaluated_configurations[self.conf_budget].add(self.next_conf)
                            else:
                                self.evaluated_configurations[self.conf_budget] = set([self.next_conf])
                            self.conf_budget = 1
                            self.next_conf = self.get_hp_config_from_trial(self.conf_budget)
                        else:
                            # Increase the budget
                            self.conf_budget += 1

        return self.next_conf, self.conf_budget

    def observe(
        self,
        hp_index: int,
        budget: int,
        learning_curve: List[float],
    ):
        """
        Respond regarding the performance of a
        hyperparameter configuration. get_next should
        be called first to retrieve the configuration.

        Args:
        -----
        hp_index: int
            The index of the evaluated hyperparameter configuration.
        budget: int
            The budget for which the hyperparameter configuration was evaluated.
        learning curve: np.ndarray, list
            validation accuracy curve. The last value is the same as the score.
        """
        assert self.next_conf is not None, 'Call get_next first.'
        pruned_trial = False

        score = learning_curve[-1]
        self.trial.report(score, self.conf_budget)

        if self.trial.should_prune():
            pruned_trial = True

        if pruned_trial:
            self.study.tell(self.trial, state=optuna.trial.TrialState.PRUNED)  # tell the pruned state
            self.evaluated_hp_curves[self.next_conf] = learning_curve
            if self.conf_budget in self.evaluated_configurations:
                self.evaluated_configurations[self.conf_budget].add(self.next_conf)
            else:
                self.evaluated_configurations[self.conf_budget] = set([self.next_conf])
            self.next_conf = None

        if self.conf_budget == self.max_budget:
            self.study.tell(self.trial, score, state=optuna.trial.TrialState.COMPLETE)
            self.evaluated_hp_curves[self.next_conf] = learning_curve
            if self.conf_budget in self.evaluated_configurations:
                self.evaluated_configurations[self.conf_budget].add(self.next_conf)
            else:
                self.evaluated_configurations[self.conf_budget] = set([self.next_conf])
            self.next_conf = None
        else:
            self.conf_budget += 1

    def create_configuration_to_indices(
        self,
        hyperparameter_candidates: np.ndarray,
    ) -> Dict[tuple, int]:
        """
        Maps every configuration to its index as specified
        in hyperparameter_candidates.

        Args:
        -----
        hyperparameter_candidates: np.ndarray
            All the possible hyperparameter candidates given
            by the calling framework.

        Returns:
        ________
        hyperparameter_mapping: dict
            A dictionary where the keys are tuples representing
            hyperparameter configurations and the values are indices
            representing their placement in hyperparameter_candidates.
        """
        hyperparameter_mapping = dict()
        for i in range(0, hyperparameter_candidates.shape[0]):
            hyperparameter_mapping[tuple(hyperparameter_candidates[i])] = i

        return hyperparameter_mapping

    def map_configuration_to_index(
        self,
        hyperparameter_candidate: np.ndarray,
    ) -> int:
        """
        Return the index of the hyperparameter_candidate from
        the given initial array of possible hyperparameters.

        Args:
        -----
        hyperparameter_candidate: np.ndarray
            Hyperparameter configuration.

        Returns:
        ________
        index of the hyperparameter_candidate.
        """
        hyperparameter_candidate = tuple(hyperparameter_candidate)

        return self.hyperparameter_mapping[hyperparameter_candidate]

    def get_optuna_search_space(self):
        """
        Get the optuna hyperparameter search space distribution.

        Returns:
        --------
        distribution: dict
            The hyperparameter search space distribution for optuna.
        """
        distribution = {}
        for i, (k, v) in enumerate(self.param_space.items()):
            hp_type = v[2]
            is_log = v[3]
            if hp_type == str:
                distribution[k] = optuna.distributions.UniformDistribution(0, 1)
            else:
                if is_log:
                    distribution[k] = optuna.distributions.LogUniformDistribution(0.00001, 1)
                else:
                    distribution[k] = optuna.distributions.UniformDistribution(0, 1)

        return distribution

    def get_hp_config_from_trial(self, budget: int):
        """
        Get the hyperparameter config index from the
        optuna trial.

        Args:
        -----
        budget: int
            The budget to run the hyperparameter configuration for.

        Returns:
        --------
        conf_index: int
            The hyperparameter config index.
        """
        hp_config = []
        for hp_name in self.param_space.keys():
            hp_config.append(self.trial.params[hp_name])

        conf_index = self.map_closest_evaluated(hp_config, budget)

        return conf_index

    def map_closest_evaluated(
        self,
        config: List,
        budget: int,
    ) -> int:
        """
        Maps the hyperparameter configuration to the closest
        available hyperparameter configuration.

        Args:
        -----
        config: List
            The hyperparameter configuration suggested by the baseline.
        budget: int
            The budget of the hyperparameter configuration.

        Returns:
        --------
        closest_configuration_index: int
            An index of the closest matching configuration.
        """
        closest_configuration_index = None
        smallest_distance = np.inf

        for i in range(0, self.transformed_hp_candidates.shape[0]):
            current_distance = 0
            possible_config = self.transformed_hp_candidates[i, :]
            for hyperparameter_index in range(0, len(config)):
                main_config_hyperparameter_value = config[hyperparameter_index]
                candidate_config_hyperparameter_value = possible_config[hyperparameter_index]
                current_distance += abs(main_config_hyperparameter_value - candidate_config_hyperparameter_value)
            if current_distance < smallest_distance:
                if len(self.evaluated_configurations) != 0:
                    # if a hyperparameter has already been evaluated for a certain
                    # budget, we do not consider it anymore.
                    if budget in self.evaluated_configurations and i in self.evaluated_configurations[budget]:
                        continue
                smallest_distance = current_distance
                closest_configuration_index = i

        return closest_configuration_index

    def from_hp_value_to_unit_cube_values(
        self,
        hp_candidates: np.ndarray,
    ) -> np.ndarray:
        """
        Maps the hyperparameter configurations from the original
        space to the unit cube space.

        Args:
        -----
        hp_candidates: np.ndarray
            The hyperparameter configuration suggested by the baseline.

        Returns:
        --------
        new_configs: np.ndarray
            An array representing the hyperparameter configurations
            in unit cube space.
        """
        assert len(hp_candidates[0]) == len(self.param_space)

        new_configs = []

        for i in range(0, hp_candidates.shape[0]):
            new_config = []
            configuration = hp_candidates[i]
            for hp_index, (k, v) in enumerate(self.param_space.items()):
                hp_type = v[2]
                value = configuration[hp_index]
                lower, upper = v[0], v[1]
                is_log = v[3]
                if hp_type == str:
                    unique_values = v[0]
                    ranges = np.arange(start=0, stop=1, step=1 / len(unique_values))
                    for range_index, unique_value in enumerate(unique_values):
                        if unique_value == value:
                            step_size = (1 / len(unique_values))
                            # set the value at the middle of the hyperparameter
                            # allocated range
                            value = ranges[range_index] + step_size / 0.5
                        else:
                            # do nothing
                            pass
                else:
                    if is_log:
                        log_range = np.log(upper) - np.log(lower)
                        value = (np.log(value) - np.log(lower)) / log_range
                    else:
                        value = (value - lower) / (upper - lower)
                    new_config.append(value)
            new_configs.append(new_config)

        return np.array(new_configs)

File Path: surrogate_models/dehb/__init__.py
Content:

File Path: surrogate_models/dehb/dehb/__init__.py
Content:
from .optimizers import DE, AsyncDE
from .optimizers import DEHB
from .utils import SHBracketManager

File Path: surrogate_models/dehb/dehb/optimizers/__init__.py
Content:
from .de import DE, AsyncDE
from .dehb import DEHB, DEHBBase

File Path: surrogate_models/dehb/dehb/optimizers/de.py
Content:
import os
import numpy as np
import ConfigSpace
import ConfigSpace.util
from typing import List
from distributed import Client


class DEBase():
    '''Base class for Differential Evolution
    '''
    def __init__(self, cs=None, f=None, dimensions=None, pop_size=None, max_age=None,
                 mutation_factor=None, crossover_prob=None, strategy=None, budget=None,
                 boundary_fix_type='random', **kwargs):
        # Benchmark related variables
        self.cs = cs
        self.f = f
        if dimensions is None and self.cs is not None:
            self.dimensions = len(self.cs.get_hyperparameters())
        else:
            self.dimensions = dimensions

        # DE related variables
        self.pop_size = pop_size
        self.max_age = max_age
        self.mutation_factor = mutation_factor
        self.crossover_prob = crossover_prob
        self.strategy = strategy
        self.budget = budget
        self.fix_type = boundary_fix_type

        # Miscellaneous
        self.configspace = True if isinstance(self.cs, ConfigSpace.ConfigurationSpace) else False
        self.hps = dict()
        if self.configspace:
            for i, hp in enumerate(cs.get_hyperparameters()):
                # maps hyperparameter name to positional index in vector form
                self.hps[hp.name] = i
        self.output_path = kwargs['output_path'] if 'output_path' in kwargs else './'
        os.makedirs(self.output_path, exist_ok=True)

        # Global trackers
        self.inc_score = np.inf
        self.inc_config = None
        self.population = None
        self.fitness = None
        self.age = None
        self.history = []

    def reset(self):
        self.inc_score = np.inf
        self.inc_config = None
        self.population = None
        self.fitness = None
        self.age = None
        self.history = []

    def _shuffle_pop(self):
        pop_order = np.arange(len(self.population))
        np.random.shuffle(pop_order)
        self.population = self.population[pop_order]
        self.fitness = self.fitness[pop_order]
        self.age = self.age[pop_order]

    def _sort_pop(self):
        pop_order = np.argsort(self.fitness)
        np.random.shuffle(pop_order)
        self.population = self.population[pop_order]
        self.fitness = self.fitness[pop_order]
        self.age = self.age[pop_order]

    def _set_min_pop_size(self):
        if self.mutation_strategy in ['rand1', 'rand2dir', 'randtobest1']:
            self._min_pop_size = 3
        elif self.mutation_strategy in ['currenttobest1', 'best1']:
            self._min_pop_size = 2
        elif self.mutation_strategy in ['best2']:
            self._min_pop_size = 4
        elif self.mutation_strategy in ['rand2']:
            self._min_pop_size = 5
        else:
            self._min_pop_size = 1

        return self._min_pop_size

    def init_population(self, pop_size: int) -> List:
        if self.configspace:
            # sample from ConfigSpace s.t. conditional constraints (if any) are maintained
            population = self.cs.sample_configuration(size=pop_size)
            if not isinstance(population, List):
                population = [population]
            # the population is maintained in a list-of-vector form where each ConfigSpace
            # configuration is scaled to a unit hypercube, i.e., all dimensions scaled to [0,1]
            population = [self.configspace_to_vector(individual) for individual in population]
        else:
            # if no ConfigSpace representation available, uniformly sample from [0, 1]
            population = np.random.uniform(low=0.0, high=1.0, size=(pop_size, self.dimensions))
        return np.array(population)

    def sample_population(self, size: int = 3, alt_pop: List = None) -> List:
        '''Samples 'size' individuals

        If alt_pop is None or a list/array of None, sample from own population
        Else sample from the specified alternate population (alt_pop)
        '''
        if isinstance(alt_pop, list) or isinstance(alt_pop, np.ndarray):
            idx = [indv is None for indv in alt_pop]
            if any(idx):
                selection = np.random.choice(np.arange(len(self.population)), size, replace=False)
                return self.population[selection]
            else:
                if len(alt_pop) < 3:
                    alt_pop = np.vstack((alt_pop, self.population))
                selection = np.random.choice(np.arange(len(alt_pop)), size, replace=False)
                alt_pop = np.stack(alt_pop)
                return alt_pop[selection]
        else:
            selection = np.random.choice(np.arange(len(self.population)), size, replace=False)
            return self.population[selection]

    def boundary_check(self, vector: np.array) -> np.array:
        '''
        Checks whether each of the dimensions of the input vector are within [0, 1].
        If not, values of those dimensions are replaced with the type of fix selected.

        if fix_type == 'random', the values are replaced with a random sampling from (0,1)
        if fix_type == 'clip', the values are clipped to the closest limit from {0, 1}

        Parameters
        ----------
        vector : array

        Returns
        -------
        array
        '''
        violations = np.where((vector > 1) | (vector < 0))[0]
        if len(violations) == 0:
            return vector
        if self.fix_type == 'random':
            vector[violations] = np.random.uniform(low=0.0, high=1.0, size=len(violations))
        else:
            vector[violations] = np.clip(vector[violations], a_min=0, a_max=1)
        return vector

    def vector_to_configspace(self, vector: np.array) -> ConfigSpace.Configuration:
        '''Converts numpy array to ConfigSpace object

        Works when self.cs is a ConfigSpace object and the input vector is in the domain [0, 1].
        '''
        # creates a ConfigSpace object dict with all hyperparameters present, the inactive too
        new_config = ConfigSpace.util.impute_inactive_values(
            self.cs.sample_configuration()
        ).get_dictionary()
        # iterates over all hyperparameters and normalizes each based on its type
        for i, hyper in enumerate(self.cs.get_hyperparameters()):
            if type(hyper) == ConfigSpace.OrdinalHyperparameter:
                ranges = np.arange(start=0, stop=1, step=1/len(hyper.sequence))
                param_value = hyper.sequence[np.where((vector[i] < ranges) == False)[0][-1]]
            elif type(hyper) == ConfigSpace.CategoricalHyperparameter:
                ranges = np.arange(start=0, stop=1, step=1/len(hyper.choices))
                param_value = hyper.choices[np.where((vector[i] < ranges) == False)[0][-1]]
            else:  # handles UniformFloatHyperparameter & UniformIntegerHyperparameter
                # rescaling continuous values
                if hyper.log:
                    log_range = np.log(hyper.upper) - np.log(hyper.lower)
                    param_value = np.exp(np.log(hyper.lower) + vector[i] * log_range)
                else:
                    param_value = hyper.lower + (hyper.upper - hyper.lower) * vector[i]
                if type(hyper) == ConfigSpace.UniformIntegerHyperparameter:
                    param_value = int(np.round(param_value))  # converting to discrete (int)
                else:
                    param_value = float(param_value)
            new_config[hyper.name] = param_value
        # the mapping from unit hypercube to the actual config space may lead to illegal
        # configurations based on conditions defined, which need to be deactivated/removed
        new_config = ConfigSpace.util.deactivate_inactive_hyperparameters(
            configuration = new_config, configuration_space=self.cs
        )
        return new_config

    def configspace_to_vector(self, config: ConfigSpace.Configuration) -> np.array:
        '''Converts ConfigSpace object to numpy array scaled to [0,1]

        Works when self.cs is a ConfigSpace object and the input config is a ConfigSpace object.
        Handles conditional spaces implicitly by replacing illegal parameters with default values
        to maintain the dimensionality of the vector.
        '''
        # the imputation replaces illegal parameter values with their default
        config = ConfigSpace.util.impute_inactive_values(config)
        dimensions = len(self.cs.get_hyperparameters())
        vector = [np.nan for i in range(dimensions)]
        for name in config:
            i = self.hps[name]
            hyper = self.cs.get_hyperparameter(name)
            if type(hyper) == ConfigSpace.OrdinalHyperparameter:
                nlevels = len(hyper.sequence)
                vector[i] = hyper.sequence.index(config[name]) / nlevels
            elif type(hyper) == ConfigSpace.CategoricalHyperparameter:
                nlevels = len(hyper.choices)
                vector[i] = hyper.choices.index(config[name]) / nlevels
            else:
                bounds = (hyper.lower, hyper.upper)
                param_value = config[name]
                if hyper.log:
                    vector[i] = np.log(param_value / bounds[0]) / np.log(bounds[1] / bounds[0])
                else:
                    vector[i] = (config[name] - bounds[0]) / (bounds[1] - bounds[0])
        return np.array(vector)

    def f_objective(self):
        raise NotImplementedError("The function needs to be defined in the sub class.")

    def mutation(self):
        raise NotImplementedError("The function needs to be defined in the sub class.")

    def crossover(self):
        raise NotImplementedError("The function needs to be defined in the sub class.")

    def evolve(self):
        raise NotImplementedError("The function needs to be defined in the sub class.")

    def run(self):
        raise NotImplementedError("The function needs to be defined in the sub class.")


class DE(DEBase):
    def __init__(self, cs=None, f=None, dimensions=None, pop_size=20, max_age=np.inf,
                 mutation_factor=None, crossover_prob=None, strategy='rand1_bin',
                 budget=None, encoding=False, dim_map=None, **kwargs):
        super().__init__(cs=cs, f=f, dimensions=dimensions, pop_size=pop_size, max_age=max_age,
                         mutation_factor=mutation_factor, crossover_prob=crossover_prob,
                         strategy=strategy, budget=budget, **kwargs)
        if self.strategy is not None:
            self.mutation_strategy = self.strategy.split('_')[0]
            self.crossover_strategy = self.strategy.split('_')[1]
        else:
            self.mutation_strategy = self.crossover_strategy = None
        self.encoding = encoding
        self.dim_map = dim_map
        self._set_min_pop_size()

    def __getstate__(self):
        """ Allows the object to picklable while having Dask client as a class attribute.
        """
        d = dict(self.__dict__)
        d["client"] = None  # hack to allow Dask client to be a class attribute
        d["logger"] = None  # hack to allow logger object to be a class attribute
        return d

    def __del__(self):
        """ Ensures a clean kill of the Dask client and frees up a port.
        """
        if hasattr(self, "client") and isinstance(self, Client):
            self.client.close()

    def reset(self):
        super().reset()
        self.traj = []
        self.runtime = []
        self.history = []

    def _set_min_pop_size(self):
        if self.mutation_strategy in ['rand1', 'rand2dir', 'randtobest1']:
            self._min_pop_size = 3
        elif self.mutation_strategy in ['currenttobest1', 'best1']:
            self._min_pop_size = 2
        elif self.mutation_strategy in ['best2']:
            self._min_pop_size = 4
        elif self.mutation_strategy in ['rand2']:
            self._min_pop_size = 5
        else:
            self._min_pop_size = 1

        return self._min_pop_size

    def map_to_original(self, vector):
        dimensions = len(self.dim_map.keys())
        new_vector = np.random.uniform(size=dimensions)
        for i in range(dimensions):
            new_vector[i] = np.max(np.array(vector)[self.dim_map[i]])
        return new_vector

    def f_objective(self, x, budget=None, **kwargs):
        if self.f is None:
            raise NotImplementedError("An objective function needs to be passed.")
        if self.encoding:
            x = self.map_to_original(x)
        if self.configspace:
            # converts [0, 1] vector to a ConfigSpace object
            config = self.vector_to_configspace(x)
        else:
            # can insert custom scaling/transform function here
            config = x.copy()
        if budget is not None:  # to be used when called by multi-fidelity based optimizers
            res = self.f(config, budget=budget, **kwargs)
        else:
            res = self.f(config, **kwargs)
        assert "fitness" in res
        assert "cost" in res
        return res

    def init_eval_pop(self, budget=None, eval=True, **kwargs):
        '''Creates new population of 'pop_size' and evaluates individuals.
        '''
        self.population = self.init_population(self.pop_size)
        self.fitness = np.array([np.inf for i in range(self.pop_size)])
        self.age = np.array([self.max_age] * self.pop_size)

        traj = []
        runtime = []
        history = []

        if not eval:
            return traj, runtime, history

        for i in range(self.pop_size):
            config = self.population[i]
            res = self.f_objective(config, budget, **kwargs)
            self.fitness[i], cost = res["fitness"], res["cost"]
            info = res["info"] if "info" in res else dict()
            if self.fitness[i] < self.inc_score:
                self.inc_score = self.fitness[i]
                self.inc_config = config
            traj.append(self.inc_score)
            runtime.append(cost)
            history.append((config.tolist(), float(self.fitness[i]), float(budget or 0), info))

        return traj, runtime, history

    def eval_pop(self, population=None, budget=None, **kwargs):
        '''Evaluates a population

        If population=None, the current population's fitness will be evaluated
        If population!=None, this population will be evaluated
        '''
        pop = self.population if population is None else population
        pop_size = self.pop_size if population is None else len(pop)
        traj = []
        runtime = []
        history = []
        fitnesses = []
        costs = []
        ages = []
        for i in range(pop_size):
            res = self.f_objective(pop[i], budget, **kwargs)
            fitness, cost = res["fitness"], res["cost"]
            info = res["info"] if "info" in res else dict()
            if population is None:
                self.fitness[i] = fitness
            if fitness <= self.inc_score:
                self.inc_score = fitness
                self.inc_config = pop[i]
            traj.append(self.inc_score)
            runtime.append(cost)
            history.append((pop[i].tolist(), float(fitness), float(budget or 0), info))
            fitnesses.append(fitness)
            costs.append(cost)
            ages.append(self.max_age)
        if population is None:
            self.fitness = np.array(fitnesses)
            return traj, runtime, history
        else:
            return traj, runtime, history, np.array(fitnesses), np.array(ages)

    def mutation_rand1(self, r1, r2, r3):
        '''Performs the 'rand1' type of DE mutation
        '''
        diff = r2 - r3
        mutant = r1 + self.mutation_factor * diff
        return mutant

    def mutation_rand2(self, r1, r2, r3, r4, r5):
        '''Performs the 'rand2' type of DE mutation
        '''
        diff1 = r2 - r3
        diff2 = r4 - r5
        mutant = r1 + self.mutation_factor * diff1 + self.mutation_factor * diff2
        return mutant

    def mutation_currenttobest1(self, current, best, r1, r2):
        diff1 = best - current
        diff2 = r1 - r2
        mutant = current + self.mutation_factor * diff1 + self.mutation_factor * diff2
        return mutant

    def mutation_rand2dir(self, r1, r2, r3):
        diff = r1 - r2 - r3
        mutant = r1 + self.mutation_factor * diff / 2
        return mutant

    def mutation(self, current=None, best=None, alt_pop=None):
        '''Performs DE mutation
        '''
        if self.mutation_strategy == 'rand1':
            r1, r2, r3 = self.sample_population(size=3, alt_pop=alt_pop)
            mutant = self.mutation_rand1(r1, r2, r3)

        elif self.mutation_strategy == 'rand2':
            r1, r2, r3, r4, r5 = self.sample_population(size=5, alt_pop=alt_pop)
            mutant = self.mutation_rand2(r1, r2, r3, r4, r5)

        elif self.mutation_strategy == 'rand2dir':
            r1, r2, r3 = self.sample_population(size=3, alt_pop=alt_pop)
            mutant = self.mutation_rand2dir(r1, r2, r3)

        elif self.mutation_strategy == 'best1':
            r1, r2 = self.sample_population(size=2, alt_pop=alt_pop)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_rand1(best, r1, r2)

        elif self.mutation_strategy == 'best2':
            r1, r2, r3, r4 = self.sample_population(size=4, alt_pop=alt_pop)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_rand2(best, r1, r2, r3, r4)

        elif self.mutation_strategy == 'currenttobest1':
            r1, r2 = self.sample_population(size=2, alt_pop=alt_pop)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_currenttobest1(current, best, r1, r2)

        elif self.mutation_strategy == 'randtobest1':
            r1, r2, r3 = self.sample_population(size=3, alt_pop=alt_pop)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_currenttobest1(r1, best, r2, r3)

        return mutant

    def crossover_bin(self, target, mutant):
        '''Performs the binomial crossover of DE
        '''
        cross_points = np.random.rand(self.dimensions) < self.crossover_prob
        if not np.any(cross_points):
            cross_points[np.random.randint(0, self.dimensions)] = True
        offspring = np.where(cross_points, mutant, target)
        return offspring

    def crossover_exp(self, target, mutant):
        '''Performs the exponential crossover of DE
        '''
        n = np.random.randint(0, self.dimensions)
        L = 0
        while ((np.random.rand() < self.crossover_prob) and L < self.dimensions):
            idx = (n+L) % self.dimensions
            target[idx] = mutant[idx]
            L = L + 1
        return target

    def crossover(self, target, mutant):
        '''Performs DE crossover
        '''
        if self.crossover_strategy == 'bin':
            offspring = self.crossover_bin(target, mutant)
        elif self.crossover_strategy == 'exp':
            offspring = self.crossover_exp(target, mutant)
        return offspring

    def selection(self, trials, budget=None, **kwargs):
        '''Carries out a parent-offspring competition given a set of trial population
        '''
        traj = []
        runtime = []
        history = []
        for i in range(len(trials)):
            # evaluation of the newly created individuals
            res = self.f_objective(trials[i], budget, **kwargs)
            fitness, cost = res["fitness"], res["cost"]
            info = res["info"] if "info" in res else dict()
            # selection -- competition between parent[i] -- child[i]
            ## equality is important for landscape exploration
            if fitness <= self.fitness[i]:
                self.population[i] = trials[i]
                self.fitness[i] = fitness
                # resetting age since new individual in the population
                self.age[i] = self.max_age
            else:
                # decreasing age by 1 of parent who is better than offspring/trial
                self.age[i] -= 1
            # updation of global incumbent for trajectory
            if self.fitness[i] < self.inc_score:
                self.inc_score = self.fitness[i]
                self.inc_config = self.population[i]
            traj.append(self.inc_score)
            runtime.append(cost)
            history.append((trials[i].tolist(), float(fitness), float(budget or 0), info))
        return traj, runtime, history

    def evolve_generation(self, budget=None, best=None, alt_pop=None, **kwargs):
        '''Performs a complete DE evolution: mutation -> crossover -> selection
        '''
        trials = []
        for j in range(self.pop_size):
            target = self.population[j]
            donor = self.mutation(current=target, best=best, alt_pop=alt_pop)
            trial = self.crossover(target, donor)
            trial = self.boundary_check(trial)
            trials.append(trial)
        trials = np.array(trials)
        traj, runtime, history = self.selection(trials, budget, **kwargs)
        return traj, runtime, history

    def sample_mutants(self, size, population=None):
        '''Generates 'size' mutants from the population using rand1
        '''
        if population is None:
            population = self.population
        elif len(population) < 3:
            population = np.vstack((self.population, population))

        old_strategy = self.mutation_strategy
        self.mutation_strategy = 'rand1'
        mutants = np.random.uniform(low=0.0, high=1.0, size=(size, self.dimensions))
        for i in range(size):
            mutant = self.mutation(current=None, best=None, alt_pop=population)
            mutants[i] = self.boundary_check(mutant)
        self.mutation_strategy = old_strategy

        return mutants

    def run(self, generations=1, verbose=False, budget=None, reset=True, **kwargs):
        # checking if a run exists
        if not hasattr(self, 'traj') or reset:
            self.reset()
            if verbose:
                print("Initializing and evaluating new population...")
            self.traj, self.runtime, self.history = self.init_eval_pop(budget=budget, **kwargs)

        if verbose:
            print("Running evolutionary search...")
        for i in range(generations):
            if verbose:
                print("Generation {:<2}/{:<2} -- {:<0.7}".format(i+1, generations, self.inc_score))
            traj, runtime, history = self.evolve_generation(budget=budget, **kwargs)
            self.traj.extend(traj)
            self.runtime.extend(runtime)
            self.history.extend(history)

        if verbose:
            print("\nRun complete!")

        return np.array(self.traj), np.array(self.runtime), np.array(self.history)


class AsyncDE(DE):
    def __init__(self, cs=None, f=None, dimensions=None, pop_size=None, max_age=np.inf,
                 mutation_factor=None, crossover_prob=None, strategy='rand1_bin',
                 budget=None, async_strategy='deferred', **kwargs):
        '''Extends DE to be Asynchronous with variations

        Parameters
        ----------
        async_strategy : str
            'deferred' - target will be chosen sequentially from the population
                the winner of the selection step will be included in the population only after
                the entire population has had a selection step in that generation
            'immediate' - target will be chosen sequentially from the population
                the winner of the selection step is included in the population right away
            'random' - target will be chosen randomly from the population for mutation-crossover
                the winner of the selection step is included in the population right away
            'worst' - the worst individual will be chosen as the target
                the winner of the selection step is included in the population right away
            {immediate, worst, random} implement Asynchronous-DE
        '''
        super().__init__(cs=cs, f=f, dimensions=dimensions, pop_size=pop_size, max_age=max_age,
                         mutation_factor=mutation_factor, crossover_prob=crossover_prob,
                         strategy=strategy, budget=budget, **kwargs)
        if self.strategy is not None:
            self.mutation_strategy = self.strategy.split('_')[0]
            self.crossover_strategy = self.strategy.split('_')[1]
        else:
            self.mutation_strategy = self.crossover_strategy = None
        self.async_strategy = async_strategy
        assert self.async_strategy in ['immediate', 'random', 'worst', 'deferred'], \
                "{} is not a valid choice for type of DE".format(self.async_strategy)

    def _add_random_population(self, pop_size, population=None, fitness=[], age=[]):
        '''Adds random individuals to the population
        '''
        new_pop = self.init_population(pop_size=pop_size)
        new_fitness = np.array([np.inf] * pop_size)
        new_age = np.array([self.max_age] * pop_size)

        if population is None:
            population = self.population
            fitness = self.fitness
            age = self.age

        population = np.concatenate((population, new_pop))
        fitness = np.concatenate((fitness, new_fitness))
        age = np.concatenate((age, new_age))

        return population, fitness, age

    def _init_mutant_population(self, pop_size, population, target=None, best=None):
        '''Generates pop_size mutants from the passed population
        '''
        mutants = np.random.uniform(low=0.0, high=1.0, size=(pop_size, self.dimensions))
        for i in range(pop_size):
            mutants[i] = self.mutation(current=target, best=best, alt_pop=population)
        return mutants

    def _sample_population(self, size=3, alt_pop=None, target=None):
        '''Samples 'size' individuals for mutation step

        If alt_pop is None or a list/array of None, sample from own population
        Else sample from the specified alternate population
        '''
        population = None
        if isinstance(alt_pop, list) or isinstance(alt_pop, np.ndarray):
            idx = [indv is None for indv in alt_pop]  # checks if all individuals are valid
            if any(idx):
                # default to the object's initialized population
                population = self.population
            else:
                # choose the passed population
                population = alt_pop
        else:
            # default to the object's initialized population
            population = self.population

        if target is not None and len(population) > 1:
            # eliminating target from mutation sampling pool
            # the target individual should not be a part of the candidates for mutation
            for i, pop in enumerate(population):
                if all(target == pop):
                    population = np.concatenate((population[:i], population[i + 1:]))
                    break
        if len(population) < self._min_pop_size:
            # compensate if target was part of the population and deleted earlier
            filler = self._min_pop_size - len(population)
            new_pop = self.init_population(pop_size=filler)  # chosen in a uniformly random manner
            population = np.concatenate((population, new_pop))

        selection = np.random.choice(np.arange(len(population)), size, replace=False)
        return population[selection]

    def eval_pop(self, population=None, budget=None, **kwargs):
        pop = self.population if population is None else population
        pop_size = self.pop_size if population is None else len(pop)
        traj = []
        runtime = []
        history = []
        fitnesses = []
        costs = []
        ages = []
        for i in range(pop_size):
            res = self.f_objective(pop[i], budget, **kwargs)
            fitness, cost = res["fitness"], res["cost"]
            info = res["info"] if "info" in res else dict()
            if population is None:
                self.fitness[i] = fitness
            if fitness <= self.inc_score:
                self.inc_score = fitness
                self.inc_config = pop[i]
            traj.append(self.inc_score)
            runtime.append(cost)
            history.append((pop[i].tolist(), float(fitness), float(budget or 0), info))
            fitnesses.append(fitness)
            costs.append(cost)
            ages.append(self.max_age)
        return traj, runtime, history, np.array(fitnesses), np.array(ages)

    def mutation(self, current=None, best=None, alt_pop=None):
        '''Performs DE mutation
        '''
        if self.mutation_strategy == 'rand1':
            r1, r2, r3 = self._sample_population(size=3, alt_pop=alt_pop, target=current)
            mutant = self.mutation_rand1(r1, r2, r3)

        elif self.mutation_strategy == 'rand2':
            r1, r2, r3, r4, r5 = self._sample_population(size=5, alt_pop=alt_pop, target=current)
            mutant = self.mutation_rand2(r1, r2, r3, r4, r5)

        elif self.mutation_strategy == 'rand2dir':
            r1, r2, r3 = self._sample_population(size=3, alt_pop=alt_pop, target=current)
            mutant = self.mutation_rand2dir(r1, r2, r3)

        elif self.mutation_strategy == 'best1':
            r1, r2 = self._sample_population(size=2, alt_pop=alt_pop, target=current)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_rand1(best, r1, r2)

        elif self.mutation_strategy == 'best2':
            r1, r2, r3, r4 = self._sample_population(size=4, alt_pop=alt_pop, target=current)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_rand2(best, r1, r2, r3, r4)

        elif self.mutation_strategy == 'currenttobest1':
            r1, r2 = self._sample_population(size=2, alt_pop=alt_pop, target=current)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_currenttobest1(current, best, r1, r2)

        elif self.mutation_strategy == 'randtobest1':
            r1, r2, r3 = self._sample_population(size=3, alt_pop=alt_pop, target=current)
            if best is None:
                best = self.population[np.argmin(self.fitness)]
            mutant = self.mutation_currenttobest1(r1, best, r2, r3)

        return mutant

    def sample_mutants(self, size, population=None):
        '''Samples 'size' mutants from the population
        '''
        if population is None:
            population = self.population

        mutants = np.random.uniform(low=0.0, high=1.0, size=(size, self.dimensions))
        for i in range(size):
            j = np.random.choice(np.arange(len(population)))
            mutant = self.mutation(current=population[j], best=self.inc_config, alt_pop=population)
            mutants[i] = self.boundary_check(mutant)

        return mutants

    def evolve_generation(self, budget=None, best=None, alt_pop=None, **kwargs):
        '''Performs a complete DE evolution, mutation -> crossover -> selection
        '''
        traj = []
        runtime = []
        history = []

        if self.async_strategy == 'deferred':
            trials = []
            for j in range(self.pop_size):
                target = self.population[j]
                donor = self.mutation(current=target, best=best, alt_pop=alt_pop)
                trial = self.crossover(target, donor)
                trial = self.boundary_check(trial)
                trials.append(trial)
            # selection takes place on a separate trial population only after
            # one iteration through the population has taken place
            trials = np.array(trials)
            traj, runtime, history = self.selection(trials, budget, **kwargs)
            return traj, runtime, history

        elif self.async_strategy == 'immediate':
            for i in range(self.pop_size):
                target = self.population[i]
                donor = self.mutation(current=target, best=best, alt_pop=alt_pop)
                trial = self.crossover(target, donor)
                trial = self.boundary_check(trial)
                # evaluating a single trial population for the i-th individual
                de_traj, de_runtime, de_history, fitnesses, costs = \
                    self.eval_pop(trial.reshape(1, self.dimensions), budget=budget, **kwargs)
                # one-vs-one selection
                ## can replace the i-the population despite not completing one iteration
                if fitnesses[0] <= self.fitness[i]:
                    self.population[i] = trial
                    self.fitness[i] = fitnesses[0]
                traj.extend(de_traj)
                runtime.extend(de_runtime)
                history.extend(de_history)
            return traj, runtime, history

        else:  # async_strategy == 'random' or async_strategy == 'worst':
            for count in range(self.pop_size):
                # choosing target individual
                if self.async_strategy == 'random':
                    i = np.random.choice(np.arange(self.pop_size))
                else:  # async_strategy == 'worst'
                    i = np.argsort(-self.fitness)[0]
                target = self.population[i]
                mutant = self.mutation(current=target, best=best, alt_pop=alt_pop)
                trial = self.crossover(target, mutant)
                trial = self.boundary_check(trial)
                # evaluating a single trial population for the i-th individual
                de_traj, de_runtime, de_history, fitnesses, costs = \
                    self.eval_pop(trial.reshape(1, self.dimensions), budget=budget, **kwargs)
                # one-vs-one selection
                ## can replace the i-the population despite not completing one iteration
                if fitnesses[0] <= self.fitness[i]:
                    self.population[i] = trial
                    self.fitness[i] = fitnesses[0]
                traj.extend(de_traj)
                runtime.extend(de_runtime)
                history.extend(de_history)

        return traj, runtime, history

    def run(self, generations=1, verbose=False, budget=None, reset=True, **kwargs):
        # checking if a run exists
        if not hasattr(self, 'traj') or reset:
            self.reset()
            if verbose:
                print("Initializing and evaluating new population...")
            self.traj, self.runtime, self.history = self.init_eval_pop(budget=budget, **kwargs)

        if verbose:
            print("Running evolutionary search...")
        for i in range(generations):
            if verbose:
                print("Generation {:<2}/{:<2} -- {:<0.7}".format(i+1, generations, self.inc_score))
            traj, runtime, history = self.evolve_generation(
                budget=budget, best=self.inc_config, **kwargs
            )
            self.traj.extend(traj)
            self.runtime.extend(runtime)
            self.history.extend(history)

        if verbose:
            print("\nRun complete!")

        return (np.array(self.traj), np.array(self.runtime), np.array(self.history))

File Path: surrogate_models/dehb/dehb/optimizers/dehb.py
Content:
import os
import pickle
import sys
import json
import time
import numpy as np
import ConfigSpace
from typing import List, Tuple
from copy import deepcopy
from loguru import logger
from distributed import Client

from surrogate_models.dehb.dehb.optimizers import DE, AsyncDE
from surrogate_models.dehb.dehb.utils import SHBracketManager


logger.configure(handlers=[{"sink": sys.stdout, "level": "INFO"}])
_logger_props = {
    "format": "{time} {level} {message}",
    "enqueue": True,
    "rotation": "500 MB"
}


class DEHBBase:
    def __init__(self, cs=None, f=None, dimensions=None, mutation_factor=None,
                 crossover_prob=None, strategy=None, min_budget=None,
                 max_budget=None, eta=None, min_clip=None, max_clip=None,
                 boundary_fix_type='random', max_age=np.inf, **kwargs):
        # Benchmark related variables
        self.cs = cs
        self.configspace = True if isinstance(self.cs, ConfigSpace.ConfigurationSpace) else False
        if self.configspace:
            self.dimensions = len(self.cs.get_hyperparameters())
        elif dimensions is None or not isinstance(dimensions, (int, np.int)):
            assert "Need to specify `dimensions` as an int when `cs` is not available/specified!"
        else:
            self.dimensions = dimensions
        self.f = f

        # DE related variables
        self.mutation_factor = mutation_factor
        self.crossover_prob = crossover_prob
        self.strategy = strategy
        self.fix_type = boundary_fix_type
        self.max_age = max_age
        self.de_params = {
            "mutation_factor": self.mutation_factor,
            "crossover_prob": self.crossover_prob,
            "strategy": self.strategy,
            "configspace": self.configspace,
            "boundary_fix_type": self.fix_type,
            "max_age": self.max_age,
            "cs": self.cs,
            "dimensions": self.dimensions,
            "f": f
        }

        # Hyperband related variables
        self.min_budget = min_budget
        self.max_budget = max_budget
        assert self.max_budget > self.min_budget, "only (Max Budget > Min Budget) supported!"
        self.eta = eta
        self.min_clip = min_clip
        self.max_clip = max_clip

        # Precomputing budget spacing and number of configurations for HB iterations
        self.max_SH_iter = None
        self.budgets = None
        if self.min_budget is not None and \
           self.max_budget is not None and \
           self.eta is not None:
            self.max_SH_iter = -int(np.log(self.min_budget / self.max_budget) / np.log(self.eta)) + 1
            self.budgets = self.max_budget * np.power(self.eta,
                                                     -np.linspace(start=self.max_SH_iter - 1,
                                                                  stop=0, num=self.max_SH_iter))

        # Miscellaneous
        self.output_path = kwargs['output_path'] if 'output_path' in kwargs else './'
        os.makedirs(self.output_path, exist_ok=True)
        self.logger = logger
        log_suffix = time.strftime("%x %X %Z")
        log_suffix = log_suffix.replace("/", '-').replace(":", '-').replace(" ", '_')
        self.logger.add(
            "{}/dehb_{}.log".format(self.output_path, log_suffix),
            **_logger_props
        )
        self.log_filename = "{}/dehb_{}.log".format(self.output_path, log_suffix)
        # Updating DE parameter list
        self.de_params.update({"output_path": self.output_path})

        # Global trackers
        self.population = None
        self.fitness = None
        self.inc_score = np.inf
        self.inc_config = None
        self.history = []

        # hyperparameters introduced for DECIM
        self.hp_configurations = kwargs['hyperparameter_canditates'] if 'hyperparameter_canditates' in kwargs else None
        self.random_state = kwargs['random_state'] if 'random_state' in kwargs else None

        self.param_space = kwargs['param_space']

    def reset(self):
        self.inc_score = np.inf
        self.inc_config = None
        self.population = None
        self.fitness = None
        self.traj = []
        self.runtime = []
        self.history = []
        self.logger.info("\n\nRESET at {}\n\n".format(time.strftime("%x %X %Z")))

    def init_population(self):
        raise NotImplementedError("Redefine!")

    def get_next_iteration(self, iteration):
        '''Computes the Successive Halving spacing

        Given the iteration index, computes the budget spacing to be used and
        the number of configurations to be used for the SH iterations.

        Parameters
        ----------
        iteration : int
            Iteration index
        clip : int, {1, 2, 3, ..., None}
            If not None, clips the minimum number of configurations to 'clip'

        Returns
        -------
        ns : array
        budgets : array
        '''
        # number of 'SH runs'
        s = self.max_SH_iter - 1 - (iteration % self.max_SH_iter)
        # budget spacing for this iteration
        budgets = self.budgets[(-s-1):]
        # number of configurations in that bracket
        n0 = int(np.floor((self.max_SH_iter)/(s+1)) * self.eta**s)
        ns = [max(int(n0*(self.eta**(-i))), 1) for i in range(s+1)]
        if self.min_clip is not None and self.max_clip is not None:
            ns = np.clip(ns, a_min=self.min_clip, a_max=self.max_clip)
        elif self.min_clip is not None:
            ns = np.clip(ns, a_min=self.min_clip, a_max=np.max(ns))

        return ns, budgets

    def get_incumbents(self):
        """ Returns a tuple of the (incumbent configuration, incumbent score/fitness). """
        if self.configspace:
            return self.vector_to_configspace(self.inc_config), self.inc_score
        else:
            config_index = self.map_closest_evaluated(self.inc_config)
            config_values = self.hp_configurations[config_index]
            config_dict = dict()
            for hp_index, hp_name in enumerate(self.param_space):
                config_dict[hp_name] = config_values[hp_index]
            return config_dict, self.inc_score

    def f_objective(self):
        raise NotImplementedError("The function needs to be defined in the sub class.")

    def run(self):
        raise NotImplementedError("The function needs to be defined in the sub class.")


class DEHB(DEHBBase):
    def __init__(self, cs=None, f=None, dimensions=None, mutation_factor=0.5,
                 crossover_prob=0.5, strategy='rand1_bin', min_budget=None,
                 max_budget=None, eta=3, min_clip=None, max_clip=None, configspace=True,
                 boundary_fix_type='random', max_age=np.inf, n_workers=None, client=None, **kwargs):
        super().__init__(cs=cs, f=f, dimensions=dimensions, mutation_factor=mutation_factor,
                         crossover_prob=crossover_prob, strategy=strategy, min_budget=min_budget,
                         max_budget=max_budget, eta=eta, min_clip=min_clip, max_clip=max_clip,
                         configspace=configspace, boundary_fix_type=boundary_fix_type,
                         max_age=max_age, **kwargs)
        self.iteration_counter = -1
        self.de = {}
        self._max_pop_size = None
        self.active_brackets = []  # list of SHBracketManager objects
        self.traj = []
        self.runtime = []
        self.history = []
        self.start = None

        # Dask variables
        if n_workers is None and client is None:
            raise ValueError("Need to specify either 'n_workers'(>0) or 'client' (a Dask client)!")
        if client is not None and isinstance(client, Client):
            self.client = client
            self.n_workers = len(client.ncores())
        else:
            self.n_workers = n_workers
            if self.n_workers > 1:
                self.client = Client(
                    n_workers=self.n_workers, processes=True, threads_per_worker=1, scheduler_port=0
                )  # port 0 makes Dask select a random free port
            else:
                self.client = None
        self.futures = []
        self.shared_data = None

        # Initializing DE subpopulations
        self._get_pop_sizes()
        self._init_subpop()

        # Misc.
        self.available_gpus = None
        self.gpu_usage = None
        self.single_node_with_gpus = None

    def __getstate__(self):
        """ Allows the object to picklable while having Dask client as a class attribute.
        """
        d = dict(self.__dict__)
        d["client"] = None  # hack to allow Dask client to be a class attribute
        d["logger"] = None  # hack to allow logger object to be a class attribute
        return d

    def __del__(self):
        """ Ensures a clean kill of the Dask client and frees up a port.
        """
        if hasattr(self, "client") and isinstance(self, Client):
            self.client.close()

    def _f_objective(self, job_info):
        """ Wrapper to call DE's objective function.
        """
        # check if job_info appended during job submission self.submit_job() includes "gpu_devices"
        if "gpu_devices" in job_info and self.single_node_with_gpus:
            # should set the environment variable for the spawned worker process
            # reprioritising a CUDA device order specific to this worker process
            os.environ.update({"CUDA_VISIBLE_DEVICES": job_info["gpu_devices"]})

        config, budget, parent_id = job_info['config'], job_info['budget'], job_info['parent_id']
        bracket_id = job_info['bracket_id']
        kwargs = job_info["kwargs"]
        res = self.de[budget].f_objective(config, budget, **kwargs)
        info = res["info"] if "info" in res else dict()
        run_info = {
            'fitness': res["fitness"],
            'cost': res["cost"],
            'config': config,
            'budget': budget,
            'parent_id': parent_id,
            'bracket_id': bracket_id,
            'info': info
        }

        if "gpu_devices" in job_info:
            # important for GPU usage tracking if single_node_with_gpus=True
            device_id = int(job_info["gpu_devices"].strip().split(",")[0])
            run_info.update({"device_id": device_id})
        return run_info

    def _create_cuda_visible_devices(self, available_gpus: List[int], start_id: int) -> str:
        """ Generates a string to set the CUDA_VISIBLE_DEVICES environment variable.

        Given a list of available GPU device IDs and a preferred ID (start_id), the environment
        variable is created by putting the start_id device first, followed by the remaining devices
        arranged randomly. The worker that uses this string to set the environment variable uses
        the start_id GPU device primarily now.
        """
        assert start_id in available_gpus
        available_gpus = deepcopy(available_gpus)
        available_gpus.remove(start_id)
        np.random.shuffle(available_gpus)
        final_variable = [str(start_id)] + [str(_id) for _id in available_gpus]
        final_variable = ",".join(final_variable)
        return final_variable

    def distribute_gpus(self):
        """ Function to create a GPU usage tracker dict.

        The idea is to extract the exact GPU device IDs available. During job submission, each
        submitted job is given a preference of a GPU device ID based on the GPU device with the
        least number of active running jobs. On retrieval of the result, this gpu usage dict is
        updated for the device ID that the finished job was mapped to.
        """
        try:
            available_gpus = os.environ["CUDA_VISIBLE_DEVICES"]
            available_gpus = available_gpus.strip().split(",")
            self.available_gpus = [int(_id) for _id in available_gpus]
        except KeyError as e:
            print("Unable to find valid GPU devices. "
                  "Environment variable {} not visible!".format(str(e)))
            self.available_gpus = []
        self.gpu_usage = dict()
        for _id in self.available_gpus:
            self.gpu_usage[_id] = 0

    def vector_to_configspace(self, config):
        assert hasattr(self, "de")
        assert len(self.budgets) > 0
        return self.de[self.budgets[0]].vector_to_configspace(config)

    def configspace_to_vector(self, config):
        assert hasattr(self, "de")
        assert len(self.budgets) > 0
        return self.de[self.budgets[0]].configspace_to_vector(config)

    def reset(self):
        super().reset()
        if self.n_workers > 1 and hasattr(self, "client") and isinstance(self.client, Client):
            self.client.restart()
        else:
            self.client = None
        self.futures = []
        self.shared_data = None
        self.iteration_counter = -1
        self.de = {}
        self._max_pop_size = None
        self.start = None
        self.active_brackets = []
        self.traj = []
        self.runtime = []
        self.history = []
        self._get_pop_sizes()
        self._init_subpop()
        self.available_gpus = None
        self.gpu_usage = None

    def init_population(self, pop_size):
        if self.configspace:
            population = self.cs.sample_configuration(size=pop_size)
            population = [self.configspace_to_vector(individual) for individual in population]
        else:
            population = np.random.uniform(low=0.0, high=1.0, size=(pop_size, self.dimensions))
        return population

    def clean_inactive_brackets(self):
        """ Removes brackets from the active list if it is done as communicated by Bracket Manager
        """
        if len(self.active_brackets) == 0:
            return
        self.active_brackets = [
            bracket for bracket in self.active_brackets if ~bracket.is_bracket_done()
        ]
        return

    def _update_trackers(self, traj, runtime, history):
        self.traj.append(traj)
        self.runtime.append(runtime)
        self.history.append(history)

    def _update_incumbents(self, config, score, info):
        self.inc_config = config
        self.inc_score = score
        self.inc_info = info

    def _get_pop_sizes(self):
        """Determines maximum pop size for each budget
        """
        self._max_pop_size = {}
        for i in range(self.max_SH_iter):
            n, r = self.get_next_iteration(i)
            for j, r_j in enumerate(r):
                self._max_pop_size[r_j] = max(
                    n[j], self._max_pop_size[r_j]
                ) if r_j in self._max_pop_size.keys() else n[j]

    def _init_subpop(self):
        """ List of DE objects corresponding to the budgets (fidelities)
        """
        self.de = {}
        for i, b in enumerate(self._max_pop_size.keys()):
            self.de[b] = AsyncDE(**self.de_params, budget=b, pop_size=self._max_pop_size[b])
            self.de[b].population = self.de[b].init_population(pop_size=self._max_pop_size[b])
            self.de[b].fitness = np.array([np.inf] * self._max_pop_size[b])
            # adding attributes to DEHB objects to allow communication across subpopulations
            self.de[b].parent_counter = 0
            self.de[b].promotion_pop = None
            self.de[b].promotion_fitness = None

    def _concat_pops(self, exclude_budget=None):
        """ Concatenates all subpopulations
        """
        budgets = list(self.budgets)
        if exclude_budget is not None:
            budgets.remove(exclude_budget)
        pop = []
        for budget in budgets:
            pop.extend(self.de[budget].population.tolist())
        return np.array(pop)

    def _start_new_bracket(self):
        """ Starts a new bracket based on Hyperband
        """
        # start new bracket
        self.iteration_counter += 1  # iteration counter gives the bracket count or bracket ID
        n_configs, budgets = self.get_next_iteration(self.iteration_counter)
        bracket = SHBracketManager(
            n_configs=n_configs, budgets=budgets, bracket_id=self.iteration_counter
        )
        self.active_brackets.append(bracket)
        return bracket

    def _get_worker_count(self):
        if isinstance(self.client, Client):
            return len(self.client.ncores())
        else:
            return 1

    def is_worker_available(self, verbose=False):
        """ Checks if at least one worker is available to run a job
        """
        if self.n_workers == 1 or self.client is None or not isinstance(self.client, Client):
            # in the synchronous case, one worker is always available
            return True
        # checks the absolute number of workers mapped to the client scheduler
        # client.ncores() should return a dict with the keys as unique addresses to these workers
        # treating the number of available workers in this manner
        workers = self._get_worker_count()  # len(self.client.ncores())
        if len(self.futures) >= workers:
            # pause/wait if active worker count greater allocated workers
            return False
        return True

    def _get_promotion_candidate(self, low_budget, high_budget, n_configs):
        """ Manages the population to be promoted from the lower to the higher budget.

        This is triggered or in action only during the first full HB bracket, which is equivalent
        to the number of brackets <= max_SH_iter.
        """
        # finding the individuals that have been evaluated (fitness < np.inf)
        evaluated_configs = np.where(self.de[low_budget].fitness != np.inf)[0]
        promotion_candidate_pop = self.de[low_budget].population[evaluated_configs]
        promotion_candidate_fitness = self.de[low_budget].fitness[evaluated_configs]
        # ordering the evaluated individuals based on their fitness values
        pop_idx = np.argsort(promotion_candidate_fitness)

        # creating population for promotion if none promoted yet or nothing to promote
        if self.de[high_budget].promotion_pop is None or \
                len(self.de[high_budget].promotion_pop) == 0:
            self.de[high_budget].promotion_pop = np.empty((0, self.dimensions))
            self.de[high_budget].promotion_fitness = np.array([])

            # iterating over the evaluated individuals from the lower budget and including them
            # in the promotion population for the higher budget only if it's not in the population
            # this is done to ensure diversity of population and avoid redundant evaluations
            for idx in pop_idx:
                individual = promotion_candidate_pop[idx]
                # checks if the candidate individual already exists in the high budget population
                if np.any(np.all(individual == self.de[high_budget].population, axis=1)):
                    # skipping already present individual to allow diversity and reduce redundancy
                    continue
                self.de[high_budget].promotion_pop = np.append(
                    self.de[high_budget].promotion_pop, [individual], axis=0
                )
                self.de[high_budget].promotion_fitness = np.append(
                    self.de[high_budget].promotion_pop, promotion_candidate_fitness[pop_idx]
                )
            # retaining only n_configs
            self.de[high_budget].promotion_pop = self.de[high_budget].promotion_pop[:n_configs]
            self.de[high_budget].promotion_fitness = \
                self.de[high_budget].promotion_fitness[:n_configs]

        if len(self.de[high_budget].promotion_pop) > 0:
            config = self.de[high_budget].promotion_pop[0]
            # removing selected configuration from population
            self.de[high_budget].promotion_pop = self.de[high_budget].promotion_pop[1:]
            self.de[high_budget].promotion_fitness = self.de[high_budget].promotion_fitness[1:]
        else:
            # in case of an edge failure case where all high budget individuals are same
            # just choose the best performing individual from the lower budget (again)
            config = self.de[low_budget].population[pop_idx[0]]
        return config

    def _get_next_parent_for_subpop(self, budget):
        """ Maintains a looping counter over a subpopulation, to iteratively select a parent
        """
        parent_id = self.de[budget].parent_counter
        self.de[budget].parent_counter += 1
        self.de[budget].parent_counter = self.de[budget].parent_counter % self._max_pop_size[budget]
        return parent_id

    def _acquire_config(self, bracket, budget):
        """ Generates/chooses a configuration based on the budget and iteration number
        """
        # select a parent/target
        parent_id = self._get_next_parent_for_subpop(budget)
        target = self.de[budget].population[parent_id]
        # identify lower budget/fidelity to transfer information from
        lower_budget, num_configs = bracket.get_lower_budget_promotions(budget)

        if self.iteration_counter < self.max_SH_iter:
            # promotions occur only in the first set of SH brackets under Hyperband
            # for the first rung/budget in the current bracket, no promotion is possible and
            # evolution can begin straight away
            # for the subsequent rungs, individuals will be promoted from the lower_budget
            if budget != bracket.budgets[0]:
                # TODO: check if generalizes to all budget spacings
                config = self._get_promotion_candidate(lower_budget, budget, num_configs)
                return config, parent_id

        # DE evolution occurs when either all individuals in the subpopulation have been evaluated
        # at least once, i.e., has fitness < np.inf, which can happen if
        # iteration_counter <= max_SH_iter but certainly never when iteration_counter > max_SH_iter

        # a single DE evolution --- (mutation + crossover) occurs here
        mutation_pop_idx = np.argsort(self.de[lower_budget].fitness)[:num_configs]
        mutation_pop = self.de[lower_budget].population[mutation_pop_idx]
        # generate mutants from previous budget subpopulation or global population
        if len(mutation_pop) < self.de[budget]._min_pop_size:
            filler = self.de[budget]._min_pop_size - len(mutation_pop) + 1
            new_pop = self.de[budget]._init_mutant_population(
                pop_size=filler, population=self._concat_pops(),
                target=None, best=self.inc_config
            )
            mutation_pop = np.concatenate((mutation_pop, new_pop))
        # generate mutant from among individuals in mutation_pop
        mutant = self.de[budget].mutation(
            current=target, best=self.inc_config, alt_pop=mutation_pop
        )
        # perform crossover with selected parent
        config = self.de[budget].crossover(target=target, mutant=mutant)
        config = self.de[budget].boundary_check(config)
        return config, parent_id

    def _get_next_job(self):
        """ Loads a configuration and budget to be evaluated next by a free worker
        """
        bracket = None
        if len(self.active_brackets) == 0 or \
                np.all([bracket.is_bracket_done() for bracket in self.active_brackets]):
            # start new bracket when no pending jobs from existing brackets or empty bracket list
            bracket = self._start_new_bracket()
        else:
            for _bracket in self.active_brackets:
                # check if _bracket is not waiting for previous rung results of same bracket
                # _bracket is not waiting on the last rung results
                # these 2 checks allow DEHB to have a "synchronous" Successive Halving
                if not _bracket.previous_rung_waits() and _bracket.is_pending():
                    # bracket eligible for job scheduling
                    bracket = _bracket
                    break
            if bracket is None:
                # start new bracket when existing list has all waiting brackets
                bracket = self._start_new_bracket()
        # budget that the SH bracket allots
        budget = bracket.get_next_job_budget()
        config, parent_id = self._acquire_config(bracket, budget)
        # notifies the Bracket Manager that a single config is to run for the budget chosen
        job_info = {
            "config": config,
            "budget": budget,
            "parent_id": parent_id,
            "bracket_id": bracket.bracket_id
        }
        return job_info

    def _get_gpu_id_with_low_load(self):
        candidates = []
        for k, v in self.gpu_usage.items():
            if v == min(self.gpu_usage.values()):
                candidates.append(k)
        device_id = np.random.choice(candidates)
        # creating string for setting environment variable CUDA_VISIBLE_DEVICES
        gpu_ids = self._create_cuda_visible_devices(
            self.available_gpus, device_id
        )
        # updating GPU usage
        self.gpu_usage[device_id] += 1
        self.logger.debug("GPU device selected: {}".format(device_id))
        self.logger.debug("GPU device usage: {}".format(self.gpu_usage))
        return gpu_ids

    def submit_job(self, job_info, **kwargs):
        """ Asks a free worker to run the objective function on config and budget
        """
        job_info["kwargs"] = self.shared_data if self.shared_data is not None else kwargs
        # submit to to Dask client
        if self.n_workers > 1 or isinstance(self.client, Client):
            if self.single_node_with_gpus:
                # managing GPU allocation for the job to be submitted
                job_info.update({"gpu_devices": self._get_gpu_id_with_low_load()})
            self.futures.append(
                self.client.submit(self._f_objective, job_info)
            )
        else:
            # skipping scheduling to Dask worker to avoid added overheads in the synchronous case
            self.futures.append(self._f_objective(job_info))

        # pass information of job submission to Bracket Manager
        for bracket in self.active_brackets:
            if bracket.bracket_id == job_info['bracket_id']:
                # registering is IMPORTANT for Bracket Manager to perform SH
                bracket.register_job(job_info['budget'])
                break

    def _fetch_results_from_workers(self):
        """ Iterate over futures and collect results from finished workers
        """
        if self.n_workers > 1 or isinstance(self.client, Client):
            done_list = [(i, future) for i, future in enumerate(self.futures) if future.done()]
        else:
            # Dask not invoked in the synchronous case
            done_list = [(i, future) for i, future in enumerate(self.futures)]
        if len(done_list) > 0:
            self.logger.debug(
                "Collecting {} of the {} job(s) active.".format(len(done_list), len(self.futures))
            )
        for _, future in done_list:
            if self.n_workers > 1 or isinstance(self.client, Client):
                run_info = future.result()
                if "device_id" in run_info:
                    # updating GPU usage
                    self.gpu_usage[run_info["device_id"]] -= 1
                    self.logger.debug("GPU device released: {}".format(run_info["device_id"]))
                future.release()
            else:
                # Dask not invoked in the synchronous case
                run_info = future
            # update bracket information
            fitness, cost = run_info["fitness"], run_info["cost"]
            info = run_info["info"] if "info" in run_info else dict()
            budget, parent_id = run_info["budget"], run_info["parent_id"]
            config = run_info["config"]
            bracket_id = run_info["bracket_id"]
            for bracket in self.active_brackets:
                if bracket.bracket_id == bracket_id:
                    # bracket job complete
                    bracket.complete_job(budget)  # IMPORTANT to perform synchronous SH

            # carry out DE selection
            if fitness <= self.de[budget].fitness[parent_id]:
                self.de[budget].population[parent_id] = config
                self.de[budget].fitness[parent_id] = fitness
            # updating incumbents
            if self.de[budget].fitness[parent_id] < self.inc_score:
                self._update_incumbents(
                    config=self.de[budget].population[parent_id],
                    score=self.de[budget].fitness[parent_id],
                    info=info
                )
            # book-keeping
            self._update_trackers(
                traj=self.inc_score, runtime=cost, history=(
                    config.tolist(), float(fitness), float(cost), float(budget), info
                )
            )
        # remove processed future
        self.futures = np.delete(self.futures, [i for i, _ in done_list]).tolist()

    def _is_run_budget_exhausted(self, fevals=None, brackets=None, total_cost=None):
        """ Checks if the DEHB run should be terminated or continued
        """
        delimiters = [fevals, brackets, total_cost]
        delim_sum = sum(x is not None for x in delimiters)
        if delim_sum == 0:
            raise ValueError(
                "Need one of 'fevals', 'brackets' or 'total_cost' as budget for DEHB to run."
            )
        if fevals is not None:
            if len(self.traj) >= fevals:
                return True
        elif brackets is not None:
            if self.iteration_counter >= brackets:
                for bracket in self.active_brackets:
                    # waits for all brackets < iteration_counter to finish by collecting results
                    if bracket.bracket_id < self.iteration_counter and \
                            not bracket.is_bracket_done():
                        return False
                return True
        else:
            if time.time() - self.start >= total_cost:
                return True
            if len(self.runtime) > 0 and self.runtime[-1] - self.start >= total_cost:
                return True
        return False

    def _save_incumbent(self, name=None):
        if name is None:
            name = time.strftime("%x %X %Z", time.localtime(self.start))
            name = name.replace("/", '-').replace(":", '-').replace(" ", '_')
        try:
            res = dict()
            if self.configspace:
                config = self.vector_to_configspace(self.inc_config)
                res["config"] = config.get_dictionary()
            else:
                # TODO Merge into only one function, code is duplicated
                #  in get_incumbents
                config_index = self.map_closest_evaluated(self.inc_config)
                config_values = self.hp_configurations[config_index]
                config_dict = dict()
                for hp_index, hp_name in enumerate(self.param_space):
                    config_dict[hp_name] = config_values[hp_index]
                res["config"] = config_dict

            res["score"] = self.inc_score
            res["info"] = self.inc_info
            with open(os.path.join(self.output_path, "incumbent_{}.json".format(name)), 'w') as f:
                json.dump(res, f)
        except Exception as e:
            self.logger.warning("Incumbent not saved: {}".format(repr(e)))

    def _save_history(self, name=None):
        if name is None:
            name = time.strftime("%x %X %Z", time.localtime(self.start))
            name = name.replace("/", '-').replace(":", '-').replace(" ", '_')
        try:

            with open(os.path.join(self.output_path, "history_{}.pkl".format(name)), 'wb') as f:
                pickle.dump(self.history, f)
        except Exception as e:
            self.logger.warning("History not saved: {}".format(repr(e)))

    def _verbosity_debug(self):
        for bracket in self.active_brackets:
            self.logger.debug("Bracket ID {}:\n{}".format(
                bracket.bracket_id,
                str(bracket)
            ))

    def _verbosity_runtime(self, fevals, brackets, total_cost):
        if fevals is not None:
            remaining = (len(self.traj), fevals, "function evaluation(s) done")
        elif brackets is not None:
            _suffix = "bracket(s) started; # active brackets: {}".format(len(self.active_brackets))
            remaining = (self.iteration_counter + 1, brackets, _suffix)
        else:
            elapsed = np.format_float_positional(time.time() - self.start, precision=2)
            remaining = (elapsed, total_cost, "seconds elapsed")
        self.logger.info(
            "{}/{} {}".format(remaining[0], remaining[1], remaining[2])
        )

    @logger.catch
    def run(self, fevals=None, brackets=None, total_cost=None, single_node_with_gpus=False,
            verbose=False, debug=False, save_intermediate=True, save_history=True, **kwargs):
        """ Main interface to run optimization by DEHB

        This function waits on workers and if a worker is free, asks for a configuration and a
        budget to evaluate on and submits it to the worker. In each loop, it checks if a job
        is complete, fetches the results, carries the necessary processing of it asynchronously
        to the worker computations.

        The duration of the DEHB run can be controlled by specifying one of 3 parameters. If more
        than one are specified, DEHB selects only one in the priority order (high to low):
        1) Number of function evaluations (fevals)
        2) Number of Successive Halving brackets run under Hyperband (brackets)
        3) Total computational cost (in seconds) aggregated by all function evaluations (total_cost)
        """
        # checks if a Dask client exists
        if len(kwargs) > 0 and self.n_workers > 1 and isinstance(self.client, Client):
            # broadcasts all additional data passed as **kwargs to all client workers
            # this reduces overload in the client-worker communication by not having to
            # serialize the redundant data used by all workers for every job
            self.shared_data = self.client.scatter(kwargs, broadcast=True)

        # allows each worker to be mapped to a different GPU when running on a single node
        # where all available GPUs are accessible
        self.single_node_with_gpus = single_node_with_gpus
        if self.single_node_with_gpus:
            self.distribute_gpus()

        self.start = time.time()
        if verbose:
            print("\nLogging at {} for optimization starting at {}\n".format(
                os.path.join(os.getcwd(), self.log_filename),
                time.strftime("%x %X %Z", time.localtime(self.start))
            ))
        if debug:
            logger.configure(handlers=[{"sink": sys.stdout}])
        while True:
            if self._is_run_budget_exhausted(fevals, brackets, total_cost):
                break
            if self.is_worker_available():
                job_info = self._get_next_job()
                if brackets is not None and job_info['bracket_id'] >= brackets:
                    # ignore submission and only collect results
                    # when brackets are chosen as run budget, an extra bracket is created
                    # since iteration_counter is incremented in _get_next_job() and then checked
                    # in _is_run_budget_exhausted(), therefore, need to skip suggestions
                    # coming from the extra allocated bracket
                    # _is_run_budget_exhausted() will not return True until all the lower brackets
                    # have finished computation and returned its results
                    pass
                else:
                    if self.n_workers > 1 or isinstance(self.client, Client):
                        self.logger.debug("{}/{} worker(s) available.".format(
                            self._get_worker_count() - len(self.futures), self._get_worker_count()
                        ))
                    # submits job_info to a worker for execution
                    self.submit_job(job_info, **kwargs)
                    if verbose:
                        budget = job_info['budget']
                        self._verbosity_runtime(fevals, brackets, total_cost)
                        self.logger.info(
                            "Evaluating a configuration with budget {} under "
                            "bracket ID {}".format(budget, job_info['bracket_id'])
                        )
                        self.logger.info(
                            "Best score seen/Incumbent score: {}".format(self.inc_score)
                        )
                    self._verbosity_debug()
            self._fetch_results_from_workers()
            if save_intermediate and self.inc_config is not None:
                self._save_incumbent()
            if save_history and self.history is not None:
                self._save_history()
            self.clean_inactive_brackets()
        # end of while

        if verbose and len(self.futures) > 0:
            self.logger.info(
                "DEHB optimisation over! Waiting to collect results from workers running..."
            )
        while len(self.futures) > 0:
            self._fetch_results_from_workers()
            if save_intermediate and self.inc_config is not None:
                self._save_incumbent()
            if save_history and self.history is not None:
                self._save_history()
            time.sleep(0.05)  # waiting 50ms

        if verbose:
            time_taken = time.time() - self.start
            self.logger.info("End of optimisation! Total duration: {}; Total fevals: {}\n".format(
                time_taken, len(self.traj)
            ))
            self.logger.info("Incumbent score: {}".format(self.inc_score))
            self.logger.info("Incumbent config: ")
            config = self.vector_to_configspace(self.inc_config)
            for k, v in config.get_dictionary().items():
                self.logger.info("{}: {}".format(k, v))
        self._save_incumbent()
        self._save_history()
        return np.array(self.traj), np.array(self.runtime), np.array(self.history, dtype=object)

    # duplicate code, is included in dehb_package/interface
    # TODO create a utilities module with static functions to share the code
    def map_closest_evaluated(
        self,
        config: Tuple,
    ) -> np.ndarray:
        """
        Maps the hyperparameter configuration to the closest
        available hyperparameter configuration.

        Args:
        -----
        config: tuple
            The hyperparameter configuration suggested by DEHB.

        Returns:
        --------
        config: np.ndarray
            An array representing the nearest available
            hyperparameter configuration.
        """
        closest_configuration_index = None
        smallest_distance = np.inf
        transformed_hp_candidates = self.from_hp_value_to_dehb_values(
            self.hp_configurations,
        )
        for i in range(0, transformed_hp_candidates.shape[0]):
            current_distance = 0
            possible_config = transformed_hp_candidates[i, :]
            for hyperparameter_index in range(0, len(config)):
                main_config_hyperparameter_value = config[hyperparameter_index]
                candidate_config_hyperparameter_value = possible_config[hyperparameter_index]
                current_distance += abs(main_config_hyperparameter_value - candidate_config_hyperparameter_value)
            if current_distance < smallest_distance:
                smallest_distance = current_distance
                closest_configuration_index = i

        return closest_configuration_index

    # duplicate code, is included in dehb_package/interface
    # TODO create a utilities module with static functions to share the code
    def from_hp_value_to_dehb_values(
        self,
        hp_candidates: np.ndarray,
    ) -> np.ndarray:
        """
        Maps the hyperparameter configurations from the original
        space to the DEHB unit cube space.

        Args:
        -----
        hp_candidates: np.ndarray
            The hyperparameter configuration suggested by DEHB.

        Returns:
        --------
        new_configs: np.ndarray
            An array representing the hyperparameter configurations
            represented in the DEHB search space.
        """
        assert len(hp_candidates[0]) == len(self.param_space)

        new_configs = []

        for i in range(0, hp_candidates.shape[0]):
            new_config = []
            configuration = hp_candidates[i]
            for i, (k, v) in enumerate(self.param_space.items()):
                hp_type = v[2]
                value = configuration[i]
                lower, upper = v[0], v[1]
                is_log = v[3]
                if hp_type == str:
                    unique_values = v[0]
                    ranges = np.arange(start=0, stop=1, step=1 / len(unique_values))
                    for range_index, unique_value in enumerate(unique_values):
                        if unique_value == value:
                            step_size = (1 / len(unique_values))
                            # set the value at the middle of the hyperparameter
                            # allocated range
                            value = ranges[range_index] + step_size / 0.5
                        else:
                            # do nothing
                            pass
                else:
                    if is_log:
                        log_range = np.log(upper) - np.log(lower)
                        value = (np.log(value) - np.log(lower)) / log_range
                    else:
                        value = (value - lower) / (upper - lower)
                    new_config.append(value)
            new_configs.append(new_config)

        return np.array(new_configs)


File Path: surrogate_models/dehb/dehb/utils/__init__.py
Content:
from .bracket_manager import SHBracketManager
File Path: surrogate_models/dehb/dehb/utils/bracket_manager.py
Content:
import numpy as np


class SHBracketManager(object):
    """ Synchronous Successive Halving utilities
    """
    def __init__(self, n_configs, budgets, bracket_id=None):
        assert len(n_configs) == len(budgets)
        self.n_configs = n_configs
        self.budgets = budgets
        self.bracket_id = bracket_id
        self.sh_bracket = {}
        self._sh_bracket = {}
        self._config_map = {}
        for i, budget in enumerate(budgets):
            # sh_bracket keeps track of jobs/configs that are still to be scheduled/allocatted
            # _sh_bracket keeps track of jobs/configs that have been run and results retrieved for
            # (sh_bracket[i] + _sh_bracket[i]) == n_configs[i] is when no jobs have been scheduled
            #   or all jobs for that budget/rung are over
            # (sh_bracket[i] + _sh_bracket[i]) < n_configs[i] indicates a job has been scheduled
            #   and is queued/running and the bracket needs to be paused till results are retrieved
            self.sh_bracket[budget] = n_configs[i]  # each scheduled job does -= 1
            self._sh_bracket[budget] = 0  # each retrieved job does +=1
        self.n_rungs = len(budgets)
        self.current_rung = 0

    def get_budget(self, rung=None):
        """ Returns the exact budget that rung is pointing to.

        Returns current rung's budget if no rung is passed.
        """
        if rung is not None:
            return self.budgets[rung]
        return self.budgets[self.current_rung]

    def get_lower_budget_promotions(self, budget):
        """ Returns the immediate lower budget and the number of configs to be promoted from there
        """
        assert budget in self.budgets
        rung = np.where(budget == self.budgets)[0][0]
        prev_rung = np.clip(rung - 1, a_min=0, a_max=self.n_rungs-1)
        lower_budget = self.budgets[prev_rung]
        num_promote_configs = self.n_configs[rung]
        return lower_budget, num_promote_configs

    def get_next_job_budget(self):
        """ Returns the budget that will be selected if current_rung is incremented by 1
        """
        if self.sh_bracket[self.get_budget()] > 0:
            # the current rung still has unallocated jobs (>0)
            return self.get_budget()
        else:
            # the current rung has no more jobs to allocate, increment it
            rung = (self.current_rung + 1) % self.n_rungs
            if self.sh_bracket[self.get_budget(rung)] > 0:
                # the incremented rung has unallocated jobs (>0)
                return self.get_budget(rung)
            else:
                # all jobs for this bracket has been allocated/bracket is complete
                # no more budgets to evaluate and can return None
                pass
            return None

    def register_job(self, budget):
        """ Registers the allocation of a configuration for the budget and updates current rung

        This function must be called when scheduling a job in order to allow the bracket manager
        to continue job and budget allocation without waiting for jobs to finish and return
        results necessarily. This feature can be leveraged to run brackets asynchronously.
        """
        assert budget in self.budgets
        assert self.sh_bracket[budget] > 0
        self.sh_bracket[budget] -= 1
        if not self._is_rung_pending(self.current_rung):
            # increment current rung if no jobs left in the rung
            self.current_rung = (self.current_rung + 1) % self.n_rungs

    def complete_job(self, budget):
        """ Notifies the bracket that a job for a budget has been completed

        This function must be called when a config for a budget has finished evaluation to inform
        the Bracket Manager that no job needs to be waited for and the next rung can begin for the
        synchronous Successive Halving case.
        """
        assert budget in self.budgets
        _max_configs = self.n_configs[list(self.budgets).index(budget)]
        assert self._sh_bracket[budget] < _max_configs
        self._sh_bracket[budget] += 1

    def _is_rung_waiting(self, rung):
        """ Returns True if at least one job is still pending/running and waits for results
        """
        job_count = self._sh_bracket[self.budgets[rung]] + self.sh_bracket[self.budgets[rung]]
        if job_count < self.n_configs[rung]:
            return True
        return False

    def _is_rung_pending(self, rung):
        """ Returns True if at least one job pending to be allocatted in the rung
        """
        if self.sh_bracket[self.budgets[rung]] > 0:
            return True
        return False

    def previous_rung_waits(self):
        """ Returns True if none of the rungs < current rung is waiting for results
        """
        for rung in range(self.current_rung):
            if self._is_rung_waiting(rung) and not self._is_rung_pending(rung):
                return True
        return False

    def is_bracket_done(self):
        """ Returns True if all configs in all rungs in the bracket have been allocated
        """
        return ~self.is_pending() and ~self.is_waiting()

    def is_pending(self):
        """ Returns True if any of the rungs/budgets have still a configuration to submit
        """
        return np.any([self._is_rung_pending(i) > 0 for i, _ in enumerate(self.budgets)])

    def is_waiting(self):
        """ Returns True if any of the rungs/budgets have a configuration pending/running
        """
        return np.any([self._is_rung_waiting(i) > 0 for i, _ in enumerate(self.budgets)])

    def __repr__(self):
        cell_width = 9
        cell = "{{:^{}}}".format(cell_width)
        budget_cell = "{{:^{}.2f}}".format(cell_width)
        header = "|{}|{}|{}|{}|".format(
            cell.format("budget"),
            cell.format("pending"),
            cell.format("waiting"),
            cell.format("done")
        )
        _hline = "-" * len(header)
        table = [header, _hline]
        for i, budget in enumerate(self.budgets):
            pending = self.sh_bracket[budget]
            done = self._sh_bracket[budget]
            waiting = np.abs(self.n_configs[i] - pending - done)
            entry = "|{}|{}|{}|{}|".format(
                budget_cell.format(budget),
                cell.format(pending),
                cell.format(waiting),
                cell.format(done)
            )
            table.append(entry)
        table.append(_hline)
        return "\n".join(table)

File Path: surrogate_models/dehb/examples/03_pytorch_mnist_hpo.py
Content:
"""
This script runs a Hyperparameter Optimisation (HPO) using DEHB to tune the architecture and
training hyperparameters for training a neural network on MNIST in PyTorch.

The parameter space is defined in the get_configspace() function. Any configuration sampled from
this space can be passed to an object of class Model() which can instantiate a CNN architecture
from it. The objective_function() is the target function that DEHB minimizes for this problem. This
function instantiates an architecture, an optimizer, as defined by a configuration and performs the
training and evaluation (on the validation set) as per the budget passed.
The argument `runtime` can be passed to DEHB as a wallclock budget for running the optimisation.

This tutorial also briefly refers to the different methods of interfacing DEHB with the Dask
parallelism framework. Moreover, also introduce how GPUs may be managed, which is recommended for
running this example tutorial.

Additional requirements:
* torch>=1.7.1
* torchvision>=0.8.2
* torchsummary>=1.5.1

PyTorch code referenced from: https://github.com/pytorch/examples/blob/master/mnist/main.py
"""


import os
import time
import pickle
import argparse
import numpy as np
from distributed import Client

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torchsummary import summary

import ConfigSpace as CS
import ConfigSpace.hyperparameters as CSH

from dehb import DEHB


class Model(nn.Module):
    def __init__(self, config, img_dim=28, output_dim=10):
        super().__init__()
        self.output_dim = output_dim
        self.pool_kernel = 2
        self.pool_stride = 1
        self.maxpool = nn.MaxPool2d(self.pool_kernel, self.pool_stride)
        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=config["channels_1"],
            kernel_size=config["kernel_1"],
            stride=config["stride_1"],
            padding=0,
            dilation=1
        )
        # updating image size after conv1
        img_dim = self._update_size(img_dim, config["kernel_1"], config["stride_1"], 0, 1)
        self.conv2 = nn.Conv2d(
            in_channels=config["channels_1"],
            out_channels=config["channels_2"],
            kernel_size=config["kernel_2"],
            stride=config["stride_2"],
            padding=0,
            dilation=1
        )
        # updating image size after conv2
        img_dim = self._update_size(img_dim, config["kernel_2"], config["stride_2"], 0, 1)
        # updating image size after maxpool
        img_dim = self._update_size(img_dim, self.pool_kernel, self.pool_stride, 0, 1)
        self.dropout = nn.Dropout(config["dropout"])
        hidden_dim = config["hidden"]
        self.fc1 = nn.Linear(img_dim * img_dim * config["channels_2"], hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, self.output_dim)

    def forward(self, x):
        # Layer 1
        x = self.conv1(x)
        x = F.relu(x)
        x = self.dropout(x)
        # Layer 2
        x = self.conv2(x)
        x = F.relu(x)
        x = self.maxpool(x)
        x = self.dropout(x)
        # FC Layer 1
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        # Output layer
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

    def _update_size(self, dim, kernel_size, stride, padding, dilation):
        return int(np.floor((dim + 2 * padding - (dilation * (kernel_size - 1) + 1)) / stride + 1))


def get_configspace(seed=None):
    cs = CS.ConfigurationSpace(seed)

    # Hyperparameter defining first Conv layer
    kernel1 = CSH.OrdinalHyperparameter("kernel_1", sequence=[3, 5, 7], default_value=5)
    channels1 = CSH.UniformIntegerHyperparameter("channels_1", lower=3, upper=64,
                                                 default_value=32)
    stride1 = CSH.UniformIntegerHyperparameter("stride_1", lower=1, upper=2, default_value=1)
    cs.add_hyperparameters([kernel1, channels1, stride1])

    # Hyperparameter defining second Conv layer
    kernel2 = CSH.OrdinalHyperparameter("kernel_2", sequence=[3, 5, 7], default_value=5)
    channels2 = CSH.UniformIntegerHyperparameter("channels_2", lower=3, upper=64,
                                                 default_value=32)
    stride2 = CSH.UniformIntegerHyperparameter("stride_2", lower=1, upper=2, default_value=1)
    cs.add_hyperparameters([kernel2, channels2, stride2])

    # Hyperparameter for FC layer
    hidden = CSH.UniformIntegerHyperparameter(
        "hidden", lower=32, upper=256, log=True, default_value=128
    )
    cs.add_hyperparameter(hidden)

    # Regularization Hyperparameter
    dropout = CSH.UniformFloatHyperparameter("dropout", lower=0, upper=0.5, default_value=0.1)
    cs.add_hyperparameter(dropout)

    # Training Hyperparameters
    batch_size = CSH.OrdinalHyperparameter(
        "batch_size", sequence=[2, 4, 8, 16, 32, 64], default_value=4
    )
    lr = CSH.UniformFloatHyperparameter("lr", lower=1e-6, upper=0.1, log=True,
                                        default_value=1e-3)
    cs.add_hyperparameters([batch_size, lr])
    return cs


def train(model, device, train_loader, optimizer):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()


def evaluate(model, device, data_loader, acc=False):
    model.eval()
    loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in data_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    loss /= len(data_loader.dataset)
    correct /= len(data_loader.dataset)

    if acc:
        return correct
    return loss


def train_and_evaluate(config, max_budget, verbose=False, **kwargs):
    device = kwargs["device"]
    batch_size = config["batch_size"]
    train_set = kwargs["train_set"]
    test_set = kwargs["test_set"]
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)
    model = Model(config).to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=config["lr"])
    for epoch in range(1, int(max_budget)+1):
        train(model, device, train_loader, optimizer)
    accuracy = evaluate(model, device, test_loader, acc=True)
    if verbose:
        summary(model, (1, 28, 28))  # image dimensions for MNIST
    return accuracy


def objective_function(config, budget, **kwargs):
    """ The target function to minimize for HPO"""
    device = kwargs["device"]

    # Data Loaders
    batch_size = config["batch_size"]
    train_set = kwargs["train_set"]
    valid_set = kwargs["valid_set"]
    test_set = kwargs["test_set"]
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)

    # Build model
    model = Model(config).to(device)

    # Optimizer
    optimizer = optim.Adadelta(model.parameters(), lr=config["lr"])

    start = time.time()  # measuring wallclock time
    for epoch in range(1, int(budget)+1):
        train(model, device, train_loader, optimizer)
    loss = evaluate(model, device, valid_loader)
    cost = time.time() - start

    # not including test score computation in the `cost`
    test_loss = evaluate(model, device, test_loader)

    # dict representation that DEHB requires
    res = {
        "fitness": loss,
        "cost": cost,
        "info": {"test_loss": test_loss, "budget": budget}
    }
    return res


def input_arguments():
    parser = argparse.ArgumentParser(description='Optimizing MNIST in PyTorch using DEHB.')
    parser.add_argument('--no_cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--seed', type=int, default=123, metavar='S',
                        help='random seed (default: 123)')
    parser.add_argument('--refit_training', action='store_true', default=False,
                        help='Refit with incumbent configuration on full training data and budget')
    parser.add_argument('--min_budget', type=float, default=None,
                        help='Minimum budget (epoch length)')
    parser.add_argument('--max_budget', type=float, default=None,
                        help='Maximum budget (epoch length)')
    parser.add_argument('--eta', type=int, default=3,
                        help='Parameter for Hyperband controlling early stopping aggressiveness')
    parser.add_argument('--output_path', type=str, default="./pytorch_mnist_dehb",
                        help='Directory for DEHB to write logs and outputs')
    parser.add_argument('--scheduler_file', type=str, default=None,
                        help='The file to connect a Dask client with a Dask scheduler')
    parser.add_argument('--n_workers', type=int, default=1,
                        help='Number of CPU workers for DEHB to distribute function evaluations to')
    parser.add_argument('--single_node_with_gpus', default=False, action="store_true",
                        help='If True, signals the DEHB run to assume all required GPUs are on '
                             'the same node/machine. To be specified as True if no client is '
                             'passed and n_workers > 1. Should be set to False if a client is '
                             'specified as a scheduler-file created. The onus of GPU usage is then'
                             'on the Dask workers created and mapped to the scheduler-file.')
    parser.add_argument('--verbose', action="store_true", default=False,
                        help='Decides verbosity of DEHB optimization')
    parser.add_argument('--runtime', type=float, default=300,
                        help='Total time in seconds as budget to run DEHB')
    args = parser.parse_args()
    return args


def main():
    args = input_arguments()

    use_cuda = not args.no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    torch.manual_seed(args.seed)

    # Data Preparation
    transform = transforms.Compose([
        transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))
    ])
    train_set = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=transform
    )
    train_set, valid_set = torch.utils.data.random_split(train_set, [50000, 10000])
    test_set = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=transform
    )

    # Get configuration space
    cs = get_configspace(args.seed)
    dimensions = len(cs.get_hyperparameters())

    # Some insights into Dask interfaces to DEHB and handling GPU devices for parallelism:
    # * if args.scheduler_file is specified, args.n_workers need not be specifed --- since
    #    args.scheduler_file indicates a Dask client/server is active
    # * if args.scheduler_file is not specified and args.n_workers > 1 --- the DEHB object
    #    creates a Dask client as at instantiation and dies with the associated DEHB object
    # * if args.single_node_with_gpus is True --- assumes that all GPU devices indicated
    #    through the environment variable "CUDA_VISIBLE_DEVICES" resides on the same machine

    # Dask checks and setups
    single_node_with_gpus = args.single_node_with_gpus
    if args.scheduler_file is not None and os.path.isfile(args.scheduler_file):
        client = Client(scheduler_file=args.scheduler_file)
        # explicitly delegating GPU handling to Dask workers defined
        single_node_with_gpus = False
    else:
        client = None

    ###########################
    # DEHB optimisation block #
    ###########################
    np.random.seed(args.seed)
    dehb = DEHB(f=objective_function, cs=cs, dimensions=dimensions, min_budget=args.min_budget,
                max_budget=args.max_budget, eta=args.eta, output_path=args.output_path,
                # if client is not None and of type Client, n_workers is ignored
                # if client is None, a Dask client with n_workers is set up
                client=client, n_workers=args.n_workers)
    traj, runtime, history = dehb.run(total_cost=args.runtime, verbose=args.verbose, 
                                      # arguments below are part of **kwargs shared across workers
                                      train_set=train_set, valid_set=valid_set, test_set=test_set,
                                      single_node_with_gpus=single_node_with_gpus, device=device)
    # end of DEHB optimisation

    # Saving optimisation trace history
    name = time.strftime("%x %X %Z", time.localtime(dehb.start))
    name = name.replace("/", '-').replace(":", '-').replace(" ", '_')
    dehb.logger.info("Saving optimisation trace history...")
    with open(os.path.join(args.output_path, "history_{}.pkl".format(name)), "wb") as f:
        pickle.dump(history, f)

    # Retrain and evaluate best found configuration
    if args.refit_training:
        dehb.logger.info("Retraining on complete training data to compute test metrics...")
        train_set = torchvision.datasets.MNIST(
            root='./data', train=True, download=True, transform=transform
        )
        incumbent = dehb.vector_to_configspace(dehb.inc_config)
        acc = train_and_evaluate(incumbent, args.max_budget, verbose=True,
                                 train_set=train_set, test_set=test_set, device=device)
        dehb.logger.info("Test accuracy of {:.3f} for the best found configuration: ".format(acc))
        dehb.logger.info(incumbent)


if __name__ == "__main__":
    main()

File Path: surrogate_models/dehb/interface.py
Content:
import threading
import time
from typing import Dict, List, OrderedDict, Tuple, Union

import ConfigSpace
import numpy as np

from surrogate_models.dehb.dehb import DEHB


class DEHBOptimizer:
    def __init__(
        self,
        hyperparameter_candidates: np.ndarray,
        min_budget: int,
        max_budget: int,
        param_space: OrderedDict,
        seed: int = 0,
        exhaustive_search_space: bool = False,
        total_cost=np.inf,
        maximization: bool = 'True',
        **kwargs,
    ):
        """
        Wrapper for the DEHB algorithm.

        Args:
        -----
        hyperparameter_candidates: np.ndarray
            2d array which contains all possible configurations which can be queried.
        min_budget: int
            Minimum number of epochs available.
        max_budget: int
            Maximum number of epochs available.
        param_space: OrderedDict
            The hyperparameter search-space, indicating the type and range of every
            hyperparameter.
        seed: int
            Seed used to reproduce the experiments.
        **kwargs: dict
            DEHB configuration if desired.
        """
        search_space_dimensions = hyperparameter_candidates.shape[1]
        self.maximization = maximization

        if len(kwargs) != 6:
            # default DEHB configuration
            configuration = {
                'strategy': 'rand1_bin',
                'mutation_factor': 0.5,
                'crossover_prob': 0.5,
                'eta': 3,
                'boundary_fix_type': 'random',
                'gens': 1,
                'nr_workers': 1,
            }
        else:
            configuration = kwargs

        self.min_budget = min_budget
        self.max_budget = max_budget
        self.hyperparameter_mapping = self.create_configuration_to_indices(
            hyperparameter_candidates,
        )
        self.hyperparameter_candidates = hyperparameter_candidates
        self.param_space = param_space
        self.exhaustive_search_space = exhaustive_search_space
        self.transformed_hp_candidates = self.from_hp_value_to_dehb_values(
            self.hyperparameter_candidates,
        )

        # empty configuration, empty budget, empty information for config
        self.next_conf = None
        self.conf_budget = None
        self.conf_info = None
        self.rng = np.random.RandomState(seed)
        np.random.seed(seed)

        configuration_space = None
        if self.exhaustive_search_space:
            configuration_space = self.create_config_space()

        self.evaluated_configurations = dict()
        self.evaluated_hp_curves = dict()

        # Initializing DEHB object
        self.dehb = \
            DEHB(
                dimensions=search_space_dimensions,
                f=self.target_function,
                cs=configuration_space,
                strategy=configuration['strategy'],
                mutation_factor=configuration['mutation_factor'],
                crossover_prob=configuration['crossover_prob'],
                eta=configuration['eta'],
                min_budget=self.min_budget,
                max_budget=self.max_budget,
                generations=configuration['gens'],
                boundary_fix_type=configuration['boundary_fix_type'],
                n_workers=configuration['nr_workers'],
                hyperparameter_canditates=self.hyperparameter_candidates if not self.exhaustive_search_space else None,
                param_space=self.param_space if not self.exhaustive_search_space else None,
                random_state=self.rng,
            )

        self.debh_run = threading.Thread(
            target=self.dehb.run,
            kwargs={
                'total_cost': total_cost,
                'verbose': False,

            },
            daemon=True,
        )
        self.debh_run.start()

    def target_function(
        self,
        config: np.ndarray,
        budget: int = 100,
    ) -> Dict[str, Union[float, Dict]]:
        """
        Function to evaluate for a given configuration.

        Args:
        -----
        config: np.ndarray
            Configuration suggested by DEHB.
        budget: int
            The budget for which the configuration will be run.

        Returns:
        ________
        result: dict
            A dictionary which contains information about the performance
            of the configuration, the cost as well as additional information.
        """
        if budget is not None:
            budget = int(budget)
        # if the search space is not exhaustive the configuration sampled may
        # not be present in the available hyperparameter configurations. Map it
        # to the closest available hyperparameter configuration.
        if not self.exhaustive_search_space:
            config_index = self.map_closest_evaluated(
                config,
                budget,
            )
        else:
            config_index = self.map_configuration_to_index(config)

        # not the first hyperparameter to be evaluated for the selected
        # budget
        if budget in self.evaluated_configurations:
            self.evaluated_configurations[budget].add(config_index)
        else:
            self.evaluated_configurations[budget] = set([config_index])

        self.conf_budget = budget

        need_to_query_framework = True
        if config_index in self.evaluated_hp_curves:
            config_curve = self.evaluated_hp_curves[config_index]
            # the hyperparameter configuration has been evaluated before
            # and it was evaluated for a higher\same budget
            if len(config_curve) >= budget:
                need_to_query_framework = False

        score = None
        if need_to_query_framework:
            # update the field so the framework can take the index and
            # reply
            self.next_conf = config_index
            while True:
                if self.conf_info is not None:
                    score = self.conf_info['score']
                    val_curve = self.conf_info['val_curve']
                    # save the curve for the evaluated hyperparameter
                    # configuration
                    self.evaluated_hp_curves[config_index] = val_curve
                    break
                else:
                    # The framework has not yet responded with a value,
                    # keep checking
                    # TODO add a delay
                    pass
        else:
            score = config_curve[budget - 1]
            val_curve = config_curve[0:budget]

        if self.maximization:
            # DE/DEHB minimizes
            fitness = - score
        else:
            fitness = score

        cost = None
        # if cost is given (from a benchmark) use the value given
        # otherwise, just calculate the duration.
        if self.conf_info is not None:
            if 'cost' in self.conf_info:
                cost = self.conf_info['cost']
            else:
                start_time = time.time()
        else:
            start_time = time.time()

        if cost is None:
            end_time = time.time()
            cost = end_time - start_time

        try:
            val_curve = val_curve.tolist()
        except Exception:
            # val_curve is not a numpy array
            # but instead it is a list, do nothing.
            pass

        result = {
            "fitness": fitness,
            "cost": cost,
            "info": {
                "val_curve": val_curve,
                "val_score": score,
                "budget": budget

            }
        }

        # need to make the previous response None since DEHB
        # continues running in the background
        self.conf_info = None

        return result

    def suggest(self) -> Tuple[int, float]:
        """
        Get information about the next configuration.

        Returns:
        ________
        next_conf, conf_budget: tuple
            A tuple that contains information about the next
            configuration (index in the hyperparameter_candidates it was
            given) and the budget for the hyperparameter to be evaluated
            on.
        """
        while self.next_conf is None:
            # DEHB has not generated the config yet
            pass
        self.conf_info = None

        return self.next_conf, self.conf_budget

    def observe(self,
        hp_index: int,
        budget: int,
        learning_curve: List[float],
    ):
        """
        Respond regarding the performance of a
        hyperparameter configuration. get_next should
        be called first to retrieve the configuration.

        Args:
        -----
        score: float
            validation accuracy, the higher the better.
        learning curve: np.ndarray, list
            validation accuracy curve. The last value is the same as the score.
        """
        assert self.next_conf is not None, 'Call get_next first.'
        self.next_conf = None
        self.conf_info = {
            'score': learning_curve[-1],
            'val_curve': learning_curve,
        }

    def create_configuration_to_indices(
        self,
        hyperparameter_candidates: np.ndarray,
    ) -> Dict[tuple, int]:
        """
        Maps every configuration to its index as specified
        in hyperparameter_candidates.

        Args:
        -----
        hyperparameter_candidates: np.ndarray
            All the possible hyperparameter candidates given
            by the calling framework.

        Returns:
        ________
        hyperparameter_mapping: dict
            A dictionary where the keys are tuples representing
            hyperparameter configurations and the values are indices
            representing their placement in hyperparameter_candidates.
        """
        hyperparameter_mapping = dict()
        for i in range(0, hyperparameter_candidates.shape[0]):
            hyperparameter_mapping[tuple(hyperparameter_candidates[i])] = i

        return hyperparameter_mapping

    def map_configuration_to_index(
        self,
        hyperparameter_candidate: np.ndarray,
    ) -> int:
        """
        Return the index of the hyperparameter_candidate from
        the given initial array of possible hyperparameters.

        Args:
        -----
        hyperparameter_candidate: np.ndarray
            Hyperparameter configuration.

        Returns:
        ________
        index of the hyperparameter_candidate.
        """
        hyperparameter_candidate = tuple(hyperparameter_candidate)

        return self.hyperparameter_mapping[hyperparameter_candidate]

    def transform_space(
        self,
        param_space: Dict[str, List],
        configuration: Tuple,
    ) -> Tuple:
        """
        Scales the [0, 1] - ranged parameter linearly to [lower, upper].

        Args:
        -----
        param_space: dict
            A dictionary containing the parameters and their meta-info.
        configuration: tuple
            A vector with each dimension in [0, 1] (from DEHB).

        Returns:
        --------
        config: tuple
            A tuple of the ordered hyperparameter values.
        """
        assert len(configuration) == len(param_space)

        config = []
        for i, (k, v) in enumerate(param_space.items()):
            value = configuration[i]
            if len(v) > 2:
                hp_type = v[2]
            else:
                hp_type = v[1]
            # str can be passed for categorical variables
            if hp_type == str:
                # Unique values
                unique_values = v[0]
                ranges = np.arange(start=0, stop=1, step=1 / len(unique_values))
                value = unique_values[np.where((value < ranges) == False)[0][-1]]
            # float and integer hyperparameters
            else:
                lower, upper = v[0], v[1]
                is_log = v[3]
                if is_log:
                    # performs linear scaling in the log-space
                    log_range = np.log(upper) - np.log(lower)
                    value = np.exp(np.log(lower) + log_range * value)
                else:
                    # linear scaling within the range of the parameter
                    value = lower + (upper - lower) * value
                if hp_type == int:
                    value = np.round(value).astype(int)
            config.append(value)

        config = tuple(config)

        return config

    def map_closest_evaluated(
        self,
        config: Tuple,
        budget: int,
    ) -> np.ndarray:
        """
        Maps the hyperparameter configuration to the closest
        available hyperparameter configuration.

        Args:
        -----
        config: tuple
            The hyperparameter configuration suggested by DEHB.
        budget: int
            The budget of the hyperparameter configuration.

        Returns:
        --------
        config: np.ndarray
            An array representing the nearest available
            hyperparameter configuration.
        """
        closest_configuration_index = None
        smallest_distance = np.inf

        for i in range(0, self.transformed_hp_candidates.shape[0]):
            current_distance = 0
            possible_config = self.transformed_hp_candidates[i, :]
            for hyperparameter_index in range(0, len(config)):
                main_config_hyperparameter_value = config[hyperparameter_index]
                candidate_config_hyperparameter_value = possible_config[hyperparameter_index]
                current_distance += abs(main_config_hyperparameter_value - candidate_config_hyperparameter_value)
            if current_distance < smallest_distance:
                if len(self.evaluated_configurations) != 0:
                    # if a hyperparameter has already been evaluated for a certain
                    # budget, we do not consider it anymore.
                    if budget in self.evaluated_configurations and i in self.evaluated_configurations[budget]:
                        continue
                smallest_distance = current_distance
                closest_configuration_index = i

        return closest_configuration_index

    def create_config_space(self) -> ConfigSpace.ConfigurationSpace:
        """
        Return a configuration space based on the specifications given
        at the param_space object.

        Returns:
        ________
        cs - ConfigurationSpace
            A configuration space from which the hyperparameters will
            be sampled.
        """
        cs = ConfigSpace.ConfigurationSpace()
        for i, (k, v) in enumerate(self.param_space.items()):
            hp_type = v[2]
            lower, upper = v[0], v[1]
            is_log = v[3]
            if hp_type == str:
                unique_values = v[0]
                cs.add_hyperparameter(
                    ConfigSpace.CategoricalHyperparameter(
                        k,
                        choices=unique_values,
                    )
                )
            else:
                if hp_type == int:
                    numerical_hp = ConfigSpace.UniformIntegerHyperparameter
                elif hp_type == float:
                    numerical_hp = ConfigSpace.UniformFloatHyperparameter
                else:
                    raise ValueError('Illegal hyperparameter type given')

                cs.add_hyperparameter(
                    numerical_hp(
                        k,
                        lower=lower,
                        upper=upper,
                        log=is_log,
                    )
                )

        return cs

    def from_hp_value_to_dehb_values(
        self,
        hp_candidates: np.ndarray,
    ) -> np.ndarray:
        """
        Maps the hyperparameter configurations from the original
        space to the DEHB unit cube space.

        Args:
        -----
        hp_candidates: np.ndarray
            The hyperparameter configuration suggested by DEHB.

        Returns:
        --------
        new_configs: np.ndarray
            An array representing the hyperparameter configurations
            represented in the DEHB search space.
        """
        assert len(hp_candidates[0]) == len(self.param_space)

        new_configs = []

        for i in range(0, hp_candidates.shape[0]):
            new_config = []
            configuration = hp_candidates[i]
            for hp_index, (k, v) in enumerate(self.param_space.items()):
                hp_type = v[2]
                value = configuration[hp_index]
                lower, upper = v[0], v[1]
                is_log = v[3]
                if hp_type == str:
                    unique_values = v[0]
                    ranges = np.arange(start=0, stop=1, step=1 / len(unique_values))
                    for range_index, unique_value in enumerate(unique_values):
                        if unique_value == value:
                            step_size = (1 / len(unique_values))
                            # set the value at the middle of the hyperparameter
                            # allocated range
                            value = ranges[range_index] + step_size / 0.5
                        else:
                            # do nothing
                            pass
                else:
                    if is_log:
                        log_range = np.log(upper) - np.log(lower)
                        value = (np.log(value) - np.log(lower)) / log_range
                    else:
                        value = (value - lower) /  (upper - lower)
                    new_config.append(value)
            new_configs.append(new_config)

        return np.array(new_configs)

File Path: surrogate_models/dragonfly.py
Content:
from argparse import Namespace
import threading

from typing import Dict, List, OrderedDict, Tuple

from dragonfly import load_config, maximize_multifidelity_function, minimize_multifidelity_function

import numpy as np


class DragonFlyOptimizer:
    def __init__(
        self,
        hyperparameter_candidates: np.ndarray,
        param_space: OrderedDict,
        seed: int = 0,
        max_budget: int = 52,
        max_nr_trials: int = 2000,
        maximization: bool = True,
        **kwargs,
    ):
        """
        Wrapper for the BOCA algorithm.

        Args:
        -----
        hyperparameter_candidates: np.ndarray
            2d array which contains all possible configurations which can be queried.
        param_space: OrderedDict
            The hyperparameter search-space, indicating the type and range of every
            hyperparameter.
        seed: int
            Seed used to reproduce the experiments.
        max_budget: int
            The number of maximal steps for a hyperparameter configuration.
        max_nr_trials: int
            The total runtime budget, given as the number of epochs spent during HPO.
        maximization: bool
            If the inner objective is to maximize or minimize.
        """
        self.maximization = maximization
        self.hyperparameter_candidates = hyperparameter_candidates
        self.param_space = param_space
        self.extra_arguments = kwargs

        self.hyperparameter_mapping = self.create_configuration_to_indices()

        # empty configuration, empty budget, empty information for config
        self.next_conf = None
        self.conf_budget = None
        self.conf_info = None
        self.fidelity_index = None
        self.rng = np.random.RandomState(seed)
        np.random.seed(seed)

        self.evaluated_configurations = dict()
        self.evaluated_hp_curves = dict()
        # Basically the same as evaluated_hp_curves. However, this will
        # be used to estimate the evaluation cost for a certain fidelity.
        # If we used evaluated_hp_curves, the cost would always be zero
        # since the configuration index is added there as evaluated already
        # before.
        self.fidelity_hp_curves = dict()
        domain_vars = [
            {'type': 'discrete_euclidean', 'items': list(self.hyperparameter_candidates)},
        ]
        fidel_vars = [
            {'type': 'int', 'min': 1, 'max': max_budget},
        ]

        fidel_to_opt = [int(max_budget)]

        config = {
            'domain': domain_vars,
            'fidel_space': fidel_vars,
            'fidel_to_opt': fidel_to_opt,
        }
        # How frequently to build a new (GP) model
        # --build_new_model_every 17
        options_namespace = Namespace(
            gpb_hp_tune_criterion='ml',
        )
        config = load_config(config)

        self.dragonfly_run = threading.Thread(
            target=maximize_multifidelity_function if self.maximization else minimize_multifidelity_function,
            kwargs={
                'func': self.target_function,
                'max_capital': max_nr_trials,
                'config': config,
                'domain': config.domain,
                'fidel_space': config.fidel_space,
                'fidel_to_opt': config.fidel_to_opt,
                'options': options_namespace,
                'fidel_cost_func': self.fidel_cost_function,
            },
            daemon=True,
        )
        self.dragonfly_run.start()

    def fidel_cost_function(self, fidelity):

        fidelity_value = fidelity[0]
        while True:
            if self.fidelity_index is not None:
                config_index = self.fidelity_index
                if config_index in self.fidelity_hp_curves:
                    budget_evaluated = self.fidelity_hp_curves[config_index]
                    # the hyperparameter configuration has been evaluated before
                    # and it was evaluated for a higher\same budget
                    if budget_evaluated >= fidelity_value:
                        # there was a curve which was evaluated for longer
                        fidelity_opt_cost = 0
                    else:
                        # will only resume training for the extra query
                        fidelity_opt_cost = fidelity_value - budget_evaluated
                        self.fidelity_hp_curves[config_index] = fidelity_value
                else:
                    # first evaluation
                    fidelity_opt_cost = fidelity_value
                    self.fidelity_hp_curves[config_index] = fidelity_value
                self.fidelity_index = None
                break

        return fidelity_opt_cost

    def target_function(
        self,
        budget: List[int],
        config: List[np.ndarray],
    ) -> float:
        """
        Function to evaluate for a given configuration.

        Args:
        -----
        budget: list
            The budget for which the configuration will be run.
        config: list
            Configuration suggested by DragonFly.

        Returns:
        ________
        score: float
            A score which indicates the validation performance
            of the configuration.
        """
        # the budget is a list initially with only one value
        budget = budget[0]
        if budget is not None:
            budget = int(budget)

        # initially the config is a list consisting of a single np.ndarray
        config = list(config[0])

        config_index = self.map_configuration_to_index(config)

        # not the first hyperparameter to be evaluated for the selected
        # budget
        if budget in self.evaluated_configurations:
            self.evaluated_configurations[budget].add(config_index)
        else:
            self.evaluated_configurations[budget] = set([config_index])

        self.conf_budget = budget

        need_to_query_framework = True
        if config_index in self.evaluated_hp_curves:
            config_curve = self.evaluated_hp_curves[config_index]
            # the hyperparameter configuration has been evaluated before
            # and it was evaluated for a higher\same budget
            if len(config_curve) >= budget:
                need_to_query_framework = False

        # Save the config index in fidelity index, since sometimes this config
        # if evaluated before it is not passed to the framework, but it would be
        # still needed for the cost estimation.
        self.fidelity_index = config_index

        if need_to_query_framework:
            # update the field so the framework can take the index and
            # reply
            self.next_conf = config_index
            while True:
                if self.conf_info is not None:
                    score = self.conf_info['score']
                    val_curve = self.conf_info['val_curve']
                    # save the curve for the evaluated hyperparameter
                    # configuration
                    self.evaluated_hp_curves[config_index] = val_curve
                    break
                else:
                    # The framework has not yet responded with a value,
                    # keep checking
                    # TODO add a delay
                    pass
        else:
            score = config_curve[budget - 1]

        # need to make the previous response None since DragonFly
        # continues running in the background
        self.conf_info = None

        return score

    def suggest(self) -> Tuple[int, int]:
        """
        Get information about the next configuration.

        Returns:
        ________
        next_conf, conf_budget: tuple
            A tuple that contains information about the next
            configuration (index in the hyperparameter_candidates it was
            given) and the budget for the hyperparameter to be evaluated
            on.
        """
        while self.next_conf is None:
            # DragonFly has not generated the config yet
            pass
        self.conf_info = None

        return self.next_conf, self.conf_budget

    def observe(
        self,
        hp_index: int,
        budget: int,
        learning_curve: List[float],
    ):
        """
        Respond regarding the performance of a
        hyperparameter configuration. get_next should
        be called first to retrieve the configuration.

        Args:
        -----
        hp_index: int
            The index of the evaluated hyperparameter configuration.
        budget: int
            The budget for which the hyperparameter configuration was evaluated.
        learning curve: np.ndarray, list
            validation accuracy curve. The last value is the same as the score.
        """
        assert self.next_conf is not None, 'Call get_next first.'
        self.next_conf = None

        self.conf_info = {
            'score': learning_curve[-1],
            'val_curve': learning_curve,
        }

    def create_configuration_to_indices(
        self,
    ) -> Dict[Tuple, int]:
        """
        Maps every configuration to its index as specified
        in hyperparameter_candidates.

        Args:
        -----
        hyperparameter_candidates: np.ndarray
            All the possible hyperparameter candidates given
            by the calling framework.

        Returns:
        ________
        hyperparameter_mapping: dict
            A dictionary where the keys are tuples representing
            hyperparameter configurations and the values are indices
            representing their placement in hyperparameter_candidates.
        """
        hyperparameter_mapping = dict()
        for i in range(0, self.hyperparameter_candidates.shape[0]):
            hyperparameter_mapping[tuple(self.hyperparameter_candidates[i])] = i

        return hyperparameter_mapping

    def map_configuration_to_index(
        self,
        hyperparameter_candidate: List,
    ) -> int:
        """
        Return the index of the hyperparameter_candidate from
        the given initial array of possible hyperparameters.

        Args:
        -----
        hyperparameter_candidate: np.ndarray
            Hyperparameter configuration.

        Returns:
        ________
        index of the hyperparameter_candidate.
        """
        hyperparameter_candidate = tuple(hyperparameter_candidate)

        return self.hyperparameter_mapping[hyperparameter_candidate]
File Path: surrogate_models/power_law_surrogate.py
Content:
from copy import deepcopy
import logging
import os
import time
from typing import List, Tuple

import numpy as np

from scipy.stats import norm
import torch
from torch.utils.data import DataLoader

from data_loader.tabular_data_loader import WrappedDataLoader
from dataset.tabular_dataset import TabularDataset

from models.conditioned_power_law import ConditionedPowerLaw


class PowerLawSurrogate:

    def __init__(
        self,
        hp_candidates: np.ndarray,
        surrogate_configs: dict = None,
        seed: int = 11,
        max_benchmark_epochs: int = 52,
        ensemble_size: int = 5,
        nr_epochs: int = 250,
        fantasize_step: int = 1,
        minimization: bool = True,
        total_budget: int = 1000,
        device: str = None,
        output_path: str = '.',
        dataset_name: str = 'unknown',
        pretrain: bool = False,
        backbone: str = 'power_law',
        max_value: float = 100,
        min_value: float = 0,
        fill_value: str = 'zero',
    ):
        """
        Args:
            hp_candidates: np.ndarray
                The full list of hyperparameter candidates for a given dataset.
            surrogate_configs: dict
                The model configurations for the surrogate.
            seed: int
                The seed that will be used for the surrogate.
            max_benchmark_epochs: int
                The maximal budget that a hyperparameter configuration
                has been evaluated in the benchmark for.
            ensemble_size: int
                The number of members in the ensemble.
            nr_epochs: int
                Number of epochs for which the surrogate should be
                trained.
            fantasize_step: int
                The number of steps for which we are looking ahead to
                evaluate the performance of a hpc.
            minimization: bool
                If for the evaluation metric, the lower the value the better.
            total_budget: int
                The total budget given. Used to calculate the initialization
                percentage.
            device: str
                The device where the experiment will be run on.
            output_path: str
                The path where all the output will be stored.
            dataset_name: str
                The name of the dataset that the experiment will be run on.
            pretrain: bool
                If the surrogate will be pretrained before with a synthetic
                curve.
            backbone: str
                The backbone, which can either be 'power_law' or 'nn'.
            max_value: float
                The maximal value for the dataset.
            min_value: float
                The minimal value for the dataset.
            fill_value: str = 'zero',
                The filling strategy for when learning curves are used.
                Either 'zero' or 'last' where last represents the last value.
        """
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        self.total_budget = total_budget
        self.fill_value = fill_value
        self.max_value = max_value
        self.min_value = min_value
        self.backbone = backbone

        self.pretrained_path = os.path.join(
            output_path,
            'power_law',
            f'checkpoint_{seed}.pth',
        )

        self.model_instances = [
            ConditionedPowerLaw,
            ConditionedPowerLaw,
            ConditionedPowerLaw,
            ConditionedPowerLaw,
            ConditionedPowerLaw,
        ]

        if device is None:
            self.dev = torch.device(
                'cuda') if torch.cuda.is_available() else torch.device('cpu')
        else:
            self.dev = torch.device(device)

        self.learning_rate = 0.001
        self.batch_size = 64
        self.refine_batch_size = 64

        self.criterion = torch.nn.L1Loss()
        self.hp_candidates = hp_candidates

        self.minimization = minimization
        self.seed = seed

        self.logger = logging.getLogger('power_law')
        logging.basicConfig(
            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',
            level=logging.INFO,
            force=True,
        )

        # with what percentage configurations will be taken randomly instead of being sampled from the model
        self.fraction_random_configs = 0.1
        self.iteration_probabilities = np.random.rand(self.total_budget)

        # the keys will be hyperparameter indices while the value
        # will be a list with all the budgets evaluated for examples
        # and with all performances for the performances
        self.examples = dict()
        self.performances = dict()

        # set a seed already, so that it is deterministic when
        # generating the seeds of the ensemble
        torch.manual_seed(seed)
        np.random.seed(seed)

        self.seeds = np.random.choice(100, ensemble_size, replace=False)
        self.max_benchmark_epochs = max_benchmark_epochs
        self.ensemble_size = ensemble_size
        self.nr_epochs = nr_epochs
        self.refine_nr_epochs = 20
        self.fantasize_step = fantasize_step

        self.pretrain = pretrain

        initial_configurations_nr = 1
        conf_individual_budget = 1
        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)
        init_budgets = [i for i in range(1, conf_individual_budget + 1)]

        self.rand_init_conf_indices = []
        self.rand_init_budgets = []

        # basically add every config index up to a certain budget threshold for the initialization
        # we will go through both lists during the initialization
        for config_index in init_conf_indices:
            for config_budget in init_budgets:
                self.rand_init_conf_indices.append(config_index)
                self.rand_init_budgets.append(config_budget)

        self.initial_random_index = 0

        if surrogate_configs is None:

            self.surrogate_configs = []
            for i in range(0, self.ensemble_size):
                self.surrogate_configs.append(
                    {
                        'nr_units': 128,
                        'nr_layers': 2,
                        'kernel_size': 3,
                        'nr_filters': 4,
                        'nr_cnn_layers': 2,
                        'use_learning_curve': False,
                    }
                )
        else:
            self.surrogate_configs = surrogate_configs

        self.nr_features = self.hp_candidates.shape[1]
        self.best_value_observed = np.inf

        self.diverged_configs = set()

        # Where the models of the ensemble will be stored
        self.models = []
        # A tuple which will have the last evaluated point
        # It will be used in the refining process
        # Tuple(config_index, budget, performance, curve)
        self.last_point = None

        self.initial_full_training_trials = 10

        # a flag if the surrogate should be trained
        self.train = True

        # the times it was refined
        self.refine_counter = 0
        # the surrogate iteration counter
        self.iterations_counter = 0
        # info dict to drop every surrogate iteration
        self.info_dict = dict()

        # the start time for the overhead of every surrogate iteration
        # will be recorded here
        self.suggest_time_duration = 0

        self.output_path = output_path
        self.dataset_name = dataset_name

        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)
        self.no_improvement_patience = 0

    def _prepare_dataset(self) -> TabularDataset:
        """This method is called to prepare the necessary training dataset
        for training a model.

        Returns:
            train_dataset: A dataset consisting of examples, labels, budgets
                and learning curves.
        """
        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()

        train_curves = self.prepare_training_curves(train_budgets, train_curves)
        train_examples = np.array(train_examples, dtype=np.single)
        train_labels = np.array(train_labels, dtype=np.single)
        train_budgets = np.array(train_budgets, dtype=np.single)

        # scale budgets to [0, 1]
        train_budgets = train_budgets / self.max_benchmark_epochs

        train_dataset = TabularDataset(
            train_examples,
            train_labels,
            train_budgets,
            train_curves,
        )

        return train_dataset

    def _refine_surrogate(self):
        """Refine the surrogate model.
        """
        for model_index, model_seed in enumerate(self.seeds):

            train_dataset = self._prepare_dataset()
            self.logger.info(f'Started refining model with index: {model_index}')
            refined_model = self.train_pipeline(
                model_index,
                train_dataset,
                nr_epochs=self.refine_nr_epochs,
                refine=True,
                weight_new_example=True,
                batch_size=self.refine_batch_size,
            )

            self.models[model_index] = refined_model

    def _train_surrogate(self, pretrain: bool = False):
        """Train the surrogate model.

        Trains all the models of the ensemble
        with different initializations and different
        data orders.

        Args:
            pretrain: bool
                If we have pretrained weights and we will just
                refine the models.
        """
        for model_index, model_seed in enumerate(self.seeds):
            train_dataset = self._prepare_dataset()
            self.logger.info(f'Started training model with index: {model_index}')

            if pretrain:
                # refine the models that were already pretrained
                trained_model = self.train_pipeline(
                    model_index,
                    train_dataset,
                    nr_epochs=self.refine_nr_epochs,
                    refine=True,
                    weight_new_example=False,
                    batch_size=self.batch_size,
                    early_stopping_it=self.refine_nr_epochs,  # basically no early stopping
                )
                self.models[model_index] = trained_model
            else:
                # train the models for the first time
                trained_model = self.train_pipeline(
                    model_index,
                    train_dataset,
                    nr_epochs=self.nr_epochs,
                    refine=False,
                    weight_new_example=False,
                    batch_size=self.batch_size,
                    early_stopping_it=self.nr_epochs,  # basically no early stopping
                )
                self.models.append(trained_model)

    def train_pipeline(
        self,
        model_index: int,
        train_dataset: TabularDataset,
        nr_epochs: int,
        refine: bool = False,
        weight_new_example: bool = True,
        batch_size: int = 64,
        early_stopping_it: int = 10,
        activate_early_stopping: bool = False,
    ) -> torch.nn.Module:
        """Train an algorithm to predict the performance
        of the hyperparameter configuration based on the budget.

        Args:
            model_index: int
                The index of the model.
            train_dataset: TabularDataset
                The tabular dataset featuring the examples, labels,
                budgets and curves.
            nr_epochs: int
                The number of epochs to train the model for.
            refine: bool
                If an existing model will be refined or if the training
                will start from scratch.
            weight_new_example: bool
                If the last example that was added should be weighted more
                by being included in every batch. This is only applicable
                when refine is True.
            batch_size: int
                The batch size to be used for training.
            early_stopping_it: int
                The early stopping iteration patience.
            activate_early_stopping: bool
                Flag controlling the activation.

        Returns:
            model: torch.nn.Module
                A trained model.
        """
        if model_index == 0:
            self.iterations_counter += 1
            self.logger.info(f'Iteration number: {self.iterations_counter}')

        surrogate_config = self.surrogate_configs[model_index]
        seed = self.seeds[model_index]
        torch.manual_seed(seed)
        np.random.seed(seed)

        if refine:
            model = self.models[model_index]
        else:
            model = self.model_instances[model_index](
                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,
                nr_units=surrogate_config['nr_units'],
                nr_layers=surrogate_config['nr_layers'],
                use_learning_curve=surrogate_config['use_learning_curve'],
                kernel_size=surrogate_config['kernel_size'],
                nr_filters=surrogate_config['nr_filters'],
                nr_cnn_layers=surrogate_config['nr_cnn_layers'],
            )
            model.to(self.dev)

        # make the training dataset here
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
        )
        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)
        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)

        patience_rounds = 0
        best_loss = np.inf
        best_state = deepcopy(model.state_dict())

        for epoch in range(0, nr_epochs):
            running_loss = 0
            model.train()

            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:

                nr_examples_batch = batch_examples.shape[0]
                # if only one example in the batch, skip the batch.
                # Otherwise, the code will fail because of batchnormalization.
                if nr_examples_batch == 1:
                    continue

                # zero the parameter gradients
                optimizer.zero_grad(set_to_none=True)

                # in case we are refining, we add the new example to every
                # batch to give it more importance.
                if refine and weight_new_example:
                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point
                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)
                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])
                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs
                    newp_performance = np.array([newp_performance], dtype=np.single)
                    modified_curve = deepcopy(newp_curve)

                    difference = self.max_benchmark_epochs - len(modified_curve) - 1
                    if difference > 0:
                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)

                    modified_curve = np.array([modified_curve], dtype=np.single)
                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)

                    # add depth dimension to the train_curves array and missing_value_matrix
                    modified_curve = np.expand_dims(modified_curve, 1)
                    newp_missing_values = np.expand_dims(newp_missing_values, 1)
                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)

                    new_example = torch.tensor(new_example, device=self.dev)
                    newp_budget = torch.tensor(newp_budget, device=self.dev)
                    newp_performance = torch.tensor(newp_performance, device=self.dev)
                    modified_curve = torch.tensor(modified_curve, device=self.dev)

                    batch_examples = torch.cat((batch_examples, new_example))
                    batch_budgets = torch.cat((batch_budgets, newp_budget))
                    batch_labels = torch.cat((batch_labels, newp_performance))
                    batch_curves = torch.cat((batch_curves, modified_curve))

                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)
                loss = self.criterion(outputs, batch_labels)
                loss.backward()
                optimizer.step()
                # print statistics
                running_loss += loss.item()

            running_loss = running_loss / len(train_dataloader)
            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')

            if activate_early_stopping:
                if running_loss < best_loss:
                    best_state = deepcopy(model.state_dict())
                    best_loss = running_loss
                    patience_rounds = 0
                elif running_loss > best_loss:
                    patience_rounds += 1
                    if patience_rounds == early_stopping_it:
                        model.load_state_dict(best_state)
                        self.logger.info(f'Stopping training since validation loss is not improving')
                        break

        if activate_early_stopping:
            model.load_state_dict(best_state)

        return model

    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:
        """
        Predict the performances of the hyperparameter configurations
        as well as the standard deviations based on the ensemble.

        Returns:
            mean_predictions, std_predictions, hp_indices, real_budgets:
            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]
                The mean predictions and the standard deviations over
                all model predictions for the given hyperparameter
                configurations with their associated indices and budgets.

        """
        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()
        # scale budgets to [0, 1]
        budgets = np.array(budgets, dtype=np.single)
        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)
        budgets = budgets / self.max_benchmark_epochs
        real_budgets = np.array(real_budgets, dtype=np.single)
        configurations = np.array(configurations, dtype=np.single)

        configurations = torch.tensor(configurations)
        configurations = configurations.to(device=self.dev)
        budgets = torch.tensor(budgets)
        budgets = budgets.to(device=self.dev)
        hp_curves = torch.tensor(hp_curves)
        hp_curves = hp_curves.to(device=self.dev)
        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)
        network_real_budgets.to(device=self.dev)
        all_predictions = []

        for model in self.models:
            model = model.eval()
            predictions = model(configurations, budgets, network_real_budgets, hp_curves)
            all_predictions.append(predictions.detach().cpu().numpy())

        mean_predictions = np.mean(all_predictions, axis=0)
        std_predictions = np.std(all_predictions, axis=0)

        return mean_predictions, std_predictions, hp_indices, real_budgets

    def suggest(self) -> Tuple[int, int]:
        """Suggest a hyperparameter configuration and a budget
        to evaluate.

        Returns:
            suggested_hp_index, budget: Tuple[int, int]
                The index of the hyperparamter configuration to be evaluated
                and the budget for what it is going to be evaluated for.
        """
        suggest_time_start = time.time()

        if self.initial_random_index < len(self.rand_init_conf_indices):
            self.logger.info(
                'Not enough configurations to build a model. \n'
                'Returning randomly sampled configuration'
            )
            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]
            budget = self.rand_init_budgets[self.initial_random_index]
            self.initial_random_index += 1
        else:
            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()
            best_prediction_index = self.find_suggested_config(
                mean_predictions,
                std_predictions,
            )
            # actually do the mapping between the configuration indices and the best prediction
            # index
            suggested_hp_index = hp_indices[best_prediction_index]

            if suggested_hp_index in self.examples:
                evaluated_budgets = self.examples[suggested_hp_index]
                max_budget = max(evaluated_budgets)
                budget = max_budget + self.fantasize_step
                if budget > self.max_benchmark_epochs:
                    budget = self.max_benchmark_epochs
            else:
                budget = self.fantasize_step

        suggest_time_end = time.time()
        self.suggest_time_duration = suggest_time_end - suggest_time_start

        return suggested_hp_index, budget

    def observe(
        self,
        hp_index: int,
        b: int,
        hp_curve: List[float],
    ):
        """Receive information regarding the performance of a hyperparameter
        configuration that was suggested.

        Args:
            hp_index: int
                The index of the evaluated hyperparameter configuration.
            b: int
                The budget for which the hyperparameter configuration was evaluated.
            hp_curve: List
                The performance of the hyperparameter configuration.
        """
        for index, curve_element in enumerate(hp_curve):
            if np.isnan(curve_element):
                self.diverged_configs.add(hp_index)
                # only use the non-nan part of the curve and the corresponding
                # budget to still have the information in the network
                hp_curve = hp_curve[0:index + 1]
                b = index
                break

        if not self.minimization:
            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)
            hp_curve = hp_curve.tolist()

        best_curve_value = min(hp_curve)

        self.examples[hp_index] = np.arange(1, b + 1)
        self.performances[hp_index] = hp_curve

        if self.best_value_observed > best_curve_value:
            self.best_value_observed = best_curve_value
            self.no_improvement_patience = 0
            self.logger.info(f'New Incumbent value found '
                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')
        else:
            self.no_improvement_patience += 1
            if self.no_improvement_patience == self.no_improvement_threshold:
                self.train = True
                self.no_improvement_patience = 0
                self.logger.info(
                    'No improvement in the incumbent value threshold reached, '
                    'restarting training from scratch'
                )

        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0
        if self.initial_random_index >= len(self.rand_init_conf_indices):
            performance = self.performances[hp_index]
            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])

            if self.train:
                # delete the previously stored models
                self.models = []
                if self.pretrain:
                    # TODO Load the pregiven weights.
                    pass

                self._train_surrogate(pretrain=self.pretrain)

                if self.iterations_counter <= self.initial_full_training_trials:
                    self.train = True
                else:
                    self.train = False
            else:
                self.refine_counter += 1
                self._refine_surrogate()

    def prepare_examples(self, hp_indices: List) -> List:
        """
        Prepare the examples to be given to the surrogate model.

        Args:
            hp_indices: List
                The list of hp indices that are already evaluated.

        Returns:
            examples: List
                A list of the hyperparameter configurations.
        """
        examples = []
        for hp_index in hp_indices:
            examples.append(self.hp_candidates[hp_index])

        return examples

    def generate_candidate_configurations(self) -> Tuple[List, List, List, List, List]:
        """Generate candidate configurations that will be
        fantasized upon.

        Returns:
            (configurations, hp_indices, hp_budgets, real_budgets, hp_curves): Tuple
                A tuple of configurations, their indices in the hp list,
                the budgets that they should be fantasized upon, the maximal
                budgets they have been evaluated and their corresponding performance
                curves.
        """
        hp_indices = []
        hp_budgets = []
        hp_curves = []
        real_budgets = []
        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0

        for hp_index in range(0, self.hp_candidates.shape[0]):

            if hp_index in self.examples:
                budgets = self.examples[hp_index]
                # Take the max budget evaluated for a certain hpc
                max_budget = budgets[-1]
                if max_budget == self.max_benchmark_epochs:
                    continue
                real_budgets.append(max_budget)
                learning_curve = self.performances[hp_index]

                hp_curve = learning_curve[0:max_budget-1] if max_budget > 1 else [initial_empty_value]
            else:
                real_budgets.append(1)
                hp_curve = [initial_empty_value]

            hp_indices.append(hp_index)
            hp_budgets.append(self.max_benchmark_epochs)
            hp_curves.append(hp_curve)

        configurations = self.prepare_examples(hp_indices)

        return configurations, hp_indices, hp_budgets, real_budgets, hp_curves

    def history_configurations(self) -> Tuple[List, List, List, List]:
        """
        Generate the configurations, labels, budgets and curves
        based on the history of evaluated configurations.

        Returns:
            (train_examples, train_labels, train_budgets, train_curves): Tuple
                A tuple of examples, labels and budgets for the
                configurations evaluated so far.
        """
        train_examples = []
        train_labels = []
        train_budgets = []
        train_curves = []
        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0

        for hp_index in self.examples:
            budgets = self.examples[hp_index]
            performances = self.performances[hp_index]
            example = self.hp_candidates[hp_index]

            for budget in budgets:
                example_curve = performances[0:budget-1]
                train_examples.append(example)
                train_budgets.append(budget)
                train_labels.append(performances[budget - 1])
                train_curves.append(example_curve if len(example_curve) > 0 else [initial_empty_value])

        return train_examples, train_labels, train_budgets, train_curves

    @staticmethod
    def acq(
        best_values: np.ndarray,
        mean_predictions: np.ndarray,
        std_predictions: np.ndarray,
        explore_factor: float = 0.25,
        acq_choice: str = 'ei',
    ) -> np.ndarray:
        """
        Calculate the acquisition function based on the network predictions.

        Args:
        -----
        best_values: np.ndarray
            An array with the best value for every configuration.
            Depending on the implementation it can be different for every
            configuration.
        mean_predictions: np.ndarray
            The mean values of the model predictions.
        std_predictions: np.ndarray
            The standard deviation values of the model predictions.
        explore_factor: float
            The explore factor, when ucb is used as an acquisition
            function.
        acq_choice: str
            The choice for the acquisition function to use.

        Returns
        -------
        acq_values: np.ndarray
            The values of the acquisition function for every configuration.
        """
        if acq_choice == 'ei':
            z = (np.subtract(best_values, mean_predictions))
            difference = deepcopy(z)
            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]
            zero_std_indicator = np.invert(not_zero_std_indicator)
            z = np.divide(z, std_predictions, where=not_zero_std_indicator)
            np.place(z, zero_std_indicator, 0)
            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))
        elif acq_choice == 'ucb':
            # we are working with error rates so we multiply the mean with -1
            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)
        elif acq_choice == 'thompson':
            acq_values = np.random.normal(mean_predictions, std_predictions)
        else:
            acq_values = mean_predictions

        return acq_values

    def find_suggested_config(
            self,
            mean_predictions: np.ndarray,
            mean_stds: np.ndarray,
    ) -> int:
        """Return the hyperparameter with the highest acq function value.

        Given the mean predictions and mean standard deviations from the DPL
        ensemble for every hyperparameter configuraiton, return the hyperparameter
        configuration that has the highest acquisition function value.

        Args:
            mean_predictions: np.ndarray
                The mean predictions of the ensemble for every hyperparameter
                configuration.
            mean_stds: np.ndarray
                The standard deviation predictions of the ensemble for every
                hyperparameter configuration.

        Returns:
            max_value_index: int
                the index of the maximal value.

        """
        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])
        acq_func_values = self.acq(
            best_values,
            mean_predictions,
            mean_stds,
            acq_choice='ei',
        )

        max_value_index = np.argmax(acq_func_values)

        return max_value_index

    def calculate_fidelity_ymax(self, fidelity: int) -> float:
        """Calculate the incumbent for a certain fidelity level.

        Args:
            fidelity: int
                The given budget fidelity.

        Returns:
            best_value: float
                The incumbent value for a certain fidelity level.
        """
        config_values = []
        for example_index in self.examples.keys():
            try:
                performance = self.performances[example_index][fidelity - 1]
            except IndexError:
                performance = self.performances[example_index][-1]
            config_values.append(performance)

        # lowest error corresponds to best value
        best_value = min(config_values)

        return best_value

    def patch_curves_to_same_length(self, curves: List):
        """
        Patch the given curves to the same length.

        Finds the maximum curve length and patches all
        other curves that are shorter with zeroes.

        Args:
            curves: List
                The hyperparameter curves.
        """
        for curve in curves:
            difference = self.max_benchmark_epochs - len(curve) - 1
            if difference > 0:
                fill_value = [curve[-1]] if self.fill_value == 'last' else [0]
                curve.extend(fill_value * difference)

    def prepare_missing_values_channel(self, budgets: List) -> List:
        """Prepare an additional channel for learning curves.

        The additional channel will represent an existing learning
        curve value with a 1 and a missing learning curve value with
        a 0.

        Args:
            budgets: List
                A list of budgets for every training point.

        Returns:
            missing_value_curves: List
                A list of curves representing existing or missing
                values for the training curves of the training points.
        """
        missing_value_curves = []

        for i in range(len(budgets)):
            budget = budgets[i]
            budget = budget - 1
            budget = int(budget)

            if budget > 0:
                example_curve = [1] * budget
            else:
                example_curve = []

            difference_in_curve = self.max_benchmark_epochs - len(example_curve) - 1
            if difference_in_curve > 0:
                example_curve.extend([0] * difference_in_curve)
            missing_value_curves.append(example_curve)

        return missing_value_curves

    def get_mean_initial_value(self):
        """Returns the mean initial value
        for all hyperparameter configurations in the history so far.

        Returns:
            mean_initial_value: float
                Mean initial value for all hyperparameter configurations
                observed.
        """
        first_values = []
        for performance_curve in self.performances.values():
            first_values.append(performance_curve[0])

        mean_initial_value = np.mean(first_values)

        return mean_initial_value

    def prepare_training_curves(
            self,
            train_budgets: List[int],
            train_curves: List[float]
    ) -> np.ndarray:
        """Prepare the configuration performance curves for training.

        For every configuration training curve, add an extra dimension
        regarding the missing values, as well as extend the curve to have
        a fixed uniform length for all.

        Args:
            train_budgets: List
                A list of the budgets for all training points.
            train_curves: List
                A list of curves that pertain to every training point.

        Returns:
            train_curves: np.ndarray
                The transformed training curves.
        """
        missing_value_matrix = self.prepare_missing_values_channel(train_budgets)
        self.patch_curves_to_same_length(train_curves)
        train_curves = np.array(train_curves, dtype=np.single)
        missing_value_matrix = np.array(missing_value_matrix, dtype=np.single)

        # add depth dimension to the train_curves array and missing_value_matrix
        train_curves = np.expand_dims(train_curves, 1)
        missing_value_matrix = np.expand_dims(missing_value_matrix, 1)
        train_curves = np.concatenate((train_curves, missing_value_matrix), axis=1)

        return train_curves

File Path: surrogate_models/random_search.py
Content:
from typing import List, Tuple

import numpy as np


class RandomOptimizer:
    def __init__(
        self,
        hyperparameter_candidates: np.ndarray,
        max_budget: int = 52,
        seed: int = 0,
        max_nr_trials=1000,
        **kwargs,
    ):
        """
        Wrapper for the Random search algorithm.

        Args:
        -----
        hyperparameter_candidates: np.ndarray
            2d array which contains all possible configurations which can be queried.
        max_budget: int
            The number of max epochs used during the HPO optimization.
        seed: int
            Seed used to reproduce the experiments.
        max_nr_trials: int
            The total runtime budget, given as the number of epochs spent during HPO.
        """
        self.hyperparameter_candidates = hyperparameter_candidates
        self.rng = np.random.RandomState(seed)
        np.random.seed(seed)
        self.evaluated_configurations = set()
        self.max_budget = max_budget
        self.max_trials = max_nr_trials
        self.extra_args = kwargs

    def suggest(self) -> Tuple[int, int]:
        """
        Get information about the next configuration.

        Returns:
        ________
        next_conf, conf_budget: tuple
            A tuple that contains information about the next
            configuration (index in the hyperparameter_candidates it was
            given) and the budget for the hyperparameter to be evaluated
            on.
        """
        possible_candidates = {i for i in range(self.hyperparameter_candidates.shape[0])}
        not_evaluated_candidates = possible_candidates - self.evaluated_configurations
        config_index = np.random.choice(list(not_evaluated_candidates))
        self.evaluated_configurations.add(config_index)

        # if not enough budget to give max fidelity, give max budget
        max_budget = min(self.max_budget, self.max_trials)

        return config_index, max_budget

    def observe(
        self,
        hp_index: int,
        budget: int,
        learning_curve: List[float],
    ):
        """
        Respond regarding the performance of a
        hyperparameter configuration. get_next should
        be called first to retrieve the configuration.

        Args:
        -----
        hp_index: int
            The index of the evaluated hyperparameter configuration.
        budget: int
            The budget for which the hyperparameter configuration was evaluated.
        learning curve: np.ndarray, list
            validation accuracy curve. The last value is the same as the score.
        """
        pass

File Path: test/__init__.py
Content:

File Path: test/benchmark/__init__.py
Content:

File Path: test/benchmark/test_lcbench.py
Content:
import os
import unittest

from benchmarks.lcbench import LCBench


class TestLCBench(unittest.TestCase):

    def setUp(self) -> None:

        project_folder = os.path.expanduser(
            os.path.join(
                '~',
                'Desktop',
                'PhD',
                'Projekte',
                'DeepRegret',
            )
        )

        benchmark_data_path = os.path.join(
            project_folder,
            'lc_bench',
            'results',
            'data_2k.json',
        )

        self.lcbench = LCBench(benchmark_data_path)
        self.dataset_name = 'credit-g'

    def test_load_dataset_names(self):

        dataset_names = [
            'APSFailure', 'Amazon_employee_access', 'Australian',
            'Fashion-MNIST', 'KDDCup09_appetency', 'MiniBooNE',
            'adult', 'airlines', 'albert', 'bank-marketing',
            'blood-transfusion-service-center', 'car', 'christine',
            'cnae-9', 'connect-4', 'covertype', 'credit-g', 'dionis',
            'fabert', 'helena', 'higgs', 'jannis', 'jasmine',
            'jungle_chess_2pcs_raw_endgame_complete', 'kc1', 'kr-vs-kp',
            'mfeat-factors', 'nomao', 'numerai28.6', 'phoneme', 'segment',
            'shuttle', 'sylvine', 'vehicle', 'volkert',
        ]

        self.assertEqual(dataset_names, self.lcbench.dataset_names)

    def test_get_hyperparameter_candidates(self):

        hp_configs = self.lcbench.get_hyperparameter_candidates(self.dataset_name)
        self.assertEqual(hp_configs.shape, (LCBench.nr_hyperparameters, len(LCBench.param_space)))

    def test_get_performance(self):

        hp_index = 0
        self.assertGreaterEqual(
            self.lcbench.get_performance(self.dataset_name, hp_index, LCBench.max_budget),
            self.lcbench.get_performance(self.dataset_name, hp_index, 1),
        )

Output:
{
    "experimental_code": "import torch\nimport torch.nn as nn\n\n\nclass ConditionedPowerLaw(nn.Module):\n\n    def __init__(\n        self,\n        nr_initial_features=10,\n        nr_units=200,\n        nr_layers=3,\n        use_learning_curve: bool = True,\n        kernel_size: int = 3,\n        nr_filters: int = 4,\n        nr_cnn_layers: int = 2,\n    ):\n        \"\"\"\n        Args:\n            nr_initial_features: int\n                The number of features per example.\n            nr_units: int\n                The number of units for every layer.\n            nr_layers: int\n                The number of layers for the neural network.\n            use_learning_curve: bool\n                If the learning curve should be use in the network.\n            kernel_size: int\n                The size of the kernel that is applied in the cnn layer.\n            nr_filters: int\n                The number of filters that are used in the cnn layers.\n            nr_cnn_layers: int\n                The number of cnn layers to be used.\n        \"\"\"\n        super(ConditionedPowerLaw, self).__init__()\n\n        self.use_learning_curve = use_learning_curve\n        self.kernel_size = kernel_size\n        self.nr_filters = nr_filters\n        self.nr_cnn_layers = nr_cnn_layers\n\n        self.act_func = torch.nn.LeakyReLU()\n        self.last_act_func = torch.nn.GLU()\n        self.tan_func = torch.nn.Tanh()\n        self.batch_norm = torch.nn.BatchNorm1d\n\n        layers = []\n        # adding one since we concatenate the features with the budget\n        nr_initial_features = nr_initial_features\n        if self.use_learning_curve:\n            nr_initial_features = nr_initial_features + nr_filters\n\n        layers.append(nn.Linear(nr_initial_features, nr_units))\n        layers.append(self.act_func)\n\n        for i in range(2, nr_layers + 1):\n            layers.append(nn.Linear(nr_units, nr_units))\n            layers.append(self.act_func)\n\n        last_layer = nn.Linear(nr_units, 3)\n        layers.append(last_layer)\n\n        self.layers = torch.nn.Sequential(*layers)\n\n        cnn_part = []\n        if use_learning_curve:\n            cnn_part.append(\n                nn.Conv1d(\n                    in_channels=2,\n                    kernel_size=(self.kernel_size,),\n                    out_channels=self.nr_filters,\n                ),\n            )\n            for i in range(1, self.nr_cnn_layers):\n                cnn_part.append(self.act_func)\n                cnn_part.append(\n                    nn.Conv1d(\n                        in_channels=self.nr_filters,\n                        kernel_size=(self.kernel_size,),\n                        out_channels=self.nr_filters,\n                    ),\n                ),\n            cnn_part.append(nn.AdaptiveAvgPool1d(1))\n\n        self.cnn = nn.Sequential(*cnn_part)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        predict_budgets: torch.Tensor,\n        evaluated_budgets: torch.Tensor,\n        learning_curves: torch.Tensor,\n    ):\n        \"\"\"\n        Args:\n            x: torch.Tensor\n                The examples.\n            predict_budgets: torch.Tensor\n                The budgets for which the performance will be predicted for the\n                hyperparameter configurations.\n            evaluated_budgets: torch.Tensor\n                The budgets for which the hyperparameter configurations have been\n                evaluated so far.\n            learning_curves: torch.Tensor\n                The learning curves for the hyperparameter configurations.\n        \"\"\"\n        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)\n        if self.use_learning_curve:\n            lc_features = self.cnn(learning_curves)\n            # revert the output from the cnn into nr_rows x nr_kernels.\n            lc_features = torch.squeeze(lc_features, 2)\n            x = torch.cat((x, lc_features), dim=1)\n\n        x = self.layers(x)\n        alphas = x[:, 0]\n        betas = x[:, 1]\n        gammas = x[:, 2]\n\n        output = torch.add(\n            alphas,\n            torch.mul(\n                self.last_act_func(torch.cat((betas, betas))),\n                torch.pow(\n                    predict_budgets,\n                    torch.mul(self.last_act_func(torch.cat((gammas, gammas))), -1)\n                )\n            ),\n        )\n\n        return output\n\nimport logging\nimport os\nimport time\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom scipy.stats import norm\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom data_loader.tabular_data_loader import WrappedDataLoader\nfrom dataset.tabular_dataset import TabularDataset\n\n# Assuming ConditionedPowerLaw is imported from models.conditioned_power_law\n# from models.conditioned_power_law import ConditionedPowerLaw\n\nclass PowerLawSurrogate:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        surrogate_configs: dict = None,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        ensemble_size: int = 5,\n        nr_epochs: int = 250,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 1000,\n        device: str = None,\n        output_path: str = '.',\n        dataset_name: str = 'unknown',\n        pretrain: bool = False,\n        backbone: str = 'power_law',\n        max_value: float = 100,\n        min_value: float = 0,\n        fill_value: str = 'zero',\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for a given dataset.\n            surrogate_configs: dict\n                The model configurations for the surrogate.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            ensemble_size: int\n                The number of members in the ensemble.\n            nr_epochs: int\n                Number of epochs for which the surrogate should be\n                trained.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If for the evaluation metric, the lower the value the better.\n            total_budget: int\n                The total budget given. Used to calculate the initialization\n                percentage.\n            device: str\n                The device where the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            pretrain: bool\n                If the surrogate will be pretrained before with a synthetic\n                curve.\n            backbone: str\n                The backbone, which can either be 'power_law' or 'nn'.\n            max_value: float\n                The maximal value for the dataset.\n            min_value: float\n                The minimal value for the dataset.\n            fill_value: str = 'zero',\n                The filling strategy for when learning curves are used.\n                Either 'zero' or 'last' where last represents the last value.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n        self.total_budget = total_budget\n        self.fill_value = fill_value\n        self.max_value = max_value\n        self.min_value = min_value\n        self.backbone = backbone\n\n        self.model_instances = [\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n        ]\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.learning_rate = 0.001\n        self.batch_size = 64\n        self.refine_batch_size = 64\n\n        self.criterion = torch.nn.L1Loss()\n        self.hp_candidates = hp_candidates\n\n        self.minimization = minimization\n        self.seed = seed\n\n        self.logger = logging.getLogger('power_law')\n        logging.basicConfig(\n            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',\n            level=logging.INFO,\n            force=True,\n        )\n\n        self.fraction_random_configs = 0.1\n        self.iteration_probabilities = np.random.rand(self.total_budget)\n\n        self.examples = dict()\n        self.performances = dict()\n\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.seeds = np.random.choice(100, ensemble_size, replace=False)\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.ensemble_size = ensemble_size\n        self.nr_epochs = nr_epochs\n        self.refine_nr_epochs = 20\n        self.fantasize_step = fantasize_step\n\n        self.pretrain = pretrain\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        init_budgets = [i for i in range(1, conf_individual_budget + 1)]\n\n        self.rand_init_conf_indices = []\n        self.rand_init_budgets = []\n\n        for config_index in init_conf_indices:\n            for config_budget in init_budgets:\n                self.rand_init_conf_indices.append(config_index)\n                self.rand_init_budgets.append(config_budget)\n\n        self.initial_random_index = 0\n\n        if surrogate_configs is None:\n\n            self.surrogate_configs = []\n            for i in range(0, self.ensemble_size):\n                self.surrogate_configs.append(\n                    {\n                        'nr_units': 128,\n                        'nr_layers': 2,\n                        'kernel_size': 3,\n                        'nr_filters': 4,\n                        'nr_cnn_layers': 2,\n                        'use_learning_curve': False,\n                    }\n                )\n        else:\n            self.surrogate_configs = surrogate_configs\n\n        self.nr_features = self.hp_candidates.shape[1]\n        self.best_value_observed = np.inf\n\n        self.diverged_configs = set()\n\n        self.models = []\n        self.last_point = None\n\n        self.initial_full_training_trials = 10\n\n        self.train = True\n\n        self.refine_counter = 0\n        self.iterations_counter = 0\n        self.info_dict = dict()\n\n        self.suggest_time_duration = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n    def _prepare_dataset(self) -> TabularDataset:\n        \"\"\"This method is called to prepare the necessary training dataset\n        for training a model.\n\n        Returns:\n            train_dataset: A dataset consisting of examples, labels, budgets\n                and learning curves.\n        \"\"\"\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_curves = self.prepare_training_curves(train_budgets, train_curves)\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_dataset = TabularDataset(\n            train_examples,\n            train_labels,\n            train_budgets,\n            train_curves,\n        )\n\n        return train_dataset\n\n    def _refine_surrogate(self):\n        \"\"\"Refine the surrogate model.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started refining model with index: {model_index}')\n            refined_model = self.train_pipeline(\n                model_index,\n                train_dataset,\n                nr_epochs=self.refine_nr_epochs,\n                refine=True,\n                weight_new_example=True,\n                batch_size=self.refine_batch_size,\n            )\n\n            self.models[model_index] = refined_model\n\n    def _train_surrogate(self, pretrain: bool = False):\n        \"\"\"Train the surrogate model.\n\n        Trains all the models of the ensemble\n        with different initializations and different\n        data orders.\n\n        Args:\n            pretrain: bool\n                If we have pretrained weights and we will just\n                refine the models.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started training model with index: {model_index}')\n\n            if pretrain:\n                # refine the models that were already pretrained\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.refine_nr_epochs,\n                    refine=True,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.refine_nr_epochs, # basically no early stopping\n                )\n                self.models[model_index] = trained_model\n            else:\n                # train the models for the first time\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.nr_epochs,\n                    refine=False,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.nr_epochs, # basically no early stopping\n                )\n                self.models.append(trained_model)\n\n    def train_pipeline(\n        self,\n        model_index: int,\n        train_dataset: TabularDataset,\n        nr_epochs: int,\n        refine: bool = False,\n        weight_new_example: bool = True,\n        batch_size: int = 64,\n        early_stopping_it: int = 10,\n        activate_early_stopping: bool = False,\n    ) -> torch.nn.Module:\n        \"\"\"Train an algorithm to predict the performance\n        of the hyperparameter configuration based on the budget.\n\n        Args:\n            model_index: int\n                The index of the model.\n            train_dataset: TabularDataset\n                The tabular dataset featuring the examples, labels,\n                budgets and curves.\n            nr_epochs: int\n                The number of epochs to train the model for.\n            refine: bool\n                If an existing model will be refined or if the training\n                will start from scratch.\n            weight_new_example: bool\n                If the last example that was added should be weighted more\n                by being included in every batch. This is only applicable\n                when refine is True.\n            batch_size: int\n                The batch size to be used for training.\n            early_stopping_it: int\n                The early stopping iteration patience.\n            activate_early_stopping: bool\n                Flag controlling the activation.\n\n        Returns:\n            model: torch.nn.Module\n                A trained model.\n        \"\"\"\n        if model_index == 0:\n            self.iterations_counter += 1\n            self.logger.info(f'Iteration number: {self.iterations_counter}')\n\n        surrogate_config = self.surrogate_configs[model_index]\n        seed = self.seeds[model_index]\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if refine:\n            model = self.models[model_index]\n        else:\n            model = self.model_instances[model_index](\n                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,\n                nr_units=surrogate_config['nr_units'],\n                nr_layers=surrogate_config['nr_layers'],\n                use_learning_curve=surrogate_config['use_learning_curve'],\n                kernel_size=surrogate_config['kernel_size'],\n                nr_filters=surrogate_config['nr_filters'],\n                nr_cnn_layers=surrogate_config['nr_cnn_layers'],\n            )\n            model.to(self.dev)\n\n        # make the training dataset here\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n        )\n        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)\n        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n\n        patience_rounds = 0\n        best_loss = np.inf\n        from copy import deepcopy\n        best_state = deepcopy(model.state_dict())\n\n        for epoch in range(0, nr_epochs):\n            running_loss = 0\n            model.train()\n\n            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:\n\n                nr_examples_batch = batch_examples.shape[0]\n                if nr_examples_batch == 1:\n                    continue\n\n                optimizer.zero_grad(set_to_none=True)\n\n                # in case we are refining, we add the new example to every\n                # batch to give it more importance.\n                if refine and weight_new_example:\n                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point\n                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)\n                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])\n                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs\n                    newp_performance = np.array([newp_performance], dtype=np.single)\n                    from copy import deepcopy\n                    modified_curve = deepcopy(newp_curve)\n\n                    difference = self.max_benchmark_epochs - len(modified_curve) - 1\n                    if difference > 0:\n                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)\n\n                    modified_curve = np.array([modified_curve], dtype=np.single)\n                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)\n\n                    # add depth dimension to the train_curves array and missing_value_matrix\n                    modified_curve = np.expand_dims(modified_curve, 1)\n                    newp_missing_values = np.expand_dims(newp_missing_values, 1)\n                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)\n\n                    new_example = torch.tensor(new_example, device=self.dev)\n                    newp_budget = torch.tensor(newp_budget, device=self.dev)\n                    newp_performance = torch.tensor(newp_performance, device=self.dev)\n                    modified_curve = torch.tensor(modified_curve, device=self.dev)\n\n                    batch_examples = torch.cat((batch_examples, new_example))\n                    batch_budgets = torch.cat((batch_budgets, newp_budget))\n                    batch_labels = torch.cat((batch_labels, newp_performance))\n                    batch_curves = torch.cat((batch_curves, modified_curve))\n\n                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)\n                loss = self.criterion(outputs, batch_labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n\n            running_loss = running_loss / len(train_dataloader)\n            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')\n\n            if activate_early_stopping:\n                if running_loss < best_loss:\n                    best_state = deepcopy(model.state_dict())\n                    best_loss = running_loss\n                    patience_rounds = 0\n                elif running_loss > best_loss:\n                    patience_rounds += 1\n                    if patience_rounds == early_stopping_it:\n                        model.load_state_dict(best_state)\n                        self.logger.info(f'Stopping training since validation loss is not improving')\n                        break\n\n        if activate_early_stopping:\n            model.load_state_dict(best_state)\n\n        return model\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the ensemble.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, real_budgets:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices and budgets.\n\n        \"\"\"\n        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()\n        # scale budgets to [0, 1]\n        budgets = np.array(budgets, dtype=np.single)\n        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)\n        budgets = budgets / self.max_benchmark_epochs\n        real_budgets = np.array(real_budgets, dtype=np.single)\n        configurations = np.array(configurations, dtype=np.single)\n\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n        hp_curves = torch.tensor(hp_curves)\n        hp_curves = hp_curves.to(device=self.dev)\n        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)\n        network_real_budgets.to(device=self.dev)\n        all_predictions = []\n\n        for model in self.models:\n            model = model.eval()\n            predictions = model(configurations, budgets, network_real_budgets, hp_curves)\n            all_predictions.append(predictions.detach().cpu().numpy())\n\n        mean_predictions = np.mean(all_predictions, axis=0)\n        std_predictions = np.std(all_predictions, axis=0)\n\n        return mean_predictions, std_predictions, hp_indices, real_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"Suggest a hyperparameter configuration and a budget\n        to evaluate.\n\n        Returns:\n            suggested_hp_index, budget: Tuple[int, int]\n                The index of the hyperparamter configuration to be evaluated\n                and the budget for what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n\n        if self.initial_random_index < len(self.rand_init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. \\n'\n                'Returning randomly sampled configuration'\n            )\n            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]\n            budget = self.rand_init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n        else:\n            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n            )\n            # actually do the mapping between the configuration indices and the best prediction\n            # index\n            suggested_hp_index = hp_indices[best_prediction_index]\n\n            if suggested_hp_index in self.examples:\n                evaluated_budgets = self.examples[suggested_hp_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        return suggested_hp_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        hp_curve: List[float],\n    ):\n        \"\"\"Receive information regarding the performance of a hyperparameter\n        configuration that was suggested.\n\n        Args:\n            hp_index: int\n                The index of the evaluated hyperparameter configuration.\n            b: int\n                The budget for which the hyperparameter configuration was evaluated.\n            hp_curve: List\n                The performance of the hyperparameter configuration.\n        \"\"\"\n        for index, curve_element in enumerate(hp_curve):\n            if np.isnan(curve_element):\n                self.diverged_configs.add(hp_index)\n                # only use the non-nan part of the curve and the corresponding\n                # budget to still have the information in the network\n                hp_curve = hp_curve[0:index + 1]\n                b = index\n                break\n\n        from copy import deepcopy\n        if not self.minimization:\n            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)\n            hp_curve = hp_curve.tolist()\n\n        best_curve_value = min(hp_curve)\n\n        self.examples[hp_index] = np.arange(1, b + 1)\n        self.performances[hp_index] = hp_curve\n\n        if self.best_value_observed > best_curve_value:\n            self.best_value_observed = best_curve_value\n            self.no_improvement_patience = 0\n            self.logger.info(f'New Incumbent value found '\n                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')\n        else:\n            self.no_improvement_patience += 1\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.train = True\n                self.no_improvement_patience = 0\n                self.logger.info(\n                    'No improvement in the incumbent value threshold reached, '\n                    'restarting training from scratch'\n                )\n\n        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0\n        if self.initial_random_index >= len(self.rand_init_conf_indices):\n            performance = self.performances[hp_index]\n            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])\n\n            if self.train:\n                # delete the previously stored models\n                self.models = []\n                if self.pretrain:\n                    # TODO Load the pregiven weights.\n                    pass\n\n                self._train_surrogate(pretrain=self.pretrain)\n\n                if self.iterations_counter <= self.initial_full_training_trials:\n                    self.train = True\n                else:\n                    self.train = False\n            else:\n                self.refine_counter += 1\n                self._refine_surrogate()\n\n    @staticmethod\n    def acq(\n        best_values: np.ndarray,\n        mean_predictions: np.ndarray,\n        std_predictions: np.ndarray,\n        explore_factor: float = 0.25,\n        acq_choice: str = 'ei',\n    ) -> np.ndarray:\n        \"\"\"\n        Calculate the acquisition function based on the network predictions.\n\n        Args:\n        -----\n        best_values: np.ndarray\n            An array with the best value for every configuration.\n            Depending on the implementation it can be different for every\n            configuration.\n        mean_predictions: np.ndarray\n            The mean values of the model predictions.\n        std_predictions: np.ndarray\n            The standard deviation values of the model predictions.\n        explore_factor: float\n            The explore factor, when ucb is used as an acquisition\n            function.\n        acq_choice: str\n            The choice for the acquisition function to use.\n\n        Returns\n        -------\n        acq_values: np.ndarray\n            The values of the acquisition function for every configuration.\n        \"\"\"\n        if acq_choice == 'ei':\n            z = (np.subtract(best_values, mean_predictions))\n            difference = deepcopy(z)\n            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]\n            zero_std_indicator = np.invert(not_zero_std_indicator)\n            z = np.divide(z, std_predictions, where=not_zero_std_indicator)\n            np.place(z, zero_std_indicator, 0)\n            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))\n        elif acq_choice == 'ucb':\n            # we are working with error rates so we multiply the mean with -1\n            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)\n        elif acq_choice == 'thompson':\n            acq_values = np.random.normal(mean_predictions, std_predictions)\n        else:\n            acq_values = mean_predictions\n\n        return acq_values\n\n    def find_suggested_config(\n            self,\n            mean_predictions: np.ndarray,\n            mean_stds: np.ndarray,\n    ) -> int:\n        \"\"\"Return the hyperparameter with the highest acq function value.\n\n        Given the mean predictions and mean standard deviations from the DPL\n        ensemble for every hyperparameter configuraiton, return the hyperparameter\n        configuration that has the highest acquisition function value.\n\n        Args:\n            mean_predictions: np.ndarray\n                The mean predictions of the ensemble for every hyperparameter\n                configuration.\n            mean_stds: np.ndarray\n                The standard deviation predictions of the ensemble for every\n                hyperparameter configuration.\n\n        Returns:\n            max_value_index: int\n                the index of the maximal value.\n\n        \"\"\"\n        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])\n        acq_func_values = self.acq(\n            best_values,\n            mean_predictions,\n            mean_stds,\n            acq_choice='ei',\n        )\n\n        max_value_index = np.argmax(acq_func_values)\n\n        return max_value_index\n\n    # other helper methods for PowerLawSurrogate are omitted for brevity",
    "experimental_info": "DPL models learning curves as power law functions: `f̂(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`. The function `g(λ)` is implemented as a neural network named `ConditionedPowerLaw`.\n\n**Neural Network (`ConditionedPowerLaw`) Settings:**\n*   **Architecture:** 2-layer feedforward neural network.\n*   **Units per layer:** 128 units.\n*   **Activation Functions:** Leaky ReLU for hidden layers, GLU non-linearity applied to the `β` and `γ` output units (implicitly handled by `self.last_act_func(torch.cat((betas, betas)))` and `self.last_act_func(torch.cat((gammas, gammas)))` in the forward pass).\n*   **Output Units:** 3 outputs corresponding to `α`, `β`, and `γ` coefficients.\n\n**Ensemble Settings:**\n*   **Ensemble Size (K):** 5 neural networks.\n\n**Training Settings:**\n*   **Loss Function:** L1 loss (`torch.nn.L1Loss`).\n*   **Optimizer:** Adam optimizer (`torch.optim.Adam`).\n*   **Learning Rate:** 0.001.\n*   **Batch Size:** 64 for initial training, 64 for refinement.\n*   **Initial Training Epochs:** 250 epochs.\n*   **Refinement Epochs:** 20 epochs.\n\n**Bayesian Optimization (BO) and Multi-fidelity Strategy:**\n*   **Acquisition Function:** Expected Improvement (EI).\n*   **Budget Step (`b_step`):** 1 epoch. New hyperparameter configurations are incrementally advanced by 1 epoch.\n*   **Total HPO Budget:** 1000 HPO iterations (total number of epochs across all configurations).\n*   **Device:** CPU."
}
