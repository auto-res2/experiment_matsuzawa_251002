
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The study formalizes hyperparameter optimization in federated learning (FL) and adapts standard non-federated algorithms (Random Search, Successive Halving) to form baselines. The core method, FedEx, exploits a connection between FL hyperparameter tuning and NAS weight-sharing. It frames the personalized tuning objective as a single-level empirical risk minimization and applies a stochastic relaxation. FedEx alternates between a standard SGD-like update for model weights and an exponentiated gradient update for a categorical distribution over local hyperparameters. It is applicable to FL methods decomposable into local training (Locc) and aggregation (Aggb). A 'local perturbation' scheme is used for selecting initial hyperparameter configurations to ensure stability. For theoretical analysis, the ARUBA (Average Regret-Upper-Bound Analysis) framework is used to prove guarantees for a FedEx variant tuning client step-sizes in an online convex optimization setting.

# Repository Content
File Path: cifar.py
Content:
import argparse
import json
import os
import pdb
import pickle
import random
import re
import string
import math
from copy import deepcopy
from collections import defaultdict
from glob import glob
import numpy as np
import torch; #torch.backends.cudnn.deterministic = False; 
torch.backends.cudnn.benchmark = True
from torch import nn
from torch import optim
from hyper import wrapped_fedex
from hyper import Server
import torch.nn.functional as F
import torchvision.datasets as datasets
import torchvision.transforms as transforms 



BATCH = 100
SERVER = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-1.0, high=1.0),
                  'momentum': np.random.choice([0.0, 0.9]),
                  'step': 1,
                  'gamma': 1.0 - 10.0 ** np.random.uniform(low=-4.0, high=-2.0),
                  }
CLIENT = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),
                  'momentum': np.random.uniform(low=0.0,high=1.0),
                  'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),
                  'epochs': np.random.choice(np.arange(1, 6)), 
                  'batch': 2 ** np.random.choice(np.arange(3, 8)),
                  'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),
                  'dropout': np.random.uniform(low=0.0, high=0.5),
                  }


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('logdir')
    parser.add_argument('--seed', default=0, type=int, help='random seed')
    parser.add_argument('--debug', default=0, type=int,
                        help='run in DEBUG mode if >0; sets number of clients and batches')

    # wrapper algorithm settings
    parser.add_argument('--rounds', default=800, type=int,
                        help='maximum number of communication rounds')
    parser.add_argument('--total', default=4000, type=int,
                        help='total number of communication rounds')
    parser.add_argument('--rate', default=3, type=int, help='elimination rate')
    parser.add_argument('--elim', default=0, type=int, help='number of elimination rounds')
    parser.add_argument('--eval', default=1, type=int, help='number of evaluation rounds')
    parser.add_argument('--discount', default=0.0, type=float,
                        help='discount factor for computing the validation score of an arm')

    # FedEx settings
    parser.add_argument('--batch', default=10, type=int, help='number of tasks per round')
    parser.add_argument('--configs', default=1, type=int,
                        help='''number of configs to optimize over with FedEx (use 1 for FedAvg):
                                - <-1: sample a random number between 1 and abs(args.configs)
                                - =-1: sample the number of arms given by the wrapper
                                - =0: sample a random number between 1 and the number of arms
                                - >0: sample the provided number, ignoring the number of arms''')
    parser.add_argument('--lr_only', action='store_true', help='tune only learning rate as a hyperparameter')
    parser.add_argument('--eps', default=0.0, type=float, help='multiplicative perturbation to client config, eps=0 is fedavg')
    parser.add_argument('--uniform', action='store_true',
                        help='run FedEx over a product set of single-parameter uniform grids')
    parser.add_argument('--random', action='store_true',
                        help='run FedEx over a product set of single-parameter random grids')
    parser.add_argument('--eta0', default=0.0, type=float,
                        help='FedEx initial step size; if 0.0 uses FedEx default')
    parser.add_argument('--sched', default='aggressive', type=str, help='FedEx step size sched')
    parser.add_argument('--cutoff', default=0.0, type=float,
                        help='stop updating FedEx config distribution if entropy below this cutoff')
    parser.add_argument('--baseline', default=-1.0, type=float,
                        help='''how FedEx computes the baseline:
                                - >=-1.0,<0.0: sample discount factor from [0.0, abs(args.baseline))
                                - =0.0: use the most recent value
                                - >0.0,<1.0: use geometrically discounted mean with this factor
                                - =1.0: use the mean of all values''')
    parser.add_argument('--diff', action='store_true',
                        help='use difference between refine and global as FedEx objective')
    parser.add_argument('--stop', action='store_true',
                        help='stop updating FedEx config distribution after last elimination')

    # evaluation settings
    parser.add_argument('--mle', action='store_true', help='use MLE config at test time')
    parser.add_argument('--loss', action='store_true', help='use loss instead of error')
    parser.add_argument('--eval_global', action='store_true', help='use global error as elimination metric instead of refine')

# data settings
    parser.add_argument('--val', default=0.2, type=float, help='proportion of training set to use for validation')
    parser.add_argument('--num-clients', default=500, type=int, help='number of clients')


    return parser.parse_args()



def file2tensor(fname):

    with open(fname, 'r') as f:
        data = json.load(f)

    X = torch.from_numpy(np.asarray(data['x'])).float()
    Y = torch.from_numpy(np.asarray(data['y'])).long()

    return X, Y

def get_loader(train_idx, test_idx, train_data, test_data, val=0.2):

    data = {}
    m = int((1.-val) * len(train_idx))
    data['train'] = torch.utils.data.DataLoader(train_data, 
                                                sampler=torch.utils.data.SubsetRandomSampler(train_idx[:m]), 
                                                batch_size=m,
                                                shuffle=False,
                                                pin_memory=True)
    data['val'] = torch.utils.data.DataLoader(train_data, 
                                              sampler=torch.utils.data.SubsetRandomSampler(train_idx[m:]), 
                                              batch_size=len(train_idx)-m,
                                              shuffle=False,
                                              pin_memory=True)
    data['test'] = torch.utils.data.DataLoader(test_data, 
                                               sampler=torch.utils.data.SubsetRandomSampler(test_idx), 
                                               batch_size=len(test_idx),
                                               shuffle=False,
                                               pin_memory=True)


    def loader(*args):
        output = []
        for arg in args:
            Xarg, Yarg = next(iter(data[arg]))
            output.append(Xarg.cuda(non_blocking=True))
            output.append(Yarg.cuda(non_blocking=True))
        return output

    return loader


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
                                   nn.Conv2d(3, 32, 3, padding=1),
                                   nn.ReLU(),
                                   nn.MaxPool2d(2),
                                   )
        self.conv2 = nn.Sequential(
                                   nn.Conv2d(32, 64, 3, padding=1),
                                   nn.ReLU(),
                                   nn.MaxPool2d(2),
                                   )
        self.conv3 = nn.Sequential(
                                   nn.Conv2d(64, 64, 3, padding=1),
                                   nn.ReLU(),
                                   nn.MaxPool2d(2),
                                   )
        self.dropout = nn.Dropout(0.0)
        self.fc = nn.Sequential(
                                nn.Linear(1024, 64),
                                nn.ReLU(),
                                )
        self.clf = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc(self.dropout(x.flatten(1)))
        return self.clf(self.dropout(x))

def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):

    if not mu:
        return criterion

    mu *= 0.5
    model0 = [param.data.clone() for param in model.parameters()]

    def objective(*args, **kwargs):

        prox = sum((param-param0).pow(2).sum()
                   for param, param0 in zip(model.parameters(), model0))
        return criterion(*args, **kwargs) + mu * prox

    return objective


def train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):

    optimizer = optim.SGD(model.parameters(), **kwargs)
    criterion = get_prox(model, mu=mu)
    model.dropout.p = dropout
    model.train()
    m = len(Y)
    for e in range(epochs):
        randperm = torch.randperm(m)
        X, Y = X[randperm], Y[randperm]
        for i in range(0, m, batch):
            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]
            pred = model(Xbatch)
            loss = criterion(pred, Ybatch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    model.eval()
    return model


def test_batch(model, X, Y):

    pred = model(X)
    return (Y != pred.argmax(1)).sum().float(), nn.CrossEntropyLoss(reduction='sum')(pred, Y).float()


def test(model, X, Y, batch=BATCH):

    model.eval()
    with torch.no_grad():
        errors, losses = zip(*(test_batch(model, X[i:i+batch], Y[i:i+batch])
                               for i in range(0, len(Y), batch)))
        return float(sum(errors)) / len(Y), float(sum(losses)) / len(Y)


def main():

    args = parse()
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
    train_data = datasets.CIFAR10(root='./data', 
                                  train=True, 
                                  transform=transforms.Compose([
                                                                transforms.RandomHorizontalFlip(),
                                                                transforms.RandomCrop(32, 4),
                                                                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
                                                                transforms.ToTensor(),
                                                                normalize,
                                                                ]), 
                                                                download=True)
    test_data =  datasets.CIFAR10(root='./data', 
                                  train=False, 
                                  transform=transforms.Compose([
                                                                transforms.ToTensor(),
                                                                normalize,
                                                                ]),
                                  download=True)
    train_perm, test_perm = torch.randperm(50000), torch.randperm(10000)
    tasks = [get_loader(train_perm[i:i+50000//args.num_clients], test_perm[j:j+10000//args.num_clients], train_data, test_data, val=args.val)
             for i, j in zip(range(0, 50000, 50000//args.num_clients), range(0, 10000, 10000//args.num_clients))]
    if args.debug:
        tasks = tasks[:args.debug]
        print('DEBUG MODE')

    def local_train(model, X, Y, **kwargs):

        if args.debug:
            return train(model, X[:args.debug*args.batch], Y[:args.debug*args.batch], **kwargs)
        return train(model, X, Y, **kwargs)

    def local_test(model, X, Y, **kwargs):

        return test(model, X, Y, **kwargs)[args.loss]

    def get_server():

        model = CNN()
        return Server(model, tasks, local_train, local_test, batch=args.batch, **SERVER())
    
    def get_client(n_clients=1):
        '''performs local tuning for each hyperparameter'''
        if args.lr_only:
            return [SIMPLE_CLIENT()]

        initial_client = CLIENT()
        client_arr = [initial_client]
        eps = args.eps

        for i in range(n_clients-1):
            other_client = deepcopy(initial_client)
            
            log_lr = np.log10(other_client['lr'])
            other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)
            
            other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)
            
            log_wd = np.log10(other_client['weight_decay'])
            other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)
            
            epochs_range = math.ceil(eps * 4)
            other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)

            log_batch = int(np.log2(other_client['batch']))
            batch_range = math.ceil(eps * 4)
            other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)

            
            log_mu = np.log10(other_client['mu'])
            other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)
            
            other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)

            client_arr.append(other_client)

        return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr

    
    
    print('Tuning',
          'FedAvg' if args.configs == 1 and not (args.uniform or args.random) else 'FedEx',
          'on Cifar10')
    os.makedirs(args.logdir, exist_ok=True)
    with open(os.path.join(args.logdir, 'args.json'), 'w') as f:
        json.dump(vars(args), f, indent=4)
    wrapped_fedex(
                  get_server,
                  get_client,
                  num_configs=args.configs,
                  prod=args.uniform or args.random,
                  stepsize_init=args.eta0 if args.eta0 else 'auto',
                  stepsize_sched=args.sched,
                  cutoff=args.cutoff,
                  baseline_discount=args.baseline,
                  diff=args.diff,
                  mle=args.mle,
                  logdir=args.logdir,
                  val_discount=args.discount,
                  last_stop=args.stop,
                  max_resources=args.rounds,
                  total_resources=args.total,
                  elim_rate=args.rate,
                  num_elim=args.elim,
                  num_eval=args.eval,
                  eval_global=args.eval_global
                  )

if __name__ == '__main__':

    main()


File Path: femnist.py
Content:
import argparse
import json
import os
import pdb
import pickle
import random
import re
import string
import math
from copy import deepcopy
from collections import defaultdict
from glob import glob
import numpy as np
import torch; #torch.backends.cudnn.deterministic = False; 
torch.backends.cudnn.benchmark = True
from torch import nn
from torch import optim
from hyper import wrapped_fedex
from hyper import Server
import torch.nn.functional as F



BATCH = 100
DATA = './data/leaf_femnist_all/'
SERVER = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-1.0, high=1.0),
                  'momentum': np.random.choice([0.0, 0.9]),
                  'step': 1,
                  'gamma': 1.0 - 10.0 ** np.random.uniform(low=-4.0, high=-2.0),
                  }
CLIENT = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),
                  'momentum': np.random.uniform(low=0.0, high=1.0),
                  'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),
                  'epochs': np.random.choice(np.arange(1, 6)),
                  'batch': 2 ** np.random.choice(np.arange(3, 8)),
                  'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),
                  'dropout': np.random.uniform(low=0.0, high=0.5),
                  }


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('logdir')
    parser.add_argument('--seed', default=0, type=int, help='random seed')
    parser.add_argument('--debug', default=0, type=int,
                        help='run in DEBUG mode if >0; sets number of clients and batches')

    # wrapper algorithm settings
    parser.add_argument('--rounds', default=50, type=int,
                        help='maximum number of communication rounds')
    parser.add_argument('--total', default=450, type=int,
                        help='total number of communication rounds')
    parser.add_argument('--rate', default=3, type=int, help='elimination rate')
    parser.add_argument('--elim', default=0, type=int, help='number of elimination rounds')
    parser.add_argument('--eval', default=1, type=int, help='number of evaluation rounds')
    parser.add_argument('--discount', default=0.0, type=float,
                        help='discount factor for computing the validation score of an arm')

    # FedEx settings
    parser.add_argument('--batch', default=10, type=int, help='number of tasks per round')
    parser.add_argument('--configs', default=1, type=int,
                        help='''number of configs to optimize over with FedEx (use 1 for FedAvg):
                                - <-1: sample a random number between 1 and abs(args.configs)
                                - =-1: sample the number of arms given by the wrapper
                                - =0: sample a random number between 1 and the number of arms
                                - >0: sample the provided number, ignoring the number of arms''')
    parser.add_argument('--lr_only', action='store_true', help='tune only learning rate as a hyperparameter')
    parser.add_argument('--eps', default=0.0, type=float, help='multiplicative perturbation to client config, eps=0 is fedavg')
    parser.add_argument('--uniform', action='store_true',
                        help='run FedEx over a product set of single-parameter uniform grids')
    parser.add_argument('--random', action='store_true',
                        help='run FedEx over a product set of single-parameter random grids')
    parser.add_argument('--eta0', default=0.0, type=float,
                        help='FedEx initial step size; if 0.0 uses FedEx default')
    parser.add_argument('--sched', default='aggressive', type=str, help='FedEx step size sched')
    parser.add_argument('--cutoff', default=0.0, type=float,
                        help='stop updating FedEx config distribution if entropy below this cutoff')
    parser.add_argument('--baseline', default=-1.0, type=float,
                        help='''how FedEx computes the baseline:
                                - >=-1.0,<0.0: sample discount factor from [0.0, abs(args.baseline))
                                - =0.0: use the most recent value
                                - >0.0,<1.0: use geometrically discounted mean with this factor
                                - =1.0: use the mean of all values''')
    parser.add_argument('--diff', action='store_true',
                        help='use difference between refine and global as FedEx objective')
    parser.add_argument('--stop', action='store_true',
                        help='stop updating FedEx config distribution after last elimination')

    # evaluation settings
    parser.add_argument('--mle', action='store_true', help='use MLE config at test time')
    parser.add_argument('--loss', action='store_true', help='use loss instead of error')
    parser.add_argument('--eval_global', action='store_true', help='use global error as elimination metric instead of refine')

    # data settings
    parser.add_argument('--iid', action='store_true', help='client data is iid')
    parser.add_argument('--val', default=0.1, type=float, help='val proportion')
    parser.add_argument('--test', default=0.1, type=float, help='test proportion')


    return parser.parse_args()



def file2tensor(fname):

    with open(fname, 'r') as f:
        data = json.load(f)

    X = torch.from_numpy(np.asarray(data['x'])).float()
    Y = torch.from_numpy(np.asarray(data['y'])).long()

    return X, Y


def get_loader(task, val=0.1, test=0.1, iid=False):
    m = len(torch.load(task)['Y'])
    if m < 1.0 / min(1.0-val-test, val, test):
        return None
    partitions = {'train': (0, int((1.0-val-test)*m)), 
                  'val': (int((1.0-val-test)*m), int((1.0-test)*m)), 
                  'test': (int((1.0-test)*m), m)}
    if iid:
        randperm = torch.randperm(m)
    dump = torch.load(task)
    X, Y = (dump['X'][randperm], dump['Y'][randperm]) if iid else (dump['X'], dump['Y'])
    data = {key: (X[a:b].pin_memory(), Y[a:b].pin_memory()) for key, (a, b) in partitions.items()}
    def loader(*args):
        output = []
        for arg in args:
            Xarg, Yarg = data[arg]
            output.append(Xarg.cuda(non_blocking=True))
            output.append(Yarg.cuda(non_blocking=True))
        return output
    return loader



class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout = nn.Dropout2d(0.0)
        self.fc1 = nn.Linear(9216, 1024)
        self.fc2 = nn.Linear(1024, 62)

    def forward(self, x):
        x = x.view(-1, 1, 28, 28)
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x





def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):

    if not mu:
        return criterion

    mu *= 0.5
    model0 = [param.data.clone() for param in model.parameters()]

    def objective(*args, **kwargs):

        prox = sum((param-param0).pow(2).sum()
                   for param, param0 in zip(model.parameters(), model0))
        return criterion(*args, **kwargs) + mu * prox

    return objective



def train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):

    optimizer = optim.SGD(model.parameters(), **kwargs)
    criterion = get_prox(model, mu=mu)
    model.dropout.p = dropout
    model.train()
    m = len(Y)
    for e in range(epochs):
        randperm = torch.randperm(m)
        X, Y = X[randperm], Y[randperm]
        for i in range(0, m, batch):
            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]
            pred = model(Xbatch)
            loss = criterion(pred, Ybatch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    model.eval()
    return model


def test_batch(model, X, Y):

    pred = model(X)
    return (Y != pred.argmax(1)).sum().float(), nn.CrossEntropyLoss(reduction='sum')(pred, Y).float()


def test(model, X, Y, batch=BATCH):

    model.eval()
    with torch.no_grad():
        errors, losses = zip(*(test_batch(model, X[i:i+batch], Y[i:i+batch])
                               for i in range(0, len(Y), batch)))
        return float(sum(errors)) / len(Y), float(sum(losses)) / len(Y)


def main():

    args = parse()
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)


    tasks = []
    for i, fname in enumerate(glob(os.path.join(DATA, '*.json'))):
        task = os.path.join(DATA, fname.split('/')[-1][:-5] + '.pt')
        if not os.path.isfile(task):
            print('\rCaching task', i+1, end='')
            dump={}
            dump['X'], dump['Y'] = file2tensor(fname)
            torch.save(dump, task)
        loader = get_loader(task, val=args.val, test=args.test, iid=args.iid)
        if not loader is None:
            tasks.append(loader)
    if args.debug:
        tasks = tasks[:args.debug]
        print('DEBUG MODE')
    print('\rCompleted cache-check')

    def local_train(model, X, Y, **kwargs):

        if args.debug:
            return train(model, X[:args.debug*args.batch], Y[:args.debug*args.batch], **kwargs)
        return train(model, X, Y, **kwargs)

    def local_test(model, X, Y, **kwargs):

        return test(model, X, Y, **kwargs)[args.loss]

    def get_server():

        model = CNN()
        return Server(model, tasks, local_train, local_test, batch=args.batch, **SERVER())
    
    def get_client(n_clients=1):
        '''performs local tuning for each hyperparameter'''
        if args.lr_only:
            return [SIMPLE_CLIENT()]

        initial_client = CLIENT()
        client_arr = [initial_client]
        eps = args.eps

        for i in range(n_clients-1):
            other_client = deepcopy(initial_client)
            
            log_lr = np.log10(other_client['lr'])
            other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(-eps * 4, eps * 4), -4.0, 0.0)
            
            other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)
            
            log_wd = np.log10(other_client['weight_decay'])
            other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(-eps*4, eps*4),-5.0, -1.0)

            epochs_range = math.ceil(eps * 4)
            other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)

            log_batch = int(np.log2(other_client['batch']))
            batch_range = math.ceil(eps * 4)
            other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)

            log_mu = np.log10(other_client['mu'])
            other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(-eps*5, eps*5), -5.0 , 0.0)
            
            other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(-eps*0.5, eps*0.5),0, 0.5)

            client_arr.append(other_client)

        return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr

    
    
    
    print('Tuning',
          'FedAvg' if args.configs == 1 and not (args.uniform or args.random) else 'FedEx',
          'on Femnist')
    os.makedirs(args.logdir, exist_ok=True)
    with open(os.path.join(args.logdir, 'args.json'), 'w') as f:
        json.dump(vars(args), f, indent=4)
    wrapped_fedex(
                  get_server,
                  get_client,
                  num_configs=args.configs,
                  prod=args.uniform or args.random,
                  stepsize_init=args.eta0 if args.eta0 else 'auto',
                  stepsize_sched=args.sched,
                  cutoff=args.cutoff,
                  baseline_discount=args.baseline,
                  diff=args.diff,
                  mle=args.mle,
                  logdir=args.logdir,
                  val_discount=args.discount,
                  last_stop=args.stop,
                  max_resources=args.rounds,
                  total_resources=args.total,
                  elim_rate=args.rate,
                  num_elim=args.elim,
                  num_eval=args.eval,
                  eval_global=args.eval_global,
                  )

if __name__ == '__main__':

    main()

File Path: hyper.py
Content:

import argparse
import os
import pdb
import pickle
import random
from copy import deepcopy
from glob import glob
from heapq import nsmallest
from itertools import product
from math import ceil
from math import log
from operator import itemgetter
import numpy as np
import torch
from numpy.linalg import norm
from scipy.special import logsumexp
from tensorboardX import SummaryWriter
from torch import optim


def discounted_mean(trace, factor=1.0):

    weight = factor ** np.flip(np.arange(len(trace)), axis=0)

    return np.inner(trace, weight) / weight.sum()


class FedEx:
    '''runs hyperparameter optimization given a federated learning server'''

    def entropy(self):

        entropy = 0.0
        for probs in product(*(theta[theta>0.0] for theta in self._theta)):
            prob = np.prod(probs)
            entropy -= prob * np.log(prob)
        return entropy

    def mle(self):
    
        return np.prod([theta.max() for theta in self._theta])

    def __init__(
                 self, 
                 server, 
                 configs, 
                 eta0='auto', 
                 sched='auto', 
                 cutoff=0.0, 
                 baseline=0.0, 
                 diff=False,
                 ):
        '''
        Args:
            server: Object that implements two methods, 'communication_round' and 'full_evaluation'
                    taking as input a single argument, 'get_config', itself a function that takes 
                    no inputs and outputs an element of the provided list 'configs'. 
                    - 'communication_round' samples a batch of clients, assigns a config to each 
                    using 'get_config', and runs local training using that config. It then 
                    aggregates the local models to to take a training step and returns three lists 
                    or arrays: a list of each client's validation error before local training, a 
                    list of each client's validation error after local training, and a list of each 
                    client's weight (e.g. size of its validation set). 
                    - 'full_evaluation' assigns a config to each client using 'get_config' and runs
                    local training using that config. It then returns three lists or arrays: a list
                    of each client's test error before local training, a list of each client's test
                    error after local training, and a list of each client's weight (e.g. size of 
                    its test set).
            configs: list of configs used for local training and testing by 'server' 
                     OR dict of (string, list) pairs denoting a grid of configs
            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))
            sched: learning rate schedule for exponentiated gradient:
                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)
                    - 'aggressive': uses eta0 / gradient l-infinity norm
                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds
                    - 'constant': uses eta0
                    - 'scale': uses sched * sqrt(2 * log(len(configs)))
            cutoff: entropy level below which to stop updating the config probability and use MLE
            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean
            diff: if True uses performance difference; otherwise uses absolute performance
        '''

        self._server = server
        self._configs = configs
        self._grid = [] if type(configs) == list else sorted(configs.keys())

        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]
        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]
        self._sched = sched
        self._cutoff = cutoff
        self._baseline = baseline
        self._diff = diff
        self._z = [np.full(size, -np.log(size)) for size in sizes]
        self._theta = [np.exp(z) for z in self._z]

        self._store = [0.0 for _ in sizes]
        self._stopped = False
        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}

    def stop(self):

        self._stopped = True

    def sample(self, mle=False, _index=[]):
        '''samples from configs using current probability vector'''

        if mle or self._stopped:
            if self._grid:
                return {param: self._configs[param][theta.argmax()] 
                        for theta, param in zip(self._theta, self._grid)}
            return self._configs[self._theta[0].argmax()]
        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])

        if self._grid:
            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}
        return self._configs[_index[-1][0]]

    def settings(self):
        '''returns FedEx input settings'''

        output = {'configs': deepcopy(self._configs)}
        output['eta0'], output['sched'] = self._eta0, self._sched
        output['cutoff'], output['baseline'] = self._cutoff, self._baseline 
        if self._trace['refine']:
            output['theta'] = self.theta()
        return output

    def step(self):
        '''takes exponentiated gradient step (calls 'communication_round' once)'''

        index = []
        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))        
        before, after = np.array(before), np.array(after)
        weight = np.array(weight, dtype=np.float64) / sum(weight)

        if self._trace['refine']:
            trace = self.trace('refine')
            if self._diff:
                trace -= self.trace('global')
            baseline = discounted_mean(trace, self._baseline)
        else:
            baseline = 0.0
        self._trace['global'].append(np.inner(before, weight))
        self._trace['refine'].append(np.inner(after, weight))
        if not index:
            self._trace['entropy'].append(0.0)
            self._trace['mle'].append(1.0)
            return

        for i, (z, theta) in enumerate(zip(self._z, self._theta)):
            grad = np.zeros(len(z))
            for idx, s, w in zip(index, after-before if self._diff else after, weight):
                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]
            if self._sched == 'adaptive':
                self._store[i] += norm(grad, float('inf')) ** 2
                denom = np.sqrt(self._store[i])
            elif self._sched == 'aggressive':
                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))
            elif self._sched == 'auto':
                self._store[i] += 1.0
                denom = np.sqrt(self._store[i])
            elif self._sched == 'constant':
                denom = 1.0
            elif self._sched == 'scale':
                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')
            else:
                raise NotImplementedError
            eta = self._eta0[i] / denom
            z -= eta * grad
            z -= logsumexp(z)
            self._theta[i] = np.exp(z)

        self._trace['entropy'].append(self.entropy())
        self._trace['mle'].append(self.mle())
        if self._trace['entropy'][-1] < self._cutoff:
            self.stop()

    def test(self, mle=False):
        '''evaluates found config (calls 'full_evaluation' once)
        Args:
            mle: use MLE config instead of sampling
        Returns:
            output of 'full_evaluation'
        '''

        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))
        return {'global': np.inner(before, weight) / weight.sum(),
                'refine': np.inner(after, weight) / weight.sum()}

    def theta(self):
        '''returns copy of config probability vector'''

        return deepcopy(self._theta)

    def trace(self, key):
        '''returns trace of one of three tracked quantities
        Args:
            key: 'entropy', 'global', or 'refine'
        Returns:
            numpy vector with length equal to number of calls to 'step'
        '''

        return np.array(self._trace[key])


def frac(p, q):

    return str(p) + '/' + str(q)


class Server:
    '''object for federated training implementing methods required by FedEx'''

    def _set_test_state(self):

        state = (np.random.get_state(), torch.get_rng_state(), torch.cuda.get_rng_state())
        if self._state is None:
            self._state = state
        else:
            np.random.set_state(self._state[0])
            torch.set_rng_state(self._state[1])
            torch.cuda.set_rng_state(self._state[2])
        return state

    def _reset_state(self, state):

        np.random.set_state(state[0])
        torch.set_rng_state(state[1])
        torch.cuda.set_rng_state(state[2])

    def __init__(
                 self, 
                 model, 
                 clients, 
                 train, 
                 test, 
                 lr=1.0, 
                 momentum=0.0, 
                 step=1, 
                 gamma=1.0, 
                 batch=10,
                 state=None,
                 ):
        '''
        Args:
            model: PyTorch model
            clients: list of clients, each a function that takes one or more strings 'train',
                     'val', 'test' and returns, as one tuple, input and output tensors for each
            train: method that takes as argument a PyTorch model, an input tensor, an output
                   tensor, and optional kwargs and returns the same PyTorch model
            test: method that takes as argument a PyTorch model, an input tensor, and an output
                  tensor and returns the model's error
            lr: server learning rate
            momentum: server momentum
            step: server learning rate decay interval
            gamma: server learning rate decay factor
            batch: number of clients to sample per communication round
            state: np.random, torch, torch.cuda random state tuple; if None uses current states
        '''

        self._model = model
        self._clients = clients
        self._train = train
        self._test = test
        self._opt = optim.SGD(self._model.parameters(), lr=lr, momentum=momentum)
        self._sched = optim.lr_scheduler.StepLR(self._opt, step, gamma=gamma)
        self._batch = batch
        self._state = state
        self._reset_state(self._set_test_state())

    def communication_round(self, get_config):
        '''runs one step of local training and model aggregation
        Args:
            get_config: returns kwargs for 'train' as a dict
        Returns:
            np.array objects for global val error, local val error, and val size of each client
        '''

        self._model.cuda()
        before, after, weight = [np.zeros(self._batch) for _ in range(3)]
        total = 0.0

        for i in range(self._batch):
            Xtrain, Ytrain, Xval, Yval = random.choice(self._clients)('train', 'val')
            before[i] = self._test(self._model, Xval, Yval)
            model = self._train(deepcopy(self._model), Xtrain, Ytrain, **get_config())
            after[i] = self._test(model, Xval, Yval)
            weight[i] = len(Yval)
            total += len(Ytrain)
            if i:
                for agg, param in zip(aggregate.parameters(), model.parameters()):
                    agg.data += len(Ytrain) * param.data
            else:
                for param in model.parameters():
                    param.data *= len(Ytrain)
                aggregate = model

        self._opt.zero_grad()
        for agg, param in zip(aggregate.parameters(), self._model.parameters()):
            param.grad = param.data - agg / total
        self._opt.step()
        self._opt.zero_grad()
        self._sched.step()
        self._model.cpu()
        return before, after, weight

    def full_evaluation(self, get_config):
        '''evaluates personalization on each client
        Args:
            get_config: returns kwargs for 'train' as a dict
        Returns:
            np.array objects for global test error, local test error, and test size of each client
        '''

        state = self._set_test_state()
        self._model.cuda()
        before, after, weight = [np.zeros(len(self._clients)) for _ in range(3)]
        for i, client in enumerate(self._clients):
            Xtrain, Ytrain, Xtest, Ytest = client('train', 'test')
            before[i] = self._test(self._model, Xtest, Ytest)
            after[i] = self._test(self._train(deepcopy(self._model), 
                                              Xtrain, Ytrain, **get_config()), 
                                  Xtest, Ytest)
            weight[i] = len(Ytest)
            print('\r\tEvaluated client', frac(i+1, len(self._clients)),
                  '    global error:', round(np.inner(before, weight) / weight.sum(), 4),
                  '    refine error:', round(np.inner(after, weight) / weight.sum(), 4), 
                  end=32*' ')
        self._model.cpu()
        self._reset_state(state)
        return before, after, weight


def random_search(max_resources=500, total_resources=2000):
    '''returns a random search rate and schedule for use by 'successive_elimination'
    Args:
        max_resources: most resources (steps) assigned to single arm
        total_resources: overall resource limit
    Returns:
        elimination rate as an int, elimination schedule as a list
    '''

    assert max_resources > 0, "max_resources must be positive"
    return int(total_resources / max_resources), [max_resources]


def get_schedule(
                 max_resources=500, 
                 total_resources=2000, 
                 elim_rate=3, 
                 num_elim=0, 
                 num_eval=1,
                 ):
    '''returns rate and schedule for use by 'successive_elimination'
    Args:
        max_resources: most resources (steps) assigned to single arm
        total_resources: overall resource limit
        elim_rate: multiplicative elimination rate
        num_elim: number of elimination rounds; if 0 runs random search
        num_eval: number of evaluation rounds
    Returns:
        elimination rate as an int, elimination schedule as a list, evaluation schedule as a list
    '''

    assert max_resources <= total_resources, "max_resources cannot be greater than total_resources"
    assert elim_rate > 1, "elim_rate must be greater than 1"
    assert num_eval <= total_resources, "num_eval cannot be greater than total_resources"

    if num_elim:
        diff = total_resources - max_resources
        geos = (elim_rate**(num_elim+1) - 1) / (elim_rate-1)
        u = int(diff / (geos-num_elim-1))
        resources = 0
        v = lambda i: 1 + ceil((diff+(num_elim-geos+elim_rate**i)*u) / (elim_rate**i-1))
        for opt in product(*(range(u, v(i)) for i in reversed(range(1, num_elim+1)))):
            used = max_resources + sum((elim_rate**i-1)*r 
                                       for i, r in zip(reversed(range(1, num_elim+1)), opt))
            if resources <= used <= total_resources:
                best, resources = opt, used
        assert not 0 in best, "invalid: use more resources or fewer eliminations, or increase rate"
        elim_sched = list(np.cumsum(best)) + [max_resources]
    else:
        elim_rate, elim_sched = random_search(max_resources=max_resources, 
                                              total_resources=total_resources)

    eval_sched = [int(step) for step in np.linspace(0, total_resources, num_eval+1)[1:]]
    return elim_rate, elim_sched, eval_sched



def successive_elimination(
                           sampler, 
                           eval_traces, 
                           logdir=None, 
                           val_discount=0.0, 
                           elim_rate=3, 
                           elim_sched=[1], 
                           eval_sched=[-1], 
                           traces=[], 
                           last_round=None,
                           eval_global=False,
                           **kwargs,
                           ):
    '''runs successive elimination according to provided schedule
    Args:
        sampler: function of n returning an iterable of n objects with methods 'step' and 'trace'
        eval_traces: list of strings of traces measuring performance; element 0 used for elimination
        logdir: directory to store tensorboard logs; if None does not log anything
        val_discount: discount factor when computing score for a trace; 0.0 is most recent, 1.0 is mean
        elim_rate: multiplicative elimination rate
        elim_sched: list of steps at which to run an elimination
        eval_sched: list of steps at which to call 'test' method of the best config
        traces: list of strings of traces to collect
        last_round: str name of function that last config executes before final round
        kwargs: passed to 'test' method of each config
    Returns:
        best config;
        also dumps tensorboard logs and results.pkl to folder 'logdir', if specified
    '''
    assert len(elim_sched) > 0, "'elim_sched' must be a list of positive length"
    assert type(elim_rate) == int, "'elim_rate' must be an int"
    logger = False if logdir is None else SummaryWriter(logdir)
    traces, eval_sched = deepcopy(traces), deepcopy(eval_sched)
    for trace in eval_traces:
        if not trace in traces:
            traces.append(trace)
    #each config is (index, config_settings())
    configs = list(enumerate(sampler(elim_rate ** max(1, len(elim_sched)-1))))
    output = {index: {'settings': config.settings()} for index, config in configs}
    for trace in eval_traces:
        output[trace+' val'] = []
        output[trace+' test'] = []
    output['eval step'] = []
    changed = {index: True for index, _ in configs}

    #evaluate fedex object with best score across all time
    best_score = 100.0
    best_config = None 
    best_config_idx = None

    start, last_start, used = 0, 0, 0
    for i, stop in enumerate(elim_sched):
        if len(configs) == 1 and not last_round is None:
            getattr(configs[0][1], last_round)()
        scores = []
        for j, (index, config) in enumerate(configs):
            scores.append(float('inf'))
            for k in range(start, stop):
                changed[index] = True
                print('\r\tRound', frac(i+1, len(elim_sched)), 
                      'config', frac(j+1, len(configs)), 
                      'step', frac(k+1, stop), end=4*' ')
                config.step()
                for trace in traces:
                    output[index][trace] = config.trace(trace)
                    print(trace+':', round(output[index][trace][-1], 4), end=4*' ')
                    if logger:
                        logger.add_scalars(trace, {str(index): output[index][trace][-1]}, k+1)
                #use refine error, if eval_global use global error
                if eval_global:
                    scores[-1] = discounted_mean(output[index][eval_traces[1]][start:],val_discount)
                else:
                    scores[-1] = discounted_mean(output[index][eval_traces[0]][start:], val_discount)
                used += 1
                current_best, score = min(enumerate(scores), key=itemgetter(1))

                #check if val score beats best score so far
                if score < best_score: 
                    best_config_idx = configs[current_best][0]
                    best_config = deepcopy(configs[current_best][1])
                    best_score = score 

                print('best:', round(best_score, 4), end=8*' ')

                for trace in eval_traces:
                    if len(output[best_config_idx][trace][start:])==0:
                        val = discounted_mean(output[best_config_idx][trace][last_start:], val_discount)
                    else:
                        val = discounted_mean(output[best_config_idx][trace][start:], val_discount)
                    output[trace+' val'].append(val)
                    if logger:
                        logger.add_scalar(trace+' val', val, used)
                if used in eval_sched:
                    if changed[best_config_idx]:
                        results = best_config.test(**kwargs)
                        changed[best_config_idx] = False
                    print('\r\tStep', used, 'test error', end='')
                    for trace in reversed(eval_traces):
                        output[trace+' test'].append(results[trace])
                        if logger:
                            logger.add_scalar(trace+' test', results[trace], used)
                        print('    '+trace, round(results[trace], 4), end='')
                    print(64*' ')
                    output['eval step'].append(eval_sched.pop(eval_sched.index(used)))
        if len(configs) == 1:
            break

        #select top n configs, index of each config is preserved
        _, configs = zip(*nsmallest(int(len(configs) / elim_rate), 
                                    zip(scores, configs), 
                                    key=itemgetter(0)))
        last_start = start 
        start = stop

    #best, config = configs[0]
    best, config = best_config_idx, best_config 
    output['best'], output[best]['settings'] = best, config.settings()
    if eval_sched:
        if changed[best]:
            results = config.test(**kwargs)
        print('\r\tStep', used, 'test error', end='')
        for trace in reversed(eval_traces):
            output[trace+' test'].append(results[trace])
            if logger:
                logger.add_scalar(trace+' test', results[trace], used)
            print('    '+trace, round(results[trace], 4), end='')
        print(64*' ')
        output['eval step'].append(used)

    if logger:
        with open(os.path.join(logdir, 'results.pkl'), 'wb') as f:
            pickle.dump(output, f)
        try:
            logger.flush()
        except AttributeError:
            pass
    return config



def wrapped_fedex(
                  get_server,
                  get_client,
                  num_configs=1,
                  prod=False,
                  stepsize_init='auto', 
                  stepsize_sched='aggressive', 
                  cutoff=1E-4, 
                  baseline_discount=-1.0, 
                  diff=False,
                  mle=False, 
                  logdir=None,
                  val_discount=0.0, 
                  last_stop=False,
                  eval_global=False,
                  **kwargs,
                  ):
    '''evaluates FedEx wrapped with successive elimination algorithm;
       uses FedAvg when num_configs = 1 and prod = False
    Args:
        get_server: function that takes no input and returns an object that can be passed as the 
                    first argument to FedEx.__init__, e.g. a Server object
        get_client: function that takes no input and returns a dict of local training configs, a
                    list of which is passed as the second argument to 'FedEx.__init__'; can also
                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'
        num_configs: determines number of configs in the list passed to 'FedEx.__init__':
                     - >0: use this value directly
                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]
                     - =-1: use the number of arms given by the wrapper
                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}
        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case
                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'
        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'
        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'
        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':
                           - >0.0: use this value directly
                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]
        diff: passed to 'diff' kwarg of 'FedEx.__init__'
        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'
        logdir: passed to 'logdir' kwarg of 'successive_elimination'
        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'
        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'
        kwargs: passed to 'get_schedule'
    Returns:
        FedEx object
    '''

    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)
    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')

    if num_configs < -1:
        samples = lambda n: random.randint(1, -num_configs)
    elif num_configs == -1:
        samples = lambda n: n
    elif num_configs == 0:
        samples = lambda n: random.randint(1, n)
    else:
        samples = lambda n: num_configs

    if baseline_discount < 0.0:
        baseline = lambda: random.uniform(0.0, -baseline_discount)
    else:
        baseline = lambda: baseline_discount

    def sampler(n):

        for _ in range(n):
            yield FedEx(
                        get_server(), 
                        get_client() if prod else get_client(samples(n)),
                        eta0=stepsize_init, 
                        sched=stepsize_sched, 
                        cutoff=cutoff, 
                        baseline=baseline(),
                        diff=diff,
                        )

    return successive_elimination(
                                  sampler, 
                                  ['refine', 'global'], 
                                  logdir=logdir, 
                                  val_discount=val_discount,
                                  elim_rate=elim_rate, 
                                  elim_sched=elim_sched, 
                                  eval_sched=eval_sched,
                                  traces=['entropy', 'mle', 'global', 'refine'], 
                                  last_round='stop' if last_stop else None,
                                  mle=mle,
                                  eval_global=eval_global,
                                  )


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('--input', default='*', help='parent directory of input logdirs')
    parser.add_argument('--output', default='.', help='output directory for tensorboard log')
    return parser.parse_args()


if __name__ == '__main__':

    args = parse()
    results = {}
    for fname in glob(os.path.join(args.input, '*/results.pkl')):
        with open(fname, 'rb') as f:
            key = '/'.join(fname.split('/')[:-1])
            results[key] = pickle.load(f)
    
    logger = SummaryWriter(args.output)
    for mode in ['global', 'refine']:
        for partition in ['val', 'test']:
            trace = mode + ' ' + partition
            for j, scores in enumerate(zip(*(val[trace] for val in results.values()))):
                step = j+1 if partition == 'val' else results[key]['eval step'][j]
                logger.add_scalar('avg '+trace+' error', np.mean(scores), step)
                logger.add_scalar('std '+trace+' error', np.std(scores), step)
                logger.add_histogram(trace+' error', np.array(scores), step)
            if partition == 'test':
                print('Average final '+trace+' error:', np.mean(scores))
                print('Standard deviation', np.std(scores))
    try:
        logger.flush()
    except AttributeError:
        pass

File Path: shakespeare.py
Content:
import argparse
import json
import os
import pdb
import pickle
import random
import re
import string
from collections import defaultdict
from glob import glob
from copy import deepcopy
import numpy as np
import torch; #torch.backends.cudnn.deterministic = False; 
torch.backends.cudnn.benchmark = True
from torch import nn
from torch import optim
from torch.nn.utils import rnn
from hyper import wrapped_fedex
from hyper import Server


BATCH = 100
CHARMAP = defaultdict(lambda: 1)
CHARMAP.update({char: i+2 for i, char in enumerate(string.printable)})
VOCAB = len(set(CHARMAP.values())) + 1
DATA = './data/shakespeare/'
SERVER = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-1.0, high=1.0),
                  'momentum': np.random.choice([0.0, 0.9]),
                  'step': 1,
                  'gamma': 1.0 - 10.0 ** np.random.uniform(low=-4.0, high=-2.0),
                  }
CLIENT = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),
                  'momentum': np.random.uniform(low=0.0, high=1.0),
                  'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),
                  'epochs': 1, 
                  'batch': 2 ** np.random.choice(np.arange(3, 8)),
                  'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),
                  'dropout': np.random.uniform(low=0.0, high=0.5),
                  }


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('logdir')
    parser.add_argument('--seed', default=0, type=int, help='random seed')
    parser.add_argument('--debug', default=0, type=int,
                        help='run in DEBUG mode if >0; sets number of clients and batches')

    # wrapper algorithm settings
    parser.add_argument('--rounds', default=800, type=int, 
                        help='maximum number of communication rounds')
    parser.add_argument('--total', default=4000, type=int, 
                        help='total number of communication rounds')
    parser.add_argument('--rate', default=3, type=int, help='elimination rate')
    parser.add_argument('--elim', default=0, type=int, help='number of elimination rounds')
    parser.add_argument('--eval', default=1, type=int, help='number of evaluation rounds')
    parser.add_argument('--discount', default=0.0, type=float,
                        help='discount factor for computing the validation score of an arm')

    # FedEx settings
    parser.add_argument('--batch', default=10, type=int, help='number of tasks per round')
    parser.add_argument('--configs', default=1, type=int,
                        help='''number of configs to optimize over with FedEx (use 1 for FedAvg):
                                - <-1: sample a random number between 1 and abs(args.configs)
                                - =-1: sample the number of arms given by the wrapper
                                - =0: sample a random number between 1 and the number of arms
                                - >0: sample the provided number, ignoring the number of arms''')
    parser.add_argument('--lr_only', action='store_true', help='tune only learning rate as a hyperparameter')
    parser.add_argument('--eps', default=0.1, type=float, help='multiplicative perturbation to client config, eps=0 is fedavg')
    parser.add_argument('--uniform', action='store_true',
                        help='run FedEx over a product set of single-parameter uniform grids')
    parser.add_argument('--random', action='store_true',
                        help='run FedEx over a product set of single-parameter random grids')
    parser.add_argument('--eta0', default=0.0, type=float, 
                        help='FedEx initial step size; if 0.0 uses FedEx default')
    parser.add_argument('--sched', default='aggressive', type=str, help='FedEx step size sched')
    parser.add_argument('--cutoff', default=0.0, type=float, 
                        help='stop updating FedEx config distribution if entropy below this cutoff')
    parser.add_argument('--baseline', default=-1.0, type=float,
                        help='''how FedEx computes the baseline:
                                - >=-1.0,<0.0: sample discount factor from [0.0, abs(args.baseline))
                                - =0.0: use the most recent value
                                - >0.0,<1.0: use geometrically discounted mean with this factor
                                - =1.0: use the mean of all values''')
    parser.add_argument('--diff', action='store_true', 
                        help='use difference between refine and global as FedEx objective')
    parser.add_argument('--stop', action='store_true',
                        help='stop updating FedEx config distribution after last elimination')

    # evaluation settings
    parser.add_argument('--mle', action='store_true', help='use MLE config at test time')
    parser.add_argument('--loss', action='store_true', help='use loss instead of error')
    parser.add_argument('--eval_global', action='store_true', help='use global error as elimination metric instead of refine')

    # data settings
    parser.add_argument('--length', default=80, type=int, help='sequence length')
    parser.add_argument('--iid', action='store_true', help='client data is iid')
    parser.add_argument('--val', default=0.1, type=float, help='val proportion')
    parser.add_argument('--test', default=0.1, type=float, help='test proportion')

    # model settings
    parser.add_argument('--hidden', default=256, type=int, help='number of hidden units in LSTM')
    parser.add_argument('--layers', default=2, type=int, help='number of layers in LSTM')

    return parser.parse_args()


def line2data(text, length=80):

    text = re.sub('  *', ' ', text.replace('\n', ' '))
    return [[text[i:i+length], text[i+length]] for i in range(len(text)-length)]


def file2tensor(fname, length=80):

    with open(fname, 'r') as f:
        data = line2data(f.read(), length=length)
    X, Y = torch.zeros(len(data), length).long(), torch.empty(len(data)).long()
    for i, (x, y) in enumerate(data):
        for j, char in enumerate(x):
            X[i,j] = CHARMAP[char]
        Y[i] = CHARMAP[y]
    return X, Y


def get_loader(task, val=0.1, test=0.1, iid=False):
    m = len(torch.load(task)['Y'])
    if m < 1.0 / min(1.0-val-test, val, test):
        return None
    partitions = {'train': (0, int((1.0-val-test)*m)), 
                  'val': (int((1.0-val-test)*m), int((1.0-test)*m)), 
                  'test': (int((1.0-test)*m), m)}
    if iid:
        randperm = torch.randperm(m)
    dump = torch.load(task)
    X, Y = (dump['X'][randperm], dump['Y'][randperm]) if iid else (dump['X'], dump['Y'])
    data = {key: (X[a:b].pin_memory(), Y[a:b].pin_memory()) for key, (a, b) in partitions.items()}
    def loader(*args):
        output = []
        for arg in args:
            Xarg, Yarg = data[arg]
            output.append(Xarg.cuda(non_blocking=True))
            output.append(Yarg.cuda(non_blocking=True))
        return output
    return loader


class CharLSTM(nn.Module):

    def __init__(self, input_size=8, hidden_size=256, **kwargs):

        super().__init__()
        self.embedding = nn.Embedding(num_embeddings=VOCAB, 
                                      embedding_dim=input_size, padding_idx=0)
        self.lstm= nn.LSTM(input_size=input_size, hidden_size=hidden_size, 
                           batch_first=True, bidirectional=False, **kwargs)
        self.linear = nn.Linear(hidden_size, VOCAB)

    def forward(self, X, lengths):

        X = self.embedding(X)
        X = rnn.pack_padded_sequence(X, lengths, batch_first=True)
        self.lstm.flatten_parameters()
        X, _ = self.lstm(X)
        X, _ = rnn.pad_packed_sequence(X, batch_first=True)
        return self.linear(X[:,-1])


def get_lengths(X):

    lengths = X.shape[1] - (X == 0).sum(1)
    return lengths.sort(0, descending=True)


def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):

    if not mu:
        return criterion

    mu *= 0.5
    model0 = [param.data.clone() for param in model.parameters()]

    def objective(*args, **kwargs):

        prox = sum((param-param0).pow(2).sum() 
                   for param, param0 in zip(model.parameters(), model0))
        return criterion(*args, **kwargs) + mu * prox

    return objective


def train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):

    optimizer = optim.SGD(model.parameters(), **kwargs)
    criterion = get_prox(model, mu=mu)
    model.lstm.dropout = dropout
    model.train()
    m = len(Y)
    for e in range(epochs):
        randperm = torch.randperm(m)
        X, Y = X[randperm], Y[randperm]
        for i in range(0, m, batch):
            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]
            lengths, sortperm = get_lengths(Xbatch)
            pred = model(Xbatch[sortperm], lengths.cpu())
            loss = criterion(pred, Ybatch[sortperm])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    model.eval()
    return model


def test_batch(model, X, Y):

    lengths, sortperm = get_lengths(X)
    pred = model(X[sortperm], lengths.cpu())
    Yperm = Y[sortperm]
    return (Yperm != pred.argmax(1)).sum().float(), nn.CrossEntropyLoss(reduction='sum')(pred, Yperm).float()


def test(model, X, Y, batch=BATCH):

    model.eval()
    with torch.no_grad():
        errors, losses = zip(*(test_batch(model, X[i:i+batch], Y[i:i+batch]) 
                               for i in range(0, len(Y), batch)))
        return float(sum(errors)) / len(Y), float(sum(losses)) / len(Y)


def main():

    args = parse()
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    tasks = []
    for i, fname in enumerate(glob(os.path.join(DATA, '*.txt'))):
        task = os.path.join(DATA, fname.split('/')[-1][:-4] + '-' + str(args.length) + 'char.pt')
        if not os.path.isfile(task):
            print('\rCaching task', i+1, end='')
            dump = {}
            dump['X'], dump['Y'] = file2tensor(fname, length=args.length)
            torch.save(dump, task)
        loader = get_loader(task, val=args.val, test=args.test, iid=args.iid)
        if not loader is None:
            tasks.append(loader)
    if args.debug:
        tasks = tasks[:args.debug]
        print('DEBUG MODE')
    print('\rCompleted cache-check')

    def local_train(model, X, Y, **kwargs):

        if args.debug:
            return train(model, X[:args.debug*args.batch], Y[:args.debug*args.batch], **kwargs)
        return train(model, X, Y, **kwargs)

    def local_test(model, X, Y, **kwargs):

        return test(model, X, Y, **kwargs)[args.loss]

    def get_server():

        model = CharLSTM(hidden_size=args.hidden, num_layers=args.layers)
        return Server(model, tasks, local_train, local_test, batch=args.batch, **SERVER())


    def get_client(n_clients=1):
        '''performs local tuning for each hyperparameter'''
        if args.lr_only:
            return [SIMPLE_CLIENT()]

        initial_client = CLIENT()
        client_arr = [initial_client]
        eps = args.eps

        for i in range(n_clients-1):
            other_client = deepcopy(initial_client)
            
            log_lr = np.log10(other_client['lr'])
            other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(-eps * 4, eps * 4), -4.0, 0.0)
            
            other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)
            
            log_wd = np.log10(other_client['weight_decay'])
            other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(-eps*4, eps*4),-5.0, -1.0)

            log_batch = int(np.log2(other_client['batch']))
            other_client['batch'] = 2 ** np.clip(np.random.choice([log_batch-1, log_batch, log_batch+1]), 3, 7)
            
            log_mu = np.log10(other_client['mu'])
            other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(-eps*5, eps*5), -5.0 , 0.0)
            
            other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(-eps*0.5, eps*0.5),0, 0.5)

            client_arr.append(other_client)

        return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr


    print('Tuning',
          'FedAvg' if args.configs == 1 and not (args.uniform or args.random) else 'FedEx',
          'on Shakespeare')
    os.makedirs(args.logdir, exist_ok=True)
    with open(os.path.join(args.logdir, 'args.json'), 'w') as f:
        json.dump(vars(args), f, indent=4)
    wrapped_fedex(
                  get_server, 
                  get_client, 
                  num_configs=args.configs,
                  prod=args.uniform or args.random,
                  stepsize_init=args.eta0 if args.eta0 else 'auto',
                  stepsize_sched=args.sched,
                  cutoff=args.cutoff, 
                  baseline_discount=args.baseline,
                  diff=args.diff, 
                  mle=args.mle, 
                  logdir=args.logdir, 
                  val_discount=args.discount,
                  last_stop=args.stop,
                  max_resources=args.rounds, 
                  total_resources=args.total,
                  elim_rate=args.rate, 
                  num_elim=args.elim,
                  num_eval=args.eval,
                  eval_global=args.eval_global
                  )


if __name__ == '__main__':

    main()

Output:
{
    "experimental_code": "def discounted_mean(trace, factor=1.0):weight = factor ** np.flip(np.arange(len(trace)), axis=0)return np.inner(trace, weight) / weight.sum()class FedEx:    '''runs hyperparameter optimization given a federated learning server'''    def entropy(self):        entropy = 0.0        for probs in product(*(theta[theta>0.0] for theta in self._theta)):            prob = np.prod(probs)            entropy -= prob * np.log(prob)        return entropy    def mle(self):        return np.prod([theta.max() for theta in self._theta])    def __init__(                 self,                  server,                  configs,                  eta0='auto',                  sched='auto',                  cutoff=0.0,                  baseline=0.0,                  diff=False,                 ):        '''        Args:            server: Object that implements two methods, 'communication_round' and 'full_evaluation'                    taking as input a single argument, 'get_config', itself a function that takes                     no inputs and outputs an element of the provided list 'configs'.                     - 'communication_round' samples a batch of clients, assigns a config to each                     using 'get_config', and runs local training using that config. It then                     aggregates the local models to to take a training step and returns three lists                     or arrays: a list of each client's validation error before local training, a                     list of each client's validation error after local training, and a list of each                     client's weight (e.g. size of its validation set).                     - 'full_evaluation' assigns a config to each client using 'get_config' and runs                    local training using that config. It then returns three lists or arrays: a list                    of each client's test error before local training, a list of each client's test                    error after local training, and a list of each client's weight (e.g. size of                     its test set).            configs: list of configs used for local training and testing by 'server'                      OR dict of (string, list) pairs denoting a grid of configs            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))            sched: learning rate schedule for exponentiated gradient:                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)                    - 'aggressive': uses eta0 / gradient l-infinity norm                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds                    - 'constant': uses eta0                    - 'scale': uses sched * sqrt(2 * log(len(configs)))            cutoff: entropy level below which to stop updating the config probability and use MLE            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean            diff: if True uses performance difference; otherwise uses absolute performance        '''        self._server = server        self._configs = configs        self._grid = [] if type(configs) == list else sorted(configs.keys())        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]        self._sched = sched        self._cutoff = cutoff        self._baseline = baseline        self._diff = diff        self._z = [np.full(size, -np.log(size)) for size in sizes]        self._theta = [np.exp(z) for z in self._z]        self._store = [0.0 for _ in sizes]        self._stopped = False        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}    def stop(self):        self._stopped = True    def sample(self, mle=False, _index=[]):        '''samples from configs using current probability vector'''        if mle or self._stopped:            if self._grid:                return {param: self._configs[param][theta.argmax()]                         for theta, param in zip(self._theta, self._grid)}            return self._configs[self._theta[0].argmax()]        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])        if self._grid:            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}        return self._configs[_index[-1][0]]    def settings(self):        '''returns FedEx input settings'''        output = {'configs': deepcopy(self._configs)}        output['eta0'], output['sched'] = self._eta0, self._sched        output['cutoff'], output['baseline'] = self._cutoff, self._baseline         if self._trace['refine']:            output['theta'] = self.theta()        return output    def step(self):        '''takes exponentiated gradient step (calls 'communication_round' once)'''        index = []        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))                before, after = np.array(before), np.array(after)        weight = np.array(weight, dtype=np.float64) / sum(weight)        if self._trace['refine']:            trace = self.trace('refine')            if self._diff:                trace -= self.trace('global')            baseline = discounted_mean(trace, self._baseline)        else:            baseline = 0.0        self._trace['global'].append(np.inner(before, weight))        self._trace['refine'].append(np.inner(after, weight))        if not index:            self._trace['entropy'].append(0.0)            self._trace['mle'].append(1.0)            return        for i, (z, theta) in enumerate(zip(self._z, self._theta)):            grad = np.zeros(len(z))            for idx, s, w in zip(index, after-before if self._diff else after, weight):                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]            if self._sched == 'adaptive':                self._store[i] += norm(grad, float('inf')) ** 2                denom = np.sqrt(self._store[i])            elif self._sched == 'aggressive':                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))            elif self._sched == 'auto':                self._store[i] += 1.0                denom = np.sqrt(self._store[i])            elif self._sched == 'constant':                denom = 1.0            elif self._sched == 'scale':                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')            else:                raise NotImplementedError            eta = self._eta0[i] / denom            z -= eta * grad            z -= logsumexp(z)            self._theta[i] = np.exp(z)        self._trace['entropy'].append(self.entropy())        self._trace['mle'].append(self.mle())        if self._trace['entropy'][-1] < self._cutoff:            self.stop()    def test(self, mle=False):        '''evaluates found config (calls 'full_evaluation' once)        Args:            mle: use MLE config instead of sampling        Returns:            output of 'full_evaluation'        '''        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))        return {'global': np.inner(before, weight) / weight.sum(),                'refine': np.inner(after, weight) / weight.sum()}    def theta(self):        '''returns copy of config probability vector'''        return deepcopy(self._theta)    def trace(self, key):        '''returns trace of one of three tracked quantities        Args:            key: 'entropy', 'global', or 'refine'        Returns:            numpy vector with length equal to number of calls to 'step'        '''        return np.array(self._trace[key])class Server:    '''object for federated training implementing methods required by FedEx'''    def _set_test_state(self):        state = (np.random.get_state(), torch.get_rng_state(), torch.cuda.get_rng_state())        if self._state is None:            self._state = state        else:            np.random.set_state(self._state[0])            torch.set_rng_state(self._state[1])            torch.cuda.set_rng_state(self._state[2])        return state    def _reset_state(self, state):        np.random.set_state(state[0])        torch.set_rng_state(state[1])        torch.cuda.set_rng_state(state[2])    def __init__(                 self,                  model,                  clients,                  train,                  test,                  lr=1.0,                  momentum=0.0,                  step=1,                  gamma=1.0,                  batch=10,                  state=None,                 ):        '''        Args:            model: PyTorch model            clients: list of clients, each a function that takes one or more strings 'train',                     'val', 'test' and returns, as one tuple, input and output tensors for each            train: method that takes as argument a PyTorch model, an input tensor, an output                   tensor, and optional kwargs and returns the same PyTorch model            test: method that takes as argument a PyTorch model, an input tensor, and an output                  tensor and returns the model's error            lr: server learning rate            momentum: server momentum            step: server learning rate decay interval            gamma: server learning rate decay factor            batch: number of clients to sample per communication round            state: np.random, torch, torch.cuda random state tuple; if None uses current states        '''        self._model = model        self._clients = clients        self._train = train        self._test = test        self._opt = optim.SGD(self._model.parameters(), lr=lr, momentum=momentum)        self._sched = optim.lr_scheduler.StepLR(self._opt, step, gamma=gamma)        self._batch = batch        self._state = state        self._reset_state(self._set_test_state())    def communication_round(self, get_config):        '''runs one step of local training and model aggregation        Args:            get_config: returns kwargs for 'train' as a dict        Returns:            np.array objects for global val error, local val error, and val size of each client        '''        self._model.cuda()        before, after, weight = [np.zeros(self._batch) for _ in range(3)]        total = 0.0        for i in range(self._batch):            Xtrain, Ytrain, Xval, Yval = random.choice(self._clients)('train', 'val')            before[i] = self._test(self._model, Xval, Yval)            model = self._train(deepcopy(self._model), Xtrain, Ytrain, **get_config())            after[i] = self._test(model, Xval, Yval)            weight[i] = len(Yval)            total += len(Ytrain)            if i:                for agg, param in zip(aggregate.parameters(), model.parameters()):                    agg.data += len(Ytrain) * param.data            else:                for param in model.parameters():                    param.data *= len(Ytrain)                aggregate = model        self._opt.zero_grad()        for agg, param in zip(aggregate.parameters(), self._model.parameters()):            param.grad = param.data - agg / total        self._opt.step()        self._opt.zero_grad()        self._sched.step()        self._model.cpu()        return before, after, weight    def full_evaluation(self, get_config):        '''evaluates personalization on each client        Args:            get_config: returns kwargs for 'train' as a dict        Returns:            np.array objects for global test error, local test error, and test size of each client        '''        state = self._set_test_state()        self._model.cuda()        before, after, weight = [np.zeros(len(self._clients)) for _ in range(3)]        for i, client in enumerate(self._clients):            Xtrain, Ytrain, Xtest, Ytest = client('train', 'test')            before[i] = self._test(self._model, Xtest, Ytest)            after[i] = self._test(self._train(deepcopy(self._model),                                               Xtrain, Ytrain, **get_config()),                                   Xtest, Ytest)            weight[i] = len(Ytest)            print('\\r\\tEvaluated client', frac(i+1, len(self._clients)),                  '    global error:', round(np.inner(before, weight) / weight.sum(), 4),                  '    refine error:', round(np.inner(after, weight) / weight.sum(), 4),                   end=32*' ')        self._model.cpu()        self._reset_state(state)        return before, after, weightdef wrapped_fedex(                  get_server,                  get_client,                  num_configs=1,                  prod=False,                  stepsize_init='auto',                  stepsize_sched='aggressive',                  cutoff=1E-4,                  baseline_discount=-1.0,                  diff=False,                  mle=False,                  logdir=None,                  val_discount=0.0,                  last_stop=False,                  eval_global=False,                  **kwargs,                  ):    '''evaluates FedEx wrapped with successive elimination algorithm;       uses FedAvg when num_configs = 1 and prod = False    Args:        get_server: function that takes no input and returns an object that can be passed as the                     first argument to FedEx.__init__, e.g. a Server object        get_client: function that takes no input and returns a dict of local training configs, a                    list of which is passed as the second argument to 'FedEx.__init__'; can also                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'        num_configs: determines number of configs in the list passed to 'FedEx.__init__':                     - >0: use this value directly                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]                     - =-1: use the number of arms given by the wrapper                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':                           - >0.0: use this value directly                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]        diff: passed to 'diff' kwarg of 'FedEx.__init__'        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'        logdir: passed to 'logdir' kwarg of 'successive_elimination'        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'        kwargs: passed to 'get_schedule'    Returns:        FedEx object    '''    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')    if num_configs < -1:        samples = lambda n: random.randint(1, -num_configs)    elif num_configs == -1:        samples = lambda n: n    elif num_configs == 0:        samples = lambda n: random.randint(1, n)    else:        samples = lambda n: num_configs    if baseline_discount < 0.0:        baseline = lambda: random.uniform(0.0, -baseline_discount)    else:        baseline = lambda: baseline_discount    def sampler(n):        for _ in range(n):            yield FedEx(                        get_server(),                         get_client() if prod else get_client(samples(n)),                        eta0=stepsize_init,                         sched=stepsize_sched,                         cutoff=cutoff,                         baseline=baseline(),                        diff=diff,                        )    return successive_elimination(                                  sampler,                                   ['refine', 'global'],                                   logdir=logdir,                                   val_discount=val_discount,                                  elim_rate=elim_rate,                                   elim_sched=elim_sched,                                   eval_sched=eval_sched,                                  traces=['entropy', 'mle', 'global', 'refine'],                                   last_round='stop' if last_stop else None,                                  mle=mle,                                  eval_global=eval_global,                                  )import argparseimport jsonimport osimport pdbimport pickleimport randomimport reimport stringimport mathfrom copy import deepcopyfrom collections import defaultdictfrom glob import globimport numpy as npimport torch; torch.backends.cudnn.benchmark = Truefrom torch import nnfrom torch import optimfrom hyper import wrapped_fedexfrom hyper import Serverimport torch.nn.functional as Fimport torchvision.datasets as datasetsimport torchvision.transforms as transforms CLIENT = lambda: {'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),'momentum': np.random.uniform(low=0.0,high=1.0),'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),'epochs': np.random.choice(np.arange(1, 6)),'batch': 2 ** np.random.choice(np.arange(3, 8)),'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),'dropout': np.random.uniform(low=0.0, high=0.5),}class CNN(nn.Module):    def __init__(self):        super(CNN, self).__init__()        self.conv1 = nn.Sequential(                                   nn.Conv2d(3, 32, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.conv2 = nn.Sequential(                                   nn.Conv2d(32, 64, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.conv3 = nn.Sequential(                                   nn.Conv2d(64, 64, 3, padding=1),                                   nn.ReLU(),                                   nn.MaxPool2d(2),                                   )        self.dropout = nn.Dropout(0.0)        self.fc = nn.Sequential(                                nn.Linear(1024, 64),                                nn.ReLU(),                                )        self.clf = nn.Linear(64, 10)    def forward(self, x):        x = self.conv1(x)        x = self.conv2(x)        x = self.conv3(x)        x = self.fc(self.dropout(x.flatten(1)))        return self.clf(self.dropout(x))def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):    if not mu:        return criterion    mu *= 0.5    model0 = [param.data.clone() for param in model.parameters()]    def objective(*args, **kwargs):        prox = sum((param-param0).pow(2).sum()                   for param, param0 in zip(model.parameters(), model0))        return criterion(*args, **kwargs) + mu * prox    return objectivedef train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):    optimizer = optim.SGD(model.parameters(), **kwargs)    criterion = get_prox(model, mu=mu)    model.dropout.p = dropout    model.train()    m = len(Y)    for e in range(epochs):        randperm = torch.randperm(m)        X, Y = X[randperm], Y[randperm]        for i in range(0, m, batch):            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]            pred = model(Xbatch)            loss = criterion(pred, Ybatch)            optimizer.zero_grad()            loss.backward()            optimizer.step()    model.eval()    return modeldef get_client(n_clients=1):    if args.lr_only:        return [SIMPLE_CLIENT()]    initial_client = CLIENT()    client_arr = [initial_client]    eps = args.eps    for i in range(n_clients-1):        other_client = deepcopy(initial_client)                log_lr = np.log10(other_client['lr'])        other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)                other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)                log_wd = np.log10(other_client['weight_decay'])        other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)                epochs_range = math.ceil(eps * 4)        other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)        log_batch = int(np.log2(other_client['batch']))        batch_range = math.ceil(eps * 4)        other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)                log_mu = np.log10(other_client['mu'])        other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)                other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)        client_arr.append(other_client)    return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr",
    "experimental_info": "Datasets: CIFAR-10, FEMNIST, Shakespeare.Specific data settings include: CIFAR-10: 500 clients, 0.2 validation proportion. FEMNIST: 0.1 validation proportion, 0.1 test proportion. Shakespeare: sequence length 80, 0.1 validation proportion, 0.1 test proportion. Both FEMNIST and Shakespeare can be configured for IID client data.Model Architectures: For CIFAR-10 and FEMNIST, a CNN is used with 3 convolutional layers (32, 64, 64 filters respectively) followed by a fully connected layer (1024 to 64 units) and a classification layer (64 to 10 for CIFAR-10, 62 for FEMNIST). For Shakespeare, a CharLSTM model is used with configurable hidden size (default 256) and number of layers (default 2), an embedding layer, and a linear output layer.Hyperparameter Search Spaces (Randomly Sampled):Server-side (from SERVER lambda):    - 'lr': 10.0 ** U(-1.0, 1.0) (learning rate)    - 'momentum': Choice([0.0, 0.9])    - 'step': 1 (learning rate decay interval)    - 'gamma': 1.0 - 10.0 ** U(-4.0, -2.0) (learning rate decay factor)Client-side (from CLIENT lambda, for local training):    - 'lr': 10.0 ** U(-4.0, 0.0) (learning rate)    - 'momentum': U(0.0, 1.0)    - 'weight_decay': 10.0 ** U(-5.0, -1.0)    - 'epochs': Choice(arange(1, 6))    - 'batch': 2 ** Choice(arange(3, 8))    - 'mu': 10.0 ** U(-5.0, 0.0) (proximal term coefficient)    - 'dropout': U(0.0, 0.5)FedEx-specific Settings (controlled by command-line arguments):    - '--configs' (default: 1): Number of hyperparameter configurations to optimize over with FedEx. Can be >0 (direct value), =0 (random from 1 to number of arms), =-1 (number of arms given by wrapper), or < -1 (random from 1 to abs(num_configs)). When `--configs`=1 and `--uniform`/`--random` are false, it behaves like FedAvg.    - '--lr_only' (action='store_true'): Tune only learning rate as a hyperparameter.    - '--eps' (default: 0.0 for CIFAR/FEMNIST, 0.1 for Shakespeare): Multiplicative perturbation to client config; eps=0.0 effectively means FedAvg (no perturbation).    - '--uniform' (action='store_true'): Run FedEx over a product set of single-parameter uniform grids.    - '--random' (action='store_true'): Run FedEx over a product set of single-parameter random grids.    - '--eta0' (default: 0.0): FedEx initial step size; if 0.0, uses FedEx default ('auto' which is sqrt(2*log(size))).    - '--sched' (default: 'aggressive'): FedEx step size schedule ('adaptive', 'aggressive', 'auto', 'constant', 'scale').    - '--cutoff' (default: 0.0): Stop updating FedEx config distribution if entropy falls below this value.    - '--baseline' (default: -1.0): How FedEx computes the baseline. If >=-1.0,<0.0, samples discount factor from [0.0, abs(args.baseline)). If =0.0, uses most recent value. If >0.0,<1.0, uses geometrically discounted mean. If =1.0, uses mean of all values.    - '--diff' (action='store_true'): Use difference between refine and global error as FedEx objective.    - '--stop' (action='store_true'): Stop updating FedEx config distribution after the last elimination round.Wrapper Algorithm Settings:    - '--rounds' (default: 800 for CIFAR/Shakespeare, 50 for FEMNIST): Maximum number of communication rounds (max_resources).    - '--total' (default: 4000 for CIFAR/Shakespeare, 450 for FEMNIST): Total number of communication rounds (total_resources).    - '--rate' (default: 3): Multiplicative elimination rate.    - '--elim' (default: 0): Number of elimination rounds; if 0, runs random search.    - '--eval' (default: 1): Number of evaluation rounds.    - '--discount' (default: 0.0): Discount factor for computing the validation score of an arm.Evaluation Settings:    - '--mle' (action='store_true'): Use MLE config at test time.    - '--loss' (action='store_true'): Use loss instead of error as evaluation metric.    - '--eval_global' (action='store_true'): Use global error as elimination metric instead of refine error.Other settings:    - BATCH (constant in code): 100, used for test function.    - Server batch size (args.batch, default: 10): Number of clients to sample per communication round.    - '--seed' (default: 0): Random seed."
}
