
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The methodology is centered on deriving stochastic lower bounds for the linearized Laplace approximation of the marginal likelihood. This is achieved by leveraging the dual function-space form of the linearized Laplace, which can be estimated using the Neural Tangent Kernel (NTK). The key steps include: 1) Proving that existing structured parametric approximations (e.g., block-diagonal and diagonal Gauss-Newton, KFAC) are, in fact, lower bounds to the linearized Laplace marginal likelihood (Theorem 1). 2) Deriving novel data-subset lower bounds using the NTK form of the log-determinant, which allows partitioning input-output pairs into batches for unbiased stochastic estimation and gradients (Theorem 2). 3) Demonstrating the equivalence of the NTK-based data-subset bound to a parametric variant (Theorem 3), and combining this with the parametric structured bounds to create 'doubly lower bounds' for structured parametric approximations (like KFAC) on data subsets (Corollary 3.1). The paper also explores strategies for partitioning data (e.g., output-wise partitioning or grouping by labels) to improve bound tightness and efficiency.

# Repository Content
File Path: arg_utils.py
Content:
import yaml


def read_yaml_config(argv):
    args_dict = dict()
    if '--config' not in argv:
        return args_dict
    ix = argv.index('--config')
    # go through arguments until the end or no 'yaml' ending present
    for i in range(ix+1, len(argv)):
        if '.yaml' not in argv[i]:
            break
        with open(argv[i]) as f:
            config = yaml.full_load(f)
        # only update intersecting keys
        args_dict.update(config)
    return args_dict


def set_defaults_with_yaml_config(parser, argv):
    config = read_yaml_config(argv)
    for action in parser._actions:
        if action.choices is not None:
            val = config.get(action.dest, action.default)
            if val is not None and val not in action.choices:
                print(action.dest, action.default, action.choices)
                raise ValueError('Invalid config argument', action.dest,
                                 config[action.dest], 'Options:', action.choices)
        action.default = config.get(action.dest, action.default)

File Path: bound_grid.py
Content:
import logging
import pandas as pd
import numpy as np
from scipy.stats import sem
import torch
from torchvision import transforms
logging.basicConfig(format='[%(filename)s:%(lineno)s]%(levelname)s: %(message)s', level=logging.INFO)

from laplace.curvature.asdl import AsdlGGN
from laplace.curvature.augmented_asdl import AugAsdlGGN
from laplace import FunctionalLaplace, BlockDiagLaplace, KronLaplace, DiagLaplace, FullLaplace

from ntkmarglik.marglik import marglik_optimization
from ntkmarglik.invariances import AffineLayer2d
from ntkmarglik.utils import set_seed
from ntkmarglik.models import MiniNet
from data_utils.utils import TensorDataLoader, SubsetTensorDataLoader, dataset_to_tensors, GroupedSubsetTensorDataLoader
from classification_image import get_dataset

INVARIANCE = 'invariance'
PRIOR = 'prior'
DATA_ROOT = '/is/cluster/fast/aimmer/data'


def get_marglik_loader(x, y, batch_size, augmenter, grouped_loader):
    data_factor = len(x) / batch_size
    DataLoaderCls = GroupedSubsetTensorDataLoader if grouped_loader else SubsetTensorDataLoader
    marglik_loader = DataLoaderCls(x, y, transform=augmenter, subset_size=batch_size,
                                   detach=False, data_factor=data_factor)
    return marglik_loader


def main(setting, approximation, single_output, grouped_loader, stochastic):
    device = 'cuda'
    subset_size = 1000
    batch_size = 250
    marglik_batch_size = 1000
    lr, lr_min, lr_hyp, lr_hyp_min, lr_aug, lr_aug_min = 1e-3, 1e-4, 1e-1, 1e-2, 0.05, 0.005
    n_epochs = 100
    n_epochs_burnin = 10
    if setting == PRIOR:
        dataset = 'mnist'
        marglik_frequency = 5
        n_hypersteps_prior = 10
        n_hypersteps = 1
    else:
        dataset = 'mnist_r180'
        marglik_frequency = 1
        n_hypersteps_prior = 2
        n_hypersteps = 2

    ####### Quickly train with NTK Laplace
    transform = transforms.ToTensor()
    train_dataset, _ = get_dataset(dataset, DATA_ROOT, False, transform)
    set_seed(711)
    subset_size = len(train_dataset) if subset_size <= 0 else subset_size
    if subset_size < len(train_dataset):
        subset_indices = torch.randperm(len(train_dataset))[:subset_size]
    else:
        subset_indices = None
    X_train, y_train = dataset_to_tensors(train_dataset, subset_indices, device)
    if setting == INVARIANCE:
        augmenter = AffineLayer2d(n_samples=30).to(device)
        augmenter_valid = augmenter
        augmenter.rot_factor.requires_grad = True
    else:
        augmenter = augmenter_valid = None

    train_loader = TensorDataLoader(X_train, y_train, transform=augmenter, batch_size=batch_size, shuffle=True, detach=True)
    marglik_loader = get_marglik_loader(X_train, y_train, marglik_batch_size, augmenter, grouped_loader=False)
    partial_loader = None
    stochastic_grad = True

    optimizer = 'Adam'
    prior_structure = 'scalar'
    model = MiniNet(in_channels=1, n_out=10, augmented=(setting == INVARIANCE)).to(device)
    backend = AugAsdlGGN if setting == INVARIANCE else AsdlGGN
    method = 'lila' if setting == INVARIANCE else 'baseline'

    la, model, margliks, valid_perfs, aug_history = marglik_optimization(
        model, train_loader, marglik_loader, None, partial_loader, likelihood='classification',
        lr=lr, lr_hyp=lr_hyp, lr_hyp_min=lr_hyp_min, lr_aug=lr_aug, n_epochs=n_epochs,
        n_hypersteps=n_hypersteps, marglik_frequency=marglik_frequency, laplace=FunctionalLaplace,
        prior_structure=prior_structure, backend=backend, n_epochs_burnin=n_epochs_burnin,
        method=method, augmenter=augmenter_valid, lr_min=lr_min, scheduler='cos', optimizer=optimizer,
        n_hypersteps_prior=n_hypersteps_prior, temperature=1.0, lr_aug_min=lr_aug_min,
        prior_prec_init=1.0, stochastic_grad=stochastic_grad, use_wandb=False,
        independent=False, kron_jac=False, single_output=False
    )
    ####### Finished training.

    ####### Assess bound at converged setting
    backend_kwargs = dict(differentiable=False, kron_jac=False)
    la_kwargs = dict(sod=True, single_output=single_output)
    if approximation == 'kernel' and single_output:
        la_kwargs['independent'] = True

    if stochastic:
        batch_sizes = [10, 20, 50, 100, 250, 500, 1000]
    else:
        # for parametric no sod bounds
        batch_sizes = [1000]

    grid = np.logspace(-4, 4, 100) if setting == PRIOR else np.linspace(0, np.pi, 100)
    result_frame = pd.DataFrame(index=batch_sizes, columns=grid)
    result_frame_sem = pd.DataFrame(index=batch_sizes, columns=grid)
    for batch_size in batch_sizes:
        for hparam in grid:
            set_seed(711)
            marglik_loader = get_marglik_loader(X_train, y_train, batch_size, augmenter, grouped_loader)
            marglik_loader = marglik_loader.detach()
            if setting == INVARIANCE:
                augmenter.rot_factor.requires_grad = False
                augmenter.rot_factor.data[2] = float(hparam)
                prior_precision = la.prior_precision
            else:
                prior_precision = float(hparam)

            margliks = list()
            n_reps = int(subset_size / batch_size)
            for rep in range(n_reps):
                if approximation == 'kernel':
                    lap_cls = FunctionalLaplace
                elif approximation == 'full':
                    lap_cls = FullLaplace
                elif approximation == 'blockdiag':
                    lap_cls = BlockDiagLaplace
                elif approximation == 'kron':
                    lap_cls = KronLaplace
                elif approximation == 'diag':
                    lap_cls = DiagLaplace
                lap = lap_cls(model, 'classification', prior_precision=prior_precision,
                              backend=backend, backend_kwargs=backend_kwargs, **la_kwargs)
                lap.fit(marglik_loader)
                marglik = lap.log_marginal_likelihood().item() / subset_size
                margliks.append(marglik)
            result_frame.loc[batch_size, hparam] = np.mean(margliks)
            result_frame_sem.loc[batch_size, hparam] = sem(margliks)
            print(setting, batch_size, hparam, np.mean(margliks), np.nan_to_num(sem(margliks)))

    str_id = f'{setting}_{approximation}_so={single_output}_grouped={grouped_loader}_sto={stochastic}'
    result_frame.to_csv(f'results_grid/grid_bound_{str_id}.csv')
    result_frame_sem.to_csv(f'results_grid/grid_bound_sem_{str_id}.csv')


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--setting', choices=[PRIOR, INVARIANCE])
    parser.add_argument('--approximation', choices=['kernel', 'full', 'blockdiag', 'kron', 'diag'])
    parser.add_argument('--single_output', default=False, action=argparse.BooleanOptionalAction)
    parser.add_argument('--grouped_loader', default=False, action=argparse.BooleanOptionalAction)
    parser.add_argument('--stochastic', default=True, action=argparse.BooleanOptionalAction)
    args = parser.parse_args()
    main(args.setting, args.approximation, args.single_output, args.grouped_loader, args.stochastic)

File Path: classification_image.py
Content:
import logging
import torch
import wandb
from dotenv import load_dotenv
logging.basicConfig(format='[%(filename)s:%(lineno)s]%(levelname)s: %(message)s', level=logging.INFO)

from laplace.curvature.asdl import AsdlGGN, AsdlEF
from laplace.curvature.augmented_asdl import AugAsdlGGN, AugAsdlEF

from ntkmarglik.marglik import marglik_optimization
from ntkmarglik.invariances import AffineLayer2d
from ntkmarglik.utils import get_laplace_approximation, set_seed
from ntkmarglik.models import MLP, LeNet, WideResNet, ResNet, MiniNet
from data_utils import (
    get_dataset, TensorDataLoader, SubsetTensorDataLoader, GroupedSubsetTensorDataLoader, 
    dataset_to_tensors, CIFAR_transform, MNIST_transform, ImageNet_transform
)


def main(
    seed, method, approx, curv, dataset, model, n_epochs, batch_size, marglik_batch_size, partial_batch_size, bound,
    subset_size, n_samples_aug, lr, lr_min, lr_hyp, lr_hyp_min, lr_aug, lr_aug_min, grouped_loader,
    prior_prec_init, n_epochs_burnin, marglik_frequency, n_hypersteps, n_hypersteps_prior, random_flip,
    device, download_data, data_root, independent_outputs, kron_jac, single_output,
    single_output_iid, temperature
):
    # dataset-specific static transforms (preprocessing)
    if 'mnist' in dataset:
        transform = MNIST_transform
    elif 'cifar' in dataset:
        transform = CIFAR_transform
    elif 'tiny' in dataset:
        transform = ImageNet_transform
    else:
        raise NotImplementedError(f'Transform for {dataset} unavailable.')

    train_dataset, test_dataset = get_dataset(dataset, data_root, download_data, transform)

    # dataset-specific number of classes
    if 'cifar100' in dataset:
        n_classes = 100
    elif 'tiny' in dataset:
        n_classes = 200
    else:
        n_classes = 10

    # Load data
    set_seed(seed)

    # Subset the data if subset_size is given.
    subset_size = len(train_dataset) if subset_size <= 0 else subset_size
    if subset_size < len(train_dataset):
        subset_indices = torch.randperm(len(train_dataset))[:subset_size]
    else:
        subset_indices = None
    X_train, y_train = dataset_to_tensors(train_dataset, subset_indices, device)
    X_test, y_test = dataset_to_tensors(test_dataset, None, device)

    if method == 'lila':
        augmenter = AffineLayer2d(n_samples=n_samples_aug, init_value=0.0, random_flip=random_flip).to(device)
        augmenter_valid = augmenter_marglik = augmenter
        augmenter.rot_factor.requires_grad = True
    else:
        augmenter = augmenter_valid = augmenter_marglik = None
    optimize_aug = (method == 'lila')

    batch_size = subset_size if batch_size <= 0 else min(batch_size, subset_size)
    ml_batch_size = subset_size if marglik_batch_size <= 0 else min(marglik_batch_size, subset_size)
    pl_batch_size = subset_size if partial_batch_size <= 0 else min(partial_batch_size, subset_size)
    train_loader = TensorDataLoader(X_train, y_train, transform=augmenter, batch_size=batch_size, shuffle=True, detach=True)
    valid_loader = TensorDataLoader(X_test, y_test, transform=augmenter_valid, batch_size=batch_size, detach=True)
    if not bound:
        stochastic_grad = False
        marglik_loader = TensorDataLoader(X_train, y_train, transform=augmenter_marglik, batch_size=ml_batch_size, shuffle=True, detach=True)
        partial_loader = TensorDataLoader(X_train, y_train, transform=augmenter_marglik, batch_size=pl_batch_size, shuffle=True, detach=False)
    else:  # use proposed lower bound estimators
        stochastic_grad = True
        data_factor = 1.0
        data_factor = len(X_train) / ml_batch_size
        DataLoaderCls = GroupedSubsetTensorDataLoader if grouped_loader else SubsetTensorDataLoader
        marglik_loader = DataLoaderCls(X_train, y_train, transform=augmenter_marglik, subset_size=ml_batch_size,
                                       detach=False, data_factor=data_factor)
        partial_loader = None

    # model
    optimizer = 'SGD'
    prior_structure = 'layerwise'
    if 'mnist' in dataset:
        if model == 'mlp':
            model = MLP(28*28, width=1000, depth=1, output_size=n_classes, activation='tanh', augmented=optimize_aug)
        elif model == 'mininet':
            optimizer = 'Adam'
            model = MiniNet(in_channels=1, n_out=10, augmented=optimize_aug)
        elif model == 'cnn':
            model = LeNet(in_channels=1, n_out=n_classes, activation='tanh', n_pixels=28, augmented=optimize_aug)
        else:
            raise ValueError('Unavailable model for (f)mnist')
    elif 'cifar' in dataset:
        if model == 'cnn':
            model = LeNet(in_channels=3, n_out=n_classes, activation='relu', n_pixels=32, augmented=optimize_aug)
        elif model == 'resnet':
            model = ResNet(depth=18, in_planes=16, num_classes=n_classes, augmented=optimize_aug)
        elif model == 'wrn':
            model = WideResNet(augmented=optimize_aug, num_classes=n_classes, depth=16, widen_factor=4)
        else:
            raise ValueError('Unavailable model for cifar')
    elif 'tiny' in dataset:
        assert model == 'resnet'
        model = ResNet(depth=50, in_planes=16, num_classes=n_classes, augmented=optimize_aug)

    model.to(device)

    laplace = get_laplace_approximation(approx)
    if optimize_aug:
        backend = AugAsdlGGN if curv in ['ggn', 'fisher'] else AugAsdlEF
    else:
        backend = AsdlGGN if curv in ['ggn', 'fisher'] else AsdlEF

    marglik_optimization(
        model, train_loader, marglik_loader, valid_loader, partial_loader, likelihood='classification',
        lr=lr, lr_hyp=lr_hyp, lr_hyp_min=lr_hyp_min, lr_aug=lr_aug, n_epochs=n_epochs, 
        n_hypersteps=n_hypersteps, marglik_frequency=marglik_frequency, laplace=laplace,
        prior_structure=prior_structure, backend=backend, n_epochs_burnin=n_epochs_burnin,
        method=method, augmenter=augmenter_valid, lr_min=lr_min, scheduler='cos', optimizer=optimizer,
        n_hypersteps_prior=n_hypersteps_prior, temperature=temperature, lr_aug_min=lr_aug_min,
        prior_prec_init=prior_prec_init, stochastic_grad=stochastic_grad, use_wandb=True,
        independent=independent_outputs, kron_jac=kron_jac, single_output=single_output, 
        single_output_iid=single_output_iid
    )


if __name__ == '__main__':
    import sys
    import argparse
    from arg_utils import set_defaults_with_yaml_config
    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', default=7, type=int)
    parser.add_argument('--method', default='baseline', choices=['baseline', 'lila'])
    parser.add_argument('--approx', default='full', choices=['full', 'kron', 'blockdiag', 'diag', 'kernel'])
    parser.add_argument('--curv', default='ggn', choices=['ggn', 'ef'])
    parser.add_argument('--dataset', default='mnist', choices=[
        'mnist', 'mnist_r90', 'mnist_r180', 'translated_mnist', 'scaled_mnist', 'scaled_mnist2',
        'fmnist', 'fmnist_r90', 'fmnist_r180', 'translated_fmnist', 'scaled_fmnist', 'scaled_fmnist2',
        'cifar10', 'cifar10_r90', 'cifar10_r180', 'translated_cifar10', 'scaled_cifar10',
        'cifar100', 'cifar100_r90', 'cifar100_r180', 'translated_cifar100', 'scaled_cifar100',
        'tinyimagenet', 
    ])
    parser.add_argument('--model', default='mlp', choices=['mininet', 'mlp', 'cnn', 'resnet', 'wrn'])
    parser.add_argument('--n_epochs', default=500, type=int)
    parser.add_argument('--n_epochs_burnin', default=10, type=int)
    parser.add_argument('--independent_outputs', default=False, action=argparse.BooleanOptionalAction,
                        help='Independent outputs (only for Functional, use in conjunction with single_output)')
    parser.add_argument('--single_output', default=False, action=argparse.BooleanOptionalAction,
                        help='Sample only single output (output-wise partitioning)')
    parser.add_argument('--single_output_iid', default=False, action=argparse.BooleanOptionalAction,
                        help='Only applies when single_output=True. Sample single output iid instead of one output per batch.')
    parser.add_argument('--kron_jac', default=True, action=argparse.BooleanOptionalAction,
                        help='Use Kronecker (approximation) for Jacobians (where applicable). Always faster, sometimes an approx.')
    parser.add_argument('--subset_size', default=-1, type=int, help='Observations in generated data.')
    parser.add_argument('--batch_size', default=-1, type=int)
    parser.add_argument('--marglik_frequency', default=1, type=int)
    parser.add_argument('--marglik_batch_size', default=-1, type=int, help='Used for fitting laplace.')
    parser.add_argument('--partial_batch_size', default=-1, type=int, help='Used for JVPs when necessary.')
    parser.add_argument('--grouped_loader', default=False, action=argparse.BooleanOptionalAction)
    parser.add_argument('--bound', default=False, action=argparse.BooleanOptionalAction,
                        help='Whether to use the lower bound estimators or aggregate full batch.')
    parser.add_argument('--n_hypersteps', default=1, help='Number of steps on every marglik estimate (partial grad accumulation)', type=int)
    parser.add_argument('--n_hypersteps_prior', default=1, help='Same as n_hypersteps but for the prior precision.', type=int)
    parser.add_argument('--n_samples_aug', default=31, type=int, help='Number of augmentation samples for lila.')
    parser.add_argument('--random_flip', default=False, action=argparse.BooleanOptionalAction,
                        help='Randomly flip input images (only for lila).')
    parser.add_argument('--prior_prec_init', default=1.0, type=float)
    parser.add_argument('--lr', default=0.1, type=float)
    parser.add_argument('--lr_min', default=0.1, type=float)
    parser.add_argument('--lr_hyp', default=0.1, type=float)
    parser.add_argument('--lr_hyp_min', default=0.1, type=float)
    parser.add_argument('--lr_aug', default=0.005, type=float)
    parser.add_argument('--lr_aug_min', default=0.00001, type=float)
    parser.add_argument('--temperature', default=1.0, type=float)
    parser.add_argument('--device', default='cuda', choices=['cpu', 'cuda'])
    parser.add_argument('--download_data', default=False, action=argparse.BooleanOptionalAction)
    parser.add_argument('--data_root', default='data')
    parser.add_argument('--config', nargs='+')
    set_defaults_with_yaml_config(parser, sys.argv)
    args = vars(parser.parse_args())
    args.pop('config')
    import uuid
    import copy
    bound_tag = 'lower' if args['bound'] else 'nobound'
    tags = [args['dataset'], args['model'], args['approx'], bound_tag]
    config = copy.deepcopy(args)
    config['map'] = (args['n_epochs_burnin'] > args['n_epochs'])
    if config['map']:  # MAP
        tags = [args['dataset'], args['model'], 'map']
    run_name = '-'.join(tags)
    if args['method'] == 'lila':
        run_name += '-lila'
    run_name += '-' + str(uuid.uuid5(uuid.NAMESPACE_DNS, str(args)))[:4]
    load_dotenv()
    wandb.init(project='ntk-marglik', config=config, name=run_name, tags=tags)
    main(**args)

File Path: data_utils/__init__.py
Content:
from .datasets import *
from .utils import *
from .augmentation import *

File Path: data_utils/augmentation.py
Content:
import torch
from torchvision import transforms

cifar_mean = (0.49139968, 0.48215841, 0.44653091)
cifar_std = (0.24703223, 0.24348513, 0.26158784)
imagenet_mean = (0.485, 0.456, 0.406)
imagenet_std = (0.229, 0.224, 0.225)

CIFAR_transform = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize(cifar_mean, cifar_std),
    ]
)

MNIST_transform = transforms.ToTensor()

ImageNet_transform = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize(imagenet_mean, imagenet_std)
    ]
)

File Path: data_utils/datasets.py
Content:
import numpy as np
import torch
from torchvision.datasets import ImageFolder
from typing import Union, Callable 
import numpy as np
import torch
import torch.nn.functional as F
from torchvision import datasets

LOG2 = np.log(2.0)


class RotatedMNIST(datasets.MNIST):
    """ Rotated MNIST class.
        Wraps regular pytorch MNIST and rotates each image randomly between given angle using fixed seed. """

    def __init__(self, root: str, degree: float, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``MNIST/processed/training.pt``
                and  ``MNIST/processed/test.pt`` exist.
            degree (float): Amount of rotation in degrees
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.MNIST)
        """
        # Use fixed seed (0 for test set, 1 for training set)
        torch.manual_seed(int(train))

        # Sample radians
        rad = np.radians(degree)
        thetas = (torch.rand(len(self.data)) * 2 - 1) * rad

        # Create rotation matrices
        c, s = torch.cos(thetas), torch.sin(thetas)
        rot_matrices = torch.stack((torch.stack((c, -s, c*0), 0),
                                    torch.stack((s, c, s*0), 0)), 0).permute(2, 0, 1)

        # Create resampling grid
        rot_grids = F.affine_grid(rot_matrices, self.data.unsqueeze(1).shape, align_corners=False)

        # Rotate images using 'mode' interpolation (e.g. bilinear sampling)
        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), rot_grids, align_corners=False, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class RotatedFashionMNIST(datasets.FashionMNIST):
    """ Rotated Fashion MNIST dataset.
        Wraps regular pytorch FashionMNIST and rotates each image randomly between given angle using fixed seed. """

    def __init__(self, root: str, degree: float, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``MNIST/processed/training.pt``
                and  ``MNIST/processed/test.pt`` exist.
            degree (float): Amount of rotation in degrees
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.MNIST)
        """

        # Use fixed seed (0 for test set, 1 for training set)
        torch.manual_seed(int(train))

        # Sample radians
        rad = np.radians(degree)
        thetas = (torch.rand(len(self.data)) * 2 - 1) * rad

        # Create rotation matrices
        c, s = torch.cos(thetas), torch.sin(thetas)
        rot_matrices = torch.stack((torch.stack((c, -s, c*0), 0),
                                    torch.stack((s, c, s*0), 0)), 0).permute(2, 0, 1)

        # Create resampling grid
        rot_grids = F.affine_grid(rot_matrices, self.data.unsqueeze(1).shape, align_corners=False)

        # Rotate images using 'mode' interpolation (e.g. bilinear sampling)
        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), rot_grids, align_corners=False, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class TranslatedMNIST(datasets.MNIST):
    """ MNIST translated by fixed amount using random seed """

    def __init__(self, root: str, amount: float = 8.0, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``MNIST/processed/training.pt``
                and  ``MNIST/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.MNIST)
        """

        torch.manual_seed(int(train))

        rad = np.radians(amount)

        r1 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0
        r2 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0

        zero = r1 * 0.0
        one = zero + 1.0

        matrices = torch.stack((torch.stack((one, zero, r1), 0),
                                torch.stack((zero, one, r2), 0)), 0).permute(2, 0, 1)

        grids = F.affine_grid(matrices, self.data.unsqueeze(1).shape, align_corners=True)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), grids, align_corners=True, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class ScaledMNIST(datasets.MNIST):
    """ MNIST scaled by fixed amount using random seed """

    def __init__(self, root: str, amount: float = LOG2, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``MNIST/processed/training.pt``
                and  ``MNIST/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.MNIST)
        """
        torch.manual_seed(int(train))
        s1 = (torch.rand(len(self.data)) * 2 - 1) * amount

        zero = s1 * 0.0

        matrices = torch.stack((torch.stack((torch.exp(s1), zero, zero), 0),
                                torch.stack((zero, torch.exp(s1), zero), 0)), 0).permute(2, 0, 1)

        grids = F.affine_grid(matrices, self.data.unsqueeze(1).shape, align_corners=True)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), grids, align_corners=True, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class RotatedFashionMNIST(datasets.FashionMNIST):
    """ Rotated FashionMNIST class.
        Wraps regular pytorch FashionMNIST and rotates each image randomly between given angle using fixed seed. """

    def __init__(self, root: str, degree: float, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``FashionMNIST/processed/training.pt``
                and  ``FashionMNIST/processed/test.pt`` exist.
            degree (float): Amount of rotation in degrees
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.FashionMNIST)
        """
        # Use fixed seed (0 for test set, 1 for training set)
        torch.manual_seed(int(train))

        # Sample radians
        rad = np.radians(degree)
        thetas = (torch.rand(len(self.data)) * 2 - 1) * rad

        # Create rotation matrices
        c, s = torch.cos(thetas), torch.sin(thetas)
        rot_matrices = torch.stack((torch.stack((c, -s, c*0), 0),
                                    torch.stack((s, c, s*0), 0)), 0).permute(2, 0, 1)

        # Create resampling grid
        rot_grids = F.affine_grid(rot_matrices, self.data.unsqueeze(1).shape, align_corners=False)

        # Rotate images using 'mode' interpolation (e.g. bilinear sampling)
        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), rot_grids, align_corners=False, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class RotatedFashionMNIST(datasets.FashionMNIST):
    """ Rotated Fashion MNIST dataset.
        Wraps regular pytorch FashionMNIST and rotates each image randomly between given angle using fixed seed. """

    def __init__(self, root: str, degree: float, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``FashionMNIST/processed/training.pt``
                and  ``FashionMNIST/processed/test.pt`` exist.
            degree (float): Amount of rotation in degrees
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.FashionMNIST)
        """
        # Use fixed seed (0 for test set, 1 for training set)
        torch.manual_seed(int(train))

        # Sample radians
        rad = np.radians(degree)
        thetas = (torch.rand(len(self.data)) * 2 - 1) * rad

        # Create rotation matrices
        c, s = torch.cos(thetas), torch.sin(thetas)
        rot_matrices = torch.stack((torch.stack((c, -s, c*0), 0),
                                    torch.stack((s, c, s*0), 0)), 0).permute(2, 0, 1)

        # Create resampling grid
        rot_grids = F.affine_grid(rot_matrices, self.data.unsqueeze(1).shape, align_corners=False)

        # Rotate images using 'mode' interpolation (e.g. bilinear sampling)
        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), rot_grids, align_corners=False, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class TranslatedFashionMNIST(datasets.FashionMNIST):
    """ FashionMNIST translated by fixed amount using random seed """

    def __init__(self, root: str, amount: float = 8.0, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``FashionMNIST/processed/training.pt``
                and  ``FashionMNIST/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.FashionMNIST)
        """
        torch.manual_seed(int(train))
        r1 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0
        r2 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0

        zero = r1 * 0.0
        one = zero + 1.0

        matrices = torch.stack((torch.stack((one, zero, r1), 0),
                                torch.stack((zero, one, r2), 0)), 0).permute(2, 0, 1)

        grids = F.affine_grid(matrices, self.data.unsqueeze(1).shape, align_corners=True)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), grids, align_corners=True, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class ScaledFashionMNIST(datasets.FashionMNIST):
    """ FashionMNIST scaled by fixed amount using random seed """

    def __init__(self, root: str, amount: float = LOG2, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``FashionMNIST/processed/training.pt``
                and  ``FashionMNIST/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.FashionMNIST)
        """

        torch.manual_seed(int(train))

        s1 = (torch.rand(len(self.data)) * 2 - 1) * amount
        zero = s1 * 0.0
        matrices = torch.stack((torch.stack((torch.exp(s1), zero, zero), 0),
                                torch.stack((zero, torch.exp(s1), zero), 0)), 0).permute(2, 0, 1)

        grids = F.affine_grid(matrices, self.data.unsqueeze(1).shape, align_corners=True)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = F.grid_sample(self.data.unsqueeze(1).float(), grids, align_corners=True, mode=mode)
        self.data = self.data[:, 0].type(data_dtype).clamp(xmin, xmax)


class RotatedCIFAR10(datasets.CIFAR10):
    """ CIFAR10 rotated by fixed amount using random seed """

    def __init__(self, root: str, degree: float, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``CIFAR10/processed/training.pt``
                and  ``CIFAR10/processed/test.pt`` exist.
            degree (float): Amount of rotation in degrees
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.CIFAR10)
        """

        torch.manual_seed(int(train))

        rad = np.radians(degree)

        thetas = (torch.rand(len(self.data)) * 2 - 1) * rad

        c, s = torch.cos(thetas), torch.sin(thetas)

        rot_matrices = torch.stack((torch.stack((c, -s, c*0), 0),
                                    torch.stack((s, c, s*0), 0)), 0).permute(2, 0, 1)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = torch.Tensor(self.data.transpose(0, 3, 1, 2))
        rot_grids = F.affine_grid(rot_matrices, self.data.shape, align_corners=False)

        self.data = F.grid_sample(self.data.float(), rot_grids, align_corners=False, mode=mode)
        self.data = self.data.clamp(xmin, xmax).numpy().astype(data_dtype)
        self.data = self.data.transpose(0, 2, 3, 1)


class TranslatedCIFAR10(datasets.CIFAR10):
    """ CIFAR10 translated by fixed amount using random seed """

    def __init__(self, root: str, amount: float = 8.0, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``CIFAR10/processed/training.pt``
                and  ``CIFAR10/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.CIFAR10)
        """

        torch.manual_seed(int(train))

        rad = np.radians(amount)

        r1 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0
        r2 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0

        zero = r1 * 0.0
        one = zero + 1.0

        matrices = torch.stack((torch.stack((one, zero, r1), 0),
                                torch.stack((zero, one, r2), 0)), 0).permute(2, 0, 1)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = torch.Tensor(self.data.transpose(0, 3, 1, 2))
        grids = F.affine_grid(matrices, self.data.shape, align_corners=False)

        self.data = F.grid_sample(self.data.float(), grids, align_corners=False, mode=mode)
        self.data = self.data.clamp(xmin, xmax).numpy().astype(data_dtype)
        self.data = self.data.transpose(0, 2, 3, 1)


class ScaledCIFAR10(datasets.CIFAR10):
    """ CIFAR10 scaled by fixed amount using random seed """

    def __init__(self, root: str, amount: float = LOG2, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``CIFAR10/processed/training.pt``
                and  ``CIFAR10/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.CIFAR10)
        """

        torch.manual_seed(int(train))

        rad = np.radians(amount)

        s1 = (torch.rand(len(self.data)) * 2 - 1) * amount

        zero = s1 * 0.0
        one = zero + 1.0

        matrices = torch.stack((torch.stack((torch.exp(s1), zero, zero), 0),
                                torch.stack((zero, torch.exp(s1), zero), 0)), 0).permute(2, 0, 1)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = torch.Tensor(self.data.transpose(0, 3, 1, 2))
        grids = F.affine_grid(matrices, self.data.shape, align_corners=False)

        self.data = F.grid_sample(self.data.float(), grids, align_corners=False, mode=mode)
        self.data = self.data.clamp(xmin, xmax).numpy().astype(data_dtype)
        self.data = self.data.transpose(0, 2, 3, 1)


class RotatedCIFAR100(datasets.CIFAR100):
    """ CIFAR100 rotated by fixed amount using random seed """

    def __init__(self, root: str, degree: float, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``CIFAR100/processed/training.pt``
                and  ``CIFAR100/processed/test.pt`` exist.
            degree (float): Amount of rotation in degrees
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.CIFAR100)
        """

        torch.manual_seed(int(train))

        rad = np.radians(degree)

        thetas = (torch.rand(len(self.data)) * 2 - 1) * rad

        c, s = torch.cos(thetas), torch.sin(thetas)

        rot_matrices = torch.stack((torch.stack((c, -s, c*0), 0),
                                    torch.stack((s, c, s*0), 0)), 0).permute(2, 0, 1)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = torch.Tensor(self.data.transpose(0, 3, 1, 2))
        rot_grids = F.affine_grid(rot_matrices, self.data.shape, align_corners=False)

        self.data = F.grid_sample(self.data.float(), rot_grids, align_corners=False, mode=mode)
        self.data = self.data.clamp(xmin, xmax).numpy().astype(data_dtype)
        self.data = self.data.transpose(0, 2, 3, 1)


class TranslatedCIFAR100(datasets.CIFAR100):
    """ CIFAR100 translated by fixed amount using random seed """

    def __init__(self, root: str, amount: float = 8.0, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``CIFAR100/processed/training.pt``
                and  ``CIFAR100/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.CIFAR100)
        """

        torch.manual_seed(int(train))

        rad = np.radians(amount)

        r1 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0
        r2 = (torch.rand(len(self.data)) * 2 - 1) * amount / 14.0

        zero = r1 * 0.0
        one = zero + 1.0

        matrices = torch.stack((torch.stack((one, zero, r1), 0),
                                torch.stack((zero, one, r2), 0)), 0).permute(2, 0, 1)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = torch.Tensor(self.data.transpose(0, 3, 1, 2))
        grids = F.affine_grid(matrices, self.data.shape, align_corners=False)

        self.data = F.grid_sample(self.data.float(), grids, align_corners=False, mode=mode)
        self.data = self.data.clamp(xmin, xmax).numpy().astype(data_dtype)
        self.data = self.data.transpose(0, 2, 3, 1)


class ScaledCIFAR100(datasets.CIFAR100):
    """ CIFAR100 scaled by fixed amount using random seed """

    def __init__(self, root: str, amount: float = LOG2, mode: str = 'bilinear',
                 train: bool = True, transform: Union[Callable, type(None)] = None,
                 target_transform: Union[Callable, type(None)] = None, download: bool = False):
        super().__init__(root, train, transform, target_transform, download)
        """
        Args:
            root (string): Root directory of dataset where ``CIFAR100/processed/training.pt``
                and  ``CIFAR100/processed/test.pt`` exist.
            amount (float): Amount of translation in pixels
            mode (float): Mode used for interpolation (nearest|bilinear|bicubic)
            train (bool, optional): If True, creates dataset from ``training.pt``,
                otherwise from ``test.pt``.
            download (bool, optional): If true, downloads the dataset from the internet and
                puts it in root directory. If dataset is already downloaded, it is not
                downloaded again.
            transform (callable, optional): A function/transform that  takes in an PIL image
                and returns a transformed version. E.g, ``transforms.RandomCrop``
            target_transform (callable, optional): A function/transform that takes in the
                target and transforms it.
                same arguments as torchvision.see arguments of torchvision.dataset.CIFAR100)
        """

        torch.manual_seed(int(train))

        rad = np.radians(amount)

        s1 = (torch.rand(len(self.data)) * 2 - 1) * amount

        zero = s1 * 0.0
        one = zero + 1.0

        matrices = torch.stack((torch.stack((torch.exp(s1), zero, zero), 0),
                                torch.stack((zero, torch.exp(s1), zero), 0)), 0).permute(2, 0, 1)

        xmin, xmax = self.data.min(), self.data.max()
        data_dtype = self.data.dtype

        self.data = torch.Tensor(self.data.transpose(0, 3, 1, 2))
        grids = F.affine_grid(matrices, self.data.shape, align_corners=False)

        self.data = F.grid_sample(self.data.float(), grids, align_corners=False, mode=mode)
        self.data = self.data.clamp(xmin, xmax).numpy().astype(data_dtype)
        self.data = self.data.transpose(0, 2, 3, 1)


def get_dataset(dataset, data_root, download_data, transform):
    if dataset == 'mnist':
        train_dataset = RotatedMNIST(data_root, 0, train=True, download=download_data, transform=transform)
        test_dataset = RotatedMNIST(data_root, 0, train=False, download=download_data, transform=transform)
    elif dataset == 'mnist_r90':
        train_dataset = RotatedMNIST(data_root, 90, train=True, download=download_data, transform=transform)
        test_dataset = RotatedMNIST(data_root, 90, train=False, download=download_data, transform=transform)
    elif dataset == 'mnist_r180':
        train_dataset = RotatedMNIST(data_root, 180, train=True, download=download_data, transform=transform)
        test_dataset = RotatedMNIST(data_root, 180, train=False, download=download_data, transform=transform)
    elif dataset == 'translated_mnist':
        train_dataset = TranslatedMNIST(data_root, 8, train=True, download=download_data, transform=transform)
        test_dataset = TranslatedMNIST(data_root, 8, train=False, download=download_data, transform=transform)
    elif dataset == 'scaled_mnist':
        train_dataset = ScaledMNIST(data_root, np.log(2), train=True, download=download_data, transform=transform)
        test_dataset = ScaledMNIST(data_root, np.log(2), train=False, download=download_data, transform=transform)
    elif dataset == 'fmnist':
        train_dataset = RotatedFashionMNIST(data_root, 0, train=True, download=download_data, transform=transform)
        test_dataset = RotatedFashionMNIST(data_root, 0, train=False, download=download_data, transform=transform)
    elif dataset == 'fmnist_r90':
        train_dataset = RotatedFashionMNIST(data_root, 90, train=True, download=download_data, transform=transform)
        test_dataset = RotatedFashionMNIST(data_root, 90, train=False, download=download_data, transform=transform)
    elif dataset == 'fmnist_r180':
        train_dataset = RotatedFashionMNIST(data_root, 180, train=True, download=download_data, transform=transform)
        test_dataset = RotatedFashionMNIST(data_root, 180, train=False, download=download_data, transform=transform)
    elif dataset == 'translated_fmnist':
        train_dataset = TranslatedFashionMNIST(data_root, 8, train=True, download=download_data, transform=transform)
        test_dataset = TranslatedFashionMNIST(data_root, 8, train=False, download=download_data, transform=transform)
    elif dataset == 'scaled_fmnist':
        train_dataset = ScaledFashionMNIST(data_root, np.log(2), train=True, download=download_data, transform=transform)
        test_dataset = ScaledFashionMNIST(data_root, np.log(2), train=False, download=download_data, transform=transform)
    elif dataset == 'cifar10':
        train_dataset = RotatedCIFAR10(data_root, 0, train=True, download=download_data, transform=transform)
        test_dataset = RotatedCIFAR10(data_root, 0, train=False, download=download_data, transform=transform)
    elif dataset == 'cifar10_r90':
        train_dataset = RotatedCIFAR10(data_root, 90, train=True, download=download_data, transform=transform)
        test_dataset = RotatedCIFAR10(data_root, 90, train=False, download=download_data, transform=transform)
    elif dataset == 'cifar10_r180':
        train_dataset = RotatedCIFAR10(data_root, 180, train=True, download=download_data, transform=transform)
        test_dataset = RotatedCIFAR10(data_root, 180, train=False, download=download_data, transform=transform)
    elif dataset == 'translated_cifar10':
        train_dataset = TranslatedCIFAR10(data_root, 8, train=True, download=download_data, transform=transform)
        test_dataset = TranslatedCIFAR10(data_root, 8, train=False, download=download_data, transform=transform)
    elif dataset == 'scaled_cifar10':
        train_dataset = ScaledCIFAR10(data_root, np.log(2), train=True, download=download_data, transform=transform)
        test_dataset = ScaledCIFAR10(data_root, np.log(2), train=False, download=download_data, transform=transform)
    elif dataset == 'cifar100':
        train_dataset = RotatedCIFAR100(data_root, 0, train=True, download=download_data, transform=transform)
        test_dataset = RotatedCIFAR100(data_root, 0, train=False, download=download_data, transform=transform)
    elif dataset == 'cifar100_r90':
        train_dataset = RotatedCIFAR100(data_root, 90, train=True, download=download_data, transform=transform)
        test_dataset = RotatedCIFAR100(data_root, 90, train=False, download=download_data, transform=transform)
    elif dataset == 'cifar100_r180':
        train_dataset = RotatedCIFAR100(data_root, 180, train=True, download=download_data, transform=transform)
        test_dataset = RotatedCIFAR100(data_root, 180, train=False, download=download_data, transform=transform)
    elif dataset == 'translated_cifar100':
        train_dataset = TranslatedCIFAR100(data_root, 8, train=True, download=download_data, transform=transform)
        test_dataset = TranslatedCIFAR100(data_root, 8, train=False, download=download_data, transform=transform)
    elif dataset == 'scaled_cifar100':
        train_dataset = ScaledCIFAR100(data_root, np.log(2), train=True, download=download_data, transform=transform)
        test_dataset = ScaledCIFAR100(data_root, np.log(2), train=False, download=download_data, transform=transform)
    elif dataset == 'tinyimagenet':
        train_dataset = ImageFolder(data_root + '/tiny-imagenet-200/train', transform=transform)
        test_dataset = ImageFolder(data_root + '/tiny-imagenet-200/val', transform=transform)
    elif dataset == 'imagenet':
        pass
    else:
        raise NotImplementedError(f'Unknown dataset: {dataset}')
    return train_dataset, test_dataset

File Path: data_utils/utils.py
Content:
import numpy as np
import torch
from math import ceil


class TensorDataLoader:
    """Combination of torch's DataLoader and TensorDataset for efficient batch
    sampling and adaptive augmentation on GPU.
    """

    def __init__(self, x, y, transform=None, transform_y=None, batch_size=500,
                 data_factor=1, shuffle=False, detach=False):
        assert x.size(0) == y.size(0), 'Size mismatch'
        self.x = x
        self.y = y
        self.device = x.device
        self.data_factor = data_factor
        self.n_data = y.size(0)
        self.batch_size = batch_size
        self.n_batches = ceil(self.n_data / self.batch_size)
        self.shuffle = shuffle
        identity = lambda x: x
        self.transform = transform if transform is not None else identity
        self.transform_y = transform_y if transform_y is not None else identity
        self._detach = detach

    def __iter__(self):
        if self.shuffle:
            permutation = torch.randperm(self.n_data, device=self.device)
            self.x = self.x[permutation]
            self.y = self.y[permutation]
        self.i_batch = 0
        return self

    def __next__(self):
        if self.i_batch >= self.n_batches:
            raise StopIteration

        start = self.i_batch * self.batch_size
        end = start + self.batch_size
        if self._detach:
            x = self.transform(self.x[start:end]).detach()
        else:
            x = self.transform(self.x[start:end])
        y = self.transform_y(self.y[start:end])
        self.i_batch += 1
        return (x, y)

    def __len__(self):
        return self.n_batches

    def attach(self):
        self._detach = False
        return self

    def detach(self):
        self._detach = True
        return self

    @property
    def dataset(self):
        return DatasetDummy(self.n_data * self.data_factor)


class SubsetTensorDataLoader(TensorDataLoader):

    def __init__(self, x, y, transform=None, transform_y=None, subset_size=500,
                 data_factor=1, detach=False):
        self.subset_size = subset_size
        super().__init__(x, y, transform, transform_y, batch_size=subset_size,
                         data_factor=data_factor, shuffle=True, detach=detach)
        self.n_batches = 1  # -> len(loader) = 1

    def __iter__(self):
        self.i_batch = 0
        return self

    def __next__(self):
        if self.i_batch >= self.n_batches:
            raise StopIteration

        sod_indices = np.random.choice(self.n_data, self.subset_size, replace=False)
        if self._detach:
            x = self.transform(self.x[sod_indices]).detach()
        else:
            x = self.transform(self.x[sod_indices])
        y = self.transform_y(self.y[sod_indices])
        self.i_batch += 1
        return (x, y)

    def __len__(self):
        return 1

    @property
    def dataset(self):
        return DatasetDummy(self.subset_size * self.data_factor)

        
class GroupedSubsetTensorDataLoader(SubsetTensorDataLoader):
    
    def __init__(self, x, y, transform=None, transform_y=None, subset_size=500, data_factor=1, detach=False):
        super().__init__(x, y, transform, transform_y, subset_size, data_factor, detach)
        self.class_indices = list()
        for c in torch.unique(self.y):
            self.class_indices.append((self.y == c).nonzero().squeeze())
        self.n_classes = len(self.class_indices)

    def __next__(self):
        if self.i_batch >= self.n_batches:
            raise StopIteration

        # class order
        # NOTE: assumes roughly stratified classes
        class_order = np.random.permutation(self.n_classes)
        sod_indices = list()
        n_data_remaining = self.subset_size
        for c in class_order:
            if n_data_remaining == 0:
                break
            local_indices = self.class_indices[c]
            local_shuffle = torch.randperm(len(local_indices))
            c_indices = local_indices[local_shuffle][:n_data_remaining]
            n_data_remaining -= len(c_indices)
            sod_indices.append(c_indices)
        sod_indices = torch.cat(sod_indices)

        if self._detach:
            x = self.transform(self.x[sod_indices]).detach()
        else:
            x = self.transform(self.x[sod_indices])
        y = self.transform_y(self.y[sod_indices])
        self.i_batch += 1
        return (x, y)


class DatasetDummy:
    def __init__(self, N):
        self.N = N

    def __len__(self):
        return int(self.N)


def dataset_to_tensors(dataset, indices=None, device='cuda'):
    if indices is None:
        indices = range(len(dataset))  # all
    xy_train = [dataset[i] for i in indices]
    x = torch.stack([e[0] for e in xy_train]).to(device)
    y = torch.stack([torch.tensor(e[1]) for e in xy_train]).to(device)
    return x, y

File Path: dependencies/asdl/asdfghjkl/__init__.py
Content:
from .utils import *
from .operations import *
from .core import extend
from .symmatrix import *
from .matrices import *
from .gradient import *
from .mvp import *
from .fisher import *
from .hessian import *
from .kernel import *
from .precondition import *
from .fr import *

File Path: dependencies/asdl/asdfghjkl/core.py
Content:
from typing import List
from contextlib import contextmanager
from asdfghjkl.operations.conv_aug import Conv2dAug, Conv1dAug

import torch.nn as nn
from .utils import im2col_2d, im2col_2d_aug, record_original_requires_grad, arr2col_1d, arr2col_1d_aug
from .operations import OP_ACCUMULATE_GRADS, get_op_class


@contextmanager
def extend(model, op_names):
    if not isinstance(op_names, (list, tuple)):
        op_names = [op_names]
    accumulate_grads = False
    if OP_ACCUMULATE_GRADS in op_names:
        accumulate_grads = True
        op_names = [name for name in op_names if name != OP_ACCUMULATE_GRADS]
    handles = []

    def forward_hook(module, in_data, out_data):
        in_data = in_data[0]
        in_data = _preprocess_in_data(module, in_data, out_data)
        _call_operations_in_forward(module, in_data)

        def backward_hook(out_grads):
            out_grads = _preprocess_out_grads(module, out_grads)
            _call_operations_in_backward(module, in_data, out_grads)

        if out_data.requires_grad:
            handles.append(out_data.register_hook(backward_hook))

    for module in model.modules():
        requires_grad = False
        for attr in ['weight', 'bias']:
            param = getattr(module, attr, None)
            if param is not None:
                requires_grad = requires_grad or param.requires_grad
                record_original_requires_grad(param)
        if not requires_grad:
            continue
        # register hooks and operations in modules
        handles.append(module.register_forward_hook(forward_hook))
        _register_operations(model, module, op_names)

    yield

    # remove hooks and operations from modules
    for handle in handles:
        handle.remove()
    for module in model.modules():
        _remove_operations(module)

    # accumulate param.grad to param.acc_grad
    if accumulate_grads:
        attr = OP_ACCUMULATE_GRADS
        for param in model.parameters():
            if param.grad is None:
                continue
            if not hasattr(param, attr):
                setattr(param, attr, param.grad)
            else:
                acc_grad = getattr(param, attr)
                acc_grad.add_(param.grad)


def _preprocess_in_data(module, in_data, out_data):
    if isinstance(module, Conv2dAug):
        in_data = im2col_2d_aug(in_data, module)

    elif isinstance(module, nn.Conv2d):
        in_data = im2col_2d(in_data, module)

    elif isinstance(module, Conv1dAug):
        in_data = arr2col_1d_aug(in_data, module)

    elif isinstance(module, nn.Conv1d):
        in_data = arr2col_1d(in_data, module)

    elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
        bnorm = module
        f = bnorm.num_features
        if isinstance(module, nn.BatchNorm1d):
            shape = (1, f)
        elif isinstance(module, nn.BatchNorm2d):
            shape = (1, f, 1, 1)
        else:
            shape = (1, f, 1, 1, 1)
        # restore normalized input
        in_data_norm = (out_data -
                        bnorm.bias.view(shape)).div(bnorm.weight.view(shape))
        in_data = in_data_norm

    elif isinstance(module, nn.LayerNorm):
        layernorm = module
        # restore normalized input
        in_data_norm = (out_data - layernorm.bias).div(layernorm.weight)
        in_data = in_data_norm

    return in_data


def _preprocess_out_grads(module, out_grads):
    if isinstance(module, Conv2dAug):
        out_grads = out_grads.flatten(start_dim=3)
    elif isinstance(module, nn.Conv2d):
        out_grads = out_grads.flatten(start_dim=2)

    return out_grads


def _register_operations(model: nn.Module, module: nn.Module, op_names: List):
    op_class = get_op_class(module)
    if op_class is not None:
        setattr(module, 'operation', op_class(module, model, op_names))


def _call_operations_in_forward(module, in_data):
    if hasattr(module, 'operation'):
        module.operation.forward_post_process(in_data)


def _call_operations_in_backward(module, in_data, out_grads):
    if hasattr(module, 'operation'):
        module.operation.backward_pre_process(in_data, out_grads)


def _remove_operations(module):
    if hasattr(module, 'operation'):
        delattr(module, 'operation')

File Path: dependencies/asdl/asdfghjkl/fisher.py
Content:
from math import sqrt
from functools import partial
from contextlib import contextmanager
import torch
import torch.nn.functional as F
from .core import extend
from .utils import disable_param_grad
from .gradient import data_loader_gradient
from .operations import *
from .symmatrix import SymMatrix, Kron, Diag, UnitWise
from .matrices import *
from .mvp import power_method, conjugate_gradient_method
from .loss import heteroscedastic_mse_loss

_SHAPE_TO_OP = {
    SHAPE_FULL: OP_BATCH_GRADS,  # full
    SHAPE_BLOCK_DIAG: OP_BATCH_GRADS,  # block-diagonal
    SHAPE_KRON: OP_COV_KRON,  # Kronecker-factored
    SHAPE_DIAG: OP_COV_DIAG,  # diagonal
}

_COV_FULL = 'cov_full'
_CVP_FULL = 'cvp_full'
_COV_BLOCK_DIAG = 'cov_block_diag'
_CVP_BLOCK_DIAG = 'cvp_block_diag'

__all__ = [
    'fisher_for_cross_entropy',
    'fvp_for_cross_entropy',
    'zero_fisher',
    'zero_fvp',
    'fisher_for_cross_entropy_eigenvalues',
    'fisher_free_for_cross_entropy'
]

_supported_types = [FISHER_EXACT, FISHER_MC, COV]
_supported_types_for_eig = _supported_types
_supported_shapes = [SHAPE_FULL, SHAPE_BLOCK_DIAG, SHAPE_KRON, SHAPE_DIAG]
_supported_shapes_for_eig = [SHAPE_FULL, SHAPE_BLOCK_DIAG]


def fisher(
    model,
    fisher_types,
    fisher_shapes,
    inputs=None,
    targets=None,
    data_loader=None,
    stats_name=None,
    compute_param_grad=False,
    n_mc_samples=1,
    is_distributed=False,
    all_reduce=False,
    is_master=True,
    matrix_manager=None,
    single_output=None,
    likelihood='classification',
    **backward_kwargs
):
    if isinstance(fisher_types, str):
        fisher_types = [fisher_types]
    if isinstance(fisher_shapes, str):
        fisher_shapes = [fisher_shapes]
    # remove duplicates
    fisher_types = set(fisher_types)
    fisher_shapes = set(fisher_shapes)
    for ftype in fisher_types:
        assert ftype in _supported_types, \
            f'Invalid fisher_type: {ftype}. ' \
            f'fisher_type must be in {_supported_types}.'
    for fshape in fisher_shapes:
        assert fshape in _supported_shapes, \
            f'Invalid fisher_shape: {fshape}. ' \
            f'fisher_shape must be in {_supported_shapes}.'

    zero_fisher(model, fisher_types)

    # setup operations for mammoth_utils.autograd.extend
    op_names = [_SHAPE_TO_OP[shape] for shape in fisher_shapes]
    if compute_param_grad:
        assert COV in fisher_types, \
            f'"{COV}" must be in fisher_types when compute_param_grad is True.'
        if data_loader is not None:
            op_names.append(OP_ACCUMULATE_GRADS)  # accumulate gradient

    # setup matrix manager as needed
    if matrix_manager is None:
        matrix_manager = MatrixManager(model, fisher_types)

    kwargs = dict(
        compute_full_fisher=SHAPE_FULL in fisher_shapes,
        compute_block_diag_fisher=SHAPE_BLOCK_DIAG in fisher_shapes,
        compute_param_grad=compute_param_grad,
        n_mc_samples=n_mc_samples,
        single_output=single_output,
        backward_kwargs=backward_kwargs
    )

    if likelihood == 'classification':
        closure = _fisher_for_cross_entropy
    elif likelihood == 'regression':
        closure = _fisher_for_regression
    elif likelihood == 'heteroscedastic_regression':
        closure = _fisher_for_heteroscedastic_regression
    else:
        raise ValueError('Invalid likelihood', likelihood)

    if data_loader is not None:
        # accumulate fisher for an epoch
        device = next(model.parameters()).device
        f = list()
        for inputs, targets in data_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            with extend(model, op_names):
                fi = closure(
                    model, fisher_types, inputs, targets, **kwargs
                )
                f.append(fi)
            f = torch.cat(f)
            if stats_name is not None:
                matrix_manager.accumulate_matrices(stats_name)
        if compute_param_grad:
            data_loader_gradient(
                model,
                data_loader,
                has_accumulated=True,
                is_distributed=is_distributed,
                all_reduce=all_reduce,
                is_master=is_master
            )
    else:
        # compute fisher for a single batch
        assert inputs is not None
        with extend(model, op_names):
            f = closure(
                model, fisher_types, inputs, targets, **kwargs
            )

    # reduce matrices
    if is_distributed:
        matrix_manager.reduce_matrices(stats_name, is_master, all_reduce)

    return f, matrix_manager


fisher_for_cross_entropy = partial(fisher, likelihood='classification')
fisher_for_regression = partial(fisher, likelihood='regression')


def zero_fisher(module, fisher_types):
    for child in module.children():
        zero_fisher(child, fisher_types)
    for ftype in fisher_types:
        if hasattr(module, ftype):
            delattr(module, ftype)


def zero_fvp(module, fisher_types):
    for child in module.children():
        zero_fvp(child, fisher_types)
    for ftype in fisher_types:
        attr = _get_fvp_attr(ftype)
        if hasattr(module, attr):
            delattr(module, attr)


def _check_fisher_type_shape(fisher_type, fisher_shape):
    assert fisher_type in _supported_types_for_eig, \
        f'Invalid fisher_type: {fisher_type}. ' \
        f'fisher_type must be in {_supported_types_for_eig}.'
    assert fisher_shape in _supported_shapes_for_eig, \
        f'Invalid fisher_shape: {fisher_shape}. ' \
        f'fisher_shape must be in {_supported_shapes_for_eig}.'


def fisher_for_cross_entropy_eigenvalues(
        model,
        fisher_type,
        fisher_shape,
        data_loader=None,
        inputs=None,
        targets=None,
        n_mc_samples=1,
        top_n=1,
        max_iters=100,
        tol=1e-3,
        is_distributed=False,
        print_progress=False
):
    _check_fisher_type_shape(fisher_type, fisher_shape)

    def fvp_fn(vec, x, y):
        return fvp_for_cross_entropy(vec,
                                     model,
                                     fisher_type,
                                     fisher_shape,
                                     inputs=x,
                                     targets=y,
                                     n_mc_samples=n_mc_samples)

    # for making MC samplings at each iteration deterministic
    random_seed = torch.rand(1) * 100 if fisher_type == FISHER_MC else None

    eigvals, eigvecs = power_method(fvp_fn,
                                    model,
                                    data_loader=data_loader,
                                    inputs=inputs,
                                    targets=targets,
                                    top_n=top_n,
                                    max_iters=max_iters,
                                    tol=tol,
                                    is_distributed=is_distributed,
                                    print_progress=print_progress,
                                    random_seed=random_seed
                                    )

    return eigvals, eigvecs


def fisher_free_for_cross_entropy(
        model,
        b,
        fisher_type,
        fisher_shape,
        data_loader=None,
        inputs=None,
        targets=None,
        init_x=None,
        damping=1e-3,
        n_mc_samples=1,
        max_iters=None,
        tol=1e-8,
        preconditioner=None,
        is_distributed=False,
        print_progress=False,
        random_seed=None,
        save_log=False
):
    _check_fisher_type_shape(fisher_type, fisher_shape)

    def fvp_fn(vec, x, y):
        return fvp_for_cross_entropy(vec,
                                     model,
                                     fisher_type,
                                     fisher_shape,
                                     inputs=x,
                                     targets=y,
                                     n_mc_samples=n_mc_samples)

    # for making MC samplings at each iteration deterministic
    if fisher_type == FISHER_MC and random_seed is None:
        random_seed = int(torch.rand(1) * 100)

    return conjugate_gradient_method(fvp_fn,
                                     b,
                                     data_loader=data_loader,
                                     inputs=inputs,
                                     targets=targets,
                                     init_x=init_x,
                                     damping=damping,
                                     max_iters=max_iters,
                                     tol=tol,
                                     preconditioner=preconditioner,
                                     is_distributed=is_distributed,
                                     print_progress=print_progress,
                                     random_seed=random_seed,
                                     save_log=save_log)


def fvp_for_cross_entropy(
    vec,
    model,
    fisher_type,
    fisher_shape,
    inputs,
    targets=None,
    n_mc_samples=1
):
    compute_full_fvp = compute_block_diag_fvp = False
    if fisher_shape == SHAPE_FULL:
        compute_full_fvp = True
    elif fisher_shape == SHAPE_BLOCK_DIAG:
        compute_block_diag_fvp = True
    else:
        raise ValueError(f'Invalid fisher_shape: {fisher_shape}.')

    zero_fvp(model, [fisher_type])

    with extend(model, OP_BATCH_GRADS):
        _fisher_for_cross_entropy(
            model, [fisher_type],
            inputs,
            targets,
            compute_full_fvp=compute_full_fvp,
            compute_block_diag_fvp=compute_block_diag_fvp,
            vec=vec,
            n_mc_samples=n_mc_samples
        )

    if fisher_shape == SHAPE_FULL:
        return getattr(model, _get_fvp_attr(fisher_type))
    else:
        rst = []
        for module in model.modules():
            fvp = getattr(module, _get_fvp_attr(fisher_type), None)
            if fvp is not None:
                rst.extend(fvp)
        return rst


def _fisher_for_cross_entropy(
    model,
    fisher_types,
    inputs,
    targets=None,
    compute_param_grad=False,
    compute_full_fisher=False,
    compute_full_fvp=False,
    compute_block_diag_fisher=False,
    compute_block_diag_fvp=False,
    vec=None,
    n_mc_samples=1,
    single_output=None,
    backward_kwargs=None
):
    if backward_kwargs is None:
        backward_kwargs = dict(retain_graph=True)
    else:
        assert type(backward_kwargs) is dict
        backward_kwargs['retain_graph'] = True

    # NOTE: this assumes we either want to diff wrt params OR inputs not both
    if inputs.requires_grad:
        with disable_param_grad(model):
            logits = model(inputs)
    else:
        logits = model(inputs)

    if logits.ndim > 2:
        # reduce augmented dimension
        logits = logits.mean(dim=1)

    if single_output is not None:
        assert FISHER_EXACT in fisher_types
        # compute J_i H_ii J_i^T (single output GGN)
        # H_ii = p_i (1 - p_i)
        probs = F.softmax(logits, dim=1)
        if single_output.ndim == 0:  # scalar
            probs = probs[:, single_output]
            logits_single = logits[:, single_output]
        elif single_output.ndim == 1:  # vector iid
            probs = probs.gather(1, single_output.unsqueeze(-1)).squeeze(-1)
            logits_single = logits.gather(1, single_output.unsqueeze(-1)).squeeze(-1)
        else:
            raise ValueError(f'Invalid single_output shape: {single_output.shape}.')

        def loss_and_backward():
            model.zero_grad(set_to_none=True)
            loss = logits_single.sum()
            loss.backward(**backward_kwargs)
        _fisher_exact_single(loss_and_backward, model, probs)
        return logits

    log_probs = F.log_softmax(logits, dim=1)
    probs = None

    def loss_and_backward(target):
        model.zero_grad(set_to_none=True)
        loss = F.nll_loss(log_probs, target, reduction='sum')
        loss.backward(**backward_kwargs)
        if compute_full_fisher:
            _full_covariance(model)
        if compute_full_fvp:
            _full_cvp(model, vec)
        if compute_block_diag_fisher:
            _block_diag_covariance(model)
        if compute_block_diag_fvp:
            _block_diag_cvp(model, vec)

    if FISHER_MC in fisher_types:
        probs = F.softmax(logits, dim=1)
        _fisher_mc(loss_and_backward, model, probs, n_mc_samples)

    if FISHER_EXACT in fisher_types:
        if probs is None:
            probs = F.softmax(logits, dim=1)
        _fisher_exact(loss_and_backward, model, probs)

    if COV in fisher_types:
        assert targets is not None, 'targets must be specified for computing covariance.'
        _covariance(loss_and_backward, model, targets, compute_param_grad)

    return logits


def _fisher_for_regression(
    model,
    fisher_types,
    inputs,
    targets=None,
    backward_kwargs=None,
    **kwargs
):
    if backward_kwargs is None:
        backward_kwargs = dict(retain_graph=True)
    else:
        assert type(backward_kwargs) is dict
        backward_kwargs['retain_graph'] = True

    # NOTE: this assumes we either want to diff wrt params OR inputs not both
    if inputs.requires_grad:
        with disable_param_grad(model):
            mean = model(inputs)
    else:
        mean = model(inputs)

    if mean.ndim > 2:
        # reduce augmented dimension
        mean = mean.mean(dim=1)
    assert mean.ndim == 2
    n, n_outputs = mean.shape

    if FISHER_MC in fisher_types:
        raise ValueError('MC Fisher pointless for regression GGN since same complexity but less accurate.')

    if FISHER_EXACT in fisher_types:
        for i in range(n_outputs):
            # for consistency with MSE loss, multiply by 2
            with _grads_scale(model, mean.new_ones((n, 1)) * sqrt(2)):  
                model.zero_grad(set_to_none=True)
                mean[:, i].sum().backward(**backward_kwargs)
            _register_fisher(model, FISHER_EXACT, accumulate=True)

    if COV in fisher_types:
        assert targets is not None, 'targets must be specified for computing covariance.'
        loss = F.mse_loss(mean, targets, reduction='sum')
        loss.backward(**backward_kwargs)
        _register_fisher(model, COV)

    return mean


def _fisher_for_heteroscedastic_regression(
    model,
    fisher_types,
    inputs,
    targets=None,
    backward_kwargs=None,
    **kwargs
):
    if backward_kwargs is None:
        backward_kwargs = dict(retain_graph=True)
    else:
        assert type(backward_kwargs) is dict
        backward_kwargs['retain_graph'] = True

    # NOTE: this assumes we either want to diff wrt params OR inputs not both
    if inputs.requires_grad:
        with disable_param_grad(model):
            eta = model(inputs)
    else:
        eta = model(inputs)

    if eta.ndim > 2:
        # reduce augmented dimension
        eta = eta.mean(dim=1)
    assert eta.ndim == 2 and eta.shape[1] == 2, 'Only single output heteroscedastic supported'

    if FISHER_MC in fisher_types:
        raise ValueError('MC Fisher pointless for regression GGN since same complexity but less accurate.')

    if FISHER_EXACT in fisher_types:
        # two implicit jvp's for backward
        eta1, eta2 = eta[:, 0], eta[:, 1]
        v1 = eta.new_zeros(eta.shape)
        v1[:, 0] = 1 / torch.sqrt(-2 * eta2)
        v1[:, 1] = - eta1 / (eta2 * torch.sqrt(-2 * eta2))
        v2 = eta.new_zeros(eta.shape)
        v2[:, 1] = 1 / (sqrt(2) * eta2)
        model.zero_grad(set_to_none=True)
        eta.backward(v1, **backward_kwargs)
        _register_fisher(model, FISHER_EXACT, accumulate=True)
        model.zero_grad(set_to_none=True)
        eta.backward(v2, **backward_kwargs)
        _register_fisher(model, FISHER_EXACT, accumulate=True)

    if COV in fisher_types:
        assert targets is not None, 'targets must be specified for computing covariance.'
        loss = heteroscedastic_mse_loss(eta, targets, reduction='sum')
        loss.backward(**backward_kwargs)
        _register_fisher(model, COV)

    return eta


def _module_batch_grads(model):
    rst = []
    for module in model.modules():
        operation = getattr(module, 'operation', None)
        if operation is None:
            continue
        batch_grads = operation.get_op_results()[OP_BATCH_GRADS]
        rst.append((module, batch_grads))
    return rst


def _module_batch_flatten_grads(model):
    rst = []
    for module, batch_grads in _module_batch_grads(model):
        batch_flatten_grads = torch.cat(
            [g.flatten(start_dim=1) for g in batch_grads.values()],
            dim=1
        )
        rst.append((module, batch_flatten_grads))
    return rst


def _module_batch_gvp(model, vec):
    rst = []
    pointer = 0
    for module, batch_grads in _module_batch_grads(model):
        batch_gvp = None
        for b_g in batch_grads.values():
            v = vec[pointer]
            b_gvp = b_g.mul(v.unsqueeze(0)).flatten(start_dim=1).sum(1)  # n
            if batch_gvp is None:
                batch_gvp = b_gvp
            else:
                batch_gvp += b_gvp
            pointer += 1
        rst.append((module, batch_gvp))
    assert pointer == len(vec)
    return rst


def _full_covariance(model):
    batch_all_g = []
    for _, batch_g in _module_batch_flatten_grads(model):
        batch_all_g.append(batch_g)
    batch_all_g = torch.cat(batch_all_g, dim=1)  # n x p_all
    cov_full = torch.matmul(batch_all_g.T, batch_all_g)  # p_all x p_all
    setattr(model, _COV_FULL, cov_full)


def _block_diag_covariance(model):
    for module, batch_g in _module_batch_flatten_grads(model):
        cov_block = torch.matmul(batch_g.T, batch_g)  # p_all x p_all
        setattr(module, _COV_BLOCK_DIAG, cov_block)


def _full_cvp(model, vec):
    """
    g: n x p
    v: p
    c = sum[gg^t]: p x p
    cvp = sum[gg^t]v = sum[g(g^t)v]: p
    """
    # compute batched (g^t)v
    batch_all_gvp = None
    for module, batch_gvp in _module_batch_gvp(model, vec):
        if batch_all_gvp is None:
            batch_all_gvp = batch_gvp
        else:
            batch_all_gvp += batch_gvp

    # compute cvp = sum[g(g^t)v]
    cvp = []
    for module, batch_grads in _module_batch_grads(model):
        for b_g in batch_grads.values():
            cvp.append(torch.einsum('n...,n->...', b_g, batch_all_gvp))

    setattr(model, _CVP_FULL, cvp)


def _block_diag_cvp(model, vec):
    """
    g: n x p
    v: p
    c = sum[gg^t]: p x p
    cvp = sum[gg^t]v = sum[g(g^t)v]: p
    """
    batch_gvp_dict = {k: v for k, v in _module_batch_gvp(model, vec)}
    for module, batch_grads in _module_batch_grads(model):
        cvp = []
        # compute cvp = sum[g(g^t)v]
        batch_gvp = batch_gvp_dict[module]
        for b_g in batch_grads.values():
            cvp.append(torch.einsum('n...,n->...', b_g, batch_gvp))

        setattr(module, _CVP_BLOCK_DIAG, cvp)


def _fisher_mc(loss_and_backward, model, probs, n_mc_samples=1):
    dist = torch.distributions.Categorical(probs)
    _targets = dist.sample((n_mc_samples, ))
    for i in range(n_mc_samples):
        loss_and_backward(_targets[i])
        _register_fisher(
            model,
            FISHER_MC,
            scale=1 / n_mc_samples,
            accumulate=True
        )


def _fisher_exact(loss_and_backward, model, probs):
    _, n_classes = probs.shape
    probs, _targets = torch.sort(probs, dim=1, descending=True)
    sqrt_probs = torch.sqrt(probs)
    for i in range(n_classes):
        with _grads_scale(model, sqrt_probs[:, i]):
            loss_and_backward(_targets[:, i])
        _register_fisher(
            model, FISHER_EXACT, accumulate=True
        )


def _covariance(loss_and_backward, model, targets, compute_param_grad=False):
    if compute_param_grad:
        loss_and_backward(targets)
    else:
        with disable_param_grad(model):
            loss_and_backward(targets)
    _register_fisher(model, COV)

    
def _fisher_exact_single(loss_and_backward, model, probs):
    sqrt_hess = torch.sqrt(probs - probs.square())
    with _grads_scale(model, sqrt_hess):
        loss_and_backward()
    _register_fisher(
        model, FISHER_EXACT, accumulate=True
    )
    

@contextmanager
def _grads_scale(model, scale):
    for module in model.modules():
        operation = getattr(module, 'operation', None)
        if operation is None:
            continue
        operation.grads_scale = scale

    yield

    for module in model.modules():
        operation = getattr(module, 'operation', None)
        if operation is None:
            continue
        operation.grads_scale = None


def _register_fisher(model, fisher_type, scale=1., accumulate=False):
    """
    module.fisher_{fisher_type} = op_results
    op_results = {
        'diag': {'weight': torch.Tensor, 'bias': torch.Tensor},
        'kron': {'A': torch.Tensor, 'B': torch.Tensor},
        'block_diag': torch.Tensor,
        'unit_wise': torch.Tensor,
    }
    """
    device = next(model.parameters()).device
    for module in model.modules():
        operation = getattr(module, 'operation', None)
        if operation is None:
            continue
        op_results = operation.get_op_results()
        kron = diag = unit = None
        if OP_COV_KRON in op_results:
            rst = op_results[OP_COV_KRON]
            kron = Kron(rst['A'], rst['B'], device=device)
        if OP_COV_DIAG in op_results:
            rst = op_results[OP_COV_DIAG]
            diag = Diag(
                rst.get('weight', None), rst.get('bias', None), device=device
            )
        if OP_COV_UNIT_WISE in op_results:
            rst = op_results[OP_COV_UNIT_WISE]
            unit = UnitWise(rst, device=device)
        operation.clear_op_results()
        # move block_diag/kron/diag fisher
        _accumulate_fisher(
            module,
            _COV_BLOCK_DIAG,
            fisher_type,
            kron=kron,
            diag=diag,
            unit=unit,
            scale=scale,
            accumulate=accumulate
        )
        # move block_diag fvp
        _accumulate_fvp(
            module, _CVP_BLOCK_DIAG, fisher_type, scale, accumulate
        )

    # move full fisher
    _accumulate_fisher(
        model, _COV_FULL, fisher_type, scale=scale, accumulate=accumulate
    )
    # move full fvp
    _accumulate_fvp(model, _CVP_FULL, fisher_type, scale, accumulate)


def _accumulate_fisher(
    module,
    data_src_attr,
    dst_attr,
    kron=None,
    diag=None,
    unit=None,
    scale=1.,
    accumulate=False
):
    data = getattr(module, data_src_attr, None)
    if all(v is None for v in [data, kron, diag, unit]):
        return
    device = next(module.parameters()).device
    fisher = SymMatrix(data, kron, diag, unit, device=device)
    fisher.scaling(scale)
    dst_fisher = getattr(module, dst_attr, None)
    if (dst_fisher is None) or (not accumulate):
        setattr(module, dst_attr, fisher)
    else:
        # accumulate fisher
        dst_fisher += fisher
        if dst_fisher.has_kron:
            # not accumulate kron.A
            dst_fisher.kron.A = fisher.kron.A
        setattr(module, dst_attr, dst_fisher)

    if data is not None:
        delattr(module, data_src_attr)


def _accumulate_fvp(module, src_attr, fisher_type, scale=1., accumulate=False):
    dst_attr = _get_fvp_attr(fisher_type)
    cvp = getattr(module, src_attr, None)
    if cvp is None:
        return
    cvp = [v * scale for v in cvp]
    dst_fvp = getattr(module, dst_attr, None)
    if (dst_fvp is None) or (not accumulate):
        setattr(module, dst_attr, cvp)
    else:
        dst_fvp = [u.add(v) for u, v in zip(dst_fvp, cvp)]
        setattr(module, dst_attr, dst_fvp)

    delattr(module, src_attr)


def _get_fvp_attr(fisher_type):
    return f'{fisher_type}_vp'

File Path: dependencies/asdl/asdfghjkl/fr.py
Content:
from typing import List
from contextlib import contextmanager

import torch
import torch.nn.functional as F
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import Subset
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

from .precondition import KFAC, DiagNaturalGradient
from .fisher import FISHER_EXACT, FISHER_MC
from .kernel import batch, empirical_implicit_ntk, empirical_class_wise_direct_ntk, get_preconditioned_kernel_fn
from .utils import add_value_to_diagonal, nvtx_range


__all__ = [
    'FROMP',
]


_precond_classes = {'kron': KFAC, 'diag': DiagNaturalGradient}
_fisher_types = {'exact': FISHER_EXACT, 'mc': FISHER_MC}
_kernel_fns = {'implicit': empirical_implicit_ntk, 'class_wise': empirical_class_wise_direct_ntk}


class PastTask:
    def __init__(self, memorable_points, class_ids=None):
        self.memorable_points = memorable_points
        self.kernel_inv = None
        self.mean = None
        self.class_ids = class_ids

    def update_kernel(self, model, kernel_fn, eps=1e-5):
        memorable_points = self.memorable_points
        if isinstance(memorable_points, DataLoader):
            kernel = batch(kernel_fn, model, memorable_points)
        else:
            kernel = kernel_fn(model, memorable_points)
        n, c = kernel.shape[0], kernel.shape[-1]  # (n, n, c, c) or (n, n, c)
        ndim = kernel.ndim
        if ndim == 4:
            kernel = kernel.transpose(1, 2).reshape(n * c, n * c)  # (nc, nc)
        elif ndim == 3:
            kernel = kernel.transpose(0, 2)  # (c, n, n)
        else:
            raise ValueError(f'Invalid kernel ndim: {ndim}. ndim must be 3 or 4.')

        kernel = add_value_to_diagonal(kernel, eps)
        self.kernel_inv = torch.inverse(kernel).detach_()

    @torch.no_grad()
    def update_mean(self, model):
        self.mean = self._evaluate_mean(model)

    def _evaluate_mean(self, model):
        means = []
        memorable_points = self.memorable_points
        if isinstance(memorable_points, DataLoader):
            device = next(model.parameters()).device
            for inputs, _ in self.memorable_points:
                inputs = inputs.to(device)
                means.append(model(inputs))
            return torch.cat(means)  # (n, c)
        else:
            return model(memorable_points)

    def get_penalty(self, model):
        assert self.kernel_inv is not None and self.mean is not None
        kernel_inv = self.kernel_inv  # (nc, nc) or (c, n, n)
        current_mean = self._evaluate_mean(model)  # (n, c)
        b = current_mean - self.mean  # (n, c)
        if kernel_inv.ndim == 2:
            # kernel_inv: (nc, nc)
            b = b.flatten()  # (nc,)
            v = torch.mv(kernel_inv, b)  # (nc,)
        else:
            # kernel_inv: (c, n, n)
            b = b.transpose(0, 1).unsqueeze(2)  # (c, n, 1)
            v = torch.matmul(kernel_inv, b)  # (c, n, 1)
            v = v.transpose(0, 1).flatten()  # (nc,)
            b = b.flatten()  # (nc,)

        return torch.dot(b, v)


class FROMP:
    """
    Implementation of a functional-regularisation method called
    Functional Regularisation of Memorable Past (FROMP):
    Pingbo Pan et al., 2020
    Continual Deep Learning by Functional Regularisation of Memorable Past
    https://arxiv.org/abs/2004.14070

    Example::

        >>> import torch
        >>> from asdfghjkl import FROMP
        >>>
        >>> model = torch.nn.Linear(5, 3)
        >>> optimizer = torch.optim.Adam(model.parameters())
        >>> loss_fn = torch.nn.CrossEntropyLoss()
        >>> fr = FROMP(model, tau=1.)
        >>>
        >>> for data_loader in data_loader_list:
        >>>     for x, y in data_loader:
        >>>         optimizer.zero_grad()
        >>>         loss = loss_fn(model(x), y)
        >>>         if fr.is_ready:
        >>>             loss += fr.get_penalty()
        >>>         loss.backward()
        >>>         optimizer.step()
        >>>     fr.update_regularization_info(data_loader)
    """
    def __init__(self,
                 model: torch.nn.Module,
                 tau=1.,
                 eps=1e-5,
                 max_tasks_for_penalty=None,
                 n_memorable_points=10,
                 ggn_shape='diag',
                 ggn_type='exact',
                 prior_prec=1e-5,
                 n_mc_samples=1,
                 kernel_type='implicit',
                 ):
        assert ggn_type in _fisher_types, f'ggn_type: {ggn_type} is not supported.' \
                                          f' choices: {list(_fisher_types.keys())}'
        assert ggn_shape in _precond_classes, f'ggn_shape: {ggn_shape} is not supported.' \
                                              f' choices: {list(_precond_classes.keys())}'
        assert kernel_type in _kernel_fns, f'kernel_type: {kernel_type} is not supported.' \
                                           f' choices: {list(_kernel_fns.keys())}'

        self.model = model
        self.tau = tau
        self.eps = eps
        self.max_tasks_for_penalty = max_tasks_for_penalty
        self.n_memorable_points = n_memorable_points

        if isinstance(model, DDP):
            # As DDP disables hook functions required for Fisher calculation,
            # the underlying module will be used instead.
            model_precond = model.module
        else:
            model_precond = model
        self.precond = _precond_classes[ggn_shape](model_precond,
                                                   fisher_type=_fisher_types[ggn_type],
                                                   pre_inv_postfix='all_tasks_ggn',
                                                   n_mc_samples=n_mc_samples,
                                                   damping=prior_prec)
        self.kernel_fn = get_preconditioned_kernel_fn(_kernel_fns[kernel_type], self.precond)
        self.observed_tasks: List[PastTask] = []

    @property
    def is_ready(self):
        return len(self.observed_tasks) > 0

    def update_regularization_info(self,
                                   data_loader: DataLoader,
                                   class_ids: List[int] = None,
                                   memorable_points_as_tensor=True,
                                   is_distributed=False):
        model = self.model
        if isinstance(model, DDP):
            # As DDP disables hook functions required for Kernel calculation,
            # the underlying module will be used instead.
            model = model.module
        model.eval()

        # update GGN and inverse for the current task
        with customize_head(model, class_ids):
            self.precond.update_curvature(data_loader=data_loader)
        if is_distributed:
            self.precond.reduce_curvature()
        self.precond.accumulate_curvature(to_pre_inv=True)
        self.precond.update_inv()

        # register the current task with the memorable points
        with customize_head(model, class_ids):
            memorable_points = collect_memorable_points(model,
                                                        data_loader,
                                                        self.n_memorable_points,
                                                        memorable_points_as_tensor,
                                                        is_distributed)
        self.observed_tasks.append(PastTask(memorable_points, class_ids))

        # update information (kernel & mean) for each observed task
        for task in self.observed_tasks:
            with customize_head(model, task.class_ids, softmax=True):
                task.update_kernel(model, self.kernel_fn)
                task.update_mean(model)

    def get_penalty(self, tau=None, eps=None, max_tasks=None, cholesky=False):
        assert self.is_ready, 'Functional regularization is not ready yet, ' \
                              'call FROMP.update_regularization_info(data_loader).'
        if tau is None:
            tau = self.tau
        if eps is None:
            eps = self.eps
        if max_tasks is None:
            max_tasks = self.max_tasks_for_penalty
        model = self.model
        model.eval()
        observed_tasks = self.observed_tasks

        # collect indices of tasks to calculate regularization penalty
        n_observed_tasks = len(observed_tasks)
        indices = list(range(n_observed_tasks))
        if max_tasks and max_tasks < n_observed_tasks:
            import random
            indices = random.sample(indices, max_tasks)

        # get regularization penalty on all the selected tasks
        with disable_broadcast_buffers(model):
            total_penalty = 0
            for idx in indices:
                task = observed_tasks[idx]
                with customize_head(model, task.class_ids, softmax=True):
                    total_penalty += task.get_penalty(model, eps=eps, cholesky=cholesky)

        return 0.5 * tau * total_penalty


@torch.no_grad()
def collect_memorable_points(model,
                             data_loader: DataLoader,
                             n_memorable_points,
                             as_tensor=True,
                             is_distributed=False):
    device = next(model.parameters()).device
    dataset = data_loader.dataset

    # create a data loader w/o shuffling so that indices in the dataset are stored
    assert data_loader.batch_size is not None, 'DataLoader w/o batch_size is not supported.'
    if is_distributed:
        indices = range(dist.get_rank(), len(dataset), dist.get_world_size())
        dataset = Subset(dataset, indices)
    no_shuffle_loader = DataLoader(dataset,
                                   batch_size=data_loader.batch_size,
                                   num_workers=data_loader.num_workers,
                                   pin_memory=True,
                                   drop_last=False,
                                   shuffle=False)
    # collect Hessian trace
    hessian_traces = []
    for inputs, _ in no_shuffle_loader:
        inputs = inputs.to(device)
        logits = model(inputs)
        probs = F.softmax(logits, dim=1)  # (n, c)
        diag_hessian = probs - probs * probs  # (n, c)
        hessian_traces.append(diag_hessian.sum(dim=1))  # [(n,)]
    hessian_traces = torch.cat(hessian_traces)

    # sort indices by Hessian trace
    indices = torch.argsort(hessian_traces, descending=True).cpu()
    top_indices = indices[:n_memorable_points]

    if as_tensor:
        # crate a Tensor for memorable points on model's device
        memorable_points = [dataset[idx][0] for idx in top_indices]
        return torch.stack(memorable_points).to(device)
    else:
        # create a DataLoader for memorable points
        memorable_points = Subset(dataset, top_indices)
        batch_size = min(n_memorable_points, data_loader.batch_size)
        return DataLoader(memorable_points,
                          batch_size=batch_size,
                          pin_memory=True,
                          drop_last=False,
                          shuffle=False)


@contextmanager
def customize_head(module: torch.nn.Module, class_ids: List[int] = None, softmax=False):

    def forward_hook(module, input, output):
        if class_ids is not None:
            output = output[:, class_ids]
        if softmax:
            return F.softmax(output, dim=1)
        else:
            return output

    handle = module.register_forward_hook(forward_hook)
    yield
    handle.remove()
    del forward_hook


@contextmanager
def disable_broadcast_buffers(module):
    tmp = False
    if isinstance(module, DDP):
        tmp = module.broadcast_buffers
        module.broadcast_buffers = False
    yield
    if isinstance(module, DDP):
        module.broadcast_buffers = tmp

File Path: dependencies/asdl/asdfghjkl/gradient.py
Content:
import torch.distributed as dist
from torch.nn.utils import parameters_to_vector, vector_to_parameters
from .core import extend
from .operations import (
    OP_ACCUMULATE_GRADS, OP_BATCH_GRADS, OP_BATCH_GRADS_KRON
)

__all__ = ['data_loader_gradient', 'batch_gradient']


def data_loader_gradient(
    model,
    data_loader,
    loss_fn=None,
    has_accumulated=False,
    is_distributed=False,
    all_reduce=False,
    is_master=True
):
    if not has_accumulated:
        # accumulate gradient for an epoch
        assert loss_fn is not None, 'loss_fn must be specified when has_accumulated is False.'
        device = next(model.parameters()).device
        for data, target in data_loader:
            with extend(model, OP_ACCUMULATE_GRADS):
                data, target = data.to(device), target.to(device)
                model.zero_grad()
                loss = loss_fn(model(data), target)
                loss.backward()

    # take average of accumulated gradient
    n_batches = len(data_loader)
    for param in model.parameters():
        if param.grad is None:
            continue
        param.grad = param.acc_grad.div(n_batches)

    # reduce gradient
    if is_distributed:
        grads = [p.grad for p in model.parameters() if p.requires_grad]
        # pack
        packed_tensor = parameters_to_vector(grads)
        # reduce
        if all_reduce:
            dist.all_reduce(packed_tensor)
        else:
            dist.reduce(packed_tensor, dst=0)
        # unpack
        if is_master or all_reduce:
            vector_to_parameters(
                packed_tensor.div_(dist.get_world_size()), grads
            )

        dist.barrier()


def batch_gradient(model, loss_fn, inputs, targets, kron_jac=False, **backward_kwargs):
    with extend(model, OP_BATCH_GRADS_KRON if kron_jac else OP_BATCH_GRADS):
        model.zero_grad()
        f = model(inputs)
        loss = loss_fn(f, targets)
        loss.backward(**backward_kwargs)
    return f


def batch_aug_gradient(model, loss_fn, inputs, targets, kron_jac=False, **backward_kwargs):
    with extend(model, OP_BATCH_GRADS_KRON if kron_jac else OP_BATCH_GRADS):
        model.zero_grad()
        f = model(inputs)
        loss = loss_fn(f, targets)
        loss.backward(**backward_kwargs)
    return f

File Path: dependencies/asdl/asdfghjkl/hessian.py
Content:
import torch
from .symmatrix import SymMatrix, Diag
from .matrices import SHAPE_FULL, SHAPE_BLOCK_DIAG, SHAPE_DIAG, HESSIAN, MatrixManager
from .mvp import power_method, conjugate_gradient_method

__all__ = [
    'hessian_eigenvalues',
    'hessian',
    'hessian_for_loss',
    'hessian_free'
]
_supported_shapes = [SHAPE_FULL, SHAPE_BLOCK_DIAG, SHAPE_DIAG]


def hessian_eigenvalues(
    model,
    loss_fn,
    data_loader=None,
    inputs=None,
    targets=None,
    top_n=1,
    max_iters=100,
    tol=1e-3,
    is_distributed=False,
    print_progress=False
):
    def hvp_fn(vec, x, y):
        model.zero_grad()
        loss = loss_fn(model(x), y)
        params = [p for p in model.parameters() if p.requires_grad]
        grads = torch.autograd.grad(loss, inputs=params, create_graph=True)
        return hvp(vec, grads, params)

    eigvals, eigvecs = power_method(hvp_fn,
                                    model,
                                    data_loader=data_loader,
                                    inputs=inputs,
                                    targets=targets,
                                    top_n=top_n,
                                    max_iters=max_iters,
                                    tol=tol,
                                    is_distributed=is_distributed,
                                    print_progress=print_progress)

    return eigvals, eigvecs


def hessian_free(
        model,
        loss_fn,
        b,
        data_loader=None,
        inputs=None,
        targets=None,
        init_x=None,
        damping=1e-3,
        max_iters=None,
        tol=1e-8,
        is_distributed=False,
        print_progress=False,
):
    def hvp_fn(vec, x, y):
        model.zero_grad()
        loss = loss_fn(model(x), y)
        params = [p for p in model.parameters() if p.requires_grad]
        grads = torch.autograd.grad(loss, inputs=params, create_graph=True)
        return hvp(vec, grads, params)

    return conjugate_gradient_method(hvp_fn,
                                     b,
                                     data_loader=data_loader,
                                     inputs=inputs,
                                     targets=targets,
                                     init_x=init_x,
                                     damping=damping,
                                     max_iters=max_iters,
                                     tol=tol,
                                     is_distributed=is_distributed,
                                     print_progress=print_progress)


def hvp(vec, grads, params):
    Hv = torch.autograd.grad(grads, inputs=params, grad_outputs=vec)
    return Hv


def hessian_for_loss(
    model,
    loss_fn,
    hessian_shapes,
    inputs=None,
    targets=None,
    data_loader=None,
    stats_name=None,
    is_distributed=False,
    all_reduce=False,
    is_master=True,
    matrix_manager=None,
):
    if isinstance(hessian_shapes, str):
        hessian_shapes = [hessian_shapes]
    # remove duplicates
    hessian_shapes = set(hessian_shapes)
    for hshape in hessian_shapes:
        assert hshape in _supported_shapes, f'Invalid hessian_shape: {hshape}. hessian_shape must be in {_supported_shapes}.'

    # setup matrix manager as needed
    if matrix_manager is None:
        matrix_manager = MatrixManager(model, HESSIAN)

    if data_loader is not None:
        device = next(model.parameters()).device
        for inputs, targets in data_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            _hessian_for_loss(model, loss_fn, hessian_shapes, inputs, targets)
        matrix_manager.accumulate_matrices(
            stats_name, scale=1 / len(data_loader)
        )
    else:
        assert inputs is not None and targets is not None
        _hessian_for_loss(model, loss_fn, hessian_shapes, inputs, targets)

    # reduce matrices
    if is_distributed:
        matrix_manager.reduce_matrices(stats_name, is_master, all_reduce)

    return matrix_manager


def _hessian_for_loss(model, loss_fn, hessian_shapes, inputs, targets):
    model.zero_grad()
    loss = loss_fn(model(inputs), targets)
    device = next(model.parameters()).device
    params = [p for p in model.parameters() if p.requires_grad]

    # full
    if SHAPE_FULL in hessian_shapes:
        full_hess = hessian(loss, params)
        setattr(model, 'hessian', SymMatrix(data=full_hess, device=device))
    else:
        full_hess = None

    if SHAPE_BLOCK_DIAG not in hessian_shapes \
            and SHAPE_DIAG not in hessian_shapes:
        return

    idx = 0
    for module in model.modules():
        w = getattr(module, 'weight', None)
        b = getattr(module, 'bias', None)
        params = [p for p in [w, b] if p is not None and p.requires_grad]
        if len(params) == 0:
            continue

        # module hessian
        if full_hess is None:
            m_hess = hessian(loss, params)
        else:
            m_numel = sum([p.numel() for p in params])
            m_hess = full_hess[idx:idx + m_numel, idx:idx + m_numel]
            idx += m_numel

        # block-diagonal
        if SHAPE_BLOCK_DIAG in hessian_shapes:
            setattr(module, 'hessian', SymMatrix(data=m_hess, device=device))

        # diagonal
        if SHAPE_DIAG in hessian_shapes:
            m_hess = torch.diag(m_hess)
            _idx = 0
            w_hess = b_hess = None
            if w is not None and w.requires_grad:
                w_numel = w.numel()
                w_hess = m_hess[_idx:_idx + w_numel].view_as(w)
                _idx += w_numel
            if b is not None and b.requires_grad:
                b_numel = b.numel()
                b_hess = m_hess[_idx:_idx + b_numel].view_as(b)
                _idx += b_numel
            diag = Diag(weight=w_hess, bias=b_hess, device=device)
            if hasattr(module, 'hessian'):
                module.hessian.diag = diag
            else:
                setattr(module, 'hessian', SymMatrix(diag=diag, device=device))


# adopted from https://github.com/mariogeiger/hessian/blob/master/hessian/hessian.py
def hessian(output, inputs, out=None, allow_unused=False, create_graph=False):
    '''
    Compute the Hessian of `output` with respect to `inputs`
    hessian((x * y).sum(), [x, y])
    '''
    assert output.ndimension() == 0

    if torch.is_tensor(inputs):
        inputs = [inputs]
    else:
        inputs = list(inputs)

    n = sum(p.numel() for p in inputs)
    if out is None:
        out = output.new_zeros(n, n)

    ai = 0
    for i, inp in enumerate(inputs):
        [grad] = torch.autograd.grad(
            output, inp, create_graph=True, allow_unused=allow_unused
        )
        grad = torch.zeros_like(inp) if grad is None else grad
        grad = grad.contiguous().view(-1)

        for j in range(inp.numel()):
            if grad[j].requires_grad:
                row = _gradient(
                    grad[j],
                    inputs[i:],
                    retain_graph=True,
                    create_graph=create_graph
                )[j:]
            else:
                row = grad[j].new_zeros(sum(x.numel() for x in inputs[i:]) - j)

            out[ai, ai:].add_(row.type_as(out))  # ai's row
            if ai + 1 < n:
                out[ai + 1:, ai].add_(row[1:].type_as(out))  # ai's column
            del row
            ai += 1
        del grad

    return out


# adopted from https://github.com/mariogeiger/hessian/blob/master/hessian/gradient.py
def _gradient(
    outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False
):
    '''
    Compute the gradient of `outputs` with respect to `inputs`
    gradient(x.sum(), x)
    gradient((x * y).sum(), [x, y])
    '''
    if torch.is_tensor(inputs):
        inputs = [inputs]
    else:
        inputs = list(inputs)
    grads = torch.autograd.grad(
        outputs,
        inputs,
        grad_outputs,
        allow_unused=True,
        retain_graph=retain_graph,
        create_graph=create_graph
    )
    grads = [
        x if x is not None else torch.zeros_like(y) for x,
        y in zip(grads, inputs)
    ]
    return torch.cat([x.contiguous().view(-1) for x in grads])

File Path: dependencies/asdl/asdfghjkl/kernel.py
Content:
import numpy as np
from typing import List
from functools import partial

import torch
from torch import Tensor
from torch.nn import functional as F
from torch.utils.data import DataLoader, Subset, TensorDataset
import torch.distributed as dist

from .core import extend
from .operations import *
from .precondition import Precondition
from .utils import flatten_after_batch, disable_param_grad


__all__ = [
    'batch',
    'empirical_direct_ntk',
    'empirical_implicit_ntk',
    'empirical_class_wise_direct_ntk',
    'empirical_class_wise_hadamard_ntk',
    'get_preconditioned_kernel_fn',
    'logits_hessian_cross_entropy',
    'natural_gradient_cross_entropy',
    'efficient_natural_gradient_cross_entropy',
    'parallel_efficient_natural_gradient_cross_entropy',
    'kernel_vector_product',
    'kernel_free_cross_entropy',
    'kernel_eigenvalues'
]


_MASTER = 'master'
_ALL = 'all'
_SPLIT = 'split'


def batch(kernel_fn, model, x1, x2=None, batch_size=1, store_on_device=True, is_distributed=False, gather_type=_MASTER):
    """
    :param kernel_fn:
    :param model:
    :param x1:
    :param x2:
    :param batch_size:
    :param store_on_device:
    :param is_distributed:
    :param gather_type:
    :return: Tensor of shape (n, n, c) or (n, n, c, c)
    """

    def _get_loader(x):
        if isinstance(x, DataLoader):
            return x
        elif isinstance(x, Tensor):
            assert x.shape[0] % batch_size == 0, \
                f'data size ({x.shape[0]}) has to be divisible by batch size ({batch_size}).'
            return DataLoader(TensorDataset(x), batch_size)
        else:
            raise ValueError(f'x1 and x2 have to be {DataLoader} or {Tensor}. {type(x)} was given.')

    loader1 = _get_loader(x1)
    if x2 is None:
        loader2 = None
    else:
        loader2 = _get_loader(x2)

    if is_distributed:
        return _parallel(kernel_fn, model, loader1, loader2, store_on_device, gather_type)
    else:
        return _serial(kernel_fn, model, loader1, loader2, store_on_device)


def _get_inputs(data):
    if isinstance(data, (tuple, list)):
        inputs = data[0]
    else:
        inputs = data
    assert isinstance(inputs, torch.Tensor)
    return inputs


def _serial(kernel_fn, model, loader1, loader2=None, store_on_device=True):
    device = next(iter(model.parameters())).device
    tmp_device = device if store_on_device else 'cpu'
    if loader2 is not None:
        rows = []
        for batch1 in loader1:
            batch1 = _get_inputs(batch1).to(device)
            row_kernels = []
            for batch2 in loader2:
                batch2 = _get_inputs(batch2).to(device)
                block = kernel_fn(model, batch1, batch2)
                row_kernels.append(block.to(tmp_device))
            rows.append(torch.cat(row_kernels, dim=1))
    else:
        n_batches1 = len(loader1)
        blocks = [[torch.empty(0) for _ in range(n_batches1)] for _ in range(n_batches1)]
        for i, batch1 in enumerate(loader1):
            batch1 = _get_inputs(batch1).to(device)
            for j, batch2 in enumerate(loader1):
                batch2 = _get_inputs(batch2).to(device)
                if i == j:
                    block = kernel_fn(model, batch1)
                elif i > j:
                    block = blocks[j][i].clone().transpose(0, 1)
                    if block.ndim == 4:
                        # n x n x c x c
                        block = block.transpose(2, 3)
                else:
                    block = kernel_fn(model, batch1, batch2)
                blocks[i][j] = block.to(device)
        rows = [torch.cat(blocks[i], dim=1) for i in range(n_batches1)]

    return torch.cat(rows, dim=0).to(device)


def _get_subset_loader(loader: DataLoader, batch_indices: List):
    batch_size = loader.batch_size
    n_samples = len(loader.dataset)
    subset_sample_indices = []
    for batch_idx in batch_indices:
        start_sample_idx = batch_idx * batch_size
        end_sample_idx = min((batch_idx + 1) * batch_size, n_samples)
        sample_indices = range(start_sample_idx, end_sample_idx)
        subset_sample_indices.extend(sample_indices)
    subset = Subset(loader.dataset, subset_sample_indices)

    return DataLoader(subset,
                      batch_size,
                      pin_memory=loader.pin_memory,
                      num_workers=loader.num_workers)


def _parallel(kernel_fn, model, loader1, loader2=None, store_on_device=True, gather_type=_MASTER):
    device = next(iter(model.parameters())).device
    tmp_device = device if store_on_device else 'cpu'
    assert gather_type in [_MASTER, _ALL, _SPLIT]
    n_batches1 = len(loader1)
    is_symmetric = loader2 is None
    if is_symmetric:
        loader2 = loader1
        n_batches2 = n_batches1
        indices = np.triu_indices(n_batches1)
        indices = [(i, j) for i, j in zip(indices[0], indices[1])]
    else:
        n_batches2 = len(loader2)
        indices = [(i, j) for i in range(n_batches1) for j in range(n_batches2)]

    rank = dist.get_rank()
    is_master = rank == 0
    world_size = dist.get_world_size()
    assert len(indices) >= world_size, f'At least 1 block have to be assigned to each process. There are only {len(indices)} blocks for {world_size} processes.'
    indices_split = np.array_split(indices, world_size)

    local_indices = indices_split[rank]
    subset_loader1 = _get_subset_loader(loader1, [idx[0] for idx in local_indices])
    subset_loader2 = _get_subset_loader(loader2, [idx[1] for idx in local_indices])
    local_blocks = []
    for (i, j), batch1, batch2 in zip(local_indices, subset_loader1, subset_loader2):
        batch1 = _get_inputs(batch1).to(device)
        if i == j and is_symmetric:
            batch2 = None
        else:
            batch2 = _get_inputs(batch2).to(device)
        # bs x bs x c x *
        block = kernel_fn(model, batch1, batch2)
        local_blocks.append(block.to(tmp_device))
    local_blocks = torch.stack(local_blocks).to(device)  # local_n_blocks x bs x bs x c x *

    # match the size of local blocks to the maximum size
    max_n_blocks = len(indices_split[0])
    for _ in range(max_n_blocks - len(local_indices)):
        dummy = torch.zeros_like(local_blocks[0]).unsqueeze(0)
        local_blocks = torch.cat([local_blocks, dummy])

    def _construct_block_matrix(block_list):
        _blocks = [[torch.empty(0) for _ in range(n_batches2)] for _ in range(n_batches1)]
        for _local_blocks, _local_indices in zip(block_list, indices_split):
            for _block, (i, j) in zip(_local_blocks, _local_indices):
                _blocks[i][j] = _block
        if is_symmetric:
            for j in range(n_batches2):
                for i in range(j+1, n_batches1):
                    _block = _blocks[j][i].clone().transpose(0, 1)
                    if _block.ndim == 4:
                        # bs x bs x c x c
                        _block = _block.transpose(2, 3)
                    _blocks[i][j] = _block
        _rows = [torch.cat(_blocks[i], dim=1) for i in range(n_batches1)]
        return torch.cat(_rows, dim=0)  # n x n x c x *

    if gather_type == _MASTER:
        if is_master:
            gather_list = [torch.zeros_like(local_blocks) for _ in range(world_size)]
            dist.gather(local_blocks, gather_list, dst=0)
            return _construct_block_matrix(gather_list)
        else:
            dist.gather(local_blocks, dst=0)
            return None

    elif gather_type == _ALL:
        gather_list = [torch.zeros_like(local_blocks) for _ in range(world_size)]
        dist.all_gather(gather_list, local_blocks)
        return _construct_block_matrix(gather_list)

    assert gather_type == _SPLIT
    assert local_blocks.ndim == 4  # local_n_blocks x bs x bs x c
    n_classes = local_blocks.shape[-1]
    classes_split = np.array_split(range(n_classes), world_size)

    # all-to-all
    gather_list = None
    for dst, local_classes in enumerate(classes_split):
        tensor = local_blocks[:, :, :, local_classes].clone()  # local_n_blocks x bs x bs x local_c
        if rank == dst:
            gather_list = [torch.zeros_like(tensor) for _ in range(world_size)]
            dist.gather(tensor, gather_list, dst=dst)
        else:
            dist.gather(tensor, dst=dst)

    local_c = len(classes_split[rank])
    if local_c > 0:
        local_class_kernels = []
        for k in range(local_c):
            class_block_list = [blocks[:, :, :, k] for blocks in gather_list]
            class_kernel = _construct_block_matrix(class_block_list)
            local_class_kernels.append(class_kernel)

        return torch.stack(local_class_kernels)  # local_c x n x n
    else:
        return None


def linear_network_kernel(model, x, scale, likelihood='classification', 
                          differentiable=False, kron_jac=False):
    operation_name = OP_BATCH_GRADS_KRON if kron_jac else OP_BATCH_GRADS
    n = x.shape
    n_params = sum(p.numel() for p in model.parameters())

    with extend(model, operation_name):
        if x.requires_grad:
            with disable_param_grad(model):
                logits = model(x)
        else:
            logits = model(x)
        if logits.ndim > 2:  # augmented
            logits = logits.mean(dim=1)
        n, c = logits.shape
        j1 = logits.new_zeros(n, c, n_params)
        for k in range(c):
            model.zero_grad()
            scalar = logits[:, k].sum()
            if differentiable:
                scalar.backward(retain_graph=True, create_graph=True)
            else:
                scalar.backward(retain_graph=(k < c - 1))
            j_k = []
            for module in model.modules():
                operation = getattr(module, 'operation', None)
                if operation is None:
                    continue
                batch_grads = operation.get_op_results()[operation_name]
                for g in batch_grads.values():
                    j_k.append(flatten_after_batch(g))
            j_k = torch.cat(j_k, dim=1)  # n x p
            j1[:, k, :] = j_k

    if likelihood == 'classification':
        L = logits_hessian_cross_entropy(logits)  # n x c x c
        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale  # n x p x c @ n x c x c or for c = 1
    elif likelihood == 'heteroscedastic_regression':
        L = hessian_heteroscedastic_regression(logits)  # n x 2 x 2
        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale  # n x p x c @ n x c x c or for c = 1
    elif likelihood == 'regression':
        j2 = j1 * scale
    else:
        raise ValueError('Invalid likelihood')
    return logits, torch.einsum('ncp,mdp->nmcd', j1, j2)  # n1 x n1 x c x c


def linear_network_kernel_indep(model, x, scale, likelihood='classification', differentiable=False, 
                                kron_jac=False, single_output=None):
    n = x.shape[0]

    module_list = [[module] * (2 if getattr(module, 'bias', None) is not None else 1)
                   for module in model.modules() if hasattr(module, 'weight')]
    module_list = [(m, loc) for sublist in module_list
                   for m, loc in zip(sublist, ['weight', 'bias'])]
    if len(scale) == 1:
        scale = [scale] * len(module_list)
    assert len(scale) == len(module_list), 'Scale should be either scalar or for each weight and bias.'
    for (module, loc), scalem in zip(module_list, scale):
        setattr(module, f'{loc}_scale', scalem)

    op_name = OP_GRAM_HADAMARD if kron_jac else OP_GRAM_DIRECT
    with extend(model, op_name):
        _zero_kernel(model, n, n)
        if x.requires_grad:
            with disable_param_grad(model):
                outputs = model(x)
        else:
            outputs = model(x)
        if outputs.ndim > 2:  # augmented
            outputs = outputs.mean(dim=1)
        n_classes = outputs.shape[-1]  # c
        if likelihood == 'classification':
            if single_output is None:
                L = logits_diag_hessian_cross_entropy(outputs)  # n x c
            else:
                L = logits_single_hessian_cross_entropy(outputs, single_output)  # n
        elif likelihood == 'heteroscedastic_regression':
            if single_output is None:
                L = hessian_diag_heteroscedastic_regression(outputs)  # n x 2
            else:
                L = hessian_single_heteroscedastic_regression(outputs, single_output)  # n
        else:
            assert likelihood == 'regression'
        kernels = []
        output_range = range(n_classes) if single_output is None else [n_classes]
        for k in output_range:
            model.zero_grad()
            if single_output is None:
                scalar = outputs[:, k].sum()
            elif single_output.ndim == 0:
                scalar = outputs[:, single_output].sum()
            elif single_output.ndim == 1:
                scalar = outputs.gather(1, single_output.unsqueeze(-1)).sum()
            else:
                raise ValueError('Invalid single_output')
            scalar.backward(
                retain_graph=differentiable or (k < n_classes - 1) or (single_output is not None),
                create_graph=differentiable
            )
            if likelihood == 'regression':
                kernels.append(model.kernel)
            else:
                kernels.append(model.kernel * (L if single_output is not None else L[:, k]))
            _zero_kernel(model, n, n)
        _clear_kernel(model)

    for (module, loc), scale in zip(module_list, scale):
        delattr(module, f'{loc}_scale')

    # returns n x c, c x n x n or n x n (for single_output)
    return outputs, kernels[0] if single_output is not None else torch.stack(kernels)


def empirical_network_kernel(model, x, y, lossfunc, scale, differentiable=False, kron_jac=False):
    n = x.shape[0]

    module_list = [[module] * (2 if getattr(module, 'bias', None) is not None else 1)
                   for module in model.modules() if hasattr(module, 'weight')]
    module_list = [(m, loc) for sublist in module_list
                   for m, loc in zip(sublist, ['weight', 'bias'])]
    if len(scale) == 1:
        scale = [scale] * len(module_list)
    assert len(scale) == len(module_list), 'Scale should be either scalar or for each weight and bias.'
    for (module, loc), scalem in zip(module_list, scale):
        setattr(module, f'{loc}_scale', scalem)

    op_name = OP_GRAM_HADAMARD if kron_jac else OP_GRAM_DIRECT
    with extend(model, op_name):
        _zero_kernel(model, n, n)
        if x.requires_grad:
            with disable_param_grad(model):
                outputs = model(x)
        else:
            outputs = model(x)
        if outputs.ndim > 2:  # augmented
            outputs = outputs.mean(dim=1)
        model.zero_grad()
        loss = lossfunc(outputs, y)
        loss.backward(retain_graph=differentiable, create_graph=differentiable)
        kernel = model.kernel
        _clear_kernel(model)

    for (module, loc), scale in zip(module_list, scale):
        delattr(module, f'{loc}_scale')

    return loss, kernel


def empirical_direct_ntk(model, x1, x2=None):
    n1 = x1.shape[0]
    is_single_batch = x2 is None
    if is_single_batch:
        inputs = x1
        n2 = None
    else:
        inputs = torch.cat([x1, x2], dim=0)
        n2 = x2.shape[0]
    n_params = sum(p.numel() for p in model.parameters())

    with extend(model, OP_BATCH_GRADS):
        outputs = model(inputs)
        n_data, n_classes = outputs.shape  # n x c
        j1 = outputs.new_zeros(n1, n_classes, n_params)
        if is_single_batch:
            j2 = None
        else:
            j2 = outputs.new_zeros(n2, n_classes, n_params)
        for k in range(n_classes):
            model.zero_grad()
            scalar = outputs[:, k].sum()
            # scalar.backward(retain_graph=(k < n_classes - 1))
            scalar.backward(retain_graph=True, create_graph=True)
            j_k = []
            for module in model.modules():
                operation = getattr(module, 'operation', None)
                if operation is None:
                    continue
                batch_grads = operation.get_op_results()[OP_BATCH_GRADS]
                for g in batch_grads.values():
                    j_k.append(flatten_after_batch(g))
            j_k = torch.cat(j_k, dim=1)  # n x p
            if is_single_batch:
                j1[:, k, :] = j_k
            else:
                j1[:, k, :] = j_k[:n1]
                j2[:, k, :] = j_k[n1:]

    if is_single_batch:
        return torch.einsum('ncp,mdp->nmcd', j1, j1)  # n1 x n1 x c x c
    else:
        return torch.einsum('ncp,mdp->nmcd', j1, j2)  # n1 x n2 x c x c


def empirical_implicit_ntk(model, x1, x2=None, precond: Precondition = None):
    n1 = x1.shape[0]
    y1 = model(x1)
    n_classes = y1.shape[-1]
    v1 = torch.ones_like(y1).requires_grad_()
    vjp1 = torch.autograd.grad(y1, model.parameters(), v1, create_graph=True)
    vjp1_clone = [v.clone() for v in vjp1]

    if precond is not None:
        # precondition
        precond.precondition_vector(vjp1_clone)

    if x2 is None:
        n2 = n1
        ntk_dot_v = torch.autograd.grad(vjp1, v1, vjp1_clone, create_graph=True)[0]
    else:
        n2 = x2.shape[0]
        y2 = model(x2)
        v2 = torch.ones_like(y2).requires_grad_()
        vjp2 = torch.autograd.grad(y2, model.parameters(), v2, create_graph=True)
        ntk_dot_v = torch.autograd.grad(vjp2, v2, vjp1_clone, create_graph=True)[0]

    print('x req grad', x1.requires_grad)

    ntk = y1.new_zeros(n1, n2, n_classes, n_classes)
    for j in range(n2):
        for k in range(n_classes):
            # retain_graph = j < n2 - 1 or k < n_classes - 1
            retain_graph=True
            kernel = torch.autograd.grad(ntk_dot_v[j][k], v1, retain_graph=retain_graph,
                    create_graph=True)[0]
            ntk[:, j, :, k] = kernel

    return ntk  # n1 x n2 x c x c


def get_preconditioned_kernel_fn(kernel_fn, precond: Precondition):
    return partial(kernel_fn, precond=precond)


def empirical_class_wise_direct_ntk(model, x1, x2=None, precond=None):
    return _empirical_class_wise_ntk(model, x1, x2, hadamard=False, precond=precond)


def empirical_class_wise_hadamard_ntk(model, x1, x2=None, precond=None):
    return _empirical_class_wise_ntk(model, x1, x2, hadamard=True, precond=precond)


def _empirical_class_wise_ntk(model, x1, x2=None, hadamard=False, precond=None):
    if x2 is not None:
        inputs = torch.cat([x1, x2], dim=0)
        n1 = x1.shape[0]
        n2 = x2.shape[0]
    else:
        inputs = x1
        n1 = n2 = x1.shape[0]

    for module in model.modules():
        setattr(module, 'gram_precond', precond)

    op_name = OP_GRAM_HADAMARD if hadamard else OP_GRAM_DIRECT
    with extend(model, op_name):
        _zero_kernel(model, n1, n2)
        outputs = model(inputs)
        n_classes = outputs.shape[-1]  # c
        kernels = []
        for k in range(n_classes):
            model.zero_grad()
            scalar = outputs[:, k].sum()
            scalar.backward(retain_graph=True, create_graph=True)
            kernels.append(model.kernel.clone())
            _zero_kernel(model, n1, n2)
        _clear_kernel(model)

    for module in model.modules():
        delattr(module, 'gram_precond')

    return torch.stack(kernels).permute(1, 2, 0)  # n1 x n2 x c


def logits_hessian_cross_entropy(logits):
    p = F.softmax(logits, dim=-1)
    return torch.diag_embed(p) - torch.bmm(p.unsqueeze(2), p.unsqueeze(1))  # n x c x c


def logits_single_hessian_cross_entropy(logits, single_output):
    if single_output.ndim == 0:
        p = F.softmax(logits, dim=-1)[:, single_output]
    elif single_output.ndim == 1:
        p = F.softmax(logits, dim=-1).gather(1, single_output.unsqueeze(-1)).squeeze(-1)
    else:
        raise ValueError('Invalid single_output')
    return p - torch.square(p)  # n


def logits_diag_hessian_cross_entropy(logits):
    p = F.softmax(logits, dim=-1)
    return p - torch.square(p)  # n x c


def logits_second_order_grad_cross_entropy(logits, targets, damping=1e-5):
    hessian = logits_hessian_cross_entropy(logits)  # n x c x c
    hessian = _add_value_to_diagonal(hessian, damping)

    loss = F.cross_entropy(logits, targets, reduction='sum')
    grads = torch.autograd.grad(loss, logits, retain_graph=True)[0]  # n x c

    return _cholesky_solve(hessian, grads)  # n x c


def hessian_heteroscedastic_regression(logits):
    L = logits.new_zeros((logits.shape[0], 2, 2))
    eta_1, eta_2 = logits[:, 0], logits[:, 1]
    L[:, 0, 0] = - 0.5 / eta_2
    L[:, 0, 1] = L[:, 1, 0] = 0.5 * eta_1 / eta_2.square()
    L[:, 1, 1] = 0.5 / eta_2.square() - 0.5 * eta_1.square() / torch.pow(eta_2, 3)
    return L


def hessian_diag_heteroscedastic_regression(logits):
    L = logits.new_zeros((logits.shape[0], 2))
    eta_1, eta_2 = logits[:, 0], logits[:, 1]
    L[:, 0] = - 0.5 / eta_2
    L[:, 1] = 0.5 / eta_2.square() - 0.5 * eta_1.square() / torch.pow(eta_2, 3)
    return L  # n x 2


def hessian_single_heteroscedastic_regression(logits, single_output):
    L = hessian_diag_heteroscedastic_regression(logits)
    if single_output.ndim == 0:
        return L[:, single_output]
    elif single_output.ndim == 1:
        return L.gather(1, single_output.unsqueeze(-1)).squeeze(-1)
    else:
        raise ValueError('Invalid single_output')


def natural_gradient_cross_entropy(model, inputs, targets, kernel, damping=1e-5):
    outputs = model(inputs)
    n, c = outputs.shape
    hessian = logits_hessian_cross_entropy(outputs)  # n x c x c

    is_class_wise = kernel.ndim == 3  # n x n x c
    mat = outputs.new_zeros(n * c, n * c)  # nc x nc
    for i in range(n):
        for j in range(n):
            if is_class_wise:
                # dense x diagonal
                diag_repeated = kernel[i, j].repeat(c, 1)  # c x c
                block = torch.mul(hessian[i], diag_repeated)
            else:
                # dense x dense
                block = torch.matmul(hessian[i], kernel[i, j])
            mat[i * c: (i+1) * c, j * c: (j+1) * c] = block
    mat.div_(n)
    mat = _add_value_to_diagonal(mat, damping)
    inv = torch.inverse(mat)

    model.zero_grad()
    loss = F.cross_entropy(outputs, targets)
    grads = torch.autograd.grad(loss, outputs, retain_graph=True)[0].flatten()  # nc x 1
    v = torch.matmul(inv, grads).reshape(n, -1)  # n x c

    # compute natural-gradient by auto-differentiation
    torch.autograd.backward(outputs, grad_tensors=v)


def efficient_natural_gradient_cross_entropy(model, inputs, targets, class_kernels, damping=1e-5):
    assert class_kernels.ndim == 3  # c x n x n
    model.zero_grad()
    outputs = model(inputs)

    v = logits_second_order_grad_cross_entropy(outputs, targets, damping)  # n x c

    v = v.transpose(0, 1)  # c x n
    v = _cholesky_solve(class_kernels, v)  # c x n
    v = v.transpose(0, 1)  # n x c

    # compute natural-gradient by auto-differentiation
    torch.autograd.backward(outputs, grad_tensors=v)


def parallel_efficient_natural_gradient_cross_entropy(model, inputs, targets, local_class_kernels, damping=1e-5):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_n = inputs.shape[0]  # n

    # compute second-order gradient w.r.t logits in a data-parallel fashion
    outputs = model(inputs)  # local_n x c
    v = logits_second_order_grad_cross_entropy(outputs, targets, damping)  # local_n x c

    # data to class-parallel (all-to-all)
    n_classes = outputs.shape[-1]  # c
    classes_split = np.array_split(range(n_classes), world_size)
    gather_list = None
    for dst, local_classes in enumerate(classes_split):
        if len(local_classes) == 0:
            break
        tensor = v[:, local_classes].clone()  # local_n x local_c
        if rank == dst:
            gather_list = [torch.zeros_like(tensor) for _ in range(world_size)]
            dist.gather(tensor, gather_list, dst=dst)
        else:
            dist.gather(tensor, dst=dst)

    # solve inverse in a class-parallel fashion
    has_local_classes = len(classes_split[rank]) > 0
    if has_local_classes:
        assert local_class_kernels is not None
        assert local_class_kernels.ndim == 3  # local_c x n x n
        local_c, n, m = local_class_kernels.shape
        assert n == local_n * world_size
        v = torch.cat(gather_list).transpose(0, 1)  # local_c x n
        assert v.shape[0] == local_c and v.shape[1] == n == m, f'rank: {rank}, v: {v.shape}, local_class_kernels: {local_class_kernels.shape}'
        v = _cholesky_solve(local_class_kernels, v)  # local_c x n
    else:
        v = None

    # class to data-parallel (all-to-all)
    gather_list = None
    max_n_classes = len(classes_split[0])
    for dst in range(world_size):
        if has_local_classes:
            tensor = v[:, dst * local_n: (dst + 1) * local_n].clone()  # local_c x local_n
            local_c = len(classes_split[rank])
            for _ in range(max_n_classes - local_c):
                dummy = torch.zeros_like(tensor[0]).unsqueeze(0)
                tensor = torch.cat([tensor, dummy])
        else:
            tensor = inputs.new_zeros(max_n_classes, local_n)
        if rank == dst:
            gather_list = [torch.zeros_like(tensor) for _ in range(world_size)]
            dist.gather(tensor, gather_list, dst=dst)
        else:
            dist.gather(tensor, dst=dst)

    tensors = []
    for tensor, local_classes in zip(gather_list, classes_split):
        local_c = len(local_classes)
        if local_c == 0:
            break
        tensors.append(tensor[:local_c])

    v = torch.cat(tensors).transpose(0, 1)  # local_n x c

    # compute natural-gradient in a data-parallel fashion
    model.zero_grad()
    torch.autograd.backward(outputs, grad_tensors=v)

    # all-reduce natural gradient
    params = [p for p in model.parameters() if p.requires_grad]
    packed_tensor = torch.cat([p.grad.flatten() for p in params])
    dist.all_reduce(packed_tensor)
    pointer = 0
    for p in params:
        numel = p.numel()
        grad = packed_tensor[pointer: pointer + numel].view_as(p.grad)
        p.grad.copy_(grad)
        pointer += numel
    assert pointer == packed_tensor.numel()


def kernel_free_cross_entropy(model,
                              inputs,
                              targets,
                              damping=1e-5,
                              tol=1e-3,
                              max_iters=None,
                              is_distributed=False,
                              print_progress=False):
    outputs = model(inputs)  # n x c
    n_data, n_classes = outputs.shape
    if is_distributed:
        n_data *= dist.get_world_size()
    if max_iters is None:
        max_iters = n_data * n_classes

    hessian = logits_hessian_cross_entropy(outputs)  # n x c x c
    loss = F.cross_entropy(outputs, targets, reduction='sum').div(n_data)
    grads = torch.autograd.grad(loss, outputs, retain_graph=True)[0]  # n x c

    gg = torch.sum(torch.pow(grads, 2))
    if is_distributed:
        dist.all_reduce(gg)
    g_norm = torch.sqrt(gg)

    x = torch.zeros_like(outputs)
    p = grads.clone().requires_grad_(True)
    r = grads.clone()

    last_n = torch.sum(torch.pow(r, 2))
    if is_distributed:
        dist.all_reduce(last_n)
    for i in range(max_iters):
        vjp = torch.autograd.grad(outputs, list(model.parameters()), grad_outputs=p, retain_graph=True, create_graph=True)
        g = [tensor.clone() for tensor in vjp]
        if is_distributed:
            g = _all_reduce_tensor_list(g)
        kernel_vp = torch.autograd.grad(vjp, p, grad_outputs=g)[0]
        u = torch.einsum('nij,nj->ni', hessian, kernel_vp).div(n_data)  # n x c
        u.add_(p, alpha=damping)

        m = torch.sum(p.mul(u))
        if is_distributed:
            dist.all_reduce(m)

        alpha = (last_n / m).item()
        x.add_(p, alpha=alpha)
        r.sub_(u, alpha=alpha)

        n = torch.sum(torch.pow(r, 2))
        if is_distributed:
            dist.all_reduce(n)

        err = n.sqrt() / g_norm
        if print_progress:
            print(f'{i+1}/{max_iters} err={err}')
        if err < tol:
            break
        beta = (n / last_n).item()
        p = r.add(p, alpha=beta)
        last_n = n

    model.zero_grad()
    torch.autograd.backward(outputs, grad_tensors=x)
    if is_distributed:
        params = [p for p in model.parameters() if p.requires_grad]
        packed_tensor = torch.cat([p.grad.flatten() for p in params])
        dist.all_reduce(packed_tensor)
        pointer = 0
        for j, p in enumerate(params):
            numel = p.grad.numel()
            p.grad.copy_(packed_tensor[pointer: pointer + numel].reshape_as(p.grad))
            pointer += numel


def kernel_vector_product(model, inputs, vec):
    outputs = model(inputs)
    vec.requires_grad_(True)
    vjp = torch.autograd.grad(outputs, list(model.parameters()), grad_outputs=vec, create_graph=True)
    return torch.autograd.grad(vjp, vec, grad_outputs=vjp)[0]


def kernel_eigenvalues(model,
                       inputs,
                       top_n=1,
                       max_iters=100,
                       tol=1e-3,
                       eps=1e-6,
                       eigenvectors=False,
                       cross_entropy=False,
                       is_distributed=False,
                       gather_type=_ALL,
                       print_progress=False):
    assert top_n >= 1
    assert max_iters >= 1

    eigvals = []
    eigvecs = []
    outputs = model(inputs)

    if cross_entropy:
        hessian = logits_hessian_cross_entropy(outputs)
    else:
        hessian = None

    for i in range(top_n):
        if print_progress:
            print(f'start power iteration for lambda({i+1}).')
        vec = torch.randn_like(outputs)
        eigval = None
        last_eigval = None
        # power iteration
        for j in range(max_iters):
            # get a vector that is orthogonal to all eigenvalues
            for v in eigvecs:
                alpha = torch.sum(vec.mul(v))
                if is_distributed:
                    dist.all_reduce(alpha)
                vec.sub_(v, alpha=alpha.item())

            # normalize the vector
            vv = torch.pow(vec, 2).sum()
            if is_distributed:
                dist.all_reduce(vv)
            vec.div_(torch.sqrt(vv))

            # J'v
            vec.requires_grad_(True)
            vjp = torch.autograd.grad(outputs, list(model.parameters()), grad_outputs=vec, create_graph=True)
            g = [tensor.clone() for tensor in vjp]
            if is_distributed:
                g = _all_reduce_tensor_list(g)

            # JJ'v
            kernel_vp = torch.autograd.grad(vjp, vec, grad_outputs=g, retain_graph=True)[0]
            if cross_entropy:
                # HJJ'v
                kernel_vp = torch.einsum('nij,nj->ni', hessian, kernel_vp)

            # v'JJ'v / v'v = v'JJ'v
            eigval = torch.sum(kernel_vp.mul(vec))
            if is_distributed:
                dist.all_reduce(eigval)

            if j > 0:
                diff = abs(eigval - last_eigval) / (abs(last_eigval) + eps)
                if print_progress:
                    print(f'{j}/{max_iters} diff={diff}')
                if diff < tol:
                    break

            last_eigval = eigval
            vec = kernel_vp
        eigvals.append(eigval)
        eigvecs.append(vec)

    # sort both in descending order
    eigvals, eigvecs = (list(t) for t in zip(*sorted(zip(eigvals, eigvecs))[::-1]))

    if eigenvectors:
        if is_distributed:
            world_size = dist.get_world_size()
            is_master = dist.get_rank() == 0
            for i, v in enumerate(eigvecs):
                gather_list = [torch.zeros_like(v) for _ in range(world_size)]
                if gather_type == _MASTER:
                    if is_master:
                        dist.gather(v, gather_list, dst=0)
                    else:
                        dist.gather(v, dst=0)
                elif gather_type == _ALL:
                    dist.all_gather(gather_list, v)
                else:
                    raise ValueError(f'Invalid gather type {gather_type}.')
                eigvecs[i] = torch.cat([_v.flatten() for _v in gather_list])
        return eigvals, eigvecs
    else:
        return eigvals


def _all_reduce_tensor_list(tensor_list):
    packed_tensor = torch.cat([tensor.clone().flatten() for tensor in tensor_list])
    dist.all_reduce(packed_tensor)
    pointer = 0
    rst = []
    for i, tensor in enumerate(tensor_list):
        numel = tensor.numel()
        v = packed_tensor[pointer: pointer + numel].clone().reshape_as(tensor)
        rst.append(v)
        pointer += numel

    return rst


def _cholesky_solve(A, b, eps=1e-8):
    A = _add_value_to_diagonal(A, eps)
    if A.ndim > b.ndim:
        b = b.unsqueeze(dim=-1)
    u = torch.cholesky(A)
    return torch.cholesky_solve(b, u).squeeze(dim=-1)


def _add_value_to_diagonal(X, value):
    if X.ndim == 3:
        return torch.stack([_add_value_to_diagonal(X[i], value) for i in range(X.shape[0])])
    else:
        assert X.ndim == 2

    indices = torch.tensor([[i, i] for i in range(X.shape[0])], device=X.device).long()

    values = X.new_ones(X.shape[0]).mul(value)
    return X.index_put(tuple(indices.t()), values, accumulate=True)


def _zero_kernel(model, n_data1, n_data2):
    p = next(iter(model.parameters()))
    kernel = torch.zeros(n_data1,
                         n_data2,
                         device=p.device,
                         dtype=p.dtype)
    setattr(model, 'kernel', kernel)


def _clear_kernel(model):
    if hasattr(model, 'kernel'):
        delattr(model, 'kernel')

File Path: dependencies/asdl/asdfghjkl/loss.py
Content:
from math import log, pi
import torch

C = - 0.5 * log(2 * pi)


def heteroscedastic_mse_loss(input, target, reduction='mean'):
    """Heteroscedastic negative log likelihood Normal.

    Parameters
    ----------
    input : torch.Tensor (n, 2)
        two natural parameters per data point
    target : torch.Tensor (n, 1)
        targets
    """
    assert input.ndim == target.ndim == 2
    assert input.shape[0] == target.shape[0]
    n, _ = input.shape
    target = torch.cat([target, target.square()], dim=1)
    inner = torch.einsum('nk,nk->n', target, input)
    log_A = input[:, 0].square() / (4 * input[:, 1]) + 0.5 * torch.log(- 2 * input[:, 1])
    log_lik = n * C + inner.sum() + log_A.sum()
    if reduction == 'mean':
        return - log_lik / n
    elif reduction == 'sum':
        return - log_lik 
    else:
        raise ValueError('Invalid reduction', reduction)

File Path: dependencies/asdl/asdfghjkl/matrices.py
Content:
import os
import copy

import torch
import torch.distributed as dist

from .symmatrix import SymMatrix

HESSIAN = 'hessian'  # Hessian
FISHER_EXACT = 'fisher_exact'  # exact Fisher
FISHER_MC = 'fisher_mc'  # Fisher estimation by Monte-Carlo sampling
COV = 'cov'  # no-centered covariance a.k.a. empirical Fisher

SHAPE_FULL = 'full'  # full
SHAPE_BLOCK_DIAG = 'block_diag'  # layer-wise block-diagonal
SHAPE_KRON = 'kron'  # Kronecker-factored
SHAPE_DIAG = 'diag'  # diagonal

__all__ = [
    'MatrixManager',
    'FISHER_EXACT',
    'FISHER_MC',
    'COV',
    'HESSIAN',
    'SHAPE_FULL',
    'SHAPE_BLOCK_DIAG',
    'SHAPE_KRON',
    'SHAPE_DIAG'
]

_supported_types = [HESSIAN, FISHER_EXACT, FISHER_MC, COV]
_supported_shapes = [SHAPE_FULL, SHAPE_BLOCK_DIAG, SHAPE_KRON, SHAPE_DIAG]

_normalizations = (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)


def _requires_matrix(module: torch.nn.Module):
    if not hasattr(module, 'weight'):
        return False
    if module.weight.requires_grad:
        return True
    return hasattr(module, 'bias') and module.bias.requires_grad


class MatrixManager:
    def __init__(self, model, matrix_types, scale=1., smoothing_weight=None):
        self._model = model
        self._device = next(model.parameters()).device
        if isinstance(matrix_types, str):
            matrix_types = [matrix_types]
        for mat_type in matrix_types:
            assert mat_type in _supported_types, f'Invalid matrix_type: {mat_type}. matrix_type must be in {_supported_types}.'
        # remove duplicates
        self._matrix_types = set(matrix_types)
        # for updating stats
        self._scale = scale
        self._smoothing_weight = smoothing_weight
        self._stats_names = set()

    @staticmethod
    def _get_save_field(matrix_type, stats_name=None):
        if stats_name is None:
            return matrix_type
        return f'{stats_name}_{matrix_type}'

    def _clear_stats(self, stats_name):
        if stats_name in self._stats_names:
            self._stats_names.remove(stats_name)

    def _check_stats_name(self, stats_name):
        if stats_name is None:
            return
        assert stats_name in self._stats_names, f'stats {stats_name} does not exist.'

    def accumulate_matrices(
        self, stats_name, scale=None, smoothing_weight=None
    ):
        """
        Accumulate the latest fisher values to acc_fisher.
        module.{fisher_type} = fisher
        module.{stats_name}_{fisher_type} = acc_fisher
        """
        self._stats_names.add(stats_name)
        if scale is None:
            scale = self._scale
        if smoothing_weight is None:
            smoothing_weight = self._smoothing_weight

        for module in self._model.modules():
            for mat_type in self._matrix_types:
                matrix = getattr(module, mat_type, None)
                if matrix is None:
                    continue
                matrix.scaling(scale)
                stats_attr = self._get_save_field(mat_type, stats_name)
                stats = getattr(module, stats_attr, None)
                if stats is None:
                    setattr(module, stats_attr, copy.deepcopy(matrix))
                    continue
                if smoothing_weight:
                    w = smoothing_weight
                    stats_ema = stats.scaling(1 - w) + matrix.scaling(w)
                    setattr(module, stats_attr, stats_ema)
                else:
                    stats = stats + matrix
                    setattr(module, stats_attr, stats)

    def save_matrices(self, root, relative_dir='', stats_name=None):
        """
        Save fisher for each fisher_type and for each module.
        module.{stats_name}_{fisher_type} = fisher
        """
        self._check_stats_name(stats_name)

        # save all fisher and collect relative_paths
        relative_paths = {}
        for mat_type in self._matrix_types:
            relative_paths[mat_type] = {}
            for mname, module in self._model.named_modules():
                stats_attr = self._get_save_field(mat_type, stats_name)
                stats = getattr(module, stats_attr, None)
                # if module does not have computed matrices, skip
                if stats is None:
                    continue
                _relative_dir = os.path.join(relative_dir, mat_type, mname)
                rst = stats.save(root, _relative_dir)
                if module is self._model:
                    relative_paths[mat_type].update(rst)
                else:
                    relative_paths[mat_type][mname] = rst

        return relative_paths

    def load_matrices(self, root, relative_paths, matrix_shapes):
        for mat_shape in matrix_shapes:
            assert mat_shape in _supported_shapes, f'Invalid matrix_shape: {mat_shape}. matrix_shape must be in {_supported_shapes}'

        def root_join(path_or_dict):
            if isinstance(path_or_dict, dict):
                rst = {}
                for k, v in path_or_dict.items():
                    rst[k] = root_join(v)
                return rst
            else:
                return os.path.join(root, path_or_dict)

        paths = root_join(relative_paths)

        for mat_type in self._matrix_types:
            mat_paths = paths.get(mat_type, None)
            if mat_paths is None:
                raise ValueError(f'matrix type {mat_type} does not exist.')

            def _load_path(mat_shape, load_key, path_key, module_name=None):
                try:
                    if module_name:
                        kwargs = {load_key: mat_paths[module_name][path_key]}
                    else:
                        kwargs = {load_key: mat_paths[path_key]}
                    matrix.load(**kwargs)
                except (KeyError, FileNotFoundError):
                    if module_name:
                        raise ValueError(
                            f'{mat_type}.{mat_shape} for module {module_name} does not exist.'
                        )
                    else:
                        raise ValueError(
                            f'{mat_type}.{mat_shape} does not exist.'
                        )

            # load layer-wise matrices
            for mname, module in self._model.named_modules():
                if module is self._model:
                    continue
                if not _requires_matrix(module):
                    continue
                matrix = SymMatrix(device=self._device)
                if SHAPE_BLOCK_DIAG in matrix_shapes:
                    _load_path(SHAPE_BLOCK_DIAG, 'path', 'tril', mname)
                if SHAPE_KRON in matrix_shapes:
                    if isinstance(module, _normalizations):
                        _load_path(
                            'unit_wise', 'unit_path', 'unit_wise', mname
                        )
                    else:
                        _load_path(SHAPE_KRON, 'kron_path', 'kron', mname)
                if SHAPE_DIAG in matrix_shapes:
                    _load_path(SHAPE_DIAG, 'diag_path', 'diag', mname)
                setattr(module, mat_type, matrix)

            # full matrix
            if SHAPE_FULL in matrix_shapes:
                matrix = SymMatrix(device=self._device)
                _load_path(SHAPE_FULL, 'path', 'tril')
                setattr(self._model, mat_type, matrix)

    def matrices_exist(self, root, relative_paths, matrix_shapes):
        try:
            self.load_matrices(root, relative_paths, matrix_shapes)
            return True
        except ValueError:
            return False

    def clear_matrices(self, stats_name):
        """
        Clear fisher for each fisher_type and for each module.
        module.{stats_name}_{fisher_type} = fisher
        """
        self._check_stats_name(stats_name)

        # save all fisher and collect relative_paths
        for mat_type in self._matrix_types:
            stats_attr = self._get_save_field(mat_type, stats_name)
            for module in self._model.modules():
                if hasattr(module, stats_attr):
                    delattr(module, stats_attr)

        self._clear_stats(stats_name)

    def matrices_to_vector(self, stats_name):
        """
        Flatten all fisher values.
        module.{stats_name}_{fisher_type} = fisher
        fisher = {
            'diag': {'weight': torch.Tensor, 'bias': torch.Tensor},
            'kron': {'A': torch.Tensor, 'B': torch.Tensor},
            'block_diag': {'F': torch.Tensor},
        }
        """
        self._check_stats_name(stats_name)
        vec = []
        for mat_type in self._matrix_types:
            stats_attr = self._get_save_field(mat_type, stats_name)
            for module in self._model.modules():
                stats = getattr(module, stats_attr, None)
                if stats is None:
                    continue
                vec.extend(stats.to_vector())

        vec = [v.flatten() for v in vec]
        return torch.cat(vec)

    def vector_to_matrices(self, vec, stats_name):
        """
        Unflatten vector like fisher.
        module.{stats_name}_{fisher_type} = fisher
        fisher = {
            'diag': {'weight': torch.Tensor, 'bias': torch.Tensor},
            'kron': {'A': torch.Tensor, 'B': torch.Tensor},
            'block_diag': {'F': torch.Tensor},
        }
        """
        self._check_stats_name(stats_name)

        pointer = 0
        for mat_type in self._matrix_types:
            stats_attr = self._get_save_field(mat_type, stats_name)
            for module in self._model.modules():
                stats = getattr(module, stats_attr, None)
                if stats is None:
                    continue
                pointer = stats.to_matrices(vec, pointer)

        assert pointer == torch.numel(vec)

    def reduce_matrices(
        self, stats_name=None, is_master=True, all_reduce=False
    ):
        # pack
        packed_tensor = self.matrices_to_vector(stats_name)
        # reduce
        if all_reduce:
            dist.all_reduce(packed_tensor)
        else:
            dist.reduce(packed_tensor, dst=0)
        # unpack
        if is_master or all_reduce:
            self.vector_to_matrices(
                packed_tensor.div_(dist.get_world_size()), stats_name
            )
        dist.barrier()

    def _collect_metrics(
        self,
        matrix_type,
        matrix_shape,
        stats_name,
        metrics_fn,
        reduce_fn,
        init
    ):
        stats_attr = self._get_save_field(matrix_type, stats_name)
        if matrix_shape == SHAPE_FULL:
            matrix = getattr(self._model, stats_attr, None)
            assert matrix is not None and matrix.has_data, f'{matrix_type}.{matrix_shape} does not exist.'
            return getattr(matrix, metrics_fn)()

        rst = init
        for mname, module in self._model.named_modules():
            if module is self._model:
                continue
            if not _requires_matrix(module):
                continue
            matrix = getattr(module, stats_attr, None)
            assert matrix is not None, f'{matrix_type} for {mname} does not exist.'
            if matrix_shape == SHAPE_BLOCK_DIAG:
                assert matrix.has_data, f'{matrix_type}.{matrix_shape} for {mname} does not exist.'
                rst = reduce_fn(rst, getattr(matrix, metrics_fn)())
            elif matrix_shape == SHAPE_KRON:
                if isinstance(module, _normalizations):
                    assert matrix.has_unit, f'{matrix_type}.unit_wise for {mname} does not exist.'
                    rst = reduce_fn(rst, getattr(matrix.unit, metrics_fn)())
                else:
                    assert matrix.has_kron, f'{matrix_type}.{matrix_shape} for {mname} does not exist.'
                    rst = reduce_fn(rst, getattr(matrix.kron, metrics_fn)())
            elif matrix_shape == SHAPE_DIAG:
                assert matrix.has_diag, f'{matrix_type}.{matrix_shape} for {mname} does not exist.'
                rst = reduce_fn(rst, getattr(matrix.diag, metrics_fn)())
            else:
                raise ValueError(f'Invalid matrix_shape: {matrix_shape}.')
        return rst

    def get_eigenvalues(self, matrix_type, matrix_shape, stats_name=None):
        def reduce(val1, val2):
            val1.append(val2)
            return val1

        rst = self._collect_metrics(
            matrix_type,
            matrix_shape,
            stats_name,
            metrics_fn='eigenvalues',
            reduce_fn=reduce,
            init=[]
        )
        if not isinstance(rst, torch.Tensor):
            rst = torch.sort(torch.cat(rst), descending=True)[0]
        return rst

    def get_top_eigenvalue(self, matrix_type, matrix_shape, stats_name=None):
        def reduce(val1, val2):
            val1 = max(val1, val2)
            return val1

        rst = self._collect_metrics(
            matrix_type,
            matrix_shape,
            stats_name,
            metrics_fn='top_eigenvalue',
            reduce_fn=reduce,
            init=-1
        )
        return rst

    def get_trace(self, matrix_type, matrix_shape, stats_name=None):
        def reduce(val1, val2):
            val1 += val2
            return val1

        rst = self._collect_metrics(
            matrix_type,
            matrix_shape,
            stats_name,
            metrics_fn='trace',
            reduce_fn=reduce,
            init=0
        )
        return rst

    def get_effective_dim(
        self, matrix_type, matrix_shape, reg, stats_name=None
    ):
        eigs = self.get_eigenvalues(
            matrix_type, matrix_shape, stats_name=stats_name
        )

        return torch.sum(eigs / (eigs + reg))

File Path: dependencies/asdl/asdfghjkl/mvp.py
Content:
import math
import copy

import torch
import torch.distributed as dist

__all__ = [
    'power_method',
    'conjugate_gradient_method',
    'mvp',
]


def power_method(mvp_fn,
                 model,
                 data_loader=None,
                 inputs=None,
                 targets=None,
                 top_n=1,
                 max_iters=100,
                 tol=1e-3,
                 is_distributed=False,
                 print_progress=False,
                 random_seed=None):
    # main logic is adopted from https://github.com/amirgholami/PyHessian/blob/master/pyhessian/hessian.py
    # modified interface and format
    # modified for various matrices and distributed memory run

    assert top_n >= 1
    assert max_iters >= 1

    params = [p for p in model.parameters() if p.requires_grad]

    def _report(message):
        if print_progress:
            print(message)

    def _call_mvp(v):
        return mvp(mvp_fn,
                   v,
                   data_loader=data_loader,
                   inputs=inputs,
                   targets=targets,
                   random_seed=random_seed,
                   is_distributed=is_distributed)

    eigvals = []
    eigvecs = []
    for i in range(top_n):
        _report(f'start power iteration for lambda({i+1}).')
        vec = [torch.randn_like(p) for p in params]
        if is_distributed:
            vec = _flatten_parameters(vec)
            dist.broadcast(vec, src=0)
            vec = _unflatten_like_parameters(vec, params)

        eigval = None
        last_eigval = None
        # power iteration
        for j in range(max_iters):
            vec = _orthnormal(vec, eigvecs)
            Mv = _call_mvp(vec)
            eigval = _group_product(Mv, vec).item()
            if j > 0:
                diff = abs(eigval - last_eigval) / (abs(last_eigval) + 1e-6)
                _report(f'{j}/{max_iters} diff={diff}')
                if diff < tol:
                    break
            last_eigval = eigval
            vec = Mv
        eigvals.append(eigval)
        eigvecs.append(vec)

    # sort both in descending order
    eigvals, eigvecs = (list(t) for t in zip(*sorted(zip(eigvals, eigvecs))[::-1]))

    return eigvals, eigvecs


def conjugate_gradient_method(mvp_fn,
                              b,
                              data_loader=None,
                              inputs=None,
                              targets=None,
                              init_x=None,
                              damping=1e-3,
                              max_iters=None,
                              tol=1e-8,
                              preconditioner=None,
                              is_distributed=False,
                              print_progress=False,
                              random_seed=None,
                              save_log=False):
    """
    Solve (A + d * I)x = b by conjugate gradient method.
    d: damping
    Return x when x is close enough to inv(A) * b.
    """
    if max_iters is None:
        n_dim = sum([_b.numel() for _b in b])
        max_iters = n_dim

    def _call_mvp(v):
        return mvp(mvp_fn,
                   v,
                   data_loader=data_loader,
                   inputs=inputs,
                   targets=targets,
                   random_seed=random_seed,
                   damping=damping,
                   is_distributed=is_distributed)

    x = init_x
    if x is None:
        x = [torch.zeros_like(_b) for _b in b]
        r = copy.deepcopy(b)
    else:
        Ax = _call_mvp(x)
        r = _group_add(b, Ax, -1)

    if preconditioner is None:
        p = copy.deepcopy(r)
        last_rz = _group_product(r, r)
    else:
        p = preconditioner.precondition_vector(r)
        last_rz = _group_product(r, p)

    b_norm = math.sqrt(_group_product(b, b))

    log = []
    for i in range(max_iters):
        Ap = _call_mvp(p)
        alpha = last_rz / _group_product(p, Ap)
        x = _group_add(x, p, alpha)
        r = _group_add(r, Ap, -alpha)
        rr = _group_product(r, r)
        err = math.sqrt(rr) / b_norm
        log.append({'step': i + 1, 'error': err})
        if print_progress:
            print(f'{i+1}/{max_iters} err={err}')
        if err < tol:
            break
        if preconditioner is None:
            z = r
            rz = rr
        else:
            z = preconditioner.precondition_vector(r)
            rz = _group_product(r, z)

        beta = rz / last_rz  # Fletcher-Reeves
        p = _group_add(z, p, beta)
        last_rz = rz

    if save_log:
        return x, log
    else:
        return x


def mvp(mvp_fn,
        vec,
        data_loader=None,
        inputs=None,
        targets=None,
        random_seed=None,
        damping=None,
        is_distributed=False):

    if random_seed:
        # for matrices that are not deterministic (e.g., fisher_mc)
        torch.manual_seed(random_seed)

    if data_loader is not None:
        Mv = _data_loader_mvp(mvp_fn, vec, data_loader)
    else:
        assert inputs is not None
        Mv = mvp_fn(vec, inputs, targets)

    if damping:
        Mv = _group_add(Mv, vec, damping)

    if is_distributed:
        Mv = _all_reduce_params(Mv)

    return Mv


def _data_loader_mvp(mvp_fn, vec, data_loader):
    device = vec[0].device
    Mv = None
    for inputs, targets in data_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        _Mv = mvp_fn(vec, inputs, targets)
        if Mv is None:
            Mv = _Mv
        else:
            Mv = [mv.add(_mv) for mv, _mv in zip(Mv, _Mv)]

    Mv = [mv.div(len(data_loader)) for mv in Mv]

    return Mv


def _all_reduce_params(params):
    world_size = dist.get_world_size()
    # pack
    packed_tensor = _flatten_parameters(params)
    # all-reduce
    dist.all_reduce(packed_tensor)
    # unpack
    rst = _unflatten_like_parameters(packed_tensor.div(world_size), params)

    dist.barrier()

    return rst


def _flatten_parameters(params):
    vec = []
    for param in params:
        vec.append(param.flatten())
    return torch.cat(vec)


def _unflatten_like_parameters(vec, params):
    pointer = 0
    rst = []
    for param in params:
        numel = param.numel()
        rst.append(vec[pointer:pointer + numel].view_as(param))
        pointer += numel
    return rst


def _group_product(xs, ys):
    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])


def _group_add(xs, ys, alpha=1.):
    return [x.add(y.mul(alpha)) for x, y in zip(xs, ys)]


def _normalization(v):
    s = _group_product(v, v)
    s = s**0.5
    s = s.cpu().item()
    v = [vi / (s + 1e-6) for vi in v]
    return v


def _orthnormal(w, v_list):
    for v in v_list:
        w = _group_add(w, v, alpha=-_group_product(w, v))
    return _normalization(w)

File Path: dependencies/asdl/asdfghjkl/operations/__init__.py
Content:
import warnings
from torch import nn
from .operation import *
from .linear import Linear
from .conv import Conv2d
from .batchnorm import BatchNorm1d, BatchNorm2d
from .bias import Bias, BiasExt
from .scale import Scale, ScaleExt
from .conv_aug import Conv2dAug, Conv1dAug, Conv2dAugExt

__all__ = [
    'Linear',
    'Conv2d',
    'BatchNorm1d',
    'BatchNorm2d',
    'Bias',
    'Scale',
    'BiasExt',
    'ScaleExt',
    'Conv2dAug',
    'Conv2dAugExt',
    'get_op_class',
    'Operation',
    'OP_COV_KRON',
    'OP_COV_DIAG',
    'OP_COV_UNIT_WISE',
    'OP_GRAM_DIRECT',
    'OP_GRAM_HADAMARD',
    'OP_BATCH_GRADS',
    'OP_BATCH_GRADS_KRON',
    'OP_ACCUMULATE_GRADS'
]


def get_op_class(module):
    if isinstance(module, nn.Linear):
        return Linear
    elif isinstance(module, Conv2dAug):
        return Conv2dAugExt
    elif isinstance(module, nn.Conv2d):
        return Conv2d
    elif isinstance(module, Conv1dAug):
        return Conv2dAugExt
    elif isinstance(module, nn.Conv1d):
        return Conv2d
    elif isinstance(module, nn.BatchNorm1d):
        return BatchNorm1d
    elif isinstance(module, nn.BatchNorm2d):
        return BatchNorm2d
    elif isinstance(module, Bias):
        return BiasExt
    elif isinstance(module, Scale):
        return ScaleExt
    else:
        warnings.warn(f'Failed to lookup operations for Module {module}.')
        return None

File Path: dependencies/asdl/asdfghjkl/operations/batchnorm.py
Content:
import torch
from torch import nn

from .operation import Operation, OP_COV_KRON, OP_COV_UNIT_WISE, OP_GRAM_HADAMARD, OP_GRAM_DIRECT  # NOQA


class _BatchNormNd(Operation):
    def __init__(self, module, model, op_names, save_attr='op_results'):
        if OP_COV_KRON in op_names:
            op_names = op_names.copy()
            # kron operation is not supported. unit_wise will be used instead.
            op_names.remove(OP_COV_KRON)
            op_names.append(OP_COV_UNIT_WISE)

        if OP_GRAM_HADAMARD in op_names:
            op_names = op_names.copy()
            # gram hadamard operation is not supported. gram direct will be used instead.
            op_names.remove(OP_GRAM_HADAMARD)
            op_names.append(OP_GRAM_DIRECT)

        super().__init__(module, model, op_names, save_attr)

    @staticmethod
    def _reduce(tensor: torch.Tensor):
        raise NotImplementedError

    def batch_grads_weight(
        self,
        module: nn.Module,
        in_data: torch.Tensor,
        out_grads: torch.Tensor
    ):
        return self._reduce(in_data.mul(out_grads))  # n x f

    def batch_grads_bias(self, module, out_grads):
        return self._reduce(out_grads)  # n x f

    def cov_diag_weight(self, module, in_data, out_grads):
        grads = self._reduce(in_data.mul(out_grads))
        return grads.mul(grads).sum(dim=0)  # f x 1

    def cov_diag_bias(self, module, out_grads):
        grads = self._reduce(out_grads)  # n x f
        return grads.mul(grads).sum(dim=0)  # f x 1

    def cov_unit_wise(self, module, in_data, out_grads):
        n_features = in_data.shape[1]  # f
        grads_w = self.batch_grads_weight(module, in_data, out_grads)  # n x f
        grads_b = self.batch_grads_bias(module, out_grads)  # n x f
        cov_ww = (grads_w ** 2).sum(0)  # f
        cov_bb = (grads_b ** 2).sum(0)  # f
        cov_wb = (grads_w * grads_b).sum(0)  # f
        blocks = torch.zeros(n_features, 2, 2).to(in_data.device)
        for i in range(n_features):
            blocks[i][0][0] = cov_ww[i]
            blocks[i][1][1] = cov_bb[i]
            blocks[i][0][1] = blocks[i][1][0] = cov_wb[i]
        return blocks  # f x 2 x 2

    @staticmethod
    def cov_kron_A(module, in_data):
        raise ValueError(
            f'{OP_COV_KRON} operation is not supported in BatchNormNd.'
        )

    @staticmethod
    def cov_kron_B(module, out_grads):
        raise ValueError(
            f'{OP_COV_KRON} operation is not supported in BatchNormNd.'
        )

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        raise ValueError(
            f'{OP_GRAM_HADAMARD} operation is not supported in BatchNormNd.'
        )

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        raise ValueError(
            f'{OP_GRAM_HADAMARD} operation is not supported in BatchNormNd.'
        )


class BatchNorm1d(_BatchNormNd):
    """
    module.weight: f x 1
    module.bias: f x 1

    Argument shapes
    in_data: n x f
    out_grads: n x f
    """
    def _reduce(self, tensor):
        return tensor


class BatchNorm2d(_BatchNormNd):
    """
    module.weight: c x 1
    module.bias: c x 1

    Argument shapes
    in_data: n x c x h x w
    out_grads: n x c x h x w
    """
    def _reduce(self, tensor):
        return tensor.sum(dim=(2, 3))

File Path: dependencies/asdl/asdfghjkl/operations/bias.py
Content:
import torch
from torch import nn

from .operation import Operation


class Bias(nn.Module):
    def __init__(self):
        super(Bias, self).__init__()
        self.weight = nn.Parameter(torch.zeros(1))
        
    def reset_parameters(self):
        nn.init.constant_(self.weight, 0)

    def forward(self, input):
        return input + self.weight


class BiasExt(Operation):
    """
    module.fixup_bias: 1

    Argument shapes
    in_data: n x f_in
    out_grads: n x f_out
    """
    @staticmethod
    def batch_grads_weight(module, in_data, out_grads):
        N = out_grads.size(0)
        return out_grads.view(N, -1).sum(dim=1)

    @staticmethod
    def batch_grads_kron_weight(module, in_data, out_grads):
        N = out_grads.size(0)
        return out_grads.view(N, -1).sum(dim=1)

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        N = out_grads.size(0)
        return out_grads.view(N, -1).sum(dim=1).square().sum()

    @staticmethod
    def cov_kron_A(module, in_data):
        return torch.ones((1, 1), device=in_data.device)

    @staticmethod
    def cov_kron_B(module, out_grads):
        N = out_grads.size(0)
        grad_grad = out_grads.view(N, -1).sum(dim=1).square().sum()
        return grad_grad.unsqueeze((0))

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        N1, N2 = in_data1.size(0), in_data2.size(0)
        return in_data1.new_ones(N1, N2)  # n x n

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        N1, N2 = out_grads1.size(0), out_grads2.size(0)
        grad1 = out_grads1.view(N1, -1).sum(dim=1)
        grad2 = out_grads2.view(N2, -1).sum(dim=1)
        return torch.outer(grad1, grad2)  # n x n

File Path: dependencies/asdl/asdfghjkl/operations/conv.py
Content:
import torch
from torch import nn

from .operation import Operation


class Conv2d(Operation):
    """
    module.weight: c_out x c_in x k_h x k_w
    module.bias: c_out x 1

    Argument shapes
    in_data: n x (c_in)(kernel_size) x out_size
    out_grads: n x c_out x out_size

    kernel_size = (k_h)(k_w)
    out_size = output feature map size
    """
    @staticmethod
    def batch_grads_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        grads = torch.bmm(
            out_grads, in_data.transpose(2, 1)
        )  # n x c_out x (c_in)(kernel_size)
        return grads.view(
            -1, *module.weight.size()
        )  # n x c_out x c_in x k_h x k_w

    @staticmethod
    def batch_grads_bias(module: nn.Module, out_grads: torch.tensor):
        return out_grads.sum(axis=2)  # n x c_out

    @staticmethod
    def batch_grads_kron_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        return Conv2d.batch_grads_weight(module, in_data, out_grads)

    @staticmethod
    def batch_grads_kron_bias(module: nn.Module, out_grads: torch.tensor):
        return Conv2d.batch_grads_bias(module, out_grads)

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        grads = torch.bmm(
            out_grads, in_data.transpose(2, 1)
        )  # n x c_out x (c_in)(kernel_size)
        rst = grads.mul(grads).sum(dim=0)  # c_out x (c_in)(kernel_size)
        return rst.view_as(module.weight)  # c_out x c_in x k_h x k_w

    @staticmethod
    def cov_diag_bias(module, out_grads):
        grads = out_grads.sum(axis=2)  # n x c_out
        return grads.mul(grads).sum(axis=0)  # c_out x 1

    @staticmethod
    def cov_kron_A(module, in_data):
        m = in_data.transpose(0, 1).flatten(
            start_dim=1
        )  # (c_in)(kernel_size) x n(out_size)
        return torch.matmul(
            m, m.T
        )  # (c_in)(kernel_size) x (c_in)(kernel_size)

    @staticmethod
    def cov_kron_B(module, out_grads):
        out_size = out_grads.shape[-1]
        m = out_grads.transpose(0,
                                1).flatten(start_dim=1)  # c_out x n(out_size)
        return torch.matmul(m, m.T).div(out_size)  # c_out x c_out

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        # n x (c_in)(kernel_size)(out_size)
        m1 = in_data1.flatten(start_dim=1)
        m2 = in_data2.flatten(start_dim=1)
        return torch.matmul(m1, m2.T)  # n x n

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        out_size = out_grads1.shape[-1]
        # n x (c_out)(out_size)
        m1 = out_grads1.flatten(start_dim=1)
        m2 = out_grads2.flatten(start_dim=1)
        return torch.matmul(m1, m2.T).div(out_size)  # n x n

File Path: dependencies/asdl/asdfghjkl/operations/conv_aug.py
Content:
import torch
from torch import nn

from .operation import Operation


class Conv2dAug(nn.Conv2d):
    
    def forward(self, input):
        k_aug = input.shape[1]
        input = super().forward(input.flatten(start_dim=0, end_dim=1))
        return input.reshape(-1, k_aug, *input.shape[1:])


class Conv1dAug(nn.Conv1d):
    
    def forward(self, input):
        k_aug = input.shape[1]
        input = super().forward(input.flatten(start_dim=0, end_dim=1))
        return input.reshape(-1, k_aug, *input.shape[1:])


class Conv2dAugExt(Operation):
    """
    module.weight: c_out x c_in x k_h x k_w
    module.bias: c_out x 1

    Argument shapes
    in_data: n x (c_in)(kernel_size) x out_size
    out_grads: n x c_out x out_size

    kernel_size = (k_h)(k_w)
    out_size = output feature map size
    """
    @staticmethod
    def batch_grads_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        grads = torch.matmul(
            out_grads, in_data.transpose(-1, -2)
        ).sum(dim=1)  # n x c_out x (c_in)(kernel_size)
        return grads.view(
            -1, *module.weight.size()
        )  # n x c_out x c_in x k_h x k_w

    @staticmethod
    def batch_grads_bias(module: nn.Module, out_grads: torch.tensor):
        return out_grads.sum(axis=[1, 3])  # n x c_out

    @staticmethod
    def batch_grads_kron_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        out_grads = out_grads.sum(dim=1)
        in_data = in_data.mean(dim=1)
        grads = torch.matmul(
            out_grads, in_data.transpose(-1, -2)
        )  # n x c_out x (c_in)(kernel_size)
        return grads.view(
            -1, *module.weight.size()
        )  # n x c_out x c_in x k_h x k_w

    @staticmethod
    def batch_grads_kron_bias(module: nn.Module, out_grads: torch.tensor):
        return out_grads.sum(axis=[1, 3])  # n x c_out

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        # efficient reduction for augmentation (sum and mean of in and out)
        grads = torch.matmul(
            out_grads.sum(dim=1), in_data.mean(dim=1).transpose(-1, -2)
        ) # n x k_aug x c_out x (c_in)(kernel_size)
        rst = grads.mul(grads).sum(dim=0)  # c_out x (c_in)(kernel_size)
        return rst.view_as(module.weight)  # c_out x c_in x k_h x k_w

    @staticmethod
    def cov_diag_bias(module, out_grads):
        grads = out_grads.sum(axis=[1, 3])  # n x c_out
        return grads.mul(grads).sum(axis=0)  # c_out x 1

    @staticmethod
    def cov_kron_A(module, in_data):
        m = in_data.sum(dim=1).transpose(0, 1).flatten(
            start_dim=1
        )  # (c_in)(kernel_size) x n(out_size)
        return torch.matmul(
            m, m.T
        )  # (c_in)(kernel_size) x (c_in)(kernel_size)

    @staticmethod
    def cov_kron_B(module, out_grads):
        out_size = out_grads.shape[-1]
        m = out_grads.mean(dim=1).transpose(0, 1).flatten(start_dim=1)  # c_out x n(out_size)
        return torch.matmul(m, m.T).div(out_size)  # c_out x c_out

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        # n x (c_in)(kernel_size)(out_size)
        m1 = in_data1.sum(dim=1).flatten(start_dim=1)
        m2 = in_data2.sum(dim=1).flatten(start_dim=1)
        return torch.matmul(m1, m2.T)  # n x n

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        out_size = out_grads1.shape[-1]
        # n x (c_out)(out_size)
        m1 = out_grads1.mean(dim=1).flatten(start_dim=1)
        m2 = out_grads2.mean(dim=1).flatten(start_dim=1)
        return torch.matmul(m1, m2.T).div(out_size)  # n x n


class Conv1dAug(nn.Conv1d):
    
    def forward(self, input):
        k_aug = input.shape[1]
        input = super().forward(input.flatten(start_dim=0, end_dim=1))
        return input.reshape(-1, k_aug, *input.shape[1:])

File Path: dependencies/asdl/asdfghjkl/operations/linear.py
Content:
import torch
from torch import nn

from .operation import Operation


class Linear(Operation):
    """
    module.weight: f_out x f_in
    module.bias: f_out x 1

    Argument shapes
    in_data: n x f_in
    out_grads: n x f_out
    """
    @staticmethod
    def batch_grads_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        if batch_grads.ndim > 3:
            batch_grads = batch_grads.sum(tuple(range(1, in_data.ndim-1)))
        return batch_grads

    @staticmethod
    def batch_grads_bias(module, out_grads):
        if out_grads.ndim > 2:
            return out_grads.sum(tuple(range(1, out_grads.ndim-1)))
        return out_grads

    @staticmethod
    def batch_grads_kron_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        if in_data.ndim > 2:
            in_data = in_data.sum(tuple(range(1, in_data.ndim-1)))
        if out_grads.ndim > 2:
            out_grads = out_grads.mean(tuple(range(1, out_grads.ndim-1)))
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads

    @staticmethod
    def batch_grads_kron_bias(module, out_grads):
        if out_grads.ndim > 2:
            return out_grads.sum(tuple(range(1, out_grads.ndim-1)))
        return out_grads

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        # efficient reduction for augmentation and weight-sharing
        if in_data.ndim > 2:
            in_data = in_data.mean(tuple(range(1, in_data.ndim-1, 1)))
        if out_grads.ndim > 2:
            out_grads = out_grads.sum(tuple(range(1, out_grads.ndim-1, 1)))
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads.square().sum(dim=0)

    @staticmethod
    def cov_diag_bias(module, out_grads):
        if out_grads.ndim > 2:
            out_grads = out_grads.sum(tuple(range(1, out_grads.ndim-1)))
        return out_grads.mul(out_grads).sum(dim=0)

    @staticmethod
    def cov_kron_A(module, in_data):
        if in_data.ndim > 2:
            in_data = in_data.mean(tuple(range(1, in_data.ndim-1, 1)))
        return torch.matmul(in_data.T, in_data)

    @staticmethod
    def cov_kron_B(module, out_grads):
        if out_grads.ndim > 2:
            out_grads = out_grads.sum(tuple(range(1, out_grads.ndim-1, 1)))
        return torch.matmul(out_grads.T, out_grads)

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        if in_data1.ndim > 2:
            in_data1 = in_data1.mean(tuple(range(1, in_data1.ndim-1)))
            in_data2 = in_data2.mean(tuple(range(1, in_data2.ndim-1)))
        return torch.matmul(in_data1, in_data2.T)  # n x n

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        if out_grads1.ndim > 2:
            out_grads1 = out_grads1.sum(tuple(range(1, out_grads1.ndim-1)))
            out_grads2 = out_grads2.sum(tuple(range(1, out_grads2.ndim-1)))
        return torch.matmul(out_grads1, out_grads2.T)  # n x n

File Path: dependencies/asdl/asdfghjkl/operations/operation.py
Content:
import torch
from math import sqrt
from ..utils import original_requires_grad, flatten_after_batch

# compute no-centered covariance
OP_COV_KRON = 'cov_kron'  # Kronecker-factored
OP_COV_DIAG = 'cov_diag'  # diagonal
OP_COV_UNIT_WISE = 'cov_unit_wise'  # unit-wise

# compute Gram matrix
OP_GRAM_DIRECT = 'gram_direct'  # direct
OP_GRAM_HADAMARD = 'gram_hada'  # Hadamard-factored

OP_BATCH_GRADS = 'batch_grads'  # compute batched gradients (per-example gradients)
# OP_BATCH_GRADS_AUG = 'batch_grads_aug'  # compute batched gradients with augmentation KFAC
OP_BATCH_GRADS_KRON = 'batch_grads_kron'  # compute batched gradients with Kronecker approx
OP_ACCUMULATE_GRADS = 'acc_grad'  # accumulate gradients


class Operation:
    def __init__(self, module, model, op_names, save_field='op_results'):
        self._module = module
        self._model = model
        if isinstance(op_names, str):
            op_names = [op_names]
        # remove duplicates
        op_names = set(op_names)
        self._op_names = op_names
        self._save_field = save_field
        self._grads_scale = None
        self._kron_A = None

    def get_op_results(self):
        return getattr(self._module, self._save_field, {})

    def set_op_results(self, op_results):
        setattr(self._module, self._save_field, op_results)

    def clear_op_results(self):
        if hasattr(self._module, self._save_field):
            delattr(self._module, self._save_field)

    @property
    def grads_scale(self):
        return self._grads_scale

    @grads_scale.setter
    def grads_scale(self, value):
        self._grads_scale = value

    def forward_post_process(self, in_data: torch.Tensor):
        module = self._module

        if OP_COV_KRON in self._op_names:
            if original_requires_grad(module, 'bias'):
                # Extend in_data with ones.
                # linear: n x f_in
                #      -> n x (f_in + 1)
                # conv2d: n x (c_in)(kernel_size) x out_size
                #      -> n x {(c_in)(kernel_size) + 1} x out_size
                if isinstance(module, torch.nn.Linear):
                    expected_dim = 2
                elif isinstance(module, (torch.nn.Conv2d, torch.nn.Conv1d)):
                    expected_dim = 3
                else:
                    raise ValueError('Fix handling bias for', type(module))

                shape = list(in_data.shape)
                const_dim = in_data.ndim - expected_dim + 1
                shape[const_dim] = 1
                ones = in_data.new_ones(shape)
                in_data = torch.cat((in_data, ones), dim=const_dim)

            op_results = self.get_op_results()

            if OP_COV_KRON in self._op_names:
                A = self.cov_kron_A(module, in_data)
                self._kron_A = A
                op_results[OP_COV_KRON] = {'A': A}

            self.set_op_results(op_results)

        if OP_GRAM_HADAMARD in self._op_names:
            op_results = self.get_op_results()

            n_data = in_data.shape[0]
            n1 = self._model.kernel.shape[0]
            weight_scale = getattr(module, 'weight_scale', 1.0)
            if n_data == n1:
                A = weight_scale * self.gram_A(module, in_data, in_data)
            else:
                A = weight_scale * self.gram_A(module, in_data[:n1], in_data[n1:])
            if original_requires_grad(module, 'bias'):
                bias_scale = getattr(module, 'bias_scale', 1.0)
                A += bias_scale
            op_results[OP_GRAM_HADAMARD] = {'A': A}

            self.set_op_results(op_results)


    def backward_pre_process(self, in_data, out_grads):
        if self._grads_scale is not None:
            shape = (-1, ) + (1, ) * (out_grads.ndim - 1)
            out_grads = torch.mul(out_grads, self._grads_scale.reshape(shape))

        module = self._module
        op_results = self.get_op_results()
        for op_name in self._op_names:
            if op_name == OP_COV_KRON:
                rst = self.cov_kron_B(module, out_grads)
                if op_name in op_results:
                    op_results[op_name]['B'] = rst
                else:
                    assert self._kron_A is not None
                    op_results[op_name] = {'A': self._kron_A, 'B': rst}

            elif op_name == OP_COV_UNIT_WISE:
                assert original_requires_grad(module, 'weight')
                assert original_requires_grad(module, 'bias')
                op_results[op_name] = self.cov_unit_wise(module, in_data, out_grads)

            elif op_name == OP_GRAM_HADAMARD:
                n_data = in_data.shape[0]
                n1 = self._model.kernel.shape[0]
                if n_data == n1:
                    B = self.gram_B(module, out_grads, out_grads)
                else:
                    B = self.gram_B(module, out_grads[:n1], out_grads[n1:])
                A = op_results[OP_GRAM_HADAMARD]['A']
                self._model.kernel += B.mul(A)

            elif op_name == OP_GRAM_DIRECT:
                n_data = in_data.shape[0]
                n1 = self._model.kernel.shape[0]

                weight_scale = getattr(module, 'weight_scale', 1.0) ** 0.5
                grads = weight_scale * self.batch_grads_weight(module, in_data, out_grads)
                v = [grads]
                if original_requires_grad(module, 'bias'):
                    bias_scale = getattr(module, 'bias_scale', 1.0) ** 0.5
                    grads_b = bias_scale * self.batch_grads_bias(module, out_grads)
                    v.append(grads_b)
                g = torch.cat([flatten_after_batch(_v) for _v in v], axis=1)

                precond = getattr(module, 'gram_precond', None)
                if precond is not None:
                    precond.precondition_vector_module(v, module)
                    g2 = torch.cat([flatten_after_batch(_v) for _v in v], axis=1)
                else:
                    g2 = g

                if n_data == n1:
                    self._model.kernel += torch.matmul(g, g2.T)
                else:
                    self._model.kernel += torch.matmul(g[:n1], g2[n1:].T)
            else:
                rst = getattr(self,
                              f'{op_name}_weight')(module, in_data, out_grads)
                op_results[op_name] = {'weight': rst}
                if original_requires_grad(module, 'bias'):
                    rst = getattr(self, f'{op_name}_bias')(module, out_grads)
                    op_results[op_name]['bias'] = rst

        self.set_op_results(op_results)

    @staticmethod
    def batch_grads_weight(module, in_data, out_grads):
        raise NotImplementedError

    @staticmethod
    def batch_grads_bias(module, out_grads):
        raise NotImplementedError

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        raise NotImplementedError

    @staticmethod
    def cov_diag_bias(module, out_grads):
        raise NotImplementedError

    @staticmethod
    def cov_kron_A(module, in_data):
        raise NotImplementedError

    @staticmethod
    def cov_kron_B(module, out_grads):
        raise NotImplementedError

    @staticmethod
    def cov_unit_wise(module, in_data, out_grads):
        raise NotImplementedError

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        raise NotImplementedError

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        raise NotImplementedError

File Path: dependencies/asdl/asdfghjkl/operations/scale.py
Content:
import torch
from torch import nn

from .operation import Operation


class Scale(nn.Module):
    def __init__(self):
        super(Scale, self).__init__()
        self.weight = nn.Parameter(torch.ones(1))
        
    def reset_parameters(self):
        nn.init.constant_(self.weight, 1)
        
    def forward(self, input):
        return self.weight * input


class ScaleExt(Operation):
    """
    module.weight: 1

    Argument shapes
    in_data: n x f_in
    out_grads: n x f_out = f_in
    """
    @staticmethod
    def batch_grads_weight(module, in_data, out_grads):
        N = out_grads.size(0)
        return (out_grads * in_data).view(N, -1).sum(dim=1)

    @staticmethod
    def batch_grads_kron_weight(module, in_data, out_grads):
        N = out_grads.size(0)
        return (out_grads * in_data).view(N, -1).sum(dim=1)

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        N = out_grads.size(0)
        return (out_grads * in_data).view(N, -1).sum(dim=1).square().sum()

    @staticmethod
    def cov_kron_A(module, in_data):
        setattr(module, 'n_in_data', in_data)
        return torch.ones(1, 1, device=in_data.device) 

    @staticmethod
    def cov_kron_B(module, out_grads):
        N = out_grads.size(0)
        return (module.n_in_data * out_grads).view(N, -1).sum(dim=1).square().sum().view(1, 1)

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        setattr(module, 'n_in_data1', in_data1)
        setattr(module, 'n_in_data2', in_data2)
        N1, N2 = in_data1.size(0), in_data2.size(0)
        return in_data1.new_ones(N1, N2)  # n x n

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        N1, N2 = out_grads1.size(0), out_grads2.size(0)
        grad1 = (module.n_in_data1 * out_grads1).view(N1, -1).sum(dim=1)
        grad2 = (module.n_in_data2 * out_grads2).view(N2, -1).sum(dim=1)
        return torch.outer(grad1, grad2)

File Path: dependencies/asdl/asdfghjkl/precondition.py
Content:
import torch
from torch import nn

from .matrices import FISHER_EXACT, SHAPE_FULL, SHAPE_BLOCK_DIAG, SHAPE_KRON, SHAPE_DIAG  # NOQA
from .fisher import fisher_for_cross_entropy
from .utils import add_value_to_diagonal

_supported_modules = (nn.Linear, nn.Conv2d, nn.BatchNorm1d, nn.BatchNorm2d)
_normalizations = (nn.BatchNorm1d, nn.BatchNorm2d)

__all__ = [
    'Precondition', 'NaturalGradient', 'LayerWiseNaturalGradient', 'KFAC',
    'DiagNaturalGradient'
]


class Precondition:
    def __init__(self):
        pass

    def update_curvature(self, inputs=None, targets=None, data_loader=None):
        raise NotImplementedError

    def accumulate_curvature(self):
        raise NotImplementedError

    def finalize_accumulation(self):
        raise NotImplementedError

    def reduce_curvature(self):
        raise NotImplementedError

    def update_inv(self, damping=None):
        raise NotImplementedError

    def precondition(self):
        raise NotImplementedError

    def precondition_vector(self, vec):
        raise NotImplementedError


class NaturalGradient(Precondition):
    def __init__(self,
                 model,
                 fisher_type=FISHER_EXACT,
                 pre_inv_postfix=None,
                 n_mc_samples=1,
                 damping=1e-5,
                 ):
        from torch.nn.parallel import DistributedDataParallel as DDP
        assert not isinstance(model, DDP), f'{DDP} is not supported.'
        del DDP
        self.model = model
        self.modules = [model]
        self.fisher_type = fisher_type
        self.n_mc_samples = n_mc_samples
        self.damping = damping
        super().__init__()
        self.fisher_shape = SHAPE_FULL
        self.fisher_manager = None
        self._pre_inv_postfix = pre_inv_postfix

    def _get_fisher_attr(self, postfix=None):
        if postfix is None:
            return self.fisher_type
        else:
            return f'{self.fisher_type}_{postfix}'

    def _get_fisher(self, module, postfix=None):
        attr = self._get_fisher_attr(postfix)
        fisher = getattr(module, attr, None)
        return fisher

    @property
    def _pre_inv_attr(self):
        return self._get_fisher_attr(self._pre_inv_postfix)

    def _get_pre_inv_fisher(self, module):
        return getattr(module, self._pre_inv_attr, None)

    def _set_fisher(self, module, data, postfix=None):
        attr = self._get_fisher_attr(postfix)
        setattr(module, attr, data)

    def _clear_fisher(self, module, postfix=None):
        attr = self._get_fisher_attr(postfix)
        if hasattr(module, attr):
            delattr(module, attr)

    def update_curvature(self, inputs=None, targets=None, data_loader=None):
        rst = fisher_for_cross_entropy(self.model,
                                       inputs=inputs,
                                       targets=targets,
                                       data_loader=data_loader,
                                       fisher_types=self.fisher_type,
                                       fisher_shapes=self.fisher_shape,
                                       n_mc_samples=self.n_mc_samples)
        self.fisher_manager = rst

    def move_curvature(self, postfix, scale=1., to_pre_inv=False):
        self.accumulate_curvature(postfix, scale, to_pre_inv, replace=True)

    def accumulate_curvature(self, postfix='acc', scale=1., to_pre_inv=False, replace=False):
        if to_pre_inv:
            postfix = self._pre_inv_postfix
        for module in self.modules:
            fisher = self._get_fisher(module)
            if fisher is None:
                continue
            fisher.scaling(scale)
            fisher_acc = self._get_fisher(module, postfix)
            if fisher_acc is None or replace:
                self._set_fisher(module, fisher, postfix)
            else:
                self._set_fisher(module, fisher_acc + fisher, postfix)
            self._clear_fisher(module)

    def finalize_accumulation(self, postfix='acc'):
        for module in self.modules:
            fisher_acc = self._get_fisher(module, postfix)
            assert fisher_acc is not None
            self._set_fisher(module, fisher_acc)
            self._clear_fisher(module, postfix)

    def reduce_curvature(self, all_reduce=True):
        self.fisher_manager.reduce_matrices(all_reduce=all_reduce)

    def update_inv(self, damping=None):
        if damping is None:
            damping = self.damping

        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            inv = _cholesky_inv(add_value_to_diagonal(fisher.data, damping))
            setattr(fisher, 'inv', inv)

    def precondition(self):
        grads = []
        for p in self.model.parameters():
            if p.requires_grad and p.grad is not None:
                grads.append(p.grad.flatten())
        g = torch.cat(grads)
        fisher = self._get_pre_inv_fisher(self.model)
        ng = torch.mv(fisher.inv, g)

        pointer = 0
        for p in self.model.parameters():
            if p.requires_grad and p.grad is not None:
                numel = p.grad.numel()
                val = ng[pointer:pointer + numel]
                p.grad.copy_(val.reshape_as(p.grad))
                pointer += numel

        assert pointer == ng.numel()


class LayerWiseNaturalGradient(NaturalGradient):
    def __init__(self,
                 model,
                 fisher_type=FISHER_EXACT,
                 pre_inv_postfix=None,
                 n_mc_samples=1,
                 damping=1e-5):
        super().__init__(model, fisher_type, pre_inv_postfix, n_mc_samples, damping)
        self.fisher_shape = SHAPE_BLOCK_DIAG
        self.modules = [
            m for m in model.modules() if isinstance(m, _supported_modules)
        ]

    def precondition(self):
        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            g = module.weight.grad.flatten()
            if _bias_requires_grad(module):
                g = torch.cat([g, module.bias.grad.flatten()])
            ng = torch.mv(fisher.inv, g)

            if _bias_requires_grad(module):
                w_numel = module.weight.numel()
                grad_w = ng[:w_numel]
                module.bias.grad.copy_(ng[w_numel:])
            else:
                grad_w = ng
            module.weight.grad.copy_(grad_w.reshape_as(module.weight.grad))


class KFAC(NaturalGradient):
    def __init__(self,
                 model,
                 fisher_type=FISHER_EXACT,
                 pre_inv_postfix=None,
                 n_mc_samples=1,
                 damping=1e-5):
        super().__init__(model, fisher_type, pre_inv_postfix, n_mc_samples, damping)
        self.fisher_shape = SHAPE_KRON
        self.modules = [
            m for m in model.modules() if isinstance(m, _supported_modules)
        ]

    def update_inv(self, damping=None):
        if damping is None:
            damping = self.damping

        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            if isinstance(module, _normalizations):
                unit = fisher.unit.data
                f = unit.shape[0]
                dmp = torch.eye(2, device=unit.device, dtype=unit.dtype).repeat(f, 1, 1) * damping
                inv = torch.inverse(fisher.unit.data + dmp)
                setattr(fisher.unit, 'inv', inv)
            else:
                A = fisher.kron.A
                B = fisher.kron.B
                A_eig_mean = A.trace() / A.shape[0]
                B_eig_mean = B.trace() / B.shape[0]
                pi = torch.sqrt(A_eig_mean / B_eig_mean)
                r = damping**0.5

                A_inv = _cholesky_inv(add_value_to_diagonal(A, r * pi))
                B_inv = _cholesky_inv(add_value_to_diagonal(B, r / pi))

                setattr(fisher.kron, 'A_inv', A_inv)
                setattr(fisher.kron, 'B_inv', B_inv)

    def precondition(self):
        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            if isinstance(module, _normalizations):
                inv = fisher.unit.inv  # (f, 2, 2)
                assert _bias_requires_grad(module)
                grad_w = module.weight.grad  # (f,)
                grad_b = module.bias.grad  # (f,)
                g = torch.stack([grad_w, grad_b], dim=1)  # (f, 2)
                g = g.unsqueeze(2)  # (f, 2, 1)
                ng = torch.matmul(inv, g).squeeze(2)  # (f, 2)
                module.weight.grad.copy_(ng[:, 0])
                module.bias.grad.copy_(ng[:, 1])
            else:
                A_inv = fisher.kron.A_inv
                B_inv = fisher.kron.B_inv
                grad2d = module.weight.grad.view(B_inv.shape[0], -1)
                if _bias_requires_grad(module):
                    grad2d = torch.cat(
                        [grad2d, module.bias.grad.unsqueeze(dim=1)], dim=1)
                ng = B_inv.mm(grad2d).mm(A_inv)
                if _bias_requires_grad(module):
                    grad_w = ng[:, :-1]
                    module.bias.grad.copy_(ng[:, -1])
                else:
                    grad_w = ng
                module.weight.grad.copy_(grad_w.reshape_as(module.weight.grad))

    def precondition_vector(self, vec):
        idx = 0
        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            if isinstance(module, _normalizations):
                inv = fisher.unit.inv  # (f, 2, 2)
                assert _bias_requires_grad(module)
                vec_w = vec[idx]  # (f,)
                vec_b = vec[idx + 1]  # (f,)
                v = torch.stack([vec_w, vec_b], dim=1)  # (f, 2)
                v = v.unsqueeze(2)  # (f, 2, 1)
                ng = torch.matmul(inv, v).squeeze(2)  # (f, 2)
                vec[idx].copy_(ng[:, 0])
                vec[idx + 1].copy_(ng[:, 1])
                idx += 2
            else:
                A_inv = fisher.kron.A_inv
                B_inv = fisher.kron.B_inv
                w_idx = idx
                vec2d = vec[w_idx].view(B_inv.shape[0], -1)
                idx += 1
                if _bias_requires_grad(module):
                    vec2d = torch.cat(
                        [vec2d, vec[idx].unsqueeze(dim=1)], dim=1)
                ng = B_inv.mm(vec2d).mm(A_inv)
                if _bias_requires_grad(module):
                    vec_w = ng[:, :-1]
                    vec[idx].copy_(ng[:, -1])
                    idx += 1
                else:
                    vec_w = ng
                vec[w_idx].copy_(vec_w.reshape_as(module.weight.data))

        assert idx == len(vec)


class DiagNaturalGradient(NaturalGradient):
    def __init__(self,
                 model,
                 fisher_type=FISHER_EXACT,
                 pre_inv_postfix=None,
                 n_mc_samples=1,
                 damping=1e-5):
        super().__init__(model, fisher_type, pre_inv_postfix, n_mc_samples, damping)
        self.fisher_shape = SHAPE_DIAG
        self.modules = [
            m for m in model.modules() if isinstance(m, _supported_modules)
        ]

    def update_inv(self, damping=None):
        if damping is None:
            damping = self.damping

        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            elif isinstance(module, _supported_modules):
                diag_w = fisher.diag.weight
                setattr(fisher.diag, 'weight_inv', 1 / (diag_w + damping))
                if _bias_requires_grad(module):
                    diag_b = fisher.diag.bias
                    setattr(fisher.diag, 'bias_inv', 1 / (diag_b + damping))

    def precondition(self):
        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            w_inv = fisher.diag.weight_inv
            module.weight.grad.mul_(w_inv)
            if _bias_requires_grad(module):
                b_inv = fisher.diag.bias_inv
                module.bias.grad.mul_(b_inv)

    def precondition_vector(self, vec):
        idx = 0
        for module in self.modules:
            fisher = self._get_pre_inv_fisher(module)
            if fisher is None:
                continue
            assert fisher.diag is not None, module
            vec[idx].mul_(fisher.diag.weight_inv)
            idx += 1
            if _bias_requires_grad(module):
                vec[idx].mul_(fisher.diag.bias_inv)
                idx += 1

        assert idx == len(vec)

    def precondition_vector_module(self, vec, module):
        fisher = self._get_pre_inv_fisher(module)
        assert fisher is not None
        assert fisher.diag is not None, module
        vec[0].mul_(fisher.diag.weight_inv)
        if _bias_requires_grad(module):
            vec[1].mul_(fisher.diag.bias_inv)


def _bias_requires_grad(module):
    return hasattr(module, 'bias') \
           and module.bias is not None \
           and module.bias.requires_grad


def _cholesky_inv(X):
    u = torch.cholesky(X)
    return torch.cholesky_inverse(u)


File Path: dependencies/asdl/asdfghjkl/symmatrix.py
Content:
import os
import numpy as np
import torch

__all__ = [
    'matrix_to_tril',
    'tril_to_matrix',
    'get_n_cols_by_tril',
    'SymMatrix',
    'Kron',
    'Diag',
    'UnitWise'
]


def matrix_to_tril(mat: torch.Tensor):
    """
    Convert matrix (2D array)
    to lower triangular of it (1D array, row direction)

    Example:
      [[1, x, x],
       [2, 3, x], -> [1, 2, 3, 4, 5, 6]
       [4, 5, 6]]
    """
    assert mat.ndim == 2
    tril_indices = torch.tril_indices(*mat.shape)
    return mat[tril_indices[0], tril_indices[1]]


def tril_to_matrix(tril: torch.Tensor):
    """
    Convert lower triangular of matrix (1D array)
    to full symmetric matrix (2D array)

    Example:
                            [[1, 2, 4],
      [1, 2, 3, 4, 5, 6] ->  [2, 3, 5],
                             [4, 5, 6]]
    """
    assert tril.ndim == 1
    n_cols = get_n_cols_by_tril(tril)
    rst = torch.zeros(n_cols, n_cols, device=tril.device, dtype=tril.dtype)
    tril_indices = torch.tril_indices(n_cols, n_cols)
    rst[tril_indices[0], tril_indices[1]] = tril
    rst = rst + rst.T - torch.diag(torch.diag(rst))
    return rst


def get_n_cols_by_tril(tril: torch.Tensor):
    """
    Get number of columns of original matrix
    by lower triangular (tril) of it.

    ncols^2 + ncols = 2 * tril.numel()
    """
    assert tril.ndim == 1
    numel = tril.numel()
    return int(np.sqrt(2 * numel + 0.25) - 0.5)


def _save_as_numpy(path, tensor):
    dirname = os.path.dirname(path)
    if not os.path.isdir(dirname):
        os.makedirs(dirname)
    np.save(path, tensor.cpu().numpy().astype('float32'))


def _load_from_numpy(path, device='cpu'):
    data = np.load(path)
    return torch.from_numpy(data).to(device)


class SymMatrix:
    def __init__(
        self, data=None, kron=None, diag=None, unit=None, device='cpu'
    ):
        self.data = data
        self.kron = kron
        self.diag = diag
        self.unit = unit
        self.device = device

    @property
    def has_data(self):
        return self.data is not None

    @property
    def has_kron(self):
        return self.kron is not None

    @property
    def has_diag(self):
        return self.diag is not None

    @property
    def has_unit(self):
        return self.unit is not None

    def __add__(self, other):
        data = kron = diag = unit = None
        if self.has_data:
            assert other.has_data
            data = self.data.add(other.data)
        if self.has_kron:
            assert other.has_kron
            kron = self.kron + other.kron
        if self.has_diag:
            assert other.has_diag
            diag = self.diag + other.diag
        if self.has_unit:
            assert other.has_unit
            unit = self.unit + other.unit
        return SymMatrix(data, kron, diag, unit, device=self.device)

    def scaling(self, scale):
        if self.has_data:
            self.data.mul_(scale)
        if self.has_kron:
            self.kron.scaling(scale)
        if self.has_diag:
            self.diag.scaling(scale)
        if self.has_unit:
            self.unit.scaling(scale)

    def eigenvalues(self):
        assert self.has_data
        eig, _ = torch.symeig(self.data)
        return torch.sort(eig, descending=True)[0]

    def top_eigenvalue(self):
        assert self.has_data
        eig, _ = torch.symeig(self.data)
        return eig.max().item()

    def trace(self):
        assert self.has_data
        return torch.diag(self.data).sum().item()

    def save(self, root, relative_dir):
        relative_paths = {}
        if self.has_data:
            tril = matrix_to_tril(self.data)
            relative_path = os.path.join(relative_dir, 'tril.npy')
            absolute_path = os.path.join(root, relative_path)
            _save_as_numpy(absolute_path, tril)
            relative_paths['tril'] = relative_path
        if self.has_kron:
            rst = self.kron.save(root, relative_dir)
            relative_paths['kron'] = rst
        if self.has_diag:
            rst = self.diag.save(root, relative_dir)
            relative_paths['diag'] = rst
        if self.has_unit:
            rst = self.unit.save(root, relative_dir)
            relative_paths['unit_wise'] = rst

        return relative_paths

    def load(self, path=None, kron_path=None, diag_path=None, unit_path=None):
        if path:
            tril = _load_from_numpy(path, self.device)
            self.data = tril_to_matrix(tril)
        if kron_path is not None:
            if not self.has_kron:
                self.kron = Kron(A=None, B=None, device=self.device)
            self.kron.load(
                A_path=kron_path['A_tril'], B_path=kron_path['B_tril']
            )
        if diag_path is not None:
            if not self.has_diag:
                self.diag = Diag(device=self.device)
            self.diag.load(
                w_path=diag_path.get('weight', None),
                b_path=diag_path.get('bias', None)
            )
        if unit_path is not None:
            if not self.has_unit:
                self.unit = UnitWise(device=self.device)
            self.unit.load(path=unit_path)

    def to_vector(self):
        vec = []
        if self.has_data:
            vec.append(self.data)
        if self.has_kron:
            vec.extend(self.kron.data)
        if self.has_diag:
            vec.extend(self.diag.data)
        if self.has_unit:
            vec.extend(self.unit.data)

        vec = [v.flatten() for v in vec]
        return vec

    def to_matrices(self, vec, pointer):
        def unflatten(mat, p):
            numel = mat.numel()
            mat.copy_(vec[p:p + numel].view_as(mat))
            p += numel
            return p

        if self.has_data:
            pointer = unflatten(self.data, pointer)
        if self.has_kron:
            pointer = self.kron.to_matrices(unflatten, pointer)
        if self.has_diag:
            pointer = self.diag.to_matrices(unflatten, pointer)
        if self.has_unit:
            pointer = self.unit.to_matrices(unflatten, pointer)

        return pointer


class Kron:
    def __init__(self, A, B, device='cpu'):
        self.A = A
        self.B = B
        self.device = device

    def __add__(self, other):
        return Kron(
            A=self.A.add(other.A), B=self.B.add(other.B), device=self.device
        )

    @property
    def data(self):
        return [self.A, self.B]

    def scaling(self, scale):
        self.A.mul_(scale)
        self.B.mul_(scale)

    def eigenvalues(self):
        eig_A, _ = torch.symeig(self.A)
        eig_B, _ = torch.symeig(self.B)
        eig = torch.ger(eig_A, eig_B).flatten()
        return torch.sort(eig, descending=True)[0]

    def top_eigenvalue(self):
        eig_A, _ = torch.symeig(self.A)
        eig_B, _ = torch.symeig(self.B)
        return (eig_A.max() * eig_B.max()).item()

    def trace(self):
        trace_A = torch.diag(self.A).sum().item()
        trace_B = torch.diag(self.B).sum().item()
        return trace_A * trace_B

    def save(self, root, relative_dir):
        relative_paths = {}
        for name in ['A', 'B']:
            mat = getattr(self, name, None)
            if mat is None:
                continue
            tril = matrix_to_tril(mat)
            tril_name = f'{name}_tril'
            relative_path = os.path.join(
                relative_dir, 'kron', f'{tril_name}.npy'
            )
            absolute_path = os.path.join(root, relative_path)
            _save_as_numpy(absolute_path, tril)
            relative_paths[tril_name] = relative_path

        return relative_paths

    def load(self, A_path, B_path):
        A_tril = _load_from_numpy(A_path, self.device)
        self.A = tril_to_matrix(A_tril)
        B_tril = _load_from_numpy(B_path, self.device)
        self.B = tril_to_matrix(B_tril)

    def to_matrices(self, unflatten, pointer):
        pointer = unflatten(self.A, pointer)
        pointer = unflatten(self.B, pointer)
        return pointer


class Diag:
    def __init__(self, weight=None, bias=None, device='cpu'):
        self.weight = weight
        self.bias = bias
        self.device = device

    def __add__(self, other):
        weight = bias = None
        if self.has_weight:
            assert other.has_weight
            weight = self.weight.add(other.weight)
        if self.has_bias:
            assert other.has_bias
            bias = self.bias.add(other.bias)
        return Diag(weight=weight, bias=bias, device=self.device)

    @property
    def data(self):
        return [d for d in [self.weight, self.bias] if d is not None]

    @property
    def has_weight(self):
        return self.weight is not None

    @property
    def has_bias(self):
        return self.bias is not None

    def scaling(self, scale):
        if self.has_weight:
            self.weight.mul_(scale)
        if self.has_bias:
            self.bias.mul_(scale)

    def eigenvalues(self):
        eig = []
        if self.has_weight:
            eig.append(self.weight.flatten())
        if self.has_bias:
            eig.append(self.bias.flatten())
        eig = torch.cat(eig)
        return torch.sort(eig, descending=True)[0]

    def top_eigenvalue(self):
        top = -1
        if self.has_weight:
            top = max(top, self.weight.max().item())
        if self.has_bias:
            top = max(top, self.bias.max().item())
        return top

    def trace(self):
        trace = 0
        if self.has_weight:
            trace += self.weight.sum().item()
        if self.has_bias:
            trace += self.bias.sum().item()
        return trace

    def save(self, root, relative_dir):
        relative_paths = {}
        for name in ['weight', 'bias']:
            mat = getattr(self, name, None)
            if mat is None:
                continue
            relative_path = os.path.join(relative_dir, 'diag', f'{name}.npy')
            absolute_path = os.path.join(root, relative_path)
            _save_as_numpy(absolute_path, mat)
            relative_paths[name] = relative_path

        return relative_paths

    def load(self, w_path=None, b_path=None):
        if w_path:
            self.weight = _load_from_numpy(w_path, self.device)
        if b_path:
            self.bias = _load_from_numpy(b_path, self.device)

    def to_matrices(self, unflatten, pointer):
        if self.has_weight:
            pointer = unflatten(self.weight, pointer)
        if self.has_bias:
            pointer = unflatten(self.bias, pointer)
        return pointer


class UnitWise:
    def __init__(self, data=None, device='cpu'):
        self.data = data
        self.device = device

    def __add__(self, other):
        data = None
        if self.has_data:
            assert other.has_data
            data = self.data.add(other.data)
        return UnitWise(data=data, device=self.device)

    @property
    def has_data(self):
        return self.data is not None

    def scaling(self, scale):
        if self.has_data:
            self.data.mul_(scale)

    def eigenvalues(self):
        assert self.has_data
        eig = [torch.symeig(block)[0] for block in self.data]
        eig = torch.cat(eig)
        return torch.sort(eig, descending=True)[0]

    def top_eigenvalue(self):
        top = max([torch.symeig(block)[0].max().item() for block in self.data])
        return top

    def trace(self):
        trace = sum([torch.trace(block).item() for block in self.data])
        return trace

    def save(self, root, relative_dir):
        relative_path = os.path.join(relative_dir, 'unit_wise.npy')
        absolute_path = os.path.join(root, relative_path)
        _save_as_numpy(absolute_path, self.data)
        return relative_path

    def load(self, path=None):
        if path:
            self.data = _load_from_numpy(path, self.device)

    def to_matrices(self, unflatten, pointer):
        if self.has_data:
            pointer = unflatten(self.data, pointer)
        return pointer

File Path: dependencies/asdl/asdfghjkl/utils.py
Content:
from contextlib import contextmanager

import torch
from torch import nn
from torch.nn import functional as F
from torch.cuda import nvtx

_REQUIRES_GRAD_ATTR = '_original_requires_grad'

__all__ = [
    'original_requires_grad',
    'record_original_requires_grad',
    'restore_original_requires_grad',
    'disable_param_grad',
    'im2col_2d',
    'add_value_to_diagonal',
    'nvtx_range'
]


def original_requires_grad(module, param_name):
    param = getattr(module, param_name, None)
    return param is not None and getattr(param, _REQUIRES_GRAD_ATTR)


def record_original_requires_grad(param):
    setattr(param, _REQUIRES_GRAD_ATTR, param.requires_grad)


def restore_original_requires_grad(param):
    param.requires_grad = getattr(
        param, _REQUIRES_GRAD_ATTR, param.requires_grad
    )


@contextmanager
def disable_param_grad(model):

    for param in model.parameters():
        record_original_requires_grad(param)
        param.requires_grad = False

    yield
    for param in model.parameters():
        restore_original_requires_grad(param)


def im2col_2d(x: torch.Tensor, conv2d: nn.Module):
    assert x.ndimension() == 4  # n x c x h_in x w_in
    assert isinstance(conv2d, (nn.Conv2d, nn.ConvTranspose2d))
    assert conv2d.dilation == (1, 1)

    ph, pw = conv2d.padding
    kh, kw = conv2d.kernel_size
    sy, sx = conv2d.stride
    if ph + pw > 0:
        x = F.pad(x, (pw, pw, ph, ph))
    x = x.unfold(2, kh, sy)  # n x c x h_out x w_in x kh
    x = x.unfold(3, kw, sx)  # n x c x h_out x w_out x kh x kw
    x = x.permute(0, 1, 4, 5, 2,
                  3).contiguous()  # n x c x kh x kw x h_out x w_out
    x = x.view(x.size(0),
               x.size(1) * x.size(2) * x.size(3),
               x.size(4) * x.size(5))  # n x c(kh)(kw) x (h_out)(w_out)
    return x


def im2col_2d_aug(x, conv2d):
    n, k_aug = x.shape[:2]
    x = im2col_2d(x.flatten(start_dim=0, end_dim=1), conv2d)
    return x.view(n, k_aug, *x.shape[1:])

    
def arr2col_1d(x: torch.Tensor, conv1d: nn.Module):
    assert x.ndimension() == 3  # n x c x w_in
    assert isinstance(conv1d, (nn.Conv1d, nn.ConvTranspose1d))
    assert conv1d.dilation == (1,)

    pw = conv1d.padding[0]
    kw = conv1d.kernel_size[0]
    sw = conv1d.stride[0]
    if pw > 0:
        x = F.pad(x, (pw,))
    x = x.unfold(2, kw, sw)  # n x c x w_out x kw
    x = x.permute(0, 1, 3, 2).contiguous()
    x = x.view(x.size(0), x.size(1) * x.size(2), x.size(3))  # n x c(kw) x w_out
    return x

    
def arr2col_1d_aug(x, conv1d):
    n, k_aug = x.shape[:2]
    x = arr2col_1d(x.flatten(start_dim=0, end_dim=1), conv1d)
    return x.view(n, k_aug, *x.shape[1:])


def add_value_to_diagonal(x: torch.Tensor, value):
    ndim = x.ndim
    assert ndim >= 2
    eye = torch.eye(x.shape[-1], device=x.device)
    if ndim > 2:
        shape = tuple(x.shape[:-2]) + (1, 1)
        eye = eye.repeat(*shape)
    return x.add_(eye, alpha=value)


@contextmanager
def nvtx_range(msg):
    try:
        nvtx.range_push(msg)
        yield
    finally:
        nvtx.range_pop()


def flatten_after_batch(tensor: torch.Tensor):
    if tensor.ndim == 1:
        return tensor.unsqueeze(-1)
    else:
        return tensor.flatten(start_dim=1)

File Path: dependencies/asdl/examples/eigen.py
Content:
import torch

from asdfghjkl import fisher_for_cross_entropy
from asdfghjkl import FISHER_MC, COV
from asdfghjkl import SHAPE_KRON, SHAPE_DIAG

import pytorch_utils as pu

# get device (CUDA or CPU)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# setup data loader (MNIST)
train_loader, test_loader = pu.get_data_loader(pu.DATASET_MNIST, batch_size=128)

# initialize model (SmallSimpleCNN)
model, _ = pu.get_model(arch_name='SimpleCNN', arch_kwargs={'width_factor': 1})
model = model.to(device)
n_params = sum(p.numel() for p in model.parameters())

print('===========================')
print(f'dataset: MNIST')
print(f'train set size: {len(train_loader.dataset)}')
print(f'network: SimpleCNN')
print(f'# params: {n_params}')
print(f'device: {device}')
print('===========================')

# evaluate matrices of various types and shapes using the entire training set
fisher_types = [FISHER_MC, COV]
fisher_shapes = [SHAPE_KRON, SHAPE_DIAG]
stats_name = 'full_batch'
print(f'Evaluating {fisher_types} of shape {fisher_shapes} ...')
matrix_manager = fisher_for_cross_entropy(model, fisher_types, fisher_shapes,
                                          data_loader=train_loader, stats_name=stats_name)
print(f'Done.')

# print eigenvalues summary
for ftype in fisher_types:
    for fshape in fisher_shapes:
        print('----------------')
        print(f'{fshape}.{ftype}')
        trace = matrix_manager.get_trace(ftype, fshape, stats_name)
        print(f'trace: {trace}')
        eigvals = matrix_manager.get_eigenvalues(ftype, fshape, stats_name)
        print(f'top-10 eigvals: {eigvals[:10].tolist()}')

File Path: dependencies/asdl/examples/line_search_train.py
Content:
import argparse
import yaml
import random
import json
import warnings
import copy

import torch
from torch import nn
from torch.nn import functional as F
import torch.backends.cudnn as cudnn
import torch.distributed as dist

import pytorch_utils as pu
from asdfghjkl.kernel import *
from asdfghjkl import FISHER_EXACT, SHAPE_FULL, SHAPE_BLOCK_DIAG
from asdfghjkl import fisher_free_for_cross_entropy, hessian_free
from asdfghjkl.precondition import NaturalGradient, LayerWiseNaturalGradient, KFAC, DiagNaturalGradient


# ignore warning from PIL/TiffImagePlugin.py
warnings.filterwarnings('ignore', message='.*(C|c)orrupt\sEXIF\sdata.*')

# yapf: disable
# Training settings
parser = argparse.ArgumentParser()
parser.add_argument('--dataset', type=str, default=pu.DATASET_MNIST,
                    choices=[
                        pu.DATASET_CIFAR10, pu.DATASET_CIFAR100,
                        pu.DATASET_FMNIST, pu.DATASET_MNIST,
                        pu.DATASET_SVHN, pu.DATASET_IMAGENET
                    ],
                    help='name of dataset')
parser.add_argument('--dataset_root', type=str, default='./data',
                    help='root of dataset')
parser.add_argument('--train_root', type=str, default=None,
                    help='root of ImageNet train')
parser.add_argument('--val_root', type=str, default=None,
                    help='root of ImageNet val')
parser.add_argument('--epochs', type=int, default=200,
                    help='maximum number of epochs to train')
parser.add_argument('--batch_size', type=int, default=64,
                    help='input batch size for training')
parser.add_argument('--test_batch_size', type=int, default=None,
                    help='input batch size for testing')
parser.add_argument('--kernel_batch_size', type=int, default=16)
parser.add_argument('--arch_file', type=str, default=None,
                    help='name of file which defines the architecture')
parser.add_argument('--arch', type=str,
                    help='name of the architecture')
parser.add_argument('--arch_args', type=json.loads, default={},
                    help='[JSON] arguments for the architecture')
parser.add_argument('--optim', type=str, default='SGD',
                    help='name of the optimizer')
parser.add_argument('--momentum', type=float, default=0.)
parser.add_argument('--damping', type=float, default=1e-2)
parser.add_argument('--condition_number', type=float, default=None)
parser.add_argument('--cg_tol', type=float, default=1e-3)
parser.add_argument('--interval', type=float, default=1e-5)
parser.add_argument('--max_lr', type=float, default=10)
# Options
parser.add_argument('--distributed_backend', type=str, default='nccl',
                    help='backend for distributed init_process_group')
parser.add_argument('--download', type=bool, default=False,
                    help='if True, downloads the dataset (CIFAR-10 or 100)')
parser.add_argument('--no_cuda', action='store_true',
                    help='disables CUDA training')
parser.add_argument('--seed', type=int, default=None,
                    help='random seed')
parser.add_argument('--n_workers', type=int, default=4,
                    help='number of sub processes for data loading')
parser.add_argument('--config', default=None, nargs='+',
                    help='config YAML file path')
parser.add_argument('--run_id', type=str, default=None,
                    help='If None, wandb.run.id will be used.')
parser.add_argument('--turn_off_wandb', action='store_true',
                    help='If True, no information will be sent to W&B.')
# yapf: enable


def topk_correct(output, target, topk=(1, )):
    with torch.no_grad():
        maxk = max(topk)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].view(-1).sum(0, keepdim=True)
            res.append(correct_k.item())
        return res


def evaluate(data_loader):
    data_size = len(data_loader.dataset)
    total_loss = 0
    correct_1 = correct_5 = 0
    loss_fn = nn.CrossEntropyLoss(reduction='sum')
    loss_fn = loss_fn.to(device)

    model.eval()
    with torch.no_grad():
        for inputs, targets in data_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            total_loss += loss_fn(outputs, targets).item()
            corrects = topk_correct(outputs, targets, (1, 5))
            correct_1 += corrects[0]
            correct_5 += corrects[1]

    if is_distributed:
        # pack
        packed_tensor = torch.tensor([total_loss, correct_1,
                                      correct_5]).to(device)
        # all-reduce
        dist.all_reduce(packed_tensor)
        # unpack
        total_loss = packed_tensor[0].item()
        correct_1 = packed_tensor[1].item()
        correct_5 = packed_tensor[2].item()

    loss = total_loss / data_size
    accuracy1 = correct_1 / data_size
    accuracy5 = correct_5 / data_size

    return accuracy1, accuracy5, loss


def reduce_gradient():
    packed_tensor = torch.cat([p.grad.flatten() for p in model.parameters() if p.requires_grad])
    dist.all_reduce(packed_tensor)
    packed_tensor.div_(world_size)
    pointer = 0
    for p in model.parameters():
        if not p.requires_grad:
            continue
        numel = p.numel()
        p.grad.copy_(packed_tensor[pointer: pointer + numel].view_as(p.grad))
        pointer += numel


def gradient(inputs, targets):
    model.zero_grad()
    loss = F.cross_entropy(model(inputs), targets)
    loss.backward()

    if is_distributed:
        reduce_gradient()


def second_order_gradient(inputs, targets, damping=1e-5):
    gradient(inputs, targets)
    grads = []
    for p in model.parameters():
        if p.requires_grad:
            grads.append(p.grad.clone().detach().requires_grad_(False))

    loss_fn = nn.CrossEntropyLoss()
    loss_fn = loss_fn.to(device)
    sg = hessian_free(model,
                      loss_fn,
                      grads,
                      inputs=inputs,
                      targets=targets,
                      damping=damping,
                      max_iters=None,
                      is_distributed=is_distributed,
                      tol=args.cg_tol,
                      print_progress=False)

    i = 0
    for p in model.parameters():
        if p.requires_grad:
            p.grad.copy_(sg[i])
            i += 1


def natural_gradient_by_precondition(inputs, targets, precond_class, fisher_type=FISHER_EXACT, damping=1e-5):
    precond = precond_class(model, fisher_type=fisher_type, damping=damping)
    precond.update_curvature(inputs, targets)

    if is_distributed:
        # reduce curvature
        precond.reduce_curvature(all_reduce=True)

    precond.update_inv()
    gradient(inputs, targets)
    precond.precondition()


def natural_gradient_by_fisher_free(inputs, targets, fisher_type=FISHER_EXACT, fisher_shape=SHAPE_FULL, precond_class=None, damping=1e-5):
    if precond_class is not None:
        precond = precond_class(model, fisher_type=fisher_type, damping=damping)
        precond.update_curvature(inputs, targets)
        if is_distributed:
            # reduce curvature
            precond.reduce_curvature(all_reduce=True)
        precond.update_inv()
    else:
        precond = None

    gradient(inputs, targets)
    grads = []
    for p in model.parameters():
        if p.requires_grad:
            grads.append(p.grad.clone().detach().requires_grad_(False))

    ng = fisher_free_for_cross_entropy(model,
                                       grads,
                                       fisher_type=fisher_type,
                                       fisher_shape=fisher_shape,
                                       inputs=inputs,
                                       targets=targets,
                                       damping=damping,
                                       max_iters=None,
                                       is_distributed=is_distributed,
                                       tol=args.cg_tol,
                                       preconditioner=precond,
                                       print_progress=False)

    i = 0
    for p in model.parameters():
        if p.requires_grad:
            p.grad.copy_(ng[i])
            i += 1


def natural_gradient_by_kernel_free(inputs, targets, damping=1e-5):
    kernel_free_cross_entropy(model,
                              inputs,
                              targets,
                              damping=damping,
                              is_distributed=is_distributed,
                              tol=args.cg_tol,
                              print_progress=False)


def natural_gradient_by_kernel(inputs, targets, data_loader, kernel_fn, damping=1e-5, efficient=False):
    if is_distributed:
        kernel = batch(kernel_fn, model, data_loader, is_distributed=True, store_on_device=False, gather_type='split')
        parallel_efficient_natural_gradient_cross_entropy(model, inputs, targets, kernel, damping=damping)
    else:
        kernel = batch(kernel_fn, model, data_loader, store_on_device=False)
        if efficient:
            assert kernel.ndim == 3
            kernel = kernel.permute(2, 0, 1)  # c x n x n
            efficient_natural_gradient_cross_entropy(model, inputs, targets, kernel, damping=damping)
        else:
            natural_gradient_cross_entropy(model, inputs, targets, kernel, damping=damping)


def get_mini_batch_gradient(inputs, targets, grad_fn, **grad_kwargs):
    grad_fn(inputs, targets, **grad_kwargs)

    grads = []
    for p in model.parameters():
        if p.requires_grad:
            grads.append(p.grad.clone().detach().requires_grad_(False))
        else:
            grads.append(torch.zeros_like(p))

    return grads


def line_search(vectors):
    # keep init params
    init_params = [p.clone().detach().requires_grad_(False) for p in model.parameters()]

    interval = args.interval
    max_lr = args.max_lr

    lr = interval
    last_acc = None
    while True:
        for i, param in enumerate(model.parameters()):
            param.data.copy_(init_params[i].sub(vectors[i].mul(lr)))
        new_acc = evaluate(test_loader)[0]
        if last_acc is not None:
            improvement = new_acc - last_acc
            if improvement / last_acc < -5e-2:
                break
            elif abs(improvement / last_acc) < 5e-2:
                interval *= 2
        last_acc = new_acc
        lr += interval
        if lr > max_lr:
            break

    # restore best params
    best_lr = lr - interval
    for i, param in enumerate(model.parameters()):
        param.data.copy_(init_params[i].sub(vectors[i].mul(best_lr)))

    return best_lr, last_acc


def train(epoch, iteration, **grad_kwargs):
    momentum = args.momentum
    global grads_momentum

    if is_distributed:
        # deterministically shuffle based on epoch
        train_loader.sampler.set_epoch(epoch)

    for batch_id, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(device), targets.to(device)

        model.train()

        eigvals = kernel_eigenvalues(model,
                                     inputs,
                                     top_n=1,
                                     cross_entropy=True,
                                     eigenvectors=False,
                                     is_distributed=is_distributed,
                                     print_progress=False)
        top_eig = eigvals[0] / args.batch_size
        if args.condition_number:
            args.damping = top_eig / args.condition_number
            if args.optim != 'sgd':
                grad_kwargs['damping'] = args.damping

        grads = get_mini_batch_gradient(inputs, targets, **grad_kwargs)

        if grads_momentum is None or momentum == 0.:
            grads_momentum = copy.deepcopy(grads)
        else:
            for i in range(len(grads)):
                grads_momentum[i].mul_(momentum).add_(grads[i])

        lr, test_acc_1 = line_search(grads_momentum)

        log = {
            'epoch': epoch,
            'iteration': iteration,
            'learning_rate': lr,
            'test_acc@1': test_acc_1,
            'fisher_top_eig': top_eig,
            'damping': args.damping
        }

        if is_master:
            logger.print_report(log)
            logger.log(log)

        iteration += 1


def main():
    if is_master:
        logger.print_header()

    base_kwargs = {
        'sgd': {
            'grad_fn': gradient,
        },
        'newton': {
            'grad_fn': second_order_gradient,
        },
        'ngd': {
            'grad_fn': natural_gradient_by_precondition,
            'precond_class': NaturalGradient,
            'fisher_type': FISHER_EXACT,
        },
        'ngd_ntk': {
            'grad_fn': natural_gradient_by_kernel,
            'kernel_fn': empirical_direct_ntk,
        },
        'ngd_cg': {
            'grad_fn': natural_gradient_by_fisher_free,
            'fisher_type': FISHER_EXACT,
            'fisher_shape': SHAPE_FULL,
            'precond_class': None,
        },
        'ngd_ntk_cg': {
            'grad_fn': natural_gradient_by_kernel_free,
        },
        'lw_ngd': {
            'grad_fn': natural_gradient_by_precondition,
            'precond_class': LayerWiseNaturalGradient,
            'fisher_type': FISHER_EXACT,
        },
        'lw_ngd_cg': {
            'grad_fn': natural_gradient_by_fisher_free,
            'fisher_type': FISHER_EXACT,
            'fisher_shape': SHAPE_BLOCK_DIAG,
            'precond_class': KFAC,
        },
        'kfac': {
            'grad_fn': natural_gradient_by_precondition,
            'precond_class': KFAC,
            'fisher_type': FISHER_EXACT,
        },
        'diag_ngd': {
            'grad_fn': natural_gradient_by_precondition,
            'precond_class': DiagNaturalGradient,
            'fisher_type': FISHER_EXACT,
        },
        'cw_ngd': {
            'grad_fn': natural_gradient_by_kernel,
            'kernel_fn': empirical_class_wise_direct_ntk,
            'efficient': False,
        },
        'cw_ngd_eff': {
            'grad_fn': natural_gradient_by_kernel,
            'kernel_fn': empirical_class_wise_direct_ntk,
            'efficient': True,
        },
    }

    start_epoch = 1
    last_iteration = 0
    n_iters_per_epoch = len(train_loader)
    grad_kwargs = base_kwargs[args.optim]
    if args.optim != 'sgd':
        grad_kwargs['damping'] = args.damping
    for epoch in range(start_epoch, args.epochs + 1):
        iteration = last_iteration + 1
        train(epoch, iteration, **grad_kwargs)
        last_iteration = epoch * n_iters_per_epoch

        train_acc_1, train_acc_5, train_loss = evaluate(train_loader)
        test_acc_1, test_acc_5, test_loss = evaluate(test_loader)

        if is_master:
            log = {
                'epoch': epoch,
                'iteration': last_iteration,
                'train_acc@1': train_acc_1,
                'train_loss': train_loss,
                'test_acc@1': test_acc_1,
                'test_loss': test_loss,
            }
            logger.print_report(log)
            logger.log(log)


if __name__ == '__main__':
    args = parser.parse_args()
    dict_args = vars(args)

    # load config file (YAML)
    if args.config is not None:
        for path in args.config:
            with open(path) as f:
                config = yaml.full_load(f)
            dict_args.update(config)

    if args.seed is not None:
        # set random seed
        random.seed(args.seed)
        torch.manual_seed(args.seed)
        cudnn.deterministic = True  # NOTE: this will slow down training

    # [COMM] get MPI rank
    device, rank, world_size = pu.get_device_and_comm_rank(args.no_cuda,
                                                           args.distributed_backend)

    is_distributed = world_size > 1
    is_master = rank == 0

    train_loader, test_loader = pu.get_data_loader(
        args.dataset,
        args.batch_size,
        args.test_batch_size,
        dataset_root=args.dataset_root,
        train_root=args.train_root,
        val_root=args.val_root,
        download=args.download,
        n_workers=args.n_workers,
        world_size=world_size)

    # setup model
    arch_kwargs = args.arch_args
    model, args.arch_args = pu.get_model(args.arch, arch_kwargs,
                                         args.arch_file, args.dataset)
    model = model.to(device)

    # momentum of mini-batch gradients
    grads_momentum = None

    if args.optim not in ['ngd_ntk', 'cw_ngd', 'cw_ngd_eff']:
        dict_args.pop('kernel_batch_size')
    if args.optim not in ['ngd_cg', 'lw_ngd_cg', 'newton', 'ngd_ntk_cg']:
        dict_args.pop('cg_tol')

    if is_master:
        turn_off_wandb = dict_args.pop('turn_off_wandb')
        run_id = dict_args.pop('run_id')

        # initialize W&B as needed
        entries = [
            'epoch',
            'iteration',
            'train_loss',
            'test_loss',
            'train_acc@1',
            'test_acc@1',
            'learning_rate',
            'fisher_top_eig',
        ]
        if args.optim != 'sgd':
            entries.append('damping')
        logger = pu.Logger(args, entries=entries, turn_off_wandb=turn_off_wandb)
        if not is_distributed:
            # DDP disables hook functions that are required for
            # tracking torch graph
            logger.record_torch_graph(model)
        if run_id is None:
            run_id = logger.get_wandb_run_id()
        assert run_id is not None, \
            'run_id is not specified. Turn on W&B or specify --run_id.'

        print('================================')
        if is_distributed:
            print('Distributed training')
            print(f'world_size: {dist.get_world_size()}')
            print(f'backend: {dist.get_backend()}')
            print('---------------------------')
        for key, val in vars(args).items():
            if key == 'dataset':
                print(f'{key}: {val}')
                print(f'train set size: {len(train_loader.dataset)}')
                print(f'test set size: {len(test_loader.dataset)}')
            else:
                print(f'{key}: {val}')
        print(f'device: {device}')
        print('================================')
    else:
        logger = None

    main()

File Path: dependencies/asdl/setup.py
Content:
from setuptools import setup

with open('requirements.txt') as f:
    requirements = f.read().splitlines()

setup(
      name='asdfghjkl',
      version='0.0.1',
      description='ASDL: Automatic Second-order Differentiation (for Fisher, Gradient covariance, Hessian, Jacobian, and Kernel) Library',
      install_requires=requirements,
      python_requires='>=3.6'
      )

File Path: dependencies/asdl/tests/test_conjugate.py
Content:
import unittest

import torch
from torch import nn
import torch.nn.functional as F

from asdfghjkl import FISHER_EXACT, FISHER_MC, COV
from asdfghjkl import SHAPE_FULL, SHAPE_BLOCK_DIAG
from asdfghjkl import fisher_free_for_cross_entropy
from asdfghjkl import NaturalGradient, LayerWiseNaturalGradient


def _relative_error(v1: torch.tensor, v2: torch.tensor):
    err = v1 - v2
    return (err.norm() / v1.norm()).item()


def _cosine_similarity(vec1, vec2):
    inp = vec1.mul(vec2).sum()
    return (inp / vec1.norm() / vec2.norm()).item()


def net(n_dim, n_classes):
    model = nn.Sequential(
        nn.Linear(n_dim, n_dim),
        nn.ReLU(),
        nn.Linear(n_dim, n_dim),
        nn.ReLU(),
        nn.Linear(n_dim, n_classes),
    )
    return model


def convnet(n_dim, n_channels, n_classes, kernel_size=2):
    n_features = int(n_dim / (kernel_size ** 4)) ** 2 * n_channels
    model = nn.Sequential(
        nn.Conv2d(n_channels, n_channels, kernel_size, kernel_size),
        nn.MaxPool2d(kernel_size),
        nn.ReLU(),
        nn.Conv2d(n_channels, n_channels, kernel_size, kernel_size),
        nn.MaxPool2d(kernel_size),
        nn.ReLU(),
        nn.Flatten(),
        nn.Linear(n_features, n_classes),
    )
    return model


class TestCG(unittest.TestCase):

    def setUp(self):
        self.n_data = 40
        self.batch_size = 10
        self.n_classes = 10
        self.places = 3
        self.damping = 1e-2
        if torch.cuda.is_available():
            self.device = 'cuda'
        else:
            self.device = 'cpu'

    def test_linear(self):
        """
        Compare kernels for LinearNet.
        """
        n_dim = 32
        inputs = torch.randn(self.n_data, n_dim)
        model = net(n_dim, self.n_classes)
        self._test_fisher_free(model, inputs)

    def test_conv(self):
        """
        Compare kernels for ConvNet.
        """
        n_dim = 16
        n_channels = 3
        inputs = torch.randn(self.n_data, n_channels, n_dim, n_dim)
        model = convnet(n_dim, n_channels, self.n_classes, kernel_size=2)
        self._test_fisher_free(model, inputs)

    def _test_fisher_free(self, model, inputs):
        model = model.to(self.device)
        inputs = inputs.to(self.device)
        targets = torch.randint(self.n_classes, (self.n_data,), device=self.device)
        damping = self.damping

        def _get_ng_by_precondition(ng_fn, fisher_type):
            precond = ng_fn(model, fisher_type=fisher_type, damping=damping)
            precond.update_curvature(inputs, targets)

            model.zero_grad()
            loss = F.cross_entropy(model(inputs), targets)
            loss.backward()

            precond.update_inv()
            precond.precondition()

            grads = []
            for p in model.parameters():
                if p.requires_grad:
                    grads.append(p.grad.flatten())
            return torch.cat(grads)

        def _get_ng_by_fisher_free(fisher_type, fisher_shape, random_seed):
            model.zero_grad()
            loss = F.cross_entropy(model(inputs), targets)
            loss.backward()

            grad = [p.grad for p in model.parameters() if p.requires_grad]

            ng = fisher_free_for_cross_entropy(model,
                                               grad,
                                               fisher_type=fisher_type,
                                               fisher_shape=fisher_shape,
                                               inputs=inputs,
                                               targets=targets,
                                               damping=damping,
                                               max_iters=None,
                                               random_seed=random_seed,
                                               tol=1e-8)
            return torch.cat([_ng.flatten() for _ng in ng])

        def _test(ng_fn, fisher_type):
            seed = int(torch.rand(1) * 100)  # for MC sampling

            torch.manual_seed(seed)
            ng = _get_ng_by_precondition(ng_fn, fisher_type)

            if ng_fn == NaturalGradient:
                fisher_shape = SHAPE_FULL
            elif ng_fn == LayerWiseNaturalGradient:
                fisher_shape = SHAPE_BLOCK_DIAG
            else:
                return

            ng2 = _get_ng_by_fisher_free(fisher_type, fisher_shape, seed)

            self._assert_almost_equal(ng, ng2)

        for ng_class in [NaturalGradient, LayerWiseNaturalGradient]:
            for ftype in [FISHER_EXACT, FISHER_MC, COV]:
                _test(ng_class, ftype)

    def _assert_almost_equal(self, v1, v2):
        cos = _cosine_similarity(v1, v2)
        self.assertAlmostEqual(cos, 1, places=self.places)
        err = _relative_error(v1, v2)
        self.assertAlmostEqual(err, 0, places=self.places)


if __name__ == '__main__':
    unittest.main()

File Path: dependencies/asdl/tests/test_mvp.py
Content:
import time
import torch

bs = 32
n_layers = 20
d_in = 28 * 28
d_hid = 128
d_out = 10
seed = 0
device = 'cuda' if torch.cuda.is_available() else 'cpu'
loop = 100

print('bs', bs)
print('n_layers', n_layers)
print('d_in', d_in)
print('d_hid', d_hid)
print('d_out', d_out)
print('device', device)
print('-------------')

torch.random.manual_seed(seed)

modules = [torch.nn.Linear(d_in, d_hid), torch.nn.ReLU()]
for i in range(n_layers - 2):
    modules.extend([
        torch.nn.Linear(d_hid, d_hid),
        torch.nn.ReLU()
    ])
modules.append(torch.nn.Linear(d_hid, d_out, bias=False))
model = torch.nn.Sequential(*modules).to(device)
x = torch.randn(bs, d_in).to(device)
targets = torch.zeros(bs).long()

y = model(x)
loss = y.sum()


def ntk_vp(v):
    v.requires_grad_(True)
    vjp = torch.autograd.grad(y, list(model.parameters()), v, create_graph=True)
    return torch.autograd.grad(vjp, v, grad_outputs=vjp, retain_graph=True)[0]


dummy_v = torch.randn_like(y).requires_grad_(True)


def ggn_vp(v):
    for _v in v:
        _v.requires_grad_(True)
    vjp = torch.autograd.grad(y, list(model.parameters()), dummy_v, create_graph=True)
    jvp = torch.autograd.grad(vjp, dummy_v, grad_outputs=v)
    return torch.autograd.grad(y, list(model.parameters()), grad_outputs=jvp, retain_graph=True)


def hvp(v):
    grad = torch.autograd.grad(loss, list(model.parameters()), create_graph=True)
    return torch.autograd.grad(grad, list(model.parameters()), grad_outputs=v, retain_graph=True)


def timeit(func, v):
    func(v)  # dummy run
    start = time.time()
    for _ in range(loop):
        v = func(v)
    elapsed = time.time() - start
    f_name = func.__name__
    print(f'{f_name}: {elapsed / loop * 1000:.3f}ms')


v = torch.randn_like(y)
timeit(ntk_vp, v)

v = [torch.randn_like(p) for p in model.parameters()]
timeit(ggn_vp, v)

v = [torch.randn_like(p) for p in model.parameters()]
timeit(hvp, v)

File Path: dependencies/laplace/laplace/__init__.py
Content:
"""
.. include:: ../README.md

.. include:: ../regression_example.md
"""
REGRESSION = 'regression'
CLASSIFICATION = 'classification'

from laplace.baselaplace import (BaseLaplace, FullLaplace, KronLaplace, 
                                 DiagLaplace, FunctionalLaplace, BlockDiagLaplace)
from laplace.lllaplace import LLLaplace, FullLLLaplace, KronLLLaplace, DiagLLLaplace
from laplace.laplace import Laplace

__all__ = ['Laplace',  # direct access to all Laplace classes via unified interface
           'BaseLaplace',  # base-class
           'FullLaplace', 'KronLaplace', 'DiagLaplace',  # all-weights
           'LLLaplace',  # base-class last-layer
           'FullLLLaplace', 'KronLLLaplace', 'DiagLLLaplace', # last-layer
           'FunctionalLaplace']

File Path: dependencies/laplace/laplace/baselaplace.py
Content:
from abc import ABC, abstractmethod, abstractproperty
from math import sqrt, pi
import numpy as np
import torch
from torch.nn.utils import parameters_to_vector, vector_to_parameters
from torch.distributions import MultivariateNormal, Dirichlet, Normal
import torch.distributed as dist

from laplace.utils import (
    parameters_per_layer, invsqrt_precision, get_nll, validate,
    diagonal_add_scalar, batch_diagonal_add_scalar
)
from laplace.matrix import Kron

try:
    from laplace.curvature import BackPackGGN as DefaultBackend
except:
    from laplace.curvature import AsdlGGN as DefaultBackend


__all__ = ['BaseLaplace', 'FullLaplace', 'KronLaplace', 'DiagLaplace']


class BaseLaplace(ABC):
    """Baseclass for all Laplace approximations in this library.
    Subclasses need to specify how the Hessian approximation is initialized,
    how to add up curvature over training data, how to sample from the
    Laplace approximation, and how to compute the functional variance.

    A Laplace approximation is represented by a MAP which is given by the
    `model` parameter and a posterior precision or covariance specifying
    a Gaussian distribution \\(\\mathcal{N}(\\theta_{MAP}, P^{-1})\\).
    The goal of this class is to compute the posterior precision \\(P\\)
    which sums as
    \\[
        P = \\sum_{n=1}^N \\nabla^2_\\theta \\log p(\\mathcal{D}_n \\mid \\theta)
        \\vert_{\\theta_{MAP}} + \\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}}.
    \\]
    Every subclass implements different approximations to the log likelihood Hessians,
    for example, a diagonal one. The prior is assumed to be Gaussian and therefore we have
    a simple form for \\(\\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}} = P_0 \\).
    In particular, we assume a scalar, layer-wise, or diagonal prior precision so that in
    all cases \\(P_0 = \\textrm{diag}(p_0)\\) and the structure of \\(p_0\\) can be varied.

    Parameters
    ----------
    model : torch.nn.Module
    likelihood : {'classification', 'regression', 'heteroscedastic_regression'}
        determines the log likelihood Hessian approximation
    sigma_noise : torch.Tensor or float, default=1
        observation noise for the regression setting; must be 1 for classification
    prior_precision : torch.Tensor or float, default=1
        prior precision of a Gaussian prior (= weight decay);
        can be scalar, per-layer, or diagonal in the most general case
    prior_mean : torch.Tensor or float, default=0
        prior mean of a Gaussian prior, useful for continual learning
    temperature : float, default=1
        temperature of the likelihood; lower temperature leads to more
        concentrated posterior and vice versa.
    backend : subclasses of `laplace.curvature.CurvatureInterface`
        backend for access to curvature/Hessian approximations
    backend_kwargs : dict, default=None
        arguments passed to the backend on initialization, for example to
        set the number of MC samples for stochastic approximations.
    """
    def __init__(self, model, likelihood, sigma_noise=1., prior_precision=1., prior_mean=0.,
                 temperature=1., backend=DefaultBackend, backend_kwargs=None, sod=False,
                 single_output=False, single_output_iid=False):
        if likelihood not in ['classification', 'regression', 'heteroscedastic_regression']:
            raise ValueError(f'Invalid likelihood type {likelihood}')

        self.model = model
        self._device = next(model.parameters()).device
        # initialize state #
        # posterior mean/mode
        self.mean = parameters_to_vector(self.model.parameters()).detach()
        self.n_params = len(self.mean)
        self.n_layers = len(list(self.model.parameters()))
        self.prior_precision = prior_precision
        self.prior_mean = prior_mean
        if sigma_noise != 1 and likelihood != 'regression':
            raise ValueError('Sigma noise != 1 only available for regression.')
        self.likelihood = likelihood
        self.sigma_noise = sigma_noise
        self.temperature = temperature
        self._backend = None
        self._backend_cls = backend
        self._backend_kwargs = dict() if backend_kwargs is None else backend_kwargs
        self.H = None
        self.sod = sod
        self.single_output = single_output
        self.single_output_iid = single_output_iid

        # log likelihood = g(loss)
        self.loss = 0.
        self.n_outputs = None
        self.n_data = None

    @property
    def backend(self):
        if self._backend is None:
            self._backend = self._backend_cls(self.model, self.likelihood,
                                              **self._backend_kwargs)
        return self._backend

    @abstractmethod
    def _init_H(self):
        self.H = None
        self.loss = 0
        self.n_data_seen = 0

    @abstractmethod
    def _curv_closure(self, X, y, N):
        pass

    def _check_fit(self):
        if self.H is None:
            raise AttributeError('Laplace not fitted. Run fit() first.')

    def fit(self, train_loader, **kwargs):
        """Fit the local Laplace approximation at the parameters of the model.

        Parameters
        ----------
        train_loader : torch.data.utils.DataLoader
            each iterate is a training batch (X, y);
            `train_loader.dataset` needs to be set to access \\(N\\), size of the data set
        """
        self._init_H()

        self.model.eval()

        if getattr(self.model, 'output_size', None) is None:
            with torch.no_grad():
                X, _ = next(iter(train_loader))
                self.n_outputs = self.model(X[:1].to(self._device)).shape[-1]
            setattr(self.model, 'output_size', self.n_outputs)
        else:
            self.n_outputs = getattr(self.model, 'output_size')

        N = len(train_loader.dataset)
        for X, y in train_loader:
            self.model.zero_grad()
            X, y = X.to(self._device), y.to(self._device)
            loss_batch, H_batch = self._curv_closure(X, y, len(y) if self.sod else N)
            self.loss += loss_batch
            self.H += H_batch
            self.n_data_seen += len(y)

        self.n_data = N

    def fit_batch(self, x, y, N):
        self._init_H()
        self.n_data = N
        self.n_outputs = getattr(self.model, 'output_size')
        self.model.zero_grad()
        loss_batch, H_batch = self._curv_closure(x, y, len(y))
        self.loss += loss_batch
        self.H += H_batch
        self.n_data_seen += len(y)

    def fit_distributed(self, train_loader, n_steps_sod=1, **kwargs):
        """Fit the local Laplace approximation at the parameters of the model.

        Parameters
        ----------
        train_loader : torch.data.utils.DataLoader
            each iterate is a training batch (X, y);
            `train_loader.dataset` needs to be set to access \\(N\\), size of the data set
        n_steps_sod : int
            number of steps to take to tighten the bound in the distributed setting
        """
        self._init_H()
        self.model.eval()
        self.n_outputs = getattr(self.model, 'output_size')
        factor = dist.get_world_size()
        N = len(train_loader.dataset)
        i = 0
        for X, y in train_loader:
            self.model.zero_grad()
            X, y = X.to(self._device, non_blocking=True), y.to(self._device, non_blocking=True)
            M = factor * len(y) * n_steps_sod  # total samples
            loss_batch, H_batch = self._curv_closure(X, y, M if self.sod else N)
            self.loss += loss_batch.detach()
            self.H += H_batch
            self.n_data_seen += len(y)
            i += 1
            if self.sod and (i == n_steps_sod):
                break

        self.n_data = N
        self.all_reduce()

    def all_reduce(self):
        total = torch.tensor(
            [self.n_data_seen, self.loss.item()], dtype=torch.float32, device=self._device
        )
        dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)
        self.n_data_seen, self.loss = total.tolist()

    def log_marginal_likelihood(self, prior_precision=None, sigma_noise=None):
        """Compute the Laplace approximation to the log marginal likelihood subject
        to specific Hessian approximations that subclasses implement.
        Requires that the Laplace approximation has been fit before.
        The resulting torch.Tensor is differentiable in `prior_precision` and
        `sigma_noise` if these have gradients enabled.
        By passing `prior_precision` or `sigma_noise`, the current value is
        overwritten. This is useful for iterating on the log marginal likelihood.

        Parameters
        ----------
        prior_precision : torch.Tensor, optional
            prior precision if should be changed from current `prior_precision` value
        sigma_noise : [type], optional
            observation noise standard deviation if should be changed

        Returns
        -------
        log_marglik : torch.Tensor
        """
        # make sure we can differentiate wrt prior and sigma_noise for regression
        self._check_fit()

        # update prior precision (useful when iterating on marglik)
        if prior_precision is not None:
            self.prior_precision = prior_precision

        # update sigma_noise (useful when iterating on marglik)
        if sigma_noise is not None:
            if self.likelihood != 'regression':
                raise ValueError('Can only change sigma_noise for regression.')
            self.sigma_noise = sigma_noise

        return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)

    @property
    def log_likelihood(self):
        """Compute log likelihood on the training data after `.fit()` has been called.
        The log likelihood is computed on-demand based on the loss and, for example,
        the observation noise which makes it differentiable in the latter for
        iterative updates.

        Returns
        -------
        log_likelihood : torch.Tensor
        """
        self._check_fit()

        sod_factor = 1.0 if not self.sod else self.n_data / self.n_data_seen
        factor = - self._H_factor
        if self.likelihood == 'regression':
            # loss used is just MSE, need to add normalizer for gaussian likelihood
            c_factor = (self.n_data if not self.sod else self.n_data_seen) * self.n_outputs / self.temperature
            c = c_factor * torch.log(self.sigma_noise * sqrt(2 * pi))
            return sod_factor * (factor * self.loss - c)
        else:
            # for classification Xent == log Cat
            return sod_factor * factor * self.loss

    def __call__(self, x, pred_type='glm', link_approx='probit', n_samples=100, het_approx='natural'):
        """Compute the posterior predictive on input data `X`.

        Parameters
        ----------
        x : torch.Tensor
            `(batch_size, input_shape)`

        pred_type : {'glm', 'nn'}, default='glm'
            type of posterior predictive, linearized GLM predictive or neural
            network sampling predictive. The GLM predictive is consistent with
            the curvature approximations used here.

        link_approx : {'mc', 'probit', 'bridge', 'mcparam'}
            how to approximate the classification link function for the `'glm'`.
            For `pred_type='nn'`, only 'mc' is possible.

        n_samples : int
            number of samples for `link_approx='mc'`.

        het_approx : {'natural', 'mean', 'mc'}
            decides whether to linearize first natural parameter, mean, or sample.

        Returns
        -------
        predictive: torch.Tensor or Tuple[torch.Tensor]
            For `likelihood='classification'`, a torch.Tensor is returned with
            a distribution over classes (similar to a Softmax).
            For `likelihood='regression'`, a tuple of torch.Tensor is returned
            with the mean and the predictive variance.
        """
        self._check_fit()

        if pred_type not in ['glm', 'nn']:
            raise ValueError('Only glm and nn supported as prediction types.')

        if link_approx not in ['mc', 'probit', 'bridge', 'mcparam']:
            raise ValueError(f'Unsupported link approximation {link_approx}.')

        if pred_type == 'glm':
            # regression
            if self.likelihood == 'regression':
                return self._glm_predictive_distribution(x)
            elif self.likelihood == 'heteroscedastic_regression':
                if het_approx == 'natural':
                    return self._glm_het_natural_predictive_distribution(x)
                elif het_approx == 'mean':
                    return self._glm_het_mean_predictive_distribution(x)
                elif het_approx == 'mc':
                    # sample and estimate with mixture of gaussians
                    f_mu, f_var = self._glm_predictive_distribution(x)
                    try:
                        dist = MultivariateNormal(f_mu, f_var)
                    except:
                        dist = Normal(f_mu, torch.diagonal(f_var, dim1=1, dim2=2).sqrt())
                    samples = dist.sample((n_samples,))  # (n_samples, batch_size, n_outputs)
                    mu_samples = - samples[:, :, 0] / (2 * samples[:, :, 1])
                    var_samples = - 0.5 / samples[:, :, 1]
                    mu = mu_samples.mean(dim=0)
                    var = (mu_samples.square().mean(dim=0) + var_samples.mean(dim=0)) - mu.square()
                    return mu, var
                else:
                    raise ValueError('')
            # classification
            if link_approx == 'mcparam':
                Js, f_mu = self.backend.jacobians(x)
                samples = self.sample(n_samples)
                f_offset = f_mu - Js @ self.mean
                f_sample_onset = Js @ self.sample(n_samples).T
                f_samples = f_offset.unsqueeze(-1) + f_sample_onset
                return torch.softmax(f_samples, dim=1).mean(dim=-1)
            elif link_approx == 'mc':
                f_mu, f_var = self._glm_predictive_distribution(x)
                try:
                    dist = MultivariateNormal(f_mu, f_var)
                except:
                    dist = Normal(f_mu, torch.diagonal(f_var, dim1=1, dim2=2).sqrt())
                return torch.softmax(dist.sample((n_samples,)), dim=-1).mean(dim=0)
            elif link_approx == 'probit':
                f_mu, f_var = self._glm_predictive_distribution(x)
                kappa = 1 / torch.sqrt(1. + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))
                return torch.softmax(kappa * f_mu, dim=-1)
            elif link_approx == 'bridge':
                f_mu, f_var = self._glm_predictive_distribution(x)
                _, K = f_mu.size(0), f_mu.size(-1)
                f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)
                sum_exp = torch.sum(torch.exp(-f_mu), dim=1).unsqueeze(-1)
                alpha = 1/f_var_diag * (1 - 2/K + torch.exp(f_mu)/(K**2) * sum_exp)
                dist = Dirichlet(alpha)
                return torch.nan_to_num(dist.mean, nan=1.0)
        else:
            samples = self._nn_predictive_samples(x, n_samples)
            if self.likelihood == 'regression':
                return samples.mean(dim=0), samples.var(dim=0)
            return samples.mean(dim=0)

    def predictive(self, x, pred_type='glm', link_approx='mc', n_samples=100):
        return self(x, pred_type, link_approx, n_samples)

    def predictive_samples(self, x, pred_type='glm', n_samples=100):
        """Sample from the posterior predictive on input data `x`.
        Can be used, for example, for Thompson sampling.

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch_size, input_shape)`

        pred_type : {'glm', 'nn'}, default='glm'
            type of posterior predictive, linearized GLM predictive or neural
            network sampling predictive. The GLM predictive is consistent with
            the curvature approximations used here.

        n_samples : int
            number of samples

        Returns
        -------
        samples : torch.Tensor
            samples `(n_samples, batch_size, output_shape)`
        """
        self._check_fit()

        if pred_type not in ['glm', 'nn']:
            raise ValueError('Only glm and nn supported as prediction types.')

        if pred_type == 'glm':
            f_mu, f_var = self._glm_predictive_distribution(x)
            assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])
            try:
                dist = MultivariateNormal(f_mu, f_var)
            except:
                dist = Normal(f_mu, torch.diagonal(f_var, dim1=1, dim2=2).sqrt())
            samples = dist.sample((n_samples,))
            if self.likelihood == 'classification':
                return torch.softmax(samples, dim=-1)
            return samples

        else:  # 'nn'
            return self._nn_predictive_samples(x, n_samples)

    @torch.enable_grad()
    def _glm_predictive_distribution(self, X):
        Js, f_mu = self.backend.jacobians(X)
        f_var = self.functional_variance(Js)
        return f_mu.detach(), f_var.detach()

    @torch.enable_grad()
    def _glm_het_natural_predictive_distribution(self, X):
        Jseta1, f = self.backend.single_jacobians(X, 0)
        y_var = - 0.5 / f[:, 1]
        Jsmean = Jseta1 * y_var.unsqueeze(-1)
        f_var = self.functional_variance(Jsmean.unsqueeze(1)).squeeze()
        f_mean = f[:, 0] * y_var
        return f_mean.detach(), f_var.detach(), y_var.detach()

    @torch.enable_grad()
    def _glm_het_mean_predictive_distribution(self, X):
        Jsmean, f = self.backend.mean_jacobians(X)
        y_var = - 0.5 / f[:, 1]
        f_var = self.functional_variance(Jsmean.unsqueeze(1)).squeeze()
        f_mean = f[:, 0] * y_var
        return f_mean.detach(), f_var.detach(), y_var.detach()

    def _nn_predictive_samples(self, X, n_samples=100):
        fs = list()
        for sample in self.sample(n_samples):
            vector_to_parameters(sample, self.model.parameters())
            fs.append(self.model(X.to(self._device)).detach())
        vector_to_parameters(self.mean, self.model.parameters())
        fs = torch.stack(fs)
        if self.likelihood == 'classification':
            fs = torch.softmax(fs, dim=-1)
        return fs

    @abstractmethod
    def functional_variance(self, Jacs):
        """Compute functional variance for the `'glm'` predictive:
        `f_var[i] = Jacs[i] @ P.inv() @ Jacs[i].T`, which is a output x output
        predictive covariance matrix.
        Mathematically, we have for a single Jacobian
        \\(\\mathcal{J} = \\nabla_\\theta f(x;\\theta)\\vert_{\\theta_{MAP}}\\)
        the output covariance matrix
        \\( \\mathcal{J} P^{-1} \\mathcal{J}^T \\).

        Parameters
        ----------
        Jacs : torch.Tensor
            Jacobians of model output wrt parameters
            `(batch, outputs, parameters)`

        Returns
        -------
        f_var : torch.Tensor
            output covariance `(batch, outputs, outputs)`
        """
        pass

    def _check_jacobians(self, Js):
        if not isinstance(Js, torch.Tensor):
            raise ValueError('Jacobians have to be torch.Tensor.')
        if not Js.device == self._device:
            raise ValueError('Jacobians need to be on the same device as Laplace.')
        m, k, p = Js.size()
        if p != self.n_params:
            raise ValueError('Invalid Jacobians shape for Laplace posterior approx.')

    @abstractmethod
    def sample(self, n_samples=100):
        """Sample from the Laplace posterior approximation, i.e.,
        \\( \\theta \\sim \\mathcal{N}(\\theta_{MAP}, P^{-1})\\).

        Parameters
        ----------
        n_samples : int, default=100
            number of samples
        """
        pass

    @property
    def scatter(self):
        """Computes the _scatter_, a term of the log marginal likelihood that
        corresponds to L-2 regularization:
        `scatter` = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).

        Returns
        -------
        [type]
            [description]
        """
        delta = (self.mean - self.prior_mean)
        return (delta * self.prior_precision_diag) @ delta

    @property
    def log_det_prior_precision(self):
        """Compute log determinant of the prior precision
        \\(\\log \\det P_0\\)

        Returns
        -------
        log_det : torch.Tensor
        """
        return self.prior_precision_diag.log().sum()

    @abstractproperty
    def log_det_posterior_precision(self):
        """Compute log determinant of the posterior precision
        \\(\\log \\det P\\) which depends on the subclasses structure
        used for the Hessian approximation.

        Returns
        -------
        log_det : torch.Tensor
        """
        pass

    @property
    def log_det_ratio(self):
        """Compute the log determinant ratio, a part of the log marginal likelihood.
        \\[
            \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0
        \\]

        Returns
        -------
        log_det_ratio : torch.Tensor
        """
        sod_factor = 1.0 if not self.sod else self.n_data / self.n_data_seen
        if self.single_output:
            sod_factor *= self.n_outputs
        return sod_factor * (self.log_det_posterior_precision - self.log_det_prior_precision)

    @property
    def prior_structure(self):
        if len(self.prior_precision) == 1:
            return 'scalar'
        elif len(self.prior_precision) == self.n_params:
            return 'diagonal'
        elif len(self.prior_precision) == self.n_layers:
            return 'layerwise'

    @property
    def prior_precision_diag(self):
        """Obtain the diagonal prior precision \\(p_0\\) constructed from either
        a scalar, layer-wise, or diagonal prior precision.

        Returns
        -------
        prior_precision_diag : torch.Tensor
        """
        if len(self.prior_precision) == 1:  # scalar
            return self.prior_precision * torch.ones_like(self.mean, device=self.prior_precision.device)

        elif len(self.prior_precision) == self.n_params:  # diagonal
            return self.prior_precision

        elif len(self.prior_precision) == self.n_layers:  # per layer
            n_params_per_layer = parameters_per_layer(self.model)
            return torch.cat([prior * torch.ones(n_params, device=self.prior_precision.device) for prior, n_params
                              in zip(self.prior_precision, n_params_per_layer)])

        else:
            raise ValueError('Mismatch of prior and model. Diagonal, scalar, or per-layer prior.')

    @property
    def prior_mean(self):
        return self._prior_mean

    @prior_mean.setter
    def prior_mean(self, prior_mean):
        if np.isscalar(prior_mean) and np.isreal(prior_mean):
            self._prior_mean = torch.tensor(prior_mean, device=self._device)
        elif torch.is_tensor(prior_mean):
            if prior_mean.ndim == 0:
                self._prior_mean = prior_mean.reshape(-1).to(self._device)
            elif prior_mean.ndim == 1:
                if not len(prior_mean) in [1, self.n_params]:
                    raise ValueError('Invalid length of prior mean.')
                self._prior_mean = prior_mean
            else:
                raise ValueError('Prior mean has too many dimensions!')
        else:
            raise ValueError('Invalid argument type of prior mean.')

    @property
    def prior_precision(self):
        return self._prior_precision

    @prior_precision.setter
    def prior_precision(self, prior_precision):
        self._posterior_scale = None
        if np.isscalar(prior_precision) and np.isreal(prior_precision):
            self._prior_precision = torch.tensor([prior_precision], device=self._device)
        elif torch.is_tensor(prior_precision):
            if prior_precision.ndim == 0:
                # make dimensional
                self._prior_precision = prior_precision.reshape(-1).to(self._device)
            elif prior_precision.ndim == 1:
                if len(prior_precision) not in [1, self.n_layers, self.n_params]:
                    raise ValueError('Length of prior precision does not align with architecture.')
                self._prior_precision = prior_precision.to(self._device)
            else:
                raise ValueError('Prior precision needs to be at most one-dimensional tensor.')
        else:
            raise ValueError('Prior precision either scalar or torch.Tensor up to 1-dim.')

    def optimize_prior_precision(self, method='marglik', n_steps=100, lr=1e-1,
                                 init_prior_prec=1., val_loader=None, loss=get_nll,
                                 log_prior_prec_min=-4, log_prior_prec_max=4, grid_size=100,
                                 pred_type='glm', link_approx='probit', n_samples=100,
                                 verbose=False):
        """Optimize the prior precision post-hoc using the `method`
        specified by the user.

        Parameters
        ----------
        method : {'marglik', 'CV'}, default='marglik'
            specifies how the prior precision should be optimized.
        n_steps : int, default=100
            the number of gradient descent steps to take.
        lr : float, default=1e-1
            the learning rate to use for gradient descent.
        init_prior_prec : float, default=1.0
            initial prior precision before the first optimization step.
        val_loader : torch.data.utils.DataLoader, default=None
            DataLoader for the validation set; each iterate is a training batch (X, y).
        loss : callable, default=get_nll
            loss function to use for CV.
        log_prior_prec_min : float, default=-4
            lower bound of gridsearch interval for CV.
        log_prior_prec_max : float, default=4
            upper bound of gridsearch interval for CV.
        grid_size : int, default=100
            number of values to consider inside the gridsearch interval for CV.
        pred_type : {'glm', 'nn'}, default='glm'
            type of posterior predictive, linearized GLM predictive or neural
            network sampling predictive. The GLM predictive is consistent with
            the curvature approximations used here.
        link_approx : {'mc', 'probit', 'bridge'}, default='probit'
            how to approximate the classification link function for the `'glm'`.
            For `pred_type='nn'`, only `'mc'` is possible.
        n_samples : int, default=100
            number of samples for `link_approx='mc'`.
        verbose : bool, default=False
            if true, the optimized prior precision will be printed
            (can be a large tensor if the prior has a diagonal covariance).
        """
        if method == 'marglik':
            self.prior_precision = init_prior_prec
            log_prior_prec = self.prior_precision.log()
            log_prior_prec.requires_grad = True
            optimizer = torch.optim.Adam([log_prior_prec], lr=lr)
            for _ in range(n_steps):
                optimizer.zero_grad()
                prior_prec = log_prior_prec.exp()
                neg_log_marglik = -self.log_marginal_likelihood(prior_precision=prior_prec)
                neg_log_marglik.backward()
                optimizer.step()
            self.prior_precision = log_prior_prec.detach().exp()
        elif method == 'CV':
            if val_loader is None:
                raise ValueError('CV requires a validation set DataLoader')
            interval = torch.logspace(
                log_prior_prec_min, log_prior_prec_max, grid_size
            )
            self.prior_precision = self._gridsearch(
                loss, interval, val_loader, pred_type=pred_type,
                link_approx=link_approx, n_samples=n_samples
            )
        else:
            raise ValueError('For now only marglik and CV is implemented.')
        if verbose:
            print(f'Optimized prior precision is {self.prior_precision}.')

    def _gridsearch(self, loss, interval, val_loader, pred_type='glm',
                    link_approx='probit', n_samples=100):
        results = list()
        prior_precs = list()
        for prior_prec in interval:
            self.prior_precision = prior_prec
            try:
                out_dist, targets = validate(
                    self, val_loader, pred_type=pred_type,
                    link_approx=link_approx, n_samples=n_samples
                )
                result = loss(out_dist, targets)
            except RuntimeError:
                result = np.inf
            results.append(result)
            prior_precs.append(prior_prec)
        return prior_precs[np.argmin(results)]

    @property
    def sigma_noise(self):
        return self._sigma_noise

    @sigma_noise.setter
    def sigma_noise(self, sigma_noise):
        self._posterior_scale = None
        if np.isscalar(sigma_noise) and np.isreal(sigma_noise):
            self._sigma_noise = torch.tensor(sigma_noise, device=self._device)
        elif torch.is_tensor(sigma_noise):
            if sigma_noise.ndim == 0:
                self._sigma_noise = sigma_noise.to(self._device)
            elif sigma_noise.ndim == 1:
                if len(sigma_noise) > 1:
                    raise ValueError('Only homoscedastic output noise supported.')
                self._sigma_noise = sigma_noise[0].to(self._device)
            else:
                raise ValueError('Sigma noise needs to be scalar or 1-dimensional.')
        else:
            raise ValueError('Invalid type: sigma noise needs to be torch.Tensor or scalar.')

    @property
    def _H_factor(self):
        sigma2 = self.sigma_noise.square()
        return 1 / sigma2 / self.temperature

    @abstractproperty
    def posterior_precision(self):
        """Compute or return the posterior precision \\(P\\).

        Returns
        -------
        posterior_prec : torch.Tensor
        """
        pass


class FunctionalLaplace(BaseLaplace):

    def __init__(self, model, likelihood, sigma_noise=1, prior_precision=1, prior_mean=0,
                 temperature=1, backend=DefaultBackend, backend_kwargs=None,
                 independent=False, single_output=False, single_output_iid=False, sod=False):
        super().__init__(model, likelihood, sigma_noise=sigma_noise,
                         prior_precision=prior_precision, sod=sod,
                         prior_mean=prior_mean, temperature=temperature,
                         backend=backend, backend_kwargs=backend_kwargs,
                         single_output=single_output,
                         single_output_iid=single_output_iid)
        self.n_data_seen = 0
        self.H = None
        self.independent = independent
        if single_output and not independent:
            raise ValueError('Single output assumes independence for splitting.')

    def _kernel_closure(self, X, y):
        if self.independent:
            if self.single_output:
                if self.single_output_iid:
                    random_ix = torch.randint(self.n_outputs, (len(y),), device=X.device)
                else:
                    random_ix = torch.randint(self.n_outputs, ())
                return self.backend.single_kernel(X, y, self.prior_precision, output_ix=random_ix)
            else:
                return self.backend.indep_kernel(X, y, self.prior_precision)
        else:
            return self.backend.kernel(X, y, self.prior_precision_diag, prec=self.prior_precision,
                                       prec_structure=self.prior_structure)

    def _init_H(self):
        self.H = list()
        self.n_data_seen = 0
        self.loss = 0

    def fit(self, train_loader):
        self._init_H()
        for X, y in train_loader:
            self.fit_batch(X, y, len(train_loader.dataset))

    def fit_batch(self, X, y, N):
        self.n_data = N
        if self.H is None:
            self._init_H()

        if getattr(self.model, 'output_size', None) is None:
            with torch.no_grad():
                self.n_outputs = self.model(X[:1].to(self._device)).shape[-1]
            setattr(self.model, 'output_size', self.n_outputs)
        else:
            self.n_outputs = getattr(self.model, 'output_size')

        self.model.zero_grad()
        X, y = X.to(self._device), y.to(self._device)
        loss, H = self._kernel_closure(X, y)
        self.loss += loss
        self.n_data_seen += len(y)
        self.H.append(H)

    @property
    def log_det_ratio(self):
        log_det_ratio = 0
        for H_kernel in self.H:
            if self.independent:
                if self.single_output:
                    # H_kernel n x n
                    log_det_ratio += self.n_outputs * torch.logdet(
                        diagonal_add_scalar(self._H_factor * H_kernel, 1.0)
                    )
                else:
                    # H_kernel c x n x n
                    log_det_ratio += torch.logdet(
                        batch_diagonal_add_scalar(self._H_factor * H_kernel, 1.0)
                    ).sum()
            else:
                # H_kernel nc x nc
                log_det_ratio += torch.logdet(
                    diagonal_add_scalar(self._H_factor * H_kernel, 1.0)
                )
        return self.n_data / self.n_data_seen * log_det_ratio

    def _curv_closure(self, X, y, N):
        return super()._curv_closure(X, y, N)

    def functional_variance(self, Jacs):
        return super().functional_variance(Jacs)

    def log_det_posterior_precision(self):
        return super().log_det_posterior_precision

    def posterior_precision(self):
        return super().posterior_precision

    def sample(self, n_samples=100):
        return super().sample(n_samples=n_samples)


class FullLaplace(BaseLaplace):
    """Laplace approximation with full, i.e., dense, log likelihood Hessian approximation
    and hence posterior precision. Based on the chosen `backend` parameter, the full
    approximation can be, for example, a generalized Gauss-Newton matrix.
    Mathematically, we have \\(P \\in \\mathbb{R}^{P \\times P}\\).
    See `BaseLaplace` for the full interface.
    """
    # key to map to correct subclass of BaseLaplace, (subset of weights, Hessian structure)
    _key = ('all', 'full')

    def __init__(self, model, likelihood, sigma_noise=1., prior_precision=1.,
                 prior_mean=0., temperature=1., backend=DefaultBackend, backend_kwargs=None,
                 sod=False, single_output=False, single_output_iid=False):
        super().__init__(model, likelihood, sigma_noise, prior_precision, prior_mean,
                         temperature, backend, backend_kwargs, sod, single_output, single_output_iid)
        self._posterior_scale = None

    def _init_H(self):
        self.H = torch.zeros(self.n_params, self.n_params, device=self._device, dtype=self.mean.dtype)
        self.n_data_seen = 0
        self.loss = 0

    def _curv_closure(self, X, y, N):
        if self.single_output:
            if self.single_output_iid:
                random_ix = torch.randint(self.n_outputs, (len(y),), device=X.device)
            else:
                random_ix = torch.randint(self.n_outputs, ())
            return self.backend.single_full(X, y, random_ix, N=N)
        return self.backend.full(X, y, N=N)

    def _compute_scale(self):
        self._posterior_scale = invsqrt_precision(self.posterior_precision)

    @property
    def posterior_scale(self):
        """Posterior scale (square root of the covariance), i.e.,
        \\(P^{-\\frac{1}{2}}\\).

        Returns
        -------
        scale : torch.tensor
            `(parameters, parameters)`
        """
        if self._posterior_scale is None:
            self._compute_scale()
        return self._posterior_scale

    @property
    def posterior_covariance(self):
        """Posterior covariance, i.e., \\(P^{-1}\\).

        Returns
        -------
        covariance : torch.tensor
            `(parameters, parameters)`
        """
        scale = self.posterior_scale
        return scale @ scale.T

    @property
    def posterior_precision(self):
        """Posterior precision \\(P\\).

        Returns
        -------
        precision : torch.tensor
            `(parameters, parameters)`
        """
        self._check_fit()
        return self._H_factor * self.H + torch.diag(self.prior_precision_diag)

    @property
    def log_det_posterior_precision(self):
        return self.posterior_precision.logdet()

    def functional_variance(self, Js):
        return torch.einsum('ncp,pq,nkq->nck', Js, self.posterior_covariance, Js)

    def sample(self, n_samples=100):
        dist = MultivariateNormal(loc=self.mean, scale_tril=self.posterior_scale)
        return dist.sample((n_samples,))


class BlockDiagLaplace(FullLaplace):
    """Naive Blockdiagonal Laplace approximation for testing and development purposes."""

    def fit(self, train_loader, **kwargs):
        super().fit(train_loader, **kwargs)
        n_params_per_layer = parameters_per_layer(self.model)
        block_list, p_cur = list(), 0
        for n_params in n_params_per_layer:
            block_list.append(self.H[p_cur:p_cur+n_params, p_cur:p_cur+n_params])
            p_cur += n_params
        self.H = torch.block_diag(*block_list)


class KronLaplace(BaseLaplace):
    """Laplace approximation with Kronecker factored log likelihood Hessian approximation
    and hence posterior precision.
    Mathematically, we have for each parameter group, e.g., torch.nn.Module,
    that \\P\\approx Q \\otimes H\\.
    See `BaseLaplace` for the full interface and see
    `laplace.matrix.Kron` and `laplace.matrix.KronDecomposed` for the structure of
    the Kronecker factors. `Kron` is used to aggregate factors by summing up and
    `KronDecomposed` is used to add the prior, a Hessian factor (e.g. temperature),
    and computing posterior covariances, marginal likelihood, etc.
    Damping can be enabled by setting `damping=True`.
    """
    # key to map to correct subclass of BaseLaplace, (subset of weights, Hessian structure)
    _key = ('all', 'kron')

    def __init__(self, model, likelihood, sigma_noise=1., prior_precision=1.,
                 prior_mean=0., temperature=1., backend=DefaultBackend, damping=False,
                 backend_kwargs=None, sod=False, single_output=False, single_output_iid=False):
        self.damping = damping
        super().__init__(model, likelihood, sigma_noise, prior_precision, prior_mean,
                         temperature, backend, backend_kwargs, sod, single_output, single_output_iid)

    def _init_H(self):
        self.H = Kron.init_from_model(self.model, self._device)
        self.n_data_seen = 0
        self.loss = 0

    def _curv_closure(self, X, y, N):
        if self.single_output:
            if self.single_output_iid:
                random_ix = torch.randint(self.n_outputs, (len(y),), device=X.device)
            else:
                random_ix = torch.randint(self.n_outputs, ())
            return self.backend.single_kron(X, y, N, random_ix)
        return self.backend.kron(X, y, N=N)

    def fit(self, train_loader, keep_factors=False):
        super().fit(train_loader)
        # Kron requires postprocessing as all quantities depend on the decomposition.
        if keep_factors:
            self.H_facs = self.H
        self.H = self.H.decompose(damping=self.damping)

    def fit_batch(self, x, y, N):
        super().fit_batch(x, y, N)
        self.H = self.H.decompose(damping=self.damping)

    def fit_distributed(self, train_loader, n_steps_sod=1, **kwargs):
        super().fit_distributed(train_loader, n_steps_sod, **kwargs)
        self.H.reduce(dst=0)
        dist.barrier()
        if dist.get_rank() == 0:
            self.H = self.H.decompose(damping=self.damping)

    @property
    def posterior_precision(self):
        """Kronecker factored Posterior precision \\(P\\).

        Returns
        -------
        precision : `laplace.matrix.KronDecomposed`
        """
        self._check_fit()
        return self.H * self._H_factor + self.prior_precision

    @property
    def log_det_posterior_precision(self):
        return self.posterior_precision.logdet()

    @property
    def effective_dimensionality(self):
        return self.posterior_precision.effective_dimensionality()

    def functional_variance(self, Js):
        return self.posterior_precision.inv_square_form(Js)

    def sample(self, n_samples=100):
        samples = torch.randn(n_samples, self.n_params, device=self._device)
        samples = self.posterior_precision.bmm(samples, exponent=-0.5)
        return self.mean.reshape(1, self.n_params) + samples.reshape(n_samples, self.n_params)

    @BaseLaplace.prior_precision.setter
    def prior_precision(self, prior_precision):
        # Extend setter from Laplace to restrict prior precision structure.
        super(KronLaplace, type(self)).prior_precision.fset(self, prior_precision)
        if len(self.prior_precision) not in [1, self.n_layers]:
            raise ValueError('Prior precision for Kron either scalar or per-layer.')


class DiagLaplace(BaseLaplace):
    """Laplace approximation with diagonal log likelihood Hessian approximation
    and hence posterior precision.
    Mathematically, we have \\(P \\approx \\textrm{diag}(P)\\).
    See `BaseLaplace` for the full interface.
    """
    # key to map to correct subclass of BaseLaplace, (subset of weights, Hessian structure)
    _key = ('all', 'diag')

    def _init_H(self):
        self.H = torch.zeros(self.n_params, device=self._device)
        self.n_data_seen = 0
        self.loss = 0

    def _curv_closure(self, X, y, N):
        if self.single_output:
            if self.single_output_iid:
                random_ix = torch.randint(self.n_outputs, (len(y),), device=X.device)
            else:
                random_ix = torch.randint(self.n_outputs, ())
            return self.backend.single_diag(X, y, random_ix, N=N)
        return self.backend.diag(X, y, N=N)

    @property
    def posterior_precision(self):
        """Diagonal posterior precision \\(p\\).

        Returns
        -------
        precision : torch.tensor
            `(parameters)`
        """
        self._check_fit()
        return self._H_factor * self.H + self.prior_precision_diag

    @property
    def posterior_covariance(self):
        """Posterior covariance, i.e., \\(P^{-1}\\).

        Returns
        -------
        covariance : torch.tensor
            `(parameters,)`
        """
        return 1.0 / self.posterior_precision

    @property
    def posterior_scale(self):
        """Diagonal posterior scale \\(\\sqrt{p^{-1}}\\).

        Returns
        -------
        precision : torch.tensor
            `(parameters)`
        """
        return self.posterior_covariance.sqrt()

    @property
    def posterior_variance(self):
        """Diagonal posterior variance \\(p^{-1}\\).

        Returns
        -------
        precision : torch.tensor
            `(parameters)`
        """
        return 1 / self.posterior_precision

    @property
    def log_det_posterior_precision(self):
        return self.posterior_precision.log().sum()

    def functional_variance(self, Js: torch.Tensor) -> torch.Tensor:
        self._check_jacobians(Js)
        return torch.einsum('ncp,p,nkp->nck', Js, self.posterior_variance, Js)

    def sample(self, n_samples=100):
        samples = torch.randn(n_samples, self.n_params, device=self._device)
        samples = samples * self.posterior_scale.reshape(1, self.n_params)
        return self.mean.reshape(1, self.n_params) + samples

File Path: dependencies/laplace/laplace/curvature/__init__.py
Content:
import logging

from laplace.curvature.curvature import CurvatureInterface, GGNInterface, EFInterface

try:
    from laplace.curvature.backpack import BackPackGGN, BackPackEF, BackPackInterface
    from laplace.curvature.augmented_backpack import AugBackPackInterface, AugBackPackGGN
except ModuleNotFoundError:
    logging.info('Backpack not available.')

try:
    from laplace.curvature.asdl import AsdlGGN, AsdlEF, AsdlInterface
    from laplace.curvature.augmented_asdl import AugAsdlInterface, AugAsdlGGN, AugAsdlEF
except ModuleNotFoundError:
    logging.info('asdfghjkl backend not available.')

__all__ = ['CurvatureInterface', 'GGNInterface', 'EFInterface',
           'BackPackInterface', 'BackPackGGN', 'BackPackEF',
           'AsdlInterface', 'AsdlGGN', 'AsdlEF',
           'AugBackPackInterface', 'AugBackPackGGN',
           'AugAsdlInterface', 'AugAsdlGGN', 'AugAsdlEF']

File Path: dependencies/laplace/laplace/curvature/asdl.py
Content:
from abc import abstractproperty
import warnings
from asdfghjkl.kernel import linear_network_kernel, linear_network_kernel_indep, empirical_network_kernel
import numpy as np
import torch

from asdfghjkl import FISHER_EXACT, FISHER_MC, COV
from asdfghjkl import SHAPE_KRON, SHAPE_DIAG
from asdfghjkl.fisher import fisher, zero_fisher
from asdfghjkl.gradient import batch_gradient

from laplace.curvature import CurvatureInterface, GGNInterface, EFInterface
from laplace.matrix import Kron
from laplace.utils import _is_batchnorm


class AsdlInterface(CurvatureInterface):
    """Interface for asdfghjkl backend.
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True, kron_jac=False):
        self.kron_jac = kron_jac
        super().__init__(model, likelihood, last_layer, differentiable)

    def jacobians(self, x):
        """Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\)
        using asdfghjkl's gradient per output dimension.

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        """
        Js = list()
        for i in range(self.model.output_size):
            def loss_fn(outputs, targets):
                return outputs[:, i].sum()

            f = batch_gradient(self.model, loss_fn, x, None, self.kron_jac, **self.backward_kwargs)
            Js.append(_get_batch_grad(self.model, self.kron_jac))
        Js = torch.stack(Js, dim=1)

        if self.differentiable:
            return Js, f
        return Js.detach(), f.detach()

    def single_jacobians(self, x, output_ix):
        def loss_fn(outputs, targets):
            if output_ix.ndim == 0:  # scalar
                return outputs[:, output_ix].sum()
            elif output_ix.ndim == 1:  # vector iid
                return outputs.gather(1, output_ix.unsqueeze(-1)).sum()
            else:
                raise ValueError('output_ix must be scalar or vector')

        f = batch_gradient(self.model, loss_fn, x, None, self.kron_jac, **self.backward_kwargs)
        Js = _get_batch_grad(self.model, self.kron_jac)
        if self.differentiable:
            return Js, f
        return Js.detach(), f.detach()

    def mean_jacobians(self, x):
        assert self.likelihood == 'heteroscedastic_regression'
        def loss_fn(outputs, targets):
            m = - outputs[:, 0] / (2 * outputs[:, 1])
            return m.sum()
        f = batch_gradient(self.model, loss_fn, x, None, self.kron_jac, **self.backward_kwargs)
        Js = _get_batch_grad(self.model, self.kron_jac)
        if self.differentiable:
            return Js, f
        return Js.detach(), f.detach()

    def gradients(self, x, y):
        """Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter
        \\(\\theta\\) using asdfghjkl's backend.

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.
        y : torch.Tensor

        Returns
        -------
        loss : torch.Tensor
        Gs : torch.Tensor
            gradients `(batch, parameters)`
        """
        f = batch_gradient(self.model, self.lossfunc, x, y, self.kron_jac, **self.backward_kwargs)
        Gs = _get_batch_grad(self._model, self.kron_jac)
        loss = self.lossfunc(f, y)
        if self.differentiable:
            return Gs, loss
        return Gs.detach(), loss.detach()

    @abstractproperty
    def _ggn_type(self):
        raise NotImplementedError()

    def _get_kron_factors(self, curv, M):
        kfacs = list()
        for module in curv._model.modules():
            if _is_batchnorm(module):
                warnings.warn('BatchNorm unsupported for Kron, ignore.')
                continue

            stats = getattr(module, self._ggn_type, None)
            if stats is None:
                continue
            if hasattr(module, 'bias') and module.bias is not None:
                # split up bias and weights
                kfacs.append([stats.kron.B, stats.kron.A[:-1, :-1]])
                kfacs.append([stats.kron.B * stats.kron.A[-1:, -1:].clone() / M])
            elif hasattr(module, 'weight'):
                p, q = np.prod(stats.kron.B.shape), np.prod(stats.kron.A.shape)
                if p == q == 1:
                    kfacs.append([stats.kron.B * stats.kron.A])
                else:
                    kfacs.append([stats.kron.B, stats.kron.A])
            else:
                raise ValueError(f'Whats happening with {module}?')
        return Kron(kfacs)

    @staticmethod
    def _rescale_kron_factors(kron, N):
        for F in kron.kfacs:
            if len(F) == 2:
                F[1] *= 1/N
        return kron

    def diag(self, X, y, **kwargs):
        if self.last_layer:
            f, X = self.model.forward_with_features(X)
        f, curv = fisher(self._model, self._ggn_type, SHAPE_DIAG, likelihood=self.likelihood,
                         inputs=X, targets=y, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        diag_ggn = curv.matrices_to_vector(None)

        if self.differentiable:
            return self.factor * loss, self.factor * diag_ggn
        return self.factor * loss.detach(), self.factor * diag_ggn.detach()

    def kron(self, X, y, N, **kwargs):
        if self.last_layer:
            f, X = self.model.forward_with_features(X)
        f, curv = fisher(self._model, self._ggn_type, SHAPE_KRON, likelihood=self.likelihood,
                         inputs=X, targets=y, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        M = len(y)
        kron = self._get_kron_factors(curv, M)
        kron = self._rescale_kron_factors(kron, N)
        zero_fisher(self._model, [self._ggn_type])

        if self.differentiable:
            return self.factor * loss, self.factor * kron
        return self.factor * loss.detach(), self.factor * kron.detach()


class AsdlGGN(AsdlInterface, GGNInterface):
    """Implementation of the `GGNInterface` using asdfghjkl.
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True, kron_jac=False, stochastic=False):
        super().__init__(model, likelihood, last_layer, differentiable, kron_jac)
        self.stochastic = stochastic

    @property
    def _ggn_type(self):
        return FISHER_MC if self.stochastic else FISHER_EXACT

    def kernel(self, x, y, prec_diag, **kwargs):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')

        f, K = linear_network_kernel(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,
                                     differentiable=self.differentiable, kron_jac=self.kron_jac)
        n, c = f.shape
        K = K.transpose(1, 2).reshape(n*c, n*c)  # n x c x n x c

        loss = self.factor * self.lossfunc(f, y)
        if self.differentiable:
            return loss, K
        return loss.detach(), K.detach()

    def indep_kernel(self, x, y, prec_diag):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')
        f, K = linear_network_kernel_indep(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,
                                           differentiable=self.differentiable, kron_jac=self.kron_jac)
        loss = self.factor * self.lossfunc(f, y)
        if self.differentiable:
            return loss, K
        return loss.detach(), K.detach()

    def single_kernel(self, x, y, prec_diag, output_ix):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')
        f, K = linear_network_kernel_indep(
            self._model, x, scale=1 / prec_diag, likelihood=self.likelihood, differentiable=self.differentiable,
            kron_jac=self.kron_jac, single_output=output_ix
        )
        loss = self.factor * self.lossfunc(f, y)
        if self.differentiable:
            return loss, K
        return loss.detach(), K.detach()

    def single_kron(self, X, y, N, output_ix, **kwargs):
        if self.last_layer:
            f, X = self.model.forward_with_features(X)
        f, curv = fisher(self._model, self._ggn_type, SHAPE_KRON, likelihood=self.likelihood,
                         inputs=X, targets=y, single_output=output_ix, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        M = len(y)
        kron = self._get_kron_factors(curv, M)
        kron = self._rescale_kron_factors(kron, N)
        zero_fisher(self._model, [self._ggn_type])

        if self.differentiable:
            return self.factor * loss, self.factor * kron
        return self.factor * loss.detach(), self.factor * kron.detach()

    def single_diag(self, X, y, output_ix, **kwargs):
        if self.last_layer:
            f, X = self.model.forward_with_features(X)
        f, curv = fisher(self._model, self._ggn_type, SHAPE_DIAG, likelihood=self.likelihood,
                         inputs=X, targets=y, single_output=output_ix, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        diag_ggn = curv.matrices_to_vector(None)

        if self.differentiable:
            return self.factor * loss, self.factor * diag_ggn
        return self.factor * loss.detach(), self.factor * diag_ggn.detach()


class AsdlEF(AsdlInterface, EFInterface):
    """Implementation of the `EFInterface` using asdfghjkl.
    """

    @property
    def _ggn_type(self):
        return COV

    def kernel(self, x, y, prec_diag, prec, prec_structure):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')

        if prec_structure == 'diagonal':
            return super().kernel(x, y, prec_diag)

        loss, K = empirical_network_kernel(self._model, x, y, self.lossfunc, 1 / prec,
                                           differentiable=self.differentiable, kron_jac=self.kron_jac)
        if self.differentiable:
            return self.factor * loss, self.factor * K
        return self.factor * loss.detach(), self.factor * K.detach()


def _flatten_after_batch(tensor: torch.Tensor):
    if tensor.ndim == 1:
        return tensor.unsqueeze(-1)
    else:
        return tensor.flatten(start_dim=1)


def _get_batch_grad(model, kron_jac=False):
    field = 'batch_grads_kron' if kron_jac else 'batch_grads'
    batch_grads = list()
    for module in model.modules():
        if hasattr(module, 'op_results'):
            res = module.op_results[field]
            if 'weight' in res:
                batch_grads.append(_flatten_after_batch(res['weight']))
            if 'bias' in res:
                batch_grads.append(_flatten_after_batch(res['bias']))
            if len(set(res.keys()) - {'weight', 'bias'}) > 0:
                raise ValueError(f'Invalid parameter keys {res.keys()}')
    return torch.cat(batch_grads, dim=1)

File Path: dependencies/laplace/laplace/curvature/augmented_asdl.py
Content:
import warnings
import numpy as np
import torch

from asdfghjkl.gradient import batch_aug_gradient
from asdfghjkl.fisher import fisher
from asdfghjkl.kernel import linear_network_kernel, linear_network_kernel_indep, empirical_network_kernel
from asdfghjkl import FISHER_EXACT, FISHER_MC, COV
from asdfghjkl import SHAPE_KRON, SHAPE_DIAG

from laplace.curvature import CurvatureInterface, GGNInterface
from laplace.curvature.asdl import _get_batch_grad
from laplace.curvature import EFInterface
from laplace.matrix import Kron
from laplace.utils import _is_batchnorm


class AugAsdlInterface(CurvatureInterface):
    """Interface for Backpack backend when using augmented Laplace.
    This ensures that Jacobians, gradients, and the Hessian approximation remain differentiable
    and deals with S-augmented sized inputs (additional to the batch-dimension).
    """

    def __init__(self, model, likelihood, last_layer=False, differentiable=True, kron_jac=False):
        self.kron_jac = kron_jac
        super().__init__(model, likelihood, last_layer, differentiable)

    def jacobians(self, x):
        """Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)
        using asdfghjkl's gradient per output dimension, averages over aug dimension.

        Parameters
        ----------
        model : torch.nn.Module
        x : torch.Tensor
            input data `(batch, n_augs, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            averaged Jacobians over `n_augs` of shape `(batch, parameters, outputs)`
        f : torch.Tensor
            averaged output function over `n_augs` of shape `(batch, outputs)`
        """
        Js = list()
        for i in range(self.model.output_size):
            def loss_fn(outputs, _):
                return outputs.mean(dim=1)[:, i].sum()

            f = batch_aug_gradient(self.model, loss_fn, x, None, self.kron_jac, 
                                   **self.backward_kwargs).mean(dim=1)
            Js.append(_get_batch_grad(self.model, self.kron_jac))
        Js = torch.stack(Js, dim=1)

        # set gradients to zero, differentiation here only serves Jacobian computation
        self.model.zero_grad()
        if self.differentiable:
            return Js, f
        return Js.detach(), f.detach()

    def single_jacobians(self, x, output_ix):
        def loss_fn(outputs, targets):
            if output_ix.ndim == 0:  # scalar case
                return outputs.mean(dim=1)[:, output_ix].sum()
            elif output_ix.ndim == 1:  # vector iid case
                return outputs.mean(dim=1).gather(1, output_ix.unsqueeze(-1)).sum()
            else:
                raise ValueError('output_ix must be scalar or vector')

        f = batch_aug_gradient(self.model, loss_fn, x, None, self.kron_jac, 
                               **self.backward_kwargs).mean(dim=1)
        Js = _get_batch_grad(self.model, self.kron_jac)
        if self.differentiable:
            return Js, f
        return Js.detach(), f.detach()

    def gradients(self, x, y):
        def loss_fn(outputs, targets):
            return self.lossfunc(outputs.mean(dim=1), targets)
        f = batch_aug_gradient(self._model, loss_fn, x, y, self.kron_jac,
                               **self.backward_kwargs).mean(dim=1)
        Gs = _get_batch_grad(self._model, self.kron_jac)
        loss = self.lossfunc(f, y)
        if self.differentiable:
            return Gs, loss
        return Gs.detach(), loss.detach()

    def _get_kron_factors(self, curv, M):
        kfacs = list()
        for module in curv._model.modules():
            if _is_batchnorm(module):
                warnings.warn('BatchNorm unsupported for Kron, ignore.')
                continue

            stats = getattr(module, self._ggn_type, None)
            if stats is None:
                continue
            if hasattr(module, 'bias') and module.bias is not None:
                # split up bias and weights
                kfacs.append([stats.kron.B, stats.kron.A[:-1, :-1]])
                kfacs.append([stats.kron.B * stats.kron.A[-1, -1].clone() / M])
            elif hasattr(module, 'weight'):
                p, q = np.prod(stats.kron.B.shape), np.prod(stats.kron.A.shape)
                if p == q == 1:
                    kfacs.append([stats.kron.B * stats.kron.A])
                else:
                    kfacs.append([stats.kron.B, stats.kron.A])
            else:
                raise ValueError(f'Whats happening with {module}?')
        return Kron(kfacs)

    @staticmethod
    def _rescale_kron_factors(kron, N):
        for F in kron.kfacs:
            if len(F) == 2:
                F[1] *= 1/N
        return kron

    def diag(self, X, y, **kwargs):
        if self.last_layer:
            raise ValueError('Not supported')
        f, curv = fisher(self._model, self._ggn_type, SHAPE_DIAG, likelihood=self.likelihood,
                         inputs=X, targets=y, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        diag_ggn = curv.matrices_to_vector(None)

        if self.differentiable:
            return self.factor * loss, self.factor * diag_ggn
        return self.factor * loss.detach(), self.factor * diag_ggn.detach()

    def kron(self, X, y, N, **wkwargs):
        if self.last_layer:
            raise ValueError('Not supported')
            f, X = self.model.forward_with_features(X)
        f, curv = fisher(self._model, self._ggn_type, SHAPE_KRON, likelihood=self.likelihood,
                         inputs=X, targets=y, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        M = len(y)
        kron = self._get_kron_factors(curv, M)
        kron = self._rescale_kron_factors(kron, N)

        if self.differentiable:
            return self.factor * loss, self.factor * kron
        return self.factor * loss.detach(), self.factor * kron.detach()


class AugAsdlGGN(AugAsdlInterface, GGNInterface):
    """Implementation of the `GGNInterface` with Asdl and augmentation support.
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True, 
                 kron_jac=False, stochastic=False):
        super().__init__(model, likelihood, last_layer, differentiable, kron_jac)
        self.stochastic = stochastic

    def full(self, x, y, **kwargs):
        """Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation
        \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
        For last-layer, reduced to \\(\\theta_{last}\\)

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, n_augs, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H_ggn : torch.Tensor
            GGN `(parameters, parameters)`
        """
        if self.stochastic:
            raise ValueError('Stochastic approximation not implemented for full GGN.')
        if self.last_layer:
            raise ValueError('Not yet tested/implemented for last layer.')

        Js, f = self.jacobians(x)
        loss, H_ggn = self._get_full_ggn(Js, f, y)

        return loss, H_ggn

    @property
    def _ggn_type(self):
        return FISHER_MC if self.stochastic else FISHER_EXACT

    def kernel(self, x, y, prec_diag, **kwargs):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')
        f, K = linear_network_kernel(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,
                                     differentiable=self.differentiable, kron_jac=self.kron_jac)
        n, c = f.shape
        K = K.transpose(1, 2).reshape(n*c, n*c)  # n x c x n x c

        loss = self.factor * self.lossfunc(f, y)
        if self.differentiable:
            return loss, K
        return loss.detach(), K.detach()
    
    def indep_kernel(self, x, y, prec_diag):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')
        f, K = linear_network_kernel_indep(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,
                                           differentiable=self.differentiable, kron_jac=self.kron_jac)
        loss = self.factor * self.lossfunc(f, y)
        if self.differentiable:
            return loss, K
        return loss.detach(), K.detach()

    def single_kernel(self, x, y, prec_diag, output_ix):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')
        f, K = linear_network_kernel_indep(
            self._model, x, scale=1 / prec_diag, likelihood=self.likelihood, differentiable=self.differentiable, 
            kron_jac=self.kron_jac, single_output=output_ix
        )
        loss = self.factor * self.lossfunc(f, y)
        if self.differentiable:
            return loss, K
        return loss.detach(), K.detach()

    def single_diag(self, X, y, output_ix, **kwargs):
        if self.last_layer:
            raise ValueError('Not supported')
        f, curv = fisher(self._model, self._ggn_type, SHAPE_DIAG, likelihood=self.likelihood,
                         inputs=X, targets=y, single_output=output_ix, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        diag_ggn = curv.matrices_to_vector(None)

        if self.differentiable:
            return self.factor * loss, self.factor * diag_ggn
        return self.factor * loss.detach(), self.factor * diag_ggn.detach()

    def single_kron(self, X, y, N, output_ix, **wkwargs):
        if self.last_layer:
            raise ValueError('Not supported')
        f, curv = fisher(self._model, self._ggn_type, SHAPE_KRON, likelihood=self.likelihood,
                         inputs=X, targets=y, single_output=output_ix, **self.backward_kwargs)
        loss = self.lossfunc(f, y)
        M = len(y)
        kron = self._get_kron_factors(curv, M)
        kron = self._rescale_kron_factors(kron, N)

        if self.differentiable:
            return self.factor * loss, self.factor * kron
        return self.factor * loss.detach(), self.factor * kron.detach()


class AugAsdlEF(AugAsdlInterface, EFInterface):
    """Implementation of the `EFInterface` using Asdl and augmentation support.
    """

    @property
    def _ggn_type(self):
        return COV

    def kernel(self, x, y, prec_diag, prec, prec_structure):
        if self.last_layer:
            raise ValueError('Unsupported last layer for kernel')

        if prec_structure == 'diagonal':
            return super().kernel(x, y, prec_diag)

        loss, K = empirical_network_kernel(self._model, x, y, self.lossfunc, 1 / prec, 
                                           differentiable=self.differentiable, kron_jac=self.kron_jac)
        if self.differentiable:
            return self.factor * loss, self.factor * K
        return self.factor * loss.detach(), self.factor * K.detach()

File Path: dependencies/laplace/laplace/curvature/augmented_backpack.py
Content:
import torch

from backpack import backpack, extend
from backpack.extensions import BatchGrad
from backpack.context import CTX

from laplace.curvature import CurvatureInterface, GGNInterface
from laplace.curvature.backpack import _cleanup


class AugBackPackInterface(CurvatureInterface):
    """Interface for Backpack backend when using augmented Laplace.
    This ensures that Jacobians, gradients, and the Hessian approximation remain differentiable
    and deals with S-augmented sized inputs (additional to the batch-dimension).
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True):
        super().__init__(model, likelihood, last_layer, differentiable)
        extend(self._model)
        extend(self.lossfunc)

    def jacobians(self, x):
        """Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)
        using backpack's BatchGrad per output dimension.

        Parameters
        ----------
        model : torch.nn.Module
        x : torch.Tensor
            input data `(batch, n_augs, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            averaged Jacobians over `n_augs` of shape `(batch, parameters, outputs)`
        f : torch.Tensor
            averaged output function over `n_augs` of shape `(batch, outputs)`
        """
        self.model = extend(self.model)
        batch_size, n_augs = x.shape[:2]
        x_aug = x
        x = x.flatten(start_dim=0, end_dim=1)
        to_stack = []
        for i in range(self.model.output_size):
            self.model.zero_grad()
            out = self.model(x)
            with backpack(BatchGrad()):
                if self.model.output_size > 1:
                    out[:, i].sum().backward(**self.backward_kwargs)
                else:
                    out.sum().backward(**self.backward_kwargs)
                to_cat = []
                for param in self.model.parameters():
                    to_cat.append(param.grad_batch.reshape(x.shape[0], -1))
                    delattr(param, 'grad_batch')
                Jk = torch.cat(to_cat, dim=1).reshape(batch_size, n_augs, -1).mean(dim=1)
                if not self.differentiable:
                    Jk = Jk.detach()
                    
            to_stack.append(Jk)
            if i == 0:
                f = out.reshape(batch_size, n_augs, -1).mean(dim=1)

        if not self.differentiable:
            f = f.detach()

        # set gradients to zero, differentiation here only serves Jacobian computation
        self.model.zero_grad()
        if x_aug.grad is not None:
            x_aug.grad.zero_()

        CTX.remove_hooks()
        _cleanup(self.model)
        if self.model.output_size > 1:
            return torch.stack(to_stack, dim=2).transpose(1, 2), f
        else:
            return Jk.unsqueeze(-1).transpose(1, 2), f

    def gradients(self, x, y):
        # Problem: averaging leads to shape issues with backpack here.
        raise NotImplementedError


class AugBackPackGGN(AugBackPackInterface, GGNInterface):
    """Implementation of the `GGNInterface` using Backpack.
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True, stochastic=False):
        super().__init__(model, likelihood, last_layer, differentiable)
        self.stochastic = stochastic

    def full(self, x, y, **kwargs):
        """Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation
        \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
        For last-layer, reduced to \\(\\theta_{last}\\)

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, n_augs, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H_ggn : torch.Tensor
            GGN `(parameters, parameters)`
        """
        if self.stochastic:
            raise ValueError('Stochastic approximation not implemented for full GGN.')
        if self.last_layer:
            raise ValueError('Not yet tested/implemented for last layer.')

        Js, f = self.jacobians(x)
        loss, H_ggn = self._get_full_ggn(Js, f, y)

        return loss, H_ggn

    def diag(self, X, y, **kwargs):
        raise NotImplementedError('Unavailable for DA.')

    def kron(self, X, y, N, **kwargs):
        raise NotImplementedError('Unavailable for DA.')

File Path: dependencies/laplace/laplace/curvature/backpack.py
Content:
import warnings
import torch

from backpack import backpack, extend, memory_cleanup
from backpack.extensions import DiagGGNExact, DiagGGNMC, KFAC, KFLR, SumGradSquared, BatchGrad
from backpack.context import CTX

from laplace.curvature import CurvatureInterface, GGNInterface, EFInterface
from laplace.matrix import Kron


class BackPackInterface(CurvatureInterface):
    """Interface for Backpack backend.
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True):
        super().__init__(model, likelihood, last_layer, differentiable)
        if differentiable:
            warnings.warn('BackPack not guaranteed to yield correct differentiable quantities')
        extend(self._model)
        extend(self.lossfunc)

    def jacobians(self, x):
        """Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)
        using backpack's BatchGrad per output dimension.

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        """
        self.model = extend(self.model)
        to_stack = []
        for i in range(self.model.output_size):
            self.model.zero_grad()
            out = self.model(x)
            with backpack(BatchGrad()):
                if self.model.output_size > 1:
                    out[:, i].sum().backward(**self.backward_kwargs)
                else:
                    out.sum().backward(**self.backward_kwargs)
                to_cat = []
                for param in self.model.parameters():
                    to_cat.append(param.grad_batch.reshape(x.shape[0], -1))
                    delattr(param, 'grad_batch')
                if self.differentiable:
                    Jk = torch.cat(to_cat, dim=1)
                else:
                    Jk = torch.cat(to_cat, dim=1).detach()
            to_stack.append(Jk)
            if i == 0:
                f = out

        if not self.differentiable:
            f = f.detach()

        self.model.zero_grad()
        CTX.remove_hooks()
        _cleanup(self.model)
        if self.model.output_size > 1:
            return torch.stack(to_stack, dim=2).transpose(1, 2), f
        else:
            return Jk.unsqueeze(-1).transpose(1, 2), f

    def gradients(self, x, y):
        """Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter
        \\(\\theta\\) using Backpack's BatchGrad.

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.
        y : torch.Tensor

        Returns
        -------
        loss : torch.Tensor
        Gs : torch.Tensor
            gradients `(batch, parameters)`
        """
        f = self.model(x)
        loss = self.lossfunc(f, y)
        with backpack(BatchGrad()):
            loss.backward(**self.backward_kwargs)
        Gs = torch.cat([p.grad_batch.flatten(start_dim=1)
                        for p in self._model.parameters()], dim=1)

        if self.differentiable:
            return Gs, loss
        return Gs.detach(), loss.detach()


class BackPackGGN(BackPackInterface, GGNInterface):
    """Implementation of the `GGNInterface` using Backpack.
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True, stochastic=False):
        super().__init__(model, likelihood, last_layer, differentiable)
        self.stochastic = stochastic

    def _get_diag_ggn(self):
        if self.stochastic:
            return torch.cat([p.diag_ggn_mc.flatten() for p in self._model.parameters()])
        else:
            return torch.cat([p.diag_ggn_exact.flatten() for p in self._model.parameters()])

    def _get_kron_factors(self):
        if self.stochastic:
            return Kron([p.kfac for p in self._model.parameters()])
        else:
            return Kron([p.kflr for p in self._model.parameters()])

    @staticmethod
    def _rescale_kron_factors(kron, M, N):
        # Renormalize Kronecker factor to sum up correctly over N data points with batches of M
        # for M=N (full-batch) just M/N=1
        for F in kron.kfacs:
            if len(F) == 2:
                F[1] *= M/N
        return kron

    def diag(self, X, y, **kwargs):
        context = DiagGGNMC if self.stochastic else DiagGGNExact
        f = self.model(X)
        loss = self.lossfunc(f, y)
        with backpack(context()):
            loss.backward(**self.backward_kwargs)
        dggn = self._get_diag_ggn()

        if self.differentiable:
            return self.factor * loss, self.factor * dggn
        return self.factor * loss.detach(), self.factor * dggn.detach()

    def kron(self, X, y, N, **kwargs):
        context = KFAC if self.stochastic else KFLR
        f = self.model(X)
        loss = self.lossfunc(f, y)
        with backpack(context()):
            loss.backward(**self.backward_kwargs)
        kron = self._get_kron_factors()
        kron = self._rescale_kron_factors(kron, len(y), N)

        if self.differentiable:
            return self.factor * loss, self.factor * kron
        return self.factor * loss.detach(), self.factor * kron.detach()


class BackPackEF(BackPackInterface, EFInterface):
    """Implementation of `EFInterface` using Backpack.
    """

    def diag(self, X, y, **kwargs):
        f = self.model(X)
        loss = self.lossfunc(f, y)
        with backpack(SumGradSquared()):
            loss.backward(**self.backward_kwargs)
        diag_EF = torch.cat([p.sum_grad_squared.flatten()
                             for p in self._model.parameters()])

        if self.differentiable:
            return self.factor * loss, self.factor * diag_EF
        return self.factor * loss.detach(), self.factor * diag_EF.detach()

    def kron(self, X, y, **kwargs):
        raise NotImplementedError('Unavailable through Backpack.')


def _cleanup(module):
    for child in module.children():
        _cleanup(child)

    setattr(module, "_backpack_extend", False)
    memory_cleanup(module)

File Path: dependencies/laplace/laplace/curvature/curvature.py
Content:
from functools import partial
import torch
from torch.nn import MSELoss, CrossEntropyLoss

from asdfghjkl.loss import heteroscedastic_mse_loss
from asdfghjkl.kernel import hessian_single_heteroscedastic_regression


class CurvatureInterface:
    """Interface to access curvature for a model and corresponding likelihood.
    A `CurvatureInterface` must inherit from this baseclass and implement the
    necessary functions `jacobians`, `full`, `kron`, and `diag`.
    The interface might be extended in the future to account for other curvature
    structures, for example, a block-diagonal one.

    Parameters
    ----------
    model : torch.nn.Module or `laplace.feature_extractor.FeatureExtractor`
        torch model (neural network)
    likelihood : {'classification', 'regression'}
    last_layer : bool, default=False
        only consider curvature of last layer

    Attributes
    ----------
    lossfunc : torch.nn.MSELoss or torch.nn.CrossEntropyLoss
    factor : float
        conversion factor between torch losses and base likelihoods
        For example, \\(\\frac{1}{2}\\) to get to \\(\\mathcal{N}(f, 1)\\) from MSELoss.
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True):
        self.likelihood = likelihood
        self.model = model
        self.last_layer = last_layer
        if likelihood == 'regression':
            self.lossfunc = MSELoss(reduction='sum')
            self.factor = 0.5
        elif likelihood == 'classification':
            self.lossfunc = CrossEntropyLoss(reduction='sum')
            self.factor = 1.
        elif likelihood == 'heteroscedastic_regression':
            self.lossfunc = partial(heteroscedastic_mse_loss, reduction='sum')
            self.factor = 1.
        else:
            raise ValueError('Invalid likelihood')
        self.differentiable = differentiable

    @property
    def _model(self):
        return self.model.last_layer if self.last_layer else self.model

    @property
    def differentiable(self):
        return self._differentiable

    @differentiable.setter
    def differentiable(self, value):
        assert type(value) is bool
        self._differentiable = value
        if value:
            self.backward_kwargs = dict(retain_graph=True, create_graph=True)
        else:
            self.backward_kwargs = dict()

    def jacobians(self, x):
        """Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        """
        pass

    def single_jacobians(self, x, output_ix):
        """Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.
        output_ix : int

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, parameters)`
        f : torch.Tensor
            output function `(batch, outputs)`
        """
        pass

    @staticmethod
    def last_layer_jacobians(model, x):
        """Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) 
        only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).

        Parameters
        ----------
        model : laplace.feature_extractor.FeatureExtractor
        x : torch.Tensor

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, last-layer-parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        """
        f, phi = model.forward_with_features(x)
        bsize = len(x)
        output_size = f.shape[-1]

        # calculate Jacobians using the feature vector 'phi'
        identity = torch.eye(output_size, device=x.device).unsqueeze(0).tile(bsize, 1, 1)
        # Jacobians are batch x output x params
        Js = torch.einsum('kp,kij->kijp', phi, identity).reshape(bsize, output_size, -1)
        if model.last_layer.bias is not None:
            Js = torch.cat([Js, identity], dim=2)

        return Js, f.detach()

    def gradients(self, x, y):
        """Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.
        y : torch.Tensor

        Returns
        -------
        loss : torch.Tensor
        Gs : torch.Tensor
            gradients `(batch, parameters)`
        """
        pass

    def full(self, x, y, **kwargs):
        """Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix
        \\(H\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H : torch.Tensor
            Hessian approximation `(parameters, parameters)`
        """
        pass

    def kron(self, x, y, **kwargs):
        """Compute a Kronecker factored curvature approximation (such as KFAC).
        The approximation to \\(H\\) takes the form of two Kronecker factors \\(Q, H\\),
        i.e., \\(H \\approx Q \\otimes H\\) for each Module in the neural network permitting 
        such curvature.
        \\(Q\\) is quadratic in the input-dimension of a module \\(p_{in} \\times p_{in}\\)
        and \\(H\\) in the output-dimension \\(p_{out} \\times p_{out}\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H : `laplace.matrix.Kron`
            Kronecker factored Hessian approximation.
        """

    def diag(self, x, y, **kwargs):
        """Compute a diagonal Hessian approximation to \\(H\\) and is represented as a 
        vector of the dimensionality of parameters \\(\\theta\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H : torch.Tensor
            vector representing the diagonal of H
        """
        pass


class GGNInterface(CurvatureInterface):
    """Generalized Gauss-Newton or Fisher Curvature Interface.
    The GGN is equal to the Fisher information for the available likelihoods.
    In addition to `CurvatureInterface`, methods for Jacobians are required by subclasses.

    Parameters
    ----------
    model : torch.nn.Module or `laplace.feature_extractor.FeatureExtractor`
        torch model (neural network)
    likelihood : {'classification', 'regression'}
    last_layer : bool, default=False
        only consider curvature of last layer
    stochastic : bool, default=False
        Fisher if stochastic else GGN
    """
    def __init__(self, model, likelihood, last_layer=False, differentiable=True, stochastic=False):
        self.stochastic = stochastic
        super().__init__(model, likelihood, last_layer, differentiable)

    def _get_full_ggn(self, Js, f, y):
        """Compute full GGN from Jacobians.

        Parameters
        ----------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            functions `(batch, outputs)`
        y : torch.Tensor
            labels compatible with loss

        Returns
        -------
        loss : torch.Tensor
        H_ggn : torch.Tensor
            full GGN approximation `(parameters, parameters)`
        """
        loss = self.factor * self.lossfunc(f, y)
        if self.likelihood == 'regression':
            H_ggn = torch.einsum('mkp,mkq->pq', Js, Js)
        elif self.likelihood == 'classification':
            # second derivative of log lik is diag(p) - pp^T
            ps = torch.softmax(f, dim=-1)
            H_lik = torch.diag_embed(ps) - torch.einsum('mk,mc->mck', ps, ps)
            H_ggn = torch.einsum('mcp,mck,mkq->pq', Js, H_lik, Js)
        elif self.likelihood == 'heteroscedastic_regression':
            H_lik = f.new_zeros((f.shape[0], 2, 2))
            eta_1, eta_2 = f[:, 0], f[:, 1]
            H_lik[:, 0, 0] = - 0.5 / eta_2
            H_lik[:, 0, 1] = H_lik[:, 1, 0] = 0.5 * eta_1 / eta_2.square()
            H_lik[:, 1, 1] = 0.5 / eta_2.square() - 0.5 * eta_1.square() / torch.pow(eta_2, 3)
            H_ggn = torch.einsum('mcp,mck,mkq->pq', Js, H_lik, Js)

        if self.differentiable:
            return loss, H_ggn
        return loss.detach(), H_ggn.detach()

    def full(self, x, y, **kwargs):
        """Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation
        \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
        For last-layer, reduced to \\(\\theta_{last}\\)

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H_ggn : torch.Tensor
            GGN `(parameters, parameters)`
        """
        if self.stochastic:
            raise ValueError('Stochastic approximation not implemented for full GGN.')

        if self.last_layer:
            Js, f = self.last_layer_jacobians(self.model, x)
        else:
            Js, f = self.jacobians(x)
        loss, H_ggn = self._get_full_ggn(Js, f, y)

        if self.differentiable:
            return loss, H_ggn
        return loss.detach(), H_ggn.detach()

    def single_full(self, x, y, output_ix, **kwargs):
        if self.last_layer:
            raise ValueError('Not supported')
        Js, f = self.single_jacobians(x, output_ix)
        loss = self.factor * self.lossfunc(f, y)

        if self.likelihood == 'regression':
            H_ggn = Js.T @ Js
        elif self.likelihood == 'classification':
            p = torch.softmax(f, dim=-1)[:, output_ix]
            hi = p - p.square()
            H_ggn = (Js * hi.unsqueeze(-1)).T @ Js
        elif self.likelihood == 'heteroscedastic_regression':
            l = hessian_single_heteroscedastic_regression(f, output_ix) 
            H_ggn = (Js * l.unsqueeze(-1)).T @ Js
        else:
            raise ValueError('Only supported for regression and classification.')

        if self.differentiable:
            return loss, H_ggn
        return loss.detach(), H_ggn.detach()
        
    def kernel(self, x, y, prec_diag, **kwargs):
        if self.last_layer:
            Js, f = self.last_layer_jacobians(self.model, x)
        else:
            Js, f = self.jacobians(x)
        M, K, P = Js.shape

        if self.likelihood == 'classification':
            p = torch.softmax(f, dim=-1)
            L = torch.diag_embed(p) - torch.einsum('mk,mc->mck', p, p)
            Js_right = (Js.transpose(1, 2) @ L).transpose(1, 2)
        elif self.likelihood == 'regression':
            Js_right = Js
        elif self.likelihood == 'heteroscedastic_regression':
            L = f.new_zeros((f.shape[0], 2, 2))
            eta_1, eta_2 = f[:, 0], f[:, 1]
            L[:, 0, 0] = - 0.5 / eta_2
            L[:, 0, 1] = L[:, 1, 0] = 0.5 * eta_1 / eta_2.square()
            L[:, 1, 1] = 0.5 / eta_2.square() - 0.5 * eta_1.square() / torch.pow(eta_2, 3)
            Js_right = (Js.transpose(1, 2) @ L).transpose(1, 2)

        Js_left = Js.reshape(-1, P) / prec_diag.reshape(1, P)
        K = Js_left @ Js_right.reshape(-1, P).T

        loss = self.factor * self.lossfunc(f, y)
        if self.differentiable:
            return loss, K
        return loss.detach(), K.detach()


class EFInterface(CurvatureInterface):
    """Interface for Empirical Fisher as Hessian approximation.
    In addition to `CurvatureInterface`, methods for gradients are required by subclasses.

    Parameters
    ----------
    model : torch.nn.Module or `laplace.feature_extractor.FeatureExtractor`
        torch model (neural network)
    likelihood : {'classification', 'regression'}
    last_layer : bool, default=False
        only consider curvature of last layer

    Attributes
    ----------
    lossfunc : torch.nn.MSELoss or torch.nn.CrossEntropyLoss
    factor : float
        conversion factor between torch losses and base likelihoods
        For example, \\(\\frac{1}{2}\\) to get to \\(\\mathcal{N}(f, 1)\\) from MSELoss.
    """

    def full(self, x, y, **kwargs):
        """Compute the full EF \\(P \\times P\\) matrix as Hessian approximation
        \\(H_{ef}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
        For last-layer, reduced to \\(\\theta_{last}\\)

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H_ef : torch.Tensor
            EF `(parameters, parameters)`
        """
        Gs, loss = self.gradients(x, y)
        H_ef = Gs.T @ Gs
        if self.differentiable:
            return self.factor * loss, self.factor * H_ef
        return self.factor * loss.detach(), self.factor * H_ef.detach()

    def kernel(self, x, y, prec_diag, **kwargs):
        Gs, loss = self.gradients(x, y)
        M, P = Gs.shape
        K = (Gs / prec_diag.reshape(1, P)) @ Gs.T
        if self.differentiable:
            return self.factor * loss, self.factor * K
        return self.factor * loss.detach(), self.factor * K.detach()

File Path: dependencies/laplace/laplace/feature_extractor.py
Content:
import torch
import torch.nn as nn
from typing import Tuple, Callable, Optional


__all__ = ['FeatureExtractor']


class FeatureExtractor(nn.Module):
    """Feature extractor for a PyTorch neural network.
    A wrapper which can return the output of the penultimate layer in addition to
    the output of the last layer for each forward pass. If the name of the last
    layer is not known, it can determine it automatically. It assumes that the
    last layer is linear and that for every forward pass the last layer is the same.
    If the name of the last layer is known, it can be passed as a parameter at
    initilization; this is the safest way to use this class.
    Based on https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.

    Parameters
    ----------
    model : torch.nn.Module
        PyTorch model
    last_layer_name : str, default=None
        if the name of the last layer is already known, otherwise it will
        be determined automatically.
    """
    def __init__(self, model: nn.Module, last_layer_name: Optional[str] = None) -> None:
        super().__init__()
        self.model = model
        self._features = dict()
        if last_layer_name is None:
            self.last_layer = None
        else:
            self.set_last_layer(last_layer_name)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass. If the last layer is not known yet, it will be
        determined when this function is called for the first time.

        Parameters
        ----------
        x : torch.Tensor
            one batch of data to use as input for the forward pass
        """
        if self.last_layer is None:
            # if this is the first forward pass and last layer is unknown
            out = self.find_last_layer(x)
        else:
            # if last and penultimate layers are already known
            out = self.model(x)
        return out

    def forward_with_features(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass which returns the output of the penultimate layer along
        with the output of the last layer. If the last layer is not known yet,
        it will be determined when this function is called for the first time.

        Parameters
        ----------
        x : torch.Tensor
            one batch of data to use as input for the forward pass
        """
        out = self.forward(x)
        features = self._features[self._last_layer_name]
        return out, features

    def set_last_layer(self, last_layer_name: str) -> None:
        """Set the last layer of the model by its name. This sets the forward
        hook to get the output of the penultimate layer.

        Parameters
        ----------
        last_layer_name : str
            the name of the last layer (fixed in `model.named_modules()`).
        """
        # set last_layer attributes and check if it is linear
        self._last_layer_name = last_layer_name
        self.last_layer = dict(self.model.named_modules())[last_layer_name]
        if not isinstance(self.last_layer, nn.Linear):
            raise ValueError('Use model with a linear last layer.')

        # set forward hook to extract features in future forward passes
        self.last_layer.register_forward_hook(self._get_hook(last_layer_name))

    def _get_hook(self, name: str) -> Callable:
        def hook(_, input, __):
            # only accepts one input (expects linear layer)
            self._features[name] = input[0].detach()
        return hook

    def find_last_layer(self, x: torch.Tensor) -> torch.Tensor:
        """Automatically determines the last layer of the model with one
        forward pass. It assumes that the last layer is the same for every
        forward pass and that it is an instance of `torch.nn.Linear`.
        Might not work with every architecture, but is tested with all PyTorch
        torchvision classification models (besides SqueezeNet, which has no
        linear last layer).

        Parameters
        ----------
        x : torch.Tensor
            one batch of data to use as input for the forward pass
        """
        if self.last_layer is not None:
            raise ValueError('Last layer is already known.')

        act_out = dict()
        def get_act_hook(name):
            def act_hook(_, input, __):
                # only accepts one input (expects linear layer)
                try:
                    act_out[name] = input[0].detach()
                except (IndexError, AttributeError):
                    act_out[name] = None
                # remove hook
                handles[name].remove()
            return act_hook

        # set hooks for all modules
        handles = dict()
        for name, module in self.model.named_modules():
            handles[name] = module.register_forward_hook(get_act_hook(name))

        # check if model has more than one module
        # (there might be pathological exceptions)
        if len(handles) <= 2:
            raise ValueError('The model only has one module.')

        # forward pass to find execution order
        out = self.model(x)

        # find the last layer, store features, return output of forward pass
        keys = list(act_out.keys())
        for key in reversed(keys):
            layer = dict(self.model.named_modules())[key]
            if len(list(layer.children())) == 0:
                self.set_last_layer(key)

                # save features from first forward pass
                self._features[key] = act_out[key]

                return out

        raise ValueError('Something went wrong (all modules have children).')

File Path: dependencies/laplace/laplace/laplace.py
Content:
from laplace.baselaplace import BaseLaplace
from laplace import *


def Laplace(model, likelihood, subset_of_weights='last_layer', hessian_structure='kron',
            *args, **kwargs):
    """Simplified Laplace access using strings instead of different classes.

    Parameters
    ----------
    model : torch.nn.Module
    likelihood : {'classification', 'regression'}
    subset_of_weights : {'last_layer', 'all'}, default='last_layer'
        subset of weights to consider for inference
    hessian_structure : {'diag', 'kron', 'full'}, default='kron'
        structure of the Hessian approximation

    Returns
    -------
    laplace : BaseLaplace
        chosen subclass of BaseLaplace instantiated with additional arguments
    """
    laplace_map = {subclass._key: subclass for subclass in _all_subclasses(BaseLaplace)
                   if hasattr(subclass, '_key')}
    laplace_class = laplace_map[(subset_of_weights, hessian_structure)]
    return laplace_class(model, likelihood, *args, **kwargs)


def _all_subclasses(cls):
    return set(cls.__subclasses__()).union(
        [s for c in cls.__subclasses__() for s in _all_subclasses(c)])

File Path: dependencies/laplace/laplace/lllaplace.py
Content:
import torch
from torch.nn.utils import parameters_to_vector, vector_to_parameters

from laplace.baselaplace import BaseLaplace, FullLaplace, KronLaplace, DiagLaplace
from laplace.feature_extractor import FeatureExtractor

from laplace.matrix import Kron

try:
    from laplace.curvature import BackPackGGN as DefaultBackend
except:
    from laplace.curvature import AsdlGGN as DefaultBackend


__all__ = ['FullLLLaplace', 'KronLLLaplace', 'DiagLLLaplace']


class LLLaplace(BaseLaplace):
    """Baseclass for all last-layer Laplace approximations in this library.
    Subclasses specify the structure of the Hessian approximation.
    See `BaseLaplace` for the full interface.

    A Laplace approximation is represented by a MAP which is given by the
    `model` parameter and a posterior precision or covariance specifying
    a Gaussian distribution \\(\\mathcal{N}(\\theta_{MAP}, P^{-1})\\).
    Here, only the parameters of the last layer of the neural network
    are treated probabilistically.
    The goal of this class is to compute the posterior precision \\(P\\)
    which sums as
    \\[
        P = \\sum_{n=1}^N \\nabla^2_\\theta \\log p(\\mathcal{D}_n \\mid \\theta)
        \\vert_{\\theta_{MAP}} + \\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}}.
    \\]
    Every subclass implements different approximations to the log likelihood Hessians,
    for example, a diagonal one. The prior is assumed to be Gaussian and therefore we have
    a simple form for \\(\\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}} = P_0 \\).
    In particular, we assume a scalar or diagonal prior precision so that in
    all cases \\(P_0 = \\textrm{diag}(p_0)\\) and the structure of \\(p_0\\) can be varied.

    Parameters
    ----------
    model : torch.nn.Module or `laplace.feature_extractor.FeatureExtractor`
    likelihood : {'classification', 'regression'}
        determines the log likelihood Hessian approximation
    sigma_noise : torch.Tensor or float, default=1
        observation noise for the regression setting; must be 1 for classification
    prior_precision : torch.Tensor or float, default=1
        prior precision of a Gaussian prior (= weight decay);
        can be scalar, per-layer, or diagonal in the most general case
    prior_mean : torch.Tensor or float, default=0
        prior mean of a Gaussian prior, useful for continual learning
    temperature : float, default=1
        temperature of the likelihood; lower temperature leads to more
        concentrated posterior and vice versa.
    backend : subclasses of `laplace.curvature.CurvatureInterface`
        backend for access to curvature/Hessian approximations
    last_layer_name: str, default=None
        name of the model's last layer, if None it will be determined automatically
    backend_kwargs : dict, default=None
        arguments passed to the backend on initialization, for example to
        set the number of MC samples for stochastic approximations.
    """
    def __init__(self, model, likelihood, sigma_noise=1., prior_precision=1.,
                 prior_mean=0., temperature=1., backend=DefaultBackend, last_layer_name=None,
                 backend_kwargs=None):
        super().__init__(model, likelihood, sigma_noise=sigma_noise, prior_precision=1.,
                         prior_mean=0., temperature=temperature, backend=backend,
                         backend_kwargs=backend_kwargs)
        self.model = FeatureExtractor(model, last_layer_name=last_layer_name)
        if self.model.last_layer is None:
            self.mean = None
            self.n_params = None
            self.n_layers = None
            # ignore checks of prior mean setter temporarily, check on .fit()
            self._prior_precision = prior_precision
            self._prior_mean = prior_mean
        else:
            self.mean = parameters_to_vector(self.model.last_layer.parameters()).detach()
            self.n_params = len(self.mean)
            self.n_layers = len(list(self.model.last_layer.parameters()))
            self.prior_precision = prior_precision
            self.prior_mean = prior_mean
        self._backend_kwargs['last_layer'] = True

    def fit(self, train_loader):
        """Fit the local Laplace approximation at the parameters of the model.

        Parameters
        ----------
        train_loader : torch.data.utils.DataLoader
            each iterate is a training batch (X, y);
            `train_loader.dataset` needs to be set to access \\(N\\), size of the data set
        """
        if self.H is not None:
            raise ValueError('Already fit.')

        self.model.eval()

        if self.model.last_layer is None:
            X, _ = next(iter(train_loader))
            with torch.no_grad():
                self.model.find_last_layer(X[:1].to(self._device))
            self.mean = parameters_to_vector(self.model.last_layer.parameters()).detach()
            self.n_params = len(self.mean)
            self.n_layers = len(list(self.model.last_layer.parameters()))
            # here, check the already set prior precision again
            self.prior_precision = self._prior_precision
            self.prior_mean = self._prior_mean

        super().fit(train_loader)

    def _glm_predictive_distribution(self, X):
        Js, f_mu = self.backend.last_layer_jacobians(self.model, X)
        f_var = self.functional_variance(Js)
        return f_mu.detach(), f_var.detach()

    def _nn_predictive_samples(self, X, n_samples=100):
        fs = list()
        for sample in self.sample(n_samples):
            vector_to_parameters(sample, self.model.last_layer.parameters())
            fs.append(self.model(X.to(self._device)).detach())
        vector_to_parameters(self.mean, self.model.last_layer.parameters())
        fs = torch.stack(fs)
        if self.likelihood == 'classification':
            fs = torch.softmax(fs, dim=-1)
        return fs

    @property
    def prior_precision_diag(self):
        """Obtain the diagonal prior precision \\(p_0\\) constructed from either
        a scalar or diagonal prior precision.

        Returns
        -------
        prior_precision_diag : torch.Tensor
        """
        if len(self.prior_precision) == 1:  # scalar
            return self.prior_precision * torch.ones_like(self.mean)

        elif len(self.prior_precision) == self.n_params:  # diagonal
            return self.prior_precision

        else:
            raise ValueError('Mismatch of prior and model. Diagonal or scalar prior.')


class FullLLLaplace(LLLaplace, FullLaplace):
    """Last-layer Laplace approximation with full, i.e., dense, log likelihood Hessian approximation
    and hence posterior precision. Based on the chosen `backend` parameter, the full
    approximation can be, for example, a generalized Gauss-Newton matrix.
    Mathematically, we have \\(P \\in \\mathbb{R}^{P \\times P}\\).
    See `FullLaplace`, `LLLaplace`, and `BaseLaplace` for the full interface.
    """
    # key to map to correct subclass of BaseLaplace, (subset of weights, Hessian structure)
    _key = ('last_layer', 'full')

    def __init__(self, model, likelihood, sigma_noise=1., prior_precision=1.,
                 prior_mean=0., temperature=1., backend=DefaultBackend, last_layer_name=None,
                 backend_kwargs=None):
        super().__init__(model, likelihood, sigma_noise, prior_precision,
                         prior_mean, temperature, backend, last_layer_name, backend_kwargs)


class KronLLLaplace(LLLaplace, KronLaplace):
    """Last-layer Laplace approximation with Kronecker factored log likelihood Hessian approximation
    and hence posterior precision.
    Mathematically, we have for the last parameter group, i.e., torch.nn.Linear,
    that \\P\\approx Q \\otimes H\\.
    See `KronLaplace`, `LLLaplace`, and `BaseLaplace` for the full interface and see
    `laplace.matrix.Kron` and `laplace.matrix.KronDecomposed` for the structure of
    the Kronecker factors. `Kron` is used to aggregate factors by summing up and
    `KronDecomposed` is used to add the prior, a Hessian factor (e.g. temperature),
    and computing posterior covariances, marginal likelihood, etc.
    Use of `damping` is possible by initializing or setting `damping=True`.
    """
    # key to map to correct subclass of BaseLaplace, (subset of weights, Hessian structure)
    _key = ('last_layer', 'kron')

    def __init__(self, model, likelihood, sigma_noise=1., prior_precision=1.,
                 prior_mean=0., temperature=1., backend=DefaultBackend, last_layer_name=None,
                 damping=False, **backend_kwargs):
        self.damping = damping
        super().__init__(model, likelihood, sigma_noise, prior_precision,
                         prior_mean, temperature, backend, last_layer_name, backend_kwargs)

    def _init_H(self):
        self.H = Kron.init_from_model(self.model.last_layer, self._device)
        self.n_data_seen = 0
        self.loss = 0


class DiagLLLaplace(LLLaplace, DiagLaplace):
    """Last-layer Laplace approximation with diagonal log likelihood Hessian approximation
    and hence posterior precision.
    Mathematically, we have \\(P \\approx \\textrm{diag}(P)\\).
    See `DiagLaplace`, `LLLaplace`, and `BaseLaplace` for the full interface.
    """
    # key to map to correct subclass of BaseLaplace, (subset of weights, Hessian structure)
    _key = ('last_layer', 'diag')

    def __init__(self, model, likelihood, sigma_noise=1., prior_precision=1.,
                 prior_mean=0., temperature=1., backend=DefaultBackend, last_layer_name=None,
                 backend_kwargs=None):
        super().__init__(model, likelihood, sigma_noise, prior_precision,
                         prior_mean, temperature, backend, last_layer_name, backend_kwargs)

File Path: dependencies/laplace/laplace/matrix.py
Content:
from math import pow
import torch
import numpy as np
from typing import Union
import torch.distributed as dist

from laplace.utils import _is_valid_scalar, symeig, kron, block_diag


class Kron:
    """Kronecker factored approximate curvature representation for a corresponding
    neural network.
    Each element in `kfacs` is either a tuple or single matrix.
    A tuple represents two Kronecker factors \\(Q\\), and \\(H\\) and a single element
    is just a full block Hessian approximation.

    Parameters
    ----------
    kfacs : list[Tuple]
        each element in the list is a Tuple of two Kronecker factors Q, H
        or a single matrix approximating the Hessian (in case of bias, for example)
    """
    def __init__(self, kfacs):
        self.kfacs = kfacs

    @classmethod
    def init_from_model(cls, model, device):
        """Initialize Kronecker factors based on a models architecture.

        Parameters
        ----------
        model : torch.nn.Module
        device : torch.device

        Returns
        -------
        kron : Kron
        """
        kfacs = list()
        for p in model.parameters():
            if p.ndim == 1:  # bias
                P = p.size(0)
                kfacs.append([torch.zeros(P, P, device=device)])
            elif 4 >= p.ndim >= 2:  # fully connected or conv
                if p.ndim == 2:  # fully connected
                    P_in, P_out = p.size()
                elif p.ndim > 2:
                    P_in, P_out = p.shape[0], np.prod(p.shape[1:])

                kfacs.append([
                    torch.zeros(P_in, P_in, device=device),
                    torch.zeros(P_out, P_out, device=device)
                ])
            else:
                raise ValueError('Invalid parameter shape in network.')
        return cls(kfacs)

    def __add__(self, other):
        """Add up Kronecker factors `self` and `other`.

        Parameters
        ----------
        other : Kron

        Returns
        -------
        kron : Kron
        """
        if not isinstance(other, Kron):
            raise ValueError('Can only add Kron to Kron.')

        kfacs = [[Hi.add(Hj) for Hi, Hj in zip(Fi, Fj)]
                 for Fi, Fj in zip(self.kfacs, other.kfacs)]
        return Kron(kfacs)

    def __sub__(self, other):
        """Subtract `other` Kronecker factors from `self`.

        Parameters
        ----------
        other : Kron

        Returns
        -------
        kron : Kron
        """
        if not isinstance(other, Kron):
            raise ValueError('Can only add Kron to Kron.')

        kfacs = [[Hi.sub(Hj) for Hi, Hj in zip(Fi, Fj)]
                 for Fi, Fj in zip(self.kfacs, other.kfacs)]
        return Kron(kfacs)

    def __mul__(self, scalar: Union[float, torch.Tensor]):
        """Multiply all Kronecker factors by scalar.
        The multiplication is distributed across the number of factors
        using `pow(scalar, 1 / len(F))`. `len(F)` is either `1` or `2`.

        Parameters
        ----------
        scalar : float, torch.Tensor

        Returns
        -------
        kron : Kron
        """
        if not _is_valid_scalar(scalar):
            raise ValueError('Input not valid python or torch scalar.')

        # distribute factors evenly so that each group is multiplied by factor
        kfacs = [[pow(scalar, 1/len(F)) * Hi for Hi in F] for F in self.kfacs]
        return Kron(kfacs)

    def __len__(self):
        return len(self.kfacs)

    def decompose(self, damping=False):
        """Eigendecompose Kronecker factors and turn into `KronDecomposed`.
        Parameters
        ----------
        damping : bool
            use damping

        Returns
        -------
        kron_decomposed : KronDecomposed
        """
        eigvecs, eigvals = list(), list()
        for F in self.kfacs:
            Qs, ls = list(), list()
            for Hi in F:
                l, Q = symeig(Hi)
                Qs.append(Q)
                ls.append(l)
            eigvecs.append(Qs)
            eigvals.append(ls)
        return KronDecomposed(eigvecs, eigvals, damping=damping)

    def reduce(self, dst=0):
        """Reduce Kronecker factors across to main worker."""
        for F in self.kfacs:
            for Hi in F:
                dist.reduce(Hi, dst=dst, op=dist.ReduceOp.SUM, async_op=True)

    def _bmm(self, W: torch.Tensor) -> torch.Tensor:
        """Implementation of `bmm` which casts the parameters to the right shape.

        Parameters
        ----------
        W : torch.Tensor
            matrix `(batch, classes, params)`

        Returns
        -------
        SW : torch.Tensor
            result `(batch, classes, params)`
        """
        # self @ W[batch, k, params]
        assert len(W.size()) == 3
        B, K, P = W.size()
        W = W.reshape(B * K, P)
        cur_p = 0
        SW = list()
        for Fs in self.kfacs:
            if len(Fs) == 1:
                Q = Fs[0]
                p = len(Q)
                W_p = W[:, cur_p:cur_p+p].T
                SW.append((Q @ W_p).T)
                cur_p += p
            elif len(Fs) == 2:
                Q, H = Fs
                p_in, p_out = len(Q), len(H)
                p = p_in * p_out
                W_p = W[:, cur_p:cur_p+p].reshape(B * K, p_in, p_out)
                SW.append((Q @ W_p @ H.T).reshape(B * K, p_in * p_out))
                cur_p += p
            else:
                raise AttributeError('Shape mismatch')
        SW = torch.cat(SW, dim=1).reshape(B, K, P)
        return SW

    def bmm(self, W: torch.Tensor, exponent: float = 1) -> torch.Tensor:
        """Batched matrix multiplication with the Kronecker factors.
        If Kron is `H`, we compute `H @ W`.
        This is useful for computing the predictive or a regularization
        based on Kronecker factors as in continual learning.

        Parameters
        ----------
        W : torch.Tensor
            matrix `(batch, classes, params)`
        exponent: float, default=1
            only can be `1` for Kron, requires `KronDecomposed` for other
            exponent values of the Kronecker factors.

        Returns
        -------
        SW : torch.Tensor
            result `(batch, classes, params)`
        """
        if exponent != 1:
            raise ValueError('Only supported after decomposition.')
        if W.ndim == 1:
            return self._bmm(W.unsqueeze(0).unsqueeze(0)).squeeze()
        elif W.ndim == 2:
            return self._bmm(W.unsqueeze(1)).squeeze()
        elif W.ndim == 3:
            return self._bmm(W)
        else:
            raise ValueError('Invalid shape for W')

    def logdet(self) -> torch.Tensor:
        """Compute log determinant of the Kronecker factors and sums them up.
        This corresponds to the log determinant of the entire Hessian approximation.

        Returns
        -------
        logdet : torch.Tensor
        """
        logdet = 0
        for F in self.kfacs:
            if len(F) == 1:
                logdet += F[0].logdet()
            else:  # len(F) == 2
                Hi, Hj = F
                p_in, p_out = len(Hi), len(Hj)
                logdet += p_out * Hi.logdet() + p_in * Hj.logdet()
        return logdet

    def diag(self) -> torch.Tensor:
        """Extract diagonal of the entire Kronecker factorization.

        Returns
        -------
        diag : torch.Tensor
        """
        diags = list()
        for F in self.kfacs:
            if len(F) == 1:
                diags.append(F[0].diagonal())
            else:
                diags.append(torch.ger(F[0].diagonal(), F[1].diagonal()).flatten())
        return torch.cat(diags)

    def flatten(self) -> torch.Tensor:
        """Flatten all matrices and return vector

        Returns
        -------
        flats : torch.Tensor
        """
        flats = list()
        for F in self.kfacs:
            if len(F) == 1:
                flats.append(F[0].flatten())
            else:
                flats.append(F[0].flatten())
                flats.append(F[1].flatten())
        return torch.cat(flats)

    def to_matrix(self) -> torch.Tensor:
        """Make the Kronecker factorization dense by computing the kronecker product.
        Warning: this should only be used for testing purposes as it will allocate
        large amounts of memory for big architectures.

        Returns
        -------
        block_diag : torch.Tensor
        """
        blocks = list()
        for F in self.kfacs:
            if len(F) == 1:
                blocks.append(F[0])
            else:
                blocks.append(kron(F[0], F[1]))
        return block_diag(blocks)

    def detach_(self):
        """ Detaches the Tensor (inplace) from the graph that created it. """

        for F in self.kfacs:
            for Hi in F:
                Hi.detach_()

        return self

    def detach(self):
        """ Returns a new Kron object, detached from the current graph.
        The result will never require gradient. """

        return Kron([[Hi.detach() for Hi in F] for F in self.kfacs])

    def cpu(self):
        """ Returns a new Kron object on cpu. """

        return Kron([[Hi.cpu() for Hi in F] for F in self.kfacs])



    # for commutative operations
    __radd__ = __add__
    __rmul__ = __mul__


class KronDecomposed:
    """Decomposed Kronecker factored approximate curvature representation
    for a corresponding neural network.
    Each matrix in `Kron` is decomposed to obtain `KronDecomposed`.
    Front-loading decomposition allows cheap repeated computation
    of inverses and log determinants.
    In contrast to `Kron`, we can add scalar or layerwise scalars but
    we cannot add other `Kron` or `KronDecomposed` anymore.

    Parameters
    ----------
    eigenvectors : list[Tuple[torch.Tensor]]
        eigenvectors corresponding to matrices in a corresponding `Kron`
    eigenvalues : list[Tuple[torch.Tensor]]
        eigenvalues corresponding to matrices in a corresponding `Kron`
    deltas : torch.Tensor
        addend for each group of Kronecker factors representing, for example,
        a prior precision
    dampen : bool, default=False
        use dampen approximation mixing prior and Kron partially multiplicatively
    """

    def __init__(self, eigenvectors, eigenvalues, deltas=None, damping=False):
        self.eigenvectors = eigenvectors
        self.eigenvalues = eigenvalues
        device = eigenvectors[0][0].device
        if deltas is None:
            self.deltas = torch.zeros(len(self), device=device)
        else:
            self._check_deltas(deltas)
            self.deltas = deltas
        self.damping = damping

    def detach(self):
        self.deltas = self.deltas.detach()
        return self

    def _check_deltas(self, deltas: torch.Tensor):
        if not isinstance(deltas, torch.Tensor):
            raise ValueError('Can only add torch.Tensor to KronDecomposed.')

        if (deltas.ndim == 0  # scalar
            or (deltas.ndim == 1  # vector of length 1 or len(self)
                and (len(deltas) == 1 or len(deltas) == len(self)))):
            return
        else:
            raise ValueError('Invalid shape of delta added to KronDecomposed.')

    def __add__(self, deltas: torch.Tensor):
        """Add scalar per layer or only scalar to Kronecker factors.

        Parameters
        ----------
        deltas : torch.Tensor
            either same length as `eigenvalues` or scalar.

        Returns
        -------
        kron : KronDecomposed
        """
        self._check_deltas(deltas)
        return KronDecomposed(self.eigenvectors, self.eigenvalues, self.deltas + deltas)

    def __mul__(self, scalar):
        """Multiply by a scalar by changing the eigenvalues.
        Same as for the case of `Kron`.

        Parameters
        ----------
        scalar : torch.Tensor or float

        Returns
        -------
        kron : KronDecomposed
        """
        if not _is_valid_scalar(scalar):
            raise ValueError('Invalid argument, can only multiply Kron with scalar.')

        eigenvalues = [[pow(scalar, 1/len(ls)) * l for l in ls] for ls in self.eigenvalues]
        return KronDecomposed(self.eigenvectors, eigenvalues, self.deltas)

    def __len__(self) -> int:
        return len(self.eigenvalues)

    def logdet(self) -> torch.Tensor:
        """Compute log determinant of the Kronecker factors and sums them up.
        This corresponds to the log determinant of the entire Hessian approximation.
        In contrast to `Kron.logdet()`, additive `deltas` corresponding to prior
        precisions are added.

        Returns
        -------
        logdet : torch.Tensor
        """
        logdet = 0
        for ls, delta in zip(self.eigenvalues, self.deltas):
            if len(ls) == 1:  # not KFAC just full
                logdet += torch.log(ls[0] + delta).sum()
            elif len(ls) == 2:
                l1, l2 = ls
                if self.damping:
                    l1d, l2d = l1 + torch.sqrt(delta), l2 + torch.sqrt(delta)
                    logdet += torch.log(torch.ger(l1d, l2d)).sum()
                else:
                    logdet += torch.log(torch.ger(l1, l2) + delta).sum()
            else:
                raise ValueError('Too many Kronecker factors. Something went wrong.')
        return logdet

    def jvp_logdet(self) -> torch.Tensor:
        """Computes the gradient of logdet(KronDecomposed^-1) wrt. Kron to use for
        Jacobian-vector product.

        Returns
        -------
        vec: torch.Tensor
        """
        vecs = list()
        for ls, vs, delta in zip(self.eigenvalues, self.eigenvectors, self.deltas):
            if len(ls) == 1:  # not KFAC, just full (e.g. on biases)
                h_inv = (vs[0] / (ls[0].reshape(1, -1) + delta)) @ vs[0].T
                vecs.append(h_inv.flatten())
            elif len(ls) == 2:  # KFAC jvp
                eig_out_inv = 1 / (torch.outer(ls[0], ls[1]) + delta)
                cp = eig_out_inv @ ls[1]
                vec_q = (vs[0] * cp.reshape(1, -1)) @ vs[0].T
                vecs.append(vec_q.flatten())
                cq = eig_out_inv.T @ ls[0]
                vec_h = (vs[1] * cq.reshape(1, -1)) @ vs[1].T
                vecs.append(vec_h.flatten())
        return torch.cat(vecs)

    def effective_dimensionality(self):
        eff_dims = list()
        for ls, delta in zip(self.eigenvalues, self.deltas):
            if len(ls) == 1:
                l = ls[0]
            elif len(ls) == 2:
                l = torch.outer(ls[0], ls[1])
            eff_dims.append(torch.sum(l / (l + delta)).item())
        return eff_dims

    def _bmm(self, W: torch.Tensor, exponent: float = -1) -> torch.Tensor:
        """Implementation of `bmm`, i.e., `self ** exponent @ W`.

        Parameters
        ----------
        W : torch.Tensor
            matrix `(batch, classes, params)`
        exponent : float
            exponent on `self`

        Returns
        -------
        SW : torch.Tensor
            result `(batch, classes, params)`
        """
        # self @ W[batch, k, params]
        assert len(W.size()) == 3
        B, K, P = W.size()
        W = W.reshape(B * K, P)
        cur_p = 0
        SW = list()
        for ls, Qs, delta in zip(self.eigenvalues, self.eigenvectors, self.deltas):
            if len(ls) == 1:
                Q, l, p = Qs[0], ls[0], len(ls[0])
                ldelta_exp = torch.pow(l + delta, exponent).reshape(-1, 1)
                W_p = W[:, cur_p:cur_p+p].T
                SW.append((Q @ (ldelta_exp * (Q.T @ W_p))).T)
                cur_p += p
            elif len(ls) == 2:
                Q1, Q2 = Qs
                l1, l2 = ls
                p = len(l1) * len(l2)
                if self.damping:
                    l1d, l2d = l1 + torch.sqrt(delta), l2 + torch.sqrt(delta)
                    ldelta_exp = torch.pow(torch.ger(l1d, l2d), exponent).unsqueeze(0)
                else:
                    ldelta_exp = torch.pow(torch.ger(l1, l2) + delta, exponent).unsqueeze(0)
                p_in, p_out = len(l1), len(l2)
                W_p = W[:, cur_p:cur_p+p].reshape(B * K, p_in, p_out)
                W_p = (Q1.T @ W_p @ Q2) * ldelta_exp
                W_p = Q1 @ W_p @ Q2.T
                SW.append(W_p.reshape(B * K, p_in * p_out))
                cur_p += p
            else:
                raise AttributeError('Shape mismatch')
        SW = torch.cat(SW, dim=1).reshape(B, K, P)
        return SW

    def inv_square_form(self, W: torch.Tensor) -> torch.Tensor:
        # W either Batch x K x params or Batch x params
        SW = self._bmm(W, exponent=-1)
        return torch.bmm(W, SW.transpose(1, 2))

    def bmm(self, W: torch.Tensor, exponent: float = -1) -> torch.Tensor:
        """Batched matrix multiplication with the decomposed Kronecker factors.
        This is useful for computing the predictive or a regularization loss.
        Compared to `Kron.bmm`, a prior can be added here in form of `deltas`
        and the exponent can be other than just 1.
        Computes \\(H^{exponent} W\\).

        Parameters
        ----------
        W : torch.Tensor
            matrix `(batch, classes, params)`
        exponent: float, default=1

        Returns
        -------
        SW : torch.Tensor
            result `(batch, classes, params)`
        """
        if W.ndim == 1:
            return self._bmm(W.unsqueeze(0).unsqueeze(0), exponent).squeeze()
        elif W.ndim == 2:
            return self._bmm(W.unsqueeze(1), exponent).squeeze()
        elif W.ndim == 3:
            return self._bmm(W, exponent)
        else:
            raise ValueError('Invalid shape for W')

    def to_matrix(self, exponent: float = 1) -> torch.Tensor:
        """Make the Kronecker factorization dense by computing the kronecker product.
        Warning: this should only be used for testing purposes as it will allocate
        large amounts of memory for big architectures.

        Returns
        -------
        block_diag : torch.Tensor
        """
        blocks = list()
        for Qs, ls, delta in zip(self.eigenvectors, self.eigenvalues, self.deltas):
            if len(ls) == 1:
                Q, l = Qs[0], ls[0]
                blocks.append(Q @ torch.diag(torch.pow(l + delta, exponent)) @ Q.T)
            else:
                Q1, Q2 = Qs
                l1, l2 = ls
                Q = kron(Q1, Q2)
                if self.damping:
                    delta_sqrt = torch.sqrt(delta)
                    l = torch.pow(torch.ger(l1 + delta_sqrt, l2 + delta_sqrt), exponent)
                else:
                    l = torch.pow(torch.ger(l1, l2) + delta, exponent)
                L = torch.diag(l.flatten())
                blocks.append(Q @ L @ Q.T)
        return block_diag(blocks)

    # for commutative operations
    __radd__ = __add__
    __rmul__ = __mul__

File Path: dependencies/laplace/laplace/utils.py
Content:
import logging
from typing import Union
import numpy as np
import torch
import torch.nn.functional as F
from torch.nn import BatchNorm1d, BatchNorm2d, BatchNorm3d
from torch.distributions.multivariate_normal import _precision_to_scale_tril


def get_nll(out_dist, targets):
    return F.nll_loss(torch.log(out_dist), targets)


@torch.no_grad()
def validate(laplace, val_loader, pred_type='glm', link_approx='probit', n_samples=100):
    laplace.model.eval()
    outputs = list()
    targets = list()
    for X, y in val_loader:
        X, y = X.to(laplace._device), y.to(laplace._device)
        out = laplace(X, pred_type=pred_type, link_approx=link_approx, n_samples=n_samples)
        outputs.append(out)
        targets.append(y)
    return torch.cat(outputs, dim=0), torch.cat(targets, dim=0)


def parameters_per_layer(model):
    """Get number of parameters per layer.

    Parameters
    ----------
    model : torch.nn.Module

    Returns
    -------
    params_per_layer : list[int]
    """
    return [np.prod(p.shape) for p in model.parameters()]


def invsqrt_precision(M):
    """Compute ``M^{-0.5}`` as a tridiagonal matrix.

    Parameters
    ----------
    M : torch.Tensor

    Returns
    -------
    M_invsqrt : torch.Tensor
    """
    try:
        return _precision_to_scale_tril(M)
    except:
        l, Q = torch.linalg.eigh
        l = torch.clamp(l, min=1e-12)
        return Q @ torch.diag(l ** (-0.5))


def _is_batchnorm(module):
    if isinstance(module, BatchNorm1d) or \
        isinstance(module, BatchNorm2d) or \
            isinstance(module, BatchNorm3d):
        return True
    return False


def _is_valid_scalar(scalar: Union[float, int, torch.Tensor]) -> bool:
    if np.isscalar(scalar) and np.isreal(scalar):
        return True
    elif torch.is_tensor(scalar) and scalar.ndim <= 1:
        if scalar.ndim == 1 and len(scalar) != 1:
            return False
        return True
    return False


def kron(t1, t2):
    """Computes the Kronecker product between two tensors.

    Parameters
    ----------
    t1 : torch.Tensor
    t2 : torch.Tensor

    Returns
    -------
    kron_product : torch.Tensor
    """
    t1_height, t1_width = t1.size()
    t2_height, t2_width = t2.size()
    out_height = t1_height * t2_height
    out_width = t1_width * t2_width

    tiled_t2 = t2.repeat(t1_height, t1_width)
    expanded_t1 = (
        t1.unsqueeze(2)
          .unsqueeze(3)
          .repeat(1, t2_height, t2_width, 1)
          .view(out_height, out_width)
    )

    return expanded_t1 * tiled_t2


def diagonal_add_scalar(X, value):
    """Add scalar value `value` to diagonal of `X`.

    Parameters
    ----------
    X : torch.Tensor
    value : torch.Tensor or float

    Returns
    -------
    X_add_scalar : torch.Tensor
    """
    if not X.device == torch.device('cpu'):
        indices = torch.cuda.LongTensor([[i, i] for i in range(X.shape[0])])
    else:
        indices = torch.LongTensor([[i, i] for i in range(X.shape[0])])
    values = X.new_ones(X.shape[0]).mul(value)
    return X.index_put(tuple(indices.t()), values, accumulate=True)

    
def batch_diagonal_add_scalar(X, value):
    """Like `diagonal_add_scalar(X, value) but with first dimension as batch."""
    if not X.device == torch.device('cpu'):
        indices = torch.cuda.LongTensor([[b, i, i] for b in range(X.shape[0]) 
                                         for i in range(X.shape[1])])
    else:
        indices = torch.LongTensor([[b, i, i] for b in range(X.shape[0]) 
                                    for i in range(X.shape[1])])
    values = X.new_ones(X.shape[0] * X.shape[1]).mul(value)
    return X.index_put(tuple(indices.t()), values, accumulate=True)


def symeig(M):
    """Symetric eigendecomposition avoiding failure cases by
    adding and removing jitter to the diagonal.

    Parameters
    ----------
    M : torch.Tensor

    Returns
    -------
    L : torch.Tensor
        eigenvalues
    W : torch.Tensor
        eigenvectors
    """
    try:
        L, W = torch.linalg.eigh(M, UPLO='U')
    except RuntimeError:  # did not converge
        logging.info('SYMEIG: adding jitter, did not converge.')
        # use W L W^T + I = W (L + I) W^T
        M = M + torch.eye(M.shape[0]).to(M.device)
        try:
            L, W = torch.linalg.eigh(M, UPLO='U')
            L = L - 1
        except RuntimeError:
            stats = f'diag: {M.diagonal()}, max: {M.abs().max()}, '
            stats = stats + f'min: {M.abs().min()}, mean: {M.abs().mean()}'
            logging.info(f'SYMEIG: adding jitter failed. Stats: {stats}')
            exit()
    # eigenvalues of symeig at least 0
    L = L.clamp(min=0.0)
    L = torch.nan_to_num(L)
    W = torch.nan_to_num(W)
    return L, W


def block_diag(blocks):
    """Compose block-diagonal matrix of individual blocks.

    Parameters
    ----------
    blocks : list[torch.Tensor]

    Returns
    -------
    M : torch.Tensor
    """
    P = sum([b.shape[0] for b in blocks])
    M = torch.zeros(P, P)
    p_cur = 0
    for block in blocks:
        p_block = block.shape[0]
        M[p_cur:p_cur+p_block, p_cur:p_cur+p_block] = block
        p_cur += p_block
    return M

File Path: dependencies/laplace/regression_example.py
Content:
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import DataLoader, TensorDataset

from laplace import Laplace

n_epochs = 1000
batch_size = 150  # full batch
true_sigma_noise = 0.3
torch.manual_seed(711)

# create simple sinusoid data set
X_train = (torch.rand(150) * 8).unsqueeze(-1)
y_train = torch.sin(X_train) + torch.randn_like(X_train) * true_sigma_noise
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size)
X_test = torch.linspace(-5, 13, 500).unsqueeze(-1)  # +-5 on top of the training X-range

# create and train MAP model
model = torch.nn.Sequential(torch.nn.Linear(1, 50),
                            torch.nn.Tanh(),
                            torch.nn.Linear(50, 1))

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4, lr=1e-2)
for i in range(n_epochs):
    for X, y in train_loader:
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()

la = Laplace(model, 'regression', subset_of_weights='all', hessian_structure='full')
la.fit(train_loader)
log_prior, log_sigma = torch.ones(1, requires_grad=True), torch.ones(1, requires_grad=True)
hyper_optimizer = torch.optim.Adam([log_prior, log_sigma], lr=1e-1)
for i in range(n_epochs):
    hyper_optimizer.zero_grad()
    neg_marglik = - la.log_marginal_likelihood(log_prior.exp(), log_sigma.exp())
    neg_marglik.backward()
    hyper_optimizer.step()
print('sigma:', log_sigma.exp().item(), '; prior precision:', log_prior.exp().item())

x = X_test.flatten().cpu().numpy()
f_mu, f_var = la(X_test)
f_mu = f_mu.squeeze().detach().cpu().numpy()
f_sigma = f_var.squeeze().sqrt().cpu().numpy()
pred_std = np.sqrt(f_sigma**2 + la.sigma_noise.item()**2)

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True,
                               figsize=(4.5, 2.8))
ax1.set_title('MAP')
ax1.scatter(X_train.flatten(), y_train.flatten(), alpha=0.7, color='tab:orange')
ax1.plot(x, f_mu, color='black', label='$f_{MAP}$')
ax1.legend()

ax2.set_title('LA')
ax2.scatter(X_train.flatten(), y_train.flatten(), alpha=0.7, color='tab:orange')
ax2.plot(x, f_mu, label='$\mathbb{E}[f]$')
ax2.fill_between(x, f_mu-pred_std*2, f_mu+pred_std*2, 
                 alpha=0.3, color='tab:blue', label='$2\sqrt{\mathbb{V}\,[f]}$')
ax2.legend()
ax1.set_ylim([-4, 6])
ax1.set_xlim([x.min(), x.max()])
ax2.set_xlim([x.min(), x.max()])
ax1.set_ylabel('$y$')
ax1.set_xlabel('$x$')
ax2.set_xlabel('$x$')
plt.tight_layout()
plt.savefig('docs/regression_example.png', dpi=300)

File Path: dependencies/laplace/setup.py
Content:
import setuptools

if __name__ == "__main__":
    setuptools.setup()

File Path: dependencies/laplace/tests/__init__.py
Content:

File Path: dependencies/laplace/tests/test_baselaplace.py
Content:
import pytest
from itertools import product
import numpy as np
import torch
from torch import nn
from torch.nn.utils import parameters_to_vector
from torch.utils.data import DataLoader, TensorDataset, Subset
from torch.distributions import Normal, Categorical
from asdfghjkl.operations import Bias, Scale
from laplace.curvature.asdl import AsdlEF, AsdlGGN
from laplace.curvature.augmented_backpack import AugBackPackGGN
from laplace.curvature.backpack import BackPackEF, BackPackGGN
from laplace.curvature.augmented_asdl import AugAsdlEF, AugAsdlGGN

from laplace.laplace import FullLaplace, KronLaplace, DiagLaplace, BlockDiagLaplace
from laplace.baselaplace import FunctionalLaplace
from tests.utils import jacobians_naive, HetHead


torch.manual_seed(240)
torch.set_default_tensor_type(torch.DoubleTensor)
flavors = [FullLaplace, KronLaplace, DiagLaplace]
backends = [BackPackGGN, BackPackEF]


def get_grad(model):
    return torch.cat([e.grad.flatten() for e in model.parameters()])


@pytest.fixture
def model():
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def model_single():
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.ReLU(), nn.Linear(20, 1))
    setattr(model, 'output_size', 1)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def model_single_fixup():
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.ReLU(), Scale(), Bias(), nn.Linear(20, 1))
    setattr(model, 'output_size', 1)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def class_loader():
    X = torch.randn(10, 3)
    y = torch.randint(2, (10,))
    return DataLoader(TensorDataset(X, y), batch_size=3)


@pytest.fixture
def reg_loader():
    X = torch.randn(10, 3)
    y = torch.randn(10, 2)
    return DataLoader(TensorDataset(X, y), batch_size=3)


@pytest.fixture
def reg_loader_single():
    X = torch.randn(10, 3)
    y = torch.randn(10, 1)
    return DataLoader(TensorDataset(X, y), batch_size=3)


@pytest.fixture
def aug_class_loader():
    X = torch.randn(12, 7, 3)
    y = torch.randint(2, (12,))
    return DataLoader(TensorDataset(X, y), batch_size=3, shuffle=True)


@pytest.fixture
def aug_reg_loader():
    X = torch.randn(12, 7, 3)
    y = torch.randn(12, 2)
    return DataLoader(TensorDataset(X, y), batch_size=3, shuffle=True)


@pytest.fixture
def aug_reg_loader_single():
    X = torch.randn(12, 7, 3)
    y = torch.randn(12, 1)
    return DataLoader(TensorDataset(X, y), batch_size=3, shuffle=True)


@pytest.fixture
def het_model():
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Tanh(), nn.Linear(20, 2), HetHead())
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def het_reg_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 3)
    y = torch.randn(10, 1)
    return X, y


@pytest.mark.parametrize('backend', backends)
def test_functional_laplace_regression(backend, model):
    X = torch.randn(10, 3)
    y = torch.randn(10, 2)
    full_lap = FullLaplace(model, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=backend)
    fun_lap = FunctionalLaplace(model, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=backend)
    full_lap.fit(DataLoader(TensorDataset(X, y)))
    fun_lap.fit_batch(X, y, len(y))
    assert torch.allclose(fun_lap.log_marginal_likelihood(), full_lap.log_marginal_likelihood())


def test_functional_indep_laplace_regression(model_single):
    X = torch.randn(10, 3)
    y = torch.randn(10, 1)
    full_lap = FullLaplace(model_single, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=AsdlGGN)
    fun_lap = FunctionalLaplace(model_single, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=AsdlGGN, 
                                independent=True, backend_kwargs=dict(kron_jac=True))
    full_lap.fit(DataLoader(TensorDataset(X, y)))
    fun_lap.fit_batch(X, y, len(y))
    assert torch.allclose(fun_lap.log_marginal_likelihood(), full_lap.log_marginal_likelihood())


def test_functional_indep_laplace_regression_fixup(model_single_fixup):
    model_single = model_single_fixup
    X = torch.randn(10, 3)
    y = torch.randn(10, 1)
    full_lap = FullLaplace(model_single, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=AsdlGGN)
    fun_lap = FunctionalLaplace(model_single, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=AsdlGGN, 
                                independent=True, backend_kwargs=dict(kron_jac=True))
    full_lap.fit(DataLoader(TensorDataset(X, y)))
    fun_lap.fit_batch(X, y, len(y))
    assert torch.allclose(fun_lap.log_marginal_likelihood(), full_lap.log_marginal_likelihood())


def test_functional_single_laplace_regression(model_single):
    X = torch.randn(10, 3)
    y = torch.randn(10, 2)
    fun_lap = FunctionalLaplace(model_single, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=AsdlGGN, independent=True)
    fun_lap_s = FunctionalLaplace(model_single, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=AsdlGGN, 
                                independent=True, single_output=True)
    fun_lap.fit_batch(X, y, len(y))
    fun_lap_s.fit_batch(X, y, len(y))
    assert torch.allclose(fun_lap.log_marginal_likelihood(), fun_lap_s.log_marginal_likelihood())


def test_functional_laplace_hetregression(het_model):
    X = torch.randn(10, 3)
    y = torch.randn(10, 1)
    full_lap = FullLaplace(het_model, 'heteroscedastic_regression', prior_precision=0.3, backend=AsdlGGN)
    fun_lap = FunctionalLaplace(het_model, 'heteroscedastic_regression', prior_precision=0.3, backend=AsdlGGN)
    full_lap.fit(DataLoader(TensorDataset(X, y)))
    fun_lap.fit_batch(X, y, len(y))
    assert torch.allclose(fun_lap.log_marginal_likelihood(), full_lap.log_marginal_likelihood())
    fun_lap = FunctionalLaplace(het_model, 'heteroscedastic_regression', prior_precision=0.3, backend=AsdlGGN, independent=True)
    fun_lap.fit_batch(X, y, len(y))
    assert fun_lap.log_marginal_likelihood() < full_lap.log_marginal_likelihood()
    fun_lap = FunctionalLaplace(het_model, 'heteroscedastic_regression', prior_precision=0.3, backend=AsdlGGN, independent=True, single_output=True)
    fun_lap.fit_batch(X, y, len(y))
    assert fun_lap.log_marginal_likelihood() < full_lap.log_marginal_likelihood()


def test_functional_laplace_restriction(model):
    with pytest.raises(ValueError):
        FunctionalLaplace(model, 'classification', independent=False, single_output=True)


@pytest.mark.parametrize('backend', backends)
def test_functional_laplace_classification(backend, model):
    X = torch.randn(10, 3)
    y = torch.randint(2, (10,))
    full_lap = FullLaplace(model, 'classification', prior_precision=0.3, backend=backend)
    fun_lap = FunctionalLaplace(model, 'classification', prior_precision=0.3, backend=backend)
    full_lap.fit(DataLoader(TensorDataset(X, y)))
    fun_lap.fit_batch(X, y, len(y))
    assert torch.allclose(fun_lap.log_marginal_likelihood(), full_lap.log_marginal_likelihood())

    
@pytest.mark.parametrize('backend', backends)
def test_functional_laplace_additivity_regression(backend, model):
    X = torch.randn(10, 3)
    y = torch.randn(10, 2)
    fun_lap = FunctionalLaplace(model, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=backend, sod=True)
    fun_lap.fit_batch(X, y, 20)
    fun_lap_add = FunctionalLaplace(model, 'regression', sigma_noise=1.8, prior_precision=0.3, backend=backend)
    fun_lap_add.fit_batch(X, y, 20)
    fun_lap_add.fit_batch(X, y, 20)
    assert torch.allclose(fun_lap.log_likelihood, fun_lap_add.log_likelihood)
    assert torch.allclose(fun_lap.log_marginal_likelihood(), fun_lap_add.log_marginal_likelihood())


@pytest.mark.parametrize('backend', backends)
def test_functional_laplace_additivity_classification(backend, model):
    X = torch.randn(10, 3)
    y = torch.randint(2, (10,))
    fun_lap = FunctionalLaplace(model, 'classification', prior_precision=0.3, backend=backend, sod=True)
    fun_lap.fit_batch(X, y, 20)
    fun_lap_add = FunctionalLaplace(model, 'classification', prior_precision=0.3, backend=backend)
    fun_lap_add.fit_batch(X, y, 20)
    fun_lap_add.fit_batch(X, y, 20)
    assert torch.allclose(fun_lap.log_marginal_likelihood(), fun_lap_add.log_marginal_likelihood())


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init(laplace, model):
    lap = laplace(model, 'classification')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_invalid_likelihood(laplace, model):
    with pytest.raises(ValueError):
        lap = laplace(model, 'otherlh')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_noise(laplace, model):
    # float
    sigma_noise = 1.2
    lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise)
    # torch.tensor 0-dim
    sigma_noise = torch.tensor(1.2)
    lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise)
    # torch.tensor 1-dim
    sigma_noise = torch.tensor(1.2).reshape(-1)
    lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise)

    # for classification should fail
    sigma_noise = 1.2
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='classification', sigma_noise=sigma_noise)

    # other than that should fail
    # higher dim
    sigma_noise = torch.tensor(1.2).reshape(1, 1)
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise)
    # other datatype, only reals supported
    sigma_noise = '1.2'
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise)


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_precision(laplace, model):
    # float
    precision = 10.6
    lap = laplace(model, likelihood='regression', prior_precision=precision)
    # torch.tensor 0-dim
    precision = torch.tensor(10.6)
    lap = laplace(model, likelihood='regression', prior_precision=precision)
    # torch.tensor 1-dim
    precision = torch.tensor(10.7).reshape(-1)
    lap = laplace(model, likelihood='regression', prior_precision=precision)
    # torch.tensor 1-dim param-shape
    precision = torch.tensor(10.7).reshape(-1).repeat(model.n_params)
    if laplace == KronLaplace:
        # Kron should not accept per parameter prior precision
        with pytest.raises(ValueError):
            lap = laplace(model, likelihood='regression', prior_precision=precision)
    else:
        lap = laplace(model, likelihood='regression', prior_precision=precision)
    # torch.tensor 1-dim layer-shape
    precision = torch.tensor(10.7).reshape(-1).repeat(model.n_layers)
    lap = laplace(model, likelihood='regression', prior_precision=precision)

    # other than that should fail
    # higher dim
    precision = torch.tensor(10.6).reshape(1, 1)
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', prior_precision=precision)
    # unmatched dim
    precision = torch.tensor(10.6).reshape(-1).repeat(17)
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', prior_precision=precision)
    # other datatype, only reals supported
    precision = '1.5'
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', prior_precision=precision)


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_prior_mean_and_scatter(laplace, model):
    mean = parameters_to_vector(model.parameters())
    P = len(mean)
    lap_scalar_mean = laplace(model, 'classification',
                              prior_precision=1e-2, prior_mean=1.)
    assert torch.allclose(lap_scalar_mean.prior_mean, torch.tensor([1.]))
    lap_tensor_mean = laplace(model, 'classification',
                              prior_precision=1e-2, prior_mean=torch.ones(1))
    assert torch.allclose(lap_tensor_mean.prior_mean, torch.tensor([1.]))
    lap_tensor_scalar_mean = laplace(model, 'classification',
                                     prior_precision=1e-2, prior_mean=torch.ones(1)[0])
    assert torch.allclose(lap_tensor_scalar_mean.prior_mean, torch.tensor(1.))
    lap_tensor_full_mean = laplace(model, 'classification',
                                   prior_precision=1e-2, prior_mean=torch.ones(P))
    assert torch.allclose(lap_tensor_full_mean.prior_mean, torch.ones(P))
    expected = ((mean - 1) * 1e-2) @ (mean - 1)
    assert expected.ndim == 0
    assert torch.allclose(lap_scalar_mean.scatter, expected)
    assert lap_scalar_mean.scatter.shape == expected.shape
    assert torch.allclose(lap_tensor_mean.scatter, expected)
    assert lap_tensor_mean.scatter.shape == expected.shape
    assert torch.allclose(lap_tensor_scalar_mean.scatter, expected)
    assert lap_tensor_scalar_mean.scatter.shape == expected.shape
    assert torch.allclose(lap_tensor_full_mean.scatter, expected)
    assert lap_tensor_full_mean.scatter.shape == expected.shape

    # too many dims
    with pytest.raises(ValueError):
        prior_mean = torch.ones(P).unsqueeze(-1)
        laplace(model, 'classification', prior_precision=1e-2, prior_mean=prior_mean)

    # unmatched dim
    with pytest.raises(ValueError):
        prior_mean = torch.ones(P-3)
        laplace(model, 'classification', prior_precision=1e-2, prior_mean=prior_mean)

    # invalid argument type
    with pytest.raises(ValueError):
        laplace(model, 'classification', prior_precision=1e-2, prior_mean='72')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_temperature(laplace, model):
    # valid float
    T = 1.1
    lap = laplace(model, likelihood='classification', temperature=T)
    assert lap.temperature == T


@pytest.mark.parametrize('laplace,lh', product(flavors, ['classification', 'regression']))
def test_laplace_functionality(laplace, lh, model, reg_loader, class_loader):
    if lh == 'classification':
        loader = class_loader
        sigma_noise = 1.
    else:
        loader = reg_loader
        sigma_noise = 0.3
    lap = laplace(model, lh, sigma_noise=sigma_noise, prior_precision=0.7)
    lap.fit(loader)
    assert lap.n_data == len(loader.dataset)
    assert lap.n_outputs == model.output_size
    f = model(loader.dataset.tensors[0])
    y = loader.dataset.tensors[1]
    assert f.shape == torch.Size([10, 2])

    # Test log likelihood (Train)
    log_lik = lap.log_likelihood
    # compute true log lik
    if lh == 'classification':
        log_lik_true = Categorical(logits=f).log_prob(y).sum()
        assert torch.allclose(log_lik, log_lik_true)
    else:
        assert y.size() == f.size()
        log_lik_true = Normal(loc=f, scale=sigma_noise).log_prob(y).sum()
        assert torch.allclose(log_lik, log_lik_true)
        # change likelihood and test again
        lap.sigma_noise = 0.72
        log_lik = lap.log_likelihood
        log_lik_true = Normal(loc=f, scale=0.72).log_prob(y).sum()
        assert torch.allclose(log_lik, log_lik_true)

    # Test marginal likelihood
    # lml = log p(y|f) - 1/2 theta @ prior_prec @ theta
    #       + 1/2 logdet prior_prec - 1/2 log det post_prec
    lml = log_lik_true
    theta = parameters_to_vector(model.parameters()).detach()
    assert torch.allclose(theta, lap.mean)
    prior_prec = torch.diag(lap.prior_precision_diag)
    assert prior_prec.shape == torch.Size([len(theta), len(theta)])
    lml = lml - 1/2 * theta @ prior_prec @ theta
    Sigma_0 = torch.inverse(prior_prec)
    if laplace == DiagLaplace:
        log_det_post_prec = lap.posterior_precision.log().sum()
    else:
        log_det_post_prec = lap.posterior_precision.logdet()
    lml = lml + 1/2 * (prior_prec.logdet() - log_det_post_prec)
    assert torch.allclose(lml, lap.log_marginal_likelihood())

    # test sampling
    torch.manual_seed(61)
    samples = lap.sample(n_samples=1)
    assert samples.shape == torch.Size([1, len(theta)])
    samples = lap.sample(n_samples=1000000)
    assert samples.shape == torch.Size([1000000, len(theta)])
    mu_comp = samples.mean(dim=0)
    mu_true = lap.mean
    assert torch.allclose(mu_comp, mu_true, rtol=1, atol=1e-3)

    # test functional variance
    if laplace == FullLaplace:
        Sigma = lap.posterior_covariance
    elif laplace == KronLaplace:
        Sigma = lap.posterior_precision.to_matrix(exponent=-1)
    elif laplace == DiagLaplace:
        Sigma = torch.diag(lap.posterior_variance)
    Js, f = jacobians_naive(model, loader.dataset.tensors[0])
    true_f_var = torch.einsum('mkp,pq,mcq->mkc', Js, Sigma, Js)
    comp_f_var = lap.functional_variance(Js)
    assert torch.allclose(true_f_var, comp_f_var, rtol=1e-4)


@pytest.mark.parametrize('laplace', flavors)
def test_regression_predictive(laplace, model, reg_loader):
    lap = laplace(model, 'regression', sigma_noise=0.3, prior_precision=0.7)
    lap.fit(reg_loader)
    X, y = reg_loader.dataset.tensors
    f = model(X)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive, functional variance tested already above.
    f_mu, f_var = lap(X, pred_type='glm')
    assert torch.allclose(f_mu, f)
    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])
    assert len(f_mu) == len(X)

    # NN predictive (only diagonal variance estimation)
    f_mu, f_var = lap(X, pred_type='nn')
    assert f_mu.shape == f_var.shape
    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1]])
    assert len(f_mu) == len(X)


@pytest.mark.parametrize('laplace', flavors)
def test_classification_predictive(laplace, model, class_loader):
    lap = laplace(model, 'classification', prior_precision=0.7)
    lap.fit(class_loader)
    X, y = class_loader.dataset.tensors
    f = torch.softmax(model(X), dim=-1)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive
    f_pred = lap(X, pred_type='glm', link_approx='mc', n_samples=100)
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1
    f_pred = lap(X, pred_type='glm', link_approx='probit')
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1
    f_pred = lap(X, pred_type='glm', link_approx='bridge')
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1


    # NN predictive
    f_pred = lap(X, pred_type='nn', n_samples=100)
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1


@pytest.mark.parametrize('laplace', flavors)
def test_regression_predictive_samples(laplace, model, reg_loader):
    lap = laplace(model, 'regression', sigma_noise=0.3, prior_precision=0.7)
    lap.fit(reg_loader)
    X, y = reg_loader.dataset.tensors
    f = model(X)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive, functional variance tested already above.
    fsamples = lap.predictive_samples(X, pred_type='glm', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])

    # NN predictive (only diagonal variance estimation)
    fsamples = lap.predictive_samples(X, pred_type='nn', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])


@pytest.mark.parametrize('laplace', flavors)
def test_classification_predictive_samples(laplace, model, class_loader):
    lap = laplace(model, 'classification', prior_precision=0.7)
    lap.fit(class_loader)
    X, y = class_loader.dataset.tensors
    f = torch.softmax(model(X), dim=-1)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive
    fsamples = lap.predictive_samples(X, pred_type='glm', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])
    assert np.allclose(fsamples.sum().item(), len(f) * 100)  # sum up to 1

    # NN predictive
    f_pred = lap.predictive_samples(X, pred_type='nn', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])
    assert np.allclose(fsamples.sum().item(), len(f) * 100)  # sum up to 1

    
@pytest.mark.parametrize('kron_jac', [True, False])
def test_marglik_indep_kernel(model_single, reg_loader_single, kron_jac):
    prior_prec = 0.089
    sigma_noise = 0.27
    backend_kwargs = dict(differentiable=True, kron_jac=kron_jac)
    # control with full laplace
    full_lap = FullLaplace(model_single, 'regression', backend=AsdlGGN, 
                           backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                           sigma_noise=sigma_noise)
    full_lap.fit(reg_loader_single)
    model_single.zero_grad()
    marglik = full_lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model_single).clone()
    # compare to kernel laplace
    kernel_lap = FunctionalLaplace(model_single, 'regression', backend=AsdlGGN, 
                                   backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                                   sigma_noise=sigma_noise, independent=True)
    # need  full batch
    kernel_lap.fit(DataLoader(reg_loader_single.dataset, batch_size=len(reg_loader_single.dataset)))
    model_single.zero_grad()
    kernel_marglik = kernel_lap.log_marginal_likelihood()
    kernel_marglik.backward()
    kernel_grad = get_grad(model_single).clone()
    assert torch.allclose(marglik, kernel_marglik)
    assert torch.allclose(grad, kernel_grad)

    
@pytest.mark.parametrize('lh', ['classification', 'regression'])
def test_marglik_indep_kernel_kron_jac(model, reg_loader, class_loader, lh):
    loader = reg_loader if lh == 'regression' else class_loader
    prior_prec = 2/3
    sigma_noise = 0.3 if lh == 'regression' else 1.0
    # for linear model both should be equivalent
    # with kron_jac
    backend_kwargs = dict(differentiable=True, kron_jac=True)
    kernel_lap = FunctionalLaplace(model, lh, backend=AsdlGGN, 
                                   backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                                   sigma_noise=sigma_noise, independent=True)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    model.zero_grad()
    marglik_kron = kernel_lap.log_marginal_likelihood()
    marglik_kron.backward()
    grad_kron = get_grad(model).clone()
    # without kron_jac
    backend_kwargs = dict(differentiable=True, kron_jac=False)
    kernel_lap = FunctionalLaplace(model, lh, backend=AsdlGGN, 
                                   backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                                   sigma_noise=sigma_noise, independent=True)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    model.zero_grad()
    marglik = kernel_lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model).clone()
    assert torch.allclose(marglik, marglik_kron)
    assert torch.allclose(grad, grad_kron)
    

@pytest.mark.parametrize('backend,lh', 
                         product([AsdlGGN, AsdlEF, BackPackGGN, BackPackEF], 
                                 ['classification', 'regression']))
def test_marglik_kernel_vs_full(model, backend, lh, class_loader, reg_loader):
    loader = reg_loader if lh == 'regression' else class_loader
    prior_prec = 2/3
    sigma_noise = 0.3 if lh == 'regression' else 1.0
    backend_kwargs = dict(differentiable=True)
    # control with full laplace
    full_lap = FullLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                           prior_precision=prior_prec, sigma_noise=sigma_noise)
    full_lap.fit(loader)
    model.zero_grad()
    marglik = full_lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model).clone()
    # compare to kernel laplace
    kernel_lap = FunctionalLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                                   prior_precision=prior_prec, sigma_noise=sigma_noise)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    model.zero_grad()
    kernel_marglik = kernel_lap.log_marginal_likelihood()
    kernel_marglik.backward()
    kernel_grad = get_grad(model).clone()
    assert torch.allclose(marglik, kernel_marglik)
    assert torch.allclose(grad, kernel_grad)


@pytest.mark.parametrize('backend,lh', product([AsdlGGN, AsdlEF], ['classification', 'regression']))
def test_marglik_bound_ordering(model, backend, lh, class_loader, reg_loader):
    loader = reg_loader if lh == 'regression' else class_loader
    prior_prec = 2/3
    sigma_noise = 0.3 if lh == 'regression' else 1.0
    backend_kwargs = dict(differentiable=False)
    # control with full laplace
    full_lap = FullLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                           prior_precision=prior_prec, sigma_noise=sigma_noise)
    full_lap.fit(loader)
    model.zero_grad()
    marglik = full_lap.log_marginal_likelihood()

    # control functional
    kernel_lap = FunctionalLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                                   prior_precision=prior_prec, sigma_noise=sigma_noise)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    kernel_marglik = kernel_lap.log_marginal_likelihood()
    assert torch.allclose(marglik, kernel_marglik)

    ## parametric bounds
    # blkdiag should be lower
    blk_lap = BlockDiagLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                               prior_precision=prior_prec, sigma_noise=sigma_noise)
    blk_lap.fit(loader)
    model.zero_grad()
    marglik_blk = blk_lap.log_marginal_likelihood()
    assert marglik_blk < marglik
    # kron should be lower
    kron_lap = KronLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                           prior_precision=prior_prec, sigma_noise=sigma_noise)
    kron_lap.fit(loader)
    model.zero_grad()
    marglik_kron = kron_lap.log_marginal_likelihood()
    assert marglik_kron <= marglik
    # diag should be even lower
    diag_lap = DiagLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                           prior_precision=prior_prec, sigma_noise=sigma_noise)
    diag_lap.fit(loader)
    model.zero_grad()
    marglik_diag = diag_lap.log_marginal_likelihood()
    assert marglik_diag <= marglik_kron


@pytest.mark.parametrize('backend,lap', 
                         product([AsdlGGN, AsdlEF], 
                                 [FullLaplace, FunctionalLaplace, KronLaplace, DiagLaplace]))
def test_marglik_sod_bounds(model, backend, lap, class_loader):
    lh = 'classification'
    loader = class_loader
    prior_prec = 2/3
    backend_kwargs = dict(differentiable=False)
    # full data
    full_loader = DataLoader(loader.dataset, batch_size=len(loader.dataset))
    la = lap(model, lh, backend=backend, backend_kwargs=backend_kwargs, 
             prior_precision=prior_prec)
    la.fit(full_loader)
    marglik = la.log_marginal_likelihood().item()

    # sod
    N = len(loader.dataset)
    sodloader = DataLoader(Subset(loader.dataset, range(int(N/2))), batch_size=int(N/2))
    sodloader2 = DataLoader(Subset(loader.dataset, range(int(N/2), N)), batch_size=int(N/2))
    la = lap(model, lh, backend=backend, backend_kwargs=backend_kwargs, 
             prior_precision=prior_prec, sod=True)
    la.fit(sodloader)
    la.n_data = N
    marglik_sod_a = la.log_marginal_likelihood().item()
    la.fit(sodloader2)
    la.n_data = N
    marglik_sod_b = la.log_marginal_likelihood().item()
    marglik_sod = (marglik_sod_a + marglik_sod_b) / 2
    assert marglik_sod <= marglik

    if backend == AsdlEF:
        return
    # sod single output for ggn
    if lap == FunctionalLaplace:
        la = lap(model, lh, backend=backend, backend_kwargs=backend_kwargs, 
                prior_precision=prior_prec, sod=True, single_output=True, independent=True)
    else:
        la = lap(model, lh, backend=backend, backend_kwargs=backend_kwargs, 
                prior_precision=prior_prec, sod=True, single_output=True)
    marglik_sod_single = 0
    torch.manual_seed(1)
    for loader in [sodloader, sodloader2]:
        la.fit(loader)
        la.n_data = N
        marglik_sod_single += la.log_marginal_likelihood().item() / 2
    assert marglik_sod_single < marglik_sod

    
@pytest.mark.parametrize('backend,lh', 
                         product([AsdlGGN, AsdlEF, BackPackGGN, BackPackEF], 
                                 ['classification', 'regression']))
def test_marglik_kernel_vs_full_sod(model, backend, lh, class_loader, reg_loader):
    loader = reg_loader if lh == 'regression' else class_loader
    prior_prec = 2/3
    sigma_noise = 0.3 if lh == 'regression' else 1.0
    backend_kwargs = dict(differentiable=True)
    # control with full laplace
    full_lap = FullLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                           prior_precision=prior_prec, sigma_noise=sigma_noise, sod=True)
    full_lap.fit(loader)
    # as if the dataset has 2x more data but loader only holds half
    full_lap.n_data *= 2
    model.zero_grad()
    marglik = full_lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model).clone()
    # compare to kernel laplace
    kernel_lap = FunctionalLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                                   prior_precision=prior_prec, sigma_noise=sigma_noise, sod=True)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    kernel_lap.n_data *= 2  
    model.zero_grad()
    kernel_marglik = kernel_lap.log_marginal_likelihood()
    kernel_marglik.backward()
    kernel_grad = get_grad(model).clone()
    assert torch.allclose(marglik, kernel_marglik)
    assert torch.allclose(grad, kernel_grad)



@pytest.mark.parametrize('backend,lh', 
                         product([AugAsdlGGN, AugAsdlEF, AugBackPackGGN],
                                 ['classification', 'regression']))
def test_marglik_kernel_vs_full_aug(model, backend, lh, aug_class_loader, aug_reg_loader):
    loader = aug_reg_loader if lh == 'regression' else aug_class_loader
    prior_prec = 0.7
    sigma_noise = 0.3 if lh == 'regression' else 1.0
    backend_kwargs = dict(differentiable=True)
    # control with full laplace
    full_lap = FullLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                           prior_precision=prior_prec, sigma_noise=sigma_noise)
    full_lap.fit(loader)
    model.zero_grad()
    marglik = full_lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model).clone()
    # compare to kernel laplace
    kernel_lap = FunctionalLaplace(model, lh, backend=backend, backend_kwargs=backend_kwargs,
                                   prior_precision=prior_prec, sigma_noise=sigma_noise)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    model.zero_grad()
    kernel_marglik = kernel_lap.log_marginal_likelihood()
    kernel_marglik.backward()
    kernel_grad = get_grad(model).clone()
    assert torch.allclose(marglik, kernel_marglik)
    assert torch.allclose(grad, kernel_grad)


@pytest.mark.parametrize('kron_jac', [True, False])
def test_marglik_indep_kernel_aug(model_single, aug_reg_loader_single, kron_jac):
    prior_prec = 0.089
    sigma_noise = 0.27
    backend_kwargs = dict(differentiable=True, kron_jac=kron_jac)
    # control with full laplace
    full_lap = FullLaplace(model_single, 'regression', backend=AugAsdlGGN, 
                           backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                           sigma_noise=sigma_noise)
    full_lap.fit(aug_reg_loader_single)
    model_single.zero_grad()
    marglik = full_lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model_single).clone()
    # compare to kernel laplace
    kernel_lap = FunctionalLaplace(model_single, 'regression', backend=AugAsdlGGN, 
                                   backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                                   sigma_noise=sigma_noise, independent=False)
    # need  full batch
    kernel_lap.fit(DataLoader(aug_reg_loader_single.dataset, batch_size=len(aug_reg_loader_single.dataset)))
    model_single.zero_grad()
    kernel_marglik = kernel_lap.log_marginal_likelihood()
    kernel_marglik.backward()
    kernel_grad = get_grad(model_single).clone()
    assert torch.allclose(marglik, kernel_marglik)
    assert torch.allclose(grad, kernel_grad, rtol=1e-2)


@pytest.mark.parametrize('lh', ['classification', 'regression'])
def test_marglik_indep_kernel_kron_jac_aug(model, aug_reg_loader, aug_class_loader, lh):
    loader = aug_reg_loader if lh == 'regression' else aug_class_loader
    prior_prec = torch.exp(torch.linspace(-1, 1, steps=len(list(model.parameters()))))
    sigma_noise = 0.27 if lh == 'regression' else 1.0
    # for linear model both should be equivalent
    # with kron_jac
    backend_kwargs = dict(differentiable=True, kron_jac=True)
    kernel_lap = FunctionalLaplace(model, lh, backend=AugAsdlGGN, 
                                   backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                                   sigma_noise=sigma_noise, independent=True)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    model.zero_grad()
    marglik_kron = kernel_lap.log_marginal_likelihood()
    marglik_kron.backward()
    grad_kron = get_grad(model).clone()
    # without kron_jac
    backend_kwargs = dict(differentiable=True, kron_jac=False)
    kernel_lap = FunctionalLaplace(model, lh, backend=AugAsdlGGN, 
                                   backend_kwargs=backend_kwargs, prior_precision=prior_prec, 
                                   sigma_noise=sigma_noise, independent=True)
    # need  full batch
    kernel_lap.fit(DataLoader(loader.dataset, batch_size=len(loader.dataset)))
    model.zero_grad()
    marglik = kernel_lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model).clone()
    assert torch.allclose(marglik, marglik_kron)
    assert torch.allclose(grad, grad_kron)


@pytest.mark.parametrize('curv_type,laplace', 
                         product(['ggn', 'ef'], [FullLaplace, DiagLaplace, KronLaplace, FunctionalLaplace]))
def test_differentiable_marglik_backends_class(laplace, model, class_loader, curv_type):
    if curv_type == 'ef' and laplace is KronLaplace:
        # not to be tested since backpack doesn't have Kron-EF
        return
    if curv_type == 'ggn':
        ba, bb = AsdlGGN, BackPackGGN
    else:
        ba, bb = AsdlEF, BackPackEF
    backend_kwargs = dict(differentiable=True)

    lap = laplace(model, 'classification', backend=ba, backend_kwargs=backend_kwargs)
    lap.fit(class_loader)
    model.zero_grad()
    marglik = lap.log_marginal_likelihood()
    marglik.backward()
    grad = get_grad(model).clone()

    lap = laplace(model, 'classification', backend=bb, backend_kwargs=backend_kwargs)
    lap.fit(class_loader)
    model.zero_grad()
    marglikb = lap.log_marginal_likelihood()
    marglikb.backward()
    gradb = get_grad(model).clone()

    assert torch.allclose(marglik, marglikb)
    # if not (curv_type == 'ggn' and laplace in [DiagLaplace, KronLaplace]):
    assert torch.allclose(grad, gradb)


@pytest.mark.parametrize('backend', [AsdlGGN, BackPackGGN])
def test_differentiable_marglik_diag(model, class_loader, backend):
    backend_kwargs = dict(differentiable=True)
    lap = FullLaplace(model, 'classification', backend=backend, backend_kwargs=backend_kwargs)
    lap.fit(class_loader)
    model.zero_grad()
    diag_posterior_prec = lap.posterior_precision.diagonal()
    pps = diag_posterior_prec.sum()
    pps.backward()
    grad = get_grad(model).clone()

    lap = DiagLaplace(model, 'classification', backend=backend, backend_kwargs=backend_kwargs)
    lap.fit(class_loader)
    model.zero_grad()
    ppsb = lap.posterior_precision.sum()
    ppsb.backward()
    gradb = get_grad(model).clone()

    assert torch.allclose(pps, ppsb)
    assert torch.allclose(grad, gradb)


@pytest.mark.parametrize('single_output', [False, True])
def test_kernel_marglik_vs_full(model, class_loader, single_output):
    loader = class_loader
    backend_kwargs = dict(differentiable=False)
    prior_prec = 2/3
    lap = FullLaplace(model, 'classification', backend=AsdlGGN, prior_precision=prior_prec,
                      backend_kwargs=backend_kwargs, sod=True, single_output=single_output)
    lapk = FunctionalLaplace(model, 'classification', backend=AsdlGGN, prior_precision=prior_prec,
                             backend_kwargs=backend_kwargs, sod=True, single_output=single_output,
                             independent=single_output)

    N = len(loader.dataset)
    sodloader = DataLoader(Subset(loader.dataset, range(int(N/2))), batch_size=int(N/2))
    model.zero_grad()
    torch.manual_seed(711)  # ensures same random_ix in closures
    lap.fit(sodloader)
    lap.n_data = N
    marglik_lap = lap.log_marginal_likelihood()
    torch.manual_seed(711)  # ensures same random_ix in closures
    lapk.fit(sodloader)
    lapk.n_data = N
    marglik_lapk = lapk.log_marginal_likelihood()
    assert torch.allclose(marglik_lapk, marglik_lap)

File Path: dependencies/laplace/tests/test_curv_backends_asdl.py
Content:
import pytest
import torch
from torch import nn
from torch.nn.utils import parameters_to_vector

from asdfghjkl.operations import Bias, Scale

from laplace.curvature import AsdlGGN, AsdlEF, BackPackGGN, BackPackEF
from tests.utils import jacobians_naive, HetHead


@pytest.fixture
def model():
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Tanh(), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def linear_model():
    model = torch.nn.Sequential(nn.Linear(3, 2, bias=False))
    setattr(model, 'output_size', 2)
    return model


@pytest.fixture
def class_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 3)
    y = torch.randint(2, (10,))
    return X, y


@pytest.fixture
def reg_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 3)
    y = torch.randn(10, 2)
    return X, y


@pytest.fixture
def complex_model():
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Conv2d(3, 4, 2, 2), nn.Flatten(), nn.Tanh(),
                                nn.Linear(16, 20), nn.Tanh(), Scale(), Bias(), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def complex_class_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 3, 5, 5)
    y = torch.randint(2, (10,))
    return X, y


@pytest.fixture
def het_model():
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Tanh(), nn.Linear(20, 2), HetHead())
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def het_reg_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 3)
    y = torch.randn(10, 1)
    return X, y

    
@pytest.mark.parametrize('output_ix', [torch.tensor(0), torch.tensor(1)])
def test_diag_single(class_Xy, model, output_ix):
    X, y = class_Xy
    backend = AsdlGGN(model, 'classification')
    loss, dggn = backend.single_diag(X, y, output_ix)
    Js, f = jacobians_naive(model, X)
    Jsi = Js[:, output_ix, :].squeeze()
    p = torch.softmax(f, dim=1)[:, output_ix].squeeze()
    h = p - p.square()
    dggn_true = torch.diag((Jsi * h.unsqueeze(1)).T @ Jsi)
    assert torch.allclose(dggn_true, dggn)


@pytest.mark.parametrize('output_ix', [torch.tensor(0), torch.tensor(1)])
def test_kron_single(class_Xy, linear_model, output_ix):
    X, y = class_Xy
    X = torch.ones_like(X)
    model = linear_model
    backend = AsdlGGN(model, 'classification')
    loss, kron = backend.single_kron(X, y, len(y), output_ix)
    Js, f = jacobians_naive(model, X)
    Jsi = Js[:, output_ix, :].squeeze()
    p = torch.softmax(f, dim=1)[:, output_ix].squeeze()
    h = p - p.square()
    ggn_true = (Jsi * h.unsqueeze(1)).T @ Jsi
    ggn_approx = kron.to_matrix()
    assert torch.allclose(ggn_true, ggn_approx)
    

def test_diag_ggn_cls_against_backpack_full(class_Xy, model):
    X, y = class_Xy
    backend = AsdlGGN(model, 'classification', stochastic=False)
    loss, dggn = backend.diag(X[:5], y[:5])
    loss2, dggn2 = backend.diag(X[5:], y[5:])
    loss += loss2
    dggn += dggn2

    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # check against manually computed full GGN:
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss_f, H_ggn = backend.full(X, y)
    assert torch.allclose(loss, loss_f)
    assert torch.allclose(dggn, H_ggn.diagonal())


def test_diag_ef_cls_against_backpack_full(class_Xy, model):
    X, y = class_Xy
    backend = AsdlEF(model, 'classification')
    loss, dggn = backend.diag(X[:5], y[:5])
    loss2, dggn2 = backend.diag(X[5:], y[5:])
    loss += loss2
    dggn += dggn2

    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # check against manually computed full GGN:
    backend = BackPackEF(model, 'classification')
    loss_f, H_ggn = backend.full(X, y)
    assert torch.allclose(loss, loss_f)
    assert torch.allclose(dggn, H_ggn.diagonal())


def test_diag_ggn_reg_asdl(reg_Xy, model):
    X, y = reg_Xy
    backend = AsdlGGN(model, 'regression', stochastic=False)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # check against manually computed full GGN:
    backend = AsdlGGN(model, 'regression', stochastic=False)
    loss_f, H_ggn = backend.full(X, y)
    assert loss == loss_f
    print(dggn[:5], H_ggn.diagonal()[:5])
    assert torch.allclose(dggn, H_ggn.diagonal())


def test_full_vs_diag_ef_reg_asdl(reg_Xy, model):
    X, y = reg_Xy
    backend = AsdlEF(model, 'regression')
    loss, diag_ef = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(diag_ef) == model.n_params

    # check against manually computed full GGN:
    backend = AsdlEF(model, 'regression')
    loss_f, H_ef = backend.full(X, y)
    assert loss == loss_f
    assert torch.allclose(diag_ef, H_ef.diagonal())


def test_diag_ggn_stoch_cls(class_Xy, model):
    X, y = class_Xy
    backend = AsdlGGN(model, 'classification', stochastic=True)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # same order of magnitude os non-stochastic.
    backend = AsdlGGN(model, 'classification', stochastic=False)
    loss_ns, dggn_ns = backend.diag(X, y)
    assert loss_ns == loss
    assert torch.allclose(dggn, dggn_ns, atol=1e-8, rtol=1e1)


@pytest.mark.parametrize('Backend', [AsdlEF, AsdlGGN])
def test_kron_vs_diag_class(class_Xy, model, Backend):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = class_Xy
    backend = Backend(model, 'classification')
    loss, dggn = backend.diag(X[:1], y[:1], N=1)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params
    loss, kron = backend.kron(X[:1], y[:1], N=1)
    assert torch.allclose(kron.diag(), dggn)


@pytest.mark.parametrize('Backend', [AsdlEF, AsdlGGN])
def test_kron_batching_correction(class_Xy, model, Backend):
    X, y = class_Xy
    backend = Backend(model, 'classification')
    loss, kron = backend.kron(X, y, N=len(X))
    assert len(kron.diag()) == model.n_params

    N = len(X)
    M = 3
    loss1, kron1 = backend.kron(X[:M], y[:M], N=N)
    loss2, kron2 = backend.kron(X[M:], y[M:], N=N)
    kron_two = kron1 + kron2
    loss_two = loss1 + loss2
    assert torch.allclose(kron.diag(), kron_two.diag())
    assert torch.allclose(loss, loss_two)


def test_kron_batching_correction_single(class_Xy, model):
    X, y = class_Xy
    backend = AsdlGGN(model, 'classification')
    loss, kron = backend.single_kron(X, y, N=len(X), output_ix=torch.tensor(0))
    assert len(kron.diag()) == model.n_params

    N = len(X)
    M = 3
    loss1, kron1 = backend.single_kron(X[:M], y[:M], N=N, output_ix=torch.tensor(0))
    loss2, kron2 = backend.single_kron(X[M:], y[M:], N=N, output_ix=torch.tensor(0))
    kron_two = kron1 + kron2
    loss_two = loss1 + loss2
    assert torch.allclose(kron.diag(), kron_two.diag())
    assert torch.allclose(loss, loss_two)


@pytest.mark.parametrize('Backend', [AsdlGGN, AsdlEF])
def test_kron_summing_up_vs_diag(class_Xy, model, Backend):
    # For a single data point, Kron is exact and should equal diag class_Xy
    X, y = class_Xy
    backend = Backend(model, 'classification')
    loss, dggn = backend.diag(X, y, N=len(X))
    loss, kron = backend.kron(X, y, N=len(X))
    assert torch.allclose(kron.diag().norm(), dggn.norm(), rtol=1e-1)


def test_complex_diag_ggn_stoch_cls(complex_class_Xy, complex_model):
    X, y = complex_class_Xy
    backend = AsdlGGN(complex_model, 'classification', stochastic=True)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == complex_model.n_params

    # same order of magnitude os non-stochastic.
    loss_ns, dggn_ns = backend.diag(X, y)
    assert loss_ns == loss
    assert torch.allclose(dggn, dggn_ns, atol=1e-8, rtol=1)


@pytest.mark.parametrize('Backend', [AsdlEF, AsdlGGN])
def test_complex_kron_vs_diag(complex_class_Xy, complex_model, Backend):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = complex_class_Xy
    backend = Backend(complex_model, 'classification')
    loss, dggn = backend.diag(X[:1], y[:1], N=1)
    # sanity check size of diag ggn
    assert len(dggn) == complex_model.n_params
    loss, kron = backend.kron(X[:1], y[:1], N=1)
    assert torch.allclose(kron.diag().norm(), dggn.norm(), rtol=1e-1)


@pytest.mark.parametrize('Backend', [AsdlEF, AsdlGGN])
def test_complex_kron_batching_correction(complex_class_Xy, complex_model, Backend):
    X, y = complex_class_Xy
    backend = Backend(complex_model, 'classification')
    loss, kron = backend.kron(X, y, N=len(X))
    assert len(kron.diag()) == complex_model.n_params

    N = len(X)
    M = 3
    loss1, kron1 = backend.kron(X[:M], y[:M], N=N)
    loss2, kron2 = backend.kron(X[M:], y[M:], N=N)
    kron_two = kron1 + kron2
    loss_two = loss1 + loss2
    assert torch.allclose(kron.diag(), kron_two.diag())
    assert torch.allclose(loss, loss_two)

@pytest.mark.parametrize('Backend', [AsdlEF, AsdlGGN])
def test_complex_kron_batching_correction(complex_class_Xy, complex_model, Backend):
    X, y =complex_class_Xy
    backend = Backend(complex_model, 'classification')
    loss, diag = backend.diag(X, y)
    assert len(diag) == complex_model.n_params

    N = len(X)
    M = 3
    loss1, diag1 = backend.diag(X[:M], y[:M])
    loss2, diag2 = backend.diag(X[M:], y[M:])
    diag_two = diag1 + diag2
    loss_two = loss1 + loss2
    assert torch.allclose(diag, diag_two)
    assert torch.allclose(loss, loss_two)

@pytest.mark.parametrize('Backend', [AsdlGGN, AsdlEF])
def test_complex_kron_summing_up_vs_diag_class(complex_class_Xy, complex_model, Backend):
    # For a single data point, Kron is exact and should equal diag class_Xy
    X, y = complex_class_Xy
    backend = Backend(complex_model, 'classification')
    loss, dggn = backend.diag(X, y, N=len(X))
    loss, kron = backend.kron(X, y, N=len(X))
    assert torch.allclose(kron.diag().norm(), dggn.norm(), rtol=1e-2)


def test_kron_normalization_ggn_class(class_Xy, model):
    X, y = class_Xy
    xi, yi = X[:1], y[:1]
    backend = AsdlGGN(model, 'classification', stochastic=False)
    loss, kron = backend.kron(xi, yi, N=1)
    kron_true = 7 * kron
    loss_true = 7 * loss
    X = torch.repeat_interleave(xi, 7, 0)
    y = torch.repeat_interleave(yi, 7, 0)
    loss_test, kron_test  = backend.kron(X, y, N=7)
    assert torch.allclose(kron_true.diag(), kron_test.diag())
    assert torch.allclose(loss_true, loss_test)


def test_kron_normalization_ef_class(class_Xy, model):
    X, y = class_Xy
    xi, yi = X[:1], y[:1]
    backend = AsdlEF(model, 'classification')
    loss, kron = backend.kron(xi, yi, N=1)
    kron_true = 7 * kron
    loss_true = 7 * loss
    X = torch.repeat_interleave(xi, 7, 0)
    y = torch.repeat_interleave(yi, 7, 0)
    loss_test, kron_test  = backend.kron(X, y, N=7)
    assert torch.allclose(kron_true.diag(), kron_test.diag())
    assert torch.allclose(loss_true, loss_test)


def test_kron_ggn_reg_asdl_vs_diag_reg(reg_Xy, model):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = reg_Xy
    backend = AsdlGGN(model, 'regression', stochastic=False)
    loss, dggn = backend.diag(X[:1], y[:1], N=1)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params
    loss, kron = backend.kron(X[:1], y[:1], N=1)
    assert torch.allclose(kron.diag(), dggn)


def test_diag_ggn_hetreg_asdl(het_reg_Xy, het_model):
    X, y = het_reg_Xy
    backend = AsdlGGN(het_model, 'heteroscedastic_regression', stochastic=False)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == het_model.n_params

    # check against manually computed full GGN:
    backend = AsdlGGN(het_model, 'heteroscedastic_regression', stochastic=False)
    loss_f, H_ggn = backend.full(X, y)
    assert loss == loss_f
    loss, h = backend.kron(X, y, N=len(y))
    assert torch.allclose(dggn, H_ggn.diagonal())


def test_kron_ggn_hetreg_asdl_vs_diag_reg(het_reg_Xy, het_model):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = het_reg_Xy
    backend = AsdlGGN(het_model, 'heteroscedastic_regression', stochastic=False)
    loss, dggn = backend.diag(X[:1], y[:1], N=1)
    # sanity check size of diag ggn
    loss, kron = backend.kron(X[:1], y[:1], N=1)
    assert torch.allclose(kron.diag(), dggn)


def test_diag_ef_hetreg_asdl(het_reg_Xy, het_model):
    X, y = het_reg_Xy
    backend = AsdlEF(het_model, 'heteroscedastic_regression')
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == het_model.n_params

    # check against manually computed full GGN:
    backend = AsdlEF(het_model, 'heteroscedastic_regression')
    loss_f, H_ggn = backend.full(X, y)
    assert loss == loss_f
    loss, h = backend.kron(X, y, N=len(y))
    assert torch.allclose(dggn, H_ggn.diagonal())


def test_kron_ef_hetreg_asdl_vs_diag_reg(het_reg_Xy, het_model):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = het_reg_Xy
    backend = AsdlGGN(het_model, 'heteroscedastic_regression')
    loss, dggn = backend.diag(X[:1], y[:1], N=1)
    # sanity check size of diag ggn
    loss, kron = backend.kron(X[:1], y[:1], N=1)
    assert torch.allclose(kron.diag(), dggn)

File Path: dependencies/laplace/tests/test_curv_backends_augmented_asdl.py
Content:
import pytest
import torch
from torch import nn
from torch.nn.utils import parameters_to_vector
from asdfghjkl.operations import Conv2dAug, Bias, Scale

from laplace.curvature import AugAsdlGGN, AugAsdlEF, AugBackPackGGN
from laplace.curvature import AsdlEF, AsdlGGN


@pytest.fixture
def model():
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Tanh(), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def class_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 5, 3, requires_grad=True)
    y = torch.randint(2, (10,))
    return X, y


@pytest.fixture
def class_Xy_single():
    torch.manual_seed(711)
    X = torch.randn(1, 1, 3, requires_grad=True)
    y = torch.randint(2, (1,))
    return X, y


@pytest.fixture
def complex_model():
    torch.manual_seed(711)
    # NOTE: Scale() will fail here because it is not posssible to detect augmentation
    # so Scale() is automatically `more exact`.
    model = torch.nn.Sequential(Conv2dAug(3, 4, 2, 2), nn.Flatten(start_dim=2), nn.Tanh(),
                                nn.Linear(16, 20), Bias(), nn.Tanh(), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model

    
@pytest.fixture
def complex_class_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 7, 3, 5, 5, requires_grad=True)
    y = torch.randint(2, (10,))
    return X, y


@pytest.fixture
def complex_class_Xy_single():
    torch.manual_seed(711)
    X = torch.randn(1, 1, 3, 5, 5, requires_grad=True)
    y = torch.randint(2, (1,))
    return X, y


@pytest.fixture
def complex_class_Xy_single_aug():
    torch.manual_seed(711)
    X = torch.randn(10, 1, 3, 5, 5, requires_grad=True)
    y = torch.randint(2, (10,))
    return X, y

    
def test_full_ggn_against_backpack(class_Xy, model):
    X, y = class_Xy
    backend_backpack = AugBackPackGGN(model, 'classification')
    loss_cmp, H_cmp = backend_backpack.full(X, y)
    X.grad.data.zero_()
    (loss_cmp + H_cmp.sum()).backward()
    xgrad_cmp = X.grad.data.clone().detach().flatten()
    backend_asdl = AugAsdlGGN(model, 'classification')
    loss, H = backend_asdl.full(X, y)
    X.grad.data.zero_()
    (loss + H.sum()).backward()
    xgrad = X.grad.data.clone().flatten()
    assert torch.allclose(loss, loss_cmp)
    assert torch.allclose(H, H_cmp)
    assert torch.allclose(xgrad, xgrad_cmp, atol=1e-7)

    
@pytest.mark.parametrize('Backend', [AugAsdlEF, AugAsdlGGN])
def test_diag_against_diagonalized_full(class_Xy, model, Backend):
    X, y = class_Xy
    backend = Backend(model, 'classification', kron_jac=True)  # Diag by default uses kron approx now
    loss_cmp, H_cmp = backend.full(X, y)
    h_cmp = torch.diag(H_cmp)
    X.grad.data.zero_()
    (loss_cmp + h_cmp.sum()).backward()
    xgrad_cmp = X.grad.data.detach().clone().flatten()
    loss, h = backend.diag(X, y)
    X.grad.data.zero_()
    (loss + h.sum()).backward()
    xgrad = X.grad.data.clone().flatten()
    assert torch.allclose(loss, loss_cmp)
    assert torch.allclose(h, h_cmp)
    assert torch.allclose(xgrad, xgrad_cmp)


@pytest.mark.parametrize('Backend', [AugAsdlEF, AugAsdlGGN])
def test_kron_against_diagonal(class_Xy_single, model, Backend):
    model.zero_grad()
    torch.manual_seed(723)
    X, y = class_Xy_single
    backend = Backend(model, 'classification')
    loss_cmp, h_cmp = backend.diag(X.repeat(5, 11, 1), y.repeat(5))
    X.grad.data.zero_()
    (loss_cmp + h_cmp.sum()).backward()
    xgrad_cmp = X.grad.data.detach().clone().flatten()
    loss, H_kron = backend.kron(X.repeat(5, 11, 1), y.repeat(5), 5)
    h = H_kron.diag()
    X.grad.data.zero_()
    (loss + h.sum()).backward()
    xgrad = X.grad.data.clone().flatten()
    assert torch.allclose(loss, loss_cmp)
    assert torch.allclose(h, h_cmp)
    assert torch.allclose(xgrad, xgrad_cmp)


@pytest.mark.parametrize('Backend', [AugAsdlEF, AugAsdlGGN])
def test_kron_against_diagonal_approx(class_Xy, model, Backend):
    X, y = class_Xy
    backend = Backend(model, 'classification')
    loss_cmp, h_cmp = backend.diag(X, y)
    X.grad.data.zero_()
    (loss_cmp + h_cmp.sum()).backward()
    xgrad_cmp = X.grad.data.detach().clone().flatten()
    loss, H_kron = backend.kron(X, y, len(X))
    h = H_kron.diag()
    X.grad.data.zero_()
    (loss + h.sum()).backward()
    xgrad = X.grad.data.clone().flatten()
    assert torch.allclose(loss, loss_cmp)
    # these are very rough, should be rather taken as integration test
    # for real test see above `test_kron_against_diagonal` with
    # repeated data which makes Kron and diagonal exactly equal and 
    # thus ensures proper scaling.
    assert torch.allclose(h, h_cmp, atol=1e-1, rtol=1e-3)
    assert torch.allclose(xgrad, xgrad_cmp, atol=1e-1, rtol=1e-3)
    

@pytest.mark.parametrize('Backend', [AugAsdlEF, AugAsdlGGN])
def test_diag_against_diagonalized_full_cnn(complex_class_Xy, complex_model, Backend):
    X, y = complex_class_Xy
    backend = Backend(complex_model, 'classification', kron_jac=True)
    loss_cmp, H_cmp = backend.full(X, y)
    h_cmp = torch.diag(H_cmp)
    X.grad.data.zero_()
    (loss_cmp + h_cmp.sum()).backward()
    xgrad_cmp = X.grad.data.detach().clone().flatten()
    loss, h = backend.diag(X, y)
    X.grad.data.zero_()
    (loss + h.sum()).backward()
    xgrad = X.grad.data.clone().flatten()
    assert torch.allclose(loss, loss_cmp)
    assert torch.allclose(h, h_cmp)
    assert torch.allclose(xgrad, xgrad_cmp)


@pytest.mark.parametrize('Backend', [AugAsdlEF, AugAsdlGGN])
def test_kron_augmentation_cnn(complex_class_Xy_single_aug, complex_model, Backend):
    # augmenting more with simple repetition shouldn't change anything.
    X, y = complex_class_Xy_single_aug
    backend = Backend(complex_model, 'classification')
    # no augmentation dimension
    loss_cmp, H_kron = backend.kron(X, y, len(y))
    X.grad.data.zero_()
    h_cmp = H_kron.diag()
    (loss_cmp + h_cmp.sum()).backward()
    xgrad_cmp = X.grad.data.detach().clone().flatten()
    # use 21 augmentation dimensions but just repeat --> has to be equivalent
    loss, H_kron = backend.kron(X.repeat(1, 21, 1, 1, 1), y, len(y))
    X.grad.data.zero_()
    h = H_kron.diag()
    (loss + h.sum()).backward()
    xgrad = X.grad.data.clone().flatten()
    assert torch.allclose(loss, loss_cmp)
    assert torch.allclose(h, h_cmp)
    idx = torch.argmax(torch.abs(xgrad - xgrad_cmp))
    print(xgrad[idx], xgrad_cmp[idx], xgrad_cmp[idx]-xgrad[idx])
    assert torch.allclose(xgrad, xgrad_cmp, atol=1e-7)


@pytest.mark.parametrize('Backend', [AugAsdlEF, AugAsdlGGN])
def test_kron_against_diagonal_approx_cnn(complex_class_Xy_single, complex_model, Backend):
    X, y = complex_class_Xy_single
    backend = Backend(complex_model, 'classification')
    loss_cmp, h_cmp = backend.diag(X, y)
    X.grad.data.zero_()
    (loss_cmp + h_cmp.sum()).backward()
    xgrad_cmp = X.grad.data.detach().clone().flatten()
    loss, H_kron = backend.kron(X, y, 1)
    X.grad.data.zero_()
    h = H_kron.diag()
    (loss + h.sum()).backward()
    xgrad = X.grad.data.clone().flatten()
    assert torch.allclose(loss, loss_cmp)
    # these are very rough, should be rather taken as integration test
    # this is because for CNN,  Kron != diagonal even for a single input
    assert torch.allclose(h, h_cmp, atol=1e-1, rtol=1e-2)
    assert torch.allclose(xgrad, xgrad_cmp, atol=1e-1, rtol=1e-2)


def test_kron_aug_against_kron_fc(model):
    X = torch.randn(11, 1, 3, requires_grad=True)
    y = torch.randint(2, (11,))
    backend = AsdlGGN(model, 'classification')
    loss, H_kron = backend.kron(X.squeeze(), y, len(X))
    (loss + H_kron.diag().sum()).backward()
    grad = X.grad.data.clone()
    X.grad = None
    backend_aug = AugAsdlGGN(model, 'classification')
    loss_aug, H_kron_aug = backend_aug.kron(X, y, len(X))
    (loss_aug + H_kron_aug.diag().sum()).backward()
    grad_aug = X.grad.data.clone()
    assert torch.allclose(loss_aug, loss)
    assert torch.allclose(H_kron.diag(), H_kron_aug.diag())
    assert torch.allclose(grad, grad_aug)


def test_kron_aug_against_kron_cnn(complex_model):
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Conv2d(3, 4, 2, 2), nn.Flatten(), nn.Tanh(),
                                nn.Linear(16, 20), Bias(), nn.Tanh(), nn.Linear(20, 2))
    X = torch.randn(11, 1, 3, 5, 5, requires_grad=True)
    y = torch.randint(2, (11,))
    backend = AsdlGGN(model, 'classification')
    loss, H_kron = backend.kron(X.squeeze(), y, len(X))
    (loss + H_kron.diag().sum()).backward()
    grad = X.grad.data.clone()
    X.grad = None
    backend_aug = AugAsdlGGN(complex_model, 'classification')
    loss_aug, H_kron_aug = backend_aug.kron(X, y, len(X))
    (loss_aug + H_kron_aug.diag().sum()).backward()
    grad_aug = X.grad.data.clone()
    assert torch.allclose(loss_aug, loss)
    assert torch.allclose(H_kron.diag(), H_kron_aug.diag())
    assert torch.allclose(grad, grad_aug)

File Path: dependencies/laplace/tests/test_curv_backends_backpack.py
Content:
import pytest
import torch
from torch import nn
from torch.nn.utils import parameters_to_vector

from laplace.curvature import BackPackGGN, BackPackEF


@pytest.fixture
def model():
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Tanh(), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def class_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 3)
    y = torch.randint(2, (10,))
    return X, y


@pytest.fixture
def reg_Xy():
    torch.manual_seed(711)
    X = torch.randn(10, 3)
    y = torch.randn(10, 2)
    return X, y

    
def test_full_ggn_backpack_reg_integration(reg_Xy, model):
    X, y = reg_Xy
    backend = BackPackGGN(model, 'regression', stochastic=True)
    with pytest.raises(ValueError):
        loss, fggn = backend.full(X, y)

    # cannot test, its implemented based on Jacobians.
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss, H_ggn = backend.full(X, y)
    assert H_ggn.size() == torch.Size((model.n_params, model.n_params))


def test_full_ggn_backpack_cls_integration(class_Xy, model):
    X, y = class_Xy
    backend = BackPackGGN(model, 'classification', stochastic=True)
    with pytest.raises(ValueError):
        loss, fggn = backend.full(X, y)

    # cannot test, its implemented based on Jacobians.
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss, H_ggn = backend.full(X, y)
    assert H_ggn.size() == torch.Size((model.n_params, model.n_params))


def test_diag_ggn_cls_backpack(class_Xy, model):
    X, y = class_Xy
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # check against manually computed full GGN:
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss_f, H_ggn = backend.full(X, y)
    assert loss == loss_f
    assert torch.allclose(dggn, H_ggn.diagonal())


def test_diag_ggn_reg_backpack(reg_Xy, model):
    X, y = reg_Xy
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # check against manually computed full GGN:
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss_f, H_ggn = backend.full(X, y)
    assert loss == loss_f
    assert torch.allclose(dggn, H_ggn.diagonal())


def test_diag_ggn_stoch_cls_backpack(class_Xy, model):
    X, y = class_Xy
    backend = BackPackGGN(model, 'classification', stochastic=True)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # same order of magnitude os non-stochastic.
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss_ns, dggn_ns = backend.diag(X, y)
    assert loss_ns == loss
    assert torch.allclose(dggn, dggn_ns, atol=1e-8, rtol=1e1)


def test_diag_ggn_stoch_reg_backpack(reg_Xy, model):
    X, y = reg_Xy
    backend = BackPackGGN(model, 'regression', stochastic=True)
    loss, dggn = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params

    # same order of magnitude os non-stochastic.
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss_ns, dggn_ns = backend.diag(X, y)
    assert loss_ns == loss
    assert torch.allclose(dggn, dggn_ns, atol=1e-8, rtol=1e1)


def test_kron_ggn_reg_backpack_vs_diag_reg(reg_Xy, model):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = reg_Xy
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss, dggn = backend.diag(X[:1], y[:1], N=1)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params
    loss, kron = backend.kron(X[:1], y[:1], N=1)
    assert torch.allclose(kron.diag(), dggn)


def test_kron_batching_correction_reg(reg_Xy, model):
    X, y = reg_Xy
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss, kron = backend.kron(X, y, N=len(X))
    assert len(kron.diag()) == model.n_params

    N = len(X)
    M = int(N / 2)
    loss1, kron1 = backend.kron(X[:M], y[:M], N=N)
    loss2, kron2 = backend.kron(X[M:], y[M:], N=N)
    kron_two = kron1 + kron2
    loss_two = loss1 + loss2
    assert torch.allclose(kron.diag(), kron_two.diag())
    assert torch.allclose(loss, loss_two)


def test_kron_summing_up_vs_diag_reg(reg_Xy, model):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = reg_Xy
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss, dggn = backend.diag(X, y, N=len(X))
    loss, kron = backend.kron(X, y, N=len(X))
    assert torch.allclose(kron.diag().norm(), dggn.norm(), rtol=1e-1)


def test_kron_ggn_reg_backpack_vs_diag_class(class_Xy, model):
    # For a single data point, Kron is exact and should equal diag GGN
    X, y = class_Xy
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss, dggn = backend.diag(X[:1], y[:1], N=1)
    # sanity check size of diag ggn
    assert len(dggn) == model.n_params
    loss, kron = backend.kron(X[:1], y[:1], N=1)
    assert torch.allclose(kron.diag(), dggn)


def test_kron_batching_correction_class(class_Xy, model):
    X, y = class_Xy
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss, kron = backend.kron(X, y, N=len(X))
    assert len(kron.diag()) == model.n_params

    N = len(X)
    M = int(N / 2)
    loss1, kron1 = backend.kron(X[:M], y[:M], N=N)
    loss2, kron2 = backend.kron(X[M:], y[M:], N=N)
    kron_two = kron1 + kron2
    loss_two = loss1 + loss2
    assert torch.allclose(kron.diag(), kron_two.diag())
    assert torch.allclose(loss, loss_two)


def test_kron_summing_up_vs_diag_class(class_Xy, model):
    # For a single data point, Kron is exact and should equal diag class_Xy
    X, y = class_Xy
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss, dggn = backend.diag(X, y, N=len(X))
    loss, kron = backend.kron(X, y, N=len(X))
    assert torch.allclose(kron.diag().norm(), dggn.norm(), rtol=1e-1)


def test_full_vs_diag_ef_cls_backpack(class_Xy, model):
    X, y = class_Xy
    backend = BackPackEF(model, 'classification')
    loss, diag_ef = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(diag_ef) == model.n_params

    # check against manually computed full GGN:
    backend = BackPackEF(model, 'classification')
    loss_f, H_ef = backend.full(X, y)
    assert loss == loss_f
    assert torch.allclose(diag_ef, H_ef.diagonal())


def test_full_vs_diag_ef_reg_backpack(reg_Xy, model):
    X, y = reg_Xy
    backend = BackPackEF(model, 'regression')
    loss, diag_ef = backend.diag(X, y)
    # sanity check size of diag ggn
    assert len(diag_ef) == model.n_params

    # check against manually computed full GGN:
    backend = BackPackEF(model, 'regression')
    loss_f, H_ef = backend.full(X, y)
    assert loss == loss_f
    assert torch.allclose(diag_ef, H_ef.diagonal())


def test_kron_normalization_reg(reg_Xy, model):
    X, y = reg_Xy
    xi, yi = X[:1], y[:1]
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss, kron = backend.kron(xi, yi, N=1)
    kron_true = 7 * kron
    loss_true = 7 * loss
    X = torch.repeat_interleave(xi, 7, 0)
    y = torch.repeat_interleave(yi, 7, 0)
    loss_test, kron_test  = backend.kron(X, y, N=7)
    assert torch.allclose(kron_true.diag(), kron_test.diag())
    assert torch.allclose(loss_true, loss_test)


def test_kron_normalization_class(class_Xy, model):
    X, y = class_Xy
    xi, yi = X[:1], y[:1]
    backend = BackPackGGN(model, 'classification', stochastic=False)
    loss, kron = backend.kron(xi, yi, N=1)
    kron_true = 7 * kron
    loss_true = 7 * loss
    X = torch.repeat_interleave(xi, 7, 0)
    y = torch.repeat_interleave(yi, 7, 0)
    loss_test, kron_test  = backend.kron(X, y, N=7)
    assert torch.allclose(kron_true.diag(), kron_test.diag())
    assert torch.allclose(loss_true, loss_test)


File Path: dependencies/laplace/tests/test_feature_extractor.py
Content:
import torch
import torch.nn as nn
import torchvision.models as models

from laplace.feature_extractor import FeatureExtractor


class CNN(nn.Module):
    def __init__(self, num_classes):
        nn.Module.__init__(self)
        self.conv1 = nn.Sequential(
            # Input shape (3, 64, 64)
            nn.Conv2d(
                in_channels=3,
                out_channels=6,
                kernel_size=5,
                stride=1,
                padding=2
            ),
            # Output shape (6, 60, 60)
            nn.ReLU(),
            # Output shape (6, 30, 30)
            nn.MaxPool2d(kernel_size=2)
        )

        self.fc = nn.Sequential(
            nn.Linear(in_features=16 * 16 * 16,
                      out_features=300),
            nn.ReLU(),
            nn.Linear(in_features=300,
                      out_features=84),
            nn.ReLU(),
            nn.Linear(in_features=84,
                      out_features=num_classes)
        )

        self.conv2 = nn.Sequential(
            # Input shape (6, 30, 30)
            nn.Conv2d(
                in_channels=6,
                out_channels=16,
                kernel_size=5,
                stride=1,
                padding=2
            ),
            # Output shape (16, 26, 26)
            nn.ReLU(),
            # Output shape (16, 13, 13)
            nn.MaxPool2d(kernel_size=2)
        )

    def forward(self, x):
        x = self.conv1(x)
        # print("x = self.conv1(x)   ", x.shape, "  ", torch.Size)
        x = self.conv2(x)
        x = x.view(x.size()[0], -1)
        x = self.fc(x)
        return x


def get_model(model_name):
    if model_name == 'resnet18':
        model = models.resnet18()
    elif model_name == 'alexnet':
        model = models.alexnet()
    elif model_name == 'vgg16':
        model = models.vgg16()
    elif model_name == 'squeezenet':
        model = models.squeezenet1_0()
    elif model_name == 'densenet':
        model = models.densenet161()
    elif model_name == 'inception':
        model = models.inception_v3(init_weights=True)
    elif model_name == 'googlenet':
        model = models.googlenet(init_weights=True)
    elif model_name == 'shufflenet':
        model = models.shufflenet_v2_x1_0()
    elif model_name == 'mobilenet_v2':
        model = models.mobilenet_v2()
    elif model_name == 'mobilenet_v3_large':
        model = models.mobilenet_v3_large()
    elif model_name == 'mobilenet_v3_small':
        model = models.mobilenet_v3_small()
    elif model_name == 'resnext50_32x4d':
        model = models.resnext50_32x4d()
    elif model_name == 'wide_resnet50_2':
        model = models.wide_resnet50_2()
    elif model_name == 'mnasnet':
        model = models.mnasnet1_0()
    elif model_name == 'switchedCNN':
        model = CNN(10)
    elif model_name == 'sequential':
        model = nn.Sequential(
            nn.Conv2d(3, 6, 3, 1, 1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(6*64*64, 10),
            nn.ReLU(),
            nn.Linear(10, 10)
        )
    else:
        raise ValueError(f'{model_name} is not supported.')
    return model


@torch.no_grad()
def test_feature_extractor():
    # all torchvision classifcation models but 'squeezenet' (no linear last layer)
    # + model where modules are initilaized in wrong order + nn.Sequential model
    model_names = ['resnet18', 'alexnet', 'vgg16', 'densenet', 'inception',
                   'googlenet', 'shufflenet', 'mobilenet_v2', 'mobilenet_v3_large',
                   'mobilenet_v3_small', 'resnext50_32x4d', 'wide_resnet50_2',
                   'mnasnet', 'switchedCNN', 'sequential']

    # to test the last_layer_name argument
    # last_layer_names = ['fc', 'classifier.6', 'classifier.6', 'classifier', 'fc',
    #                     'fc', 'fc', 'classifier.1', 'classifier.3', 'classifier.3',
    #                     'fc', 'fc', 'classifier.1', 'fc.4', '5']

    for model_name in model_names:
        # generate random test input
        if model_name == 'inception':
            x = torch.randn(1, 3, 299, 299)
        else:
            x = torch.randn(1, 3, 64, 64)

        model = get_model(model_name)
        model.eval()

        # extract features and get output
        feature_extractor = FeatureExtractor(model)
        out, features = feature_extractor.forward_with_features(x)

        # test if it worked
        last_layer = feature_extractor.last_layer
        out2 = last_layer(features)
        assert (out == out2).all().item()

File Path: dependencies/laplace/tests/test_jacobians.py
Content:
import pytest
import torch
from torch import nn
from torch.nn.utils.convert_parameters import parameters_to_vector
from asdfghjkl.operations import Scale, Bias, Conv2dAug

from laplace.curvature import AsdlInterface, BackPackInterface, AugBackPackInterface, AugAsdlInterface
from laplace.feature_extractor import FeatureExtractor
from tests.utils import jacobians_naive, jacobians_naive_aug, gradients_naive_aug


@pytest.fixture
def complex_model():
    torch.manual_seed(711)
    model = torch.nn.Sequential(nn.Conv2d(3, 4, 2, 2), nn.Flatten(), nn.Tanh(),
                                nn.Linear(16, 20), nn.Tanh(), Scale(), Bias(), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def complex_model_aug():
    torch.manual_seed(711)
    model = torch.nn.Sequential(Conv2dAug(3, 4, 2, 2), nn.Flatten(start_dim=2), nn.Tanh(),
                                nn.Linear(16, 20), nn.Tanh(), Scale(), Bias(), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def complex_X():
    torch.manual_seed(711)
    return torch.randn(10, 3, 5, 5)


@pytest.fixture
def complex_X_aug():
    torch.manual_seed(711)
    return torch.randn(10, 7, 3, 5, 5)


@pytest.fixture
def multioutput_model():
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    return model


@pytest.fixture
def singleoutput_model():
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Linear(20, 1))
    setattr(model, 'output_size', 1)
    return model


@pytest.fixture
def linear_model():
    model = torch.nn.Sequential(nn.Linear(3, 1, bias=False))
    setattr(model, 'output_size', 1)
    return model


@pytest.fixture
def X():
    torch.manual_seed(15)
    return torch.randn(200, 3)

    
@pytest.fixture
def X_aug():
    torch.manual_seed(711)
    return torch.randn(200, 5, 3)

    
@pytest.mark.parametrize('backend', [AsdlInterface, BackPackInterface])
def test_linear_jacobians(linear_model, X, backend):
    # jacobian of linear model is input X.
    backend = backend(linear_model, 'classification')
    Js, f = backend.jacobians(X)
    # into Jacs shape (batch_size, output_size, params)
    true_Js = X.reshape(len(X), 1, -1)
    assert true_Js.shape == Js.shape
    assert torch.allclose(true_Js, Js, atol=1e-5)
    assert torch.allclose(f, linear_model(X), atol=1e-5)


@pytest.mark.parametrize('backend', [AsdlInterface, BackPackInterface])
def test_jacobians_singleoutput(singleoutput_model, X, backend):
    model = singleoutput_model
    backend = backend(model, 'classification')
    Js, f = backend.jacobians(X)
    Js_naive, f_naive = jacobians_naive(model, X)
    assert Js.shape == Js_naive.shape
    assert torch.abs(Js-Js_naive).max() < 1e-6
    assert torch.allclose(model(X), f_naive)
    assert torch.allclose(f, f_naive)


@pytest.mark.parametrize('backend', [AsdlInterface, BackPackInterface])
def test_jacobians_multioutput(multioutput_model, X, backend):
    model = multioutput_model
    backend = backend(model, 'classification')
    Js, f = backend.jacobians(X)
    Js_naive, f_naive = jacobians_naive(model, X)
    assert Js.shape == Js_naive.shape
    assert torch.abs(Js-Js_naive).max() < 1e-6
    assert torch.allclose(model(X), f_naive)
    assert torch.allclose(f, f_naive)


def test_jacobians_cnn(complex_model, complex_X):
    model = complex_model
    backend = AsdlInterface(model, 'classification')
    Js, f = backend.jacobians(complex_X)
    Js_naive, f_naive = jacobians_naive(model, complex_X)
    assert Js.shape == Js_naive.shape
    assert torch.abs(Js-Js_naive).max() < 1e-6
    assert torch.allclose(model(complex_X), f_naive)
    assert torch.allclose(f, f_naive)


@pytest.mark.parametrize('backend', [AsdlInterface, BackPackInterface])
def test_last_layer_jacobians_singleoutput(singleoutput_model, X, backend):
    model = FeatureExtractor(singleoutput_model)
    Js, f = backend.last_layer_jacobians(model, X)
    _, phi = model.forward_with_features(X)
    Js_naive, f_naive = jacobians_naive(model.last_layer, phi)
    assert Js.shape == Js_naive.shape
    assert torch.abs(Js-Js_naive).max() < 1e-6
    assert torch.allclose(model(X), f_naive)
    assert torch.allclose(f, f_naive)


@pytest.mark.parametrize('backend', [AsdlInterface, BackPackInterface])
def test_last_layer_jacobians_multioutput(multioutput_model, X, backend):
    model = FeatureExtractor(multioutput_model)
    Js, f = backend.last_layer_jacobians(model, X)
    _, phi = model.forward_with_features(X)
    Js_naive, f_naive = jacobians_naive(model.last_layer, phi)
    assert Js.shape == Js_naive.shape
    assert torch.abs(Js-Js_naive).max() < 1e-6
    assert torch.allclose(model(X), f_naive)
    assert torch.allclose(f, f_naive)


@pytest.mark.parametrize('backend', [AugAsdlInterface, AugBackPackInterface])
def test_jacobians_augmented(multioutput_model, X_aug, backend):
    backend = backend(multioutput_model, 'classification')
    Js, f = backend.jacobians(X_aug)
    Js_naive, f_naive = jacobians_naive_aug(multioutput_model, X_aug)
    assert Js.shape == Js_naive.shape
    assert torch.abs(Js-Js_naive).max() < 1e-6
    assert torch.allclose(multioutput_model(X_aug).mean(dim=1), f_naive)
    assert torch.allclose(f, f_naive)

    
def test_jacobians_cnn_augmented(complex_model_aug, complex_X_aug):
    model = complex_model_aug
    backend = AugAsdlInterface(model, 'classification')
    Js, f = backend.jacobians(complex_X_aug)
    Js_naive, f_naive = jacobians_naive_aug(model, complex_X_aug)
    assert Js.shape == Js_naive.shape
    assert torch.allclose(model(complex_X_aug).mean(dim=1), f_naive)
    assert torch.allclose(f, f_naive)
    assert torch.abs(Js-Js_naive).max() < 1e-6


@pytest.mark.parametrize('likelihood', ['classification', 'regression'])
def test_gradients_augmented(multioutput_model, X_aug, likelihood):
    if likelihood == 'regression':
        y = torch.randn(len(X_aug), 2)
    else:
        y = torch.empty(len(X_aug), dtype=torch.long).random_(2)
    backend = AugAsdlInterface(multioutput_model, likelihood)
    Gs, loss = backend.gradients(X_aug, y)
    Gs_naive, loss_naive = gradients_naive_aug(multioutput_model, X_aug, y, likelihood)
    assert torch.allclose(loss, loss_naive)
    assert Gs.shape == Gs_naive.shape
    assert torch.allclose(Gs, Gs_naive, atol=1e-7)

File Path: dependencies/laplace/tests/test_laplace.py
Content:
import pytest
import torch
from torch import nn
from torch.nn.utils import parameters_to_vector

from laplace.laplace import Laplace
from laplace.baselaplace import FullLaplace, KronLaplace, DiagLaplace
from laplace.lllaplace import FullLLLaplace, KronLLLaplace, DiagLLLaplace


torch.manual_seed(240)
torch.set_default_tensor_type(torch.DoubleTensor)
flavors = [FullLaplace, KronLaplace, DiagLaplace,
           FullLLLaplace, KronLLLaplace, DiagLLLaplace]
all_keys = [('all', 'full'), ('all', 'kron'), ('all', 'diag'),
            ('last_layer', 'full'), ('last_layer', 'kron'), ('last_layer', 'diag')]


@pytest.fixture
def model():
    model = nn.Sequential(nn.Linear(3, 20), nn.Linear(20, 2))
    model_params = list(model.parameters())
    return model


def test_default_init(model, likelihood='classification'):
    # test if default initialization works, id=(last-layer, kron)
    lap = Laplace(model, likelihood)
    assert isinstance(lap, KronLLLaplace)


@pytest.mark.parametrize('laplace, key', zip(flavors, all_keys))
def test_all_init(laplace, key, model, likelihood='classification'):
    # test if all flavors are correctly initialized
    w, s = key
    lap = Laplace(model, likelihood, subset_of_weights=w, hessian_structure=s)
    assert isinstance(lap, laplace)


@pytest.mark.parametrize('key', all_keys)
def test_opt_keywords(key, model, likelihood='classification'):
    # test if optional keywords are correctly passed on
    w, s = key
    prior_mean = torch.zeros_like(parameters_to_vector(model.parameters()))
    lap = Laplace(model, likelihood, subset_of_weights=w, hessian_structure=s,
                  prior_precision=0.01, prior_mean=prior_mean, temperature=10.)
    assert torch.allclose(lap.prior_mean, prior_mean)
    assert lap.prior_precision == 0.01
    assert lap.temperature == 10.

File Path: dependencies/laplace/tests/test_lllaplace.py
Content:
import pytest
from itertools import product
import numpy as np
import torch
from torch import nn
from torch.nn.utils import parameters_to_vector
from torch.utils.data import DataLoader, TensorDataset
from torch.distributions import Normal, Categorical

from laplace.lllaplace import LLLaplace, FullLLLaplace, KronLLLaplace, DiagLLLaplace
from laplace.feature_extractor import FeatureExtractor
from tests.utils import jacobians_naive


torch.manual_seed(240)
torch.set_default_tensor_type(torch.DoubleTensor)
flavors = [FullLLLaplace, KronLLLaplace, DiagLLLaplace]


@pytest.fixture
def model():
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    return model


@pytest.fixture
def class_loader():
    X = torch.randn(10, 3)
    y = torch.randint(2, (10,))
    return DataLoader(TensorDataset(X, y), batch_size=3)


@pytest.fixture
def reg_loader():
    X = torch.randn(10, 3)
    y = torch.randn(10, 2)
    return DataLoader(TensorDataset(X, y), batch_size=3)


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init(laplace, model):
    lap = laplace(model, 'classification', last_layer_name='1')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_invalid_likelihood(laplace, model):
    with pytest.raises(ValueError):
        lap = laplace(model, 'otherlh', last_layer_name='1')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_noise(laplace, model):
    # float
    sigma_noise = 1.2
    lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise,
                  last_layer_name='1')
    # torch.tensor 0-dim
    sigma_noise = torch.tensor(1.2)
    lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise,
                  last_layer_name='1')
    # torch.tensor 1-dim
    sigma_noise = torch.tensor(1.2).reshape(-1)
    lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise,
                  last_layer_name='1')

    # for classification should fail
    sigma_noise = 1.2
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='classification',
                      sigma_noise=sigma_noise, last_layer_name='1')

    # other than that should fail
    # higher dim
    sigma_noise = torch.tensor(1.2).reshape(1, 1)
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise,
                      last_layer_name='1')
    # other datatype, only reals supported
    sigma_noise = '1.2'
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', sigma_noise=sigma_noise,
                      last_layer_name='1')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_precision(laplace, model):
    feature_extractor = FeatureExtractor(model, last_layer_name='1')
    model_params = list(feature_extractor.last_layer.parameters())
    setattr(model, 'n_layers', 2)  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    # float
    precision = 10.6
    lap = laplace(model, likelihood='regression', prior_precision=precision,
                  last_layer_name='1')
    # torch.tensor 0-dim
    precision = torch.tensor(10.6)
    lap = laplace(model, likelihood='regression', prior_precision=precision,
                  last_layer_name='1')
    # torch.tensor 1-dim
    precision = torch.tensor(10.7).reshape(-1)
    lap = laplace(model, likelihood='regression', prior_precision=precision,
                  last_layer_name='1')
    # torch.tensor 1-dim param-shape
    if laplace == KronLLLaplace:  # kron only supports per layer
        with pytest.raises(ValueError):
            precision = torch.tensor(10.7).reshape(-1).repeat(model.n_params)
            lap = laplace(model, likelihood='regression', prior_precision=precision,
                        last_layer_name='1')
    else:
        precision = torch.tensor(10.7).reshape(-1).repeat(model.n_params)
        lap = laplace(model, likelihood='regression', prior_precision=precision,
                    last_layer_name='1')
    # torch.tensor 1-dim layer-shape
    precision = torch.tensor(10.7).reshape(-1).repeat(model.n_layers)
    lap = laplace(model, likelihood='regression', prior_precision=precision,
                  last_layer_name='1')

    # other than that should fail
    # higher dim
    precision = torch.tensor(10.6).reshape(1, 1)
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', prior_precision=precision,
                      last_layer_name='1')
    # unmatched dim
    precision = torch.tensor(10.6).reshape(-1).repeat(17)
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', prior_precision=precision,
                      last_layer_name='1')
    # other datatype, only reals supported
    precision = '1.5'
    with pytest.raises(ValueError):
        lap = laplace(model, likelihood='regression', prior_precision=precision,
                      last_layer_name='1')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_prior_mean_and_scatter(laplace, model):
    lap_scalar_mean = laplace(model, 'classification', last_layer_name='1',
                              prior_precision=1e-2, prior_mean=1.)
    assert torch.allclose(lap_scalar_mean.prior_mean, torch.tensor([1.]))
    lap_tensor_mean = laplace(model, 'classification', last_layer_name='1',
                              prior_precision=1e-2, prior_mean=torch.ones(1))
    assert torch.allclose(lap_tensor_mean.prior_mean, torch.tensor([1.]))
    lap_tensor_scalar_mean = laplace(model, 'classification', last_layer_name='1',
                                     prior_precision=1e-2, prior_mean=torch.ones(1)[0])
    assert torch.allclose(lap_tensor_scalar_mean.prior_mean, torch.tensor(1.))
    lap_tensor_full_mean = laplace(model, 'classification', last_layer_name='1',
                                   prior_precision=1e-2, prior_mean=torch.ones(20*2+2))
    assert torch.allclose(lap_tensor_full_mean.prior_mean, torch.ones(20*2+2))
    expected = lap_scalar_mean.scatter
    assert expected.ndim == 0
    assert torch.allclose(lap_tensor_mean.scatter, expected)
    assert lap_tensor_mean.scatter.shape == expected.shape
    assert torch.allclose(lap_tensor_scalar_mean.scatter, expected)
    assert lap_tensor_scalar_mean.scatter.shape == expected.shape
    assert torch.allclose(lap_tensor_full_mean.scatter, expected)
    assert lap_tensor_full_mean.scatter.shape == expected.shape

    # too many dims
    with pytest.raises(ValueError):
        prior_mean = torch.ones(20*2+2).unsqueeze(-1)
        laplace(model, 'classification', last_layer_name='1',
                prior_precision=1e-2, prior_mean=prior_mean)

    # unmatched dim
    with pytest.raises(ValueError):
        prior_mean = torch.ones(20*2-3)
        laplace(model, 'classification', last_layer_name='1',
                prior_precision=1e-2, prior_mean=prior_mean)

    # invalid argument type
    with pytest.raises(ValueError):
        laplace(model, 'classification', last_layer_name='1',
                prior_precision=1e-2, prior_mean='72')


@pytest.mark.parametrize('laplace', flavors)
def test_laplace_init_temperature(laplace, model):
    # valid float
    T = 1.1
    lap = laplace(model, likelihood='classification', temperature=T,
                  last_layer_name='1')
    assert lap.temperature == T


@pytest.mark.parametrize('laplace,lh', product(flavors, ['classification', 'regression']))
def test_laplace_functionality(laplace, lh, model, reg_loader, class_loader):
    if lh == 'classification':
        loader = class_loader
        sigma_noise = 1.
    else:
        loader = reg_loader
        sigma_noise = 0.3
    lap = laplace(model, lh, sigma_noise=sigma_noise, prior_precision=0.7)
    lap.fit(loader)
    assert lap.n_data == len(loader.dataset)
    assert lap.n_outputs == model.output_size
    X = loader.dataset.tensors[0]
    f = model(X)
    y = loader.dataset.tensors[1]
    assert f.shape == torch.Size([10, 2])

    # Test log likelihood (Train)
    log_lik = lap.log_likelihood
    # compute true log lik
    if lh == 'classification':
        log_lik_true = Categorical(logits=f).log_prob(y).sum()
        assert torch.allclose(log_lik, log_lik_true)
    else:
        assert y.size() == f.size()
        log_lik_true = Normal(loc=f, scale=sigma_noise).log_prob(y).sum()
        assert torch.allclose(log_lik, log_lik_true)
        # change likelihood and test again
        lap.sigma_noise = 0.72
        log_lik = lap.log_likelihood
        log_lik_true = Normal(loc=f, scale=0.72).log_prob(y).sum()
        assert torch.allclose(log_lik, log_lik_true)

    # Test marginal likelihood
    # lml = log p(y|f) - 1/2 theta @ prior_prec @ theta
    #       + 1/2 logdet prior_prec - 1/2 log det post_prec
    lml = log_lik_true
    feature_extractor = FeatureExtractor(model, last_layer_name='1')
    theta = parameters_to_vector(feature_extractor.last_layer.parameters()).detach()
    assert torch.allclose(theta, lap.mean)
    prior_prec = torch.diag(lap.prior_precision_diag)
    assert prior_prec.shape == torch.Size([len(theta), len(theta)])
    lml = lml - 1/2 * theta @ prior_prec @ theta
    Sigma_0 = torch.inverse(prior_prec)
    if laplace == DiagLLLaplace:
        log_det_post_prec = lap.posterior_precision.log().sum()
    else:
        log_det_post_prec = lap.posterior_precision.logdet()
    lml = lml + 1/2 * (prior_prec.logdet() - log_det_post_prec)
    assert torch.allclose(lml, lap.log_marginal_likelihood())

    # test sampling
    torch.manual_seed(61)
    samples = lap.sample(n_samples=1)
    assert samples.shape == torch.Size([1, len(theta)])
    samples = lap.sample(n_samples=1000000)
    assert samples.shape == torch.Size([1000000, len(theta)])
    mu_comp = samples.mean(dim=0)
    mu_true = lap.mean
    assert torch.allclose(mu_comp, mu_true, rtol=1)

    # test functional variance
    if laplace == FullLLLaplace:
        Sigma = lap.posterior_covariance
    elif laplace == KronLLLaplace:
        Sigma = lap.posterior_precision.to_matrix(exponent=-1)
    elif laplace == DiagLLLaplace:
        Sigma = torch.diag(lap.posterior_variance)
    _, phi = feature_extractor.forward_with_features(X)
    Js, f = jacobians_naive(feature_extractor.last_layer, phi)
    true_f_var = torch.einsum('mkp,pq,mcq->mkc', Js, Sigma, Js)
    # test last-layer Jacobians
    comp_Js, comp_f = lap.backend.last_layer_jacobians(lap.model, X)
    assert torch.allclose(Js, comp_Js)
    assert torch.allclose(f, comp_f)
    comp_f_var = lap.functional_variance(comp_Js)
    assert torch.allclose(true_f_var, comp_f_var, rtol=1e-4)


@pytest.mark.parametrize('laplace', flavors)
def test_regression_predictive(laplace, model, reg_loader):
    lap = laplace(model, 'regression', sigma_noise=0.3, prior_precision=0.7)
    lap.fit(reg_loader)
    X, y = reg_loader.dataset.tensors
    f = model(X)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive, functional variance tested already above.
    f_mu, f_var = lap(X, pred_type='glm')
    assert torch.allclose(f_mu, f)
    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])
    assert len(f_mu) == len(X)

    # NN predictive (only diagonal variance estimation)
    f_mu, f_var = lap(X, pred_type='nn')
    assert f_mu.shape == f_var.shape
    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1]])
    assert len(f_mu) == len(X)


@pytest.mark.parametrize('laplace', flavors)
def test_classification_predictive(laplace, model, class_loader):
    lap = laplace(model, 'classification', prior_precision=0.7)
    lap.fit(class_loader)
    X, y = class_loader.dataset.tensors
    f = torch.softmax(model(X), dim=-1)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive
    f_pred = lap(X, pred_type='glm', link_approx='mc', n_samples=100)
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1
    f_pred = lap(X, pred_type='glm', link_approx='probit')
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1
    f_pred = lap(X, pred_type='glm', link_approx='bridge')
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1

    # NN predictive
    f_pred = lap(X, pred_type='nn', n_samples=100)
    assert f_pred.shape == f.shape
    assert torch.allclose(f_pred.sum(), torch.tensor(len(f_pred), dtype=torch.double))  # sum up to 1


@pytest.mark.parametrize('laplace', flavors)
def test_regression_predictive_samples(laplace, model, reg_loader):
    lap = laplace(model, 'regression', sigma_noise=0.3, prior_precision=0.7)
    lap.fit(reg_loader)
    X, y = reg_loader.dataset.tensors
    f = model(X)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive, functional variance tested already above.
    fsamples = lap.predictive_samples(X, pred_type='glm', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])

    # NN predictive (only diagonal variance estimation)
    fsamples = lap.predictive_samples(X, pred_type='nn', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])


@pytest.mark.parametrize('laplace', flavors)
def test_classification_predictive_samples(laplace, model, class_loader):
    lap = laplace(model, 'classification', prior_precision=0.7)
    lap.fit(class_loader)
    X, y = class_loader.dataset.tensors
    f = torch.softmax(model(X), dim=-1)

    # error
    with pytest.raises(ValueError):
        lap(X, pred_type='linear')

    # GLM predictive
    fsamples = lap.predictive_samples(X, pred_type='glm', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])
    assert np.allclose(fsamples.sum().item(), len(f) * 100)  # sum up to 1

    # NN predictive
    f_pred = lap.predictive_samples(X, pred_type='nn', n_samples=100)
    assert fsamples.shape == torch.Size([100, f.shape[0], f.shape[1]])
    assert np.allclose(fsamples.sum().item(), len(f) * 100)  # sum up to 1

File Path: dependencies/laplace/tests/test_matrix.py
Content:
import pytest
import numpy as np
import torch
from torch import nn
from torch.nn.utils import parameters_to_vector

from laplace.matrix import Kron, KronDecomposed
from laplace.utils import kron as kron_prod
from laplace.curvature import BackPackGGN
from laplace.utils import block_diag
from tests.utils import get_psd_matrix, jacobians_naive


torch.set_default_tensor_type(torch.DoubleTensor)


@pytest.fixture
def model():
    model = torch.nn.Sequential(nn.Linear(3, 20), nn.Linear(20, 2))
    setattr(model, 'output_size', 2)
    model_params = list(model.parameters())
    setattr(model, 'n_layers', len(model_params))  # number of parameter groups
    setattr(model, 'n_params', len(parameters_to_vector(model_params)))
    return model


@pytest.fixture
def small_model():
    model = torch.nn.Sequential(nn.Linear(3, 5), nn.Tanh(), nn.Linear(5, 2))
    setattr(model, 'output_size', 2)
    return model


def test_init_from_model(model):
    kron = Kron.init_from_model(model, 'cpu')
    expected_sizes = [[20*20, 3*3], [20*20], [2*2, 20*20], [2*2]]
    for facs, exp_facs in zip(kron.kfacs, expected_sizes):
        for fi, exp_fi in zip(facs, exp_facs):
            assert torch.all(fi == 0)
            assert np.prod(fi.shape) == exp_fi


def test_addition(model):
    kron = Kron.init_from_model(model, 'cpu')
    expected_sizes = [[20, 3], [20], [2, 20], [2]]
    to_add = Kron([[torch.randn(i, i) for i in sizes] for sizes in expected_sizes])
    kron += to_add
    for facs, exp_facs in zip(kron.kfacs, to_add.kfacs):
        for fi, exp_fi in zip(facs, exp_facs):
            assert torch.allclose(fi, exp_fi)

def test_multiplication():
    # kron * x should be the same as the expanded kronecker product * x
    expected_sizes = [[20, 3], [20], [2, 20], [2]]
    kfacs = [[torch.randn(i, i) for i in sizes] for sizes in expected_sizes]
    kron = Kron(kfacs)
    kron *= 1.5
    for facs, exp_facs in zip(kron.kfacs, kfacs):
        if len(facs) == 1:
            assert torch.allclose(facs[0], 1.5 * exp_facs[0])
        else:  # len(facs) == 2
            exp = 1.5 * kron_prod(*exp_facs)
            facs = kron_prod(*facs)
            assert torch.allclose(exp, facs)

def test_decompose():
    expected_sizes = [[20, 3], [20], [2, 20], [2]]
    P = 20 * 3 + 20 + 2 * 20 + 2
    torch.manual_seed(7171)
    kfacs = [[get_psd_matrix(i) for i in sizes] for sizes in expected_sizes]
    kron = Kron(kfacs)
    kron_decomp = kron.decompose()
    for facs, Qs, ls in zip(kron.kfacs, kron_decomp.eigenvectors, kron_decomp.eigenvalues):
        if len(facs) == 1:
            H, Q, l = facs[0], Qs[0], ls[0]
            reconstructed = Q @ torch.diag(l) @ Q.T
            assert torch.allclose(H, reconstructed, rtol=1e-3)
        if len(facs) == 2:
            gtruth = kron_prod(facs[0], facs[1])
            rec_1 = Qs[0] @ torch.diag(ls[0]) @ Qs[0].T
            rec_2 = Qs[1] @ torch.diag(ls[1]) @ Qs[1].T
            reconstructed = kron_prod(rec_1, rec_2)
            assert torch.allclose(gtruth, reconstructed, rtol=1e-2)
    W = torch.randn(P)
    SW_kron = kron.bmm(W)
    SW_kron_decomp = kron_decomp.bmm(W, exponent=1)
    assert torch.allclose(SW_kron, SW_kron_decomp)


def test_logdet_consistent():
    expected_sizes = [[20, 3], [20], [2, 20], [2]]
    torch.manual_seed(7171)
    kfacs = [[get_psd_matrix(i) for i in sizes] for sizes in expected_sizes]
    kron = Kron(kfacs)
    kron_decomp = kron.decompose()
    assert torch.allclose(kron.logdet(), kron_decomp.logdet())


def test_bmm(small_model):
    model = small_model
    # model = single_output_model
    X = torch.randn(5, 3)
    y = torch.randn(5, 2)
    backend = BackPackGGN(model, 'regression', stochastic=False)
    loss, kron = backend.kron(X, y, N=5)
    kron_decomp = kron.decompose()
    Js, f = jacobians_naive(model, X)
    blocks = list()
    for F in kron.kfacs:
        if len(F) == 1:
            blocks.append(F[0])
        else:
            blocks.append(kron_prod(*F))
    S = block_diag(blocks)
    assert torch.allclose(S, S.T)
    assert torch.allclose(S.diagonal(), kron.diag())

    # test J @ Kron_decomp @ Jt (square form)
    JS = kron_decomp.bmm(Js, exponent=1)
    JS_true = Js @ S
    JSJ_true = torch.bmm(JS_true, Js.transpose(1,2))
    JSJ = torch.bmm(JS, Js.transpose(1,2))
    assert torch.allclose(JSJ, JSJ_true)
    assert torch.allclose(JS, JS_true)

    # test J @ Kron @ Jt (square form)
    JS_nodecomp = kron.bmm(Js)
    JSJ_nodecomp = torch.bmm(JS_nodecomp, Js.transpose(1,2))
    assert torch.allclose(JSJ_nodecomp, JSJ)
    assert torch.allclose(JS_nodecomp, JS)

    # test J @ S_inv @ J (funcitonal variance)
    JSJ = kron_decomp.inv_square_form(Js)
    S_inv = S.inverse()
    JSJ_true = torch.bmm(Js @ S_inv, Js.transpose(1,2))
    assert torch.allclose(JSJ, JSJ_true)

    # test J @ S^-1/2  (sampling)
    JS = kron_decomp.bmm(Js, exponent=-1/2)
    JSJ = torch.bmm(JS, Js.transpose(1,2))
    l, Q = torch.linalg.eigh(S_inv)
    JS_true = Js @ Q @ torch.diag(torch.sqrt(l)) @ Q.T
    JSJ_true = torch.bmm(JS_true, Js.transpose(1,2))
    assert torch.allclose(JS, JS_true)
    assert torch.allclose(JSJ, JSJ_true)

    # test different Js shapes:
    # 2 - dimensional
    JS = kron_decomp.bmm(Js[:, 0, :].squeeze(), exponent=1)
    JS_nodecomp = kron.bmm(Js[:, 0, :].squeeze())
    JS_true = Js[:, 0, :].squeeze() @ S
    assert torch.allclose(JS, JS_true)
    assert torch.allclose(JS, JS_nodecomp)
    # 1 - dimensional
    JS = kron_decomp.bmm(Js[0, 0, :].squeeze(), exponent=1)
    JS_nodecomp = kron.bmm(Js[0, 0, :].squeeze())
    JS_true = Js[0, 0, :].squeeze() @ S
    assert torch.allclose(JS, JS_true)
    assert torch.allclose(JS, JS_nodecomp)


def test_matrix_consistent():
    expected_sizes = [[20, 3], [20], [2, 20], [2]]
    torch.manual_seed(7171)
    kfacs = [[get_psd_matrix(i) for i in sizes] for sizes in expected_sizes]
    kron = Kron(kfacs)
    kron_decomp = kron.decompose()
    assert torch.allclose(kron.to_matrix(), kron_decomp.to_matrix(exponent=1))
    assert torch.allclose(kron.to_matrix().inverse(), kron_decomp.to_matrix(exponent=-1))
    M_true = kron.to_matrix()
    M_true.diagonal().add_(3.4)
    kron_decomp += torch.tensor(3.4)
    assert torch.allclose(M_true, kron_decomp.to_matrix(exponent=1))


def test_inplace_detach(model):
    kron = Kron.init_from_model(model, 'cpu')
    expected_sizes = [[20, 3], [20], [2, 20], [2]]
    to_add = Kron([[torch.ones(i, i, requires_grad=True) for i in sizes] for sizes in expected_sizes])
    kron += to_add

    for facs in kron.kfacs:
        for fi in facs:
            assert fi.grad_fn is not None

    returned_kron = kron.detach_()

    # Test if detached inplace
    for facs in kron.kfacs:
        for fi in facs:
            assert fi.grad_fn is None

    # Assert returned value is the same reference
    assert returned_kron is kron

def test_detach(model):
    kron = Kron.init_from_model(model, 'cpu')
    expected_sizes = [[20, 3], [20], [2, 20], [2]]
    to_add = Kron([[torch.randn(i, i, requires_grad=True) for i in sizes] for sizes in expected_sizes])
    kron += to_add

    detached_kron = kron.detach()
    
    # Assert original stays attached, returned is detached and values are equal
    for facs, detached_facs in zip(kron.kfacs, detached_kron.kfacs):
        for fi, detached_fi in zip(facs, detached_facs):
            assert torch.allclose(fi, detached_fi)
            assert fi.grad_fn is not None
            assert detached_fi.grad_fn is None

    assert detached_kron is not kron


File Path: dependencies/laplace/tests/test_utils.py
Content:
import torch
from laplace.utils import invsqrt_precision, diagonal_add_scalar, symeig


def test_sqrt_precision():
    X = torch.randn(20, 100)
    M = X @ X.T
    Scale = invsqrt_precision(M)
    torch.allclose(Scale @ Scale.T, torch.inverse(M))


def test_diagonal_add_scalar():
    M = torch.randn(50, 50)
    M_plus_diag = diagonal_add_scalar(M, 1.98)
    diag = 1.98 * torch.eye(len(M))
    torch.allclose(M_plus_diag, M + diag)


def test_symeig_custom():
    X = torch.randn(20, 100)
    M = X @ X.T
    l1, W1 = torch.linalg.eigh(M, UPLO='U')
    l2, W2 = symeig(M)
    assert torch.allclose(l1, l2)
    assert torch.allclose(W1, W2)
    

def test_symeig_custom_low_rank():
    X = torch.randn(1000, 10)
    M = X @ X.T
    l1, W1 = torch.linalg.eigh(M, UPLO='U')
    l2, W2 = symeig(M)
    # symeig should fail for low-rank
    assert not torch.all(l1 >= 0.0)
    # test clamping to zeros
    assert torch.all(l2 >= 0.0)
File Path: dependencies/laplace/tests/utils.py
Content:
import torch
from torch import nn


def get_psd_matrix(dim):
    X = torch.randn(dim, dim*3)
    return X @ X.T / (dim * 3)

def grad(model):
    return torch.cat([p.grad.data.flatten() for p in model.parameters()]).detach()

def jacobians_naive(model, data):
    model.zero_grad()
    f = model(data)
    Jacs = list()
    for i in range(f.shape[0]):
        if len(f.shape) > 1:
            jacs = list()
            for j in range(f.shape[1]):
                rg = (i != (f.shape[0] - 1) or j != (f.shape[1] - 1))
                f[i, j].backward(retain_graph=rg)
                Jij = grad(model)
                jacs.append(Jij)
                model.zero_grad()
            jacs = torch.stack(jacs).t()
        else:
            rg = (i != (f.shape[0] - 1))
            f[i].backward(retain_graph=rg)
            jacs = grad(model)
            model.zero_grad()
        Jacs.append(jacs)
    Jacs = torch.stack(Jacs).transpose(1, 2)
    return Jacs.detach(), f.detach()


def jacobians_naive_aug(model, data):
    model.zero_grad()
    f = model(data).mean(dim=1)
    Jacs = list()
    for i in range(f.shape[0]):
        if len(f.shape) > 1:
            jacs = list()
            for j in range(f.shape[1]):
                rg = (i != (f.shape[0] - 1) or j != (f.shape[1] - 1))
                f[i, j].backward(retain_graph=rg)
                Jij = grad(model)
                jacs.append(Jij)
                model.zero_grad()
            jacs = torch.stack(jacs).t()
        else:
            rg = (i != (f.shape[0] - 1))
            f[i].backward(retain_graph=rg)
            jacs = grad(model)
            model.zero_grad()
        Jacs.append(jacs)
    Jacs = torch.stack(Jacs).transpose(1, 2)
    return Jacs.detach(), f.detach()

    
def gradients_naive_aug(model, data, targets, likelihood):
    if likelihood == 'classification':
        lossf = torch.nn.CrossEntropyLoss(reduction='sum')
    elif likelihood == 'regression':
        lossf = torch.nn.MSELoss(reduction='sum')
    output = model(data).mean(dim=1)
    grads = list()
    loss = torch.zeros(0)
    for i in range(len(targets)):
        model.zero_grad()
        lossi = lossf(output[i].unsqueeze(0), targets[i].unsqueeze(0)) 
        lossi.backward(retain_graph=True)
        loss += lossi.detach()
        grads.append(grad(model))
    return torch.stack(grads), loss
 

class HetHead(nn.Module):

    def forward(self, input):
        return torch.stack([input[:, 0], -0.5 * torch.exp(input[:, 1])], 1)

File Path: generate_illustration_figures.py
Content:
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from tueplots import bundles, figsizes, axes
from scipy.interpolate import interp1d

import wandb


def get_runs(query):
    api = wandb.Api()
    return api.runs('aleximmer/ntk-marglik', query) 


def get_frame(runs, group_keys):
    df_marglik = pd.DataFrame(columns=group_keys)
    df_nll = pd.DataFrame(columns=group_keys)
    for i, run in enumerate(runs):
        if run.state != 'finished':
            continue
        for k in group_keys:
            df_marglik.loc[i, k] = df_nll.loc[i, k] = run.config[k]
        df_run = run.history(keys=['train/marglik', 'valid/nll'])
        try:
            steps = df_run._step.values.astype(int)
        except:
            print(run.config, 'failed')
            print(df_run.head())
            continue
        df_marglik.loc[i, df_run._step.values.astype(int)] = df_run['train/marglik'].values
        df_nll.loc[i, df_run._step.values.astype(int)] = df_run['valid/nll'].values
    steps = pd.DataFrame(index=steps)
    return steps, df_marglik, df_nll


def get_runtime_frame(runs, group_keys):
    df = pd.DataFrame(columns=group_keys + ['runtime'])
    for i, run in enumerate(runs):
        df.loc[i, group_keys] = [run.config[k] for k in group_keys]
        df_run = run.history(keys=['train/time_hyper'])
        df.loc[i, 'runtime'] = df_run.iloc[-3:].mean()['train/time_hyper']
    return df
    

def get_map_baseline():
    query = {'$and': [{'config.model': 'mininet'}, {'config.map': True}]}
    runs = get_runs(query)
    df_nll = pd.DataFrame(columns=['seed'])
    for i, run in enumerate(runs):
        df_nll.loc[i, 'seed'] = run.config['seed']
        # df_nll.loc[i, 'nll'] = run.summary['valid/nll']
        df_run = run.history(keys=['valid/nll'])
        steps = df_run._step.values.astype(int)
        df_nll.loc[i, steps] = df_run['valid/nll'].values
    steps = pd.DataFrame(index=steps)
    return steps, df_nll.drop('seed', axis=1)

    
def load_with_cache(base_name, query, group_keys):
    try:
        print('Trying to load locally')
        steps = pd.read_csv(f'results/{base_name}_steps.csv', index_col=0)
        marglik = pd.read_csv(f'results/{base_name}_marglik.csv', index_col=0)
        nll = pd.read_csv(f'results/{base_name}_nll.csv', index_col=0)
    except:
        print('Failed local loading. Loading from wandb.')
        steps, marglik, nll = get_frame(get_runs(query), group_keys)
        steps.to_csv(f'results/{base_name}_steps.csv')
        marglik.to_csv(f'results/{base_name}_marglik.csv')
        nll.to_csv(f'results/{base_name}_nll.csv')
    steps = steps.index.values
    return steps, marglik, nll

    
def parametric_bound_figure(method='baseline'):
    base_name = 'parametric_bound' + ('' if method == 'baseline' else '_lila')
    query = {
        '$and': [{'config.model': 'mininet'}, {'config.bound': 'lower'}, 
                 {'config.marglik_batch_size': 1000}, {'config.single_output': False},
                 {'config.grouped_loader': False}, {'config.approx': {'$ne': 'kernel'}},
                 {'config.method': method}, {'config.n_epochs': 500}]
    }
    steps, marglik, nll = load_with_cache(base_name, query, ['approx', 'seed'])
    seeds = len(set(marglik['seed']))
    marglik = marglik.drop('seed', axis=1)
    nll = nll.drop('seed', axis=1)
    marglik_group = marglik.groupby(by=['approx'])
    nll_group = nll.groupby(by=['approx'])
    marglik_m, marglik_ste = marglik_group.mean(), marglik_group.std() / np.sqrt(seeds)
    nll_m, nll_ste = nll_group.mean(), nll_group.std() / np.sqrt(seeds)

    with plt.rc_context({**bundles.icml2022(column='half'), 
                        **axes.lines(), 
                        **figsizes.icml2022_half(height_to_width_ratio=0.38)}):
        fig, axs = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False)
        axs[0].grid()
        axs[1].grid()
        axs[0].set_xlim([10, 500])
        axs[1].set_xlim([10, 500])
        if method == 'baseline':
            axs[0].set_ylim([-3, -0.48])
            axs[1].set_ylim([-0.8, -0.18])
        else:
            axs[0].set_ylim([-4, -0.7])
            axs[1].set_ylim([-2.3, -0.3])
        axs[0].set_ylabel('$\log q(\mathcal{D} | \mathbf{h})$')
        axs[1].set_ylabel('test log likelihood')
        axs[0].set_xlabel('steps')
        axs[1].set_xlabel('steps')

        approxs = ['full', 'blockdiag', 'kron', 'diag']
        blue = blue = mpl.cm.get_cmap('Blues')(1.0)
        colors = [blue, 'tab:red', 'tab:orange', 'tab:purple']

        for i, approx in enumerate(approxs):
            color = colors[i]
            m, ste = -marglik_m.loc[approx].rolling(5).mean(), marglik_ste.loc[approx].rolling(5).mean()
            axs[0].plot(steps, m, label=approx, c=color, alpha=0.9)
            axs[0].fill_between(steps, m-ste, m+ste, alpha=0.3, color=color)

            m, ste = -nll_m.loc[approx].rolling(5).mean(), nll_ste.loc[approx].rolling(5).mean()
            axs[1].plot(steps, m, label=approx, c=color, alpha=0.9)
            axs[1].fill_between(steps, m-ste, m+ste, alpha=0.3, color=color)

        axs[1].legend()
        # plt.savefig(f'figures/{base_name}.pdf')
        plt.show()


def bound_figure(method, base_name, approx, cmap):
    base_name = f'{base_name}_bound' + ('' if method == 'baseline' else '_lila')
    query = {
        '$and': [{'config.model': 'mininet'}, {'config.approx': approx},
                 {'config.single_output': False}, {'config.grouped_loader': False},
                 {'config.bound': 'lower'}, {'config.method': method},
                 {'config.n_epochs': 500}]
    }
    steps, marglik, nll = load_with_cache(base_name, query, ['marglik_batch_size', 'seed'])
    seeds = len(set(marglik['seed']))
    marglik = marglik.drop('seed', axis=1)
    nll = nll.drop('seed', axis=1)
    marglik_group = marglik.groupby(by=['marglik_batch_size'])
    nll_group = nll.groupby(by=['marglik_batch_size'])
    marglik_m, marglik_ste = marglik_group.mean(), marglik_group.std() / np.sqrt(seeds)
    nll_m, nll_ste = nll_group.mean(), nll_group.std() / np.sqrt(seeds)

    cmap = plt.cm.get_cmap(cmap)
    batch_sizes = sorted(list(set(marglik['marglik_batch_size'])))

    with plt.rc_context({**bundles.icml2022(column='half'), 
                        **axes.lines(), 
                        **figsizes.icml2022_half(height_to_width_ratio=0.38)}):
        fig, axs = plt.subplots(nrows=1, ncols=3, sharex=False, sharey=False, 
                                gridspec_kw={'width_ratios': [10, 10, 1]})
        axs[0].grid()
        axs[1].grid()
        axs[0].set_xlim([10, 500])
        axs[1].set_xlim([10, 500])
        if method == 'baseline':
            axs[0].set_ylim([-3, -0.48])
            axs[1].set_ylim([-0.8, -0.18])
        else:
            axs[0].set_ylim([-4, -0.7])
            axs[1].set_ylim([-2.3, -0.3])
        axs[0].set_ylabel('$\log q(\mathcal{D} | \mathbf{h})$')
        axs[1].set_ylabel('test log likelihood')
        axs[0].set_xlabel('steps')
        axs[1].set_xlabel('steps')
        min_bs, max_bs = min(batch_sizes), max(batch_sizes)
        norm = mpl.colors.LogNorm(vmin=min_bs/10, vmax=max_bs)

        for bs in batch_sizes:
            color = cmap(norm(bs))
            m, ste = -marglik_m.loc[bs].rolling(5).mean(), marglik_ste.loc[bs].rolling(5).mean()
            axs[0].plot(steps, m, label=bs, c=color)
            axs[0].fill_between(steps, m-ste, m+ste, alpha=0.3, color=color)

            m, ste = -nll_m.loc[bs].rolling(5).mean(), nll_ste.loc[bs].rolling(5).mean()
            axs[1].plot(steps, m, label=bs, c=color)
            axs[1].fill_between(steps, m-ste, m+ste, alpha=0.3, color=color)

        cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), cax=axs[2])
        axs[2].set_ylim([10, 1000])
        cbar.minorticks_off()
        cbar.set_label('indices $|\mathcal{B}_m|$')
        cbar.set_ticks(batch_sizes)
        C = 10
        cbar.set_ticklabels([str(e*C) for e in batch_sizes])
        # plt.savefig(f'figures/{base_name}.pdf')
        plt.show()


def classwise_figure(method, base_name, approx, cmap):
    base_name = f'{base_name}_classwise' + ('' if method == 'baseline' else '_lila')
    query = {
        '$and': [{'config.model': 'mininet'}, {'config.approx': approx},
                 {'config.bound': 'lower'}, {'config.method': method},
                 {'config.grouped_loader': False}, {'config.n_epochs': 500},
                 {'config.single_output': True}]
    }
    steps, marglik, nll = load_with_cache(base_name, query, ['marglik_batch_size', 'seed'])
    seeds = len(set(marglik['seed']))
    marglik = marglik.drop('seed', axis=1)
    nll = nll.drop('seed', axis=1)
    marglik_group = marglik.groupby(by=['marglik_batch_size'])
    nll_group = nll.groupby(by=['marglik_batch_size'])
    marglik_m, marglik_ste = marglik_group.mean(), marglik_group.std() / np.sqrt(seeds)
    nll_m, nll_ste = nll_group.mean(), nll_group.std() / np.sqrt(seeds)
    cmap = plt.cm.get_cmap(cmap)

    batch_sizes = sorted(list(set(marglik['marglik_batch_size'])))

    with plt.rc_context({**bundles.icml2022(column='half'), 
                        **axes.lines(), 
                        **figsizes.icml2022_half(height_to_width_ratio=0.38)}):
        fig, axs = plt.subplots(nrows=1, ncols=3, sharex=False, sharey=False, 
                                gridspec_kw={'width_ratios': [10, 10, 1]})
        axs[0].grid()
        axs[1].grid()
        axs[0].set_xlim([10, 500])
        axs[1].set_xlim([10, 500])
        if method == 'baseline':
            axs[0].set_ylim([-3, -0.48])
            axs[1].set_ylim([-0.8, -0.18])
        else:
            axs[0].set_ylim([-4, -0.7])
            axs[1].set_ylim([-2.3, -0.3])
        axs[0].set_ylabel('$\log q(\mathcal{D} | \mathbf{h})$')
        axs[1].set_ylabel('test log likelihood')
        axs[0].set_xlabel('steps')
        axs[1].set_xlabel('steps')
        min_bs, max_bs = min(batch_sizes), max(batch_sizes)
        norm = mpl.colors.LogNorm(vmin=min_bs/10, vmax=max_bs)

        for bs in batch_sizes:
            color = cmap(norm(bs))
            m, ste = -marglik_m.loc[bs].rolling(5).mean(), marglik_ste.loc[bs].rolling(5).mean()
            axs[0].plot(steps, m, label=bs, c=color)
            axs[0].fill_between(steps, m-ste, m+ste, alpha=0.3, color=color)

            m, ste = -nll_m.loc[bs].rolling(5).mean(), nll_ste.loc[bs].rolling(5).mean()
            axs[1].plot(steps, m, label=bs, c=color)
            axs[1].fill_between(steps, m-ste, m+ste, alpha=0.3, color=color)

        cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), cax=axs[2])
        axs[2].set_ylim([10, 1000])
        cbar.minorticks_off()
        cbar.set_label('indices $|\mathcal{B}_m|$')
        cbar.set_ticks(batch_sizes)
        cbar.set_ticklabels([str(e) for e in batch_sizes])
        # plt.savefig(f'figures/{base_name}.pdf')
        plt.show()

        
def pareto_figure(method):
    query = {
        '$and': [{'config.model': 'mininet', 'config.bound': 'lower', 'config.grouped_loader': False, 
                  'config.method': method, 'config.n_epochs': 500, 'config.approx': {'$ne': 'blockdiag'}}]
    }
    # runs = get_runs(query_nobound_lila)
    query_time = {
        '$and': [{'config.model': 'mininet', 'config.bound': 'lower', 'config.grouped_loader': False, 
                  'config.method': method, 'config.n_epochs': 5}]
    }
    group_keys = ['marglik_batch_size', 'single_output', 'approx']
    _, df_marglik, df_nll = load_with_cache('pareto_' + method, query, group_keys)
    margliks = df_marglik.groupby(['marglik_batch_size', 'single_output', 'approx']).mean().iloc[:, -1]
    nlls = df_nll.groupby(['marglik_batch_size', 'single_output', 'approx']).mean().iloc[:, -1]
    df_time = get_runtime_frame(get_runs(query_time), group_keys)
    df = df_time.groupby(group_keys).mean()
    df.loc[margliks.index, 'marglik'] = margliks
    df.loc[margliks.index, 'nll'] = nlls
    df = df.reset_index()
    df.loc[~df.single_output, 'marglik_batch_size'] *= 10

    with plt.rc_context({**bundles.icml2022(column='half'), 
                        **axes.lines(), 
                        **figsizes.icml2022_half(),
                        **{'figure.figsize': (2.75, 2.0)}}):
        fig, axs = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, 
                                gridspec_kw={'width_ratios': [20, 1]})
        scax, cax = axs
        scax.grid()
        
        batch_sizes = sorted(list(set(df['marglik_batch_size'])))
        batch_sizes.remove(200)
        
        # marker types
        approx_to_marker = {
            'full': 's', 'kernel': 'd', 'kron': 'P', 'diag': '^'
        }
        df['marker'] = [approx_to_marker[e] for e in df.approx]
        df['line'] = None
        
        # labels
        approxs = ['full', 'kernel', 'kron', 'diag']
        labels = ['Full $\\textsc{ggn}$', '$\\textsc{ntk}$', '$\\textsc{kfac}$', 'Diag $\\textsc{ggn}$']
        mapper = {app: lbl for app, lbl in zip(approxs, labels)}
        df['label'] = [mapper[e] for e in df.approx]
        
        
        # colors
        cmap = plt.cm.get_cmap('inferno_r')
        min_bs, max_bs = min(df.marglik_batch_size), max(df.marglik_batch_size)
        norm = mpl.colors.LogNorm(vmin=min_bs, vmax=max_bs)
        
        for i, row in df.iterrows():
            color = cmap(norm(row.marglik_batch_size))
            line = scax.scatter(row.runtime, row.marglik, marker=row.marker, s=30 if row.approx == 'full' else 40,
                            facecolor=color, label=row.label, linewidth=0.1, alpha=0.9, color='black')
            df.loc[i, 'line'] = line
        
        handles = list()
        base_mask = (df.marglik_batch_size == 10000)
        for approx in approxs:
            handles.append(df.loc[(df.approx == approx) & base_mask, 'line'].values[0])
        scax.legend(handles, labels)
            
        cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cax)
        cax.set_ylim([10, 10000])
        cbar.minorticks_off()
        cbar.set_label('batch size')
        cbar.set_ticks(batch_sizes)
        cbar.set_ticklabels([str(e) for e in batch_sizes])

        if method == 'lila':
            scax.set_xticks([0, 0.5, 1, 1.5, 2, 2.5, 3])
            scax.set_yticks([0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])
            scax.set_ylim([0.7, 2.57])
            scax.set_xlim([-0.25, 3.25])
            pareto_line = np.array([[0.05, 2.7], [0.072, 2.4], [0.08, 1.6], [0.1, 1.4], [0.2, 1.2],
                                    [0.25, 1.02], [0.8, 0.95], [1.8, 0.82], [3.0, 0.815], [3.2, 0.812]])
            xs = np.linspace(0.05, 3.2, 100)
            f = interp1d(pareto_line[:, 0], pareto_line[:, 1], kind='slinear')
            scax.plot(xs-0.05, f(xs)-0.02, color='black', ls='--')
            scax.text(0.25, 0.8, 'Pareto', rotation=-10)
        else:
            scax.set_xscale('log')
            base_pareto_line = np.array([[0.0139, 2.9], [0.0145, 2.3], [0.022, 1.25], [0.028, 1.0], 
                                         [0.045, 0.71], [0.35, 0.69], [1.0, 0.55], [1.2, 0.549]])
            f = interp1d(base_pareto_line[:, 0], base_pareto_line[:, 1], kind='slinear')
            xs = np.linspace(0.0139, 1.2, 100)
            scax.plot(xs*0.9, f(xs)*0.9, color='black', ls='--')
            scax.set_xlim([9e-3, 1.7])
            scax.set_ylim([0.4, 2.53])
        
        scax.set_ylabel('Negative Log Marginal Likelihood')
        scax.set_xlabel('Runtime (s)')
        # plt.savefig(f'figures/pareto_{method}.pdf')
        plt.show()


def grid_bound_stochastic_figure(approximation, cmap, single_output=False, grouped_loader=False):
    str_id = f'{approximation}_so={single_output}_grouped={grouped_loader}_sto=True'
    df_prior = pd.read_csv(f'results_grid/grid_bound_prior_{str_id}.csv', index_col=0).astype(float)
    df_inv = pd.read_csv(f'results_grid/grid_bound_invariance_{str_id}.csv', index_col=0).astype(float)
    df_prior_sem = pd.read_csv(f'results_grid/grid_bound_sem_prior_{str_id}.csv', index_col=0).astype(float)
    df_inv_sem = pd.read_csv(f'results_grid/grid_bound_sem_invariance_{str_id}.csv', index_col=0).astype(float)
    cmap = plt.cm.get_cmap(cmap)
    batch_sizes = list(df_prior.index)
    with plt.rc_context({**bundles.icml2022(column='half'), 
                        **axes.lines(), 
                        **figsizes.icml2022_half(height_to_width_ratio=0.38)}):
        fig, axs = plt.subplots(nrows=1, ncols=3, sharex=False, sharey=False, 
                                gridspec_kw={'width_ratios': [10, 10, 1]})
        axs[0].grid()
        axs[1].grid()
        # axs[0].set_title('Prior MNIST')
        # axs[1].set_title('Invar. rotated MNIST')
        axs[0].set_xscale('log')
        axs[0].set_xlim([1e-2, 5e2])
        ymin = -6 if approximation == 'kernel' else -7
        axs[0].set_yticks([-2, -4, -6])
        axs[1].set_yticks([-2, -4, -6])
        axs[0].set_ylim([ymin, -0.5])
        axs[1].set_ylim([ymin, -1.1])
        axs[0].set_ylabel('$\log q_{\\tilde{\mathbf{w}}}(\mathcal{D} | \mathbf{h})$')
        axs[0].set_xlabel('prior precision')
        axs[1].set_xlabel('rotational invariance')
        axs[1].set_xticks([0, np.pi/2, np.pi])
        axs[1].set_xticklabels(['0', '$\pi/2$', '$\pi$'])
        axs[1].set_xlim([0, np.pi])
        min_bs, max_bs = min(batch_sizes), max(batch_sizes)
        norm = mpl.colors.LogNorm(vmin=min_bs/10, vmax=max_bs)

        for bs in batch_sizes:
            xs = df_prior.columns.astype(float)
            color = cmap(norm(bs))
            ys = df_prior.loc[bs]
            yerrs = df_prior_sem.loc[bs]
            mask = (~ np.isnan(ys))
            axs[0].plot(xs[mask], ys[mask], label=bs, c=color)
            axs[0].fill_between(xs[mask], ys[mask] - yerrs[mask], ys[mask] + yerrs[mask], alpha=0.2, color=color)
            xs = df_inv.columns.astype(float)
            ys = df_inv.loc[bs]
            yerrs = df_inv_sem.loc[bs]
            mask = (~ np.isnan(ys))
            axs[1].plot(xs[mask], ys[mask], label=bs, c=color)
            axs[1].fill_between(xs[mask], ys[mask] - yerrs[mask], ys[mask] + yerrs[mask], alpha=0.2, color=color)

        cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), cax=axs[2])
        axs[2].set_ylim([10, 1000])
        cbar.minorticks_off()
        cbar.set_label('indices $|\mathcal{B}_m|$')
        cbar.set_ticks(batch_sizes)
        C = 1 if single_output else 10 
        cbar.set_ticklabels([str(e*C) for e in batch_sizes])
        identifier = f'{approximation}_so={single_output}_grouped={grouped_loader}'
        # plt.savefig(f'figures/grid_bound_{identifier}.pdf')
        plt.show()


def grid_bound_parametric_figure():
    approxs = ['full', 'blockdiag', 'kron', 'diag']
    blue = blue = mpl.cm.get_cmap('Blues')(1.0)
    colors = [blue, 'tab:red', 'tab:orange', 'tab:purple']
    results = dict()
    for approximation in approxs:
        str_id = f'{approximation}_so=False_grouped=False_sto=False'
        df_prior = pd.read_csv(f'results_grid/grid_bound_prior_{str_id}.csv', index_col=0).astype(float)
        df_inv = pd.read_csv(f'results_grid/grid_bound_invariance_{str_id}.csv', index_col=0).astype(float)
        df_prior_sem = pd.read_csv(f'results_grid/grid_bound_sem_prior_{str_id}.csv', index_col=0).astype(float)
        df_inv_sem = pd.read_csv(f'results_grid/grid_bound_sem_invariance_{str_id}.csv', index_col=0).astype(float)
        results[approximation] = dict(prior=dict(mean=df_prior, sem=df_prior_sem), inv=dict(mean=df_inv, sem=df_inv_sem))
    
    with plt.rc_context({**bundles.icml2022(column='half'), 
                        **axes.lines(), 
                        **figsizes.icml2022_half(height_to_width_ratio=0.38)}):
        fig = plt.figure(constrained_layout=True)
        gs = fig.add_gridspec(2, 2, height_ratios=[4, 3], wspace=0.00)
        ax0 = fig.add_subplot(gs[:, 0])
        ax1_top = fig.add_subplot(gs[0, 1])
        ax1_bottom = fig.add_subplot(gs[1, 1])
        axs = [ax0, ax1_top, ax1_bottom]
        ax1_top.set_xticklabels([])
        ax1_top.set_xticks([])
        ax1_top.set_xlabel('')
        axs[0].grid()
        axs[1].grid()
        axs[2].grid()
        axs[0].set_xscale('log')
        axs[0].set_xlim([1e-2, 1e3])
        axs[0].set_ylim([-6, -0.5])
        axs[0].set_ylabel('$\log q_{\\tilde{\mathbf{w}}}(\mathcal{D} | \mathbf{h})$')
        axs[0].set_xlabel('prior precision')
        axs[2].set_xlabel('rotational invariance')
        axs[2].set_xticks([0, np.pi/2, np.pi])
        axs[2].set_xticklabels(['0', '$\pi/2$', '$\pi$'])
        axs[2].set_xlim([0, np.pi])
        axs[1].set_xlim([0, np.pi])
        axs[1].spines.bottom.set_visible(False)
        axs[1].set_xticks([np.pi/2])
        axs[1].tick_params(labelbottom=False)  # don't put tick labels at the top
        axs[2].spines.top.set_visible(False)
        d = .33  # proportion of vertical to horizontal extent of the slanted line
        kwargs = dict(marker=[(-1, -d), (1, d)], markersize=5,
                    linestyle="none", color='k', mec='k', mew=1, clip_on=False)
        ax1_top.plot([0, 1], [0, 0], transform=ax1_top.transAxes, **kwargs)
        ax1_bottom.plot([0, 1], [1, 1], transform=ax1_bottom.transAxes, **kwargs)

        for approx, color in zip(approxs, colors):
            df_prior = results[approx]['prior']['mean']
            df_inv = results[approx]['inv']['mean']
            df_prior_sem = results[approx]['prior']['sem']
            df_inv_sem = results[approx]['inv']['sem']
            xs = df_prior.columns.astype(float)
            ys = df_prior.loc[1000].values.astype(float)
            yerr = df_prior_sem.loc[1000].values.astype(float)
            axs[0].plot(xs, ys, label=approx, c=color)
            axs[0].fill_between(xs, ys - yerr, ys + yerr, alpha=0.2, color=color)
            xs = df_inv.columns.astype(float)
            ys = df_inv.loc[1000].values.astype(float)
            yerr = df_inv_sem.loc[1000].values.astype(float)
            if approx == 'diag':
                axs[2].plot(xs, ys, label=approx, c=color)
                axs[2].fill_between(xs, ys - yerr, ys + yerr, alpha=0.2, color=color)
            else:
                axs[1].plot(xs, ys, label=approx, c=color)
                axs[1].fill_between(xs, ys - yerr, ys + yerr, alpha=0.2, color=color)
        axs[0].legend(loc='lower left')
        # plt.savefig(f'figures/grid_bound_parametric.pdf')
        plt.show()


if __name__ == '__main__':
    grid_bound_parametric_figure()
    grid_bound_stochastic_figure('kernel', 'Blues')
    grid_bound_stochastic_figure('kernel', 'Blues', single_output=True)
    grid_bound_stochastic_figure('kernel', 'Blues', grouped_loader=True)
    grid_bound_stochastic_figure('kernel', 'Blues', single_output=True, grouped_loader=True)
    grid_bound_stochastic_figure('blockdiag', 'Reds')
    grid_bound_stochastic_figure('kron', 'Oranges')
    pareto_figure('lila')
    pareto_figure('baseline')

    parametric_bound_figure('baseline')
    bound_figure('baseline', 'kernel', 'kernel', 'Blues')
    bound_figure('baseline', 'doubly_full', 'full', 'Blues')
    bound_figure('baseline', 'doubly_block', 'blockdiag', 'Reds')
    bound_figure('baseline', 'doubly_kron', 'kron', 'Oranges')
    bound_figure('baseline', 'doubly_diag', 'diag', 'Purples')
    classwise_figure('baseline', 'kernel', 'kernel', 'Blues')
    classwise_figure('baseline', 'kron', 'kron', 'Oranges')
    classwise_figure('baseline', 'blockdiag', 'blockdiag', 'Reds')

    parametric_bound_figure('lila')
    bound_figure('lila', 'kernel', 'kernel', 'Blues')
    bound_figure('lila', 'doubly_full', 'full', 'Blues')
    bound_figure('lila', 'doubly_block', 'blockdiag', 'Reds')
    bound_figure('lila', 'doubly_kron', 'kron', 'Oranges')
    bound_figure('lila', 'doubly_diag', 'diag', 'Purples')
    classwise_figure('lila', 'kernel', 'kernel', 'Blues')
    classwise_figure('lila', 'kron', 'kron', 'Oranges')
    classwise_figure('lila', 'blockdiag', 'blockdiag', 'Reds')
    
    # improvement of using grouping
    for method in ['baseline', 'lila']:
        print('*' * 10 + method + '*' * 10)
        for approx in ['full', 'kernel', 'kron', 'diag']:
            base_name = f'{approx}_partition' + ('' if method == 'baseline' else '_lila')
            query = {
                '$and': [{'config.model': 'mininet'}, {'config.approx': approx},
                         {'config.bound': 'lower'}, {'config.method': method},
                         {'config.n_epochs': 500}]
            }
            steps, marglik, nll = load_with_cache(
                base_name, query, ['grouped_loader']
            )
            # print(marglik.mean(axis=1).head())
            res = marglik.groupby('grouped_loader').mean()
            t, f = res.loc[True].mean(), res.loc[False].mean()
            print(f'Marglik: {approx} with grouping {t}, without {f} improvement {(f/t-1)*100:.2f}%')
            res = nll.groupby('grouped_loader').mean()
            t, f = res.loc[True].mean(), res.loc[False].mean()
            print(f'NLL: {approx} with grouping {t}, without {f} improvement {(f/t-1)*100:.2f}%')

File Path: ntkmarglik/__init__.py
Content:

File Path: ntkmarglik/invariances.py
Content:
import torch
import torch.nn.functional as F
import numpy as np
from math import pi
import warnings
warnings.filterwarnings('ignore')

from torchvision.transforms import RandomHorizontalFlip


class RotationLayer(torch.nn.Module):
    """ Layer that creates rotated versions of input sampled between U[-parameter, +parameter],
        with single scalar parameter.

        Args:
            rot_factor: Initial rotation factor (0 = no invariance, 1 = full circle ±180 degrees)
            deterministic: If true, we replace the stochastic reparameterization trick by scaling fixed linspaced samples
            n_samples: Amount of samples to use.
            independent: Independently sample datapoints within batch (otherwise the n_samples samples are joinedly sampled within the batch for speed-up)

     """
    def __init__(self, rot_factor=0.0, deterministic=False, n_samples=32, independent=False):
        super().__init__()

        self.deterministic = deterministic
        self.n_samples = n_samples
        self.independent = independent
        assert not (independent and self.deterministic), f"Independent sampling is only possible when sampling non deterministically."

        self.rot_factor = torch.nn.Parameter(torch.full((1,), rot_factor))

        if deterministic:
            # Distribute equally over circle in such way that endpoints don't overlap when fully invariant;
            if n_samples % 2 == 0:
                # even
                start = -(n_samples // 2 - 1) / (n_samples // 2)
                end = 1
            else:
                # uneven
                start = -(1 - 1 / n_samples)
                end = (1 - 1 / n_samples)
            self.register_buffer('_linspace', torch.linspace(start, end, n_samples), persistent=False)


    def forward(self, x):
        """Connects to the next available port.

        Args:
          x: Input tensor with dimensions (B, 2)

        Returns:
          Rotated input 'n_samples' times with rotations uniformly sampled between -rot_factor*pi and +rot_factor*pi rads.
          Output dimension: (B, n_samples, 2)

        """

        # Obtain sample values
        if self.deterministic:
            samples = self._linspace
        else:
            device = x.device

            if self.independent:
                batch_size = len(x)
                samples = torch.rand((batch_size, self.n_samples), device=device) * 2 - 1
            else:
                samples = torch.rand(self.n_samples, device=device) * 2 - 1

        # Build rotation matrices
        rads = samples * np.pi * self.rot_factor

        c, s = torch.cos(rads), torch.sin(rads)

        if self.independent:
            matrices = torch.stack((torch.stack((c, -s), 0), torch.stack((s, c), 0)), 0)
            matrices = matrices.permute((2, 3, 0, 1)) # (batch_size, n_samples, 2, 2)

            # Apply to input
            return torch.einsum('bcji,bi->bcj', matrices, x)
        else:
            matrices = torch.stack((torch.stack((c, -s), 0), torch.stack((s, c), 0)), 0)
            matrices = matrices.permute((2, 0, 1)) # (n_samples, 2, 2)

            # Apply to input
            return torch.einsum('cji,bi->bcj', matrices, x)


class AffineLayer2d(torch.nn.Module):
    """ Layer that creates affine transformed versions of 2d image inputs sampled between U(-parameter, +parameter),
        with single scalar parameter.
        Args:
            rot_factor: Initial rotation factor (0 = no invariance, 1 = full circle ±180 degrees)
            deterministic: If true, we replace the stochastic reparameterization trick by scaling fixed linspaced samples
            n_samples: Amount of samples to use.
            independent: Independently sample datapoints within batch (otherwise the n_samples samples are joinedly sampled within the batch for speed-up)
     """
    def __init__(self, n_samples=32, init_value=0.0, softplus=False, softcap_rotation=False, random_flip=False):
        super().__init__()

        self.n_samples = n_samples
        if random_flip:
            self.flip = RandomHorizontalFlip()
        else:
            self.flip = False
        if softplus:
            assert init_value > 0.0, 'Cannot initialize to 0 with softplus, would be -inf.'
            init_value = np.log(np.exp(init_value) - 1)
        self.rot_factor = torch.nn.Parameter(torch.full((6,), init_value))
        self.softplus = softplus
        self.softcap_rotation = softcap_rotation

        # Define generators for the Lie group
        G1 = torch.tensor([0, 0, 1,
                           0, 0, 0,
                           0, 0, 0]).reshape(1, 1, 3, 3)  # x-translation
        G2 = torch.tensor([0, 0, 0,
                           0, 0, 1,
                           0, 0, 0]).reshape(1, 1, 3, 3)  # y-translation
        G3 = torch.tensor([0, -1, 0,
                           1, 0, 0,
                           0, 0, 0]).reshape(1, 1, 3, 3) # rotation
        G4 = torch.tensor([1, 0, 0,
                           0, 0, 0,
                           0, 0, 0]).reshape(1, 1, 3, 3)  # x-scale
        G5 = torch.tensor([0, 0, 0,
                           0, 1, 0,
                           0, 0, 0]).reshape(1, 1, 3, 3) # y-scale
        G6 = torch.tensor([0, 1, 0,
                           1, 0, 0,
                           0, 0, 0]).reshape(1, 1, 3, 3)  # shearing

        self.register_buffer('G1', G1)
        self.register_buffer('G2', G2)
        self.register_buffer('G3', G3)
        self.register_buffer('G4', G4)
        self.register_buffer('G5', G5)
        self.register_buffer('G6', G6)


    def forward(self, x, align_corners=True, mode='bilinear'):
        """Connects to the next available port.
        Args:
            x: Input tensor with dimensions (B, C, H, W)
            align_corners: Uses align_corners convention.
            mode: Type of interpolation. (nearest|bilinear)
        Returns:
            Rotated input 'n_samples' times with rotations uniformly sampled between -rot_factor*pi and +rot_factor*pi rads.
            Output dimension: (B, n_samples, C, H, W)
        """
        # Obtain sample values
        device = x.device

        # Build resampling grids
        N, C, H, W = x.shape
        k = 6

        # Independently sample random points from U[-1, 1]^k
        k_samples = [torch.rand(N, self.n_samples, device=device).unsqueeze(2).unsqueeze(3) * 2 - 1 for _ in range(k)]

        rot_factor = self.rot_factor if not self.softplus else F.softplus(self.rot_factor)

        # Cap rotation. Optionally soft, always hard
        rot_id = 2
        if self.softcap_rotation:
            first_part = rot_factor[:rot_id]
            rot_part = rot_factor[rot_id:rot_id+1]
            last_part = rot_factor[rot_id+1:]
            rot_factor = torch.cat((first_part, pi * torch.tanh(rot_part), last_part))

        M = (k_samples[0] * rot_factor[0] * self.G1 + \
             k_samples[1] * rot_factor[1] * self.G2 + \
             k_samples[2] * rot_factor[2].clamp(-pi, pi) * self.G3 + \
             k_samples[3] * rot_factor[3] * self.G4 + \
             k_samples[4] * rot_factor[4] * self.G5 + \
             k_samples[5] * rot_factor[5] * self.G6)

        # Exponentiate from Lie algebra to Lie group
        matrices_batch = torch.matrix_exp(M.view(N*self.n_samples, 3, 3)).view(N, self.n_samples, 3, 3)[:, :, :2, :]

        # Evaluate corresponding vector field on pixel locations
        out_shape = (N * self.n_samples, C, H, W)

        matrices_batch = matrices_batch.view(N * self.n_samples, 2, 3)

        grids_batch = F.affine_grid(matrices_batch, out_shape, align_corners=align_corners)

        if N > 1000:  # need to split up to avoid cuda contiguous error
            outs = list()
            for s in range(0, N, 1000):
                e, n =  min(s+1000, N), min(s+1000, N) - s
                out = F.grid_sample(x[s:e].unsqueeze(1).expand(n, self.n_samples, C, H, W).contiguous().view(n*self.n_samples, C, H, W), 
                                    grids_batch[s*self.n_samples:e*self.n_samples], align_corners=align_corners, mode=mode)
                outs.append(out)
            out = torch.cat(outs)
        else:
            out = F.grid_sample(x.unsqueeze(1).expand(N, self.n_samples, C, H, W).contiguous().view(N*self.n_samples, C, H, W), grids_batch, align_corners=align_corners, mode=mode)
        out = out.view(N, self.n_samples, C, H, W)
        if self.flip:
            out = self.flip(out)

        return out

File Path: ntkmarglik/marglik.py
Content:
import logging
from time import time
import numpy as np
import torch
from torch.nn.utils.convert_parameters import vector_to_parameters
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR
from torch.nn import CrossEntropyLoss, MSELoss
from torch.nn.utils import parameters_to_vector
import wandb

from laplace import KronLaplace, FunctionalLaplace
from laplace.curvature import AsdlGGN

from ntkmarglik.utils import (
    wandb_log_invariance, wandb_log_prior, wandb_log_parameter_norm
)

GB_FACTOR = 1024 ** 3


def expand_prior_precision(prior_prec, model):
    theta = parameters_to_vector(model.parameters())
    device, P = theta.device, len(theta)
    assert prior_prec.ndim == 1
    if len(prior_prec) == 1:  # scalar
        return torch.ones(P, device=device) * prior_prec
    elif len(prior_prec) == P:  # full diagonal
        return prior_prec.to(device)
    else:
        return torch.cat([delta * torch.ones_like(m).flatten() for delta, m
                          in zip(prior_prec, model.parameters())])


def get_prior_hyperparams(prior_prec_init, prior_structure, H, P, device):
    log_prior_prec_init = np.log(prior_prec_init)
    if prior_structure == 'scalar':
        log_prior_prec = log_prior_prec_init * torch.ones(1, device=device)
    elif prior_structure == 'layerwise':
        log_prior_prec = log_prior_prec_init * torch.ones(H, device=device)
    elif prior_structure == 'diagonal':
        log_prior_prec = log_prior_prec_init * torch.ones(P, device=device)
    else:
        raise ValueError(f'Invalid prior structure {prior_structure}')
    log_prior_prec.requires_grad = True
    return log_prior_prec


def valid_performance(model, test_loader, likelihood, criterion, method, device):
    N = len(test_loader.dataset)
    perf = 0
    nll = 0
    for X, y in test_loader:
        X, y = X.detach().to(device), y.detach().to(device)
        with torch.no_grad():
            if method == 'lila':
                f = model(X).mean(dim=1)
            else:
                f = model(X)
        if likelihood == 'classification':
            perf += (torch.argmax(f, dim=-1) == y).sum() / N
        else:
            perf += (f - y).square().sum() / N
        nll += criterion(f, y) / len(test_loader)
    return perf.item(), nll


def get_scheduler(scheduler, optimizer, train_loader, n_epochs, lr, lr_min):
    n_steps = n_epochs * len(train_loader)
    if scheduler == 'exp':
        min_lr_factor = lr_min / lr
        gamma = np.exp(np.log(min_lr_factor) / n_steps)
        return ExponentialLR(optimizer, gamma=gamma)
    elif scheduler == 'cos':
        return CosineAnnealingLR(optimizer, n_steps, eta_min=lr_min)
    else:
        raise ValueError(f'Invalid scheduler {scheduler}')


def get_model_optimizer(optimizer, model, lr, weight_decay=0):
    if optimizer == 'Adam':
        return Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer == 'SGD':
        # fixup parameters should have 10x smaller learning rate
        is_fixup = lambda param: param.size() == torch.Size([1])  # scalars
        fixup_params = [p for p in model.parameters() if is_fixup(p)]
        standard_params = [p for p in model.parameters() if not is_fixup(p)]
        params = [{'params': standard_params}, {'params': fixup_params, 'lr': lr / 10.}]
        return SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)
    else:
        raise ValueError(f'Invalid optimizer {optimizer}')


def gradient_to_vector(parameters):
    return parameters_to_vector([e.grad for e in parameters])


def vector_to_gradient(vec, parameters):
    return vector_to_parameters(vec, [e.grad for e in parameters])


def marglik_optimization(model,
                         train_loader,
                         marglik_loader=None,
                         valid_loader=None,
                         partial_loader=None,
                         likelihood='classification',
                         prior_structure='layerwise',
                         prior_prec_init=1.,
                         sigma_noise_init=1.,
                         temperature=1.,
                         n_epochs=500,
                         lr=1e-1,
                         lr_min=None,
                         optimizer='SGD',
                         scheduler='cos',
                         n_epochs_burnin=0,
                         n_hypersteps=100,
                         n_hypersteps_prior=1,
                         marglik_frequency=1,
                         lr_hyp=1e-1,
                         lr_hyp_min=1e-1,
                         lr_aug=1e-2,
                         lr_aug_min=1e-2,
                         laplace=KronLaplace,
                         backend=AsdlGGN,
                         independent=False,
                         single_output=False,
                         single_output_iid=False,
                         kron_jac=True,
                         method='baseline',
                         augmenter=None,
                         stochastic_grad=False,
                         use_wandb=False):
    """Runs marglik optimization training for a given model and training dataloader.

    Parameters
    ----------
    model : torch.nn.Module
        torch model
    train_loader : DataLoader
        pytorch training dataset loader
    marglik_loader : DataLoader
        pytorch data loader for fitting Laplace
    valid_loader : DataLoader
        pytorch data loader for validation
    partial_loader : DataLoader
        pytorch data loader for partial fitting for lila's grad accumulation
    likelihood : str
        'classification' or 'regression'
    prior_structure : str
        'scalar', 'layerwise', 'diagonal'
    prior_prec_init : float
        initial prior precision
    sigma_noise_init : float
        initial observation noise (for regression only)
    temperature : float
        factor for the likelihood for 'overcounting' data.
        Often required when using data augmentation.
    n_epochs : int
    lr : float
        learning rate for model optimizer
    lr_min : float
        minimum learning rate, defaults to lr and hence no decay
        to have the learning rate decay from 1e-3 to 1e-6, set
        lr=1e-3 and lr_min=1e-6.
    optimizer : str
        either 'Adam' or 'SGD'
    scheduler : str
        either 'exp' for exponential and 'cos' for cosine decay towards lr_min
    n_epochs_burnin : int default=0
        how many epochs to train without estimating and differentiating marglik
    n_hypersteps : int
        how many steps to take on diff. hyperparameters when marglik is estimated
    n_hypersteps_prior : int
        how many steps to take on the prior when marglik is estimated
    marglik_frequency : int
        how often to estimate (and differentiate) the marginal likelihood
    lr_hyp : float
        learning rate for hyperparameters (should be between 1e-3 and 1)
    lr_hyp_min : float
        minimum learning rate, decayed to using cosine schedule
    lr_aug : float
        learning rate for augmentation parameters
    lr_aug_min : float
        minimum learning rate, decayed to using cosine schedule
    laplace : Laplace
        type of Laplace approximation (Kron/Diag/Full)
    backend : Backend
        AsdlGGN/AsdlEF or BackPackGGN/BackPackEF
    independent : bool
        whether to use independent functional laplace
    single_output : bool
        whether to use single random output for functional laplace
    single_output_iid : bool
        whether to sample single output per sample iid (otherwise per batch)
    kron_jac : bool
        whether to use kron_jac in the backend
    method : augmentation strategy, one of ['baseline'] -> no change
        or ['lila'] -> change in protocol.
    augmenter : torch.nn.Module with differentiable parameter
    stochastic_grad : bool
        whether to use stochastic gradients of marginal likelihood
        usually would correspond to lower bound (unless small data)

    Returns
    -------
    lap : Laplace
        lapalce approximation
    model : torch.nn.Module
    margliks : list
    valid_perfs : list
    aug_history: list
        None for method == 'baseline'
    """
    if lr_min is None:  # don't decay lr
        lr_min = lr
    if marglik_loader is None:
        marglik_loader = train_loader
    if partial_loader is None:
        partial_loader = marglik_loader
    device = parameters_to_vector(model.parameters()).device
    N = len(train_loader.dataset)
    H = len(list(model.parameters()))
    P = len(parameters_to_vector(model.parameters()))
    optimize_aug = augmenter is not None and parameters_to_vector(augmenter.parameters()).requires_grad
    backend_kwargs = dict(differentiable=(stochastic_grad and optimize_aug) or laplace is FunctionalLaplace,
                          kron_jac=kron_jac)
    la_kwargs = dict(sod=stochastic_grad, single_output=single_output, single_output_iid=single_output_iid)
    if laplace is FunctionalLaplace:
        la_kwargs['independent'] = independent
    if use_wandb:
        wandb.config.update(dict(n_params=P, n_param_groups=H, n_data=N))

    # differentiable hyperparameters
    hyperparameters = list()
    # prior precision
    log_prior_prec = get_prior_hyperparams(prior_prec_init, prior_structure, H, P, device)
    hyperparameters.append(log_prior_prec)

    # set up loss (and observation noise hyperparam)
    if likelihood == 'classification':
        criterion = CrossEntropyLoss(reduction='mean')
        sigma_noise = 1
    elif likelihood == 'regression':
        criterion = MSELoss(reduction='mean')
        log_sigma_noise_init = np.log(sigma_noise_init)
        log_sigma_noise = log_sigma_noise_init * torch.ones(1, device=device)
        log_sigma_noise.requires_grad = True
        hyperparameters.append(log_sigma_noise)

    # set up model optimizer and scheduler
    optimizer = get_model_optimizer(optimizer, model, lr)
    scheduler = get_scheduler(scheduler, optimizer, train_loader, n_epochs, lr, lr_min)

    n_steps = ((n_epochs - n_epochs_burnin) // marglik_frequency) * n_hypersteps_prior
    hyper_optimizer = Adam(hyperparameters, lr=lr_hyp)
    hyper_scheduler = CosineAnnealingLR(hyper_optimizer, n_steps, eta_min=lr_hyp_min)
    if optimize_aug:
        logging.info('MARGLIK: optimize augmentation.')
        aug_optimizer = Adam(augmenter.parameters(), lr=lr_aug)
        n_steps = ((n_epochs - n_epochs_burnin) // marglik_frequency) * (n_hypersteps if stochastic_grad else 1)
        aug_scheduler = CosineAnnealingLR(aug_optimizer, n_steps, eta_min=lr_aug_min)
        aug_history = [parameters_to_vector(augmenter.parameters()).squeeze().detach().cpu().numpy()]

    losses = list()
    valid_perfs = list()
    margliks = list()

    for epoch in range(1, n_epochs + 1):
        epoch_time_fwd = 0.0
        epoch_time_fit = 0.0

        epoch_loss = 0
        epoch_perf = 0
        epoch_nll = 0
        epoch_log = dict(epoch=epoch)

        # standard NN training per batch
        torch.cuda.empty_cache()
        for X, y in train_loader:
            X, y = X.detach().to(device), y.to(device)
            optimizer.zero_grad()

            if likelihood == 'regression':
                sigma_noise = torch.exp(log_sigma_noise).detach()
                crit_factor = 1 / temperature / (2 * sigma_noise.square())
            else:
                crit_factor = 1 / temperature
            prior_prec = torch.exp(log_prior_prec).detach()
            delta = expand_prior_precision(prior_prec, model)

            # fit data
            time_fwd = time()
            if method == 'lila':
                f = model(X).mean(dim=1)
            else:
                f = model(X)
            epoch_time_fwd += time() - time_fwd # log total time fwd fit in epoch

            time_fit = time()
            theta = parameters_to_vector(model.parameters())
            loss = criterion(f, y) + (0.5 * (delta * theta) @ theta) / N / crit_factor
            loss.backward()
            optimizer.step()
            epoch_time_fit += time() - time_fit # log total time bwd fit in epoch

            epoch_loss += loss.cpu().item() / len(train_loader)
            epoch_nll += criterion(f.detach(), y).item() / len(train_loader)
            if likelihood == 'regression':
                epoch_perf += (f.detach() - y).square().sum() / N
            else:
                epoch_perf += torch.sum(torch.argmax(f.detach(), dim=-1) == y).item() / N
            scheduler.step()

        losses.append(epoch_loss)
        logging.info('MAP memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')
        logging.info(f'MARGLIK[epoch={epoch}]: train. perf={epoch_perf*100:.2f}%; loss={epoch_loss:.5f}; nll={epoch_nll:.5f}')
        optimizer.zero_grad(set_to_none=True)
        llr = scheduler.get_last_lr()[0]
        epoch_log.update({'train/loss': epoch_loss, 'train/nll': epoch_nll, 'train/perf': epoch_perf, 'train/lr': llr,
                          'train/time_fwd': epoch_time_fwd, 'train/time_fit': epoch_time_fit})
        if use_wandb and ((epoch % 5) == 0):
            wandb_log_parameter_norm(model)

        # compute validation error to report during training
        if valid_loader is not None:
            with torch.no_grad():
                val_perf, val_nll = valid_performance(model, valid_loader, likelihood, criterion, method, device)
                valid_perfs.append(val_perf)
                logging.info(f'MARGLIK[epoch={epoch}]: valid. perf={val_perf*100:.2f}%; nll={val_nll:.5f}.')
                epoch_log.update({'valid/perf': val_perf, 'valid/nll': val_nll})

        # only update hyperparameters every "Frequency" steps after "burnin"
        if (epoch % marglik_frequency) != 0 or epoch < n_epochs_burnin:
            if use_wandb:
                wandb.log(epoch_log, step=epoch, commit=((epoch % 10) == 0))
            continue

        # optimizer hyperparameters by differentiating marglik
        time_hyper = time()

        # 1. fit laplace approximation
        torch.cuda.empty_cache()
        if optimize_aug:
            if stochastic_grad:  # differentiable
                marglik_loader.attach()
            else:  # jvp
                marglik_loader.detach()

        # first optimize prior precision jointly with direct marglik grad
        margliks_local = list()
        n_hyper = max(n_hypersteps_prior, n_hypersteps) if stochastic_grad else n_hypersteps_prior
        for i in range(n_hyper):
            if i == 0 or stochastic_grad:
                sigma_noise = 1 if likelihood == 'classification' else torch.exp(log_sigma_noise)
                prior_prec = torch.exp(log_prior_prec)
                lap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,
                              temperature=temperature, backend=backend, backend_kwargs=backend_kwargs,
                              **la_kwargs)
                lap.fit(marglik_loader)
            if i < n_hypersteps and optimize_aug and stochastic_grad:
                aug_optimizer.zero_grad()
            if i < n_hypersteps_prior:
                hyper_optimizer.zero_grad()
            if i < n_hypersteps_prior and not stochastic_grad:  # does not fit every it
                sigma_noise = None if likelihood == 'classification' else torch.exp(log_sigma_noise)
                prior_prec = torch.exp(log_prior_prec)
                marglik = -lap.log_marginal_likelihood(prior_prec, sigma_noise) / N
            else:  # fit with updated hparams
                marglik = -lap.log_marginal_likelihood() / N
            marglik.backward()
            margliks_local.append(marglik.item())
            if i < n_hypersteps_prior:
                hyper_optimizer.step()
                hyper_scheduler.step()
            if i < n_hypersteps and optimize_aug and stochastic_grad:
                aug_optimizer.step()
                aug_scheduler.step()

        if stochastic_grad:
            marglik = np.mean(margliks_local)
        else:
            marglik = margliks_local[-1]

        if use_wandb:
            wandb_log_prior(torch.exp(log_prior_prec.detach()), prior_structure, model)
        if likelihood == 'regression':
            epoch_log['hyperparams/sigma_noise'] = torch.exp(log_sigma_noise.detach()).cpu().item()
        epoch_log['train/marglik'] = marglik
        logging.info('LA memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')

        # option 2: jvp (not direct_grad)
        torch.cuda.empty_cache()
        if optimize_aug and not stochastic_grad:  # accumulate gradient with JVP
            partial_loader.attach()
            aug_grad = torch.zeros_like(parameters_to_vector(augmenter.parameters()))
            lap.backend.differentiable = True
            if isinstance(lap, KronLaplace):
                # does the inversion internally
                hess_inv = lap.posterior_precision.jvp_logdet()
            else:
                hess_inv = lap.posterior_covariance.flatten()
            for i, (X, y) in zip(range(n_hypersteps), partial_loader):
                lap.loss, H_batch = lap._curv_closure(X, y, N)
                # curv closure creates gradient already, need to zero
                aug_optimizer.zero_grad()
                # compute grad wrt. neg. log-lik
                (- lap.log_likelihood).backward(inputs=list(augmenter.parameters()), retain_graph=True)
                # compute grad wrt. log det = 0.5 vec(P_inv) @ (grad-vec H)
                (0.5 * H_batch.flatten()).backward(gradient=hess_inv, inputs=list(augmenter.parameters()))
                aug_grad = (aug_grad + gradient_to_vector(augmenter.parameters()).data.clone())

            lap.backend.differentiable = False

            vector_to_gradient(aug_grad, augmenter.parameters())
            aug_optimizer.step()
            aug_scheduler.step()

        epoch_time_hyper = time() - time_hyper
        epoch_log.update({'train/time_hyper': epoch_time_hyper})

        if optimize_aug:
            aug_history.append(parameters_to_vector(augmenter.parameters()).squeeze().detach().cpu().numpy())
            logging.info(f'Augmentation params epoch {epoch}: {aug_history[-1]}')
            if use_wandb:
                wandb_log_invariance(augmenter)

        logging.info('LA memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')

        margliks.append(marglik)
        del lap
        if use_wandb:
            if optimize_aug:
                epoch_log['train/lr_aug'] = aug_scheduler.get_last_lr()[0]
            epoch_log['train/lr_hyp'] = hyper_scheduler.get_last_lr()[0]
            wandb.log(epoch_log, step=epoch, commit=((epoch % 10) == 0))

        # early stopping on marginal likelihood
        logging.info(f'MARGLIK[epoch={epoch}]: marglik optimization. MargLik={margliks[-1]:.5f}, prec: {prior_prec.detach().mean().item():.2f}.')

    sigma_noise = 1 if sigma_noise is None else sigma_noise
    lap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,
                  temperature=temperature, backend=backend, backend_kwargs=backend_kwargs,
                  **la_kwargs)
    lap.fit(marglik_loader.detach())
    if optimize_aug:
        return lap, model, margliks, valid_perfs, aug_history
    return lap, model, margliks, valid_perfs, None

File Path: ntkmarglik/models.py
Content:
import torch
import numpy as np
from torch import nn
from torch import nn

from asdfghjkl.operations import Bias, Scale
from asdfghjkl.operations.conv_aug import Conv2dAug


def get_activation(act_str):
    if act_str == 'relu':
        return nn.ReLU
    elif act_str == 'tanh':
        return nn.Tanh
    elif act_str == 'selu':
        return nn.SELU
    elif act_str == 'silu':
        return nn.SiLU
    else:
        raise ValueError('invalid activation')


class MaxPool2dAug(nn.MaxPool2d):

    def forward(self, input):
        k_aug = input.shape[1]
        input = super().forward(input.flatten(start_dim=0, end_dim=1))
        return input.reshape(-1, k_aug, *input.shape[1:])


class AvgPool2dAug(nn.AvgPool2d):

    def forward(self, input):
        k_aug = input.shape[1]
        input = super().forward(input.flatten(start_dim=0, end_dim=1))
        return input.reshape(-1, k_aug, *input.shape[1:])


class AdaptiveAvgPool2dAug(nn.AdaptiveAvgPool2d):

    def forward(self, input):
        k_aug = input.shape[1]
        input = super().forward(input.flatten(start_dim=0, end_dim=1))
        return input.reshape(-1, k_aug, *input.shape[1:])


class MLP(nn.Sequential):
    def __init__(self, input_size, width, depth, output_size, activation='relu',
                 bias=True, fixup=False, augmented=False):
        super(MLP, self).__init__()
        self.input_size = input_size
        self.width = width
        self.depth = depth
        hidden_sizes = depth * [width]
        self.activation = activation
        flatten_start_dim = 2 if augmented else 1
        act = get_activation(activation)
        output_size = output_size

        self.add_module('flatten', nn.Flatten(start_dim=flatten_start_dim))

        if len(hidden_sizes) == 0:  # i.e. when depth == 0.
            # Linear Model
            self.add_module('lin_layer', nn.Linear(self.input_size, output_size, bias=bias))
        else:
            # MLP
            in_outs = zip([self.input_size] + hidden_sizes[:-1], hidden_sizes)
            for i, (in_size, out_size) in enumerate(in_outs):
                self.add_module(f'layer{i+1}', nn.Linear(in_size, out_size, bias=bias))
                if fixup:
                    self.add_module(f'bias{i+1}b', Bias())
                    self.add_module(f'scale{i+1}b', Scale())
                self.add_module(f'{activation}{i+1}', act())
            self.add_module('out_layer', nn.Linear(hidden_sizes[-1], output_size, bias=bias))


class LeNet(nn.Sequential):

    def __init__(self, in_channels=1, n_out=10, activation='relu', n_pixels=28,
                 augmented=False):
        super().__init__()
        mid_kernel_size = 3 if n_pixels == 28 else 5
        act = get_activation(activation)
        conv = Conv2dAug if augmented else nn.Conv2d
        pool = MaxPool2dAug if augmented else nn.MaxPool2d
        flatten = nn.Flatten(start_dim=2) if augmented else nn.Flatten(start_dim=1)
        self.add_module('conv1', conv(in_channels, 6, 5, 1))
        self.add_module('act1', act())
        self.add_module('pool1', pool(2))
        self.add_module('conv2', conv(6, 16, mid_kernel_size, 1))
        self.add_module('act2', act())
        self.add_module('pool2', pool(2))
        self.add_module('conv3', conv(16, 120, 5, 1))
        self.add_module('flatten', flatten)
        self.add_module('act3', act())
        self.add_module('lin1', torch.nn.Linear(120*1*1, 84))
        self.add_module('act4', act())
        self.add_module('linout', torch.nn.Linear(84, n_out))


class MiniNet(nn.Sequential):

    def __init__(self, in_channels=1, n_out=10, augmented=False):
        super().__init__()
        conv = Conv2dAug if augmented else nn.Conv2d
        pool = MaxPool2dAug if augmented else nn.MaxPool2d
        flatten = nn.Flatten(start_dim=2) if augmented else nn.Flatten(start_dim=1)
        self.add_module('conv1', conv(in_channels, 8, 5, 1))
        self.add_module('act1', nn.ReLU())
        self.add_module('pool1', pool(2))
        self.add_module('conv2', conv(8, 16, 3, 1))
        self.add_module('act2', nn.ReLU())
        self.add_module('pool2', pool(2))
        self.add_module('conv3', conv(16, 32, 5, 1))
        self.add_module('flatten', flatten)
        self.add_module('act3', nn.ReLU())
        self.add_module('linout', nn.Linear(32, n_out))


def conv3x3(in_planes, out_planes, stride=1, augmented=False):
    """3x3 convolution with padding"""
    Conv2d = Conv2dAug if augmented else nn.Conv2d
    return Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                  padding=1, bias=False)


class FixupBasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, augmented=False):
        super(FixupBasicBlock, self).__init__()
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.augmented = augmented
        self.bias1a = Bias()
        self.conv1 = conv3x3(inplanes, planes, stride, augmented=augmented)
        self.bias1b = Bias()
        self.relu = nn.ReLU(inplace=True)
        self.bias2a = Bias()
        self.conv2 = conv3x3(planes, planes, augmented=augmented)
        self.scale = Scale()
        self.bias2b = Bias()
        self.downsample = downsample

    def forward(self, x):
        identity = x

        biased_x = self.bias1a(x)
        out = self.conv1(biased_x)
        out = self.relu(self.bias1b(out))

        out = self.conv2(self.bias2a(out))
        out = self.bias2b(self.scale(out))

        if self.downsample is not None:
            identity = self.downsample(biased_x)
            cat_dim = 2 if self.augmented else 1
            identity = torch.cat((identity, torch.zeros_like(identity)), cat_dim)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):
    """
    FixupResnet-depth where depth is a `4 * 2 * n + 2` with `n` blocks per residual layer.
    The two added layers are the input convolution and fully connected output.
    """

    def __init__(self, depth, num_classes=10, in_planes=16, in_channels=3, augmented=False):
        super(ResNet, self).__init__()
        n_out = num_classes
        assert (depth - 2) % 8 == 0, 'Invalid ResNet depth, has to conform to 8 * n + 2'
        layer_size = (depth - 2) // 8
        layers = 4 * [layer_size]
        self.num_layers = 4 * layer_size
        self.inplanes = in_planes
        self.augmented = augmented
        AdaptiveAvgPool2d = AdaptiveAvgPool2dAug if augmented else nn.AdaptiveAvgPool2d
        self.conv1 = conv3x3(in_channels, in_planes, augmented=augmented)
        self.bias1 = Bias()
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(FixupBasicBlock, in_planes, layers[0])
        self.layer2 = self._make_layer(FixupBasicBlock, in_planes * 2, layers[1], stride=2)
        self.layer3 = self._make_layer(FixupBasicBlock, in_planes * 4, layers[2], stride=2)
        self.layer4 = self._make_layer(FixupBasicBlock, in_planes * 8, layers[3], stride=2)
        self.avgpool = AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten(start_dim=2 if augmented else 1)
        self.bias2 = Bias()
        self.fc = nn.Linear(in_planes * 8, n_out)

        for m in self.modules():
            if isinstance(m, FixupBasicBlock):
                nn.init.normal_(m.conv1.weight,
                                mean=0,
                                std=np.sqrt(2 / (m.conv1.weight.shape[0] * np.prod(m.conv1.weight.shape[2:]))) * self.num_layers ** (-0.5))
                nn.init.constant_(m.conv2.weight, 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.weight, 0)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        AvgPool2d = AvgPool2dAug if self.augmented else nn.AvgPool2d
        if stride != 1:
            downsample = AvgPool2d(1, stride=stride)

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, augmented=self.augmented))
        self.inplanes = planes
        for _ in range(1, blocks):
            layers.append(block(planes, planes, augmented=self.augmented))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(self.bias1(x))

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = self.flatten(x)
        x = self.fc(self.bias2(x))

        return x


class WRNFixupBasicBlock(nn.Module):
    def __init__(self, in_planes, out_planes, stride, dropRate=0.0, augmented=False, fixup=True):
        super(WRNFixupBasicBlock, self).__init__()
        self.bias1 = Bias() if fixup else nn.Identity()
        self.relu1 = nn.ReLU(inplace=True)
        basemodul = Conv2dAug if augmented else nn.Conv2d
        self.augmented = augmented
        self.conv1 = basemodul(in_planes, out_planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bias2 = Bias() if fixup else nn.Identity()
        self.relu2 = nn.ReLU(inplace=True)
        self.bias3 = Bias() if fixup else nn.Identity()
        self.conv2 = basemodul(out_planes, out_planes, kernel_size=3, stride=1,
                               padding=1, bias=False)
        self.bias4 = Bias() if fixup else nn.Identity()
        self.scale1 = Scale() if fixup else nn.Identity()
        self.droprate = dropRate
        self.equalInOut = (in_planes == out_planes)
        self.convShortcut = (not self.equalInOut) and basemodul(in_planes, out_planes, kernel_size=1, stride=stride,
                                                                padding=0, bias=False) or None

    def forward(self, x):
        if not self.equalInOut:
            x = self.relu1(self.bias1(x))
        else:
            out = self.relu1(self.bias1(x))
        if self.equalInOut:
            out = self.bias3(self.relu2(self.bias2(self.conv1(out))))
        else:
            out = self.bias3(self.relu2(self.bias2(self.conv1(x))))
        if self.droprate > 0:
            out = F.dropout(out, p=self.droprate, training=self.training)
        out = self.bias4(self.scale1(self.conv2(out)))
        if not self.equalInOut:
            return torch.add(self.convShortcut(x), out)
        else:
            return torch.add(x, out)


class WRNFixupNetworkBlock(nn.Module):
    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0, augmented=False, fixup=True):
        super(WRNFixupNetworkBlock, self).__init__()
        self.augmented = augmented
        self.fixup = fixup
        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)

    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):
        layers = []
        for i in range(nb_layers):
            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate, self.augmented, self.fixup))
        return nn.Sequential(*layers)

    def forward(self, x):
        return self.layer(x)


class WideResNet(nn.Module):
    def __init__(self, depth=16, widen_factor=4, num_classes=10, dropRate=0.0, augmented=False, fixup=True):
        super(WideResNet, self).__init__()
        n_out = num_classes
        self.fixup = fixup
        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]
        assert ((depth - 4) % 6 == 0)
        n = (depth - 4) // 6
        block = WRNFixupBasicBlock
        # 1st conv before any network block
        self.num_layers = n * 3
        basemodul = Conv2dAug if augmented else nn.Conv2d
        self.augmented = augmented
        self.conv1 = basemodul(3, nChannels[0], kernel_size=3, stride=1,
                               padding=1, bias=False)
        self.bias1 = Bias() if fixup else nn.Identity()
        # 1st block
        self.block1 = WRNFixupNetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate, augmented=augmented, fixup=fixup)
        # 2nd block
        self.block2 = WRNFixupNetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate, augmented=augmented, fixup=fixup)
        # 3rd block
        self.block3 = WRNFixupNetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate, augmented=augmented, fixup=fixup)
        # global average pooling and classifier
        self.bias2 = Bias() if fixup else nn.Identity()
        self.relu = nn.ReLU()
        self.pool = AvgPool2dAug(8) if augmented else nn.AvgPool2d(8)
        self.fc = nn.Linear(nChannels[3], n_out)
        self.nChannels = nChannels[3]

        for m in self.modules():
            if isinstance(m, WRNFixupBasicBlock):
                conv = m.conv1
                k = conv.weight.shape[0] * np.prod(conv.weight.shape[2:])
                nn.init.normal_(conv.weight,
                                mean=0,
                                std=np.sqrt(2. / k) * self.num_layers ** (-0.5))
                nn.init.constant_(m.conv2.weight, 0)
                if m.convShortcut is not None:
                    cs = m.convShortcut
                    k = cs.weight.shape[0] * np.prod(cs.weight.shape[2:])
                    nn.init.normal_(cs.weight,
                                    mean=0,
                                    std=np.sqrt(2. / k))
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.weight, 0)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        out = self.bias1(self.conv1(x))
        out = self.block1(out)
        out = self.block2(out)
        out = self.block3(out)
        out = self.relu(out)
        out = self.pool(out)
        if self.augmented:
            out = out.flatten(start_dim=2)
        else:
            out = out.flatten(start_dim=1)
        out = self.fc(self.bias2(out))
        return out

File Path: ntkmarglik/utils.py
Content:
import logging
from time import time
import numpy as np
import torch
import wandb
from torch.nn.utils.convert_parameters import parameters_to_vector

from laplace import FullLaplace, KronLaplace, DiagLaplace, FunctionalLaplace, BlockDiagLaplace


def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        try:
            from torch import cudnn
            cudnn.deterministic = True
            cudnn.benchmark = False
        except:
            pass


def get_laplace_approximation(structure):
    if structure == 'full':
        return FullLaplace
    elif structure == 'kron':
        return KronLaplace
    elif structure == 'diag':
        return DiagLaplace
    elif structure == 'blockdiag':
        return BlockDiagLaplace
    elif structure == 'kernel' or structure == 'kernel-stochastic':
        return FunctionalLaplace


def wandb_log_parameter_hist(model):
    for name, param in model.named_parameters():
        hist, edges = param.data.cpu().histogram(bins=64)
        wandb.log({f'params/{name}': wandb.Histogram(
            np_histogram=(hist.numpy().tolist(), edges.numpy().tolist())
        )}, commit=False)


def wandb_log_parameter_norm(model):
    for name, param in model.named_parameters():
        avg_norm = (param.data.flatten() ** 2).sum().item() / np.prod(param.data.shape)
        wandb.log({f'params/{name}': avg_norm}, commit=False)


def wandb_log_invariance(augmenter):
    aug_params = np.abs(
        parameters_to_vector(augmenter.parameters()).detach().cpu().numpy()
    ).tolist()
    if len(aug_params) == 6:
        names = ['Tx', 'Ty', 'R', 'Sx', 'Sy', 'H']
    else:
        names = [f'aug_{i}' for i in range(6)]
    log = {f'invariances/{n}': p for n, p in zip(names, aug_params)}
    wandb.log(log, commit=False)


def wandb_log_prior(prior_prec, prior_structure, model):
    prior_prec = prior_prec.detach().cpu().numpy().tolist()
    if prior_structure == 'scalar':
        wandb.log({'hyperparams/prior_prec': prior_prec[0]}, commit=False)
    elif prior_structure == 'layerwise':
        log = {f'hyperparams/prior_prec_{n}': p for p, (n, _) in
               zip(prior_prec, model.named_parameters())}
        wandb.log(log, commit=False)
    elif prior_structure == 'diagonal':
        hist, edges = prior_prec.data.cpu().histogram(bins=64)
        log = {f'hyperparams/prior_prec': wandb.Histogram(
            np_histogram=(hist.numpy().tolist(), edges.numpy().tolist())
        )}
        wandb.log(log, commit=False)


class Timer:
    def __init__(self, name, logger=False) -> None:
        self.logger = logger
        self.name = name
    def __enter__(self):
        self.start_time = time()
    def __exit__(self, *args, **kwargs):
        msg = f'{self.name} took {time() - self.start_time:.3f}s'
        print(msg)
        if self.logger:
            logging.info(msg)

File Path: scripts/generate_bound_commands.py
Content:
# Illustration of bounds
base_cmds = ['classification_image.py --config configs/bound_illustration.yaml']
seeds = [1, 2, 3]
for base_cmd in base_cmds:
    for seed in seeds:
        cmd = base_cmd + f' --seed {seed}'
        # baseline MAP
        print(cmd, '--n_epochs_burnin 1000 --approx kron --method baseline')
        for batch_size in [10, 20, 50, 100, 250, 500, 1000]:
            cmd = base_cmd + f' --seed {seed} --marglik_batch_size {batch_size} --bound'
            for approx in ['full', 'blockdiag', 'kron', 'diag', 'kernel']:
                print(cmd, '--approx', approx)
                print(cmd, '--approx', approx, '--grouped_loader')
                print(cmd, '--approx', approx, '--independent_outputs', '--single_output')
                print(cmd, '--approx', approx, '--independent_outputs', '--single_output', '--grouped_loader')

# Illustration of bounds with lila
base_cmds = ['classification_image.py --config configs/bound_illustration_lila.yaml']
seeds = [1, 2, 3]
for base_cmd in base_cmds:
    for seed in seeds:
        cmd = base_cmd + f' --seed {seed}'
        # baseline MAP
        print(cmd, '--n_epochs_burnin 1000 --approx kron --method baseline')
        for batch_size in [10, 20, 50, 100, 250, 500, 1000]:
            cmd = base_cmd + f' --seed {seed} --marglik_batch_size {batch_size} --bound'
            for approx in ['full', 'blockdiag', 'kron', 'diag', 'kernel']:
                print(cmd, '--approx', approx)
                print(cmd, '--approx', approx, '--grouped_loader')
                print(cmd, '--approx', approx, '--independent_outputs', '--single_output')
                print(cmd, '--approx', approx, '--independent_outputs', '--single_output', '--grouped_loader')

# TIMING commands
base_cmd = 'classification_image.py --config configs/bound_illustration_timing.yaml'
for batch_size in [10, 20, 50, 100, 250, 500, 1000]:
    cmd = base_cmd + f' --marglik_batch_size {batch_size} --bound'
    for approx in ['full', 'kron', 'diag', 'kernel']:
        print(cmd, '--approx', approx)
        print(cmd, '--approx', approx, '--independent_outputs', '--single_output')

# TIMING commands lila
base_cmd = 'classification_image.py --config configs/bound_illustration_lila_timing.yaml'
for batch_size in [10, 20, 50, 100, 250, 500, 1000]:
    cmd = base_cmd + f' --marglik_batch_size {batch_size} --bound'
    for approx in ['full', 'kron', 'diag', 'kernel']:
        print(cmd, '--approx', approx)
        print(cmd, '--approx', approx, '--independent_outputs', '--single_output')

File Path: scripts/generate_cifar_commands.py
Content:
# CIFAR10 and CIFAR100
base_cmd = 'classification_image.py'
configs = ['--config configs/cifar.yaml --dataset cifar10', '--config configs/cifar.yaml --dataset cifar100']
seeds = [1, 2, 3, 4, 5]
model = 'wrn'
for config in configs:
    for seed in seeds:
        cmd_parts = [base_cmd, config, f'--model {model} --seed {seed} --batch_size 128 --lr_min 1e-8 --n_epochs 300']
        cmd = ' '.join(cmd_parts)
        # MAP
        print(cmd, '--n_epochs_burnin 1000 --approx kron')
        # Marglik full batch updates
        print(cmd, '--approx kron')
        print(cmd, '--approx kron --single_output --independent_outputs')
        print(cmd, '--approx kron --single_output --independent_outputs --single_output_iid')
        for approx in ['kron', 'kernel']:
            print(cmd, f'--approx {approx} --bound', '--independent_outputs --single_output')
            print(cmd, f'--approx {approx} --bound', '--independent_outputs --single_output --single_output_iid')
            print(cmd, f'--approx {approx} --bound', '--independent_outputs --single_output --grouped_loader')
            if approx == 'kron':
                print(cmd, f'--approx {approx} --bound')
                print(cmd, f'--approx {approx} --bound', '--grouped_loader')

File Path: scripts/generate_tiny_commands.py
Content:
# TinyImageNet
base_cmd = 'classification_image.py --config configs/tiny.yaml'
seeds = [1, 2, 3]
for seed in seeds:
    cmd_parts = [base_cmd, f'--seed {seed}']
    cmd = ' '.join(cmd_parts)
    # MAP
    print(cmd, '--n_epochs_burnin 1000 --approx kron')
    # Marglik full batch updates
    print(cmd, '--approx kron')
    # full batch single output
    print(cmd, '--approx kron --single_output --independent_outputs')
    for approx in ['kron', 'kernel']:
        print(cmd, f'--approx {approx} --bound lower', '--independent_outputs --single_output')
        if approx == 'kron':
            print(cmd, f'--approx {approx} --bound lower')

base_cmd = 'classification_image.py --config configs/tiny_lila.yaml'
for seed in seeds:
    cmd_parts = [base_cmd, f'--seed {seed} --random_flip']
    cmd = ' '.join(cmd_parts)
    print(cmd, '--approx kernel')
    print(cmd, '--approx kron')
    # tiny_lila.yaml uses single output settings already
    print(cmd, '--approx kron --bound None --marglik_batch_size 60 --partial_batch_size 60 --n_hypersteps 100 --lr_aug 0.05 --lr_aug_min 0.005')

Output:
{
    "experimental_code": "# File Path: bound_grid.py\ndef get_marglik_loader(x, y, batch_size, augmenter, grouped_loader):\n    data_factor = len(x) / batch_size\n    DataLoaderCls = GroupedSubsetTensorDataLoader if grouped_loader else SubsetTensorDataLoader\n    marglik_loader = DataLoaderCls(x, y, transform=augmenter, subset_size=batch_size,\n                                   detach=False, data_factor=data_factor)\n    return marglik_loader\n\ndef main(setting, approximation, single_output, grouped_loader, stochastic):\n    # ... initial setup ...\n    X_train, y_train = dataset_to_tensors(train_dataset, subset_indices, device)\n    # ... augmenter setup ...\n\n    # ... initial marglik_optimization (MAP training, not the focus for stochastic bounds here) ...\n\n    ####### Assess bound at converged setting\n    backend_kwargs = dict(differentiable=False, kron_jac=False)\n    la_kwargs = dict(sod=True, single_output=single_output)\n    if approximation == 'kernel' and single_output:\n        la_kwargs['independent'] = True\n\n    if stochastic:\n        batch_sizes = [10, 20, 50, 100, 250, 500, 1000]\n    else:\n        # for parametric no sod bounds\n        batch_sizes = [1000]\n\n    # grid for prior precision or rotation factor\n    grid = np.logspace(-4, 4, 100) if setting == PRIOR else np.linspace(0, np.pi, 100)\n    result_frame = pd.DataFrame(index=batch_sizes, columns=grid)\n    result_frame_sem = pd.DataFrame(index=batch_sizes, columns=grid)\n    for batch_size in batch_sizes:\n        for hparam in grid:\n            set_seed(711)\n            # Create data subset loader for the current batch_size\n            marglik_loader = get_marglik_loader(X_train, y_train, batch_size, augmenter, grouped_loader)\n            marglik_loader = marglik_loader.detach() # Detach from training graph\n\n            # Set hyperparameter (prior precision or rotation factor) for current grid point\n            if setting == INVARIANCE:\n                augmenter.rot_factor.requires_grad = False\n                augmenter.rot_factor.data[2] = float(hparam) # Update rotation factor\n                prior_precision = la.prior_precision # Use converged prior precision\n            else: # setting == PRIOR\n                prior_precision = float(hparam) # Use grid search prior precision\n\n            margliks = list()\n            n_reps = int(subset_size / batch_size) # Number of data subsets to average over\n            for rep in range(n_reps):\n                # Select Laplace approximation class based on 'approximation'\n                if approximation == 'kernel':\n                    lap_cls = FunctionalLaplace\n                elif approximation == 'full':\n                    lap_cls = FullLaplace\n                elif approximation == 'blockdiag':\n                    lap_cls = BlockDiagLaplace\n                elif approximation == 'kron':\n                    lap_cls = KronLaplace\n                elif approximation == 'diag':\n                    lap_cls = DiagLaplace\n                \n                # Initialize Laplace approximation and fit to data subset\n                lap = lap_cls(model, 'classification', prior_precision=prior_precision,\n                              backend=backend, backend_kwargs=backend_kwargs, **la_kwargs)\n                lap.fit(marglik_loader) # Fit to a data subset\n                marglik = lap.log_marginal_likelihood().item() / subset_size # Compute log marginal likelihood\n                margliks.append(marglik)\n            result_frame.loc[batch_size, hparam] = np.mean(margliks)\n            result_frame_sem.loc[batch_size, hparam] = sem(margliks)\n            print(setting, batch_size, hparam, np.mean(margliks), np.nan_to_num(sem(margliks)))\n    # ... saving results ...\n\n# File Path: laplace/baselaplace.py\nclass FunctionalLaplace(BaseLaplace):\n    # ... __init__ ...\n    def _kernel_closure(self, X, y):\n        # Calls the backend's kernel computation based on configuration\n        if self.independent:\n            if self.single_output:\n                if self.single_output_iid:\n                    random_ix = torch.randint(self.n_outputs, (len(y),), device=X.device)\n                else:\n                    random_ix = torch.randint(self.n_outputs, ())\n                return self.backend.single_kernel(X, y, self.prior_precision, output_ix=random_ix)\n            else:\n                return self.backend.indep_kernel(X, y, self.prior_precision)\n        else:\n            return self.backend.kernel(X, y, self.prior_precision_diag, prec=self.prior_precision,\n                                       prec_structure=self.prior_structure)\n\n    def fit_batch(self, X, y, N):\n        # ... setup model/output_size ...\n        self.model.zero_grad()\n        X, y = X.to(self._device), y.to(self._device)\n        loss, H = self._kernel_closure(X, y) # H here is the computed kernel matrix\n        self.loss += loss\n        self.n_data_seen += len(y)\n        self.H.append(H) # Accumulate kernel matrices\n\n    @property\n    def log_det_ratio(self):\n        log_det_ratio = 0\n        for H_kernel in self.H:\n            if self.independent:\n                if self.single_output:\n                    # H_kernel n x n (for single output)\n                    log_det_ratio += self.n_outputs * torch.logdet(\n                        diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                    )\n                else:\n                    # H_kernel c x n x n (for independent outputs)\n                    log_det_ratio += torch.logdet(\n                        batch_diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                    ).sum()\n            else:\n                # H_kernel nc x nc (for full kernel)\n                log_det_ratio += torch.logdet(\n                    diagonal_add_scalar(self._H_factor * H_kernel, 1.0)\n                )\n        return self.n_data / self.n_data_seen * log_det_ratio # Scale by data factor\n\n# File Path: laplace/curvature/asdl.py\nclass AsdlGGN(AsdlInterface, GGNInterface):\n    # ... __init__ ...\n    def kernel(self, x, y, prec_diag, **kwargs):\n        # Calls linear_network_kernel to compute the kernel matrix\n        f, K = linear_network_kernel(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,\n                                     differentiable=self.differentiable, kron_jac=self.kron_jac)\n        n, c = f.shape\n        K = K.transpose(1, 2).reshape(n*c, n*c) # Reshape to (N*C) x (N*C) for full kernel\n\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n    def indep_kernel(self, x, y, prec_diag):\n        # Calls linear_network_kernel_indep for independent outputs\n        f, K = linear_network_kernel_indep(self._model, x, scale=1 / prec_diag, likelihood=self.likelihood,\n                                           differentiable=self.differentiable, kron_jac=self.kron_jac)\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n    def single_kernel(self, x, y, prec_diag, output_ix):\n        # Calls linear_network_kernel_indep for a single output\n        f, K = linear_network_kernel_indep(\n            self._model, x, scale=1 / prec_diag, likelihood=self.likelihood, differentiable=self.differentiable, \n            kron_jac=self.kron_jac, single_output=output_ix\n        )\n        loss = self.factor * self.lossfunc(f, y)\n        if self.differentiable:\n            return loss, K\n        return loss.detach(), K.detach()\n\n# File Path: dependencies/asdl/asdfghjkl/kernel.py\ndef linear_network_kernel(model, x, scale, likelihood='classification', \n                          differentiable=False, kron_jac=False):\n    # Computes J(x)^T L J(x) where L is the Hessian of the likelihood w.r.t. logits\n    operation_name = OP_BATCH_GRADS_KRON if kron_jac else OP_BATCH_GRADS\n    n_data = x.shape[0]\n    n_params = sum(p.numel() for p in model.parameters())\n\n    with extend(model, operation_name):\n        if x.requires_grad:\n            with disable_param_grad(model):\n                logits = model(x)\n        else:\n            logits = model(x)\n        if logits.ndim > 2: # handle augmented inputs by averaging\n            logits = logits.mean(dim=1)\n        n_data, c = logits.shape\n        j1 = logits.new_zeros(n_data, c, n_params)\n        for k in range(c): # Iterate over output dimensions to get individual Jacobians\n            model.zero_grad()\n            scalar = logits[:, k].sum()\n            if differentiable:\n                scalar.backward(retain_graph=True, create_graph=True)\n            else:\n                scalar.backward(retain_graph=(k < c - 1))\n            j_k = []\n            for module in model.modules():\n                operation = getattr(module, 'operation', None)\n                if operation is None:\n                    continue\n                batch_grads = operation.get_op_results()[operation_name]\n                for g in batch_grads.values():\n                    j_k.append(flatten_after_batch(g))\n            j_k = torch.cat(j_k, dim=1)\n            j1[:, k, :] = j_k\n\n    if likelihood == 'classification':\n        L = logits_hessian_cross_entropy(logits)\n        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale\n    elif likelihood == 'heteroscedastic_regression':\n        L = hessian_heteroscedastic_regression(logits)\n        j2 = (j1.transpose(1, 2) @ L).transpose(1, 2) * scale\n    elif likelihood == 'regression':\n        j2 = j1 * scale\n    else:\n        raise ValueError('Invalid likelihood')\n    return logits, torch.einsum('ncp,mdp->nmcd', j1, j2)\n\n\ndef linear_network_kernel_indep(model, x, scale, likelihood='classification', differentiable=False, \n                                kron_jac=False, single_output=None):\n    # Similar to linear_network_kernel but for independent outputs or single output\n    n = x.shape[0]\n\n    module_list = [[module] * (2 if getattr(module, 'bias', None) is not None else 1)\n                   for module in model.modules() if hasattr(module, 'weight')]\n    module_list = [(m, loc) for sublist in module_list\n                   for m, loc in zip(sublist, ['weight', 'bias'])]\n    if len(scale) == 1:\n        scale = [scale] * len(module_list)\n    assert len(scale) == len(module_list), 'Scale should be either scalar or for each weight and bias.'\n    for (module, loc), scalem in zip(module_list, scale):\n        setattr(module, f'{loc}_scale', scalem)\n\n    op_name = OP_GRAM_HADAMARD if kron_jac else OP_GRAM_DIRECT\n    with extend(model, op_name):\n        _zero_kernel(model, n, n)\n        if x.requires_grad:\n            with disable_param_grad(model):\n                outputs = model(x)\n        else:\n            outputs = model(x)\n        if outputs.ndim > 2:\n            outputs = outputs.mean(dim=1)\n        n_classes = outputs.shape[-1]\n        if likelihood == 'classification':\n            if single_output is None:\n                L = logits_diag_hessian_cross_entropy(outputs)\n            else:\n                L = logits_single_hessian_cross_entropy(outputs, single_output)\n        elif likelihood == 'heteroscedastic_regression':\n            if single_output is None:\n                L = hessian_diag_heteroscedastic_regression(outputs)\n            else:\n                L = hessian_single_heteroscedastic_regression(outputs, single_output)\n        else:\n            assert likelihood == 'regression'\n        kernels = []\n        output_range = range(n_classes) if single_output is None else [n_classes]\n        for k in output_range:\n            model.zero_grad()\n            if single_output is None:\n                scalar = outputs[:, k].sum()\n            elif single_output.ndim == 0:\n                scalar = outputs[:, single_output].sum()\n            elif single_output.ndim == 1:\n                scalar = outputs.gather(1, single_output.unsqueeze(-1)).sum()\n            else:\n                raise ValueError('Invalid single_output')\n            scalar.backward(\n                retain_graph=differentiable or (k < n_classes - 1) or (single_output is not None),\n                create_graph=differentiable\n            )\n            if likelihood == 'regression':\n                kernels.append(model.kernel)\n            else:\n                kernels.append(model.kernel * (L if single_output is not None else L[:, k]))\n            _zero_kernel(model, n, n)\n        _clear_kernel(model)\n\n    for (module, loc), scale in zip(module_list, scale):\n        delattr(module, f'{loc}_scale')\n\n    return outputs, kernels[0] if single_output is not None else torch.stack(kernels)",
    "experimental_info": "The experiments focus on deriving and assessing stochastic lower bounds for the linearized Laplace approximation of the marginal likelihood. \n\n**Models:**\n- Primary model: `MiniNet` (a small convolutional neural network).\n- Other models supported: `MLP`, `LeNet`, `WideResNet`, `ResNet` (FixupResnet).\n\n**Datasets:**\n- Main datasets for bound assessment: `mnist` and `mnist_r180` (rotated MNIST).\n- Other supported datasets: `fmnist`, `cifar10`, `cifar100`, `tinyimagenet`, including their rotated, translated, or scaled variants.\n\n**Laplace Approximations and Backends:**\n- **NTK-based (Functional) Lower Bounds:** Implemented using `FunctionalLaplace` with `AsdlGGN` or `AugAsdlGGN` backends.\n- **Parametric Structured Lower Bounds:** Implemented using `FullLaplace`, `BlockDiagLaplace`, `KronLaplace`, `DiagLaplace` with `AsdlGGN` or `AugAsdlGGN` backends.\n- `AsdlEF` and `AugAsdlEF` backends are also available for Empirical Fisher approximations.\n\n**Experimental Settings for Stochastic Bounds (from `bound_grid.py` and `classification_image.py`):**\n- **Device:** 'cuda' (GPU).\n- **Subset Size (`subset_size`):** Total number of data points considered, can be `len(train_dataset)` or `1000` for specific evaluations. Used to determine `n_reps` for averaging stochastic estimates.\n- **Marginal Likelihood Batch Size (`marglik_batch_size`):** For stochastic bounds (`stochastic=True`), batch sizes are varied: `[10, 20, 50, 100, 250, 500, 1000]`. For non-stochastic (parametric) bounds, `batch_sizes = [1000]` is typically used.\n- **Data Subsetting:** `sod=True` (subset of data) is enabled in `la_kwargs` for all evaluated bounds.\n- **Output Partitioning:** `single_output=True` can be enabled for output-wise partitioning (which requires `independent=True` for `FunctionalLaplace`). `single_output_iid` controls whether single outputs are sampled iid or one per batch.\n- **Data Grouping:** `grouped_loader=True` uses `GroupedSubsetTensorDataLoader` (grouping by labels) for creating data subsets, otherwise `SubsetTensorDataLoader` (random subsets).\n- **Prior Precision / Invariance Parameter Grid:** A grid search is performed over the prior precision (for `PRIOR` setting, `logspace(-4, 4, 100)`) or the rotational invariance parameter of `AffineLayer2d` (for `INVARIANCE` setting, `linspace(0, pi, 100)`).\n- **Number of Repetitions (`n_reps`):** Stochastic estimates are averaged over `int(subset_size / batch_size)` repetitions for each batch size and hyperparameter grid point.\n\n**Optimization during initial training (MAP + hyperparameter search) from `marglik_optimization`:**\n- **Epochs (`n_epochs`):** `100` in `bound_grid.py`, `500` in `classification_image.py`.\n- **Learning Rates:** `lr = 1e-3`, `lr_hyp = 1e-1`, `lr_aug = 0.05` (for augmentation parameters).\n- **Learning Rate Minima:** `lr_min = 1e-4`, `lr_hyp_min = 1e-2`, `lr_aug_min = 0.005` for cosine annealing scheduling.\n- **Optimizers:** `Adam` (default for `MiniNet`), `SGD` (for other models, with special handling for Fixup parameters).\n- **Burn-in Epochs (`n_epochs_burnin`):** `10` epochs before marginal likelihood optimization starts.\n- **Hyperparameter Steps (`n_hypersteps`, `n_hypersteps_prior`):** `1` or `2` hyperparameter steps in `bound_grid.py` (can be `100` for stochastic gradients in `classification_image.py`).\n- **Marglik Frequency (`marglik_frequency`):`1` or `5` epochs before re-estimating marginal likelihood.\n- **Stochastic Gradients (`stochastic_grad`):** Set to `True` when using lower bound estimators.\n- **Kronecker Jacobians (`kron_jac`):** Enabled by default (`True`) for efficiency where applicable.\n- **Augmentation:** `AffineLayer2d` is used as an augmenter for `lila` method, and its parameters can be optimized.\n\n**Evaluation:**\n- The primary metric is the `log_marginal_likelihood`."
}
