
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The hyperparameter optimization problem is framed as a bi-level optimization problem, where an outer loop optimizes a criterion (e.g., hold-out loss, SURE) w.r.t. hyperparameters (λ) and an inner loop computes Lasso-type regression coefficients (β̂(λ)). The core innovation lies in computing the weak Jacobian (∂β̂/∂λ) efficiently. Unlike standard implicit differentiation that relies on optimality conditions for smooth functions and requires solving a p×p linear system, this method leverages the fixed-point iteration property of proximal BCD algorithms for Lasso (which involve soft-thresholding). The authors derive sparse closed-form solutions for the Jacobian of Lasso and weighted Lasso (Proposition 1). The proposed algorithm, termed 'implicit forward iterative differentiation' (Algorithm 2), first computes the regression coefficients and identifies their support, then applies a modified forward differentiation recursion restricted to this support. This decouples computation, avoids large matrix inversions, and guarantees linear convergence of the Jacobian once the support is identified (Proposition 2). Hyperparameters are parametrized as e^λ to handle positivity constraints and scaling. For SURE, a weakly differentiable approximation based on Finite Differences Monte-Carlo (dof_FDMC) is used.

# Repository Content
File Path: doc/conf.py
Content:
# -*- coding: utf-8 -*-
#
# sparse-ho documentation build configuration file, created by
# sphinx-quickstart on Thu Jun  1 00:35:01 2017.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
# import os
# import sys
# sys.path.insert(0, os.path.abspath('.'))
import sphinx_bootstrap_theme
import sphinx_gallery
from distutils.version import LooseVersion
import matplotlib

# -- General configuration ------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.autosummary',
    'sphinx.ext.doctest',
    'sphinx.ext.intersphinx',
    'sphinx.ext.mathjax',
    'sphinx_gallery.gen_gallery',
    'numpydoc',
]

if LooseVersion(sphinx_gallery.__version__) < LooseVersion('0.2'):
    raise ImportError('Must have at least version 0.2 of sphinx-gallery, got '
                      '%s' % (sphinx_gallery.__version__,))

matplotlib.use('agg')


# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'sparse-ho'
copyright = u'2020-2020, sparse-ho contributors'
author = u'sparse-ho contributors'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
from sparse_ho import __version__ as version  # noqa
# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
# language = None

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This patterns also effect to html_static_path and html_extra_path
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = False

# generate autosummary even if no references
autosummary_generate = True

# remove warnings: "toctree contains reference to nonexisting document"
numpydoc_show_class_members = False

# -- Options for HTML output ----------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'bootstrap'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {
    'navbar_sidebarrel': False,
    'navbar_pagenav': False,
    'source_link_position': "",
    'navbar_links': [
        ("Examples", "auto_examples/index"),
        ("API", "api"),
        ("GitHub", "https://github.com/QB3/sparse-ho", True)
    ],
    # 'bootswatch_theme': "united",
    # 'bootswatch_theme': "sandstone",
    'bootswatch_theme': "flatly",
    # 'bootswatch_theme': "simplex",
    'bootstrap_version': "3",
}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = sphinx_bootstrap_theme.get_html_theme_path()

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']


# -- Options for HTMLHelp output ------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = 'sparse_ho_doc'


# -- Options for LaTeX output ---------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',

    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, 'sparse-ho.tex', u'sparse-ho Documentation',
     u'sparse-ho contributors', 'manual'),
]


# -- Options for manual page output ---------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    (master_doc, 'sparse-ho', u'sparse-ho Documentation',
     [author], 1)
]


# -- Options for Texinfo output -------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'sparse-ho', u'sparse-ho Documentation',
     author, 'sparse-ho', 'One line description of project.',
     'Miscellaneous'),
]

scrapers = ('matplotlib',)
try:
    import mne
    import pyvista
    mne.viz.set_3d_backend('pyvistaqt')
    pyvista.OFF_SCREEN = False
    scrapers += ('pyvista',)
except:
    pass

sphinx_gallery_conf = {
    'doc_module': ('sparse_ho',),
    'reference_url': dict(sparse_ho=None),
    'examples_dirs': '../examples',
    'gallery_dirs': 'auto_examples',
    'image_scrapers': scrapers,
    'reference_url': {
        'numpy': 'http://docs.scipy.org/doc/numpy-1.9.1',
        'scipy': 'http://docs.scipy.org/doc/scipy-0.17.0/reference',
    }
}


def setup(app):
    app.add_css_file('style.css')

File Path: examples/plot_compare_optimizers.py
Content:
"""
========================
Compare outer optimizers
========================

This example shows how to perform hyperparameter optimization
for sparse logistic regression using a held-out test set.

"""

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#          Mathurin Massias
#
# License: BSD (3-clause)


import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from libsvmdata.datasets import fetch_libsvm

from sparse_ho import ImplicitForward
from sparse_ho.ho import grad_search
from sparse_ho.utils import Monitor
from sparse_ho.models import SparseLogreg
from sparse_ho.criterion import HeldOutLogistic
from sparse_ho.utils_plot import discrete_cmap
from sparse_ho.grid_search import grid_search
from sparse_ho.optimizers import LineSearch, GradientDescent, Adam


print(__doc__)

dataset = 'rcv1.binary'
# dataset = 'simu'

if dataset != 'simu':
    X, y = fetch_libsvm(dataset)
    X = X[:, :100]
else:
    X, y = make_classification(
        n_samples=100, n_features=1_000, random_state=42, flip_y=0.02)


n_samples = X.shape[0]
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)

n_samples = len(y[idx_train])
alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train]))

alpha_max /= 2 * len(idx_train)
alpha_max = alpha_max
alpha_min = alpha_max / 100
max_iter = 100

tol = 1e-8

n_alphas = 30
p_alphas = np.geomspace(1, 0.0001, n_alphas)
alphas = alpha_max * p_alphas

##############################################################################
# Grid-search
# -----------

estimator = LogisticRegression(
    penalty='l1', fit_intercept=False, max_iter=max_iter)
model = SparseLogreg(estimator=estimator)
criterion = HeldOutLogistic(idx_train, idx_val)
monitor_grid = Monitor()
grid_search(
    criterion, model, X, y, alpha_min, alpha_max,
    monitor_grid, alphas=alphas, tol=tol)
objs = np.array(monitor_grid.objs)


##############################################################################
# Grad-search
# -----------
optimizer_names = ['line-search', 'gradient-descent', 'adam']
optimizers = {
    'line-search': LineSearch(n_outer=10, tol=tol),
    'gradient-descent': GradientDescent(n_outer=10, step_size=100),
    'adam': Adam(n_outer=10, lr=0.11)}

monitors = {}
alpha0 = alpha_max / 10  # starting point

for optimizer_name in optimizer_names:
    estimator = LogisticRegression(
        penalty='l1', fit_intercept=False, solver='saga', tol=tol)
    model = SparseLogreg(estimator=estimator)
    criterion = HeldOutLogistic(idx_train, idx_val)

    monitor_grad = Monitor()
    algo = ImplicitForward(tol_jac=tol, n_iter_jac=1000)

    optimizer = optimizers[optimizer_name]
    grad_search(
        algo, criterion, model, optimizer, X, y, alpha0,
        monitor_grad)
    monitors[optimizer_name] = monitor_grad


current_palette = sns.color_palette("colorblind")
dict_colors = {
    'line-search': 'Greens',
    'gradient-descent': 'Purples',
    'adam': 'Reds'}


fig, ax = plt.subplots(figsize=(8, 3))
ax.plot(alphas / alphas[0], objs, color=current_palette[0])
ax.plot(
    alphas / alphas[0], objs, 'bo',
    label='0-order method (grid-search)', color=current_palette[1])

for optimizer_name in optimizer_names:
    monitor = monitors[optimizer_name]
    p_alphas_grad = np.array(monitor.alphas) / alpha_max
    objs_grad = np.array(monitor.objs)
    cmap = discrete_cmap(len(p_alphas_grad), dict_colors[optimizer_name])
    ax.scatter(
        p_alphas_grad, objs_grad, label=optimizer_name,
        marker='X', color=cmap(np.linspace(0, 1, 10)), zorder=10)

ax.set_xlabel(r"$\lambda / \lambda_{\max}$")
ax.set_ylabel(
    r"$ \sum_i^n \log \left ( 1 + e^{-y_i^{\rm{val}} X_i^{\rm{val}} "
    r"\hat \beta^{(\lambda)} } \right ) $")

ax.set_xscale("log")
plt.tick_params(width=5)
plt.legend()
plt.tight_layout()
plt.show(block=False)

File Path: examples/plot_held_out_enet.py
Content:
"""
==================================
Elastic net with held-out test set
==================================

This example shows how to perform hyperparameter optimization
for an elastic-net using a held-out validation set.

"""

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#
# License: BSD (3-clause)

import time
import numpy as np
import matplotlib.pyplot as plt
import celer
from libsvmdata.datasets import fetch_libsvm
from celer.datasets import make_correlated_data
from sklearn.metrics import mean_squared_error

from sparse_ho import ImplicitForward
from sparse_ho.criterion import HeldOutMSE
from sparse_ho.models import ElasticNet
from sparse_ho.ho import grad_search
from sparse_ho.utils import Monitor
from sparse_ho.utils_plot import discrete_cmap
from sparse_ho.optimizers import GradientDescent


# dataset = "rcv1"
dataset = 'simu'

##############################################################################
# Load some data

# dataset = 'rcv1'
dataset = 'simu'

if dataset == 'rcv1':
    X, y = fetch_libsvm('rcv1.binary')
    y -= y.mean()
    y /= np.linalg.norm(y)
else:
    X, y, _ = make_correlated_data(
        n_samples=200, n_features=400, snr=5, random_state=0)


n_samples = X.shape[0]
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)

print("Starting path computation...")
alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)

alpha_min = 1e-4 * alpha_max

n_grid = 15
alphas_l1 = np.geomspace(alpha_max, alpha_min, n_grid)
alphas_l2 = np.geomspace(alpha_max, alpha_min, n_grid)

results = np.zeros((n_grid, n_grid))
tol = 1e-5
max_iter = 10_000

estimator = celer.ElasticNet(
    fit_intercept=False, tol=tol, max_iter=50, warm_start=True)

##############################################################################
# grid search with scikit-learn
# -----------------------------

print("Started grid search")
t_grid_search = - time.time()
for i in range(n_grid):
    print("lambda %i / %i" % (i * n_grid, n_grid * n_grid))
    for j in range(n_grid):
        estimator.alpha = (alphas_l1[i] + alphas_l2[j])
        estimator.l1_ratio = alphas_l1[i] / (alphas_l1[i] + alphas_l2[j])
        estimator.fit(X[idx_train, :], y[idx_train])
        results[i, j] = mean_squared_error(
            y[idx_val], estimator.predict(X[idx_val, :]))
t_grid_search += time.time()
print("Finished grid search")
print("Minimum outer criterion value with grid search %0.3e" % results.min())

##############################################################################
# Grad-search with sparse-ho
# --------------------------
estimator = celer.ElasticNet(
    fit_intercept=False, max_iter=50, warm_start=True)
print("Started grad-search")
t_grad_search = - time.time()
monitor = Monitor()
n_outer = 10
alpha0 = np.array([alpha_max * 0.9, alpha_max * 0.9])
model = ElasticNet(estimator=estimator)
criterion = HeldOutMSE(idx_train, idx_val)
algo = ImplicitForward(tol_jac=1e-3, n_iter_jac=100, max_iter=max_iter)
optimizer = GradientDescent(
    n_outer=n_outer, tol=tol, p_grad_norm=1.5, verbose=True)
grad_search(
    algo, criterion, model, optimizer, X, y, alpha0=alpha0,
    monitor=monitor)
t_grad_search += time.time()
monitor.alphas = np.array(monitor.alphas)

print("Time grid search %f" % t_grid_search)
print("Time grad-search %f" % t_grad_search)
print("Minimum grid search %0.3e" % results.min())
print("Minimum grad search %0.3e" % np.array(monitor.objs).min())

##############################################################################
# Plot results
# ------------

cmap = discrete_cmap(n_outer, 'Reds')
X, Y = np.meshgrid(alphas_l1 / alpha_max, alphas_l2 / alpha_max)
fig, ax = plt.subplots(1, 1)
cp = ax.contour(X, Y, results.T, levels=40)
ax.scatter(
    X, Y, s=10, c="orange", marker="o", label="$0$th order (grid search)",
    clip_on=False)
ax.scatter(
    monitor.alphas[:, 0] / alpha_max, monitor.alphas[:, 1] / alpha_max,
    s=40, color=cmap(np.linspace(0, 1, n_outer)), zorder=10,
    marker="X", label="$1$st order")
ax.plot(
    monitor.alphas[:, 0] / alpha_max, monitor.alphas[:, 1] / alpha_max,
    c=cmap(0))
ax.set_xlim(X.min(), X.max())
ax.set_xlabel("L1 regularization")
ax.set_ylabel("L2 regularization")
ax.set_ylim(Y.min(), Y.max())
ax.set_title("Elastic net held out prediction loss on test set")
cb = fig.colorbar(cp)
cb.set_label("Held-out loss")
plt.xscale('log')
plt.yscale('log')
plt.legend(loc='upper left')
plt.show(block=False)

File Path: examples/plot_held_out_lasso.py
Content:
"""
============================
Lasso with held-out test set
============================

This example shows how to perform hyperparameter optimization
for a Lasso using a held-out validation set.

"""

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#          Mathurin Massias
#
# License: BSD (3-clause)

import time
import celer
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from celer.datasets import make_correlated_data
from libsvmdata.datasets import fetch_libsvm

from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE
from sparse_ho import ImplicitForward
from sparse_ho.utils import Monitor
from sparse_ho.utils_plot import discrete_cmap
from sparse_ho.ho import grad_search
from sparse_ho.grid_search import grid_search
from sparse_ho.optimizers import LineSearch


print(__doc__)

dataset = 'rcv1'
# dataset = 'simu'

if dataset == 'rcv1':
    X, y = fetch_libsvm('rcv1.binary')
else:
    X, y, _ = make_correlated_data(n_samples=1000, n_features=2000,
                                   random_state=0)

n_samples = X.shape[0]
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)

print("Starting path computation...")
n_samples = len(y[idx_train])
alpha_max = np.max(np.abs(X[idx_train, :].T.dot(y[idx_train])))
alpha_max /= len(idx_train)
alpha0 = alpha_max / 5

n_alphas = 10
alphas = np.geomspace(alpha_max, alpha_max/1_000, n_alphas)
tol = 1e-7

##############################################################################
# Grid search with scikit-learn
# -----------------------------

estimator = celer.Lasso(fit_intercept=False, warm_start=True)

print('Grid search started')

t0 = time.time()
model = Lasso(estimator=estimator)
criterion = HeldOutMSE(idx_train, idx_val)
monitor_grid_sk = Monitor()
grid_search(
    criterion, model, X, y, None, None, monitor_grid_sk,
    alphas=alphas, tol=tol)
objs = np.array(monitor_grid_sk.objs)
t_sk = time.time() - t0

print('Grid search finished')


##############################################################################
# Grad-search with sparse-ho
# --------------------------

print('sparse-ho started')

t0 = time.time()
model = Lasso(estimator=estimator)
criterion = HeldOutMSE(idx_train, idx_val)
algo = ImplicitForward()
monitor_grad = Monitor()
optimizer = LineSearch(n_outer=10, tol=tol)
grad_search(
    algo, criterion, model, optimizer, X, y, alpha0, monitor_grad)

t_grad_search = time.time() - t0

print('sparse-ho finished')

##############################################################################
# Plot results
# ------------

p_alphas_grad = np.array(monitor_grad.alphas) / alpha_max

objs_grad = np.array(monitor_grad.objs)

print('sparse-ho finished')
print(f"Time for grid search: {t_sk:.2f} s")
print(f"Time for grad search (sparse-ho): {t_grad_search:.2f} s")

print(f'Minimum outer criterion value with grid search: {objs.min():.5f}')
print(f'Minimum outer criterion value with grad search: {objs_grad.min():.5f}')


current_palette = sns.color_palette("colorblind")
cmap = discrete_cmap(len(objs_grad), 'Greens')


fig, ax = plt.subplots(1, 1, figsize=(5, 3))
ax.plot(alphas / alphas[0], objs, color=current_palette[0])
ax.plot(
    alphas / alphas[0], objs, 'bo', label='0-th order method (grid search)',
    color=current_palette[1])
ax.scatter(
    p_alphas_grad, objs_grad, label='1-st order method',  marker='X',
    color=cmap(np.linspace(0, 1, len(objs_grad))), s=40, zorder=40)
ax.set_xlabel(r"$\lambda / \lambda_{\max}$")
ax.set_ylabel(
    r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$")
plt.tick_params(width=5)
plt.legend()
ax.set_xscale("log")
plt.tight_layout()
plt.show(block=False)

File Path: examples/plot_lassoCV.py
Content:
"""
=============================
Lasso with Cross-validation
=============================

This example shows how to perform hyperparameter optimization
for a Lasso using a full cross-validation score.
"""

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#          Mathurin Massias

# License: BSD (3-clause)

import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

from libsvmdata import fetch_libsvm
from sklearn.datasets import make_regression
from celer import LassoCV
from sklearn.model_selection import KFold

from sparse_ho import ImplicitForward, grad_search
from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE, CrossVal
from sparse_ho.optimizers import GradientDescent
from sparse_ho.utils import Monitor
from sparse_ho.utils_plot import discrete_cmap

print(__doc__)

# dataset = 'rcv1'
dataset = 'simu'

if dataset == 'rcv1':
    X, y = fetch_libsvm('rcv1.binary')
else:
    X, y = make_regression(
        n_samples=500, n_features=1000, noise=40,
        random_state=42)

kf = KFold(n_splits=5, shuffle=True, random_state=42)

print("Starting path computation...")
n_samples = len(y)
alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples

n_alphas = 10
alphas = np.geomspace(alpha_max, alpha_max / 1_000, n_alphas)

tol = 1e-8

#############################################################################
# Cross-validation with scikit-learn
# ----------------------------------
print('scikit started')

t0 = time.time()
reg = LassoCV(
    cv=kf, verbose=True, tol=tol, fit_intercept=False,
    alphas=alphas, max_iter=100_000).fit(X, y)
reg.score(X, y)
t_sk = time.time() - t0

print('scikit finished')

##############################################################################
# Now do the hyperparameter optimization with implicit differentiation
# --------------------------------------------------------------------

estimator = sklearn.linear_model.Lasso(fit_intercept=False,
                                       warm_start=True, max_iter=100_000)

print('sparse-ho started')

t0 = time.time()
model = Lasso(estimator)
criterion = HeldOutMSE(None, None)
alpha0 = 0.9 * alpha_max
monitor_grad = Monitor()
cross_val_criterion = CrossVal(criterion, cv=kf)
algo = ImplicitForward()
optimizer = GradientDescent(n_outer=10, tol=tol)
grad_search(
    algo, cross_val_criterion, model, optimizer, X, y, alpha0,
    monitor_grad)

t_grad_search = time.time() - t0

print('sparse-ho finished')

##############################################################################
# Plot results
# ------------
objs = reg.mse_path_.mean(axis=1)

p_alphas_grad = np.array(monitor_grad.alphas) / alpha_max
objs_grad = np.array(monitor_grad.objs)


print(f"Time for grid search: {t_sk:.2f} s")
print(f"Time for grad search (sparse-ho): {t_grad_search:.2f} s")

print(f'Minimum outer criterion value with grid search: {objs.min():.5f}')
print(f'Minimum outer criterion value with grad search: {objs_grad.min():.5f}')

current_palette = sns.color_palette("colorblind")
cmap = discrete_cmap(len(objs_grad), 'Greens')

fig, ax = plt.subplots(figsize=(5, 3))
ax.plot(alphas / alphas[0], objs, color=current_palette[0])
ax.plot(
    alphas / alphas[0], objs,
    'bo', label='0-th order method (grid search)',
    color=current_palette[1])
ax.scatter(
    p_alphas_grad, objs_grad,
    label='1-st order method',  marker='X',
    color=cmap(np.linspace(0, 1, len(objs_grad))), s=40, zorder=40)
plt.xlabel(r"$\lambda / \lambda_{\max}$")
plt.ylabel("Cross-validation loss")
ax.set_xscale("log")
plt.tick_params(width=5)
plt.legend()
plt.tight_layout()
plt.show(block=False)

File Path: examples/plot_meg_lasso_vs_wlasso.py
Content:
"""
==================================
Weighted Lasso versus Lasso on MEG
==================================

This example compares the Lasso and the weighted Lasso on real MEG data.
While the bias of the Lasso leads to optimal coefficients with a lot of
sources in the brain, the weighted Lasso is able to recover 1 source per
hemisphere in the brain, as expected from a neuroscience point of view.
"""

# Authors: Mathurin Massias <mathurin.massas@gmail.com>
#          Alexandre Gramfort <alexandre.gramfort@inria.fr>
#
# License: BSD (3-clause)

# sphinx_gallery_thumbnail_number = 4

import numpy as np
import mne
from mne.viz import plot_sparse_source_estimates
from mne.datasets import sample

from mne.inverse_sparse.mxne_inverse import (
    _prepare_gain, is_fixed_orient, _reapply_source_weighting,
    _make_sparse_stc)
from celer import Lasso as celer_Lasso
from sparse_ho.utils import Monitor
from sparse_ho.models import WeightedLasso, Lasso
from sparse_ho.criterion import FiniteDiffMonteCarloSure
from sparse_ho import Implicit
from sparse_ho.ho import grad_search
from sparse_ho.optimizers import GradientDescent


def apply_solver(
        evoked, forward, noise_cov, loose=0.2, depth=0.8, p_alpha0=0.7,
        model_name="wlasso"):
    """Call a custom solver on evoked data.

    This function does all the necessary computation:

    - to select the channels in the forward given the available ones in
      the data
    - to take into account the noise covariance and do the spatial whitening
    - to apply loose orientation constraint as MNE solvers
    - to apply a weighting of the columns of the forward operator as in the
      weighted Minimum Norm formulation in order to limit the problem
      of depth bias.


    Parameters
    ----------
    evoked : instance of mne.Evoked
        The evoked data
    forward : instance of Forward
        The forward solution.
    noise_cov : instance of Covariance
        The noise covariance.
    loose : float in [0, 1] | 'auto'
        Value that weights the source variances of the dipole components
        that are parallel (tangential) to the cortical surface. If loose
        is 0 then the solution is computed with fixed orientation.
        If loose is 1, it corresponds to free orientations.
        The default value ('auto') is set to 0.2 for surface-oriented source
        space and set to 1.0 for volumic or discrete source space.
    depth : None | float in [0, 1]
        Depth weighting coefficients. If None, no depth weighting is performed.
    p_alpha0 : float (default=0.7)
        Proportion of alpha_max for the initial point alpha0.
    model_name : string (default="wlasso")
        Name of the model to use, "lasso" or "wLasso" in this case.

    Returns
    -------
    stc : instance of SourceEstimate
        The source estimates.
    """
    all_ch_names = evoked.ch_names

    # Handle depth weighting and whitening (here is no weights)
    forward, gain, gain_info, whitener, source_weighting, _ = _prepare_gain(
        forward, evoked.info, noise_cov, pca=False, depth=depth,
        loose=0, weights=None, weights_min=None, rank=None)

    # Select channels of interest
    sel = [all_ch_names.index(name) for name in gain_info['ch_names']]
    M = evoked.data[sel]

    # Whiten data
    M = np.dot(whitener, M)

    n_orient = 1 if is_fixed_orient(forward) else 3

    X, active_set, monitor = solver(
        M, gain, n_orient, evoked.nave, p_alpha0=p_alpha0,
        model_name=model_name)
    X = _reapply_source_weighting(X, source_weighting, active_set)

    stc = _make_sparse_stc(X, active_set, forward, tmin=evoked.times[0],
                           tstep=1. / evoked.info['sfreq'])

    return stc, monitor


###############################################################################
# Define your solver
def solver(
        y_train, X_train, n_orient, nave, p_alpha0=0.7, model_name="wlasso"):
    n_times = y_train.shape[1]
    idx_max = np.argmax(np.sum(y_train ** 2, axis=0))
    y_train = y_train[:, idx_max]

    n_samples, n_features = X_train.shape
    alpha_max_old = (np.abs(X_train.T @ y_train)).max() / n_samples
    X_train /= alpha_max_old

    alpha_max = (np.abs(X_train.T @ y_train)).max() / n_samples
    alpha0 = p_alpha0 * alpha_max

    estimator = celer_Lasso(
        fit_intercept=False, max_iter=100, warm_start=True,
        tol=1e-3)
    if model_name == "wlasso":
        alpha0 = alpha0 * np.ones(n_features)
        model = WeightedLasso(estimator=estimator)

    else:
        model = Lasso(estimator=estimator)

    sigma = 1 / np.sqrt(nave)
    criterion = FiniteDiffMonteCarloSure(sigma=sigma)
    algo = Implicit()
    optimizer = GradientDescent(
        n_outer=4, tol=1e-7, verbose=True, p_grad_norm=1.9)
    monitor = Monitor()
    grad_search(algo, criterion, model, optimizer,
                X_train, y_train, alpha0, monitor)

    X = criterion.dense0[:, np.newaxis] * np.ones((1, n_times))
    active_set = criterion.mask0
    X /= alpha_max_old

    return X, active_set, monitor


data_path = sample.data_path()
fwd_fname = data_path + '/MEG/sample/sample_audvis-meg-eeg-oct-6-fwd.fif'
ave_fname = data_path + '/MEG/sample/sample_audvis-ave.fif'
cov_fname = data_path + '/MEG/sample/sample_audvis-shrunk-cov.fif'
condition = 'Left Auditory'


# %%
# Read noise covariance matrix and evoked data
noise_cov = mne.read_cov(cov_fname)
evoked = mne.read_evokeds(ave_fname, condition=condition, baseline=(None, 0))
evoked.crop(tmin=0.04, tmax=0.18)

# Crop data around the period of interest
evoked = evoked.pick_types(eeg=False, meg=True)

# %%
# Handling forward solution
forward = mne.read_forward_solution(fwd_fname)

loose, depth = 0., .8  # corresponds to free orientation

# %%
# Run estimation with Lasso
stc = apply_solver(
    evoked, forward, noise_cov, loose, depth, model_name="lasso")[0]
# Plot glass brain
plot_sparse_source_estimates(
    forward['src'], stc, bgcolor=(1, 1, 1), opacity=0.1)

# %%
# Run estimation with Weighted Lasso
stc = apply_solver(
    evoked, forward, noise_cov, loose, depth, model_name="wlasso")[0]
# Plot glass brain
plot_sparse_source_estimates(
    forward['src'], stc, bgcolor=(1, 1, 1), opacity=0.1)

File Path: examples/plot_sparse_log_reg.py
Content:
"""
===========================
Sparse logistic regression
===========================

This example shows how to perform hyperparameter optimisation
for sparse logistic regression using a held-out test set.

"""

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#
# License: BSD (3-clause)


import time
from libsvmdata.datasets import fetch_libsvm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_classification
from celer import LogisticRegression

from sparse_ho.ho import grad_search
from sparse_ho.utils import Monitor
from sparse_ho.models import SparseLogreg
from sparse_ho.criterion import HeldOutLogistic
from sparse_ho import ImplicitForward
from sparse_ho.grid_search import grid_search
from sparse_ho.optimizers import GradientDescent
from sparse_ho.utils_plot import discrete_cmap

print(__doc__)

dataset = 'rcv1.binary'
# dataset = 'simu'

if dataset != 'simu':
    X, y = fetch_libsvm(dataset)
    X = X[:, :100]
else:
    X, y = make_classification(
        n_samples=100, n_features=1_000, random_state=42, flip_y=0.02)


n_samples = X.shape[0]
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)

print("Starting path computation...")
n_samples = len(y[idx_train])
alpha_max = np.max(np.abs(X[idx_train, :].T.dot(y[idx_train])))

alpha_max /= 4 * len(idx_train)
alpha_max = alpha_max
alpha_min = alpha_max / 100
max_iter = 100

alpha0 = 0.1 * alpha_max
tol = 1e-8

n_alphas = 20
alphas = np.geomspace(alpha_max,  alpha_max / 1_000, n_alphas)

##############################################################################
# Grid-search
# -----------

print('Grid search started')
t0 = time.time()

estimator = LogisticRegression(
    penalty='l1', fit_intercept=False, max_iter=max_iter)
model = SparseLogreg(estimator=estimator)
criterion = HeldOutLogistic(idx_train, idx_val)
monitor_grid = Monitor()
grid_search(
    criterion, model, X, y, alpha_min, alpha_max,
    monitor_grid, alphas=alphas, tol=tol)
objs = np.array(monitor_grid.objs)

t_grid_search = time.time() - t0

print('scikit finished')
print(f"Time to compute grad search: {t_grid_search:.2f} s")


##############################################################################
# Grad-search
# -----------

print('sparse-ho started')

t0 = time.time()

estimator = LogisticRegression(
    penalty='l1', fit_intercept=False, tol=tol)
model = SparseLogreg(estimator=estimator)
criterion = HeldOutLogistic(idx_train, idx_val)

monitor_grad = Monitor()
algo = ImplicitForward(tol_jac=tol, n_iter_jac=1000)

optimizer = GradientDescent(n_outer=10, tol=tol)
grad_search(
    algo, criterion, model, optimizer, X, y, alpha0,
    monitor_grad)
objs_grad = np.array(monitor_grad.objs)

t_grad_search = time.time() - t0

print('sparse-ho finished')
print(f"Time to compute grad search: {t_grad_search:.2f} s")


p_alphas_grad = np.array(monitor_grad.alphas) / alpha_max

objs_grad = np.array(monitor_grad.objs)

current_palette = sns.color_palette("colorblind")

fig = plt.figure(figsize=(5, 3))
cmap = discrete_cmap(len(p_alphas_grad), "Greens")

plt.plot(alphas / alphas[0], objs, color=current_palette[0])
plt.plot(
    alphas / alphas[0], objs, 'bo',
    label='0-order method (grid-search)', color=current_palette[1])
plt.scatter(
    p_alphas_grad, objs_grad, label='1-st order method',
    marker='X', color=cmap(np.linspace(0, 1, len(objs_grad))), zorder=10)
plt.xlabel(r"$\lambda / \lambda_{\max}$")
plt.ylabel(
    r"$ \sum_i^n \log \left ( 1 + e^{-y_i^{\rm{val}} X_i^{\rm{val}} "
    r"\hat \beta^{(\lambda)} } \right ) $")

plt.xscale("log")
plt.tick_params(width=5)
plt.legend()
plt.tight_layout()
plt.show(block=False)

File Path: examples/plot_use_callback.py
Content:
"""
==============================================
Monitor custom metrics along hyperoptimization
==============================================
This example shows how to compute customize metrics using a callback function,
as in scipy.optimize.
"""

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#          Mathurin Massias <mathurin.massias@gmail.com>
# License: BSD (3-clause)

import numpy as np
import matplotlib.pyplot as plt

import celer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression

from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE
from sparse_ho import ImplicitForward
from sparse_ho.utils import Monitor
from sparse_ho.ho import grad_search
from sparse_ho.optimizers import LineSearch

from libsvmdata.datasets import fetch_libsvm


print(__doc__)

# dataset = 'rcv1'
dataset = 'simu'

if dataset == 'rcv1':
    X, y = fetch_libsvm('rcv1.binary')
else:
    X, y = make_regression(
        n_samples=1000, n_features=1000, noise=40, random_state=0)

# The dataset is split in 2: the data for training and validation: X/y and
# the unseen data X_test/y_test, use to assess the quality of the model
X, X_test, y, y_test = train_test_split(X, y, test_size=0.333, random_state=0)

n_samples = X.shape[0]
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)

alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)
alpha0 = alpha_max / 10

estimator = celer.Lasso(
    fit_intercept=False, max_iter=50, warm_start=True)

#############################################################################
# Call back definition
objs_test = []


def callback(val, grad, mask, dense, alpha):
    # The custom quantity is added at each outer iteration:
    # here the prediction MSE on test data
    objs_test.append(mean_squared_error(X_test[:, mask] @ dense, y_test))


##############################################################################
# Grad-search with sparse-ho and callback
# ---------------------------------------
model = Lasso(estimator=estimator)
criterion = HeldOutMSE(idx_train, idx_val)
algo = ImplicitForward()
# use Monitor(callback) with your custom callback
monitor = Monitor(callback=callback)
optimizer = LineSearch(n_outer=30)

grad_search(algo, criterion, model, optimizer, X, y, alpha0, monitor)

##############################################################################
# Plot results
# ------------
plt.figure(figsize=(5, 3))
plt.plot(monitor.times, objs_test)
plt.tick_params(width=5)
plt.xlabel("Times (s)")
plt.ylabel(r"$\|y^{\rm{test}} - X^{\rm{test}} \hat \beta^{(\lambda)} \|^2$")
plt.tight_layout()
plt.show(block=False)

File Path: examples/plot_wlasso.py
Content:
"""
=====================================
Weighted Lasso with held-out test set
=====================================

This example shows how to perform hyperparameter optimization
for a weighted Lasso using a held-out validation set.
In particular we compare the weighted Lasso to LassoCV on a toy example
"""

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#          Kenan Sehic
#          Mathurin Massias
# License: BSD (3-clause)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from celer import Lasso, LassoCV
from celer.datasets import make_correlated_data

from sparse_ho.models import WeightedLasso
from sparse_ho.criterion import HeldOutMSE, CrossVal
from sparse_ho import ImplicitForward
from sparse_ho.utils import Monitor
from sparse_ho.ho import grad_search
from sparse_ho.optimizers import GradientDescent


##############################################################################
# Dataset creation
X, y, w_true = make_correlated_data(
    n_samples=100, n_features=1000, random_state=0, snr=5)

##############################################################################
X, X_test, y, y_test = train_test_split(X, y, test_size=0.333, random_state=0)

n_samples, n_features = X.shape
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)
##############################################################################

##############################################################################
# Max penalty value
alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)
n_alphas = 30
alphas = np.geomspace(alpha_max, alpha_max / 1_000, n_alphas)
##############################################################################

# Create cross validation object
##############################################################################
cv = KFold(n_splits=5, shuffle=True, random_state=42)
##############################################################################

##############################################################################
# Vanilla LassoCV
print("========== Celer's LassoCV started ===============")
model_cv = LassoCV(
    verbose=False, fit_intercept=False, alphas=alphas, tol=1e-7, max_iter=100,
    cv=cv, n_jobs=2).fit(X, y)

# Measure mse on test
mse_cv = mean_squared_error(y_test, model_cv.predict(X_test))
print("Vanilla LassoCV: Mean-squared error on test data %f" % mse_cv)
##############################################################################


##############################################################################
# Weighted Lasso with sparse-ho.
# We use the vanilla lassoCV coefficients as a starting point
alpha0 = model_cv.alpha_ * np.ones(n_features)
# Weighted Lasso: Sparse-ho: 1 param per feature
estimator = Lasso(fit_intercept=False, max_iter=100, warm_start=True)
model = WeightedLasso(estimator=estimator)
sub_criterion = HeldOutMSE(idx_train, idx_val)
criterion = CrossVal(sub_criterion, cv=cv)
algo = ImplicitForward()
monitor = Monitor()
optimizer = GradientDescent(
    n_outer=100, tol=1e-7, verbose=True, p_grad_norm=1.9)
results = grad_search(
    algo, criterion, model, optimizer, X, y, alpha0, monitor)
##############################################################################

estimator.weights = monitor.alphas[-1]
estimator.fit(X, y)
##############################################################################
# MSE on validation set
mse_sho_val = mean_squared_error(y, estimator.predict(X))

# MSE on test set, ie unseen data
mse_sho_test = mean_squared_error(y_test, estimator.predict(X_test))

# Oracle MSE
mse_oracle = mean_squared_error(y_test, X_test @ w_true)

print("Sparse-ho: Mean-squared error on validation data %f" % mse_sho_val)
print("Sparse-ho: Mean-squared error on test (unseen) data %f" % mse_sho_test)


labels = ['WeightedLasso val', 'WeightedLasso test', 'Lasso CV', 'Oracle']

df = pd.DataFrame(
    np.array([mse_sho_val, mse_sho_test, mse_cv, mse_oracle]).reshape((1, -1)),
    columns=labels)
df.plot.bar(rot=0)
plt.xlabel("Estimator")
plt.ylabel("Mean squared error")
plt.tight_layout()
plt.show(block=False)
##############################################################################

File Path: expes/expe_cvxpy/figure_cvxpy.py
Content:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sparse_ho.utils_plot import configure_plt, plot_legend_apart
configure_plt()
fontsize = 18

# save_fig = False
save_fig = True

fig_dir = "results/"
fig_dir_svg = "results/"

current_palette = sns.color_palette("colorblind")
dict_method = {}
dict_method["forward"] = 'PCD Forward Iterdiff'
dict_method["backward"] = 'PCD Backward Iterdiff'
dict_method['cvxpy'] = 'Cvxpylayers'

dict_div_alphas = {}
dict_div_alphas[10] = r'$10$'
dict_div_alphas[100] = r'$10^2$'


dict_title = {}
dict_title["lasso"] = "Lasso"
dict_title["enet"] = "Elastic net"

dict_color = {}
dict_color["cvxpy"] = current_palette[3]
dict_color["backward"] = current_palette[9]
dict_color["forward"] = current_palette[4]

dict_markers = {}
dict_markers["forward"] = 'o'
dict_markers["backward"] = 'o'
dict_markers['cvxpy'] = 'o'

models = ["lasso"]
div_alphas = [10, 100]

fig, axarr = plt.subplots(
     len(models), len(div_alphas), sharex=False, sharey=True,
     figsize=[10.67, 3])
for idx, div_alpha in enumerate(div_alphas):
    for _, model in enumerate(models):
        times_fwd = np.load(
            "results/times_%s_forward_%s.npy" % (model, div_alpha),
            allow_pickle=True)
        times_bwd = np.load(
            "results/times_%s_backward_%s.npy" % (model, div_alpha),
            allow_pickle=True)
        times_cvxpy = np.load(
            "results/times_%s_cvxpy_%s.npy" % (model, div_alpha),
            allow_pickle=True)

        n_features = np.load(
            "results/nfeatures_%s_%s.npy" % (model, div_alpha),
            allow_pickle=True)
        print(n_features)
        axarr[idx].loglog(
            n_features, times_fwd, color=dict_color["forward"],
            marker=dict_markers["forward"], label=dict_method["forward"])
        axarr[idx].loglog(
            n_features, times_bwd, color=dict_color["backward"],
            marker=dict_markers["backward"], label=dict_method["backward"])
        axarr[idx].loglog(
            n_features, times_cvxpy, color=dict_color["cvxpy"],
            marker=dict_markers["cvxpy"], label=dict_method["cvxpy"])

        axarr[idx].set_xlabel(
            '\# features p', fontsize=fontsize)
        axarr[idx].set_yticks([1e-2, 1e0, 1e2])
        axarr.flat[idx].set_title(
            r"$e^\lambda = e^{\lambda_{\max}}/$ %s" % div_alpha)

axarr.flat[0].set_ylabel(
    " Time (s)", fontsize=fontsize)

fig.tight_layout()
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"
if save_fig:
    fig.savefig(fig_dir + "hypergrad_cvxpy.pdf", bbox_inches="tight")
    fig.savefig(fig_dir_svg + "hypergrad_cvxpy.svg", bbox_inches="tight")

    plot_legend_apart(
        axarr[0],
        fig_dir + "legend_cvxpy.pdf", ncol=3)
fig.show()

File Path: expes/expe_cvxpy/main_cvxpy.py
Content:
from joblib import Parallel, delayed, parallel_backend
from itertools import product
import numpy as np
from sparse_ho.utils import Monitor
from andersoncd.data.real import get_gina_agnostic
from sparse_ho import Forward, Backward
from sparse_ho.models import Lasso, ElasticNet
from sparse_ho.tests.cvxpylayer import lasso_cvxpy, enet_cvxpy
from sparse_ho.criterion import HeldOutMSE
import time

X, y = get_gina_agnostic(normalize_y=False)
n_samples, n_features = X.shape
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)

name_models = ["lasso", "enet"]

dict_models = {}
dict_models["lasso"] = Lasso()
dict_models["enet"] = ElasticNet()

dict_cvxpy = {}
dict_cvxpy["lasso"] = lasso_cvxpy
dict_cvxpy["enet"] = enet_cvxpy

dict_ncols = {}
dict_ncols[10] = np.geomspace(100, n_features, num=10, dtype=int)
dict_ncols[100] = np.geomspace(100, n_features, num=10, dtype=int)


tol = 1e-6
l1_ratio = 0.8
repeat = 10
div_alphas = [10, 100]


def parallel_function(name_model, div_alpha):
    index_col = np.arange(10)
    alpha_max = (np.abs(X[np.ix_(idx_train, index_col)].T
                 @ y[idx_train])).max() / len(idx_train)
    if name_model == "lasso":
        log_alpha = np.log(alpha_max / div_alpha)
    elif name_model == "enet":
        alpha0 = alpha_max / div_alpha
        alpha1 = (1 - l1_ratio) * alpha0 / l1_ratio
        log_alpha = np.log(np.array([alpha0, alpha1]))

    criterion = HeldOutMSE(idx_train, idx_val)
    algo = Forward()
    monitor = Monitor()
    val, grad = criterion.get_val_grad(
        dict_models[name_model], X[:, index_col], y, log_alpha,
        algo.compute_beta_grad, tol=tol, monitor=monitor)

    criterion = HeldOutMSE(idx_train, idx_val)
    algo = Backward()
    monitor = Monitor()
    val, grad = criterion.get_val_grad(
        dict_models[name_model], X[:, index_col], y, log_alpha,
        algo.compute_beta_grad, tol=tol, monitor=monitor)

    val_cvxpy, grad_cvxpy = dict_cvxpy[name_model](
        X[:, index_col], y, np.exp(log_alpha), idx_train, idx_val)

    list_times_fwd = []
    list_times_bwd = []
    list_times_cvxpy = []
    for n_col in dict_ncols[div_alpha]:
        temp_fwd = []
        temp_bwd = []
        temp_cvxpy = []
        for i in range(repeat):

            rng = np.random.RandomState(i)
            index_col = rng.choice(n_features, n_col, replace=False)
            alpha_max = (np.abs(X[np.ix_(idx_train, index_col)].T
                         @ y[idx_train])).max() / len(idx_train)
            if name_model == "lasso":
                log_alpha = np.log(alpha_max / div_alpha)
            elif name_model == "enet":
                alpha0 = alpha_max / div_alpha
                alpha1 = (1 - l1_ratio) * alpha0 / l1_ratio
                log_alpha = np.log(np.array([alpha0, alpha1]))

            criterion = HeldOutMSE(idx_train, idx_val)
            algo = Forward()
            monitor = Monitor()
            val, grad = criterion.get_val_grad(
                dict_models[name_model], X[:, index_col], y,
                log_alpha, algo.compute_beta_grad,
                tol=tol, monitor=monitor)
            temp_fwd.append(monitor.times)

            criterion = HeldOutMSE(idx_train, idx_val)
            algo = Backward()
            monitor = Monitor()
            val, grad = criterion.get_val_grad(
                dict_models[name_model], X[:, index_col], y,
                log_alpha, algo.compute_beta_grad,
                tol=tol, monitor=monitor)
            temp_bwd.append(monitor.times)

            t0 = time.time()
            val_cvxpy, grad_cvxpy = dict_cvxpy[name_model](
                X[:, index_col], y, np.exp(log_alpha), idx_train, idx_val)
            temp_cvxpy.append(time.time() - t0)

            print(np.abs(grad - grad_cvxpy * np.exp(log_alpha)))
        list_times_fwd.append(np.mean(np.array(temp_fwd)))
        list_times_bwd.append(np.mean(np.array(temp_bwd)))
        list_times_cvxpy.append(np.mean(np.array(temp_cvxpy)))

    np.save("results/times_%s_forward_%s" % (name_model, div_alpha),
            list_times_fwd)
    np.save("results/times_%s_backward_%s" % (name_model, div_alpha),
            list_times_bwd)
    np.save("results/times_%s_cvxpy_%s" % (name_model, div_alpha),
            list_times_cvxpy)
    np.save("results/nfeatures_%s_%s" % (name_model, div_alpha),
            dict_ncols[div_alpha])


print("enter parallel")
backend = 'loky'
n_jobs = len(name_models) * len(div_alphas)
with parallel_backend(backend, n_jobs=n_jobs, inner_max_num_threads=1):
    Parallel()(
        delayed(parallel_function)(
            name_model, div_alpha)
        for name_model, div_alpha in product(
            name_models, div_alphas))
print('OK finished parallel')

File Path: expes/expe_elastic/main.py
Content:
from itertools import product

import numpy as np
from numpy.linalg import norm

from joblib import Parallel, delayed

import pandas

from bcdsugar.utils import Monitor

from sparse_ho.ho import grad_search
from sparse_ho.criterion import HeldOutMSE
from sparse_ho.models import ElasticNet
from sparse_ho.forward import Forward
from sparse_ho.implicit_forward import ImplicitForward
from sparse_ho.implicit import Implicit
from sparse_ho.datasets.real import get_data
from sparse_ho.grid_search import grid_search


dataset_names = ["real-sim"]
# dataset_names = ["real-sim"]
# dataset_names = ["20news"]

# methods = ["grid_search"]
methods = ["random", "forward", "implicit_forward", "implicit", "grid_search"]
tolerance_decreases = ["constant"]
tols = 1e-5
n_outers = [25]

dict_t_max = {}
dict_t_max["rcv1"] = 50
dict_t_max["real-sim"] = 100
dict_t_max["leukemia"] = 10
dict_t_max["20news"] = 500


def parallel_function(
        dataset_name, method, tol=1e-5, n_outer=50,
        tolerance_decrease='exponential'):

    # load data
    X_train, X_val, X_test, y_train, y_val, y_test = get_data(dataset_name)
    n_samples, n_features = X_train.shape
    print('n_samples', n_samples)
    print('n_features', n_features)
    y_train[y_train == 0.0] = -1.0
    y_val[y_val == 0.0] = -1.0
    y_test[y_test == 0.0] = -1.0

    alpha_max = np.max(np.abs(X_train.T @ y_train))
    alpha_max /= X_train.shape[0]
    log_alpha_max = np.log(alpha_max)

    alpha_min = alpha_max * 1e-2
    # alphas = np.geomspace(alpha_max, alpha_min, 10)
    # log_alphas = np.log(alphas)

    log_alpha1_0 = np.log(0.1 * alpha_max)
    log_alpha2_0 = np.log(0.1 * alpha_max)

    log_alpha_max = np.log(alpha_max)
    n_outer = 25

    if dataset_name == "rcv1":
        size_loop = 2
    else:
        size_loop = 2
    model = ElasticNet(
        X_train, y_train, log_alpha1_0, log_alpha2_0, log_alpha_max, max_iter=1000, tol=tol)
    for i in range(size_loop):
        monitor = Monitor()

        if method == "implicit_forward":
            criterion = HeldOutMSE(X_val, y_val, model, X_test=X_test,
                                   y_test=y_test)
            algo = ImplicitForward(criterion, tol_jac=1e-3, n_iter_jac=100)
            _, _, _ = grad_search(
                algo=algo, verbose=False,
                log_alpha0=np.array([log_alpha1_0, log_alpha2_0]), tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "forward":
            criterion = HeldOutMSE(X_val, y_val, model, X_test=X_test,
                                   y_test=y_test)
            algo = Forward(criterion)
            _, _, _ = grad_search(
                algo=algo,
                log_alpha0=np.array([log_alpha1_0, log_alpha2_0]), tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "implicit":
            criterion = HeldOutMSE(X_val, y_val, model, X_test=X_test,
                                   y_test=y_test)
            algo = Implicit(criterion)
            _, _, _ = grad_search(
                algo=algo,
                log_alpha0=np.array([log_alpha1_0, log_alpha2_0]), tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "grid_search":
            criterion = HeldOutMSE(X_val, y_val, model, X_test=X_test,
                                   y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(alpha_min)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, log_alpha_max, monitor, max_evals=10, tol=tol, samp="grid",
                t_max=dict_t_max[dataset_name], log_alphas=None, nb_hyperparam=2)
            print(log_alpha_opt)

        elif method == "random":
            criterion = HeldOutMSE(X_val, y_val, model, X_test=X_test,
                                   y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(alpha_min)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, np.log(alpha_max), monitor, max_evals=10, tol=tol, samp="random",
                t_max=dict_t_max[dataset_name], nb_hyperparam=2)
            print(log_alpha_opt)

        elif method == "lhs":
            criterion = HeldOutMSE(X_val, y_val, model, X_test=X_test,
                                   y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(alpha_min)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, np.log(alpha_max), monitor, max_evals=10, tol=tol, samp="lhs",
                t_max=dict_t_max[dataset_name])
            print(log_alpha_opt)

    monitor.times = np.array(monitor.times).copy()
    monitor.objs = np.array(monitor.objs).copy()
    monitor.objs_test = np.array(monitor.objs_test).copy()
    monitor.log_alphas = np.array(monitor.log_alphas).copy()
    return (dataset_name, method, tol, n_outer, tolerance_decrease,
            monitor.times, monitor.objs, monitor.objs_test,
            monitor.log_alphas, norm(y_val), norm(y_test), log_alpha_max)


print("enter parallel")
backend = 'loky'
# n_jobs = 1
n_jobs = len(methods)
results = Parallel(n_jobs=n_jobs, verbose=100, backend=backend)(
    delayed(parallel_function)(
        dataset_name, method, n_outer=n_outer,
        tolerance_decrease=tolerance_decrease, tol=tols)
    for dataset_name, method, n_outer,
    tolerance_decrease in product(
        dataset_names, methods, n_outers, tolerance_decreases))
print('OK finished parallel')

df = pandas.DataFrame(results)
df.columns = [
    'dataset', 'method', 'tol', 'n_outer', 'tolerance_decrease',
    'times', 'objs', 'objs_test', 'log_alphas', 'norm y_val',
    'norm y_test', "log_alpha_max"]

for dataset_name in dataset_names:
    df[df['dataset'] == dataset_name].to_pickle(
        "%s.pkl" % dataset_name)

File Path: expes/expe_elastic/main_heatmap.py
Content:
import time
from sparse_ho.datasets.real import get_data
import numpy as np
from celer import ElasticNet as ElasticNet_celer
from numpy.linalg import norm
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sparse_ho.implicit_forward import ImplicitForward
from sparse_ho.criterion import HeldOutMSE
from sparse_ho.models import ElasticNet
from sparse_ho.ho import grad_search
from bcdsugar.utils import Monitor

Axes3D
dataset = "rcv1"
# dataset = "real-sim"
use_small_part = False
# use_small_part = True

#############################
print("Started to load data")

X_train, X_val, X_test, y_train, y_val, y_test = get_data(dataset)
if use_small_part:
    idx = np.abs((X_train.T @ y_train)).argsort()[-1000:]
    X_train = X_train[:, idx]
    X_val = X_val[:, idx]
    X_test = X_test[:, idx]
y_train -= y_train.mean()
y_val -= y_val.mean()
y_test -= y_test.mean()

print("Finished loading data")

alpha_max = np.max(np.abs(X_train.T @ y_train))
alpha_max /= X_train.shape[0]
log_alpha_max = np.log(alpha_max)

alpha_min = 1e-4 * alpha_max

n_grid = 10
alphas_1 = np.geomspace(0.6 * alpha_max, alpha_min, n_grid)
log_alphas_1 = np.log(alphas_1)
alphas_2 = np.geomspace(0.6 * alpha_max, alpha_min, n_grid)
log_alphas_2 = np.log(alphas_2)

results = np.zeros((n_grid, n_grid))
tol = 1e-7
max_iter = 50000

# grid search with scikit
print("Started grid-search")
t_grid_search = - time.time()

for i in range(n_grid):
    print("lambda %i / %i" % (i, n_grid))
    for j in range(n_grid):
        print("lambda %i / %i" % (j, n_grid))
        clf = ElasticNet_celer(
            alpha=(alphas_1[i] + alphas_2[j]), fit_intercept=False,
            l1_ratio=alphas_1[i] / (alphas_1[i] + alphas_2[j]),
            tol=tol, max_iter=max_iter, warm_start=True)
        clf.fit(X_train, y_train)
        results[i, j] = norm(y_val - X_val @ clf.coef_) ** 2 / X_val.shape[0]
t_grid_search += time.time()
print("Finished grid-search")

# grad search
print("Started grad-search")
t_grad_search = - time.time()
monitor = Monitor()
n_outer = 10
model = ElasticNet(
    X_train, y_train, log_alphas_1[-1], log_alphas_2[-1], log_alpha_max, max_iter=max_iter, tol=tol)
criterion = HeldOutMSE(
    X_val, y_val, model, X_test=X_test, y_test=y_test)
algo = ImplicitForward(
    criterion, tol_jac=1e-2, n_iter_jac=1000, max_iter=max_iter)

_, _, _ = grad_search(
    algo=algo, verbose=True,
    log_alpha0=np.array(
        [np.log(alpha_max/10), np.log(alpha_max/10)]),
    tol=tol, n_outer=n_outer, monitor=monitor, tolerance_decrease='constant')
alphas_grad = np.exp(np.array(monitor.log_alphas))
alphas_grad /= alpha_max

t_grad_search += time.time()

print("Time grid-search %f" % t_grid_search)
print("Minimum grid-search %.3e" % results.min())

print("Time grad-search %f" % t_grad_search)
print("Minimum grad-search %.3e" % np.array(monitor.objs).min())

X, Y = np.meshgrid(alphas_1 / alpha_max, alphas_2 / alpha_max)
fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(
    np.log(X), np.log(Y), results, rstride=1, cstride=1,
    cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter3D(
    np.log(alphas_grad[:, 0]), np.log(alphas_grad[:, 1]),
    monitor.objs, c="red", s=200, marker="X")
ax.set_xlabel("lambda1")
ax.set_ylabel("lambda2")
ax.set_zlabel("Loss on validation set")
ax.title.set_text(dataset)
fig.show()

File Path: expes/expe_elastic/plot.py
Content:
import numpy as np
import pandas
import seaborn as sns
import matplotlib.pyplot as plt
from expes.utils import configure_plt

save_fig = False
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"

configure_plt()

# init()

fontsize = 16

current_palette = sns.color_palette("colorblind")
dict_color = {}
dict_color["grid_search"] = current_palette[3]
dict_color["random"] = current_palette[5]
dict_color["bayesian"] = current_palette[0]
dict_color["implicit_forward"] = current_palette[2]
dict_color["fast_iterdiff"] = current_palette[2]
dict_color["forward"] = current_palette[4]
dict_color["implicit"] = current_palette[1]
dict_color["lhs"] = current_palette[6]

dict_method = {}
dict_method["forward"] = 'F. Iterdiff.'
dict_method["implicit_forward"] = 'Imp. F. Iterdiff. (ours)'
dict_method["fast_iterdiff"] = 'Imp. F. Iterdiff. (ours)'
dict_method['implicit'] = 'Implicit'
dict_method['grid_search'] = 'Grid-search'
dict_method['bayesian'] = 'Bayesian'
dict_method['random'] = 'Random-search'
dict_method['hyperopt'] = 'Random-search'
dict_method['backward'] = 'B. Iterdiff.'
dict_method['lhs'] = 'Lattice Hyp.'


dict_markers = {}
dict_markers["forward"] = 'o'
dict_markers["implicit_forward"] = 'X'
dict_markers["fast_iterdiff"] = 'X'
dict_markers['implicit'] = 'v'
dict_markers['grid_search'] = 'd'
dict_markers['bayesian'] = 'P'
dict_markers['random'] = '*'
dict_markers['lhs'] = 'H'

# current_palette = sns.color_palette("colorblind")
# dict_method = {}
# dict_method["forward"] = 'F. iterdiff.'
# dict_method["implicit_forward"] = 'Imp. F. iterdiff. (ours)'
# dict_method['implicit'] = 'Implicit diff. (ours)'
# dict_method['grid_search'] = 'Grid-search'
# dict_method['bayesian'] = 'Bayesian'
# dict_method['random'] = 'Random-search'
# dict_method['hyperopt'] = 'Random-search'

# TODO isolate
# current_palette[i]
# dict_color = {}
# dict_color["implicit_forward"] = current_palette[0]
# dict_color['implicit'] = current_palette[1]
# dict_color["forward"] = current_palette[2]
# dict_color['grid_search'] = current_palette[3]
# dict_color['bayesian'] = current_palette[4]
# dict_color['random'] = current_palette[5]
# dict_color['hyperopt'] = current_palette[6]

dict_title = {}
dict_title["rcv1"] = "rcv1"
dict_title["20news"] = "20news"
dict_title["finance"] = "finance"
dict_title["kdda_train"] = "kdda"
dict_title["climate"] = "climate"
dict_title["leukemia"] = "leukemia"
dict_title["real-sim"] = "real-sim"

dict_markevery = {}
dict_markevery["20news"] = 5
dict_markevery["finance"] = 10
dict_markevery["rcv1"] = 5
dict_markevery["leukemia"] = 1
dict_markevery["real-sim"] = 5

dict_n_feature = {}
dict_n_feature["rcv1"] = r"($p=19,959$)"
dict_n_feature["20news"] = r"($p=130,107$)"
dict_n_feature["finance"] = r"($p=1,668,737$)"
dict_n_feature["leukemia"] = r"($p=7,129$)"
dict_n_feature["real-sim"] = r"($p=20,958$)"

markersize = 8

dict_marker_size = {}
dict_marker_size["forward"] = 4
dict_marker_size["implicit_forward"] = 9
dict_marker_size["fast_iterdiff"] = 4
dict_marker_size['implicit'] = 4
dict_marker_size['grid_search'] = 5
dict_marker_size['bayesian'] = 4
dict_marker_size['random'] = 5
dict_marker_size['lhs'] = 4

# dataset_names = ["rcv1", "real-sim"]
dataset_names = ["real-sim"]
# dataset_names = ["20newsgroups"]
# dataset_names = ["finance"]
# dataset_names = ["leukemia"]
# dataset_names = [
#     "leukemia", "climate", "rcv1", "20newsgroups", "finance", "kdda_train"]


plt.close('all')
fig, axarr = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4],)

fig2, axarr2 = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4],)


fig3, axarr3 = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4],)


for idx, dataset in enumerate(dataset_names):
    df_data = pandas.read_pickle("%s.pkl" % dataset)
    df_data = df_data[df_data['tolerance_decrease'] == 'constant']

    methods = df_data['method']
    times = df_data['times']
    objs = df_data['objs']
    objs_tests = df_data['objs_test']
    log_alphas = df_data['log_alphas']
    tols = df_data['tolerance_decrease']
    norm_y_vals = df_data['norm y_val']
    norm_val = 0
    for norm_y_valss in norm_y_vals:
        norm_val = norm_y_valss

    min_objs = np.infty
    for obj in objs:
        min_objs = np.minimum(min_objs, obj.min())
        # obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]

    lines = []

    plt.figure()
    for i, (time, obj, log_alpha, method, tol) in enumerate(
            zip(times, objs, log_alphas, methods, tols)):
        marker = dict_markers[method]
        # objs_test = [np.min(objs_test[:k]) for k in np.arange(
        #     len(objs_test)) + 1]
        # if method == 'grid_search' or method == "implicit_forward":
        if method.startswith(('grid_search', "implicit_forward", "random")):
            axarr3.flat[idx].plot(
                np.array(log_alpha), obj,
                "bX", color=dict_color[method],
                label="%s" % (dict_method[method]),
                marker=marker, markersize=dict_marker_size[method],
                markevery=1)

    for i, (time, obj, objs_test, method, tol) in enumerate(
            zip(times, objs, objs_tests, methods, tols)):
        marker = dict_markers[method]
        objs_test = [np.min(objs_test[:k]) for k in np.arange(
            len(objs_test)) + 1]
        axarr2.flat[idx].semilogy(
            time, objs_test, color=dict_color[method],
            label="%s" % (dict_method[method]),
            marker=marker, markersize=markersize,
            markevery=dict_markevery[dataset])

    for i, (time, obj, method, tol) in enumerate(
            zip(times, objs, methods, tols)):
        marker = dict_markers[method]
        obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]
        lines.append(
            axarr.flat[idx].semilogy(
                time, (obj-min_objs),
                # time, (obj-min_objs) / norm_val,
                color=dict_color[method],
                label="%s" % (dict_method[method]),
                # label="%s, %s" % (dict_method[method], tol),
                marker=marker, markersize=markersize,
                markevery=dict_markevery[dataset]))
        # axarr.flat[i].legend()

    axarr3.flat[idx].set_title("%s %s" % (
        dict_title[dataset], dict_n_feature[dataset]), size=fontsize)
    # axarr.flat[idx].title.set_text(dict_title[dataset], size=18)
    axarr.flat[0].set_xlim(0, 15)
    axarr.flat[1].set_xlim(0, 40)
    axarr.flat[2].set_xlim(0, 210)

    axarr2.flat[0].set_xlim(0, 15)
    axarr2.flat[1].set_xlim(0, 40)
    axarr2.flat[2].set_xlim(0, 210)

axarr.flat[0].set_ylabel("Objective minus optimum", fontsize=fontsize)
# axarr.flat[0].set_ylabel("Objective minus optimum", fontsize=fontsize)


axarr2.flat[0].set_ylabel("Loss on test set", fontsize=fontsize)
axarr2.flat[0].set_xlabel("Time (s)", fontsize=fontsize)
axarr2.flat[1].set_xlabel("Time (s)", fontsize=fontsize)
axarr2.flat[2].set_xlabel("Time (s)", fontsize=fontsize)

axarr3.flat[0].set_ylabel("Loss on validation set", fontsize=fontsize)
axarr3.flat[0].set_xlabel(
    r"$\lambda - \lambda_{\max}$", fontsize=fontsize)
axarr3.flat[1].set_xlabel(
    r"$\lambda - \lambda_{\max}$", fontsize=fontsize)
axarr3.flat[2].set_xlabel(
    r"$\lambda - \lambda_{\max}$", fontsize=fontsize)

fig.tight_layout()
if save_fig:
    fig.savefig(
        fig_dir + "pred_log_reg_validation_set.pdf",
        bbox_inches="tight")
    fig.savefig(
        fig_dir_svg + "pred_log_reg_validation_set.svg",
        bbox_inches="tight")
fig.show()

fig2.tight_layout()
if save_fig:
    fig2.savefig(
        fig_dir + "pred_log_reg_test_set.pdf",
        bbox_inches="tight")
    fig2.savefig(
        fig_dir_svg + "pred_log_reg_test_set.svg",
        bbox_inches="tight")
fig2.show()

fig3.tight_layout()
if save_fig:
    fig3.savefig(
        fig_dir + "pred_vs_alpha_log_reg_validation_set.pdf",
        bbox_inches="tight")
    fig3.savefig(
        fig_dir_svg + "pred_vs_alpha_log_reg_validation_set.svg",
        bbox_inches="tight")
fig3.show()

File Path: expes/expe_elastic/plot_heatmap.py
Content:
import matplotlib.pyplot as plt
import numpy as np
from sparse_ho.utils_plot import configure_plt, plot_legend_apart

# save_fig = True
save_fig = False
save_fig_grad = True
# save_fig_grad = False
fig_dir = "../../../CD_SUGAR/tex/slides_qbe_long/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/slides_qbe_long/images/"


configure_plt()

objs_grad = np.load("grad_search.npy")
objs_grid = np.load("grid_search.npy")

alpha_1 = np.load("alpha_1.npy")
alpha_2 = np.load("alpha_2.npy")
alphas_grad = np.load("alpha_grad.npy")
alpha_max = np.load("alpha_max.npy")

X, Y = np.meshgrid(alpha_1/alpha_max, alpha_2/alpha_max)
Z = objs_grid

levels = np.geomspace(0.2, 1, num=20)
levels = np.round(levels, 2)

fontsize = 22

plt.set_cmap(plt.cm.viridis)

# fig, ax = plt.subplots(1, 1)
# cp = ax.contourf(X, Y, Z.T, levels)
# ax.scatter(
#     X, Y, s=10, c="orange", marker="o", label="$0$ order (grid search)")
# # cp.ax.tick_params(labelsize=2)
# ax.scatter(
#     alphas_grad[:, 0]/alpha_max,
#     alphas_grad[:, 1]/alpha_max,
#     s=100, color=[plt.cm.Reds((i + len(objs_grad) / 5 + 1) / len(objs_grad)) for i in np.arange(
#         len(objs_grad))], marker="x", label="$1$st order")
# cb = fig.colorbar(cp)
# for t in cb.ax.get_yticklabels():
#     t.set_fontsize(fontsize)
# ax.set_xlim(alpha_1.min()/alpha_max, alpha_1.max()/alpha_max)
# ax.set_ylim(alpha_2.min()/alpha_max, alpha_2.max()/alpha_max)
# plt.xscale('log')
# plt.yscale('log')
# fig.legend(loc=2, ncol=2, fontsize=fontsize, bbox_to_anchor=(0.1653, 1))
# ax.set_xlabel(r'$\lambda_1 / \lambda_\max$', fontsize=fontsize)
# ax.set_ylabel(r'$\lambda_2 / \lambda_\max$', fontsize=fontsize)
# # ax.set_xticklabels(fontsize=fontsize)
# plt.xticks(fontsize=fontsize)
# plt.yticks(fontsize=fontsize)
# # plt.tight_layout()

# if save_fig:
#     fig.savefig(
#         fig_dir + "held_out_real_sim_enet.pdf", bbox_inches="tight")
#     fig.savefig(
#         fig_dir_svg + "held_out_real_sim_enet.svg", bbox_inches="tight")

# plt.show(block=False)


color = [
    plt.cm.Reds((i + len(objs_grad) / 5 + 1) / len(objs_grad))
    for i in np.arange(len(objs_grad))]
#################################################################
# code for GRID SEARCH + GRAD SEARCH
for i in np.arange(len(objs_grad)+1):
    fig, ax = plt.subplots(1, 1, constrained_layout=True)
    cp = ax.contourf(X, Y, Z.T, levels)
    ax.scatter(
        X, Y, s=10, c="orange", marker="o", label=r"$0$-order (grid-search)")
    cb = fig.colorbar(cp)
    for t in cb.ax.get_yticklabels():
        t.set_fontsize(fontsize)
    ax.scatter(
        alphas_grad[:i, 0]/alpha_max,
        alphas_grad[:i, 1]/alpha_max,
        s=100, color=color[:i], marker="X", label=r"$1$st-order")
    plt.xscale('log')
    plt.yscale('log')
    ax.set_xlim(alpha_1.min()/alpha_max, alpha_1.max()/alpha_max)
    ax.set_ylim(alpha_2.min()/alpha_max, alpha_2.max()/alpha_max)
    plt.xscale('log')
    plt.yscale('log')
    # fig.legend(loc=2, ncol=2, fontsize=fontsize, bbox_to_anchor=(0.125, 1))
    ax.set_xlabel(r'$\lambda_1 / \lambda_{\max}$', fontsize=fontsize)
    ax.set_ylabel(r'$\lambda_2 / \lambda_{\max}$', fontsize=fontsize)
    # ax.set_xticklabels(fontsize=fontsize)
    plt.xticks(fontsize=fontsize)
    plt.yticks(fontsize=fontsize)
    # ax.set_xlim(p_alphas.min(), p_alphas.max())
    # ax.set_ylim(ymin, ymax)
    # plt.xlabel(r"$\lambda / \lambda_{\max}$", fontsize=28)
    # plt.ylabel(
    #     r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$",
    #     fontsize=28)
    # plt.tick_params(width=5)
    # plt.legend(fontsize=fontsize, loc=2)
    # plt.tight_layout()

    if save_fig_grad:
        fig.savefig(
            fig_dir + "grad_grid_search_real_sim_enet_%i.pdf" % i,
            bbox_inches="tight")
        fig.savefig(
            fig_dir_svg + "grad_grid_search_real_sim_enet_%i.svg" % i,
            bbox_inches="tight")
    plt.show(block=False)
    # if i == 1:
    #     plot_legend_apart(
    #         ax, fig_dir + "grad_grid_search_real_sim_enet_legend.pdf")

if save_fig_grad:
    fig, ax = plt.subplots(2, 2, constrained_layout=True, figsize=[10.67, 3])
    ax[0, 0].plot(
        np.linspace(0, 5),
        np.linspace(0, 5), label=r"$0$-order", marker="o", color="orange")
    ax[0, 0].plot(
        np.linspace(0, 5),
        np.linspace(0, 5), label=r"$1$st-order", marker="X", color=color[0])

    plot_legend_apart(
        ax[0, 0], fig_dir + "grad_grid_search_real_sim_enet_legend.pdf",
        figwidth=10.67)

File Path: expes/expe_enet/figure_enet_pred.py
Content:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sparse_ho.utils_plot import (
    discrete_color, dict_color, dict_color_2Dplot, dict_markers,
    dict_method, dict_title, configure_plt)

# save_fig = False
save_fig = True
# fig_dir = "./"
# fig_dir_svg = "./"
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"

configure_plt()
fontsize = 18

dict_markevery = {}
dict_markevery["news20"] = 1
dict_markevery["finance"] = 10
dict_markevery["rcv1_train"] = 1
dict_markevery["real-sim"] = 1
dict_markevery["leukemia"] = 10


dict_marker_size = {}
dict_marker_size["forward"] = 4
dict_marker_size["implicit_forward"] = 10
dict_marker_size["implicit_forward_approx"] = 10
dict_marker_size["fast_iterdiff"] = 4
dict_marker_size['implicit'] = 4
dict_marker_size['grid_search'] = 5
dict_marker_size['bayesian'] = 10
dict_marker_size['random'] = 5
dict_marker_size['lhs'] = 4

dict_s = {}
dict_s["implicit_forward"] = 50
dict_s["implicit_forward_approx"] = 70
dict_s['grid_search'] = 40
dict_s['bayesian'] = 70
dict_s['random'] = 5
dict_s['lhs'] = 4

dict_n_feature = {}
dict_n_feature["rcv1_train"] = r"($p=19,959$)"
dict_n_feature["real-sim"] = r"($p=20,958$)"
dict_n_feature["news20"] = r"($p=632,982$)"
dict_n_feature["finance"] = r"($p=1,668,737$)"
dict_n_feature["leukemia"] = r"($p=7129$)"

dict_xmax = {}
dict_xmax["enet", "rcv1_train"] = 250
dict_xmax["enet", "real-sim"] = 400
dict_xmax["enet", "leukemia"] = 5
dict_xmax["enet", "news20"] = 2000

dict_xticks = {}
dict_xticks["enet", "rcv1_train"] = (-6, -4, -2, 0)
dict_xticks["enet", "real-sim"] = (-6, -4, -2, 0)
dict_xticks["enet", "leukemia"] = (-6, -4, -2, 0)
dict_xticks["enet", "news20"] = (-8, -6, -4, -2, 0)

dict_xticks["logreg", "rcv1"] = (-8, -6, -4, -2, 0)
dict_xticks["logreg", "real-sim"] = (-8, -6, -4, -2, 0)
dict_xticks["logreg", "leukemia"] = (-8, -6, -4, -2, 0)
dict_xticks["logreg", "news20"] = (-8, -6, -4, -2, 0)

markersize = 8

dataset_names = ["rcv1_train", "real-sim", "news20"]


plt.close('all')
fig_val, axarr_val = plt.subplots(
    1, len(dataset_names), sharex=False, sharey=True, figsize=[10.67, 3.5],)

fig_test, axarr_test = plt.subplots(
    1, len(dataset_names), sharex=False, sharey=False, figsize=[10.67, 3.5],)

fig_grad, axarr_grad = plt.subplots(
    3, len(dataset_names), sharex=False, sharey=False, figsize=[11, 10],
    )


model_name = "enet"

for idx, dataset in enumerate(dataset_names):
    df_data = pd.read_pickle("results/%s_%s.pkl" % (model_name, dataset))
    df_data = df_data[df_data['tolerance_decrease'] == 'constant']

    methods = df_data['method']
    times = df_data['times']
    objs = df_data['objs']
    all_alphas = df_data['alphas']
    alpha_max = df_data['alpha_max'].to_numpy()[0]
    tols = df_data['tolerance_decrease']

    min_objs = np.infty
    for obj in objs:
        min_objs = min(min_objs, obj.min())

    lines = []

    axarr_test.flat[idx].set_xlim(0, dict_xmax[model_name, dataset])
    axarr_test.flat[idx].set_xlabel("Time (s)", fontsize=fontsize)

    E0 = df_data.objs.to_numpy()[2][0]
    for _, (time, obj, alphas, method, _) in enumerate(
            zip(times, objs, all_alphas, methods, tols)):
        if method == 'grid_search':
            alpha1D = np.unique(alphas)
            alpha1D.sort()
            alpha1D = np.log(np.flip(alpha1D) / alpha_max)
            X, Y = np.meshgrid(alpha1D, alpha1D)
            results = obj.reshape(len(alpha1D), -1)
            levels = np.geomspace(5 * 1e-3, 1, num=30) * (
                results.max() - min_objs) / min_objs

            cmap = 'Greys_r'
            for i in range(3):
                axarr_grad[i, idx].contour(
                    X, Y, (results.T - min_objs) / min_objs, levels=levels,
                    cmap=cmap, linewidths=0.5)

    for _, (time, obj, alphas, method, _) in enumerate(
            zip(times, objs, all_alphas, methods, tols)):
        marker = dict_markers[method]
        n_outer = len(obj)
        s = dict_s[method]
        color = discrete_color(n_outer, dict_color_2Dplot[method])
        if method == 'grid_search':
            i = 0
            axarr_grad[i, idx].scatter(
                np.log(alphas[:, 0] / alpha_max),
                np.log(alphas[:, 1] / alpha_max),
                s=s, color=color,
                marker=dict_markers[method], label="todo", clip_on=False)
        elif method == 'bayesian':
            i = 1
            axarr_grad[i, idx].scatter(
                np.log(alphas[:, 0] / alpha_max),
                np.log(alphas[:, 1] / alpha_max),
                s=s, color=color,
                marker=dict_markers[method], label="todo", clip_on=False)
        elif method == 'implicit_forward_approx':
            i = 2
            axarr_grad[i, idx].scatter(
                np.log(alphas[:, 0] / alpha_max),
                np.log(alphas[:, 1] / alpha_max),
                s=s, color=color,
                marker=dict_markers[method], label="todo", clip_on=False)
        else:
            pass
        axarr_grad[i, 0].set_ylabel(
            "%s \n" % dict_method[method] + r"$\lambda_2 - \lambda_{\max}$",
            fontsize=fontsize)

    for i in range(3):
        axarr_grad[i, idx].set_xlim((alpha1D.min(), alpha1D.max()))
        axarr_grad[i, idx].set_ylim((alpha1D.min(), alpha1D.max()))

    for _, (time, obj, method, _) in enumerate(
            zip(times, objs, methods, tols)):
        marker = dict_markers[method]
        markersize = dict_marker_size[method]
        obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]
        lines.append(
            axarr_val.flat[idx].plot(
                time, obj / E0,
                color=dict_color[method], label="%s" % (dict_method[method]),
                marker=marker, markersize=markersize,
                markevery=dict_markevery[dataset]))
    axarr_val.flat[idx].set_xlim(0, dict_xmax[model_name, dataset])
    axarr_val.flat[idx].set_xlabel("Time (s)", fontsize=fontsize)

    axarr_grad.flat[idx].set_title("%s %s" % (
        dict_title[dataset], dict_n_feature[dataset]), size=fontsize)

axarr_val.flat[0].set_ylim(0.15, 0.6)

for i in range(len(dataset_names)):
    axarr_grad[2, i].set_xlabel(
        r"$\lambda_1 - \lambda_{\max}$", fontsize=fontsize)
    for j in range(len(dataset_names)):
        axarr_grad[i, j].set_aspect('equal', adjustable='box')
        axarr_grad[i, j].set_xticks([-10, -5, 0])
        axarr_grad[i, j].set_yticks([-10, -5, 0])

axarr_val.flat[0].set_ylabel(
    "Cross-validation \n loss", fontsize=fontsize)
axarr_test.flat[0].set_ylabel("Loss on test set", fontsize=fontsize)


fig_val.tight_layout()
# fig_test.tight_layout()
fig_grad.tight_layout()


if save_fig:
    fig_val.savefig(
        fig_dir + "%s_val.pdf" % model_name, bbox_inches="tight")
    fig_val.savefig(
        fig_dir_svg + "%s_val.svg" % model_name, bbox_inches="tight")
    fig_test.savefig(
        fig_dir + "%s_test.pdf" % model_name, bbox_inches="tight")
    fig_test.savefig(
        fig_dir_svg + "%s_test.svg" % model_name, bbox_inches="tight")
    fig_grad.savefig(
        fig_dir + "%s_val_grad.pdf" % model_name, bbox_inches="tight")
    fig_grad.savefig(
        fig_dir + "%s_val_grad_grid.pdf" % model_name, bbox_inches="tight")
    fig_grad.savefig(
        fig_dir_svg + "%s_val_grad.svg" % model_name,
        bbox_inches="tight")


fig_val.show()
fig_grad.show()
# fig_grad_bayesian.show()
# fig_grad_sparseho.show()

#################################################################
# plot legend
labels = []
for method in methods:
    labels.append(dict_method[method])

fig_legend = plt.figure(figsize=[9, 2])
fig_legend.legend(
    [l[0] for l in lines], labels,
    ncol=4, loc='upper center', fontsize=fontsize - 4)
fig_legend.tight_layout()
if save_fig:
    fig_legend.savefig(
        fig_dir + "enet_pred_legend.pdf", bbox_inches="tight")
fig_legend.show()

File Path: expes/expe_enet/main_enet_pred.py
Content:
"""
This is the file to reproduce the experiments of the figure
'Computation time for the HO of the enet on real data.'
It is recommended to run this script on a cluster with several CPUs.
"""

import numpy as np
from joblib import Parallel, delayed, parallel_backend
from itertools import product
import pandas as pd
from sklearn import linear_model

from sklearn.model_selection import KFold

from libsvmdata import fetch_libsvm

from sparse_ho.models import ElasticNet, SparseLogreg
from sparse_ho.criterion import HeldOutMSE, HeldOutLogistic, CrossVal
from sparse_ho.utils import Monitor
from sparse_ho.optimizers import GradientDescent

from sparse_ho import ImplicitForward
from sparse_ho.grid_search import grid_search
from sparse_ho.ho import hyperopt_wrapper

from sparse_ho.ho import grad_search

model_name = "enet"

dict_t_max = {}
dict_t_max["rcv1_train"] = 1000
dict_t_max["real-sim"] = 1000
dict_t_max["leukemia"] = 10
dict_t_max["news20"] = 10_000

dict_point_grid_search = {}
dict_point_grid_search["rcv1_train"] = 10
dict_point_grid_search["real-sim"] = 10
dict_point_grid_search["leukemia"] = 10
dict_point_grid_search["news20"] = 10

#######################################################################
dataset_names = ["news20"]
# dataset_names = ["rcv1_train", "real-sim"]
methods = [
    "implicit_forward", "implicit_forward_approx", 'grid_search', 'bayesian']
tolerance_decreases = ["constant"]
# tols = [1e-8]
tol = 1e-6
# tol = 1e-8
n_outers = [75]
n_alphas = 100

dict_n_outers = {}
dict_n_outers["news20", "implicit_forward"] = 50
dict_n_outers["news20", "forward"] = 60
dict_n_outers["news20", "implicit"] = 6
dict_n_outers["news20", "bayesian"] = 75
dict_n_outers["news20", "random"] = 35

dict_n_outers["finance", "implicit_forward"] = 125
dict_n_outers["finance", "forward"] = 75
dict_n_outers["finance", "implicit"] = 6
dict_n_outers["finance", "bayesian"] = 75
dict_n_outers["finance", "random"] = 50

#######################################################################
# n_jobs = 1
n_jobs = len(dataset_names) * len(methods) * len(tolerance_decreases)
n_jobs = min(n_jobs, 10)
#######################################################################
dict_palphamin = {}
dict_palphamin["rcv1_train"] = 1 / 100_000
dict_palphamin["real-sim"] = 1 / 100_000
dict_palphamin["news20"] = 1 / 1_000_000


def parallel_function(
        dataset_name, method, tol=1e-5, n_outer=50,
        tolerance_decrease='constant'):

    # load data
    X, y = fetch_libsvm(dataset_name)
    y -= np.mean(y)
    # compute alpha_max
    alpha_max = np.abs(X.T @ y).max() / len(y)

    if model_name == "logreg":
        alpha_max /= 2
    alpha_min = alpha_max * dict_palphamin[dataset_name]

    if model_name == "enet":
        estimator = linear_model.ElasticNet(
            fit_intercept=False, max_iter=10_000, warm_start=True, tol=tol)
        model = ElasticNet(estimator=estimator)
    elif model_name == "logreg":
        model = SparseLogreg(estimator=estimator)

    # TODO improve this
    try:
        n_outer = dict_n_outers[dataset_name, method]
    except Exception:
        n_outer = 20

    size_loop = 2
    for _ in range(size_loop):
        if model_name == "lasso" or model_name == "enet":
            sub_criterion = HeldOutMSE(None, None)
        elif model_name == "logreg":
            criterion = HeldOutLogistic(None, None)
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        criterion = CrossVal(sub_criterion, cv=kf)

        algo = ImplicitForward(tol_jac=1e-3)
        monitor = Monitor()
        t_max = dict_t_max[dataset_name]
        if method == 'grid_search':
            num1D = dict_point_grid_search[dataset_name]
            alpha1D = np.geomspace(alpha_max, alpha_min, num=num1D)
            alphas = [np.array(i) for i in product(alpha1D, alpha1D)]
            grid_search(
                algo, criterion, model, X, y, alpha_min, alpha_max,
                monitor, max_evals=100, tol=tol, alphas=alphas)
        elif method == 'random' or method == 'bayesian':
            hyperopt_wrapper(
                algo, criterion, model, X, y, alpha_min, alpha_max,
                monitor, max_evals=30, tol=tol, method=method, size_space=2,
                t_max=t_max)
        elif method.startswith("implicit_forward"):
            # do gradient descent to find the optimal lambda
            alpha0 = np.array([alpha_max / 100, alpha_max / 100])
            n_outer = 30
            if method == 'implicit_forward':
                optimizer = GradientDescent(
                    n_outer=n_outer, p_grad_norm=1, verbose=True, tol=tol,
                    t_max=t_max)
            else:
                optimizer = GradientDescent(
                    n_outer=n_outer, p_grad_norm=1, verbose=True, tol=tol,
                    t_max=t_max,
                    tol_decrease="geom")
            grad_search(
                algo, criterion, model, optimizer, X, y, alpha0,
                monitor)
        else:
            raise NotImplementedError

    monitor.times = np.array(monitor.times)
    monitor.objs = np.array(monitor.objs)
    monitor.objs_test = 0  # TODO
    monitor.alphas = np.array(monitor.alphas)
    return (dataset_name, method, tol, n_outer, tolerance_decrease,
            monitor.times, monitor.objs, monitor.objs_test,
            monitor.alphas, alpha_max,
            model_name)


print("enter parallel")

with parallel_backend("loky", inner_max_num_threads=1):
    results = Parallel(n_jobs=n_jobs, verbose=100)(
        delayed(parallel_function)(
            dataset_name, method, n_outer=n_outer,
            tolerance_decrease=tolerance_decrease, tol=tol)
        for dataset_name, method, n_outer,
        tolerance_decrease in product(
            dataset_names, methods, n_outers, tolerance_decreases))
    print('OK finished parallel')

df = pd.DataFrame(results)
df.columns = [
    'dataset', 'method', 'tol', 'n_outer', 'tolerance_decrease',
    'times', 'objs', 'objs_test', 'alphas', 'alpha_max', 'model_name']

for dataset_name in dataset_names:
    df[df['dataset'] == dataset_name].to_pickle(
        "results/%s_%s.pkl" % (model_name, dataset_name))

File Path: expes/expe_enet/plot_enet_pred.py
Content:
"""
=============================
Expe enet
=============================

File to play with expes for the enet
"""

from itertools import product
import numpy as np
import matplotlib.pyplot as plt

import celer
from sklearn.datasets import make_regression
from sklearn.model_selection import KFold
from libsvmdata import fetch_libsvm

from sparse_ho import ImplicitForward
from sparse_ho import grad_search, hyperopt_wrapper
from sparse_ho.models import ElasticNet
from sparse_ho.criterion import HeldOutMSE, CrossVal
from sparse_ho.optimizers import GradientDescent
from sparse_ho.utils import Monitor
from sparse_ho.utils_plot import configure_plt
from sparse_ho.grid_search import grid_search
from sparse_ho.utils_plot import discrete_color

configure_plt()

# dataset = 'real-sim'
dataset = 'rcv1_train'
# dataset = 'simu'

if dataset != 'simu':
    X, y = fetch_libsvm(dataset)
    y -= y.mean()
else:
    X, y = make_regression(
        n_samples=500, n_features=1000, noise=40,
        random_state=42)

n_samples = len(y)
alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples
alpha_min = alpha_max / 100_000

num1D = 5
alpha1D = np.geomspace(alpha_max, alpha_min, num=num1D)
alphas = [np.array(i) for i in product(alpha1D, alpha1D)]

tol = 1e-3

estimator = celer.ElasticNet(
    fit_intercept=False, max_iter=50, warm_start=True, tol=tol)


dict_monitor = {}

all_algo_name = ['grid_search']
# , 'implicit_forward', "implicit_forward_approx", 'bayesian']
# , 'random_search']
# all_algo_name = ['random_search']

for algo_name in all_algo_name:
    model = ElasticNet(estimator=estimator)
    sub_criterion = HeldOutMSE(None, None)
    alpha0 = np.array([alpha_max / 10, alpha_max / 10])
    monitor = Monitor()
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    criterion = CrossVal(sub_criterion, cv=kf)
    algo = ImplicitForward(tol_jac=1e-3)
    # optimizer = LineSearch(n_outer=10, tol=tol)
    if algo_name.startswith('implicit_forward'):
        if algo_name == "implicit_forward_approx":
            optimizer = GradientDescent(
                n_outer=30, p_grad_norm=1., verbose=True, tol=tol,
                tol_decrease="geom")
        else:
            optimizer = GradientDescent(
                n_outer=30, p_grad_norm=1., verbose=True, tol=tol)
        grad_search(
            algo, criterion, model, optimizer, X, y, alpha0,
            monitor)
    elif algo_name == 'grid_search':
        grid_search(
            algo, criterion, model, X, y, None, None,
            monitor, max_evals=20, tol=tol, alphas=alphas)
    elif algo_name == 'random_search' or algo_name == 'bayesian':
        hyperopt_wrapper(
            algo, criterion, model, X, y, alpha_min, alpha_max,
            monitor, max_evals=20, tol=tol, method=algo_name, size_space=2)
    else:
        1 / 0
    dict_monitor[algo_name] = monitor


min_objs = np.infty
for monitor in dict_monitor.values():
    monitor.objs = np.array(monitor.objs)
    min_objs = min(min_objs, monitor.objs.min())

scaling_factor = (y @ y) / len(y)
plt.figure()
for monitor in dict_monitor.values():
    obj = monitor.objs
    obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]
    plt.plot(monitor.times, obj / scaling_factor)
plt.xlabel('Time (s)')
plt.ylabel('Objective')
plt.show(block=False)


dict_colors = {
    'implicit_forward': 'Greens',
    'implicit_forward_approx':  'OrRd',
    'grid_search': 'Reds',
    'bayesian': 'Blues'
}

##############################################################


results = dict_monitor["grid_search"].objs.reshape(len(alpha1D), -1)
scaling_factor = results.max()
levels = np.geomspace(min_objs / scaling_factor, 1, num=20)

X, Y = np.meshgrid(alpha1D / alpha_max, alpha1D / alpha_max)
fig, ax = plt.subplots(1, 1)
cp = ax.contourf(
    X, Y, results.T / scaling_factor, levels=levels, cmap="viridis")
ax.scatter(
    X, Y, s=10, c="orange", marker="o", label="$0$th order (grid search)",
    clip_on=False)


for method in dict_monitor.keys():
    if method != 'grid_search':
        monitor = dict_monitor[method]
        monitor.alphas = np.array(monitor.alphas)
        n_outer = len(monitor.objs)
        color = discrete_color(n_outer, dict_colors[method])
        ax.scatter(
            monitor.alphas[:, 0] / alpha_max,
            monitor.alphas[:, 1] / alpha_max,
            s=50, color=color,
            marker="X", label="$1$st order", clip_on=False)
ax.set_xlim(X.min(), X.max())
ax.set_xlabel("L1 regularization")
ax.set_ylabel("L2 regularization")
ax.set_ylim(Y.min(), Y.max())
ax.set_title("Elastic net held out prediction loss on test set")
cb = fig.colorbar(cp)
cb.set_label("Held-out loss")
plt.xscale('log')
plt.yscale('log')
plt.legend()
plt.show(block=False)

File Path: expes/expe_fig_cross_val/figure_cross_val.py
Content:
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sparse_ho.utils_plot import configure_plt, discrete_cmap


save_fig = True
# save_fig = False

configure_plt()
fontsize = 18
current_palette = sns.color_palette("colorblind")


algorithms = ['grid_search10', 'random', 'bayesian', 'grad_search']

dict_title = {}
dict_title['grid_search10'] = 'Grid-search'
dict_title['random'] = 'Random-search'
dict_title['bayesian'] = 'Bayesian'
dict_title['grad_search'] = '1st order method'

plt.close('all')
fig, axarr = plt.subplots(
    1, len(algorithms), sharex=True, sharey=True,
    figsize=[10.67, 3])

objs_full = np.load("results/objs_grid_search100.npy", allow_pickle=True)
log_alphas_full = np.load(
    "results/log_alphas_grid_search100.npy", allow_pickle=True)

cmap = discrete_cmap(10, 'Reds')
c = np.linspace(1, 10, 10)

for i, algorithm in enumerate(algorithms):
    objs = np.load("results/objs_%s.npy" % algorithm, allow_pickle=True)
    log_alphas = np.load(
        "results/log_alphas_%s.npy" % algorithm, allow_pickle=True)

    axarr[i].plot(
        log_alphas_full, objs_full / objs_full[0], color=current_palette[0],
        zorder=1)
    pcm = axarr[i].scatter(
        log_alphas, objs / objs_full[0], c=c, cmap=cmap, marker='x', zorder=10)
    axarr[i].scatter(
        log_alphas, np.zeros(len(log_alphas)), c=c, cmap=cmap, marker='x',
        # zorder=10)
        clip_on=False, zorder=10)

    axarr[i].set_title(dict_title[algorithm])
    axarr[i].set_xlabel("$\lambda - \lambda_{\max}$", fontsize=fontsize)
    axarr[i].set_ylim((0, 1))
    print(objs.min())

axarr[0].set_ylabel(r"$\mathcal{C}(\beta^{(\lambda)})$", fontsize=fontsize)
cba = fig.colorbar(pcm, ax=axarr[3], ticks=np.linspace(1, 10, 10))
cba.set_label('Iterations', fontsize=fontsize)
fig.tight_layout()

if save_fig:
    fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
    fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"
    fig.savefig(
        fig_dir + "intro_lassoCV.pdf", bbox_inches="tight")
    fig.savefig(
        fig_dir_svg + "intro_lassoCV.svg", bbox_inches="tight")
plt.show(block=False)
fig.show()

File Path: expes/expe_fig_cross_val/figure_cross_val_slides.py
Content:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sparse_ho.utils_plot import configure_plt

configure_plt()

# save_fig = False
# save_fig_grid = True
save_fig_grid = False
# save_fig = True
save_fig_grad = True
# save_fig_grad = False
fig_dir = "../../../CD_SUGAR/tex/slides_qbe_long/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/slides_qbe_long/images/"


current_palette = sns.color_palette("colorblind")

p_alphas_grid = np.load("p_alphas.npy")
objs_grid = np.load("objs.npy")

p_alphas_grad = np.load("p_alphas_grad.npy")
objs_grad = np.load("objs_grad.npy")

fontsize = 23
for i in np.arange(len(objs_grad)+1):
    fig, ax = plt.subplots(1, 1)
    ax.plot(
        p_alphas_grid, objs_grid, color=current_palette[0], linewidth=7.0)
    ax.plot(
        p_alphas_grid[:i], objs_grid[:i], 'bo', label='0-order method',
        color=current_palette[1], markersize=15)
    plt.xscale('log')
    ax.set_xlabel(r'$\lambda / \lambda_{\max}$', fontsize=fontsize)
    plt.ylabel(
        r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$",
        fontsize=fontsize)
    plt.xticks(fontsize=fontsize)
    plt.yticks(fontsize=fontsize)
    ax.set_xlim(p_alphas_grid.min(), p_alphas_grid.max())
    plt.tight_layout()

    if save_fig_grid:
        fig.savefig(
            fig_dir + "grid_search_real_sim_lasso_%i.pdf" % i,
            bbox_inches="tight")
        fig.savefig(
            fig_dir_svg + "grid_search_real_sim_lasso_%i.svg" % i,
            bbox_inches="tight")
    else:
        plt.show(block=False)

color = [
    plt.cm.Reds((i + len(objs_grad) / 3 + 1) / len(objs_grad))
    for i in np.arange(len(objs_grad))]

# for i in np.arange(1):
for i in np.arange(len(objs_grad)+1):
    fig, ax = plt.subplots(1, 1)
    ax.plot(
        p_alphas_grid, objs_grid, color=current_palette[0], linewidth=7.0,
        zorder=1)
    # ax.plot(
    #     p_alphas_grid, objs_grid, 'bo', label='0-order method',
    #     color=current_palette[1], markersize=15)
    ax.scatter(
        p_alphas_grid, objs_grid,
        s=300, color=current_palette[1], marker="o", label="$O$ order",
        zorder=2)
    ax.scatter(
        p_alphas_grad[:i], objs_grad[:i],
        s=700, color=color[:i], marker="X", label="$1$st order",
        zorder=3)
    plt.xscale('log')
    ax.set_xlabel(r'$\lambda / \lambda_{\max}$', fontsize=fontsize)
    plt.ylabel(
        r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$",
        fontsize=fontsize)
    plt.xticks(fontsize=fontsize)
    plt.yticks(fontsize=fontsize)
    ax.set_xlim(p_alphas_grid.min(), p_alphas_grid.max())
    ax.set_ylim(0.17, 1)
    plt.tight_layout()

    if save_fig_grad:
        fig.savefig(
            fig_dir + "grid_grad_search_real_sim_lasso_%i.pdf" % i,
            bbox_inches="tight")
        fig.savefig(
            fig_dir_svg + "grid_grad_search_real_sim_lasso_%i.svg" % i,
            bbox_inches="tight")
    # else:
    plt.show(block=False)

# for i in np.arange(len(objs_grad)+1):
#     fig, ax = plt.subplots(1, 1)
#     ax.plot(
#         p_alphas_grid, objs_grid, color=current_palette[0], linewidth=7.0)
#     ax.plot(
#         p_alphas_grid, objs_grid, 'bo', label='0-order',
#         color=current_palette[1], markersize=15)
#     ax.set_xlim(p_alphas_grid.min(), p_alphas_grid.max())
#     ax.plot(
#         p_alphas_grad[:i], objs_grad[:i], 'bo', label='1st-order',
#         color=current_palette[1], markersize=15)
#     plt.xscale('log')
#     # ax.set_ylim(alpha_2.min()/alpha_max, alpha_2.max()/alpha_max)
#     # plt.xscale('log')
#     # plt.yscale('log')
#     # fig.legend(loc=2, ncol=2, fontsize=fontsize, bbox_to_anchor=(0.125, 1))
#     # ax.set_xlabel(r'$\lambda_1 / \lambda_\max$', fontsize=fontsize)
#     # ax.set_ylabel(r'$\lambda_2 / \lambda_\max$', fontsize=fontsize)
#     # # ax.set_xticklabels(fontsize=fontsize)
#     # plt.xticks(fontsize=fontsize)
#     # plt.yticks(fontsize=fontsize)
#     # ax.set_xlim(p_alphas.min(), p_alphas.max())
#     # ax.set_ylim(ymin, ymax)
#     # plt.xlabel(r"$\lambda / \lambda_{\max}$", fontsize=28)
#     # plt.ylabel(
#     #     r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$",
#     #     fontsize=28)
#     # plt.tick_params(width=5)
#     # plt.legend(fontsize=17, loc=2)
#     # plt.tight_layout()

#     if save_fig_grad:
#         fig.savefig(
#             fig_dir + "grad_grid_search_real_sim_enet_%i.pdf" % i,
#             bbox_inches="tight")
#         fig.savefig(
#             fig_dir_svg + "grad_grid_search_real_sim_enet_%i.svg" % i,
#             bbox_inches="tight")
#     else:
#         plt.show(block=False)


# plt.ylabel(
#     r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$",
#     fontsize=28)
# plt.tick_params(width=5)
# plt.legend(fontsize=28)
# plt.tight_layout()

# if save_fig:
#     fig.savefig(
#         fig_dir + "cross_val_real_sim.pdf", bbox_inches="tight")
#     fig.savefig(
#         fig_dir_svg + "cross_val_real_sim.svg", bbox_inches="tight")
# fig.show()

# plt.show(block=False)


# plot fig with 0 order and 1rst order methods

# fig = plt.figure()
# plt.semilogx(
#     p_alphas, objs, color=current_palette[0], linewidth=7.0)
# plt.semilogx(
#     p_alphas, objs, 'bo', label='0-order (grid-search)',
#     color=current_palette[1], markersize=15)
# plt.semilogx(
#     p_alphas_grad, objs_grad, 'bX', label='1-st order',
#     color=current_palette[2], markersize=25)
# plt.xlabel(r"$\lambda / \lambda_{\max}$", fontsize=28)
# plt.ylabel(
#     r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$",
#     fontsize=28)
# plt.tick_params(width=5)
# plt.legend(fontsize=28)
# plt.tight_layout()

# if save_fig:
#     fig.savefig(
#         fig_dir + "cross_val_and_grad_search_real_sim.pdf", bbox_inches="tight")
#     fig.savefig(
#         fig_dir + "cross_val_and_grad_search_real_sim.png", bbox_inches="tight")
#     fig.savefig(
#         fig_dir_svg + "cross_val_and_grad_search_real_sim.svg", bbox_inches="tight")
# fig.show()

# plt.show(block=False)

File Path: expes/expe_fig_cross_val/figure_crossval_enet.py
Content:
import numpy as np
import matplotlib.pyplot as plt

from sparse_ho.utils_plot import configure_plt, round_down, discrete_cmap

# save_fig = True
save_fig = False

configure_plt()
fontsize = 18

algorithms = [
    'grid_search10', 'random', 'bayesian', 'grad_search']

dict_title = {}
dict_title['grid_search10'] = 'Grid-search'
dict_title['random'] = 'Random-search'
dict_title['bayesian'] = 'Bayesian'
dict_title['grad_search'] = '1st order method'
dict_title['grad_search_ls'] = '1st order method + LS'

dataset = "rcv1_train"
# dataset = "real-sim"

objs_full = np.load(
    "results/%s_objs_grid_search100_enet.npy" % dataset, allow_pickle=True)
log_alphas_full = np.load(
    "results/%s_log_alphas_grid_search100_enet.npy" % dataset,
    allow_pickle=True)
X = log_alphas_full[:, 0].reshape(30, 30)
log_alphas_full = X[:, 0]
log_alpha_min = log_alphas_full.min()
log_alpha_max = log_alphas_full.max()
X, Y = np.meshgrid(log_alphas_full, log_alphas_full)
Z = objs_full.reshape(30, 30)

fig, axarr = plt.subplots(
    1, len(algorithms), sharex=True, sharey=True,
    figsize=[10.67, 3], constrained_layout=True)

min_grid = Z.min()
for algorithm in algorithms:
    objs = np.load(
        "results/%s_objs_%s_enet.npy" % (dataset, algorithm),
        allow_pickle=True)
    min_grid = min(min_grid, objs.min())
min_grid = min(min_grid, objs_full.min())

levels = np.geomspace(min_grid, objs_full.max(), num=40)
levels = round_down(levels, 2)

plt.figure()
plt.contourf(X, Y, Z)
plt.plot()

for i, algorithm in enumerate(algorithms):
    objs = np.load(
        "results/%s_objs_%s_enet.npy" % (dataset, algorithm),
        allow_pickle=True)
    log_alphas = np.load(
        "results/%s_log_alphas_%s_enet.npy" % (dataset, algorithm),
        allow_pickle=True)
    assert objs.min() >= min_grid
    cmap = discrete_cmap(len(objs), 'Reds')
    c = np.linspace(1, len(objs), len(objs))
    # cs = axarr[i].contourf(X, Y, Z.T, levels=levels, cmap='binary')
    cs = axarr[i].contourf(X, Y, Z.T, levels=levels, cmap='viridis')
    pcm = axarr[i].scatter(
        log_alphas[:, 0], log_alphas[:, 1], c=c, marker='x', cmap=cmap,
        clip_on=False)

    axarr[i].set_title(dict_title[algorithm])
    axarr[i].set_xlabel("$\lambda_1 - \lambda_{\max}$", fontsize=fontsize)
    print(objs.min())
    axarr[i].set_xlim([log_alpha_min, log_alpha_max])
    axarr[i].set_ylim([log_alpha_min, log_alpha_max])
    axarr[i].set_aspect('equal', adjustable='box')

cba = fig.colorbar(pcm, ax=axarr[3], ticks=[1, 5, 10, 15, 20, 25], shrink=0.6)
cba.set_label('Iterations', fontsize=fontsize)
cba2 = fig.colorbar(cs, ax=axarr[0], location='left', shrink=0.6)
cba2.set_label(r"$\mathcal{C}(\beta^{(\lambda)})$", fontsize=fontsize)

axarr[0].set_ylabel(r"$\mathcal{C}(\beta^{(\lambda)})$", fontsize=fontsize)
axarr[0].set_ylabel("$\lambda_2 - \lambda_{\max}$", fontsize=fontsize)

if save_fig:
    fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
    fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"
    fig.savefig(
        fig_dir + "%s_intro_enetCV.pdf" % dataset, bbox_inches="tight")
    fig.savefig(
        fig_dir_svg + "%s_intro_enetCV.svg" % dataset, bbox_inches="tight")

fig.show()

File Path: expes/expe_fig_cross_val/main_crossval.py
Content:
# Experiment for Lasso CV
# License: BSD (3-clause)

import numpy as np
import celer

from libsvmdata import fetch_libsvm
from sklearn.datasets import make_regression
from celer import LassoCV
from sklearn.model_selection import KFold

from sparse_ho import ImplicitForward
from sparse_ho import grad_search, hyperopt_wrapper
from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE, CrossVal
from sparse_ho.optimizers import GradientDescent
from sparse_ho.utils import Monitor

print(__doc__)

dataset = 'real-sim'
# dataset = 'rcv1_train'

if dataset != 'simu':
    X, y = fetch_libsvm(dataset)
    y -= np.mean(y)
    y /= np.std(y)
else:
    X, y = make_regression(
        n_samples=500, n_features=1000, noise=40, random_state=42)

kf = KFold(n_splits=5, shuffle=True, random_state=42)

n_samples = len(y)
alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples


tol = 1e-3
max_iter = 100_000

algorithms = ['bayesian']
# ['grid_search100', 'grid_search10', 'grad_search', 'random', 'bayesian']

p_alpha_min = 1 / 10_000
print("Starting path computation...")
for algorithm in algorithms:
    estimator = celer.Lasso(
        fit_intercept=False, max_iter=50, warm_start=True, tol=tol,
        verbose=True)

    print('%s started' % algorithm)

    model = Lasso(estimator=estimator)
    criterion = HeldOutMSE(None, None)
    alpha0 = alpha_max / 10
    monitor = Monitor()
    cross_val_criterion = CrossVal(criterion, cv=kf)
    algo = ImplicitForward()
    optimizer = GradientDescent(
        n_outer=10, tol=tol, verbose=True, p_grad_norm=1)
    # optimizer = LineSearch(n_outer=10, tol=tol, verbose=True)
    if algorithm == 'grad_search':
        grad_search(
            algo, cross_val_criterion, model, optimizer, X, y, alpha0,
            monitor)
        objs = np.array(monitor.objs)
        log_alphas = np.log(np.array(monitor.alphas) / alpha_max)

    elif algorithm.startswith('grid_search'):
        if algorithm == 'grid_search10':
            n_alphas = 10
        else:
            n_alphas = 100
        p_alphas = np.geomspace(1, p_alpha_min, n_alphas)
        alphas = alpha_max * p_alphas
        reg = LassoCV(
            cv=kf, verbose=True, tol=tol, fit_intercept=False,
            alphas=alphas, max_iter=max_iter).fit(X, y)
        reg.score(X, y)
        objs = reg.mse_path_.mean(axis=1)
        log_alphas = np.log(alphas / alpha_max)
    else:
        hyperopt_wrapper(
            algo, cross_val_criterion, model, X, y,
            alpha_max * p_alpha_min,
            alpha_max, monitor, max_evals=10,
            method=algorithm, size_space=1, tol=tol, random_state=4)
        objs = np.array(monitor.objs)
        log_alphas = np.log(np.array(monitor.alphas) / alpha_max)
    np.save("results/log_alphas_%s" % algorithm, log_alphas)
    np.save("results/objs_%s" % algorithm, objs)
    print('%s finished' % algorithm)

File Path: expes/expe_fig_cross_val/main_crossval_elastic.py
Content:
# Experiment for elastic net CV
# License: BSD (3-clause)

import itertools
import numpy as np
import celer

from libsvmdata import fetch_libsvm
from sklearn.datasets import make_regression
from sklearn.model_selection import KFold

from sparse_ho import ImplicitForward
from sparse_ho import grad_search, hyperopt_wrapper
from sparse_ho.models import ElasticNet
from sparse_ho.criterion import HeldOutMSE, CrossVal
from sparse_ho.optimizers import LineSearch, GradientDescent
from sparse_ho.utils import Monitor
from sparse_ho.grid_search import grid_search


print(__doc__)

dataset = 'rcv1_train'
# dataset = 'simu'

if dataset != 'simu':
    X, y = fetch_libsvm(dataset)
    y -= np.mean(y)
else:
    X, y = make_regression(
        n_samples=500, n_features=1000, noise=40, random_state=42)

kf = KFold(n_splits=5, shuffle=True, random_state=42)

n_samples = len(y)
alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples
log_alpha_max = np.log(alpha_max)
p_alpha_min = alpha_max / 10_000
log_alpha_min = np.log(p_alpha_min * alpha_max)

tol = 1e-8

algorithms = ['grad_search']

max_evals = 25
print("Starting path computation...")
for algorithm in algorithms:
    estimator = celer.ElasticNet(
        fit_intercept=False, max_iter=50, warm_start=True, tol=tol)

    print('%s started' % algorithm)

    model = ElasticNet(estimator=estimator)
    criterion = HeldOutMSE(None, None)
    log_alpha0 = np.array([np.log(alpha_max / 10), np.log(alpha_max / 10)])
    monitor = Monitor()
    cross_val_criterion = CrossVal(criterion, cv=kf)
    algo = ImplicitForward()
    # optimizer = LineSearch(n_outer=10, tol=tol, verbose=True)
    if algorithm.startswith('grad_search'):
        if algorithm == 'grad_search':
            optimizer = GradientDescent(
                n_outer=max_evals, tol=tol, verbose=True, p_grad_norm=1.9)
        else:
            optimizer = LineSearch(n_outer=25, verbose=True, tol=tol)
        grad_search(
            algo, cross_val_criterion, model, optimizer, X, y, log_alpha0,
            monitor)

    elif algorithm.startswith('grid_search'):
        if algorithm == 'grid_search10':
            n_alphas = 5
        else:
            n_alphas = 30
        p_alphas = np.geomspace(1, p_alpha_min, n_alphas)
        alphas = alpha_max * p_alphas
        log_alphas = np.log(alphas)
        grid_alphas = [i for i in itertools.product(log_alphas, log_alphas)]

        grid_search(
            algo, cross_val_criterion, model, X, y, None, None, monitor,
            log_alphas=grid_alphas)
    else:
        hyperopt_wrapper(
            algo, cross_val_criterion, model, X, y, log_alpha_min,
            log_alpha_max, monitor, max_evals=max_evals,
            method=algorithm, size_space=2)

    objs = np.array(monitor.objs)
    log_alphas = np.array(monitor.log_alphas)
    log_alphas -= np.log(alpha_max)
    np.save("results/%s_log_alphas_%s_enet" % (dataset, algorithm), log_alphas)
    np.save("results/%s_objs_%s_enet" % (dataset, algorithm), objs)
    print('%s finished' % algorithm)

File Path: expes/expe_lasso/figure_lasso_pred.py
Content:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sparse_ho.utils_plot import (
    discrete_color, dict_color, dict_color_2Dplot, dict_markers,
    dict_method, dict_title, dict_n_features, configure_plt)

save_fig = True
# save_fig = False
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"

configure_plt()
fontsize = 18


dict_markevery = {}
dict_markevery["news20"] = 1
dict_markevery["finance"] = 10
dict_markevery["rcv1_train"] = 1
dict_markevery["real-sim"] = 1
dict_markevery["leukemia"] = 10

dict_marker_size = {}
dict_marker_size["forward"] = 4
dict_marker_size["implicit_forward"] = 5
dict_marker_size["fast_iterdiff"] = 4
dict_marker_size['implicit'] = 4
dict_marker_size['grid_search'] = 1
dict_marker_size['bayesian'] = 10
dict_marker_size['random'] = 5
dict_marker_size['lhs'] = 4

dict_s = {}
dict_s["implicit_forward"] = 100
dict_s["implicit_forward_approx"] = 60
dict_s['grid_search'] = 60
dict_s['bayesian'] = 60
dict_s['random'] = 5
dict_s['lhs'] = 4

dict_xmax = {}
dict_xmax["logreg", "rcv1_train"] = 20
dict_xmax["logreg", "real-sim"] = 30
dict_xmax["logreg", "leukemia"] = 5
dict_xmax["logreg", "news20"] = None

dict_xmax["lasso", "rcv1_train"] = 60
dict_xmax["lasso", "real-sim"] = 200
dict_xmax["lasso", "leukemia"] = 5
dict_xmax["lasso", "news20"] = 1200

dict_xticks = {}
dict_xticks["lasso", "rcv1_train"] = (-6, -4, -2, 0)
dict_xticks["lasso", "real-sim"] = (-6, -4, -2, 0)
dict_xticks["lasso", "leukemia"] = (-6, -4, -2, 0)
dict_xticks["lasso", "news20"] = (-8, -6, -4, -2, 0)

dict_xticks["logreg", "rcv1"] = (-8, -6, -4, -2, 0)
dict_xticks["logreg", "real-sim"] = (-8, -6, -4, -2, 0)
dict_xticks["logreg", "leukemia"] = (-8, -6, -4, -2, 0)
dict_xticks["logreg", "news20"] = (-8, -6, -4, -2, 0)

markersize = 8

dataset_names = ["rcv1_train", "real-sim", "news20"]


plt.close('all')
fig_val, axarr_val = plt.subplots(
    1, len(dataset_names), sharex=False, sharey=True, figsize=[10.67, 2.5],)

fig_grad, axarr_grad = plt.subplots(
    3, len(dataset_names), sharex='col', sharey=True, figsize=[10.67, 6],
    )

model_name = "lasso"

for idx, dataset in enumerate(dataset_names):
    df_data = pd.read_pickle("results/%s_%s.pkl" % (model_name, dataset))
    df_data = df_data[df_data['tolerance_decrease'] == 'constant']

    methods = df_data['method']
    times = df_data['times']
    objs = df_data['objs']
    alphas = df_data['alphas']
    alpha_max = df_data['alpha_max'].to_numpy()[0]
    log_alpha_max = np.log(alpha_max)
    tols = df_data['tolerance_decrease']

    min_objs = np.infty
    for obj in objs:
        min_objs = min(min_objs, obj.min())

    lines = []

    E0 = df_data.objs[2][0]
    for _, (time, obj, alpha, method, _) in enumerate(
            zip(times, objs, alphas, methods, tols)):
        log_alpha = np.log(alpha)
        if method == 'grid_search':
            for i in range(3):
                axarr_grad[i, idx].plot(
                    np.array(log_alpha) - log_alpha_max, obj / E0,
                    color='grey', zorder=-10)

    for _, (time, obj, alpha, method, _) in enumerate(
            zip(times, objs, alphas, methods, tols)):
        log_alpha = np.log(alpha)
        marker = dict_markers[method]
        n_outer = len(obj)
        s = dict_s[method]
        color = discrete_color(n_outer, dict_color_2Dplot[method])
        if method == 'grid_search':
            i = 0
        elif method == 'bayesian':
            i = 1
        elif method == 'implicit_forward_approx':
            i = 2
        else:
            continue
        axarr_grad[i, idx].scatter(
            np.array(log_alpha) - log_alpha_max, obj / E0,
            s=s, color=color,
            marker=marker, label="todo", clip_on=False)
        if method != 'random':
            axarr_grad[i, 0].set_ylabel(
                "%s \n" % dict_method[method] + "CV loss",
                fontsize=fontsize)

    # plot for objective minus optimum on validation set
    for _, (time, obj, method, _) in enumerate(
            zip(times, objs, methods, tols)):
        marker = dict_markers[method]
        obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]
        lines.append(
            axarr_val.flat[idx].plot(
                time, obj / E0,
                color=dict_color[method], label="%s" % (dict_method[method]),
                marker=marker, markersize=markersize,
                markevery=dict_markevery[dataset]))
    axarr_val.flat[idx].set_xlim(0, dict_xmax[model_name, dataset])
    axarr_val.flat[idx].set_ylim(0.15, 0.4)
    axarr_val.flat[idx].set_xlabel("Time (s)", fontsize=fontsize)

    axarr_grad[idx, 0].set_ylim(0.15, 1)

    axarr_grad.flat[idx].set_title("%s %s" % (
        dict_title[dataset], dict_n_features[dataset]), size=fontsize)


for j in range(len(dataset_names)):
    axarr_grad[2, j].set_xlabel(
        r"$\lambda - \lambda_{\max}$", fontsize=fontsize)
axarr_val.flat[0].set_ylabel(
    "Cross-validation \n loss", fontsize=fontsize)

axarr_grad.flat[0].set_yticks([0.2, 0.4, 0.6, 0.8, 1])

fig_val.tight_layout()
fig_grad.tight_layout()
if save_fig:
    fig_val.savefig(
        fig_dir + "%s_val.pdf" % model_name)
    fig_val.savefig(
        fig_dir_svg + "%s_val.svg" % model_name)
    fig_grad.savefig(
        fig_dir + "%s_val_grad.pdf" % model_name)
    fig_grad.savefig(
        fig_dir_svg + "%s_lasso_val_grad.svg" % model_name,
        bbox_inches="tight")


fig_val.show()
fig_grad.show()


#################################################################
# plot legend
labels = []
for method in methods:
    labels.append(dict_method[method])

fig_legend = plt.figure(figsize=[9, 2])
fig_legend.legend(
    [l[0] for l in lines], labels,
    ncol=5, loc='upper center', fontsize=fontsize - 4)
fig_legend.tight_layout()
if save_fig:
    fig_legend.savefig(
        fig_dir + "lasso_pred_legend.pdf", bbox_inches="tight")
fig_legend.show()

File Path: expes/expe_lasso/main_lasso_pred.py
Content:
"""
This is the file to reproduce the experiments of Figure 2:
'Computation time for the HO of the Lasso on real data.'
It is recommended to run this script on a cluster with several CPUs.
"""

import numpy as np
from joblib import Parallel, delayed, parallel_backend
from itertools import product
import pandas as pd
import celer

from sklearn.model_selection import KFold

from libsvmdata import fetch_libsvm

from sparse_ho.models import Lasso, SparseLogreg
from sparse_ho.criterion import HeldOutMSE, HeldOutLogistic, CrossVal
from sparse_ho.utils import Monitor
from sparse_ho.optimizers import GradientDescent

from sparse_ho import ImplicitForward
from sparse_ho.grid_search import grid_search
from sparse_ho.ho import hyperopt_wrapper

from sparse_ho.ho import grad_search

model_name = "lasso"

dict_t_max = {}
dict_t_max["rcv1_train"] = 300
dict_t_max["real-sim"] = 1800
dict_t_max["leukemia"] = 10
dict_t_max["news20"] = 10_000

#######################################################################
# dataset_names = ["rcv1_train"]
# uncomment the following line to launch the experiments on other
dataset_names = ["real-sim"]
methods = [
    "implicit_forward", "implicit_forward_approx", 'grid_search',
    'random', 'bayesian']
tolerance_decreases = ["constant"]
tol = 1e-8
n_outers = [75]

dict_n_outers = {}
dict_n_outers["news20", "implicit_forward"] = 50
dict_n_outers["news20", "forward"] = 60
dict_n_outers["news20", "implicit"] = 6
dict_n_outers["news20", "bayesian"] = 75
dict_n_outers["news20", "random"] = 35

dict_n_outers["finance", "implicit_forward"] = 125
dict_n_outers["finance", "forward"] = 75
dict_n_outers["finance", "implicit"] = 6
dict_n_outers["finance", "bayesian"] = 75
dict_n_outers["finance", "random"] = 50

#######################################################################
# n_jobs = 1
n_jobs = len(dataset_names) * len(methods) * len(tolerance_decreases)
n_jobs = min(n_jobs, 10)
#######################################################################


def parallel_function(
        dataset_name, method, tol=1e-5, n_outer=50,
        tolerance_decrease='constant'):

    # load data
    X, y = fetch_libsvm(dataset_name)
    y -= y.mean()
    # compute alpha_max
    alpha_max = np.abs(X.T @ y).max() / len(y)

    if model_name == "logreg":
        alpha_max /= 2
    alpha_min = alpha_max / 10_000

    if model_name == "lasso":
        estimator = celer.Lasso(
            fit_intercept=False, max_iter=50, warm_start=True, tol=tol)
        model = Lasso(estimator=estimator)
    elif model_name == "logreg":
        model = SparseLogreg(estimator=estimator)

    # TODO improve this
    try:
        n_outer = dict_n_outers[dataset_name, method]
    except Exception:
        n_outer = 20

    size_loop = 2

    for _ in range(size_loop):
        if model_name == "lasso":
            sub_criterion = HeldOutMSE(None, None)
        elif model_name == "logreg":
            criterion = HeldOutLogistic(None, None)
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        criterion = CrossVal(sub_criterion, cv=kf)

        algo = ImplicitForward(tol_jac=1e-3)
        monitor = Monitor()
        t_max = dict_t_max[dataset_name]
        if method == 'grid_search':
            grid_search(
                algo, criterion, model, X, y, alpha_min, alpha_max,
                monitor, max_evals=100, tol=tol)
        elif method == 'random' or method == 'bayesian':
            hyperopt_wrapper(
                algo, criterion, model, X, y, alpha_min, alpha_max,
                monitor, max_evals=30, tol=tol, method=method, size_space=1,
                t_max=t_max)
        elif method.startswith("implicit_forward"):
            # do gradient descent to find the optimal lambda
            alpha0 = alpha_max / 100
            n_outer = 30
            if method == 'implicit_forward':
                optimizer = GradientDescent(
                    n_outer=n_outer, p_grad_norm=1, verbose=True, tol=tol,
                    t_max=t_max)
            else:
                optimizer = GradientDescent(
                    n_outer=n_outer, p_grad_norm=1, verbose=True, tol=tol,
                    t_max=t_max,
                    tol_decrease="geom")
            grad_search(
                algo, criterion, model, optimizer, X, y, alpha0,
                monitor)
        else:
            raise NotImplementedError

    monitor.times = np.array(monitor.times)
    monitor.objs = np.array(monitor.objs)
    monitor.objs_test = 0  # TODO
    monitor.alphas = np.array(monitor.alphas)
    return (dataset_name, method, tol, n_outer, tolerance_decrease,
            monitor.times, monitor.objs, monitor.objs_test,
            monitor.alphas, alpha_max,
            model_name)


print("enter sequential")

with parallel_backend("loky", inner_max_num_threads=1):
    results = Parallel(n_jobs=n_jobs, verbose=100)(
        delayed(parallel_function)(
            dataset_name, method, n_outer=n_outer,
            tolerance_decrease=tolerance_decrease, tol=tol)
        for dataset_name, method, n_outer,
        tolerance_decrease in product(
            dataset_names, methods, n_outers, tolerance_decreases))
    print('OK finished parallel')

df = pd.DataFrame(results)
df.columns = [
    'dataset', 'method', 'tol', 'n_outer', 'tolerance_decrease',
    'times', 'objs', 'objs_test', 'alphas', 'alpha_max', 'model_name']

for dataset_name in dataset_names:
    df[df['dataset'] == dataset_name].to_pickle(
        "results/%s_%s.pkl" % (model_name, dataset_name))

File Path: expes/expe_lasso/plot_lasso_pred.py
Content:
"""
=============================
Expe Lasso
=============================

File to play with expes for the Lasso
"""

import numpy as np
import matplotlib.pyplot as plt
from numpy.linalg import norm

from sklearn.datasets import make_regression
from sklearn.model_selection import KFold
import celer
from libsvmdata import fetch_libsvm

from sparse_ho import ImplicitForward
from sparse_ho import grad_search, hyperopt_wrapper
from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE, CrossVal
from sparse_ho.optimizers import GradientDescent
from sparse_ho.utils import Monitor
from sparse_ho.utils_plot import configure_plt
from sparse_ho.grid_search import grid_search

configure_plt()

dataset = 'rcv1_train'
# dataset = 'simu'

if dataset != 'simu':
    X, y = fetch_libsvm(dataset)
    y -= y.mean()
    y /= norm(y)
else:
    X, y = make_regression(
        n_samples=500, n_features=1000, noise=40,
        random_state=42)

n_samples = len(y)
alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples
alpha_min = alpha_max / 10_000


tol = 1e-8

estimator = celer.Lasso(
    fit_intercept=False, max_iter=50, warm_start=True, tol=tol)


dict_monitor = {}

all_algo_name = ['implicit_forward', 'grid_search']

for algo_name in all_algo_name:
    model = Lasso(estimator=estimator)
    sub_criterion = HeldOutMSE(None, None)
    alpha0 = alpha_max / 10
    monitor = Monitor()
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    criterion = CrossVal(sub_criterion, cv=kf)
    algo = ImplicitForward(tol_jac=1e-3)
    optimizer = GradientDescent(
        n_outer=30, p_grad_norm=1., verbose=True, tol=tol)
    if algo_name == 'implicit_forward':
        grad_search(
            algo, criterion, model, optimizer, X, y, alpha0,
            monitor)
    elif algo_name == 'grid_search':
        grid_search(
            algo, criterion, model, X, y, alpha_min, alpha_max,
            monitor, max_evals=20, tol=tol)
    elif algo_name == 'random_search':
        hyperopt_wrapper(
            algo, criterion, model, X, y, alpha_min, alpha_max,
            monitor, max_evals=20, tol=tol, method='random', size_space=1)
    dict_monitor[algo_name] = monitor


min_objs = np.infty
for monitor in dict_monitor.values():
    monitor.objs = np.array(monitor.objs)
    min_objs = min(min_objs, monitor.objs.min())

scaling_factor = (y @ y) / len(y)
plt.figure()
for monitor in dict_monitor.values():
    plt.plot(monitor.times, monitor.objs / scaling_factor)
plt.xlabel('Time (s)')
plt.ylabel('Objective')
plt.show(block=False)

plt.figure()
for monitor in dict_monitor.values():
    plt.semilogy(monitor.times, (monitor.objs - min_objs) / scaling_factor)
plt.xlabel('Time (s)')
plt.ylabel('Objective - optimum')
plt.show(block=False)

plt.figure()
monitor_grid = dict_monitor['grid_search']
plt.semilogx(
    monitor_grid.alphas / alpha_max,
    monitor_grid.objs / scaling_factor)
for monitor in dict_monitor.values():
    plt.scatter(
        monitor.alphas / alpha_max, monitor.objs / scaling_factor, marker='X')
plt.xlabel('alpha')
plt.ylabel('Objective')
plt.show(block=False)

File Path: expes/expe_linear_convergence/expe_trash_svm.py
Content:


import numpy as np
import matplotlib.pyplot as plt

# from scipy.sparse.linalg import norm

from sparse_ho.models import SparseLogreg
from sparse_ho.criterion import HeldOutLogistic
from sparse_ho.forward import Forward
from sparse_ho.utils import Monitor
from sparse_ho.datasets.real import get_real_sim
# from sparse_ho.datasets.real import get_rcv1
# from sparse_ho.datasets.real import get_leukemia
from sparse_ho.grid_search import grid_search
# from sparse_ho.ho import grad_search
X_train, X_val, X_test, y_train, y_val, y_test = get_real_sim(csr=False)

# X_train, X_val, X_test, y_train, y_val, y_test = get_leukemia()
n_samples, n_features = X_train.shape

print("Starting path computation...")

alpha_max = np.max(np.abs(X_train.T @ (- y_train)))
alpha_max /= (2 * n_samples)
n_alphas = 10
p_alphas = np.geomspace(1, 1e-4, n_alphas)
alphas = p_alphas * alpha_max
log_alphas = np.log(alphas)
tol = 1e-5

# grid search
model = SparseLogreg(X_train, y_train, log_alphas[0], max_iter=1000)
criterion = HeldOutLogistic(X_val, y_val, model, X_test=X_test, y_test=y_test)
algo = Forward(criterion)
monitor_grid_sk = Monitor()
grid_search(
    algo, None, None, monitor_grid_sk, log_alphas=log_alphas,
    tol=tol)
monitor = Monitor()
# grad_search(
#     algo, logCs[0], monitor, n_outer=5, verbose=True,
#     tolerance_decrease='constant', tol=1e-8,
#     t_max=10000)

plt.figure()
plt.plot(monitor_grid_sk.log_alphas, monitor_grid_sk.objs)
plt.plot(monitor.log_alphas, monitor.objs, 'bo')
plt.show(block=False)

File Path: expes/expe_linear_convergence/figure_linear_conv_jacobian.py
Content:
import pandas
import matplotlib.pyplot as plt

from sparse_ho.utils_plot import configure_plt


# save_fig = False
save_fig = True
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"


configure_plt()

fontsize = 18

dataset_names = ["leu", "rcv1_train", "news20", "real-sim"]
model_names = ["lasso", "logreg", "svm"]


dict_title = {}
dict_title["rcv1_train"] = "rcv1"
dict_title["news20"] = "20news"
dict_title["finance"] = "finance"
dict_title["kdda_train"] = "kdda"
dict_title["climate"] = "climate"
dict_title["leu"] = "Leukemia"
dict_title["real-sim"] = "real-sim"

for model_name in model_names:
    fig, axarr = plt.subplots(
        2, 4, sharex=False, sharey=False, figsize=[10.67, 3.5],)
    for idx, dataset in enumerate(dataset_names):
        df_data = pandas.read_pickle(
            "%s_%s.pkl" % (dataset, model_name))
        diff_beta = df_data["diff_beta"].to_numpy()[0]
        diff_jac = df_data["diff_jac"].to_numpy()[0]
        supp_id = df_data["supp_id"].to_numpy()[0]
        #
        axarr.flat[idx].semilogy(diff_beta)
        axarr.flat[idx].axvline(x=supp_id, c='red', linestyle="--")

        axarr.flat[idx+4].semilogy(diff_jac)
        axarr.flat[idx+4].axvline(x=supp_id, c='red', linestyle="--")

        axarr.flat[idx+4].set_xlabel(r"$\#$ epochs", fontsize=fontsize)

        axarr.flat[idx].set_title("%s" % (
            dict_title[dataset]), size=fontsize)

    axarr.flat[0].set_ylabel(
        r"$\|\beta^{(k)} - \hat \beta\|$", fontsize=fontsize)
    axarr.flat[4].set_ylabel(
        r"$\|\mathcal{J}^{(k)} - \hat \mathcal{J}\|$", fontsize=fontsize)

    fig.tight_layout()

    if save_fig:
        fig.savefig(
            fig_dir + "linear_convergence_%s.pdf" % model_name,
            bbox_inches="tight")
        fig.savefig(
            fig_dir_svg + "linear_convergence_%s.svg" % model_name,
            bbox_inches="tight")
    fig.show()

File Path: expes/expe_linear_convergence/main_clean.py
Content:
import numpy as np
from numpy.linalg import norm
from joblib import Parallel, delayed
import pandas
from itertools import product
import blitzl1
# from sklearn.linear_model import Lasso as Lasso_sk
# from sklearn.linear_model import LogisticRegression
# from celer import LogisticRegression
from scipy.sparse.linalg import cg
from scipy.sparse import csc_matrix
from sparse_ho.models import Lasso, SparseLogreg
# , SparseLogreg
# import matplotlib.pyplot as plt

from celer import Lasso as Lasso_cel
# from sparse_ho.utils import sigma
from sparse_ho.forward import compute_beta
from sparse_ho.datasets.real import load_libsvm

p_alphas = {}
p_alphas["leu", "lasso"] = 0.01
p_alphas["rcv1_train", "lasso"] = 0.075
p_alphas["news20", "lasso"] = 0.3
p_alphas["finance", "lasso"] = 0.3
p_alphas["real-sim", "lasso"] = 0.1

p_alphas["leu", "logreg"] = 0.1
p_alphas["rcv1_train", "logreg"] = 0.25
p_alphas["news20", "logreg"] = 0.8
p_alphas["finance", "logreg"] = 0.3
p_alphas["real-sim", "logreg"] = 0.15

max_iters = {}
max_iters["leu"] = 2000
max_iters["rcv1_train"] = 200
max_iters["news20"] = 100
max_iters["real-sim"] = 100

# dataset_names = [""]
# dataset_names = ["news20"]
#
dataset_names = ["leu"]
# dataset_names = ["rcv1_train"]
# dataset_names = ["leu"]
# dataset_names = ["real-sim"]
# dataset_names = ["finance"]
# dataset_names = ["leu"]
model_names = ["logreg"]
# model_names = ["lasso"]


def linear_cv(
        dataset_name, tol=1e-3, compute_jac=True, model_name="lasso"):

    X, y = load_libsvm(dataset_name)
    X = csc_matrix(X)
    n_samples, n_features = X.shape
    p_alpha = p_alphas[dataset_name, model_name]

    max_iter = max_iters[dataset_name]
    if model_name == "lasso":
        model = Lasso(X, y, 0, max_iter=max_iter, tol=tol)
    elif model_name == "logreg":
        model = SparseLogreg(X, y, 0, max_iter=max_iter, tol=tol)

    alpha_max = np.exp(model.compute_alpha_max())

    alpha = p_alpha * alpha_max
    if model_name == "lasso":
        clf = Lasso_cel(
            alpha=alpha, fit_intercept=False, warm_start=True,
            tol=tol * norm(y) ** 2 / 2, max_iter=10000)
        clf.fit(X, y)
        beta_star = clf.coef_
        mask = beta_star != 0
        dense = beta_star[mask]
    elif model_name == "logreg":
        # clf = LogisticRegression(
        #     penalty='l1', C=(1 / (alpha * n_samples)),
        #     fit_intercept=False,
        #     warm_start=True, max_iter=10000,
        #     tol=tol, verbose=True).fit(X, y)
        # clf = LogisticRegression(
        #     penalty='l1', C=(1 / (alpha * n_samples)),
        #     fit_intercept=False,
        #     warm_start=True, max_iter=10000,
        #     tol=tol, verbose=True,
        #     solver='liblinear').fit(X, y)
        # beta_star = clf.coef_[0]

        blitzl1.set_use_intercept(False)
        blitzl1.set_tolerance(1e-32)
        blitzl1.set_verbose(True)
        # blitzl1.set_min_time(60)
        prob = blitzl1.LogRegProblem(X, y)
        # # lammax = prob.compute_lambda_max()
        clf = prob.solve(alpha * n_samples)
        beta_star = clf.x
        mask = beta_star != 0
        mask = np.array(mask)
        dense = beta_star[mask]
    # if model == "lasso":
    v = - alpha * np.sign(beta_star[mask])
    mat_to_inv = model.get_mat_vec(mask, dense, np.log(alpha))
    # mat_to_inv = X[:, mask].T  @ X[:, mask]

    jac_temp = cg(mat_to_inv, v, tol=1e-10)
    jac_star = np.zeros(n_features)
    jac_star[mask] = jac_temp[0]
    # elif model == "logreg":
    #     v = - n_samples * alpha * np.sign(beta_star[mask])

    log_alpha = np.log(alpha)

    list_beta, list_jac = compute_beta(
        X, y, log_alpha, model, save_iterates=True, tol=tol,
        max_iter=max_iter, compute_jac=compute_jac)

    diff_beta = norm(list_beta - beta_star, axis=1)
    diff_jac = norm(list_jac - jac_star, axis=1)

    supp_star = beta_star != 0
    n_iter = list_beta.shape[0]
    for i in np.arange(n_iter)[::-1]:
        supp = list_beta[i, :] != 0
        if not np.all(supp == supp_star):
            supp_id = i + 1
            break
        supp_id = 0

    return dataset_name, p_alpha, diff_beta, diff_jac, n_iter, supp_id


# parameter of the algo
tol = 1e-16
# max_iter = 100
# max_iter = 10000

# dataset_name = "news20"
# dataset_name = "rcv1_train"
# dataset_name = "leu"
p_alpha = 0.01
# p_alpha = 0.3

# diff_beta, diff_jac, n_iter, supp_id = linear_cv(
# results = linear_cv(
#     dataset_name, p_alpha,
#     max_iter=max_iter, tol=tol, compute_jac=True)

print("enter sequential")
backend = 'loky'
n_jobs = 1
results = Parallel(n_jobs=n_jobs, verbose=100, backend=backend)(
    delayed(linear_cv)(
        dataset_name, tol=tol, compute_jac=True, model_name=model_name)
    for dataset_name, model_name in product(dataset_names, model_names))
print('OK finished parallel')

df = pandas.DataFrame(results)

df.columns = [
    'dataset_name', 'p_alpha', 'diff_beta', 'diff_jac', 'n_iter',
    'supp_id']

for dataset_name in dataset_names:
    for model_name in model_names:
        df[df['dataset_name'] == dataset_name].to_pickle(
            "%s_%s.pkl" % (dataset_name, model_name))


# Same for the Logistic regression

# n_samples = 100
# n_features = 1000
# SNR = 3.0
# seed = 10
# tol = 1e-14
# max_iter = 10000

# X_train, y_train = datasets.make_classification(
#     n_samples=n_samples,
#     n_features=n_features, n_informative=50,
#     random_state=110, flip_y=0.1, n_redundant=0)

# y_train[y_train == 0.0] = -1.0

# X_val, y_val = datasets.make_classification(
#     n_samples=n_samples,
#     n_features=n_features, n_informative=50,
#     random_state=122, flip_y=0.1, n_redundant=0)


# alpha_max = norm(X_train.T @ y_train, ord=np.inf) / (2 * n_samples)
# alpha = 0.05 * alpha_max

# clf = LogisticRegression(penalty="l1", tol=1e-12, C=(
#                          1 / (alpha * n_samples)), fit_intercept=False, max_iter=100000,
#                          solver="liblinear")
# clf.fit(X_train, y_train)

# beta_star_logreg = clf.coef_
# supp_sk = beta_star_logreg != 0
# supp_sk = supp_sk[0, :].T
# dense_sk = beta_star_logreg[0, supp_sk].T

# v = - n_samples * alpha * np.sign(dense_sk)
# r = y_train * (X_train[:, supp_sk] @ dense_sk)
# mat_to_inv = X_train[:, supp_sk].T  @ np.diag(sigma(r) * (1 - sigma(r))) @ X_train[:, supp_sk]

# jac_temp = cg(mat_to_inv, v, atol=1e-7)
# jac_star = np.zeros(n_features)
# jac_star[supp_sk] = jac_temp[0]

# model = SparseLogreg(X_train, y_train, np.log(alpha), max_iter=max_iter, tol=tol)
# diff_beta_logreg, diff_jac_logreg, n_iter_logreg, supp_id = linear_cv(X_train, y_train, np.log(alpha), model, clf.coef_, jac_star,
#                                                                       max_iter=max_iter, tol=tol, compute_jac=True)

# fig, axarr = plt.subplots(
#     1, 2, sharex=False, sharey=False, figsize=[10, 4])
# plt.figure()
# axarr.flat[0].semilogy(range(n_iter_logreg+1), diff_beta_logreg, linewidth=2.0)
# axarr.flat[1].semilogy(range(n_iter_logreg+1), diff_jac_logreg, linewidth=2.0)
# axarr.flat[0].set_xlabel("epoch")
# axarr.flat[0].set_ylabel("||beta - beta_star||")
# axarr.flat[1].set_xlabel("epoch")
# axarr.flat[1].set_ylabel("||jac - jac_Star||")
# axarr.flat[0].axvline(x=supp_id[0], c='red', linestyle="--")
# axarr.flat[1].axvline(x=supp_id[0], c='red', linestyle="--")
# axarr.flat[0].set_title("Iterates convergence for the Logistic Regression")
# axarr.flat[1].set_title("Jacobian convergence for the Logistic Regression")
# plt.show(block=False)

File Path: expes/expe_linear_convergence/main_svm.py
Content:
import numpy as np
from numpy.linalg import norm
from joblib import Parallel, delayed
import pandas
from lightning.classification import SDCAClassifier

from scipy.sparse.linalg import cg
from scipy.sparse import csc_matrix
from sparse_ho.models import SVM
# from cvxopt import spmatrix, matrix
# from cvxopt import solvers
from sparse_ho.forward import compute_beta
from sparse_ho.datasets.real import load_libsvm


# dataset_names = ["news20"]
dataset_names = ["rcv1_train"]
# dataset_names = ["real-sim"]
# dataset_names = ["leu"]
# dataset_names = ["finance"]
# dataset_names = ["leu", "rcv1_train", "news20"]
Cs = {}
Cs["leu"] = 1e-5
Cs["rcv1_train"] = 0.03
Cs["news20"] = 0.001
Cs["finance"] = 0.01
Cs["real-sim"] = 0.05

max_iters = {}
max_iters["leu"] = 2000
max_iters["rcv1_train"] = 2000
max_iters["news20"] = 1000
max_iters["real-sim"] = 50


# def scipy_sparse_to_spmatrix(A):
#     coo = A.tocoo()
#     SP = spmatrix(coo.data, coo.row.tolist(), coo.col.tolist())
#     return SP


def linear_cv(dataset_name, max_iter=1000, tol=1e-3, compute_jac=True):
    max_iter = max_iters[dataset_name]
    X, y = load_libsvm(dataset_name)
    X = X.tocsr()
    num_nonzeros = np.diff(X.indptr)
    X = X[num_nonzeros != 0]
    y = y[num_nonzeros != 0]
    n_samples, n_features = X.shape
    C = Cs[dataset_name]
    # Computation of dual solution of SVM via cvxopt

    clf = SDCAClassifier(
        alpha=1/(C * n_samples), loss='hinge', verbose=True, tol=1e-16,
        max_iter=max_iter)
    clf.fit(X, y)
    beta_star = np.abs(clf.dual_coef_[0])
    primal_star = np.sum(X.T.multiply(y * beta_star), axis=1)
    # full_supp = np.logical_and(beta_star > 0, beta_star < C)
    full_supp = np.logical_and(np.logical_not(np.isclose(beta_star, 0)), np.logical_not(np.isclose(beta_star, C)))
    # Q = (X.multiply(y[:, np.newaxis]))  @  (X.multiply(y[:, np.newaxis])).T
    yX = X.multiply(y[:, np.newaxis])
    yX = yX.tocsr()

    # TODO to optimize
    temp3 = np.zeros(n_samples)
    temp3[np.isclose(beta_star, C)] = np.ones(
        (np.isclose(beta_star, C)).sum()) * C
    # temp3 = temp3[full_supp]
    v = temp3[full_supp] - yX[full_supp, :] @ (yX[np.isclose(beta_star, C), :].T @ temp3[np.isclose(beta_star, C)])
    # v = np.array((np.eye(n_samples, n_samples) - Q)[np.ix_(full_supp, np.isclose(beta_star, C))] @ (np.ones((np.isclose(beta_star, C)).sum()) * C))
    # v = np.squeeze(v)
    temp = yX[full_supp, :] @ yX[full_supp, :].T
    temp = csc_matrix(temp)
    # temp = temp[:, full_supp]
    # Q = csc_matrix(Q)
    print("size system to solve %i" % v.shape[0])
    jac_dense = cg(temp, v, tol=1e-12)
    jac_star = np.zeros(n_samples)
    jac_star[full_supp] = jac_dense[0]
    jac_star[np.isclose(beta_star, C)] = C
    primal_jac_star = np.sum(X.T.multiply(y * jac_star), axis=1)
    model = SVM(X, y, np.log(C), max_iter=max_iter, tol=tol)
    list_beta, list_jac = compute_beta(
        X, y, np.log(C), model, save_iterates=True, tol=1e-32,
        max_iter=max_iter, compute_jac=True)

    M = X.T @ (list_beta * y).T
    M_jac = X.T @ (list_jac * y).T
    diff_beta = norm(M - primal_star, axis=0)
    diff_jac = norm(M_jac - primal_jac_star, axis=0)
    full_supp_star = full_supp
    full_supp_star = np.logical_and(np.logical_not(np.isclose(list_beta[-1], 0)), np.logical_not(np.isclose(list_beta[-1], C)))
    n_iter = list_beta.shape[0]
    for i in np.arange(n_iter)[::-1]:
        full_supp = np.logical_and(np.logical_not(np.isclose(list_beta[i, :], 0)), np.logical_not(np.isclose(list_beta[i, :], C)))
        if not np.all(full_supp == full_supp_star):
            supp_id = i + 1
            break
        supp_id = 0
    return dataset_name, C, diff_beta, diff_jac, n_iter, supp_id


# parameter of the algo
tol = 1e-32
max_iter = 10000

print("enter sequential")
backend = 'loky'
n_jobs = 1
results = Parallel(n_jobs=n_jobs, verbose=100, backend=backend)(
    delayed(linear_cv)(
        dataset_name,
        max_iter=max_iter, tol=tol, compute_jac=True)
    for dataset_name in dataset_names)
print('OK finished parallel')

df = pandas.DataFrame(results)

df.columns = [
    'dataset_name', 'p_alpha', 'diff_beta', 'diff_jac', 'n_iter',
    'supp_id']

for dataset_name in dataset_names:
    df[df['dataset_name'] == dataset_name].to_pickle(
        "%s_svm.pkl" % dataset_name)

File Path: expes/expe_linear_convergence/plot_expe_trash_svm.py
Content:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sparse_ho.utils_plot import configure_plt

configure_plt()

save_fig = False
fig_dir = "../../../CD_SUGAR/tex/ICML2020slides/prebuiltimages/"


current_palette = sns.color_palette("colorblind")

p_alphas = np.load("p_alphas.npy")
objs = np.load("objs.npy")

# ax = plt.gca()
# ax.tick_params(width=10)

fig = plt.figure()
plt.semilogx(
    p_alphas, objs, color=current_palette[0], linewidth=7.0)
plt.semilogx(
    p_alphas, objs, 'bo', label='grid-search scikit-learn',
    color=current_palette[1])
plt.xlabel(r"$C$", fontsize=28)
plt.ylabel("validation loss", fontsize=28)
plt.tick_params(width=5)
# plt.legend()
plt.tight_layout()

if save_fig:
    fig.savefig(
        fig_dir + "cross_val_real_sim.pdf", bbox_inches="tight")
fig.show()

plt.show(block=False)

File Path: expes/expe_linear_convergence/plot_slides.py
Content:
# import numpy as np
import pandas
# import seaborn as sns
import matplotlib.pyplot as plt

from sparse_ho.utils_plot import configure_plt

# save_fig = False
save_fig = True
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"

configure_plt()

fontsize = 25

# dataset_names = ["leu", "rcv1_train", "news20"]
# dataset_names = ["leu", "rcv1_train", "news20", "finance"]
dataset_names = ["leu", "rcv1_train", "news20", "real-sim"]

dict_title = {}
dict_title["rcv1_train"] = "rcv1"
dict_title["news20"] = "20news"
dict_title["finance"] = "finance"
dict_title["kdda_train"] = "kdda"
dict_title["climate"] = "climate"
dict_title["leu"] = "Leukemia"
dict_title["real-sim"] = "real-sim"

plt.close('all')

model_names = ["lasso", "logreg", "svm"]


for model_name in model_names:

    fig, axarr = plt.subplots(
        2, 4, sharex=False, sharey=False, figsize=[14, 8],)

    lines = []

    for idx, dataset in enumerate(dataset_names):
        df_data = pandas.read_pickle("%s_%s.pkl" % (dataset, model_name))
        diff_beta = df_data["diff_beta"].to_numpy()[0]
        diff_jac = df_data["diff_jac"].to_numpy()[0]
        supp_id = df_data["supp_id"].to_numpy()[0]
        #
        axarr.flat[idx].semilogy(diff_beta)
        lines.append(axarr.flat[idx].axvline(
            x=supp_id, c='red', linestyle="--", label="Support identification"))

        axarr.flat[idx+4].semilogy(diff_jac)
        axarr.flat[idx+4].axvline(x=supp_id, c='red', linestyle="--")

        axarr.flat[idx+4].set_xlabel(r"$\#$ epochs", size=fontsize)

        axarr.flat[idx].set_title("%s" % (
            dict_title[dataset]), size=fontsize)
        # xarr.flat[idx].set_title("%s %s" % (
        #     dict_title[dataset], dict_n_feature[dataset]), size=fontsize)

    axarr.flat[0].set_ylabel(
        r"$||\beta^{(k)} - \hat \beta||$", fontsize=fontsize)
    axarr.flat[4].set_ylabel(
        r"$||\mathcal{J}^{(k)} - \hat \mathcal{J}||$", fontsize=fontsize)

    fig.tight_layout()

    if save_fig:
        fig.savefig(
            fig_dir + "linear_convergence_%s.pdf" % model_name,
            bbox_inches="tight")
        fig.savefig(
            fig_dir_svg + "linear_convergence_%s.svg" % model_name,
            bbox_inches="tight")
    fig.show()


labels = ["Generalized support identification"]

fig3 = plt.figure(figsize=[18, 4])
fig3.legend([l for l in lines], labels,
            ncol=1, loc='upper center', fontsize=fontsize-4)  # , frameon=False)
fig3.tight_layout()
if save_fig:
    fig3.savefig(
        fig_dir + "linear_convergence_lasso_legend.pdf",
        bbox_inches="tight")
    fig3.savefig(
        fig_dir_svg + "linear_convergence_lasso_legend.svg",
        bbox_inches="tight")
fig3.show()

# fig4 = plt.figure(figsize=[18, 4])
# fig4.legend([l[0] for l in lines], labels,
# # fig2.legend([l[0] for l in lines],
#             ncol=3, loc='upper center', fontsize=fontsize-4)  # , frameon=False)
# fig4.tight_layout()
# if save_fig:
#     fig4.savefig(fig_dir + "lasso_pred_legend_2_columns.pdf", bbox_inches="tight")
#     fig4.savefig(fig_dir_svg + "lasso_pred_legend_2_columns.svg", bbox_inches="tight")
# fig4.show()

File Path: expes/expe_log_reg/main.py
Content:
import numpy as np
from numpy.linalg import norm
from joblib import Parallel, delayed
import pandas
from bcdsugar.utils import Monitor
from sparse_ho.ho import grad_search
from itertools import product
from sparse_ho.criterion import HeldOutLogistic
from sparse_ho.models import SparseLogreg
from sparse_ho.forward import Forward
from sparse_ho.implicit_forward import ImplicitForward
from sparse_ho.implicit import Implicit
from sparse_ho.datasets.real import get_data
from sparse_ho.grid_search import grid_search


dataset_names = ["rcv1"]
# dataset_names = ["real-sim"]
# dataset_names = ["20news"]

# methods = ["grid_search"]
methods = ["implicit_forward", "forward",
           "grid_search", "random"]
# "grid_search",
tolerance_decreases = ["constant"]
tols = 1e-7
n_outers = [25]

dict_t_max = {}
dict_t_max["rcv1"] = 50
dict_t_max["real-sim"] = 100
dict_t_max["leukemia"] = 10
dict_t_max["20news"] = 500


def parallel_function(
        dataset_name, method, tol=1e-5, n_outer=50,
        tolerance_decrease='exponential'):

    # load data
    X_train, X_val, X_test, y_train, y_val, y_test = get_data(dataset_name)
    n_samples, n_features = X_train.shape
    print('n_samples', n_samples)
    print('n_features', n_features)
    y_train[y_train == 0.0] = -1.0
    y_val[y_val == 0.0] = -1.0
    y_test[y_test == 0.0] = -1.0

    alpha_max = np.max(np.abs(X_train.T @ y_train))
    alpha_max /= X_train.shape[0]
    alpha_max /= 4
    log_alpha_max = np.log(alpha_max)

    alpha_min = alpha_max * 1e-4
    alphas = np.geomspace(alpha_max, alpha_min, 10)
    log_alphas = np.log(alphas)

    log_alpha0 = np.log(0.1 * alpha_max)
    log_alpha_max = np.log(alpha_max)
    n_outer = 25

    if dataset_name == "rcv1":
        size_loop = 2
    else:
        size_loop = 2
    model = SparseLogreg(
        X_train, y_train, max_iter=1000, log_alpha_max=log_alpha_max)
    for i in range(size_loop):
        monitor = Monitor()

        if method == "implicit_forward":
            criterion = HeldOutLogistic(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = ImplicitForward(criterion, tol_jac=1e-5, n_iter_jac=100)
            _, _, _ = grad_search(
                algo=algo, verbose=False,
                log_alpha0=log_alpha0, tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "forward":
            criterion = HeldOutLogistic(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            _, _, _ = grad_search(
                algo=algo,
                log_alpha0=log_alpha0, tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "implicit":
            criterion = HeldOutLogistic(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Implicit(criterion)
            _, _, _ = grad_search(
                algo=algo,
                log_alpha0=log_alpha0, tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "grid_search":
            criterion = HeldOutLogistic(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            # log_alpha_min = np.log(alpha_min)
            log_alphas = np.log(np.geomspace(alpha_max, alpha_min, num=100))
            log_alpha_opt, min_g_func = grid_search(
                algo, None, None, monitor, tol=tol, samp="grid",
                t_max=dict_t_max[dataset_name], log_alphas=log_alphas)
            print(log_alpha_opt)

        elif method == "random":
            criterion = HeldOutLogistic(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(alpha_min)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, np.log(alpha_max), monitor, max_evals=100, tol=tol, samp="random",
                t_max=dict_t_max[dataset_name])
            print(log_alpha_opt)

        elif method == "lhs":
            criterion = HeldOutLogistic(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(alpha_min)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, np.log(alpha_max), monitor, max_evals=100, tol=tol, samp="lhs",
                t_max=dict_t_max[dataset_name])
            print(log_alpha_opt)

    monitor.times = np.array(monitor.times).copy()
    monitor.objs = np.array(monitor.objs).copy()
    monitor.objs_test = np.array(monitor.objs_test).copy()
    monitor.log_alphas = np.array(monitor.log_alphas).copy()
    return (dataset_name, method, tol, n_outer, tolerance_decrease,
            monitor.times, monitor.objs, monitor.objs_test,
            monitor.log_alphas, norm(y_val), norm(y_test), log_alpha_max)


print("enter parallel")
backend = 'loky'
# n_jobs = 1
n_jobs = len(methods)
results = Parallel(n_jobs=n_jobs, verbose=100, backend=backend)(
    delayed(parallel_function)(
        dataset_name, method, n_outer=n_outer,
        tolerance_decrease=tolerance_decrease, tol=tols)
    for dataset_name, method, n_outer,
    tolerance_decrease in product(
        dataset_names, methods, n_outers, tolerance_decreases))
print('OK finished parallel')

df = pandas.DataFrame(results)
df.columns = [
    'dataset', 'method', 'tol', 'n_outer', 'tolerance_decrease',
    'times', 'objs', 'objs_test', 'log_alphas', 'norm y_val',
    'norm y_test', "log_alpha_max"]

for dataset_name in dataset_names:
    df[df['dataset'] == dataset_name].to_pickle(
        "%s.pkl" % dataset_name)

File Path: expes/expe_log_reg/plot.py
Content:
import numpy as np
import pandas
import seaborn as sns
import matplotlib.pyplot as plt
from expes.utils import configure_plt

save_fig = False
# save_fig = True
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"

configure_plt()

# init()

fontsize = 16

current_palette = sns.color_palette("colorblind")
dict_color = {}
dict_color["grid_search"] = current_palette[3]
dict_color["random"] = current_palette[5]
dict_color["bayesian"] = current_palette[0]
dict_color["implicit_forward"] = current_palette[2]
dict_color["fast_iterdiff"] = current_palette[2]
dict_color["forward"] = current_palette[4]
dict_color["implicit"] = current_palette[1]
dict_color["lhs"] = current_palette[6]

dict_method = {}
dict_method["forward"] = 'F. Iterdiff.'
dict_method["implicit_forward"] = 'Imp. F. Iterdiff. (ours)'
dict_method["fast_iterdiff"] = 'Imp. F. Iterdiff. (ours)'
dict_method['implicit'] = 'Implicit'
dict_method['grid_search'] = 'Grid-search'
dict_method['bayesian'] = 'Bayesian'
dict_method['random'] = 'Random-search'
dict_method['hyperopt'] = 'Random-search'
dict_method['backward'] = 'B. Iterdiff.'
dict_method['lhs'] = 'Lattice Hyp.'


dict_markers = {}
dict_markers["forward"] = 'o'
dict_markers["implicit_forward"] = 'X'
dict_markers["fast_iterdiff"] = 'X'
dict_markers['implicit'] = 'v'
dict_markers['grid_search'] = 'd'
dict_markers['bayesian'] = 'P'
dict_markers['random'] = '*'
dict_markers['lhs'] = 'H'

# current_palette = sns.color_palette("colorblind")
# dict_method = {}
# dict_method["forward"] = 'F. iterdiff.'
# dict_method["implicit_forward"] = 'Imp. F. iterdiff. (ours)'
# dict_method['implicit'] = 'Implicit diff. (ours)'
# dict_method['grid_search'] = 'Grid-search'
# dict_method['bayesian'] = 'Bayesian'
# dict_method['random'] = 'Random-search'
# dict_method['hyperopt'] = 'Random-search'

# TODO isolate
# current_palette[i]
# dict_color = {}
# dict_color["implicit_forward"] = current_palette[0]
# dict_color['implicit'] = current_palette[1]
# dict_color["forward"] = current_palette[2]
# dict_color['grid_search'] = current_palette[3]
# dict_color['bayesian'] = current_palette[4]
# dict_color['random'] = current_palette[5]
# dict_color['hyperopt'] = current_palette[6]

dict_title = {}
dict_title["rcv1"] = "rcv1"
dict_title["20news"] = "20news"
dict_title["finance"] = "finance"
dict_title["kdda_train"] = "kdda"
dict_title["climate"] = "climate"
dict_title["leukemia"] = "leukemia"
dict_title["real-sim"] = "real-sim"

dict_markevery = {}
dict_markevery["20news"] = 5
dict_markevery["finance"] = 10
dict_markevery["rcv1"] = 5
dict_markevery["leukemia"] = 1
dict_markevery["real-sim"] = 5

dict_n_feature = {}
dict_n_feature["rcv1"] = r"($p=19,959$)"
dict_n_feature["20news"] = r"($p=130,107$)"
dict_n_feature["finance"] = r"($p=1,668,737$)"
dict_n_feature["leukemia"] = r"($p=7,129$)"
dict_n_feature["real-sim"] = r"($p=20,958$)"

markersize = 8

dict_marker_size = {}
dict_marker_size["forward"] = 4
dict_marker_size["implicit_forward"] = 9
dict_marker_size["fast_iterdiff"] = 4
dict_marker_size['implicit'] = 4
dict_marker_size['grid_search'] = 5
dict_marker_size['bayesian'] = 4
dict_marker_size['random'] = 5
dict_marker_size['lhs'] = 4

# dataset_names = ["rcv1", "real-sim"]
dataset_names = ["rcv1", "real-sim", '20newsgroups']
# dataset_names = ["20newsgroups"]
# dataset_names = ["finance"]
# dataset_names = ["leukemia"]
# dataset_names = [
#     "leukemia", "climate", "rcv1", "20newsgroups", "finance", "kdda_train"]


plt.close('all')
fig_subopt, axarr_subopt = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4])

fig_obj, axarr_obj = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4])


fig_alpha, axarr_alpha = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4])


for idx, dataset in enumerate(dataset_names):
    df_data = pandas.read_pickle("%s.pkl" % dataset)
    df_data = df_data[df_data['tolerance_decrease'] == 'constant']

    methods = df_data['method']
    times = df_data['times']
    objs = df_data['objs']
    objs_tests = df_data['objs_test']
    log_alphas = df_data['log_alphas']
    tols = df_data['tolerance_decrease']
    norm_y_vals = df_data['norm y_val']
    norm_val = 0
    for norm_y_valss in norm_y_vals:
        norm_val = norm_y_valss

    min_objs = np.infty
    for obj in objs:
        min_objs = np.minimum(min_objs, obj.min())
        # obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]

    lines = []

    plt.figure()
    for i, (time, obj, log_alpha, method, tol) in enumerate(
            zip(times, objs, log_alphas, methods, tols)):
        marker = dict_markers[method]
        if method.startswith(('grid_search', "implicit_forward")):
            if method == 'implicit_forward':
                color = [plt.cm.Greens(
                    (i+len(obj)/1.1) / len(obj) / 2) for i in np.arange(len(obj))]
                s = 100
            else:
                # markevery = dict_markevery[dataset]
                color = dict_color[method]
                s = dict_marker_size[method]
            axarr_alpha.flat[idx].scatter(
                np.array(log_alpha), obj, color=color,
                label="%s" % (dict_method[method]),
                marker=marker, s=s)
            # axarr_alpha.flat[idx].plot(
            #     np.array(log_alpha), obj,
            #     "bX", color=dict_color[method],
            #     label="%s" % (dict_method[method]),
            #     marker=marker, markersize=dict_marker_size[method],
            #     markevery=1)

    for i, (time, obj, objs_test, method, tol) in enumerate(
            zip(times, objs, objs_tests, methods, tols)):
        marker = dict_markers[method]
        objs_test = [np.min(objs_test[:k]) for k in np.arange(
            len(objs_test)) + 1]
        axarr_obj.flat[idx].plot(
            time, objs_test, color=dict_color[method],
            label="%s" % (dict_method[method]),
            marker=marker, markersize=markersize,
            markevery=dict_markevery[dataset])

    for i, (time, obj, method, tol) in enumerate(
            zip(times, objs, methods, tols)):
        marker = dict_markers[method]
        obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]
        lines.append(
            axarr_subopt.flat[idx].semilogy(
                time, (obj-min_objs),
                color=dict_color[method],
                label="%s" % (dict_method[method]),
                marker=marker, markersize=markersize,
                markevery=dict_markevery[dataset]))

    axarr_alpha.flat[idx].set_title("%s %s" % (
        dict_title[dataset], dict_n_feature[dataset]), size=fontsize)
    axarr_subopt.flat[0].set_xlim(0, 15)
    axarr_subopt.flat[1].set_xlim(0, 40)
    axarr_subopt.flat[2].set_xlim(0, 210)

    axarr_obj.flat[0].set_xlim(0, 15)
    axarr_obj.flat[1].set_xlim(0, 40)
    axarr_obj.flat[2].set_xlim(0, 300)

axarr_subopt.flat[0].set_ylabel("Objective minus optimum", fontsize=fontsize)


axarr_obj.flat[0].set_ylabel("Loss on test set", fontsize=fontsize)
axarr_obj.flat[0].set_xlabel("Time (s)", fontsize=fontsize)
axarr_obj.flat[1].set_xlabel("Time (s)", fontsize=fontsize)
axarr_obj.flat[2].set_xlabel("Time (s)", fontsize=fontsize)

axarr_alpha.flat[0].set_ylabel("Loss on validation set", fontsize=fontsize)
axarr_alpha.flat[0].set_xlabel(
    r"$\lambda - \lambda_{\max}$", fontsize=fontsize)
axarr_alpha.flat[1].set_xlabel(
    r"$\lambda - \lambda_{\max}$", fontsize=fontsize)
axarr_alpha.flat[2].set_xlabel(
    r"$\lambda - \lambda_{\max}$", fontsize=fontsize)

fig_subopt.tight_layout()
if save_fig:
    fig_subopt.savefig(
        fig_dir + "pred_log_reg_validation_set.pdf",
        bbox_inches="tight")
    fig_subopt.savefig(
        fig_dir_svg + "pred_log_reg_validation_set.svg",
        bbox_inches="tight")
fig_subopt.show()

fig_obj.tight_layout()
if save_fig:
    fig_obj.savefig(
        fig_dir + "pred_log_reg_test_set.pdf",
        bbox_inches="tight")
    fig_obj.savefig(
        fig_dir_svg + "pred_log_reg_test_set.svg",
        bbox_inches="tight")
fig_obj.show()

fig_alpha.tight_layout()
if save_fig:
    fig_alpha.savefig(
        fig_dir + "pred_vs_alpha_log_reg_validation_set.pdf",
        bbox_inches="tight")
    fig_alpha.savefig(
        fig_dir_svg + "pred_vs_alpha_log_reg_validation_set.svg",
        bbox_inches="tight")
fig_alpha.show()

File Path: expes/expe_svm/main.py
Content:
import numpy as np
from numpy.linalg import norm
from joblib import Parallel, delayed
import pandas
from bcdsugar.utils import Monitor
from sparse_ho.ho import grad_search
from itertools import product
from sparse_ho.criterion import HeldOutSmoothedHinge
from sparse_ho.models import SVM
from sparse_ho.forward import Forward
from sparse_ho.implicit_forward import ImplicitForward
from sparse_ho.implicit import Implicit
from sparse_ho.datasets.real import get_data
from sparse_ho.grid_search import grid_search

# from my_data import get_data

dataset_names = ["real-sim"]

# methods = ["implicit_forward", "implicit"]
methods = ["forward", "implicit_forward"]
# "grid_search",
tolerance_decreases = ["constant"]
tols = 1e-5
n_outers = [1]

dict_t_max = {}
dict_t_max["rcv1"] = 50
dict_t_max["real-sim"] = 100
dict_t_max["leukemia"] = 10
dict_t_max["20news"] = 500


def parallel_function(
        dataset_name, method, tol=1e-5, n_outer=50,
        tolerance_decrease='exponential'):

    # load data
    X_train, X_val, X_test, y_train, y_val, y_test = get_data(dataset_name, csr=True)
    n_samples, n_features = X_train.shape
    print('n_samples', n_samples)
    print('n_features', n_features)
    y_train[y_train == 0.0] = -1.0
    y_val[y_val == 0.0] = -1.0
    y_test[y_test == 0.0] = -1.0

    C_max = 100
    logC = np.log(1e-2)
    n_outer = 5

    if dataset_name == "rcv1":
        size_loop = 1
    else:
        size_loop = 1
    model = SVM(
        X_train, y_train, logC, max_iter=10000, tol=tol)
    for i in range(size_loop):
        monitor = Monitor()

        if method == "implicit_forward":
            criterion = HeldOutSmoothedHinge(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = ImplicitForward(criterion, tol_jac=1e-3, n_iter_jac=100)
            _, _, _ = grad_search(
                algo=algo, verbose=False,
                log_alpha0=logC, tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "forward":
            criterion = HeldOutSmoothedHinge(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            _, _, _ = grad_search(
                algo=algo,
                log_alpha0=logC, tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "implicit":
            criterion = HeldOutSmoothedHinge(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Implicit(criterion)
            _, _, _ = grad_search(
                algo=algo,
                log_alpha0=logC, tol=tol,
                n_outer=n_outer, monitor=monitor,
                t_max=dict_t_max[dataset_name],
                tolerance_decrease=tolerance_decrease)

        elif method == "grid_search":
            criterion = HeldOutSmoothedHinge(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(1e-2)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, np.log(C_max), monitor, max_evals=25,
                tol=tol, samp="grid")
            print(log_alpha_opt)

        elif method == "random":
            criterion = HeldOutSmoothedHinge(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(1e-2)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, np.log(C_max), monitor, max_evals=25,
                tol=tol, samp="random")
            print(log_alpha_opt)

        elif method == "lhs":
            criterion = HeldOutSmoothedHinge(X_val, y_val, model, X_test=X_test, y_test=y_test)
            algo = Forward(criterion)
            log_alpha_min = np.log(1e-2)
            log_alpha_opt, min_g_func = grid_search(
                algo, log_alpha_min, np.log(C_max), monitor, max_evals=25,
                tol=tol, samp="lhs")
            print(log_alpha_opt)

    monitor.times = np.array(monitor.times)
    monitor.objs = np.array(monitor.objs)
    monitor.objs_test = np.array(monitor.objs_test)
    monitor.log_alphas = np.array(monitor.log_alphas)
    return (dataset_name, method, tol, n_outer, tolerance_decrease,
            monitor.times, monitor.objs, monitor.objs_test,
            monitor.log_alphas, norm(y_val), norm(y_test))


print("enter parallel")
backend = 'loky'
n_jobs = 1
results = Parallel(n_jobs=n_jobs, verbose=100, backend=backend)(
    delayed(parallel_function)(
        dataset_name, method, n_outer=n_outer,
        tolerance_decrease=tolerance_decrease, tol=tols)
    for dataset_name, method, n_outer,
    tolerance_decrease in product(
        dataset_names, methods, n_outers, tolerance_decreases))
print('OK finished parallel')

df = pandas.DataFrame(results)
df.columns = [
    'dataset', 'method', 'tol', 'n_outer', 'tolerance_decrease',
    'times', 'objs', 'objs_test', 'log_alphas', 'norm y_val',
    'norm y_test']

for dataset_name in dataset_names:
    df[df['dataset'] == dataset_name].to_pickle(
        "%s.pkl" % dataset_name)

File Path: expes/expe_svm/plot.py
Content:
import numpy as np
import pandas
import seaborn as sns
import matplotlib.pyplot as plt
from expes.utils import configure_plt

configure_plt()
# init()

fontsize = 16

current_palette = sns.color_palette("colorblind")
dict_color = {}
dict_color["grid_search"] = current_palette[3]
dict_color["random"] = current_palette[5]
dict_color["bayesian"] = current_palette[0]
dict_color["implicit_forward"] = current_palette[2]
dict_color["fast_iterdiff"] = current_palette[2]
dict_color["forward"] = current_palette[4]
dict_color["implicit"] = current_palette[1]
dict_color["lhs"] = current_palette[6]

dict_method = {}
dict_method["forward"] = 'F. Iterdiff.'
dict_method["implicit_forward"] = 'Imp. F. Iterdiff. (ours)'
dict_method["fast_iterdiff"] = 'Imp. F. Iterdiff. (ours)'
dict_method['implicit'] = 'Implicit'
dict_method['grid_search'] = 'Grid-search'
dict_method['bayesian'] = 'Bayesian'
dict_method['random'] = 'Random-search'
dict_method['hyperopt'] = 'Random-search'
dict_method['backward'] = 'B. Iterdiff.'
dict_method['lhs'] = 'Lattice Hyp.'


dict_markers = {}
dict_markers["forward"] = 'o'
dict_markers["implicit_forward"] = 'X'
dict_markers["fast_iterdiff"] = 'X'
dict_markers['implicit'] = 'v'
dict_markers['grid_search'] = 'd'
dict_markers['bayesian'] = 'P'
dict_markers['random'] = '*'
dict_markers['lhs'] = 'H'

# current_palette = sns.color_palette("colorblind")
# dict_method = {}
# dict_method["forward"] = 'F. iterdiff.'
# dict_method["implicit_forward"] = 'Imp. F. iterdiff. (ours)'
# dict_method['implicit'] = 'Implicit diff. (ours)'
# dict_method['grid_search'] = 'Grid-search'
# dict_method['bayesian'] = 'Bayesian'
# dict_method['random'] = 'Random-search'
# dict_method['hyperopt'] = 'Random-search'

# TODO isolate
# current_palette[i]
# dict_color = {}
# dict_color["implicit_forward"] = current_palette[0]
# dict_color['implicit'] = current_palette[1]
# dict_color["forward"] = current_palette[2]
# dict_color['grid_search'] = current_palette[3]
# dict_color['bayesian'] = current_palette[4]
# dict_color['random'] = current_palette[5]
# dict_color['hyperopt'] = current_palette[6]

dict_title = {}
dict_title["rcv1"] = "rcv1"
dict_title["20news"] = "20news"
dict_title["finance"] = "finance"
dict_title["kdda_train"] = "kdda"
dict_title["climate"] = "climate"
dict_title["leukemia"] = "leukemia"
dict_title["real-sim"] = "real-sim"

dict_markevery = {}
dict_markevery["20news"] = 5
dict_markevery["finance"] = 10
dict_markevery["rcv1"] = 1
dict_markevery["leukemia"] = 1
dict_markevery["real-sim"] = 1

dict_n_feature = {}
dict_n_feature["rcv1"] = r"($p=19,959$)"
dict_n_feature["20news"] = r"($p=632,982$)"
dict_n_feature["finance"] = r"($p=1,668,737$)"
dict_n_feature["leukemia"] = r"($p=7,129$)"
dict_n_feature["real-sim"] = r"($p=20,958$)"
markersize = 8

dataset_names = ["leukemia"]
# dataset_names = ["20newsgroups"]
# dataset_names = ["finance"]
# dataset_names = ["leukemia"]
# dataset_names = [
#     "leukemia", "climate", "rcv1", "20newsgroups", "finance", "kdda_train"]


plt.close('all')
fig, axarr = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4],)

fig2, axarr2 = plt.subplots(
    1, 3, sharex=False, sharey=False, figsize=[14, 4],)


for idx, dataset in enumerate(dataset_names):
    df_data = pandas.read_pickle("%s.pkl" % dataset)
    df_data = df_data[df_data['tolerance_decrease'] == 'constant']

    methods = df_data['method']
    times = df_data['times']
    objs = df_data['objs']
    objs_tests = df_data['objs_test']
    log_alphas = df_data['log_alphas']
    tols = df_data['tolerance_decrease']
    norm_y_vals = df_data['norm y_val']
    norm_val = 0
    for norm_y_valss in norm_y_vals:
        norm_val = norm_y_valss

    min_objs = np.infty
    for obj in objs:
        min_objs = np.minimum(min_objs, obj.min())
        # obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]

    lines = []

    plt.figure()
    for i, (time, obj, objs_test, method, tol) in enumerate(
            zip(times, objs, objs_tests, methods, tols)):
        marker = dict_markers[method]
        objs_test = [np.min(objs_test[:k]) for k in np.arange(
            len(objs_test)) + 1]
        axarr2.flat[idx].semilogy(
            time, objs_test, color=dict_color[method],
            label="%s" % (dict_method[method]),
            # label="%s, %s" % (dict_method[method], tol),
            marker=marker, markersize=markersize,
            markevery=dict_markevery[dataset])
        # plt.legend()

    for i, (time, obj, method, tol) in enumerate(
            zip(times, objs, methods, tols)):
        marker = dict_markers[method]
        obj = [np.min(obj[:k]) for k in np.arange(len(obj)) + 1]
        lines.append(
            axarr.flat[idx].semilogy(
                time, (obj-min_objs),
                # time, (obj-min_objs) / norm_val,
                color=dict_color[method],
                label="%s" % (dict_method[method]),
                # label="%s, %s" % (dict_method[method], tol),
                marker=marker, markersize=markersize,
                markevery=dict_markevery[dataset]))
        # axarr.flat[i].legend()

    axarr.flat[idx].set_title("%s %s" % (
        dict_title[dataset], dict_n_feature[dataset]), size=fontsize)
    # axarr.flat[idx].title.set_text(dict_title[dataset], size=18)

axarr.flat[0].set_ylabel("Objective minus optimum", fontsize=fontsize)

fig.tight_layout()
fig.show()

axarr2.flat[0].set_ylabel("Loss on test set", fontsize=fontsize)
fig2.tight_layout()
fig2.show()

File Path: expes/hypergradient/figure_hypergradient.py
Content:
import numpy as np
import pandas
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sparse_ho.utils_plot import configure_plt, plot_legend_apart
configure_plt()
fontsize = 18

# save_fig = False
save_fig = True

fig_dir = "results/"
fig_dir_svg = "results/"

current_palette = sns.color_palette("colorblind")
dict_method = {}
dict_method["forward"] = 'PCD Forward Iterdiff.'
dict_method["implicit_forward"] = 'Imp. F. Iterdiff.'
dict_method['celer'] = 'Imp. F. Iterdiff. + Celer'
dict_method['grid_search'] = 'Grid-search'
dict_method['bayesian'] = 'Bayesian'
dict_method['random'] = 'Random-search'
dict_method['hyperopt'] = 'Random-search'
dict_method['backward'] = 'B. Iterdiff.'

dict_color = {}
dict_color["grid_search"] = current_palette[3]
dict_color["backward"] = current_palette[9]
dict_color["random"] = current_palette[5]
dict_color["bayesian"] = current_palette[0]
dict_color["implicit_forward"] = current_palette[2]
dict_color["forward"] = current_palette[4]
dict_color["celer"] = current_palette[1]

dict_markevery = {}
dict_markevery["forward"] = 2
dict_markevery["implicit_forward"] = 1
dict_markevery["backward"] = 3
dict_markevery["celer"] = 4


dict_div_alphas = {}
dict_div_alphas[10] = "10"
dict_div_alphas[100] = "10^2"

dict_markers = {}
dict_markers["forward"] = 'o'
dict_markers["implicit_forward"] = 'X'
dict_markers['celer'] = 'v'
dict_markers['grid_search'] = 'd'
dict_markers['bayesian'] = 'P'
dict_markers['random'] = '*'
dict_markers["backward"] = ">"

##############################################
y_lims = {}
y_lims["news20", 10] = 1e-10
y_lims["news20", 100] = 1e-10
y_lims["real-sim", 10] = 1e-10
y_lims["real-sim", 5] = 1e-10
y_lims["real-sim", 50] = 1e-10
y_lims["real-sim", 100] = 1e-10

y_lims["rcv1_train", 10] = 1e-10
y_lims["rcv1_train", 5] = 1e-10
y_lims["rcv1_train", 50] = 1e-10
y_lims["rcv1_train", 100] = 1e-10

##############################################

epoch_lims = {}
epoch_lims["news20", 10] = 250
epoch_lims["news20", 5] = 500
epoch_lims["real-sim", 10] = 45
epoch_lims["real-sim", 25] = 195
epoch_lims["rcv1_train", 10] = 145
epoch_lims["rcv1_train", 5] = 990
##############################################
time_lims = {}
time_lims["real-sim", 10] = (1e0, 100)
time_lims["real-sim", 5] = (1e0, 100)
time_lims["rcv1_train", 10] = (1e0, 500)
time_lims["rcv1_train", 5] = (1e0, 500)
time_lims["news20", 10] = (1e0, 1000)
time_lims["news20", 25] = (1e0, 10000)
time_lims["colon", 10] = (1e-1, 50)
time_lims["colon", 25] = (1e-1, 100)
##############################################

dict_title = {}
dict_title["rcv1_train"] = "rcv1"
dict_title["news20"] = "20news"
dict_title["colon"] = "colon"
dict_title["real-sim"] = "real-sim"

files = os.listdir('results/')

for i in range(len(files)):
    if i == 0:
        df_data = pandas.read_pickle("results/" + files[i])
    else:
        print(files[i])
        df_temp = pandas.read_pickle("results/" + files[i])
        df_data = pandas.concat([df_data, df_temp])


methods = df_data['method'].unique()
methods = np.delete(methods, np.where(methods == "ground_truth"))
list_datasets = ["rcv1_train", "real-sim", "news20"]
div_alphas = df_data['div_alpha'].unique()
div_alphas = np.sort(div_alphas)


fig, axarr = plt.subplots(
    len(div_alphas), len(list_datasets), sharex=False, sharey=True,
    figsize=[10.67, 5])
fig2, axarr2 = plt.subplots(
    len(div_alphas), len(list_datasets), sharex=False, sharey=True,
    figsize=[10.67, 5])

for idx1, dataset in enumerate(list_datasets):
    df_dataset = df_data[df_data['dataset'] == dataset]
    for idx2, div_alpha in enumerate(div_alphas):
        df_div = df_dataset[df_dataset['div_alpha'] == div_alpha]
        grad = np.float(
            df_div['grad'][df_div['method'] == "ground_truth"].unique())
        lines = []
        # plt.figure()
        for method in methods:
            df_method = df_div[df_div['method'] == method]
            df_method = df_method.sort_values('maxit')
            lines.append(
                axarr2.flat[idx2 * len(list_datasets) + idx1].semilogy(
                    df_method['maxit'], np.abs(df_method['grad'] - grad),
                    label=dict_method[method], color=dict_color[method],
                    marker=""))
            diff_grad = np.abs(df_method['grad'] - grad)
            diff_grad[diff_grad < 1e-9] = 1e-9
            axarr.flat[idx2 * len(list_datasets) + idx1].semilogy(
                df_method['time'], np.abs(df_method['grad'] - grad),
                label=dict_method[method], color=dict_color[method],
                marker="o")

            if dataset == "news20":
                axarr.flat[idx2 * len(list_datasets) + idx1].set_xlim([0, 420])

        axarr2.flat[idx2 * len(list_datasets) + idx1].set_ylim(
                y_lims[dataset, div_alpha])
        axarr.flat[idx2 * len(list_datasets) + idx1].set_ylim(
                y_lims[dataset, div_alpha])
        axarr.flat[idx2 * len(list_datasets)].set_ylabel(
                r"$e^\lambda = e^{\lambda_{\max}}/ %s$" %
                dict_div_alphas[div_alpha], fontsize=fontsize)

        axarr.flat[idx1].set_title(dict_title[dataset], fontsize=fontsize)

        axarr2.flat[idx1].set_title(dict_title[dataset])

for i in np.arange(len(list_datasets)):
    axarr.flat[-(i + 1)].set_xlabel("Time (s)", fontsize=fontsize)
    axarr2.flat[-(i + 1)].set_xlabel(r"$\#$ epochs", fontsize=fontsize)

fig.tight_layout()
fig2.tight_layout()

fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"
if save_fig:
    fig.savefig(fig_dir + "hypergradient_computation.pdf", bbox_inches="tight")
    fig.savefig(fig_dir_svg + "hypergradient_computation.svg",
                bbox_inches="tight")

    plot_legend_apart(
        axarr[0][0],
        fig_dir + "legend_hypergradient_computation.pdf", ncol=3)
fig.show()
fig2.show()

File Path: expes/hypergradient/figure_hypergradient_svm.py
Content:
import numpy as np
import pandas
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sparse_ho.utils_plot import configure_plt, plot_legend_apart
from main_hypergradient_svm import dict_max_iter
configure_plt()
fontsize = 18

# save_fig = False
save_fig = True
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"

current_palette = sns.color_palette("colorblind")
dict_method = {}
dict_method["forward"] = 'PCD Forward Iterdiff.'
dict_method["implicit_forward"] = 'Imp. F. Iterdiff.'
dict_method['sota'] = 'Imp. F. Iterdiff. + Lightning'
dict_method['grid_search'] = 'Grid-search'
dict_method['bayesian'] = 'Bayesian'
dict_method['random'] = 'Random-search'
dict_method['hyperopt'] = 'Random-search'

dict_color = {}
dict_color["grid_search"] = current_palette[3]
dict_color["backward"] = current_palette[9]
dict_color["random"] = current_palette[5]
dict_color["bayesian"] = current_palette[0]
dict_color["implicit_forward"] = current_palette[2]
dict_color["forward"] = current_palette[4]
dict_color["sota"] = current_palette[1]

dict_markevery = {}
dict_markevery["forward"] = 2
dict_markevery["implicit_forward"] = 1
dict_markevery["backward"] = 3
dict_markevery["sota"] = 4


dict_div_alphas = {}
dict_div_alphas[10] = "10"
dict_div_alphas[100] = "10^2"

dict_markers = {}
dict_markers["forward"] = 'o'
dict_markers["implicit_forward"] = 'X'
dict_markers['sota'] = 'v'
dict_markers['grid_search'] = 'd'
dict_markers['bayesian'] = 'P'
dict_markers['random'] = '*'
dict_markers["backward"] = ">"

##############################################
y_lims = {}
y_lims["news20", 10] = 1e-10
y_lims["news20", 100] = 1e-10
y_lims["real-sim", 10] = 1e-10
y_lims["real-sim", 5] = 1e-10
y_lims["real-sim", 50] = 1e-10
y_lims["real-sim", 100] = 1e-10

y_lims["rcv1_train", 10] = 1e-10
y_lims["rcv1_train", 5] = 1e-10
y_lims["rcv1_train", 50] = 1e-10
y_lims["rcv1_train", 100] = 1e-10

##############################################

epoch_lims = {}
epoch_lims["news20", 10] = 250
epoch_lims["news20", 5] = 500
epoch_lims["real-sim", 10] = 45
epoch_lims["real-sim", 25] = 195
epoch_lims["rcv1_train", 10] = 145
epoch_lims["rcv1_train", 5] = 990
##############################################
dict_xlim = {}
dict_xlim["rcv1_train"] = 60
dict_xlim["real-sim"] = 4
##############################################

dict_title = {}
dict_title["rcv1_train"] = "rcv1"
dict_title["news20"] = "20news"
dict_title["colon"] = "colon"
dict_title["real-sim"] = "real-sim"

# n_points = 5
# dict_max_iter = {}
# dict_max_iter["real-sim"] = np.linspace(5, 100, n_points, dtype=np.int)
methods = ["implicit_forward", "sota", "forward"]

list_datasets = ["rcv1_train", "real-sim"]


fig, axarr = plt.subplots(
    1, len(list_datasets), sharex=False, sharey=True,
    figsize=[10.67, 3])

for idx1, dataset_name in enumerate(list_datasets):
    all_max_iter = dict_max_iter[dataset_name]

    str_results = "results_svm/hypergradient_svm_%s_%s_%i.pkl" % (
        dataset_name, 'ground_truth', 5)
    df_data = pandas.read_pickle(str_results)
    true_grad = df_data['grad'].to_numpy()[0]

    for method in methods:
        grads = np.zeros(len(all_max_iter))
        times = np.zeros(len(all_max_iter))
        for i, max_iter in enumerate(all_max_iter):
            str_results = "results_svm/hypergradient_svm_%s_%s_%i.pkl" % (
                dataset_name, method, max_iter)
            df_data = pandas.read_pickle(str_results)
            grads[i] = df_data['grad'].to_numpy()[0]
            times[i] = df_data['time'].to_numpy()[0]
        axarr[idx1].semilogy(
            times, np.abs(grads - true_grad), label=dict_method[method],
            color=dict_color[method], marker="o")

        axarr[idx1].set_title(dict_title[dataset_name], fontsize=fontsize)
        axarr[idx1].set_xlabel("Time (s)", fontsize=fontsize)
        axarr[idx1].set_ylim((1e-14, 1e-2))
        axarr[idx1].set_xlim((0, dict_xlim[dataset_name]))

fig.tight_layout()

if save_fig:
    fig.savefig(
        fig_dir + "hypergradient_computation_svm.pdf", bbox_inches="tight")
    fig.savefig(
        fig_dir_svg + "hypergradient_computation_svm.svg", bbox_inches="tight")
    plot_legend_apart(
        axarr[0],
        fig_dir + "legend_hypergradient_computation_svm.pdf", ncol=3)
fig.show()

File Path: expes/hypergradient/main_hypergradient.py
Content:
import os
from itertools import product
import numpy as np
from joblib import Parallel, delayed, parallel_backend
import pandas

from celer import Lasso as Lasso_celer
from libsvmdata import fetch_libsvm

from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE
from sparse_ho import ImplicitForward, Implicit
from sparse_ho import Forward, Backward
from sparse_ho.utils import Monitor


tol = 1e-32
methods = ["ground_truth"]
div_alphas = [100]
dataset_names = ["news20"]
rep = 10
dict_maxits = {}
dict_maxits[("real-sim", 10)] = np.linspace(5, 50, rep, dtype=np.int)
dict_maxits[("real-sim", 100)] = np.linspace(5, 200, rep, dtype=np.int)
dict_maxits[("rcv1_train", 10)] = np.linspace(5, 150, rep, dtype=np.int)
dict_maxits[("rcv1_train", 100)] = np.linspace(5, 1000, rep, dtype=np.int)
dict_maxits[("news20", 10)] = np.linspace(5, 1000, rep, dtype=np.int)
dict_maxits[("news20", 100)] = np.linspace(5, 2500, rep, dtype=np.int)


def parallel_function(
        dataset_name, div_alpha, method):
    X, y = fetch_libsvm(dataset_name)
    n_samples = len(y)
    if dataset_name == "news20" and div_alpha == 100:
        rng = np.random.RandomState(42)
        y += rng.randn(n_samples) * 0.01
    for maxit in dict_maxits[(dataset_name, div_alpha)]:
        print("Dataset %s, maxit %i" % (method, maxit))
        for i in range(2):
            rng = np.random.RandomState(i)
            idx_train = rng.choice(n_samples, n_samples//2, replace=False)
            idx = np.arange(0, n_samples)
            idx_val = idx[np.logical_not(np.isin(idx, idx_train))]
            alpha_max = np.max(np.abs(X[idx_train, :].T.dot(y[idx_train])))
            alpha_max /= len(idx_train)
            log_alpha = np.log(alpha_max / div_alpha)
            monitor = Monitor()
            if method == "celer":
                clf = Lasso_celer(
                    alpha=np.exp(log_alpha), fit_intercept=False,
                    tol=1e-12, max_iter=maxit)
                model = Lasso(estimator=clf, max_iter=maxit)
                criterion = HeldOutMSE(idx_train, idx_val)
                algo = ImplicitForward(
                    tol_jac=1e-32, n_iter_jac=maxit, use_stop_crit=False)
                algo.max_iter = maxit
                val, grad = criterion.get_val_grad(
                        model, X, y, log_alpha, algo.compute_beta_grad, tol=1e-12,
                        monitor=monitor, max_iter=maxit)
            elif method == "ground_truth":
                for file in os.listdir("results/"):
                    if file.startswith(
                            "hypergradient_%s_%i_%s" % (
                            dataset_name, div_alpha, method)):
                        return
                clf = Lasso_celer(
                        alpha=np.exp(log_alpha), fit_intercept=False,
                        warm_start=True, tol=1e-14, max_iter=10000)
                criterion = HeldOutMSE(idx_train, idx_val)
                if dataset_name == "news20":
                    algo = ImplicitForward(tol_jac=1e-11, n_iter_jac=100000)
                else:
                    algo = Implicit(criterion)
                model = Lasso(estimator=clf, max_iter=10000)
                val, grad = criterion.get_val_grad(
                        model, X, y, log_alpha, algo.compute_beta_grad, tol=1e-14,
                        monitor=monitor)
            else:
                model = Lasso(max_iter=maxit)
                criterion = HeldOutMSE(idx_train, idx_val)
                if method == "forward":
                    algo = Forward(use_stop_crit=False)
                elif method == "implicit_forward":
                    algo = ImplicitForward(
                        tol_jac=1e-8, n_iter_jac=maxit, use_stop_crit=False)
                elif method == "implicit":
                    algo = Implicit(max_iter=1000)
                elif method == "backward":
                    algo = Backward()
                else:
                    raise NotImplementedError
                algo.max_iter = maxit
                algo.use_stop_crit = False
                val, grad = criterion.get_val_grad(
                        model, X, y, log_alpha, algo.compute_beta_grad, tol=tol,
                        monitor=monitor, max_iter=maxit)

        results = (
            dataset_name, div_alpha, method, maxit,
            val, grad, monitor.times[0])
        df = pandas.DataFrame(results).transpose()
        df.columns = [
            'dataset', 'div_alpha', 'method', 'maxit', 'val', 'grad', 'time']
        str_results = "results/hypergradient_%s_%i_%s_%i.pkl" % (
            dataset_name, div_alpha, method, maxit)
        df.to_pickle(str_results)


print("enter parallel")
backend = 'loky'
n_jobs = 1
with parallel_backend(backend, n_jobs=n_jobs, inner_max_num_threads=1):
    Parallel()(
        delayed(parallel_function)(
            dataset_name, div_alpha, method)
        for dataset_name, div_alpha, method in product(
            dataset_names, div_alphas, methods))
print('OK finished parallel')

File Path: expes/hypergradient/main_hypergradient_svm.py
Content:
import os
import numpy as np
import pandas
from itertools import product
from sklearn.model_selection import StratifiedShuffleSplit
from joblib import Parallel, delayed, parallel_backend
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import norm

from lightning.classification import LinearSVC
from libsvmdata import fetch_libsvm

from sparse_ho.models import SVM
from sparse_ho.criterion import HeldOutSmoothedHinge
from sparse_ho import ImplicitForward, Implicit
from sparse_ho import Forward
from sparse_ho.utils import Monitor


tol = 1e-32
methods = ["ground_truth", "forward", "implicit_forward", "sota"]
# div_alphas = [100]
dataset_names = ["rcv1_train"]
# dataset_names = ["real-sim"]

n_points = 10
dict_max_iter = {}
dict_max_iter["real-sim"] = np.linspace(5, 100, n_points, dtype=np.int)
dict_max_iter["rcv1_train"] = np.linspace(5, 5_000, n_points, dtype=np.int)

dict_logC = {}
dict_logC["real-sim"] = [np.log(0.1)]
dict_logC["rcv1_train"] = [np.log(0.2)]


def parallel_function(
        dataset_name, method):
    X, y = fetch_libsvm(dataset_name)
    X, y = fetch_libsvm(dataset_name)
    if dataset_name == "real-sim":
        X = X[:, :2000]
    X = csr_matrix(X)  # very important for SVM
    my_bool = norm(X, axis=1) != 0
    X = X[my_bool, :]
    y = y[my_bool]
    logC = dict_logC[dataset_name]
    for max_iter in dict_max_iter[dataset_name]:
        print("Dataset %s, max iter %i" % (method, max_iter))
        for i in range(2):  # TODO change this
            sss1 = StratifiedShuffleSplit(
                n_splits=2, test_size=0.3333, random_state=0)
            idx_train, idx_val = sss1.split(X, y)
            idx_train = idx_train[0]
            idx_val = idx_val[0]

            monitor = Monitor()
            criterion = HeldOutSmoothedHinge(idx_train, idx_val)
            model = SVM(estimator=None, max_iter=10_000)

            if method == "ground_truth":
                for file in os.listdir("results_svm/"):
                    if file.startswith("hypergradient_svm_%s_%s" % (
                            dataset_name, method)):
                        return
                clf = LinearSVC(
                        C=np.exp(logC), tol=1e-32, max_iter=10_000,
                        loss='hinge', permute=False)
                algo = Implicit(criterion)
                model.estimator = clf
                val, grad = criterion.get_val_grad(
                        model, X, y, logC, algo.compute_beta_grad, tol=1e-14,
                        monitor=monitor)
            else:
                if method == "sota":
                    clf = LinearSVC(
                        C=np.exp(logC), loss='hinge', max_iter=max_iter,
                        tol=1e-32, permute=False)
                    model.estimator = clf
                    algo = ImplicitForward(
                        tol_jac=1e-32, n_iter_jac=max_iter,
                        use_stop_crit=False)
                elif method == "forward":
                    algo = Forward(use_stop_crit=False)
                elif method == "implicit_forward":
                    algo = ImplicitForward(
                        tol_jac=1e-8, n_iter_jac=max_iter, use_stop_crit=False)
                else:
                    raise NotImplementedError
                algo.max_iter = max_iter
                algo.use_stop_crit = False
                val, grad = criterion.get_val_grad(
                        model, X, y, logC, algo.compute_beta_grad, tol=tol,
                        monitor=monitor, max_iter=max_iter)

        results = (
            dataset_name, method, max_iter,
            val, grad, monitor.times[0])
        df = pandas.DataFrame(results).transpose()
        df.columns = [
            'dataset', 'method', 'maxit', 'val', 'grad', 'time']
        str_results = "results_svm/hypergradient_svm_%s_%s_%i.pkl" % (
            dataset_name, method, max_iter)
        df.to_pickle(str_results)


if __name__ == "__main__":
    print("enter parallel")
    backend = 'loky'
    n_jobs = 1
    with parallel_backend(backend, n_jobs=n_jobs, inner_max_num_threads=1):
        Parallel()(
            delayed(parallel_function)(dataset_name, method)
            for dataset_name, method in product(dataset_names, methods))
    print('OK finished parallel')

File Path: expes/hypergradient/plot_hypergradient.py
Content:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedShuffleSplit

from celer import Lasso as Lasso_celer
from libsvmdata import fetch_libsvm

from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE
from sparse_ho import ImplicitForward, Implicit
from sparse_ho import Forward, Backward
from sparse_ho.utils import Monitor

maxits = [5, 10, 25, 50, 75, 100]
methods = ["forward", "implicit_forward", "celer"]

dict_label = {}
dict_label["forward"] = "forward"
dict_label["implicit_forward"] = "Implicit"
dict_label["celer"] = "Implicit + celer"

dataset_name = "real-sim"

p_alpha_max = 0.1


tol = 1e-32

X, y = fetch_libsvm(dataset_name)
n_samples = len(y)

sss1 = StratifiedShuffleSplit(n_splits=2, test_size=0.3333, random_state=0)
idx_train, idx_val = sss1.split(X, y)
idx_train = idx_train[0]
idx_val = idx_val[0]

dict_res = {}


for maxit in maxits:
    for method in methods:
        print("Dataset %s, maxit %i" % (method, maxit))
        for i in range(2):
            alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples
            log_alpha = np.log(alpha_max * p_alpha_max)
            monitor = Monitor()
            if method == "celer":
                clf = Lasso_celer(
                    alpha=np.exp(log_alpha), fit_intercept=False,
                    tol=1e-12, max_iter=maxit)
                model = Lasso(estimator=clf, max_iter=maxit)
                criterion = HeldOutMSE(idx_train, idx_val)
                algo = ImplicitForward(
                    tol_jac=1e-32, n_iter_jac=maxit, use_stop_crit=False)
                algo.max_iter = maxit
                val, grad = criterion.get_val_grad(
                        model, X, y, log_alpha, algo.compute_beta_grad, tol=1e-12,
                        monitor=monitor, max_iter=maxit)
            else:
                model = Lasso(max_iter=maxit)
                criterion = HeldOutMSE(idx_train, idx_val)
                if method == "forward":
                    algo = Forward(use_stop_crit=False)
                elif method == "implicit_forward":
                    algo = ImplicitForward(
                        tol_jac=1e-8, n_iter_jac=maxit, max_iter=maxit,
                        use_stop_crit=False)
                elif method == "implicit":
                    algo = Implicit(max_iter=1000)
                elif method == "backward":
                    algo = Backward()
                else:
                    1 / 0
                algo.max_iter = maxit
                algo.use_stop_crit = False
                val, grad = criterion.get_val_grad(
                        model, X, y, log_alpha, algo.compute_beta_grad, tol=tol,
                        monitor=monitor, max_iter=maxit)

        dict_res[method, maxit] = (
            dataset_name, p_alpha_max, method, maxit,
            val, grad, monitor.times[0])

true_monitor = Monitor()
clf = Lasso_celer(
        alpha=np.exp(log_alpha), fit_intercept=False,
        warm_start=True, tol=1e-14, max_iter=10000)
criterion = HeldOutMSE(idx_train, idx_val)
algo = Implicit(criterion)
model = Lasso(estimator=clf, max_iter=10000)
true_val, true_grad = criterion.get_val_grad(
        model, X, y, log_alpha, algo.compute_beta_grad, tol=1e-14,
        monitor=true_monitor)

fig_time, ax_time = plt.subplots()
fig_iter, ax_iter = plt.subplots()

for method in methods:
    grads = np.zeros(len(maxits))
    times = np.zeros(len(maxits))
    for i, maxit in enumerate(maxits):
        grads[i] = dict_res[method, maxit][5]
        print(dict_res[method, maxit][6])
        times[i] = dict_res[method, maxit][6]
    ax_time.semilogy(
        times, np.abs(grads - true_grad), label=dict_label[method])

    ax_iter.semilogy(
        maxits, np.abs(grads - true_grad), label=dict_label[method])

ax_time.set_xlabel("Time (s)")
ax_time.set_ylabel("Grad - Grad Opt")
ax_time.legend()

ax_iter.set_xlabel("Iteration")
ax_iter.set_ylabel("Grad - Grad Opt")
ax_iter.legend()

fig_iter.show()
fig_time.show()

File Path: expes/hypergradient/plot_hypergradient_svm.py
Content:
import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix
from sklearn.model_selection import StratifiedShuffleSplit

from lightning.classification import LinearSVC
from libsvmdata import fetch_libsvm
from scipy.sparse.linalg import norm

from sparse_ho.models import SVM
from sparse_ho.criterion import HeldOutSmoothedHinge
from sparse_ho import ImplicitForward, Implicit
from sparse_ho import Forward
from sparse_ho.utils import Monitor

# maxits = [5, 10, 25, 50, 75, 100]
maxits = [5, 10, 25, 50, 75, 100, 500, 1000]
# maxits = [5, 10, 25, 50, 75, 100, 500, 1000, 5000, 10_000]
methods = ["forward", "implicit_forward", "sota"]

dict_label = {}
dict_label["forward"] = "forward"
dict_label["implicit_forward"] = "Implicit"
dict_label["sota"] = "Implicit + sota"


# logC = np.log(10000)
logC = np.log(0.0008)
# logC = np.log(0.15)

tol = 1e-32

dataset_name = "gisette"
# dataset_name = "covtype"
# dataset_name = "rcv1_train"
# dataset_name = "real-sim"
X, y = fetch_libsvm(dataset_name)
y[y == 2] = -1  # for covtype
X = X[:, :100]
X = csr_matrix(X)  # very important for SVM
my_bool = norm(X, axis=1) != 0
X = X[my_bool, :]
y = y[my_bool]
# X = X[:100, :]
# y = y[:100]

sss1 = StratifiedShuffleSplit(n_splits=2, test_size=0.3333, random_state=0)
idx_train, idx_val = sss1.split(X, y)
idx_train = idx_train[0]
idx_val = idx_val[0]


true_monitor = Monitor()
clf = LinearSVC(
        C=np.exp(logC), tol=1e-32, max_iter=1_000, loss='hinge',
        permute=False, verbose=True)
criterion = HeldOutSmoothedHinge(idx_train, idx_val)
algo = Implicit(criterion)
model = SVM(estimator=clf)
true_val, true_grad = criterion.get_val_grad(
        model, X, y, logC, algo.compute_beta_grad, tol=1e-14,
        monitor=true_monitor, max_iter=10_000)

dict_res = {}
for max_iter in maxits:
    for method in methods:
        print("Dataset %s, maxit %i" % (method, max_iter))
        for i in range(2):
            monitor = Monitor()
            model = SVM(max_iter=max_iter)
            criterion = HeldOutSmoothedHinge(idx_train, idx_val)
            if method == "sota":
                clf = LinearSVC(
                    C=np.exp(logC), loss='hinge', max_iter=max_iter, tol=1e-32,
                    permute=False)
                model.estimator = clf
                algo = ImplicitForward(
                    tol_jac=1e-32, n_iter_jac=max_iter, use_stop_crit=False)
                algo.max_iter = max_iter
                val, grad = criterion.get_val_grad(
                        model, X, y, logC, algo.compute_beta_grad, tol=1e-12,
                        monitor=monitor, max_iter=max_iter)
            else:
                if method == "forward":
                    algo = Forward(use_stop_crit=False)
                elif method == "implicit_forward":
                    algo = ImplicitForward(
                        tol_jac=1e-8, n_iter_jac=max_iter, max_iter=max_iter,
                        use_stop_crit=False)
                elif method == "implicit":
                    algo = Implicit(max_iter=1000)
                else:
                    raise NotImplementedError
                algo.max_iter = max_iter
                algo.use_stop_crit = False
                val, grad = criterion.get_val_grad(
                        model, X, y, logC, algo.compute_beta_grad, tol=tol,
                        monitor=monitor, max_iter=max_iter)

        dict_res[method, max_iter] = (
            dataset_name, logC, method, max_iter,
            val, grad, monitor.times[0])

fig_time, ax_time = plt.subplots()
fig_iter, ax_iter = plt.subplots()

for method in methods:
    grads = np.zeros(len(maxits))
    times = np.zeros(len(maxits))
    for i, maxit in enumerate(maxits):
        grads[i] = dict_res[method, maxit][5]
        print(dict_res[method, maxit][6])
        times[i] = dict_res[method, maxit][6]
    ax_time.semilogy(
        times, np.abs(grads - true_grad), label=dict_label[method])

    ax_iter.semilogy(
        maxits, np.abs(grads - true_grad), label=dict_label[method])

ax_time.set_xlabel("Time (s)")
ax_time.set_ylabel("Grad - Grad Opt")
ax_time.legend()

ax_iter.set_xlabel("Iteration")
ax_iter.set_ylabel("Grad - Grad Opt")
ax_iter.legend()

fig_iter.show()
fig_time.show()

File Path: expes/multiclass/figure_multiclass.py
Content:
import numpy as np
import pandas
# import seaborn as sns
import matplotlib.pyplot as plt
from sparse_ho.utils_plot import (
    configure_plt, plot_legend_apart, dict_color, dict_method)

# save_fig = False
save_fig = True
fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"

configure_plt()

fontsize = 18


dict_markers = {}
dict_markers["forward"] = 'o'
dict_markers["implicit_forward"] = 'X'
dict_markers["implicit_forward_cdls"] = 'X'
dict_markers["implicit_forward_scipy"] = 'X'
dict_markers["fast_iterdiff"] = 'X'
dict_markers['implicit'] = 'v'
dict_markers['grid_search'] = 'd'
dict_markers['bayesian'] = 'P'
dict_markers['random'] = '*'
dict_markers['lhs'] = 'H'

dict_title = {}
dict_title["mnist"] = "mnist"
dict_title["rcv1_multiclass"] = "rcv1"
dict_title["20news"] = "20news"
dict_title["finance"] = "finance"
dict_title["kdda_train"] = "kdda"
dict_title["climate"] = "climate"
dict_title["leukemia"] = "leukemia"
dict_title["real-sim"] = "real-sim"
dict_title["sensit"] = "sensit"
dict_title["aloi"] = "aloi"
dict_title["usps"] = "usps"
dict_title["sector_scale"] = "sector"

dict_n_classes = {}
dict_n_classes["mnist"] = 10
dict_n_classes["rcv1_multiclass"] = 53
dict_n_classes["20news"] = "20news"
dict_n_classes["finance"] = "finance"
dict_n_classes["kdda_train"] = "kdda"
dict_n_classes["climate"] = "climate"
dict_n_classes["leukemia"] = "leukemia"
dict_n_classes["real-sim"] = "real-sim"
dict_n_classes["sensit"] = "sensit"
dict_n_classes["aloi"] = 1_000
dict_n_classes["usps"] = 10
dict_n_classes["sector_scale"] = 105


dict_markevery = {}
dict_markevery["20news"] = 5
dict_markevery["finance"] = 10
dict_markevery["rcv1"] = 5
dict_markevery["leukemia"] = 1
dict_markevery["real-sim"] = 5
dict_markevery["sensit"] = 1
dict_markevery["aloi"] = 1
dict_markevery["usps"] = 1
dict_markevery["sector_scale"] = 1

dict_n_feature = {}
dict_n_feature["rcv1"] = r"($p=19,959$)"
dict_n_feature["20news"] = r"($p=130,107$)"
dict_n_feature["finance"] = r"($p=1,668,737$)"
dict_n_feature["leukemia"] = r"($p=7,129$)"
dict_n_feature["real-sim"] = r"($p=20,958$)"
dict_n_feature["sensit"] = r"($p=0$)"
dict_n_feature["aloi"] = r"($p=0$)"
dict_n_feature["sector_scale"] = r"($p=0$)"

dict_marker_size = {}
dict_marker_size["forward"] = 4
dict_marker_size["implicit_forward"] = 9
dict_marker_size["implicit_forward_cdls"] = 9
dict_marker_size["implicit_forward_scipy"] = 9
dict_marker_size["fast_iterdiff"] = 4
dict_marker_size['implicit'] = 4
dict_marker_size['grid_search'] = 5
dict_marker_size['bayesian'] = 4
dict_marker_size['random'] = 5
dict_marker_size['lhs'] = 4

dict_xlim = {}
dict_xlim["mnist"] = 500
dict_xlim["usps"] = 1_200
dict_xlim["rcv1_multiclass"] = 1_400
dict_xlim["aloi"] = 2_000

# dataset_names = ["mnist", "rcv1_multiclass", "sector_scale","aloi"]
dataset_names = ["mnist", "usps", "rcv1_multiclass", "aloi"]
# methods = ["implicit_forward_cdls", "random"]
# methods = ["implicit_forward_cdls", "random", "bayesian"]
# methods = ["implicit_forward", "random"]
methods = ["random", "bayesian", "implicit_forward"]
# methods = ["random", "bayesian", "grid_search", "implicit_forward"]

plt.close('all')
fig_acc_val, axarr_acc_val = plt.subplots(
    1, len(dataset_names), sharex=False, sharey=False, figsize=[10.67, 3])

fig_acc_test, axarr_acc_test = plt.subplots(
    1, len(dataset_names), sharex=False, sharey=False, figsize=[10.67, 3])

# figure for cross entropy
fig_ce, axarr_ce = plt.subplots(
    1, len(dataset_names), sharex=False, sharey=False, figsize=[10.67, 3])

all_figs = [fig_acc_val, fig_acc_test, fig_ce]
all_axarr = [axarr_acc_val, axarr_acc_test, axarr_ce]
all_strings = ["acc_val", "acc_test", "crossentropy"]


for idx, dataset_name in enumerate(dataset_names):
    plt.figure()
    for method in methods:
        try:
            df_data = pandas.read_pickle(
                "results/%s_%s.pkl" % (dataset_name, method))

            method = df_data['method'].to_numpy()[0]
            times = df_data['times'].to_numpy()[0]
            objs = df_data['objs'].to_numpy()[0]
            objs = [
                np.min(objs[:k]) for k in np.arange(len(objs)) + 1]
            log_alphas = df_data['log_alphas'].to_numpy()[0]
            acc_vals = df_data['acc_vals'].to_numpy()[0]
            acc_vals = [
                np.max(acc_vals[:k]) for k in np.arange(len(acc_vals)) + 1]
            acc_tests = df_data['acc_tests'].to_numpy()[0]
            acc_tests = [
                np.max(acc_tests[:k]) for k in np.arange(len(acc_tests)) + 1]

            axarr_acc_val.flat[idx].plot(
                times, acc_vals, label=dict_method[method],
                marker=dict_markers[method],
                color=dict_color[method])
            axarr_acc_test.flat[idx].plot(
                times, acc_tests, label=dict_method[method],
                marker=dict_markers[method],
                color=dict_color[method])
            axarr_ce.flat[idx].plot(
                times, objs, label=dict_method[method],
                color=dict_color[method],
                marker=dict_markers[method])
        except Exception:
            print("No dataset found")
    for axarr in all_axarr:
        axarr[idx].set_xlim(0, dict_xlim[dataset_name])

for i, dataset_name in enumerate(dataset_names):
    axarr_ce[i].set_title("%s (q=%i)" % (
        dict_title[dataset_name], dict_n_classes[dataset_name]),
        fontsize=fontsize)
    axarr_acc_test[i].set_xlabel("Time (s)", fontsize=fontsize)


axarr_acc_val.flat[0].set_ylabel("Accuracy validation set", fontsize=fontsize)
axarr_acc_test.flat[0].set_ylabel("Accuracy test set", fontsize=fontsize)
axarr_ce.flat[0].set_ylabel("Multiclass cross-entropy", fontsize=fontsize)

for fig in all_figs:
    fig.tight_layout()


if save_fig:
    for string, fig in zip(all_strings, all_figs):
        fig.savefig("%smulticlass_%s.pdf" % (
            fig_dir, string), bbox_inches="tight")
        fig.savefig("%smulticlass_%s.svg" % (
            fig_dir_svg, string), bbox_inches="tight")
    plot_legend_apart(
        axarr_acc_val[0], "%smulticlass_%s_legend.pdf" % (fig_dir, string),
        ncol=3)

for fig in all_figs:
    fig.legend()
    fig.show()


for dataset_name in dataset_names:
    df_data = pandas.read_pickle(
        "results/%s_grid_search.pkl" % dataset_name)
    objs = df_data['objs'].to_numpy()[0]
    print(objs)
    log_alphas = df_data['log_alphas'].to_numpy()[0]
    log_alpha_max = log_alphas[0][0]
    log_alpha_min = log_alphas[-1][0]
    log_alpha_opt = log_alphas[idx][0]

    print("p_alpha %f " % np.exp(log_alpha_opt - log_alpha_max))
    print("range_alpha %f " % np.exp(log_alpha_min - log_alpha_max))

File Path: expes/multiclass/main_multiclass.py
Content:
import numpy as np
from joblib import Parallel, delayed
import pandas
from itertools import product
from libsvmdata.datasets import fetch_libsvm
from sklearn.linear_model import LogisticRegression

from sparse_ho.models import SparseLogreg
from sparse_ho.ho import grad_search, hyperopt_wrapper
from sparse_ho.implicit_forward import ImplicitForward
from sparse_ho.utils_datasets import (clean_dataset, alpha_max_multiclass,
                                      get_splits)
from sparse_ho.utils import Monitor
from sparse_ho.criterion import LogisticMulticlass
from sparse_ho.optimizers import LineSearch


dataset_names = ["rcv1_multiclass"]
# dataset_names = ["mnist", "usps", "sector_scale"]
# dataset_names = ["news20_multiclass"]
# dataset_names = ["sector_scale"]
# dataset_names = ["aloi"]
# dataset_names = ["sector_scale", "aloi"]

# methods = ['implicit_forward_scipy']
# methods = ['grid_search']
# methods = ["implicit_forward", "random", "bayesian"]
methods = ["random", "bayesian"]
# methods = ["bayesian"]
# methods = ["implicit_forward_cdls"]
# methods = ["implicit_forward"]
# methods = ["implicit_forward_cdls", "implicit_forward"]
# methods = ["implicit_forward", "random", "bayesian"]

tols = 1e-7
n_outers = [40]

dict_t_max = {}
dict_t_max["rcv1_multiclass"] = 3600
dict_t_max["real-sim"] = 100
dict_t_max["leukemia"] = 10
dict_t_max["20news"] = 500
dict_t_max["usps"] = 1500
dict_t_max["sensit"] = 3600
dict_t_max["aloi"] = 3600
dict_t_max["sector_scale"] = 3600
dict_t_max["news20_multiclass"] = 3600
dict_t_max["mnist"] = 1200


dict_subsampling = {}
dict_subsampling["mnist"] = (5_000, 1000)
dict_subsampling["rcv1_multiclass"] = (21_000, 20_000)
dict_subsampling["aloi"] = (5_000, 100)
dict_subsampling["aloi"] = (5_000, 100)
dict_subsampling["usps"] = (10_000, 10_000)
dict_subsampling["sensit"] = (100_000, 100)
dict_subsampling["sector_scale"] = (10_000, 30_000)
dict_subsampling["news20_multiclass"] = (10_000, 30_000)

dict_max_eval = {}
dict_max_eval["mnist"] = 50
dict_max_eval["rcv1_multiclass"] = 100
dict_max_eval["aloi"] = 50
dict_max_eval["usps"] = 40
dict_max_eval["sensit"] = 40
dict_max_eval["sector_scale"] = 40
dict_max_eval["news20_multiclass"] = 40


def parallel_function(
        dataset_name, method, tol=1e-8, n_outer=15):

    # load data
    X, y = fetch_libsvm(dataset_name)
    # subsample the samples and the features
    n_samples, n_features = dict_subsampling[dataset_name]
    t_max = dict_t_max[dataset_name]
    # t_max = 3600

    X, y = clean_dataset(X, y, n_samples, n_features)
    alpha_max, n_classes = alpha_max_multiclass(X, y)
    log_alpha_max = np.log(alpha_max)  # maybe to change alpha max value

    algo = ImplicitForward(None, n_iter_jac=2000)
    estimator = LogisticRegression(
        C=1, fit_intercept=False, warm_start=True, max_iter=30, verbose=False)

    model = SparseLogreg(estimator=estimator)
    idx_train, idx_val, idx_test = get_splits(X, y)

    logit_multiclass = LogisticMulticlass(
        idx_train, idx_val, algo, idx_test=idx_test)

    monitor = Monitor()
    if method == "implicit_forward":
        log_alpha0 = np.ones(n_classes) * np.log(0.1 * alpha_max)
        optimizer = LineSearch(n_outer=100)
        grad_search(
            algo, logit_multiclass, model, optimizer, X, y, log_alpha0,
            monitor)
    elif method.startswith(('random', 'bayesian')):
        max_evals = dict_max_eval[dataset_name]
        log_alpha_min = np.log(alpha_max) - 7
        hyperopt_wrapper(
            algo, logit_multiclass, model, X, y, log_alpha_min, log_alpha_max,
            monitor, max_evals=max_evals, tol=tol, t_max=t_max, method=method,
            size_space=n_classes)
    elif method == 'grid_search':
        n_alphas = 20
        p_alphas = np.geomspace(1, 0.001, n_alphas)
        p_alphas = np.tile(p_alphas, (n_classes, 1))
        for i in range(n_alphas):
            log_alpha_i = np.log(alpha_max * p_alphas[:, i])
            logit_multiclass.get_val(
                model, X, y, log_alpha_i, None, monitor, tol)

    monitor.times = np.array(monitor.times).copy()
    monitor.objs = np.array(monitor.objs).copy()
    monitor.acc_vals = np.array(monitor.acc_vals).copy()
    monitor.acc_tests = np.array(monitor.acc_tests).copy()
    monitor.log_alphas = np.array(monitor.log_alphas).copy()
    return (
        dataset_name, method, tol, n_outer, monitor.times, monitor.objs,
        monitor.acc_vals, monitor.acc_tests, monitor.log_alphas, log_alpha_max,
        n_samples, n_features, n_classes)


print("enter parallel")
backend = 'loky'
# n_jobs = 1
n_jobs = len(methods) * len(dataset_names)
results = Parallel(n_jobs=n_jobs, verbose=100, backend=backend)(
    delayed(parallel_function)(
        dataset_name, method, n_outer=n_outer, tol=tols)
    for dataset_name, method, n_outer in product(
        dataset_names, methods, n_outers))
print('OK finished parallel')

df = pandas.DataFrame(results)
df.columns = [
    'dataset', 'method', 'tol', 'n_outer', 'times', 'objs', 'acc_vals',
    'acc_tests', 'log_alphas', "log_alpha_max",
    "n_subsamples", "n_subfeatures", "n_classes"]

for dataset_name in dataset_names:
    for method in methods:
        df[(df['dataset'] == dataset_name) & (
            df['method'] == method)].to_pickle(
                "results/%s_%s.pkl" % (dataset_name, method))

File Path: expes/multiclass/plot_multiclass.py
Content:
import numpy as np

from libsvmdata.datasets import fetch_libsvm
from sklearn.linear_model import LogisticRegression
# from sklearn.linear_model import LogisticRegression

from sparse_ho.models import SparseLogreg
from sparse_ho.criterion import LogisticMulticlass
from sparse_ho import ImplicitForward
from sparse_ho.optimizers import GradientDescent

from sparse_ho.ho import grad_search, hyperopt_wrapper
from sparse_ho.utils import Monitor
from sparse_ho.datasets.utils_datasets import (
    alpha_max_multiclass, clean_dataset, get_splits)


# load data
n_samples = 1_000
n_features = 1_000
# n_samples = 1_100
# n_features = 3_200
# X, y = fetch_libsvm('sensit')
# X, y = fetch_libsvm('usps')
X, y = fetch_libsvm('rcv1_multiclass')
# X, y = fetch_libsvm('sector_scale')
# X, y = fetch_libsvm('sector')
# X, y = fetch_libsvm('smallNORB')
# X, y = fetch_libsvm('mnist')


# clean data and subsample
X, y = clean_dataset(X, y, n_samples, n_features)
idx_train, idx_val, idx_test = get_splits(X, y)
n_samples, n_features = X.shape

algo = ImplicitForward(n_iter_jac=1000)
estimator = LogisticRegression(
    C=1, fit_intercept=False, warm_start=True, max_iter=2000, verbose=False)

model = SparseLogreg(estimator=estimator)
logit_multiclass = LogisticMulticlass(
    idx_train, idx_val, algo, idx_test=idx_test)


alpha_max, n_classes = alpha_max_multiclass(X, y)
tol = 1e-5


n_alphas = 10
p_alphas = np.geomspace(1, 0.001, n_alphas)
p_alphas = np.tile(p_alphas, (n_classes, 1))

print("###################### GRID SEARCH ###################")
monitor_grid = Monitor()
for i in range(n_alphas):
    log_alpha_i = np.log(alpha_max * p_alphas[:, i])
    logit_multiclass.get_val(
        model, X, y, log_alpha_i, None, monitor_grid, tol)

1/0
print("###################### GRAD SEARCH LS ###################")
n_outer = 100
model = SparseLogreg(estimator=estimator)
logit_multiclass = LogisticMulticlass(idx_train, idx_val, idx_test, algo)

monitor = Monitor()
log_alpha0 = np.ones(n_classes) * np.log(0.1 * alpha_max)

idx_min = np.argmin(np.array(monitor_grid.objs))
log_alpha0 = monitor_grid.log_alphas[idx_min]
optimizer = GradientDescent(
    n_outer=n_outer, step_size=None, p_grad_norm=0.1, tol=tol)
grad_search(
    algo, logit_multiclass, model, optimizer, X, y, log_alpha0, monitor)


print("###################### USE HYPEROPT ###################")
log_alpha_max = np.log(alpha_max)
log_alpha_min = np.log(alpha_max / 10_000)
monitor_hyp = Monitor()
hyperopt_wrapper(
    algo, logit_multiclass, model, X, y, log_alpha_min, log_alpha_max,
    monitor_hyp, tol=tol, size_space=n_classes, max_evals=10)

File Path: expes/paths/figure_paths.py
Content:
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sparse_ho.utils_plot import configure_plt
configure_plt()
fontsize = 18
current_palette = sns.color_palette("colorblind")

dict_title = {}
dict_title["lasso"] = "Lasso"
dict_title["enet"] = "Elastic net"
dict_title["logreg"] = "Sparse logistic regression"

model_names = ["lasso", "enet", "logreg"]

plt.close('all')
fig, axarr = plt.subplots(
    1, len(model_names), sharex=True, sharey=False,
    figsize=[10.67, 3.5], constrained_layout=True)
# check
# loading results
for i, model_name in enumerate(model_names):
    coefs = np.load("results/coefs_%s.npy" % model_name, allow_pickle=True)
    alphas = np.load(
            "results/alphas_%s.npy" % model_name, allow_pickle=True)

    if model_name == 'logreg':
        coefs = coefs[:, 0, :]
    n_features = coefs.shape[1]

    neg_log_alphas = -np.log(alphas) + np.log(alphas[0])
    for j in range(n_features):
        axarr[i].plot(neg_log_alphas, coefs[:, j], color=current_palette[j])

    axarr[i].set_title(dict_title[model_name], fontsize=fontsize)
    axarr[i].set_xlabel(r"$\lambda_{\max} - \lambda$", fontsize=fontsize)

axarr[0].set_ylabel("Coefficient " + r"$\hat{\beta}^{(\lambda)}$",
                    fontsize=fontsize)
save_fig = True

if save_fig:
    fig_dir = "../../../CD_SUGAR/tex/journal/prebuiltimages/"
    fig_dir_svg = "../../../CD_SUGAR/tex/journal/images/"
    fig.savefig(
        fig_dir + "intro_reg_paths.pdf", bbox_inches="tight")
    fig.savefig(
        fig_dir_svg + "intro_reg_paths.svg", bbox_inches="tight")
plt.show(block=False)
fig.show()

File Path: expes/paths/main.py
Content:
# License: BSD 3 clause

# from itertools import cycle

import os
import numpy as np

from celer import Lasso, ElasticNet
from sklearn.linear_model import LogisticRegression
from sklearn import datasets
from sklearn.svm import l1_min_c
# load diabetes dataset for regression model
X, y = datasets.load_diabetes(return_X_y=True)
# Standardize data (easier to set the l1_ratio parameter)
X /= X.std(axis=0)
n_samples = len(y)

dict_X = {}
dict_X["lasso"] = X
dict_X["enet"] = X

dict_y = {}
dict_y["lasso"] = y
dict_y["enet"] = y
# load iris for classification model

dict_X["logreg"] = X
dict_y["logreg"] = (y > 100) * 1.0

name_models = ["lasso", "enet", "logreg"]

dict_models = {}
dict_models["lasso"] = Lasso(fit_intercept=False, warm_start=False)
dict_models["logreg"] = LogisticRegression(
    penalty="l1", fit_intercept=False, warm_start=False, solver='liblinear',
    max_iter=10000, tol=1e-9)
dict_models["enet"] = ElasticNet(fit_intercept=False, warm_start=False)

# Compute alpha_max

dict_alpha_max = {}
dict_alpha_max["lasso"] = np.max(
    np.abs(dict_X["lasso"].T.dot(dict_y["lasso"]))) / n_samples
dict_alpha_max["logreg"] = 1 / l1_min_c(dict_X['logreg'], dict_y['logreg'])
dict_alpha_max["enet"] = dict_alpha_max["lasso"]
# Setting grid of values for alpha
n_alphas = 1000
p_alpha_min = 1e-5
p_alphas = np.geomspace(1, p_alpha_min, n_alphas)

os.makedirs('./results', exist_ok=True)

for name_model in name_models:
    alphas = dict_alpha_max[name_model] * p_alphas
    coefs = []

    print("Starting path computation for ", name_model)
    for alpha in alphas:
        if name_model == "lasso":
            dict_models[name_model].set_params(alpha=alpha)
        elif name_model == "enet":
            l1_ratio = 0.8
            alpha1 = 0.2 * alpha / l1_ratio
            dict_models[name_model].set_params(alpha=alpha+alpha1,
                                               l1_ratio=alpha/(alpha+alpha1))
        elif name_model == "logreg":
            dict_models[name_model].set_params(C=1/(alpha))

        dict_models[name_model].fit(dict_X[name_model], dict_y[name_model])
        coefs.append(dict_models[name_model].coef_)
    print("End path computation for ", name_model)

    coefs = np.array(coefs)
    np.save("results/coefs_%s" % name_model, coefs)
    np.save("results/alphas_%s" % name_model, alphas)

File Path: setup.py
Content:
#! /usr/bin/env python

import os
from setuptools import setup, find_packages

descr = 'Implicit forward differentiation for Lasso-type problems'

version = None
with open(os.path.join('sparse_ho', '__init__.py'), 'r') as fid:
    for line in (line.strip() for line in fid):
        if line.startswith('__version__'):
            version = line.split('=')[1].strip().strip('\'')
            break
if version is None:
    raise RuntimeError('Could not determine version')


DISTNAME = 'sparse_ho'
DESCRIPTION = descr
AUTHOR = ('Q. Bertrand', 'Q. Klopfenstein')
AUTHOR_EMAIL = 'quentin.bertrand@inria.fr'
LICENSE = 'BSD'
DOWNLOAD_URL = 'https://github.com/QB3/sparse-ho.git'
VERSION = version
URL = 'https://github.com/QB3/sparse-ho'


if __name__ == "__main__":
    setup(name=DISTNAME,
          author=AUTHOR,
          author_email=AUTHOR_EMAIL,
          description=DESCRIPTION,
          license=LICENSE,
          version=VERSION,
          url=URL,
          download_url=DOWNLOAD_URL,
          long_description=open('README.rst').read(),
          classifiers=[
              'Intended Audience :: Science/Research',
              'Intended Audience :: Developers',
              'License :: OSI Approved',
              'Programming Language :: Python',
              'Topic :: Software Development',
              'Topic :: Scientific/Engineering',
              'Operating System :: Microsoft :: Windows',
              'Operating System :: POSIX',
              'Operating System :: Unix',
              'Operating System :: MacOS',
          ],
          platforms='any',
          packages=find_packages(),
          install_requires=[
            "celer",
            "download", "hyperopt",
            "libsvmdata", "matplotlib>=2.0.0", "numba",
            "numpy", "scipy>=0.18.0",
            "scikit-learn>=0.21", "seaborn>=0.7", ]
          )

File Path: sparse_ho/__init__.py
Content:
from .ho import grad_search, hyperopt_wrapper

from .algo import Backward
from .algo import Forward
from .algo import ImplicitForward
from .algo import Implicit


__version__ = '0.1.dev'

File Path: sparse_ho/algo/__init__.py
Content:
from sparse_ho.algo.backward import Backward
from sparse_ho.algo.forward import Forward
from sparse_ho.algo.implicit import Implicit
from sparse_ho.algo.implicit_forward import ImplicitForward

__all__ = ['Backward',
           'Forward',
           'Implicit',
           'ImplicitForward']

File Path: sparse_ho/algo/backward.py
Content:
import numpy as np
from numpy.linalg import norm
from scipy.sparse import issparse
import scipy.sparse.linalg as slinalg
from sparse_ho.algo.forward import compute_beta


class Backward():
    """Algorithm to compute the hypergradient using backward differentiation.

    The algorithm first computes the regression coefficients beta, using
    proximal coordinate descent, storing all the iterates.
    Then the gradient is computed in a backward way.

    Parameters
    ----------
    use_stop_crit: bool, optional (default=True)
        Use stopping criterion in hypergradient computation. If False,
        run to maximum number of iterations.
    verbose: bool, optional (default=False)
        Verbosity of the algorithm.
    """

    def __init__(self, use_stop_crit=True, verbose=False):
        self.use_stop_crit = use_stop_crit
        self.verbose = verbose

    def compute_beta_grad(
            self, X, y, log_alpha, model, get_grad_outer, mask0=None,
            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,
            full_jac_v=False):
        """Compute beta and hypergradient with backward differentiation of
        proximal coordinate descent.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array, shape (n_features,)
            Logarithm of hyperparameter.
        model:  instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        get_grad_outer: callable
            Function which returns the gradient of the outer criterion.
        mask0: ndarray, shape (n_features,)
            Boolean of active feature of the previous regression coefficients
            beta for warm start.
        dense0: ndarray, shape (mask.sum(),)
            Initial value of the previous regression coefficients
            beta for warm start.
        quantity_to_warm_start: ndarray
            Previous Jacobian of the inner optimization problem.
        max_iter: int
            Maximum number of iteration for the inner solver.
        tol: float
            The tolerance for the inner optimization problem.
        full_jac_v: bool
            TODO
        """

        # 1 compute the regression coefficients beta
        mask, dense, list_sign = compute_beta(
            X, y, log_alpha, model, mask0=mask0, dense0=dense0,
            jac0=None, max_iter=max_iter, tol=tol,
            compute_jac=False, return_all=True,
            use_stop_crit=self.use_stop_crit)
        v = np.zeros(X.shape[1])
        v[mask] = get_grad_outer(mask, dense)
        # 2 compute the gradient in a backward way
        grad = get_grad_backward(
            X, np.exp(log_alpha), list_sign, v, model,
            jac_v0=quantity_to_warm_start)

        if not full_jac_v:
            grad = model.get_mask_jac_v(mask, grad)

        grad = np.atleast_1d(grad)
        return mask, dense, grad, grad


def get_grad_backward(X, alpha, list_beta, v, model, jac_v0=None):
    n_samples, n_features = X.shape
    is_sparse = issparse(X)
    if is_sparse:
        L = slinalg.norm(X, axis=0) ** 2 / n_samples
    else:
        L = norm(X, axis=0) ** 2 / n_samples
    v_ = v.copy()
    list_beta = np.asarray(list_beta)
    grad = model._init_g_backward(None, n_features)
    for k in (np.arange(list_beta.shape[0] - 1, -1, -1)):
        beta = list_beta[k, :]
        if is_sparse:
            grad = model._update_bcd_jac_backward_sparse(
                X.data, X.indptr, X.indices, n_samples, n_features,
                alpha, grad, beta, v_, L)
        else:
            grad = model._update_bcd_jac_backward(
                X, alpha, grad, beta, v_, L)

    return grad

File Path: sparse_ho/algo/base.py
Content:

File Path: sparse_ho/algo/forward.py
Content:
import numpy as np
from scipy.sparse import issparse


class Forward():
    """Algorithm to compute the hypergradient using forward differentiation of
    proximal coordinate descent.

    The algorithm jointly and iteratively computes the regression coefficients
    and the Jacobian using forward differentiation of proximal
    coordinate descent.

    Parameters
    ----------
    use_stop_crit: bool, optional (default=True)
        Use stopping criterion in hypergradient computation. If False,
        run to maximum number of iterations.
    verbose: bool, optional (default=False)
        Verbosity of the algorithm.
    """

    def __init__(self, use_stop_crit=True, verbose=False):
        self.use_stop_crit = use_stop_crit
        self.verbose = verbose

    def compute_beta_grad(
            self, X, y, log_alpha, model, get_grad_outer, mask0=None,
            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,
            full_jac_v=False):
        """Compute beta and hypergradient, with forward differentiation of
        proximal coordinate descent.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array, shape (n_features,)
            Logarithm of hyperparameter.
        model:  instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        get_grad_outer: callable
            Function which returns the gradient of the outer criterion.
        mask0: ndarray, shape (n_features,)
            Boolean of active feature of the previous regression coefficients
            beta for warm start.
        dense0: ndarray, shape (mask.sum(),)
            Initial value of the previous regression coefficients
            beta for warm start.
        quantity_to_warm_start: ndarray
            Previous Jacobian of the inner optimization problem.
        max_iter: int
            Maximum number of iteration for the inner solver.
        tol: float
            The tolerance for the inner optimization problem.
        full_jac_v: bool
            TODO
        """
        # jointly compute the regression coefficients beta and the Jacobian
        mask, dense, jac = compute_beta(
            X, y, log_alpha, model, mask0=mask0, dense0=dense0,
            jac0=quantity_to_warm_start, max_iter=max_iter, tol=tol,
            compute_jac=True, verbose=self.verbose,
            use_stop_crit=self.use_stop_crit)
        if jac is not None:
            jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)
            if full_jac_v:
                jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])
        else:
            jac_v = None

        return mask, dense, jac_v, jac


def compute_beta(
        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,
        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,
        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):
    """
    Parameters
    --------------
    X: array-like, shape (n_samples, n_features)
        Design matrix.
    y: ndarray, shape (n_samples,)
        Observation vector.
    log_alpha: float or np.array, shape (n_features,)
        Logarithm of hyperparameter.
    beta0: ndarray, shape (n_features,)
        initial value of the regression coefficients
        beta for warm start
    dbeta0: ndarray, shape (n_features,)
        initial value of the jacobian dbeta for warm start
    max_iter: int
        number of iterations of the algorithm
    tol: float
        The tolerance for the optimization: if the updates are
        smaller than ``tol``, the optimization code checks the
        primal decrease for optimality and continues until it
        is smaller than ``tol``
    compute_jac: bool
        to compute or not the Jacobian along with the regression
        coefficients
    model:  instance of ``sparse_ho.base.BaseModel``
        A model that follows the sparse_ho API.
    return_all: bool
        to store the iterates or not in order to compute the Jacobian in a
        backward way
    use_stop_crit: bool
        use a stopping criterion or do all the iterations
    gap_freq : int
        After how many passes on the data the dual gap should be computed
        to stop the iterations.

    Returns
    -------
    mask : ndarray, shape (n_features,)
        The mask of non-zero coefficients in beta.
    dense : ndarray, shape (n_nonzeros,)
        The beta coefficients on the support
    jac : ndarray, shape (n_nonzeros,) or (n_nonzeros, q)
        The jacobian restricted to the support. If there are more than
        one hyperparameter then it has two dimensions.
    """
    n_samples, n_features = X.shape
    is_sparse = issparse(X)
    if not is_sparse and not np.isfortran(X):
        X = np.asfortranarray(X)
    L = model.get_L(X)

    ############################################
    alpha = np.exp(log_alpha)

    if hasattr(model, 'estimator') and model.estimator is not None:
        return model._use_estimator(X, y, alpha, tol)

    try:
        alpha.shape[0]
        alphas = alpha.copy()
    except Exception:
        alphas = np.ones(n_features) * alpha
    ############################################
    # warm start for beta
    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)
    ############################################
    # warm start for dbeta
    dbeta, ddual_var = model._init_dbeta_ddual_var(
        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)

    # store the values of the objective
    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)
    pobj = []

    ############################################
    # store the iterates if needed
    if return_all:
        list_beta = []
    if save_iterates:
        list_beta = []
        list_jac = []

    for i in range(max_iter):
        if verbose:
            print("%i -st iteration over %i" % (i, max_iter))
        if is_sparse:
            model._update_beta_jac_bcd_sparse(
                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,
                dbeta, dual_var, ddual_var, alphas, L,
                compute_jac=compute_jac)
        else:
            model._update_beta_jac_bcd(
                X, y, beta, dbeta, dual_var, ddual_var, alphas,
                L, compute_jac=compute_jac)

        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))

        if i > 1:
            if verbose:
                print("relative decrease = ", (pobj[-2] - pobj[-1]) / pobj0)

        if use_stop_crit and i % gap_freq == 0 and i > 0:
            if hasattr(model, "_get_dobj"):
                dobj = model._get_dobj(dual_var, X, beta, alpha, y)
                dual_gap = pobj[-1] - dobj
                if verbose:
                    print("dual gap %.2e" % dual_gap)
                if verbose:
                    print("gap %.2e" % dual_gap)
                if dual_gap < pobj0 * tol:
                    break
            else:
                if (pobj[-2] - pobj[-1] <= pobj0 * tol):
                    break
        if return_all:
            list_beta.append(beta.copy())
        if save_iterates:
            list_beta.append(beta.copy())
            list_jac.append(dbeta.copy())
    else:
        if verbose:
            print('did not converge !')

    mask = beta != 0
    dense = beta[mask]
    jac = model._get_jac(dbeta, mask)
    if hasattr(model, 'dual'):
        model.dual_var = dual_var
        if compute_jac:
            model.ddual_var = ddual_var
    if save_iterates:
        return np.array(list_beta), np.array(list_jac)
    if return_all:
        return mask, dense, list_beta
    else:
        if compute_jac:
            return mask, dense, jac
        else:
            return mask, dense, None

File Path: sparse_ho/algo/implicit.py
Content:
import numpy as np

from scipy.sparse.linalg import cg

from sparse_ho.utils import init_dbeta0_new
from sparse_ho.algo.forward import compute_beta


class Implicit():
    """Algorithm to compute the hypergradient using implicit differentiation.

    First the algorithm computes the regression coefficients beta, then the
    gradient is computed after resolution of a linear system on the generalized
    support of beta.

    Parameters
    ----------
    max_iter: int (default=100)
        Maximum number of iteration for the inner solver.
    max_iter_lin_sys: int (default=100)
        Maximum number of iteration for the resolution of the linear system.
    tol_lin_sys: float (default=1e-6)
        Tolerance for the resolution of the linear system.
    """

    def __init__(self, max_iter=100, max_iter_lin_sys=100, tol_lin_sys=1e-6):
        self.max_iter = max_iter
        self.max_iter_lin_sys = max_iter_lin_sys
        self.tol_lin_sys = tol_lin_sys

    def compute_beta_grad(
            self, X, y, log_alpha, model, get_grad_outer, mask0=None,
            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,
            full_jac_v=False):
        """Compute beta and the hypergradient, with implicit differentiation.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array, shape (n_features,)
            Logarithm of hyperparameter.
        model:  instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        get_grad_outer: callable
            Function which returns the gradient of the outer criterion.
        mask0: ndarray, shape (n_features,)
            Boolean of active feature of the previous regression coefficients
            beta for warm start.
        dense0: ndarray, shape (mask.sum(),)
            Initial value of the previous regression coefficients
            beta for warm start.
        quantity_to_warm_start: ndarray
            Previous solution of the linear system.
        max_iter: int
            Maximum number of iteration for the inner solver.
        tol: float
            The tolerance for the inner optimization problem.
        full_jac_v: bool
            TODO
        """
        mask, dense, jac_v, sol_lin_sys = compute_beta_grad_implicit(
            X, y, log_alpha, get_grad_outer, mask0=mask0, dense0=dense0,
            max_iter=max_iter, tol=tol, sol_lin_sys=quantity_to_warm_start,
            tol_lin_sys=self.tol_lin_sys,
            max_iter_lin_sys=self.max_iter_lin_sys, model=model)

        if full_jac_v:
            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])

        return mask, dense, jac_v, sol_lin_sys


def compute_beta_grad_implicit(
        X, y, log_alpha, get_grad_outer, mask0=None, dense0=None, tol=1e-3,
        model="lasso", max_iter=1000, sol_lin_sys=None,
        tol_lin_sys=1e-6, max_iter_lin_sys=100):
    """Compute beta and the hypergradient with implicit differentiation.

    The hypergradient computation is done in 3 steps:
    - 1 solve the inner optimization problem.
    - 2 solve a linear system on the support (ie the non-zeros coefficients)
    of the solution.
    - 3 use the solution of the linear system to compute the gradient.

    Parameters
    ----------
    X: array-like, shape (n_samples, n_features)
        Design matrix.
    y: ndarray, shape (n_samples,)
        Observation vector.
    log_alpha: float or np.array, shape (n_features,)
        Logarithm of hyperparameter.
    mask0: ndarray, shape (n_features,)
        Boolean of active feature of the previous regression coefficients
        beta for warm start.
    dense0: ndarray, shape (mask.sum(),)
        Initial value of the previous regression coefficients
        beta for warm start.
    tol: float
        The tolerance for the inner optimization problem.
    model:  instance of ``sparse_ho.base.BaseModel``
        A model that follows the sparse_ho API.
    max_iter: int
        Maximum number of iterations for the inner solver.
    sol_lin_sys: ndarray
        Previous solution of the linear system for warm start.
    tol_lin_sys: float
        Tolerance for the resolution of the linear system.
    max_iter_lin_sys: int
        Maximum number of iterations for the resolution of the linear system.
    """

    # 1 compute the regression coefficients beta, stored in mask and dense
    alpha = np.exp(log_alpha)
    mask, dense, _ = compute_beta(
        X, y, log_alpha, mask0=mask0, dense0=dense0,
        tol=tol, max_iter=max_iter, compute_jac=False, model=model)
    n_features = X.shape[1]

    mat_to_inv = model.get_mat_vec(X, y, mask, dense, log_alpha)

    v = get_grad_outer(mask, dense)
    if hasattr(model, 'dual'):
        v = model.get_dual_v(mask, dense, X, y, v, log_alpha)

    # 2 solve the linear system
    # TODO I think this should be removed
    if not alpha.shape:
        alphas = np.ones(n_features) * alpha
    else:
        alphas = alpha.copy()
    if sol_lin_sys is not None and not hasattr(model, 'dual'):
        sol0 = init_dbeta0_new(sol_lin_sys, mask, mask0)
    else:
        sol0 = None  # TODO add warm start for SVM and SVR
    sol = cg(
        mat_to_inv, - model.generalized_supp(X, v, log_alpha),
        x0=sol0, tol=tol_lin_sys, maxiter=max_iter_lin_sys)
    sol_lin_sys = sol[0]

    # 3 compute the gradient
    grad = model._get_grad(X, y, sol_lin_sys, mask, dense, alphas, v)
    return mask, dense, grad, sol_lin_sys

File Path: sparse_ho/algo/implicit_forward.py
Content:
import numpy as np
from scipy.sparse import issparse
from sparse_ho.algo.forward import compute_beta


class ImplicitForward():
    """Algorithm to compute the hypergradient using implicit forward
    differentiation.

    First the algorithm computes the regression coefficients.
    Then the iterations of the forward differentiation are applied to compute
    the Jacobian.

    Parameters
    ----------
    tol_jac: float
        Tolerance for the Jacobian computation.
    max_iter: int
        Maximum number of iterations for the inner solver.
    n_iter_jac: int
        Maximum number of iterations for the Jacobian computation.
    use_stop_crit: bool, optional (default=True)
        Use stopping criterion in hypergradient computation. If False,
        run to maximum number of iterations.
    verbose: bool, optional (default=False)
        Verbosity of the algorithm.
    """

    def __init__(
            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,
            use_stop_crit=True, verbose=False):
        self.max_iter = max_iter
        self.tol_jac = tol_jac
        self.n_iter_jac = n_iter_jac
        self.use_stop_crit = use_stop_crit
        self.verbose = verbose

    def get_beta_jac(
            self, X, y, log_alpha, model, get_grad_outer, mask0=None,
            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,
            full_jac_v=False):
        """Compute beta and hypergradient using implicit forward
        differentiation.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array, shape (n_features,)
            Logarithm of hyperparameter.
        model:  instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        get_grad_outer: callable
            Function which returns the gradient of the outer criterion.
        mask0: ndarray, shape (n_features,)
            Boolean of active feature of the previous regression coefficients
            beta for warm start.
        dense0: ndarray, shape (mask.sum(),)
            Initial value of the previous regression coefficients
            beta for warm start.
        quantity_to_warm_start: ndarray
            Previous Jacobian of the inner optimization problem.
        max_iter: int
            Maximum number of iteration for the inner solver.
        tol: float
            The tolerance for the inner optimization problem.
        full_jac_v: bool
            TODO
        """

        mask, dense, jac = get_bet_jac_implicit_forward(
            X, y, log_alpha, mask0=mask0, dense0=dense0,
            jac0=quantity_to_warm_start,
            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,
            max_iter=self.max_iter, verbose=self.verbose)
        return mask, dense, jac

    def compute_beta_grad(
            self, X, y, log_alpha, model, get_grad_outer, mask0=None,
            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,
            full_jac_v=False):
        mask, dense, jac = get_bet_jac_implicit_forward(
            X, y, log_alpha, mask0=mask0, dense0=dense0,
            jac0=quantity_to_warm_start,
            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,
            model=model, max_iter=self.max_iter, verbose=self.verbose,
            use_stop_crit=self.use_stop_crit)
        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)
        if full_jac_v:
            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])

        return mask, dense, jac_v, jac


def get_bet_jac_implicit_forward(
        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,
        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,
        use_stop_crit=True):

    mask, dense, _ = compute_beta(
        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,
        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,
        use_stop_crit=use_stop_crit)
    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)
    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)

    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)
    jac = get_only_jac(
        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,
        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,
        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,
        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)

    return mask, dense, jac


def get_only_jac(
        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,
        tol_jac=1e-4, model="lasso", mask=None, dense=None, verbose=False,
        use_stop_crit=True):
    n_samples, n_features = Xs.shape

    L = model.get_L(Xs)

    residual_norm = []

    if hasattr(model, 'dual'):
        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)
        dbeta = model.dbeta
    else:
        if dbeta is None:
            dbeta = model._init_dbeta(n_features)
        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)

    for i in range(niter_jac):
        if verbose:
            print("%i -st iterations over %i" % (i, niter_jac))
        if issparse(Xs):
            model._update_only_jac_sparse(
                Xs.data, Xs.indptr, Xs.indices, y, n_samples,
                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)
        else:
            model._update_only_jac(
                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)
        residual_norm.append(
            model.get_jac_residual_norm(
                Xs, y, n_samples, sign_beta, dbeta, dual_var,
                ddual_var, alpha))
        if use_stop_crit and i > 1:
            # relative stopping criterion for the computation of the jacobian
            # and absolute stopping criterion to handle warm start
            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])
            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac
                    or residual_norm[-1] < 1e-10):
                break
    # HACK we only need this for one test, do not rely on it
    get_only_jac.n_iter = i

    return dbeta

File Path: sparse_ho/criterion/__init__.py
Content:
from sparse_ho.criterion.held_out import HeldOutMSE, HeldOutLogistic
from sparse_ho.criterion.cross_val import CrossVal
from sparse_ho.criterion.sure import FiniteDiffMonteCarloSure
from sparse_ho.criterion.held_out import HeldOutSmoothedHinge
from sparse_ho.criterion.multiclass_logreg import LogisticMulticlass

__all__ = ['CrossVal',
           'FiniteDiffMonteCarloSure',
           'HeldOutMSE',
           'HeldOutLogistic',
           'HeldOutSmoothedHinge',
           'LogisticMulticlass']

File Path: sparse_ho/criterion/base.py
Content:
from abc import ABC, abstractmethod


class BaseCriterion(ABC):

    @abstractmethod
    def __init__(cls):
        pass

    @abstractmethod
    def get_val_outer(cls, *args, **kwargs):
        return NotImplemented

    @abstractmethod
    def get_val(cls, *args, **kwargs):
        return NotImplemented

    @abstractmethod
    def get_val_grad(cls, *args, **kwargs):
        return NotImplemented

    # @abstractmethod
    # def proj_hyperparam(cls, *args, **kwargs):
    #     return NotImplemented

File Path: sparse_ho/criterion/cross_val.py
Content:
import copy
import numpy as np
from sklearn.model_selection import check_cv
from sparse_ho.criterion.base import BaseCriterion


class CrossVal(BaseCriterion):
    """Cross-validation loss.

    Parameters
    ----------
    criterion : instance of ``BaseCriterion``
        A criterion that follows the sparse-ho API.
    cv : int, cross-validation generator or iterable, default=None
        Determines the cross-validation splitting strategy.
        Possible inputs for cv are:

        - None, to use the default 5-fold cross-validation,
        - int, to specify the number of folds.
        - scikit-learn CV splitter
        - An iterable yielding (train, test) splits as arrays of indices.

        For int/None inputs, KFold is used.

    Attributes
    ----------
    dict_crits : dict
        The instances of criterion used for each fold.
    """

    # XXX TODO pass criterion as a string, MSE, logistic
    # do directly crossval in MSE and Logistic

    def __init__(self, criterion, cv=None):
        self.criterion = criterion
        self.cv = check_cv(cv)
        self.dict_crits = None
        self.dict_models = None
        self.n_splits = None

    def _initialize(self, model, X):
        self.dict_crits = {}
        self.dict_models = {}
        self.n_splits = self.cv.get_n_splits(X)

        for i, (idx_train, idx_val) in enumerate(self.cv.split(X)):
            self.dict_crits[i] = copy.deepcopy(self.criterion)
            self.dict_crits[i].idx_train = idx_train
            self.dict_crits[i].idx_val = idx_val
            self.dict_models[i] = copy.deepcopy(model)

    def get_val(self, model, X, y, log_alpha, monitor=None, tol=1e-3):
        """Get value of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        monitor: instance of Monitor.
            Monitor.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        """

        if self.dict_crits is None:
            self._initialize(model, X)
        val = np.mean([
            self.dict_crits[i].get_val(
                self.dict_models[i], X, y, log_alpha, tol=tol) for i in range(
                    self.n_splits)])
        monitor(val, None, alpha=np.exp(log_alpha))
        return val

    def get_val_grad(
            self, model, X, y, log_alpha, compute_beta_grad, max_iter=10000,
            tol=1e-5, monitor=None):
        """Get value and gradient of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        compute_beta_grad: callable
            Returns the regression coefficients beta and the hypergradient.
        max_iter: int
            Maximum iteration for the inner optimization problem.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        monitor: instance of Monitor.
            Monitor.
        """
        if self.dict_crits is None:
            self._initialize(model, X)

        val = 0
        grad = 0
        for i in range(self.n_splits):
            vali, gradi = self.dict_crits[i].get_val_grad(
                self.dict_models[i], X, y, log_alpha, compute_beta_grad,
                max_iter=max_iter, tol=tol)
            val += vali
            if gradi is not None:
                grad += gradi
        val /= self.n_splits
        if gradi is not None:
            grad /= self.n_splits
        else:
            grad = None
        if monitor is not None:
            monitor(val, grad, alpha=np.exp(log_alpha))
        return val, grad

    def get_val_outer(cls, *args, **kwargs):
        """Get value of outer criterion.

        This is not implemented because for CV the loss is computed on each
        fold.

        Parameters
        ----------
        cls: TODO
        """
        return NotImplemented

    def proj_hyperparam(self, model, X, y, log_alpha):
        """Project hyperparameter on a range of admissible values.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float
            Logarithm of hyperparameter.
        """
        # TODO is this not done by the models?
        # TODO to improve this proj_hyperparam procedure
        return model.proj_hyperparam(X, y, log_alpha)

File Path: sparse_ho/criterion/held_out.py
Content:
import numpy as np
from numpy.linalg import norm
from scipy.sparse import issparse

from sparse_ho.utils import sigma, smooth_hinge
from sparse_ho.utils import derivative_smooth_hinge
from sparse_ho.algo.forward import compute_beta
from sparse_ho.criterion.base import BaseCriterion


class HeldOutMSE(BaseCriterion):
    """Held out loss for quadratic datafit.

    Parameters
    ----------
    idx_train: ndarray
        indices of the training set
    idx_val: ndarray
        indices of the validation set
    """
    # XXX : this code should be the same as CrossVal as you can pass
    # cv as [(train, test)] ie directly the indices of the train
    # and test splits.

    def __init__(self, idx_train, idx_val):
        self.idx_train = idx_train
        self.idx_val = idx_val

        self.mask0 = None
        self.dense0 = None
        self.quantity_to_warm_start = None

    def get_val_outer(self, X, y, mask, dense):
        """Compute the MSE on the validation set.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: array-like, shape (n_features,)
            Boolean array corresponding to the non-zeros coefficients.
        dense: ndarray
            Values of the non-zeros coefficients.
        """
        return norm(y - X[:, mask] @ dense) ** 2 / len(y)

    def get_val(self, model, X, y, log_alpha, monitor=None, tol=1e-3):
        """Get value of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        monitor: instance of Monitor.
            Monitor.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        """
        mask, dense, _ = compute_beta(
            X[self.idx_train], y[self.idx_train], log_alpha, model,
            mask0=self.mask0, dense0=self.dense0, tol=tol,
            compute_jac=False)
        value_outer = self.get_val_outer(
            X[self.idx_val, :], y[self.idx_val], mask, dense)

        self.mask0 = mask
        self.dense0 = dense

        if monitor is not None:
            monitor(value_outer, None, alpha=np.exp(log_alpha))
        return value_outer

    def get_val_grad(
            self, model, X, y, log_alpha, compute_beta_grad, max_iter=10000,
            tol=1e-5, monitor=None):
        """Get value and gradient of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        compute_beta_grad: callable
            Returns the regression coefficients beta and the hypergradient.
        max_iter: int
            Maximum number of iteration for the inner problem.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        monitor: instance of Monitor.
            Monitor.
        """

        X_train, X_val = X[self.idx_train, :], X[self.idx_val, :]
        y_train, y_val = y[self.idx_train], y[self.idx_val]

        def get_grad_outer(mask, dense):
            X_val_m = X_val[:, mask]
            return 2 * (X_val_m.T @ (X_val_m @ dense - y_val)) / len(y_val)
        mask, dense, grad, quantity_to_warm_start = compute_beta_grad(
            X_train, y_train, log_alpha, model,
            get_grad_outer, mask0=self.mask0, dense0=self.dense0,
            quantity_to_warm_start=self.quantity_to_warm_start,
            max_iter=max_iter, tol=tol, full_jac_v=True)

        self.mask0 = mask
        self.dense0 = dense
        self.quantity_to_warm_start = quantity_to_warm_start
        val = self.get_val_outer(X_val, y_val, mask, dense)
        if monitor is not None:
            monitor(val, grad, mask, dense, alpha=np.exp(log_alpha))
        return val, grad

    def proj_hyperparam(self, model, X, y, log_alpha):
        """Project hyperparameter on a range of admissible values.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float
            Logarithm of hyperparameter.
        """
        return model.proj_hyperparam(
            X[self.idx_train, :], y[self.idx_train], log_alpha)


class HeldOutLogistic(BaseCriterion):
    """Logistic loss on held out data

    Parameters
    ----------
    idx_train: ndarray
        indices of the training set
    idx_val: ndarray
        indices of the validation set
    """

    def __init__(self, idx_train, idx_val):
        self.idx_train = idx_train
        self.idx_val = idx_val

        self.mask0 = None
        self.dense0 = None
        self.quantity_to_warm_start = None

    @staticmethod
    def get_val_outer(X, y, mask, dense):
        """Compute the logistic loss on the validation set.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: array-like, shape (n_features,)
            Boolean array corresponding to the non-zeros coefficients.
        dense: ndarray
            Values of the non-zeros coefficients.
        """
        val = np.sum(np.log(1 + np.exp(-y * (X[:, mask] @ dense))))
        val /= X.shape[0]
        return val

    def get_val(self, model, X, y, log_alpha, monitor=None, tol=1e-3):
        """Get value of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        monitor: instance of Monitor.
            Monitor.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        """
        mask, dense, _ = compute_beta(
            X[self.idx_train], y[self.idx_train], log_alpha, model,
            mask0=self.mask0, dense0=self.dense0, tol=tol, compute_jac=False)
        val = self.get_val_outer(
            X[self.idx_val, :], y[self.idx_val], mask, dense)

        self.mask0 = mask
        self.dense0 = dense

        if monitor is not None:
            monitor(val, None, mask, dense, alpha=np.exp(log_alpha))
        return val

    def get_val_grad(
            self, model, X, y, log_alpha, compute_beta_grad, max_iter=10000,
            tol=1e-5, monitor=None):
        """Get value and gradient of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        compute_beta_grad: callable
            Returns the regression coefficients beta and the hypergradient.
        max_iter: int
            Maximum number of iteration for the inner problem.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        monitor: instance of Monitor.
            Monitor.
        """

        X_train, X_val = X[self.idx_train, :], X[self.idx_val, :]
        y_train, y_val = y[self.idx_train], y[self.idx_val]

        def get_grad_outer(mask, dense):
            X_val_m = X_val[:, mask]
            temp = sigma(y_val * (X_val_m @ dense))
            v = X_val_m.T @ (y_val * (temp - 1))
            v /= len(y_val)
            return v

        mask, dense, grad, quantity_to_warm_start = compute_beta_grad(
            X_train, y_train, log_alpha, model, get_grad_outer, mask0=self.
            mask0, dense0=self.dense0,
            quantity_to_warm_start=self.quantity_to_warm_start,
            max_iter=max_iter, tol=tol, full_jac_v=True)

        self.mask0 = mask
        self.dense0 = dense
        self.quantity_to_warm_start = quantity_to_warm_start
        val = self.get_val_outer(X_val, y_val, mask, dense)
        if monitor is not None:
            monitor(val, grad, mask, dense, alpha=np.exp(log_alpha))

        return val, grad

    def proj_hyperparam(self, model, X, y, log_alpha):
        """Project hyperparameter on a range of admissible values.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float
            Logarithm of hyperparameter.
        """
        return model.proj_hyperparam(
            X[self.idx_train, :], y[self.idx_train], log_alpha)


class HeldOutSmoothedHinge(BaseCriterion):
    """Smooth Hinge loss.

    Parameters
    ----------
    idx_train: ndarray
        indices of the training set
    idx_val: ndarray
        indices of the validation set
    """

    def __init__(self, idx_train, idx_val):
        """
        Parameters:
        ----------
        idx_train: ndarray
            indices of the training set
        idx_val: ndarray
            indices of the validation set
        """
        self.idx_train = idx_train
        self.idx_val = idx_val

        self.mask0 = None
        self.dense0 = None
        self.quantity_to_warm_start = None

    def get_val_outer(self, X, y, mask, dense):
        """Compute the smoothed Hinge on the validation set.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: array-like, shape (n_features,)
            Boolean array corresponding to the non-zeros coefficients.
        dense: ndarray
            Values of the non-zeros coefficients.
        """

        if issparse(X):
            Xbeta_y = (X[:, mask].T).multiply(y).T @ dense
        else:
            Xbeta_y = y * (X[:, mask] @ dense)
        return np.sum(smooth_hinge(Xbeta_y)) / len(y)

    def get_val_grad(
            self, model, X, y, log_alpha, compute_beta_grad, max_iter=10000,
            tol=1e-5, monitor=None):
        """Get value and gradient of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        compute_beta_grad: callable
            Returns the regression coefficients beta and the hypergradient.
        max_iter: int
            Maximum number of iteration for the inner problem.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        monitor: instance of Monitor.
            Monitor.
        """

        X_train, X_val = X[self.idx_train, :], X[self.idx_val, :]
        y_train, y_val = y[self.idx_train], y[self.idx_val]

        def get_grad_outer(mask, dense):
            X_val_m = X_val[:, mask]
            Xbeta_y = y_val * (X_val_m @ dense)
            deriv = derivative_smooth_hinge(Xbeta_y)
            if issparse(X):
                v = X_val_m.T.multiply(deriv * y_val)
                v = np.array(np.sum(v, axis=1))
                v = np.squeeze(v)
            else:
                v = (deriv * y_val)[:, np.newaxis] * X_val_m
                v = np.sum(v, axis=0)
            v /= len(self.idx_val)
            return v

        mask, dense, grad, quantity_to_warm_start = compute_beta_grad(
            X_train, y_train, log_alpha, model, get_grad_outer,
            mask0=self.mask0, dense0=self.dense0,
            quantity_to_warm_start=self.quantity_to_warm_start,
            max_iter=max_iter, tol=tol, full_jac_v=True)

        self.mask0 = mask
        self.dense0 = dense
        self.quantity_to_warm_start = quantity_to_warm_start

        val = self.get_val_outer(X_val, y_val, mask, dense)

        if monitor is not None:
            monitor(val, grad, mask, dense, alpha=np.exp(log_alpha))

        return val, grad

    def get_val(self, model, X, y, log_alpha, tol=1e-3):
        """Get value of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        """
        # TODO add maxiter param for all get_val
        mask, dense, _ = compute_beta(
            X, y, log_alpha, model,
            tol=tol, compute_jac=False)
        val = self.get_val_outer(
            X[self.idx_val], y[self.idx_val], mask, dense)
        return val

    def proj_hyperparam(self, model, X, y, log_alpha):
        """Project hyperparameter on a range of admissible values.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float
            Logarithm of hyperparameter.
        """
        return model.proj_hyperparam(
            X[self.idx_train, :], y[self.idx_train], log_alpha)

File Path: sparse_ho/criterion/multiclass_logreg.py
Content:
import copy
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sparse_ho.utils_cross_entropy import (
    cross_entropy, grad_cross_entropy, accuracy)


class LogisticMulticlass():
    """Multiclass logistic loss.

    Parameters
    ----------
    idx_train: ndarray
        indices of the training set
    idx_val: ndarray
        indices of the validation set
    algo: instance of ``sparse_ho.base.AlgoModel``
        A model that follows the sparse_ho API.
    idx_test: ndarray
        indices of the test set

    Attributes
    ----------
    dict_models: dict
        dict with the models corresponding to each class.
    """

    def __init__(self, idx_train, idx_val, algo, idx_test=None):
        self.idx_train = idx_train
        self.idx_val = idx_val
        # passing test is dirty but we need it for the multiclass logreg
        self.idx_test = idx_test
        # passing algo is dirty but we need it for the multiclass logreg
        self.algo = algo
        self.dict_models = None

    def _initialize(self, model, X, y):
        enc = OneHotEncoder(sparse=False)  # maybe remove the sparse=False
        # split data set in test validation and train
        self.one_hot_code = enc.fit_transform(pd.DataFrame(y))

        self.n_classes = self.one_hot_code.shape[1]

        # dict with all the one vs all models
        self.dict_models = {}
        for k in range(self.n_classes):
            self.dict_models[k] = copy.deepcopy(model)
        self.dict_warm_start = {}
        self.n_samples, self.n_features = X.shape

    def get_val_grad(
            self, model, X, y, log_alpha, compute_beta_grad, monitor,
            tol=1e-3):
        """Get value and gradient of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        compute_beta_grad: callable
            Returns the regression coefficients beta and the hypergradient.
        monitor: instance of Monitor.
            Monitor.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        """
        # TODO use sparse matrices
        if self.dict_models is None:
            self._initialize(model, X, y)
        all_betas = np.zeros((self.n_features, self.n_classes))
        all_jacs = np.zeros((self.n_features, self.n_classes))
        for k in range(self.n_classes):
            mask0, dense0, jac0 = self.dict_warm_start.get(
                k, (None, None, None))
            mask, dense, jac = self.algo.get_beta_jac(
                X[self.idx_train, :], self.one_hot_code[self.idx_train, k],
                log_alpha[k], self.dict_models[k], None, mask0=mask0,
                dense0=dense0,
                quantity_to_warm_start=jac0, tol=tol)
            self.dict_warm_start[k] = (mask, dense, jac)
            all_betas[mask, k] = dense  # maybe use np.ix_
            all_jacs[mask, k] = jac  # maybe use np.ix_
        acc_val = accuracy(
            all_betas, X[self.idx_val, :], self.one_hot_code[self.idx_val, :])
        val = cross_entropy(
            all_betas, X[self.idx_val, :], self.one_hot_code[self.idx_val, :])
        grad = self.grad_total_loss(
            all_betas, all_jacs, X[self.idx_val, :],
            self.one_hot_code[self.idx_val, :])

        if self.idx_test is not None:
            acc_test = accuracy(
                all_betas, X[self.idx_test, :], self.one_hot_code[
                    self.idx_test, :])
            print(
                "Value outer %f || Acc. validation %f || Acc. test %f" %
                (val, acc_val, acc_test))
        else:
            acc_test = None
            print("Value outer %f || Acc. validation %f" %
                  (val, acc_val))

        monitor(
            val, alpha=np.exp(log_alpha), grad=grad.copy(), acc_val=acc_val,
            acc_test=acc_test)

        self.all_betas = all_betas
        return val, grad

    def get_val(
            self, model, X, y, log_alpha, compute_beta_grad, monitor,
            tol=1e-3):
        # TODO not the same as for other losses?
        """Get value of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        compute_beta_grad: callable
            Returns the regression coefficients beta and the hypergradient.
        monitor: instance of Monitor.
            Monitor.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        """
        if self.dict_models is None:
            self._initialize(model, X, y)
        all_betas = np.zeros((self.n_features, self.n_classes))
        for k in range(self.n_classes):
            mask0, dense0, jac0 = self.dict_warm_start.get(
                k, (None, None, None))
            mask, dense, jac = self.algo.get_beta_jac(
                X[self.idx_train, :], self.one_hot_code[self.idx_train, k],
                log_alpha[k], self.dict_models[k], None, mask0=mask0,
                dense0=dense0,
                quantity_to_warm_start=jac0, tol=tol)
            self.dict_warm_start[k] = (mask, dense, jac)
            all_betas[mask, k] = dense  # maybe use np.ix_
        acc_val = accuracy(
            all_betas, X[self.idx_val, :], self.one_hot_code[self.idx_val, :])
        acc_test = accuracy(
            all_betas, X[self.idx_test, :],
            self.one_hot_code[self.idx_test, :])
        val = cross_entropy(
            all_betas, X[self.idx_val, :], self.one_hot_code[self.idx_val, :])
        monitor(
            val, alpha=np.exp(log_alpha), grad=None, acc_val=acc_val,
            acc_test=acc_test)
        print("Value outer %f || Accuracy validation %f || Accuracy test %f" %
              (val, acc_val, acc_test))
        self.all_betas = all_betas
        return val

    def proj_hyperparam(self, model, X, y, log_alpha):
        """Project hyperparameter on admissible range of values

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        """
        # TODO doesn't an other object do this?
        # TODO model not needed I think
        log_alpha_max = model.compute_alpha_max(X, y)
        log_alpha[log_alpha < log_alpha_max - 7] = log_alpha_max - 7
        log_alpha[log_alpha > log_alpha_max - np.log(0.9)] = (
            log_alpha_max - np.log(0.9))
        return log_alpha

    def grad_total_loss(self, all_betas, all_jacs, X, Y):
        """Compute the gradient of the multiclass logistic loss.

        Parameters
        ----------
        all_betas: array-like, shape (n_features, n_classes)
            Solutions of the optimization problems corresponding to each class.
        all_jacs: array-like, shape (n_features, n_classes)
            Jacobians of the optimization problems corresponding to each class.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        Y: ndarray, shape (n_samples, n_classes)
            One hot encoding representation of the observation y.
        """
        grad_ce = grad_cross_entropy(all_betas, X, Y)
        grad_total = (grad_ce * all_jacs).sum(axis=0)
        return grad_total

    # def grad_k_loss(self, all_betas, jack, X, Y, k):
    #     grad_ce = grad_cross_entropyk(all_betas, X, Y, k)
    #     grad_k = grad_ce @ jack
    #     return grad_k

File Path: sparse_ho/criterion/sure.py
Content:
import numpy as np
from numpy.linalg import norm
from sklearn.utils import check_random_state

from sparse_ho.algo.forward import compute_beta
from sparse_ho.criterion.base import BaseCriterion


class FiniteDiffMonteCarloSure(BaseCriterion):
    """Smoothed version of the Stein Unbiased Risk Estimator (SURE).

    Implements the iterative Finite-Difference Monte-Carlo approximation of the
    SURE. By default, the approximation is ruled by a power law heuristic [1].

    Parameters
    ----------
    sigma: float
        Noise level
    finite_difference_step: float, optional
        Finite difference step used in the approximation of the SURE.
        By default, use a power law heuristic.
    random_state : int, RandomState instance, default=42
        The seed of the pseudo random number generator.
        Pass an int for reproducible output across multiple function calls.

    Attributes
    ----------
    Finite differentiation Monte Carlo SURE relies on the resolution of 2
    optimization problems.
    mask0: array-like, shape (n_features,)
        Boolean array corresponding to the non-zeros coefficients of the
        solution of the first optimization problem.
    mask02: array-like, shape (n_features,)
        Boolean array corresponding to the non-zeros coefficients of the
        solution of the second optimization problem.
    dense: ndarray
        Values of the non-zeros coefficients of the
        solution of the first optimization problem.
    dense2: ndarray
        Values of the non-zeros coefficients of the
        solution of the second optimization problem.

    References
    ----------
    .. [1] C.-A. Deledalle, Stein Unbiased GrAdient estimator of the Risk
    (SUGAR) for multiple parameter selection.
    SIAM J. Imaging Sci., 7(4), 2448-2487.
    """

    def __init__(self, sigma, finite_difference_step=None,
                 random_state=42):
        self.sigma = sigma
        self.random_state = random_state
        self.finite_difference_step = finite_difference_step
        self.init_delta_epsilon = False

        self.mask0 = None
        self.dense0 = None
        self.quantity_to_warm_start = None

        self.mask02 = None
        self.dense02 = None
        self.quantity_to_warm_start2 = None

        self.rmse = None

    def get_val_outer(self, X, y, mask, dense, mask2, dense2):
        """Compute the value of the smoothed version of the
        Stein Unbiased Risk Estimator (SURE).

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: array-like, shape (n_features,)
            Boolean array corresponding to the non-zeros coefficients of the
            solution of the first optimization problem.
        dense: ndarray
            Values of the non-zeros coefficients of the
            solution of the first optimization problem.
        mask2: array-like, shape (n_features,)
            Boolean array corresponding to the non-zeros coefficients of the
            solution of the second optimization problem.
        dense2: ndarray
            Values of the non-zeros coefficients of the
            solution of the second optimization problem.
        """
        X_m = X[:, mask]
        dof = ((X[:, mask2] @ dense2 - X_m @ dense) @ self.delta)
        dof /= self.epsilon
        # compute the value of the sure
        val = norm(y - X_m @ dense) ** 2
        val -= len(y) * self.sigma ** 2
        val += 2 * self.sigma ** 2 * dof
        return val

    def get_val(self, model, X, y, log_alpha, monitor=None, tol=1e-3):
        """Get value of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        monitor: instance of Monitor.
            Monitor.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        """
        if not self.init_delta_epsilon:
            self._init_delta_epsilon(X)
        mask, dense, _ = compute_beta(
            X, y, log_alpha, model,
            tol=tol, mask0=self.mask0, dense0=self.dense0, compute_jac=False)
        mask2, dense2, _ = compute_beta(
            X, y + self.epsilon * self.delta, log_alpha, model,
            mask0=self.mask02, dense0=self.dense02, tol=tol, compute_jac=False)

        self.mask0 = None
        self.dense0 = None
        self.mask02 = None
        self.dense02 = None

        val = self.get_val_outer(X, y, mask, dense, mask2, dense2)
        if monitor is not None:
            monitor(val, None, mask, dense, alpha=np.exp(log_alpha))
        return val

    def _init_delta_epsilon(self, X):
        if self.finite_difference_step:
            self.epsilon = self.finite_difference_step
        else:
            # Use Deledalle et al. 2014 heuristic
            self.epsilon = 2.0 * self.sigma / (X.shape[0]) ** 0.3
        rng = check_random_state(self.random_state)
        self.delta = rng.randn(X.shape[0])  # sample random noise for MCMC step
        self.init_delta_epsilon = True

    def get_val_grad(
            self, model, X, y, log_alpha, compute_beta_grad, max_iter=1000,
            tol=1e-3, monitor=None):
        """Get value and gradient of criterion.

        Parameters
        ----------
        model: instance of ``sparse_ho.base.BaseModel``
            A model that follows the sparse_ho API.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float or np.array
            Logarithm of hyperparameter.
        compute_beta_grad: callable
            Returns the regression coefficients beta and the hypergradient.
        max_iter: int
            Maximum number of iteration for the inner problem.
        tol: float, optional (default=1e-3)
            Tolerance for the inner problem.
        monitor: instance of Monitor.
            Monitor.
        """
        if not self.init_delta_epsilon:
            self._init_delta_epsilon(X)

        def v(mask, dense):
            X_m = X[:, mask]  # avoid multiple calls to X[:, mask]
            return (2 * X_m.T @ (
                    X_m @ dense - y -
                    self.delta * self.sigma ** 2 / self.epsilon))

        def v2(mask, dense):
            return ((2 * self.sigma ** 2 *
                     X[:, mask].T @ self.delta / self.epsilon))

        mask, dense, jac_v, quantity_to_warm_start = compute_beta_grad(
            X, y, log_alpha, model, v,
            mask0=self.mask0, dense0=self.dense0,
            quantity_to_warm_start=self.quantity_to_warm_start,
            max_iter=max_iter, tol=tol, full_jac_v=True)
        mask2, dense2, jac_v2, quantity_to_warm_start2 = compute_beta_grad(
            X, y + self.epsilon * self.delta,
            log_alpha, model, v2, mask0=self.mask02,
            dense0=self.dense02,
            quantity_to_warm_start=self.quantity_to_warm_start2,
            max_iter=max_iter, tol=tol, full_jac_v=True)
        val = self.get_val_outer(X, y, mask, dense, mask2, dense2)
        self.mask0 = mask
        self.dense0 = dense
        self.quantity_to_warm_start = quantity_to_warm_start

        self.mask02 = mask2
        self.dense02 = dense2
        self.quantity_to_warm_start2 = quantity_to_warm_start2

        if jac_v is not None and jac_v2 is not None:
            grad = jac_v + jac_v2
        else:
            grad = None
        if monitor is not None:
            monitor(val, grad, mask, dense, alpha=np.exp(log_alpha))

        return val, grad

File Path: sparse_ho/datasets/__init__.py
Content:

File Path: sparse_ho/datasets/utils_datasets.py
Content:
import numpy as np
from numpy.linalg import norm

import scipy

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

import pandas as pd


def get_splits(X, y, train_size=0.333):
    """
    Parameters
    ----------
    X: array-like, shape (n_samples, n_features)
        Design matrix.
    y: ndarray, shape (n_samples,)
        Observation vector.
    train_size: float
        Proportion of the dataset to be used for the training
    """

    idx_train, idx = train_test_split(
        np.arange(len(y)), stratify=y, train_size=train_size)

    idx_val, idx_test = train_test_split(idx, stratify=y[idx], test_size=0.5)

    return idx_train, idx_val, idx_test


def clean_dataset(X, y, n_samples, n_features, seed=0):
    """Reduce the number of features and / or samples.
    And remove lines or columns with only 0.

    Parameters
    ----------
    X: array-like, shape (n_samples, n_features)
        Design matrix.
    y: ndarray, shape (n_samples,)
        Observation vector.
    n_samples: int
        Number of samples to keep
    n_features: int
        Number of features to keep
    seed: int
        Seed for the random selection of the samples or features
    """
    np.random.seed(seed)
    idx = np.random.choice(
        X.shape[0], min(n_samples, X.shape[0]), replace=False)
    feats = np.random.choice(
        X.shape[1], min(n_features, X.shape[1]), replace=False)
    X = X[idx, :]
    X = X[:, feats]
    y = y[idx]

    bool_to_keep = scipy.sparse.linalg.norm(X, axis=0) != 0
    X = X[:, bool_to_keep]
    bool_to_keep = scipy.sparse.linalg.norm(X, axis=1) != 0
    X = X[bool_to_keep, :]
    y = y[bool_to_keep]

    ypd = pd.DataFrame(y)
    bool_to_keep = ypd.groupby(0)[0].transform(len) > 2
    ypd = ypd[bool_to_keep]
    X = X[bool_to_keep.to_numpy(), :]
    y = y[bool_to_keep.to_numpy()]

    bool_to_keep = scipy.sparse.linalg.norm(X, axis=0) != 0
    X = X[:, bool_to_keep]
    bool_to_keep = scipy.sparse.linalg.norm(X, axis=1) != 0
    X = X[bool_to_keep, :]
    y = y[bool_to_keep]

    return X, y


def alpha_max_multiclass(X, y):
    """
    Parameters
    ----------
    X: array-like, shape (n_samples, n_features)
        Design matrix.
    y: ndarray, shape (n_samples,)
        Observation vector.
    """
    ypd = pd.DataFrame(y)
    one_hot_code = OneHotEncoder(sparse=False).fit_transform(ypd)
    n_classes = one_hot_code.shape[1]

    alpha_max = np.infty
    for k in range(n_classes):
        alpha_max = min(alpha_max, norm(
            X.T @ (2 * one_hot_code[:, k] - 1), ord=np.inf) / (2 * X.shape[0]))
        if alpha_max == 0:
            raise ValueError("X and y are uncorrelated")
    return alpha_max, n_classes

File Path: sparse_ho/grid_search.py
Content:
# This files contains the functions to perform zero order descent for HO
# hyperparameter setting
import numpy as np


def grid_search(
        criterion, model, X, y, alpha_min, alpha_max, monitor,
        max_evals=50, tol=1e-5, nb_hyperparam=1,
        random_state=42, samp="grid", alphas=None,
        t_max=100_000, reverse=True):
    if alphas is None and samp == "grid":
        if reverse:
            alphas = np.geomspace(alpha_max, alpha_min, max_evals)
        else:
            alphas = np.linspace(alpha_min, alpha_max, max_evals)
        if nb_hyperparam == 2:
            alphas = np.array(np.meshgrid(
                alphas, alphas)).T.reshape(-1, 2)

    elif samp == "random":
        rng = np.random.RandomState(random_state)
        # sample uniformly on log scale
        alphas = np.exp(rng.uniform(
            np.log(alpha_min), np.log(alpha_max), size=max_evals))
        if reverse:
            alphas = np.sort(alphas)[::-1]
        else:
            alphas = np.sort(alphas)
        if nb_hyperparam == 2:
            alphas2 = np.exp(rng.uniform(
                np.log(alpha_min), np.log(alpha_max), size=max_evals))
            if reverse:
                alphas2 = np.sort(alphas2)[::-1]
            else:
                alphas2 = np.sort(alphas2)
            alphas = np.array(np.meshgrid(
                alphas, alphas2)).T.reshape(-1, 2)

    min_g_func = np.inf
    alpha_opt = alphas[0]

    for i, alpha in enumerate(alphas):
        print("Iteration %i / %i" % (i+1, len(alphas)))

        g_func = criterion.get_val(
            model, X, y, np.log(alpha), monitor, tol=tol)

        if g_func < min_g_func:
            min_g_func = g_func
            alpha_opt = alpha

        if monitor.times[-1] > t_max:
            break
    return alpha_opt, min_g_func

File Path: sparse_ho/ho.py
Content:

# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
#
# License: BSD (3-clause)

# This files contains the functions to perform first order descent for HO
# hyperparameter setting


import numpy as np
from hyperopt import hp
from hyperopt import fmin, tpe, rand
from functools import partial
from sklearn.utils import check_random_state


def grad_search(
        algo, criterion, model, optimizer, X, y, alpha0, monitor):
    """
    Parameters
    ----------
    algo: instance of BaseAlgo
        algorithm used to compute hypergradient.
    criterion:  instance of BaseCriterion
        criterion to optimize during hyperparameter optimization
        (outer optimization problem).
    model: instance of BaseModel
        model on which hyperparameter has to be selected
        (inner optimization problem).
    optimizer: instance of Optimizer
        optimizer used to minimize the criterion (outer optimization)
    X: array like of shape (n_samples, n_features)
        Design matrix.
    y: array like of shape (n_samples,)
        Target.
    alpha0: float
        initial value of the hyperparameter alpha.
    monitor: instance of Monitor
        used to store the value of the cross-validation function.


    Returns
    -------
    XXX missing
    """

    def _get_val_grad(log_alpha, tol, monitor):
        return criterion.get_val_grad(
            model, X, y, log_alpha, algo.compute_beta_grad, tol=tol,
            monitor=monitor)

    def _proj_hyperparam(log_alpha):
        return criterion.proj_hyperparam(model, X, y, log_alpha)

    return optimizer._grad_search(
        _get_val_grad, _proj_hyperparam, np.log(alpha0), monitor)


def hyperopt_wrapper(
        algo, criterion, model, X, y, alpha_min, alpha_max, monitor,
        max_evals=50, tol=1e-5, random_state=42, t_max=100_000,
        method='bayesian', size_space=1):
    """
    Parameters
    ----------
    algo: instance of BaseAlgo
        algorithm used to compute hypergradient.
    criterion:  instance of BaseCriterion
        criterion to optimize during hyperparameter optimization
        (outer optimization problem).
    model:  instance of BaseModel
        model on which hyperparameter has to be selected
        (inner optimization problem).
    X: array like of shape (n_samples, n_features)
        Design matrix.
    y: array like of shape (n_samples,)
        Target.
    alpha_min: float
        minimum value for the regularization coefficient alpha.
    alpha_max: float
        maximum value for the regularization coefficient alpha.
    monitor: instance of Monitor
        used to store the value of the cross-validation function.
    max_evals: int (default=50)
        maximum number of evaluation of the function
    tol: float (default=1e-5)
        tolerance for TODO
    random_state: int or instance of RandomState
        Random number generator used for reproducibility.
    t_max: int, optional (default=100_000)
        TODO
    method: 'random' | 'bayesian' (default='bayesian')
        method for hyperopt
    size_space: int (default=1)
        size of the hyperparameter space

    Returns
    -------
    monitor:
        The instance of Monitor used during iterations.
    """

    def objective(log_alpha):
        log_alpha = np.array(log_alpha)
        val_func = criterion.get_val(
            model, X, y, log_alpha, monitor, tol=tol)
        return val_func

    # TODO, also size_space = n_hyperparam ?
    space = [
        hp.uniform(str(dim), np.log(alpha_min), np.log(alpha_max)) for
        dim in range(size_space)]

    rng = check_random_state(random_state)

    if method == "bayesian":
        algo = partial(tpe.suggest, n_startup_jobs=5)
        fmin(
            objective, space, algo=algo, max_evals=max_evals,
            timeout=t_max, rstate=rng)
    elif method == "random":
        fmin(
            objective, space, algo=rand.suggest, max_evals=max_evals,
            timeout=t_max, rstate=rng)
    return monitor

File Path: sparse_ho/models/__init__.py
Content:
from sparse_ho.models.lasso import Lasso
from sparse_ho.models.enet import ElasticNet
from sparse_ho.models.svm import SVM
from sparse_ho.models.svr import SVR
from sparse_ho.models.ssvr import SimplexSVR
from sparse_ho.models.wlasso import WeightedLasso
from sparse_ho.models.logreg import SparseLogreg

__all__ = ['Lasso',
           'ElasticNet',
           'SVM',
           'SVR',
           'SimplexSVR',
           'WeightedLasso',
           'SparseLogreg']

File Path: sparse_ho/models/base.py
Content:
from abc import ABC, abstractmethod


class BaseModel(ABC):

    @abstractmethod
    def __init__(cls):
        pass

    @abstractmethod
    def get_mat_vec(cls, *args, **kwargs):
        return NotImplemented

File Path: sparse_ho/models/enet.py
Content:
import numpy as np
from numpy.linalg import norm
import scipy.sparse.linalg as slinalg
from scipy.sparse import issparse
from scipy.sparse.linalg import LinearOperator

from numba import njit

from sparse_ho.models.base import BaseModel
from sparse_ho.utils import prox_elasticnet, ST


class ElasticNet(BaseModel):
    """Sparse ho ElasticNet model (inner problem).

    Parameters
    ----------
    estimator: sklearn estimator
        Estimator used to solve the optimization problem. Must follow the
        scikit-learn API.
    """

    def __init__(self, estimator=None):
        self.estimator = estimator

    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,
                              dense0=None, compute_jac=True):
        n_samples, n_features = X.shape
        dbeta = np.zeros((n_features, 2))
        if jac0 is None or not compute_jac:
            ddual_var = np.zeros((n_samples, 2))
        else:
            dbeta[mask0, :] = jac0.copy()
            ddual_var = - X[:, mask0] @ jac0.copy()
        return dbeta, ddual_var

    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):
        beta = np.zeros(X.shape[1])
        if dense0 is None or len(dense0) == 0:
            dual_var = y.copy()
            dual_var = dual_var.astype(np.float)
        else:
            beta[mask0] = dense0.copy()
            dual_var = y - X[:, mask0] @ dense0
        return beta, dual_var

    @staticmethod
    @njit
    def _update_beta_jac_bcd(
            X, y, beta, dbeta, dual_var, ddual_var, alpha,
            L, compute_jac=True):
        n_samples, n_features = X.shape
        non_zeros = np.where(L != 0)[0]
        for j in non_zeros:
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j, :].copy()
                # compute derivatives
            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)
            beta[j] = prox_elasticnet(zj, alpha[0] / L[j], alpha[1] / L[j])
            if compute_jac:
                dzj = dbeta[j, :] + X[:, j] @ ddual_var / (L[j] * n_samples)
                dbeta[j:j+1, :] = (1 / (1 + alpha[1] / L[j])) * \
                    np.abs(np.sign(beta[j])) * dzj
                dbeta[j:j+1, 0] -= (alpha[0] * np.sign(beta[j])
                                    ) / L[j] / (1 + alpha[1] / L[j])
                dbeta[j:j+1, 1] -= (alpha[1] / L[j] * beta[j]
                                    ) / (1 + alpha[1] / L[j])
                # update residuals
                ddual_var[:, 0] -= X[:, j] * (dbeta[j, 0] - dbeta_old[0])
                ddual_var[:, 1] -= X[:, j] * (dbeta[j, 1] - dbeta_old[1])
            dual_var -= X[:, j] * (beta[j] - beta_old)

    @staticmethod
    @njit
    def _update_beta_jac_bcd_sparse(
            data, indptr, indices, y, n_samples, n_features, beta,
            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):

        non_zeros = np.where(L != 0)[0]

        for j in non_zeros:
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get the non zero indices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j, :].copy()
            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)
            beta[j:j+1] = prox_elasticnet(zj,
                                          alphas[0] / L[j], alphas[1] / L[j])
            if compute_jac:
                dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \
                    (L[j] * n_samples)
                dbeta[j:j+1, :] = (1 / (1 + alphas[1] / L[j])) * \
                    np.abs(np.sign(beta[j])) * dzj
                dbeta[j:j+1, 0] -= alphas[0] * \
                    np.sign(beta[j]) / L[j] / (1 + (alphas[1] / L[j]))
                dbeta[j:j+1, 1] -= (alphas[1] / L[j] * beta[j]
                                    ) / (1 + (alphas[1] / L[j]))
                # update residuals
                ddual_var[idx_nz, 0] -= Xjs * (dbeta[j, 0] - dbeta_old[0])
                ddual_var[idx_nz, 1] -= Xjs * (dbeta[j, 1] - dbeta_old[1])
            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)

    @staticmethod
    # @njit
    def _update_bcd_jac_backward(X, alphas, grad, beta, v_t_jac, L):
        sign_beta = np.sign(beta)
        n_samples, n_features = X.shape
        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):
            grad[0] -= (v_t_jac[j]) * alphas[0] * \
                sign_beta[j] / L[j] / (1 + (alphas[1] / L[j]))
            grad[1] -= (v_t_jac[j]) * (alphas[1] / L[j] * beta[j]) / \
                (1 + (alphas[1] / L[j]))
            v_t_jac[j] *= (1 / (1 + alphas[1] / L[j])) * \
                np.abs(np.sign(beta[j]))
            v_t_jac -= v_t_jac[j] / (L[j] * n_samples) * X[:, j] @ X

        return grad

    @staticmethod
    def _get_pobj0(dual_var, beta, alphas, y=None):
        n_samples = dual_var.shape[0]
        return norm(y) ** 2 / (2 * n_samples)

    @staticmethod
    def _get_pobj(dual_var, X, beta, alphas, y=None):
        n_samples = dual_var.shape[0]
        pobj = norm(dual_var) ** 2 / (2 * n_samples) + \
            np.abs(alphas[0] * beta).sum()
        pobj += 0.5 * alphas[1] * norm(beta) ** 2
        return pobj

    @staticmethod
    def _get_dobj(dual_var, X, beta, alpha, y=None):
        # the dual variable is theta = (y - X beta) / (alpha[0] * n_samples)
        n_samples = X.shape[0]
        theta = dual_var / (alpha[0] * n_samples)
        dobj = alpha[0] * y @ theta
        dobj -= alpha[0] ** 2 * n_samples / 2 * np.dot(theta, theta)
        dobj -= alpha[0] ** 2 / alpha[1] / 2 * (ST(X.T @ theta, 1) ** 2).sum()
        return dobj

    @staticmethod
    def _get_jac(dbeta, mask):
        return dbeta[mask, :]

    @staticmethod
    def get_full_jac_v(mask, jac_v, n_features):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        n_features: int
            Number of features.
        """
        # MM sorry I don't get what this does
        return jac_v

    @staticmethod
    def get_mask_jac_v(mask, jac_v):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        """
        return jac_v

    @staticmethod
    def _init_dbeta0(mask, mask0, jac0):
        size_mat = mask.sum()
        if jac0 is not None:
            mask_both = np.logical_and(mask0, mask)
            size_mat = mask.sum()
            dbeta0_new = np.zeros((size_mat, 2))
            count = 0
            count_old = 0
            n_features = mask.shape[0]
            for j in range(n_features):
                if mask_both[j]:
                    dbeta0_new[count, :] = jac0[count_old, :]
                if mask0[j]:
                    count_old += 1
                if mask[j]:
                    count += 1
        else:
            dbeta0_new = np.zeros((size_mat, 2))
        return dbeta0_new

    @staticmethod
    def _init_dbeta(n_features):
        dbeta = np.zeros((n_features, 2))
        return dbeta

    @staticmethod
    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):
        return - X @ dbeta

    @staticmethod
    def _init_g_backward(jac_v0, n_features):
        if jac_v0 is None:
            return np.array([0.0, 0.0])
        else:
            return jac_v0

    @staticmethod
    @njit
    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var, L, alpha, beta):
        n_samples, n_features = Xs.shape
        for j in range(n_features):
            dbeta_old = dbeta[j, :].copy()
            dzj = dbeta[j, :] + Xs[:, j] @ ddual_var / (L[j] * n_samples)
            dbeta[j:j+1, :] = (1 / (1 + alpha[1] / L[j])) * dzj

            dbeta[j:j+1, 0] -= (alpha[0] * np.sign(beta[j])
                                ) / L[j] / (1 + alpha[1] / L[j])
            dbeta[j:j+1, 1] -= (alpha[1] / L[j] * beta[j]
                                ) / (1 + alpha[1] / L[j])
            # update residuals
            ddual_var[:, 0] -= Xs[:, j] * (dbeta[j, 0] - dbeta_old[0])
            ddual_var[:, 1] -= Xs[:, j] * (dbeta[j, 1] - dbeta_old[1])

    @staticmethod
    @njit
    def _update_only_jac_sparse(
            data, indptr, indices, y, n_samples, n_features,
            dbeta, dual_var, ddual_var, L, alpha, beta):
        for j in range(n_features):
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get the non zero idices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            # store old beta j for fast update
            dbeta_old = dbeta[j, :].copy()
            dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \
                (L[j] * n_samples)
            dbeta[j:j+1, :] = (1 / (1 + alpha[1] / L[j])) * dzj

            dbeta[j:j+1, 0] -= (alpha[0] * np.sign(beta[j])
                                ) / L[j] / (1 + alpha[1] / L[j])
            dbeta[j:j+1, 1] -= (alpha[1] / L[j] * beta[j]
                                ) / (1 + alpha[1] / L[j])
            # update residuals
            ddual_var[idx_nz, 0] -= Xjs * (dbeta[j, 0] - dbeta_old[0])
            ddual_var[idx_nz, 1] -= Xjs * (dbeta[j, 1] - dbeta_old[1])

    @staticmethod
    @njit
    def _reduce_alpha(alpha, mask):
        return alpha

    @staticmethod
    def _get_grad(X, y, jac, mask, dense, alphas, v):
        return np.array([alphas[0] * np.sign(dense) @ jac,
                         alphas[1] * dense @ jac])

    def proj_hyperparam(self, X, y, log_alpha):
        """Project hyperparameter on an admissible range of values.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: ndarray, shape (2,)
            Logarithm of hyperparameter.

        Returns
        -------
        log_alpha: float
            Logarithm of projected hyperparameter.
        """
        if not hasattr(self, "log_alpha_max"):
            alpha_max = np.max(np.abs(X.T @ y))
            alpha_max /= X.shape[0]
            self.log_alpha_max = np.log(alpha_max)

        log_alpha = np.clip(log_alpha, self.log_alpha_max - 7,
                            self.log_alpha_max + np.log(0.9))
        return log_alpha

    @staticmethod
    def get_L(X):
        """Compute Lipschitz constant of datafit.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.

        Returns
        -------
        L: float
            The Lipschitz constant.
        """

        if issparse(X):
            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])
        else:
            return norm(X, axis=0) ** 2 / (X.shape[0])

    def _use_estimator(self, X, y, alpha, tol):
        if self.estimator is None:
            raise ValueError("You did not pass a solver with sklearn API")
        self.estimator.set_params(
            tol=tol, alpha=alpha[0]+alpha[1],
            l1_ratio=alpha[0]/(alpha[0]+alpha[1]))
        self.estimator.fit(X, y)
        mask = self.estimator.coef_ != 0
        dense = self.estimator.coef_[mask]
        return mask, dense, None

    @staticmethod
    def reduce_X(X, mask):
        """Reduce design matrix to generalized support.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        mask : ndarray, shape (n_features,)
            Generalized support.
        """
        return X[:, mask]

    @staticmethod
    def reduce_y(y, mask):
        """Reduce observation vector to generalized support.

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Observation vector.
        mask : ndarray, shape (n_features,)  TODO shape n_samples right?
            Generalized support.
        """
        return y

    def sign(self, x, log_alpha):
        """Get sign of iterate.

        Parameters
        ----------
        x : ndarray, shape TODO
        log_alpha : ndarray, shape TODO
            Logarithm of hyperparameter.
        """
        # TODO why is it x ?
        return x

    def get_beta(self, X, y, mask, dense):
        """Return primal iterate.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        """
        return mask, dense

    def get_jac_v(self, X, y, mask, dense, jac, v):
        """Compute hypergradient.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        jac: TODO
        v: TODO
        """
        return jac.T @ v(mask, dense)

    @staticmethod
    def get_mat_vec(X, y, mask, dense, log_alpha):
        """Returns a LinearOperator computing the matrix vector product
        with the Hessian of datafit. It is necessary to avoid storing a
        potentially large matrix, and keep advantage of the sparsity of X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        log_alpha: ndarray, shape (2,)
            Logarithm of hyperparameter.
        """
        X_m = X[:, mask]
        n_samples, size_supp = X_m.shape

        def mv(v):
            return X_m.T @ (X_m @ v) / n_samples + np.exp(log_alpha[1]) * v
        return LinearOperator((size_supp, size_supp), matvec=mv)

    def generalized_supp(self, X, v, log_alpha):
        """Generalized support of iterate.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        v : TODO
        log_alpha : float
            Log of hyperparameter.

        Returns
        -------
        TODO
        """
        return v

    def get_jac_residual_norm(self, Xs, ys, n_samples, beta, dbeta, dual_var,
                              ddual_var, alpha):
        res1 = (1 / n_samples) * ddual_var[:, 0].T @ ddual_var[:, 0] + \
            alpha[1] * dbeta[:, 0].T @ dbeta[:, 0] + alpha[0] * \
            np.sign(beta) @ dbeta[:, 0]
        res2 = (1 / n_samples) * ddual_var[:, 1].T @ ddual_var[:, 1] + \
            alpha[1] * dbeta[:, 1].T @ dbeta[:, 1] + alpha[1] * \
            beta @ dbeta[:, 1]
        return(norm(res2) + norm(res1))

File Path: sparse_ho/models/lasso.py
Content:
import numpy as np
from numpy.linalg import norm
from scipy.sparse import issparse
import scipy.sparse.linalg as slinalg
from numba import njit
from scipy.sparse.linalg import LinearOperator

from sparse_ho.utils import init_dbeta0_new, ST
from sparse_ho.utils import sparse_scalar_product
from sparse_ho.models.base import BaseModel


class Lasso(BaseModel):
    """Linear Model trained with L1 prior as regularizer (aka the Lasso).

    The optimization objective for Lasso is:
    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

    Parameters
    ----------
    estimator: sklearn estimator
        Estimator used to solve the optimization problem. Must follow the
        scikit-learn API.
    """

    def __init__(self, estimator=None):
        self.estimator = estimator

    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,
                              dense0=None, compute_jac=True):
        n_samples, n_features = X.shape
        dbeta = np.zeros(n_features)
        if jac0 is None or not compute_jac:
            ddual_var = np.zeros(n_samples)
        else:
            dbeta[mask0] = jac0.copy()
            ddual_var = - X[:, mask0] @ jac0.copy()
        return dbeta, ddual_var

    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):
        beta = np.zeros(X.shape[1])
        if dense0 is None or len(dense0) == 0:
            dual_var = y.copy()
            dual_var = dual_var.astype(np.float)
        else:
            beta[mask0] = dense0.copy()
            dual_var = y - X[:, mask0] @ dense0
        return beta, dual_var

    @staticmethod
    @njit
    def _update_beta_jac_bcd(
            X, y, beta, dbeta, dual_var, ddual_var,
            alpha, L, compute_jac=True):
        n_samples, n_features = X.shape
        non_zeros = np.where(L != 0)[0]

        for j in non_zeros:
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j]
                # compute derivatives
            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)
            beta[j] = ST(zj, alpha[j] / L[j])
            # beta[j:j+1] = ST(zj, alpha[j] / L[j])
            if compute_jac:
                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)
                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj
                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]
                # update residuals
                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)
            dual_var -= X[:, j] * (beta[j] - beta_old)

    @staticmethod
    @njit
    def _update_beta_jac_bcd_sparse(
            data, indptr, indices, y, n_samples, n_features, beta,
            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):

        non_zeros = np.where(L != 0)[0]

        for j in non_zeros:
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get the non zero indices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j]
            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)
            beta[j:j+1] = ST(zj, alphas[j] / L[j])
            if compute_jac:
                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)
                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj
                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]
                # update residuals
                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)
            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)

    @staticmethod
    @njit
    def _update_bcd_jac_backward(X, alpha, grad, beta, v_t_jac, L):
        sign_beta = np.sign(beta)
        n_samples, n_features = X.shape
        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):
            grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]
            v_t_jac[j] *= np.abs(sign_beta[j])
            v_t_jac -= v_t_jac[j] / (L[j] * n_samples) * X[:, j] @ X

        return grad

    @staticmethod
    @njit
    def _update_bcd_jac_backward_sparse(
            data, indptr, indices, n_samples, n_features,
            alpha, grad, beta, v_t_jac, L):
        sign_beta = np.sign(beta)
        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):
            if L[j] != 0:
                Xjs = data[indptr[j]:indptr[j+1]]
                idx_nz = indices[indptr[j]:indptr[j+1]]
                grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]
                v_t_jac[j] *= np.abs(sign_beta[j])
                cste = v_t_jac[j] / (L[j] * n_samples)
                for i in (np.arange(sign_beta.shape[0] - 1, -1, -1)):
                    Xis = data[indptr[i]:indptr[i+1]]
                    idx = indices[indptr[i]:indptr[i+1]]
                    product = sparse_scalar_product(Xjs, idx_nz, Xis, idx)
                    v_t_jac[i] -= cste * product

        return grad

    @staticmethod
    def _get_pobj0(dual_var, beta, alphas, y=None):
        n_samples = dual_var.shape[0]
        return norm(y) ** 2 / (2 * n_samples)

    @staticmethod
    def _get_pobj(dual_var, X, beta, alphas, y=None):
        n_samples = dual_var.shape[0]
        return (
            norm(dual_var) ** 2 / (2 * n_samples) +
            np.abs(alphas * beta).sum())

    @staticmethod
    def _get_dobj(dual_var, X, beta, alpha, y=None):
        # the dual variable is theta = (y - X beta) / (alpha n_samples)
        n_samples = X.shape[0]
        theta = dual_var / (alpha * n_samples)
        norm_inf_XTtheta = np.max(np.abs(X.T @ theta))
        if norm_inf_XTtheta > 1:
            theta /= norm_inf_XTtheta
        dobj = alpha * y @ theta
        dobj -= alpha ** 2 * n_samples / 2 * (theta ** 2).sum()
        return dobj

    @staticmethod
    def _get_jac(dbeta, mask):
        return dbeta[mask]

    @staticmethod
    def get_full_jac_v(mask, jac_v, n_features):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        n_features: int
            Number of features.
        """
        # MM sorry I don't get what this does
        return jac_v

    @staticmethod
    def get_mask_jac_v(mask, jac_v):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        """
        return jac_v

    @staticmethod
    def _init_dbeta0(mask, mask0, jac0):
        size_mat = mask.sum()
        if jac0 is not None:
            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)
        else:
            dbeta0_new = np.zeros(size_mat)
        return dbeta0_new

    @staticmethod
    def _init_dbeta(n_features):
        dbeta = np.zeros(n_features)
        return dbeta

    @staticmethod
    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):
        return - X @ dbeta

    @staticmethod
    def _init_g_backward(jac_v0, n_features):
        if jac_v0 is None:
            return 0.0
        else:
            return jac_v0

    @staticmethod
    @njit
    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,
                         L, alpha, sign_beta):
        n_samples, n_features = Xs.shape
        for j in range(n_features):
            # dbeta_old = dbeta[j].copy()
            dbeta_old = dbeta[j]
            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)
            dbeta[j] -= alpha * sign_beta[j] / L[j]
            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)

    @staticmethod
    @njit
    def _update_only_jac_sparse(
            data, indptr, indices, y, n_samples, n_features,
            dbeta, dual_var, ddual_var, L, alpha, sign_beta):
        for j in range(n_features):
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get the non zero idices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            # store old beta j for fast update
            dbeta_old = dbeta[j]
            # update of the Jacobian dbeta
            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)
            dbeta[j] -= alpha * sign_beta[j] / L[j]
            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)

    @staticmethod
    @njit
    def _reduce_alpha(alpha, mask):
        return alpha

    @staticmethod
    def _get_grad(X, y, jac, mask, dense, alphas, v):
        return alphas[mask] * np.sign(dense) @ jac

    def proj_hyperparam(self, X, y, log_alpha):
        """Project hyperparameter on an admissible range of values.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float
            Logarithm of hyperparameter.

        Returns
        -------
        log_alpha: float
            Logarithm of projected hyperparameter.
        """
        if not hasattr(self, "log_alpha_max"):
            alpha_max = np.max(np.abs(X.T @ y))
            alpha_max /= X.shape[0]
            self.log_alpha_max = np.log(alpha_max)
        return np.clip(log_alpha, self.log_alpha_max - 12,
                       self.log_alpha_max + np.log(0.9))

    @staticmethod
    def get_L(X):
        """Compute Lipschitz constant of datafit.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.

        Returns
        -------
        L: float
            The Lipschitz constant.
        """
        if issparse(X):
            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])
        else:
            return norm(X, axis=0) ** 2 / (X.shape[0])

    def _use_estimator(self, X, y, alpha, tol):
        if self.estimator is None:
            raise ValueError("You did not pass a solver with sklearn API")
        self.estimator.set_params(tol=tol, alpha=alpha)
        self.estimator.fit(X, y)
        mask = self.estimator.coef_ != 0
        dense = self.estimator.coef_[mask]
        return mask, dense, None

    @staticmethod
    def reduce_X(X, mask):
        """Reduce design matrix to generalized support.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        mask : ndarray, shape (n_features,)
            Generalized support.
        """
        return X[:, mask]

    @staticmethod
    def reduce_y(y, mask):
        """Reduce observation vector to generalized support.

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Observation vector.
        mask : ndarray, shape (n_features,)  TODO shape n_samples right?
            Generalized support.
        """
        return y

    def sign(self, x, log_alpha):
        """Get sign of iterate.

        Parameters
        ----------
        x : ndarray, shape TODO
        log_alpha : ndarray, shape TODO
            Logarithm of hyperparameter.
        """
        return np.sign(x)

    def get_beta(self, X, y, mask, dense):
        """Return primal iterate.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        """
        return mask, dense

    def get_jac_v(self, X, y, mask, dense, jac, v):
        """Compute hypergradient.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        jac: TODO
        v: TODO
        """
        return jac.T @ v(mask, dense)

    @staticmethod
    def get_mat_vec(X, y, mask, dense, log_alpha):
        """Returns a LinearOperator computing the matrix vector product
        with the Hessian of datafit. It is necessary to avoid storing a
        potentially large matrix, and keep advantage of the sparsity of X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        log_alpha: ndarray
            Logarithm of hyperparameter.
        """
        X_m = X[:, mask]
        n_samples, size_supp = X_m.shape

        def mv(v):
            return X_m.T @ (X_m @ v) / n_samples
        return LinearOperator((size_supp, size_supp), matvec=mv)

    def generalized_supp(self, X, v, log_alpha):
        """Generalized support of iterate.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        v : TODO
        log_alpha : float
            Log of hyperparameter.

        Returns
        -------
        TODO
        """
        return v

    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta, dbeta,
                              dual_var, ddual_var, alpha):
        return norm(ddual_var.T @ ddual_var +
                    n_samples * alpha * sign_beta @ dbeta)

File Path: sparse_ho/models/logreg.py
Content:
import numpy as np
from numpy.linalg import norm
from numba import njit
from scipy.sparse.linalg import LinearOperator

from sparse_ho.utils import init_dbeta0_new, ST, sigma, dual_logreg
from sparse_ho.models.base import BaseModel


class SparseLogreg(BaseModel):
    """Sparse Logistic Regression classifier.

    The objective function is:

    sum_1^n_samples log(1 + e^{-y_i x_i^T w}) + 1. / C * ||w||_1

    Parameters
    ----------
    estimator: sklearn estimator
        Estimator used to solve the optimization problem. Must follow the
        scikit-learn API.
    """

    def __init__(self, estimator=None):
        self.estimator = estimator

    def _init_dbeta_ddual_var(self, X, y, dense0=None,
                              mask0=None, jac0=None, compute_jac=True):
        n_samples, n_features = X.shape
        dbeta = np.zeros(n_features)
        if jac0 is None or not compute_jac:
            ddual_var = np.zeros(n_samples)
        else:
            dbeta[mask0] = jac0.copy()
            ddual_var = y * (X[:, mask0] @ jac0.copy())
        return dbeta, ddual_var

    def _init_beta_dual_var(self, X, y, mask0, dense0):
        beta = np.zeros(X.shape[1])
        if dense0 is None:
            dual_var = np.zeros(X.shape[0])
        else:
            beta[mask0] = dense0
            dual_var = y * (X[:, mask0] @ dense0)
        return beta, dual_var

    @staticmethod
    @njit
    def _update_beta_jac_bcd(
            X, y, beta, dbeta, dual_var, ddual_var,
            alpha, L, compute_jac=True):
        n_samples, n_features = X.shape
        for j in range(n_features):
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j]
                # compute derivatives
            sigmar = sigma(dual_var)
            grad_j = X[:, j] @ (y * (sigmar - 1))
            L_temp = np.sum(X[:, j] ** 2 * sigmar * (1 - sigmar))
            L_temp /= n_samples
            zj = beta[j] - grad_j / (L_temp * n_samples)
            beta[j] = ST(zj, alpha[j] / L_temp)
            dual_var += y * X[:, j] * (beta[j] - beta_old)
            if compute_jac:
                dsigmar = sigmar * (1 - sigmar) * ddual_var
                hess_fj = X[:, j] @ (y * dsigmar)
                dzj = dbeta[j] - hess_fj / (L_temp * n_samples)
                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj
                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L_temp
                # update residuals
                ddual_var += y * X[:, j] * (dbeta[j] - dbeta_old)

    @staticmethod
    @njit
    def _update_beta_jac_bcd_sparse(
            data, indptr, indices, y, n_samples, n_features, beta,
            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):

        for j in range(n_features):
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get the non zero indices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j]
            sigmar = sigma(dual_var[idx_nz])
            grad_j = Xjs @ (y[idx_nz] * (sigmar - 1))
            L_temp = (Xjs ** 2 * sigmar * (1 - sigmar)).sum()
            # Xjs2 = (Xjs ** 2 * sigmar * (1 - sigmar)).sum()
            # temp1 =
            # # temp2 = temp1 * Xjs2
            # L_temp = temp2.sum()
            L_temp /= n_samples
            if L_temp != 0:
                zj = beta[j] - grad_j / (L_temp * n_samples)
                beta[j:j+1] = ST(zj, alphas[j] / L_temp)
                if compute_jac:
                    dsigmar = sigmar * (1 - sigmar) * ddual_var[idx_nz]
                    hess_fj = Xjs @ (y[idx_nz] * dsigmar)
                    dzj = dbeta[j] - hess_fj / (L_temp * n_samples)
                    dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj
                    dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L_temp
                    # update residuals
                    ddual_var[idx_nz] += y[idx_nz] * Xjs * \
                        (dbeta[j] - dbeta_old)
                dual_var[idx_nz] += y[idx_nz] * Xjs * (beta[j] - beta_old)

    @staticmethod
    @njit
    def _update_bcd_jac_backward(X, alpha, grad, beta, v_t_jac, L):
        sign_beta = np.sign(beta)
        r = X @ beta
        n_samples, n_features = X.shape
        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):
            hess_fj = sigma(r) * (1 - sigma(r))
            grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]
            v_t_jac[j] *= np.abs(sign_beta[j])
            v_t_jac -= v_t_jac[j] / (
                L[j] * n_samples) * (X[:, j] * hess_fj) @ X
            # TODO be careful r = y X beta
            r += X[:, j] * (beta[j-1] - beta[j])

        return grad

    @staticmethod
    def _get_pobj(dual_var, X, beta, alphas, y):
        pobj = (np.log1p(np.exp(- dual_var)).mean() +
                np.abs(alphas * beta).sum())
        return pobj

    @staticmethod
    def _get_dobj(dual_var, X, beta, alpha, y):
        n_samples = len(y)
        theta = y * sigma(- dual_var) / (alpha * n_samples)

        d_norm_theta = np.max(np.abs(X.T @ theta))
        if d_norm_theta > 1:
            theta /= d_norm_theta
        dobj = dual_logreg(y, theta, alpha)

        return dobj

    @staticmethod
    def _get_pobj0(r, beta, alphas, y):
        n_samples = r.shape[0]
        return np.log(2) / n_samples

    @staticmethod
    def _get_jac(dbeta, mask):
        return dbeta[mask]

    @staticmethod
    def get_full_jac_v(mask, jac_v, n_features):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        n_features: int
            Number of features.
        """
        return jac_v

    @staticmethod
    def get_mask_jac_v(mask, jac_v):
        """TODO I have the feeling this is not used anywhere

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        """
        return jac_v

    @staticmethod
    def _init_dbeta0(mask, mask0, jac0):
        size_mat = mask.sum()
        if jac0 is not None:
            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)
        else:
            dbeta0_new = np.zeros(size_mat)
        return dbeta0_new

    @staticmethod
    def _init_dbeta(n_features):
        dbeta = np.zeros(n_features)
        return dbeta

    @staticmethod
    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):
        return y * (X @ dbeta)

    def _init_g_backward(self, jac_v0):
        if jac_v0 is None:
            return 0.0
        else:
            return jac_v0

    @staticmethod
    @njit
    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,
                         L, alpha, sign_beta):
        n_samples, n_features = Xs.shape
        for j in range(n_features):
            sigmar = sigma(dual_var)
            L_temp = np.sum(Xs[:, j] ** 2 * sigmar * (1 - sigmar))
            L_temp /= n_samples

            dbeta_old = dbeta[j]
            dsigmar = sigmar * (1 - sigmar) * ddual_var
            hess_fj = Xs[:, j] @ (y * dsigmar)
            dbeta[j:j+1] += - hess_fj / (L_temp * n_samples)
            dbeta[j:j+1] -= alpha * sign_beta[j] / L_temp
            # update residuals
            ddual_var += y * Xs[:, j] * (dbeta[j] - dbeta_old)

    @staticmethod
    @njit
    def _update_only_jac_sparse(
            data, indptr, indices, y, n_samples, n_features,
            dbeta, dual_var, ddual_var, L, alpha, sign_beta):
        for j in range(n_features):
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get the non zero idices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            sigmar = sigma(dual_var[idx_nz])
            L_temp = np.sum(Xjs ** 2 * sigmar * (1 - sigmar))
            L_temp /= n_samples
            if L_temp != 0:
                # store old beta j for fast update
                dbeta_old = dbeta[j]
                dsigmar = sigmar * (1 - sigmar) * ddual_var[idx_nz]

                hess_fj = Xjs @ (y[idx_nz] * dsigmar)
                # update of the Jacobian dbeta
                dbeta[j] -= hess_fj / (L_temp * n_samples)
                dbeta[j] -= alpha * sign_beta[j] / L_temp
                ddual_var[idx_nz] += y[idx_nz] * Xjs * (dbeta[j] - dbeta_old)

    @staticmethod
    @njit
    def _reduce_alpha(alpha, mask):
        return alpha

    @staticmethod
    def _get_grad(X, y, jac, mask, dense, alphas, v):
        return alphas[mask] * np.sign(dense) @ jac

    def proj_hyperparam(self, X, y, log_alpha):
        """Project hyperparameter on an admissible range of values.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: float
            Logarithm of hyperparameter.

        Returns
        -------
        log_alpha: float
            Logarithm of projected hyperparameter.
        """
        if not hasattr(self, "log_alpha_max"):
            alpha_max = np.max(np.abs(X.T @ y)) / (2 * X.shape[0])
            self.log_alpha_max = np.log(alpha_max)

        log_alpha = np.clip(log_alpha, self.log_alpha_max - 8,
                            self.log_alpha_max + np.log(0.9))
        return log_alpha

    @staticmethod
    def get_L(X):
        """Compute Lipschitz constant of datafit.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.

        Returns
        -------
        L: float
            The Lipschitz constant.
        """
        return 0.0  # TODO implement?

    @staticmethod
    def reduce_X(X, mask):
        """Reduce design matrix to generalized support.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        mask : ndarray, shape (n_features,)
            Generalized support.
        """
        return X[:, mask]

    @staticmethod
    def reduce_y(y, mask):
        """Reduce observation vector to generalized support.

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Observation vector.
        mask : ndarray, shape (n_features,)  TODO shape n_samples right?
            Generalized support.
        """
        return y

    def sign(self, x, log_alpha):
        """Get sign of iterate.

        Parameters
        ----------
        x : ndarray, shape TODO
        log_alpha : ndarray, shape TODO
            Logarithm of hyperparameter.
        """
        return np.sign(x)

    def get_beta(self, X, y, mask, dense):
        """Return primal iterate.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        """
        return mask, dense

    def get_jac_v(self, X, y, mask, dense, jac, v):
        """Compute hypergradient.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        jac: TODO
        v: TODO
        """
        return jac.T @ v(mask, dense)

    @staticmethod
    def get_mat_vec(X, y, mask, dense, log_alpha):
        """Returns a LinearOperator computing the matrix vector product
        with the Hessian of datafit. It is necessary to avoid storing a
        potentially large matrix, and keep advantage of the sparsity of X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        log_alpha: ndarray
            Logarithm of hyperparameter.
        """
        X_m = X[:, mask]
        n_samples, size_supp = X_m.shape
        a = y * (X_m @ dense)
        grad_sigmoid = sigma(a) * (1 - sigma(a))

        def mv(v):
            return X_m.T @ (grad_sigmoid * (X_m @ v)) / n_samples

        return LinearOperator((size_supp, size_supp), matvec=mv)

    def generalized_supp(self, X, v, log_alpha):
        """Generalized support of iterate.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        v : TODO
        log_alpha : float
            Log of hyperparameter.

        Returns
        -------
        TODO
        """
        return v

    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta,
                              dbeta, dual_var, ddual_var, alpha):
        return(
            norm(ddual_var.T @ ddual_var +
                 n_samples * alpha * sign_beta @ dbeta))

    def _use_estimator(self, X, y, alpha, tol):
        n_samples = X.shape[0]
        if self.estimator is None:
            raise ValueError("You did not pass a solver with sklearn API")
        self.estimator.set_params(tol=tol, C=1./(alpha*n_samples))
        self.estimator.fit(X, y)
        mask = self.estimator.coef_ != 0
        dense = self.estimator.coef_[mask]
        return mask[0], dense, None

File Path: sparse_ho/models/ssvr.py
Content:
import numpy as np
from numpy.linalg import norm
from numba import njit
from scipy.sparse.linalg import LinearOperator
from sparse_ho.models import SVR
from sparse_ho.models.svr import (
    _compute_jac_aux, _compute_jac_aux_sparse,
    _update_beta_jac_bcd_aux, _update_beta_jac_bcd_aux_sparse)


class SimplexSVR(SVR):
    """The simplex support vector regression without bias
    The optimization problem is solved in the dual.

    It solves the SVR with probability vector constraints:
    sum_i beta_i = 1
    beta_i >= 0

    Parameters
    ----------
    estimator: sklearn
        An estimator that follows the scikit-learn API.
    """

    def __init__(self, estimator=None):
        super().__init__(estimator)

    def _init_dbeta_ddual_var(self, X, y, dense0=None,
                              mask0=None, jac0=None, compute_jac=True):
        n_samples, n_features = X.shape
        self.n_samples = n_samples
        ddual_var = np.zeros((2 * n_samples + n_features + 1, 2))

        if jac0 is None or not compute_jac or self.ddual_var is None:
            dbeta = np.zeros((n_features, 2))
        else:
            if self.ddual_var.shape[0] != 2 * n_samples + n_features + 1:
                dbeta = np.zeros((n_features, 2))
            else:
                ddual_var = self.ddual_var
                dbeta = X.T @ (
                    ddual_var[0:n_samples, :] -
                    ddual_var[n_samples:(2 * n_samples), :])
                dbeta += ddual_var[(2 * n_samples):
                                   (2 * n_samples + n_features), :]
                dbeta += ddual_var[-1, :]
        return dbeta, ddual_var

    def _init_beta_dual_var(self, X, y, mask0, dense0):
        n_samples, n_features = X.shape
        dual_var = np.zeros(2 * n_samples + n_features + 1)
        if mask0 is None or self.dual_var is None:
            beta = np.zeros(n_features)
        else:
            if self.dual_var.shape[0] != (2 * n_samples + n_features + 1):
                beta = np.zeros(n_features)
            else:
                dual_var = self.dual_var
                beta = X.T @ (dual_var[0:n_samples] -
                              dual_var[n_samples:(2 * n_samples)])
                beta += dual_var[(2 * n_samples):(2 * n_samples + n_features)]
                beta += dual_var[-1]
        return beta, dual_var

    @staticmethod
    @njit
    def _update_beta_jac_bcd(
            X, y, beta, dbeta, dual_var, ddual_var,
            hyperparam, L, compute_jac=True):

        C = hyperparam[0]
        epsilon = hyperparam[1]
        n_samples, n_features = X.shape
        for j in range(2 * n_samples + n_features + 1):
            if j < (2 * n_samples):
                if j < n_samples:
                    j1, j2, sign = j, j, 1
                elif j >= n_samples:
                    j1, j2, sign = j - n_samples, j, -1

                _update_beta_jac_bcd_aux(
                    X, y, epsilon, beta, dbeta, dual_var,
                    ddual_var, L, C / n_samples, j1, j2, sign, compute_jac)
            else:
                if j < (2 * n_samples + n_features):
                    F = beta[j - (2 * n_samples)]
                    dual_var_old = dual_var[j]
                    zj = dual_var[j] - F
                    dual_var[j] = max(0, zj)
                    beta[j - (2 * n_samples)] -= (dual_var_old - dual_var[j])
                    if compute_jac:
                        dF = dbeta[j - (2 * n_samples)]
                        ddual_var_old = ddual_var[j, :].copy()
                        dzj = ddual_var[j, :] - dF
                        if zj > 0:
                            ddual_var[j, :] = dzj
                        else:
                            ddual_var[j, :] = np.repeat(0.0, 2)
                        dbeta[j - (2 * n_samples), 0] -= (ddual_var_old[0] -
                                                          ddual_var[j, 0])
                        dbeta[j - (2 * n_samples), 1] -= (ddual_var_old[1] -
                                                          ddual_var[j, 1])
                else:
                    F = np.sum(beta) - 1
                    dual_var_old = dual_var[-1]
                    zj = dual_var[j] - F / n_features
                    dual_var[j] = zj
                    beta -= (dual_var_old - dual_var[-1])
                    if compute_jac:
                        dF = np.sum(dbeta, axis=0)
                        ddual_var_old = ddual_var[-1, :].copy()
                        dzj = ddual_var[j] - dF / n_features
                        ddual_var[j, :] = dzj
                        dbeta[:, 0] -= (ddual_var_old[0] - ddual_var[-1, 0])
                        dbeta[:, 1] -= (ddual_var_old[1] - ddual_var[-1, 1])

    @staticmethod
    @njit
    def _update_beta_jac_bcd_sparse(
            data, indptr, indices, y, n_samples, n_features, beta,
            dbeta, dual_var, ddual_var, hyperparam, L, compute_jac=True):
        C = hyperparam[0]
        epsilon = hyperparam[1]
        for j in range(2 * n_samples + n_features + 1):
            if j < (2 * n_samples):
                if j < n_samples:
                    j1, j2, sign = j, j, 1
                elif j >= n_samples and j < (2 * n_samples):
                    j1, j2, sign = j - n_samples, j, -1

                _update_beta_jac_bcd_aux_sparse(
                    data, indptr, indices, y, epsilon,
                    beta, dbeta, dual_var, ddual_var,
                    L, C / n_samples, j1, j2, sign, compute_jac)

            else:
                if j >= (2 * n_samples) and j < (2 * n_samples + n_features):
                    F = beta[j - (2 * n_samples)]
                    dual_var_old = dual_var[j]
                    zj = dual_var[j] - F
                    dual_var[j] = max(0, zj)
                    beta[j - (2 * n_samples)] -= (dual_var_old - dual_var[j])
                    if compute_jac:
                        dF = dbeta[j - (2 * n_samples)]
                        ddual_var_old = ddual_var[j, :].copy()
                        dzj = ddual_var[j, :] - dF
                        if zj > 0:
                            ddual_var[j, :] = dzj
                        else:
                            ddual_var[j, :] = 0
                        dbeta[j - (2 * n_samples), 0] -= (ddual_var_old[0] -
                                                          ddual_var[j, 0])
                        dbeta[j - (2 * n_samples), 1] -= (ddual_var_old[1] -
                                                          ddual_var[j, 1])
                else:
                    F = np.sum(beta) - 1
                    dual_var_old = dual_var[-1]
                    zj = dual_var[j] - F / n_features
                    dual_var[j] = zj
                    beta -= (dual_var_old - dual_var[-1])
                    if compute_jac:
                        dF = np.sum(dbeta, axis=0)
                        ddual_var_old = ddual_var[-1, :].copy()
                        dzj = ddual_var[j, :] - dF / n_features
                        ddual_var[j, :] = dzj
                        dbeta[:, 0] -= (ddual_var_old[0] - ddual_var[-1, 0])
                        dbeta[:, 1] -= (ddual_var_old[1] - ddual_var[-1, 1])

    def _get_pobj0(self, dual_var, beta, hyperparam, y):
        hyperparameters = hyperparam.copy()
        hyperparameters[0] /= len(y)
        return super()._get_pobj0(dual_var, beta, hyperparameters, y)

    def _get_pobj(self, dual_var, X, beta, hyperparam, y):
        hyperparameters = hyperparam.copy()
        hyperparameters[0] /= len(y)
        return super()._get_pobj(dual_var, X, beta, hyperparameters, y)

    @staticmethod
    def _get_dobj(dual_var, X, beta, hyperparam, y):
        n_samples = X.shape[0]
        obj_dual = 0.5 * beta.T @ beta
        obj_dual += hyperparam[1] * np.sum(dual_var[0:(2 * n_samples)])
        obj_dual -= np.sum(y * (dual_var[0:n_samples] -
                                dual_var[n_samples:(2 * n_samples)]))
        obj_dual -= dual_var[-1]
        return -obj_dual

    @staticmethod
    def _get_jac(dbeta, mask):
        return dbeta[mask, :]

    def _init_dbeta0(self, mask, mask0, jac0):
        return super()._init_dbeta0(mask, mask0, jac0)

    def _init_dbeta(self, n_features):
        if self.dbeta is not None:
            return self.dbeta
        else:
            return np.zeros((n_features, 2))

    def _init_ddual_var(self, dbeta, X, y, sign_beta, hyperparam):
        dual_var = self.dual_var
        C = hyperparam[0]
        n_samples, n_features = X.shape
        sign = np.zeros(dual_var.shape[0])
        bool_temp = np.isclose(dual_var[0:(2 * n_samples + n_features)], 0.0)
        sign[0:(2 * n_samples + n_features)][bool_temp] = -1.0
        sign[0:(2 * n_samples)][np.isclose(
            dual_var[0:(2 * n_samples)], C / n_samples)] = 1.0
        ddual_var = np.zeros((dual_var.shape[0], 2))
        if np.any(sign == 1.0):
            ddual_var[sign == 1.0, 0] = np.repeat(
                C / n_samples, (sign == 1).sum())
            ddual_var[sign == 1.0, 1] = np.repeat(
                0, (sign == 1).sum())
        self.ddual_var = ddual_var
        self.dbeta = X.T @ (
            ddual_var[0:n_samples, :] -
            ddual_var[n_samples:(2 * n_samples), :])
        self.dbeta += (
            ddual_var[(2 * n_samples):(2 * n_samples + n_features), :])
        self.dbeta += ddual_var[-1, :]
        return ddual_var

    @staticmethod
    @njit
    def _update_only_jac(X, y, dual_var, dbeta, ddual_var,
                         L, hyperparam, sign_beta):
        n_samples, n_features = X.shape
        length_dual = dual_var.shape[0]
        C = hyperparam[0]
        epsilon = hyperparam[1]
        gen_supp = np.zeros(length_dual)
        bool_temp = dual_var[0:(2 * n_samples + n_features)] == 0.0
        gen_supp[0:(2 * n_samples + n_features)][bool_temp] = -1.0
        is_right_border = dual_var[0:(2 * n_samples)] == C / n_samples
        gen_supp[0:(2 * n_samples)][is_right_border] = 1.0
        for j in np.arange(0, length_dual)[gen_supp == 0.0]:
            if j < (2 * n_samples):
                if j < n_samples:
                    j1, j2, sign = j, j, 1
                elif j >= n_samples:
                    j1, j2, sign = j - n_samples, j, -1

                _compute_jac_aux(
                    X, epsilon, dbeta, ddual_var, dual_var[j2], L,
                    C / n_samples, j1, j2, sign)
            else:
                if j < (2 * n_samples + n_features):
                    dF = dbeta[j - (2 * n_samples)]
                    ddual_var_old = ddual_var[j, :].copy()
                    dzj = ddual_var[j, :] - dF
                    ddual_var[j, :] = dzj
                    dbeta[j - (2 * n_samples), 0] -= (ddual_var_old[0] -
                                                      ddual_var[j, 0])
                    dbeta[j - (2 * n_samples), 1] -= (ddual_var_old[1] -
                                                      ddual_var[j, 1])
                else:
                    dF = np.sum(dbeta, axis=0)
                    ddual_var_old = ddual_var[-1, :].copy()
                    dzj = ddual_var[j, :] - dF / n_features
                    ddual_var[j, :] = dzj
                    dbeta[:, 0] -= (ddual_var_old[0] - ddual_var[-1, 0])
                    dbeta[:, 1] -= (ddual_var_old[1] - ddual_var[-1, 1])

    @staticmethod
    @njit
    def _update_only_jac_sparse(
            data, indptr, indices, y, n_samples, n_features,
            dbeta, dual_var, ddual_var, L, hyperparam, sign_beta):
        C = hyperparam[0]
        epsilon = hyperparam[1]

        gen_supp = np.zeros(dual_var.shape[0])
        bool_temp = dual_var[0:(2 * n_samples + n_features)] == 0.0
        gen_supp[0:(2 * n_samples + n_features)][bool_temp] = -1.0
        is_right_border = dual_var[0:(2 * n_samples)] == C / n_samples
        gen_supp[0:(2 * n_samples)][is_right_border] = 1.0

        iter = np.arange(0, (2 * n_samples + n_features + 1))[gen_supp == 0.0]
        for j in iter:
            if j < (2 * n_samples):
                if j < n_samples:
                    j1, j2, sign = j, j, 1
                elif j >= n_samples:
                    j1, j2, sign = j - n_samples, j, -1

                _compute_jac_aux_sparse(
                    data, indptr, indices, epsilon, dbeta, ddual_var,
                    dual_var[j2], L, C / n_samples, j1, j2, sign)

            else:
                if j >= (2 * n_samples) and j < (2 * n_samples + n_features):
                    dF = dbeta[j - (2 * n_samples)]
                    ddual_var_old = ddual_var[j, :].copy()
                    dzj = ddual_var[j, :] - dF
                    ddual_var[j, :] = dzj
                    dbeta[j - (2 * n_samples), 0] -= (ddual_var_old[0] -
                                                      ddual_var[j, 0])
                    dbeta[j - (2 * n_samples), 1] -= (ddual_var_old[1] -
                                                      ddual_var[j, 1])
                else:
                    dF = np.sum(dbeta, axis=0)
                    ddual_var_old = ddual_var[-1, :].copy()
                    dzj = ddual_var[j, :] - dF / n_features
                    ddual_var[j, :] = dzj
                    dbeta[:, 0] -= (ddual_var_old[0] - ddual_var[-1, 0])
                    dbeta[:, 1] -= (ddual_var_old[1] - ddual_var[-1, 1])

    @staticmethod
    @njit
    def _reduce_alpha(alpha, mask):
        return alpha

    def get_L(self, X):
        """Compute Lipschitz constant of datafit.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.

        Returns
        -------
        L: float
            The Lipschitz constant.
        """
        return super().get_L(X)

    @staticmethod
    def reduce_X(X, mask):
        """Reduce design matrix to generalized support.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        mask : ndarray, shape (n_features,)
            Generalized support.
        """
        return X[:, mask]

    @staticmethod
    def reduce_y(y, mask):
        """Reduce observation vector to generalized support.

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Observation vector.
        mask : ndarray, shape (n_features,)  TODO shape n_samples right?
            Generalized support.
        """
        return y

    def sign(self, beta, log_hyperparams):
        """Get sign of iterate. Here sign means -1.0 if the iterate is 0,
        1.0 if it is equal to C / n_samples.

        Parameters
        ----------
        beta : ndarray, shape TODO
        log_hyperparams : ndarray, shape (2, )
            Logarithm of hyperparameter C and epsilon.
        """
        return super().sign(beta, log_hyperparams)

    def get_jac_v(self, X, y, mask, dense, jac, v):
        """Compute hypergradient.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        jac: TODO
        v: TODO
        """
        return super().get_jac_v(X, y, mask, dense, jac, v)

    @staticmethod
    def get_full_jac_v(mask, jac_v, n_features):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        n_features: int
            Number of features.
        """
        return jac_v

    def get_dual_v(self, mask, dense, X, y, v, log_hyperparam):
        """Compute the dual of v

        Parameters
        ----------
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        v: TODO.

        log_hyperparam:
            ndarray, shape (2, )
            Logarithm of hyperparameter C and epsilon.
        """
        full_v = np.zeros(X.shape[1])
        full_v[mask] = v
        if v.shape[0] != 0:
            return np.hstack((-X @ full_v, full_v, np.sum(full_v)))
        else:
            return np.zeros(X.shape[0] + X.shape[1] + 1)

    def _get_grad(self, X, y, jac, mask, dense, hyperparam, v):
        C = hyperparam[0]
        n_samples = X.shape[0]
        epsilon = hyperparam[1]
        alpha = self.dual_var[0:n_samples] - \
            self.dual_var[n_samples:(2 * n_samples)]
        n_features = X.shape[1]
        gamma = self.dual_var[(2 * n_samples):(2 * n_samples + n_features)]
        mask0 = np.logical_not(np.isclose(gamma, 0))
        full_supp = np.logical_not(
            np.logical_or(
                np.isclose(alpha, 0),
                np.isclose(np.abs(alpha), C / n_samples)))
        maskC = np.isclose(np.abs(alpha), C / n_samples)
        sub_id = np.zeros((mask0.sum(), n_features))
        sub_id[:, mask0] = 1.0

        hessian = np.concatenate((X[full_supp, :], -sub_id,
                                  -np.ones((1, n_features))), axis=0)
        hessian_vec = hessian @ X[maskC, :].T @ alpha[maskC]
        jac_t_v = -hessian_vec.T @ jac
        jac_t_v -= alpha[maskC].T @ v[0:n_samples][maskC]
        jac_t_v2 = -epsilon * np.sign(alpha[full_supp]) @ \
            jac[0:full_supp.sum()]
        return np.array([jac_t_v, jac_t_v2])

    def generalized_supp(self, X, v, log_hyperparam):
        """Generalized support of iterate.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        v: TODO.

        log_hyperparam: ndarray, shape (2, )
            Logarithm of hyperparameter C and epsilon.

        Returns
        -------
        TODO
        """
        n_samples, n_features = X.shape
        C = np.exp(log_hyperparam[0])
        alpha = self.dual_var[0:n_samples] -\
            self.dual_var[n_samples:(2 * n_samples)]
        full_supp = np.logical_not(
            np.logical_or(
                np.isclose(alpha, 0),
                np.isclose(np.abs(alpha), C / n_samples)))
        mask0 = np.logical_not(np.isclose(
            self.dual_var[(2 * n_samples):(2 * n_samples + n_features)], 0))
        return v[np.hstack((full_supp, mask0, True))]

    def proj_hyperparam(self, X, y, log_hyperparam):
        """Project hyperparameter on an admissible range of values.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_hyperparam: ndarray, shape (2, )
            Logarithm of hyperparameters C and epsilon.

        Returns
        -------
        log_hyperparam: float
            Logarithm of projected hyperparameters.
        """
        return super().proj_hyperparam(X, y, log_hyperparam)

    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta,
                              dbeta, dual_var, ddual_var, hyperparam):

        n_features = dbeta.shape[0]
        C = hyperparam[0]
        alpha = dual_var[0:n_samples] - dual_var[n_samples:(2 * n_samples)]
        dalpha = ddual_var[0:n_samples, 0] - \
            ddual_var[n_samples:(2 * n_samples), 0]
        dgamma = ddual_var[(2 * n_samples):(2 * n_samples + n_features), 0]
        dmu = ddual_var[-1, 0]
        maskC = np.isclose(np.abs(alpha), C / n_samples)

        full_supp = np.logical_not(
            np.logical_or(
                np.isclose(alpha, 0),
                np.isclose(np.abs(alpha), C / n_samples)))

        vecX = dalpha[full_supp].T @ Xs[full_supp, :]
        vecX += dgamma + np.repeat(dmu, n_features)
        quadratic_term = vecX.T @ vecX
        linear_term = vecX.T @ Xs[maskC, :].T @ alpha[maskC]
        return norm(quadratic_term + linear_term)

    def get_mat_vec(self, X, y, mask, dense, log_C):
        """Returns a LinearOperator computing the matrix vector product
        with the Hessian of datafit. It is necessary to avoid storing a
        potentially large matrix, and keep advantage of the sparsity of X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        log_C: ndarray
            Logarithm of hyperparameter.
        """
        C = np.exp(log_C)[0]
        n_samples, n_features = X.shape
        alpha = self.dual_var[0:n_samples] - \
            self.dual_var[n_samples:(2 * n_samples)]
        gamma = self.dual_var[(2 * n_samples):(2 * n_samples + n_features)]
        mask0 = np.logical_not(np.isclose(gamma, 0))
        full_supp = np.logical_not(
            np.logical_or(
                np.isclose(alpha, 0),
                np.isclose(np.abs(alpha), C / n_samples)))
        sub_id = np.zeros((mask0.sum(), n_features))
        sub_id[:, mask0] = 1.0
        X_m = np.concatenate((X[full_supp, :],
                              -sub_id, -np.ones((1, n_features))), axis=0)
        size_supp = X_m.shape[0]

        def mv(v):
            return X_m @ (X_m.T @ v)
        return LinearOperator((size_supp, size_supp), matvec=mv)

    def _use_estimator(self, X, y, hyperparam, tol, max_iter):
        return super()._use_estimator(X, y, hyperparam, tol, max_iter)

File Path: sparse_ho/models/svm.py
Content:
import numpy as np
from numpy.linalg import norm
import scipy.sparse.linalg as slinalg
from scipy.sparse import issparse
from scipy.sparse.linalg import LinearOperator

from numba import njit

from sparse_ho.models.base import BaseModel
from sparse_ho.utils import proj_box_svm, ind_box
from sparse_ho.utils import init_dbeta0_new


class SVM(BaseModel):
    """Support Vector Machine classifier without bias.

    The optimization problem is solved in the dual:
        1/2 r^T(y * X)(y * X)^T r - sum_i^n r_i
        s.t 0 <= r_i <= C

    Parameters
    ----------
    estimator: instance of ``sklearn.base.BaseEstimator``
        An estimator that follows the scikit-learn API.
    """

    def __init__(self, estimator=None):
        self.estimator = estimator
        self.dual = True  # solve the pb in the dual
        self.dual_var = None  # save the last dual_var variable for warm_start
        self.ddual_var = None  # save the last dual_var jacobian for warm_start

    def _init_dbeta_ddual_var(
            self, X, y, dense0=None, mask0=None, jac0=None, compute_jac=True):
        n_samples, n_features = X.shape
        ddual_var = np.zeros(n_samples)
        if self.ddual_var is None:
            dbeta = np.zeros(n_features)
        else:
            if self.dual_var.shape[0] != n_samples:
                dbeta = np.zeros(n_features)
            else:
                ddual_var = self.ddual_var.copy()
                if issparse(X):
                    dbeta = (X.T).multiply(y * ddual_var)
                    dbeta = np.sum(dbeta, axis=1)
                    dbeta = np.squeeze(np.array(dbeta))
                else:
                    dbeta = np.sum(y * ddual_var * X.T, axis=1)
        return dbeta, ddual_var

    def _init_beta_dual_var(self, X, y, mask0, dense0):
        n_samples, n_features = X.shape
        dual_var = np.zeros(n_samples)
        if mask0 is None or self.dual_var is None:
            beta = np.zeros(n_features)
        else:
            if self.dual_var.shape[0] != n_samples:
                beta = np.zeros(n_features)
            else:
                dual_var = self.dual_var
                if issparse(X):
                    beta = (X.T).multiply(y * dual_var)
                    beta = np.sum(beta, axis=1)
                    beta = np.squeeze(np.array(beta))
                else:
                    beta = np.sum(y * dual_var * X.T, axis=1)
        return beta, dual_var

    @staticmethod
    @njit
    def _update_beta_jac_bcd(
            X, y, beta, dbeta, dual_var, ddual_var, C, L, compute_jac=True):
        """
            beta : dual variable of the svm
            r : primal used for cheap updates
            dbeta : jacobian of the dual variables
            dr : jacobian of the primal variable
        """
        C = C[0]
        n_samples = X.shape[0]
        for j in range(n_samples):
            F = y[j] * np.sum(beta * X[j, :]) - 1.0
            dual_var_old = dual_var[j]
            zj = dual_var[j] - F / L[j]
            dual_var[j] = proj_box_svm(zj, C)
            beta += (dual_var[j] - dual_var_old) * y[j] * X[j, :]
            if compute_jac:
                dF = y[j] * np.sum(dbeta * X[j, :])
                ddual_var_old = ddual_var[j]
                dzj = ddual_var[j] - dF / L[j]
                ddual_var[j] = ind_box(zj, C) * dzj
                ddual_var[j] += C * (C <= zj)
                dbeta += (ddual_var[j] - ddual_var_old) * y[j] * X[j, :]

    @staticmethod
    @njit
    def _update_beta_jac_bcd_sparse(
            data, indptr, indices, y, n_samples, n_features, beta,
            dbeta, dual_var, ddual_var, C, L, compute_jac=True):
        # data needs to be a row sparse matrix
        C = C[0]
        for j in range(n_samples):
            # get the i-st row of X in sparse format
            Xis = data[indptr[j]:indptr[j+1]]
            # get the non zero indices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            # Compute the gradient
            F = y[j] * np.sum(beta[idx_nz] * Xis) - 1.0
            dual_var_old = dual_var[j]
            zj = dual_var[j] - F / L[j]
            dual_var[j] = proj_box_svm(zj, C)
            beta[idx_nz] += (dual_var[j] - dual_var_old) * y[j] * Xis
            if compute_jac:
                dF = y[j] * np.sum(dbeta[idx_nz] * Xis)
                ddual_var_old = ddual_var[j]
                dzj = ddual_var[j] - dF / L[j]
                ddual_var[j] = ind_box(zj, C) * dzj
                ddual_var[j] += C * (C <= zj)
                dbeta[idx_nz] += (ddual_var[j] - ddual_var_old) * y[j] * Xis

    @staticmethod
    def _get_pobj0(dual_var, beta, C, y):
        C = C[0]
        n_samples = dual_var.shape[0]
        obj_prim = C * np.sum(np.maximum(
            np.ones(n_samples), np.zeros(n_samples)))
        return obj_prim

    @staticmethod
    def _get_pobj(dual_var, X, beta, C, y):
        C = C[0]
        n_samples = X.shape[0]
        obj_prim = 0.5 * norm(beta) ** 2 + C * np.sum(np.maximum(
            np.ones(n_samples) - (X @ beta) * y, np.zeros(n_samples)))
        obj_dual = 0.5 * beta.T @ beta - np.sum(dual_var)
        return (obj_dual + obj_prim)

    @staticmethod
    def _get_jac(dbeta, mask):
        return dbeta

    @staticmethod
    def _init_dbeta0(mask, mask0, jac0):
        size_mat = mask.sum()
        if jac0 is not None:
            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)
        else:
            dbeta0_new = np.zeros(size_mat)
        return dbeta0_new

    def _init_dbeta(self, n_features):
        if self.dbeta is not None:
            return self.dbeta
        else:
            return np.zeros(n_features)

    def _init_ddual_var(self, dbeta, X, y, sign_beta, C):
        is_sparse = issparse(X)
        sign = np.zeros(self.dual_var.shape[0])
        sign[self.dual_var == 0.0] = -1.0
        sign[self.dual_var == C] = 1.0
        ddual_var = np.zeros(X.shape[0])
        self.ddual_var = ddual_var
        if np.any(sign == 1.0):
            ddual_var[sign == 1.0] = np.repeat(C, (sign == 1).sum())
        if is_sparse:
            self.dbeta = np.array(
                np.sum(X.T.multiply(y * ddual_var), axis=1))[:, 0]
        else:
            self.dbeta = np.sum(y * ddual_var * X.T, axis=1)
        return ddual_var

    @staticmethod
    @njit
    def _update_only_jac(Xs, ys, dual_var, dbeta, ddual_var,
                         L, C, sign_beta):
        sign = np.zeros(dual_var.shape[0])
        sign[dual_var == 0.0] = -1.0
        sign[dual_var == C] = 1.0
        for j in np.arange(0, Xs.shape[0])[sign == 0.0]:
            dF = ys[j] * np.sum(dbeta * Xs[j, :])
            ddual_var_old = ddual_var[j]
            dzj = ddual_var[j] - (dF / L[j])
            ddual_var[j] = dzj
            dbeta += (ddual_var[j] - ddual_var_old) * ys[j] * Xs[j, :]

    @staticmethod
    @njit
    def _update_only_jac_sparse(
            data, indptr, indices, y, n_samples, n_features,
            dbeta, dual_var, ddual_var, L, C, sign_beta):
        sign = np.zeros(n_samples)
        sign[dual_var == 0.0] = -1.0
        sign[dual_var == C] = 1.0
        for j in np.arange(0, n_samples)[sign == 0.0]:
            # get the i-st row of X in sparse format
            Xis = data[indptr[j]:indptr[j+1]]
            # get the non zero idices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            # store old beta j for fast update
            dF = y[j] * np.sum(dbeta[idx_nz] * Xis)
            ddual_var_old = ddual_var[j]
            dzj = ddual_var[j] - (dF / L[j])
            ddual_var[j] = dzj
            dbeta[idx_nz] += ((ddual_var[j] - ddual_var_old) * y[j] * Xis)

    @staticmethod
    @njit
    def _reduce_alpha(alpha, mask):
        return alpha

    @staticmethod
    def get_L(X):
        """Compute Lipschitz constant of datafit.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.

        Returns
        -------
        L: float
            The Lipschitz constant.
        """
        if issparse(X):
            return slinalg.norm(X, axis=1) ** 2
        else:
            return norm(X, axis=1) ** 2

    @staticmethod
    def reduce_X(X, mask):
        """Reduce design matrix to generalized support.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        mask : ndarray, shape (n_features,)
            Generalized support.
        """
        return X

    @staticmethod
    def reduce_y(y, mask):
        """Reduce observation vector to generalized support.

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Observation vector.
        mask : ndarray, shape (n_features,)  TODO shape n_samples right?
            Generalized support.
        """
        # TODO why is nothing reduced?
        return y

    def sign(self, x, log_C):
        """Get sign of iterate.

        Parameters
        ----------
        x : ndarray, shape TODO
        log_C: ndarray, shape TODO
            Logarithm of hyperparameter.
        """
        sign = np.zeros(x.shape[0])
        sign[np.isclose(x, 0.0)] = -1.0
        sign[np.isclose(x, np.exp(log_C))] = 1.0
        return sign

    def get_dual_v(self, mask, dense, X, y, v, log_C):
        """TODO

        Parameters
        ----------
        mask: TODO
        dense: TODO
        X: TODO
        y: TODO
        v: TODO
        log_C: TODO
        """
        if issparse(X):
            v_dual = v @ (X[:, mask].T).multiply(y)
            # v_dual = np.sum(v_dual)
            # v_dual = np.squeeze(np.array(v_dual))
        else:
            v_dual = (y * X[:, mask].T).T @ v
        return v_dual

    @staticmethod
    def get_jac_v(X, y, mask, dense, jac, v):
        """Compute hypergradient.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        jac: TODO
        v: TODO
        """
        return jac[mask].T @ v(mask, dense)

    @staticmethod
    def get_full_jac_v(mask, jac_v, n_features):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        n_features: int
            Number of features.
        """
        # MM sorry I don't get what this does
        return jac_v

    def get_mat_vec(self, X, y, mask, dense, log_C):
        """Returns a LinearOperator computing the matrix vector product
        with the Hessian of datafit. It is necessary to avoid storing a
        potentially large matrix, and keep advantage of the sparsity of X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        log_C: ndarray
            Logarithm of hyperparameter.
        """
        C = np.exp(log_C)
        full_supp = np.logical_and(self.dual_var != 0, self.dual_var != C)

        X_m = X[full_supp, :]
        y_m = y[full_supp]
        size_supp = X_m.shape[0]

        def mv(v):
            return y_m * (X_m @ ((X_m.T @ (y_m * v))))

        return LinearOperator((size_supp, size_supp), matvec=mv)

    def _get_grad(self, X, y, jac, mask, dense, C, v):
        C = C[0]
        full_supp = np.logical_and(self.dual_var != 0, self.dual_var != C)
        maskC = self.dual_var == C
        if issparse(X):
            Xy = X[full_supp, :].multiply(y[full_supp, np.newaxis])
            hessian = Xy @ X[maskC, :].multiply(y[maskC, np.newaxis]).T
        else:
            hessian = (y[full_supp] * X[full_supp, :].T).T @ \
                (y[maskC] * X[maskC, :].T)

        hessian_vec = hessian @ np.repeat(C, maskC.sum())
        jac_t_v = hessian_vec.T @ jac
        jac_t_v += np.repeat(C, maskC.sum()).T @ v[maskC]
        return jac_t_v

    def generalized_supp(self, X, v, log_C):
        """Generalized support of iterate.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        v : TODO
        log_C : float
            Log of hyperparameter.

        Returns
        -------
        TODO
        """
        full_supp = np.logical_and(
            self.dual_var != 0, self.dual_var != np.exp(log_C))
        return v[full_supp]

    def proj_hyperparam(self, X, y, log_alpha):
        """Project hyperparameter on an admissible range of values.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: ndarray, shape (2,)
            Logarithm of hyperparameter.

        Returns
        -------
        log_alpha: float
            Logarithm of projected hyperparameter.
        """
        # TODO harmonize C vs alpha, OK for alpha everywhere for me (MM)
        return np.clip(log_alpha, -16, 4)

    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta,
                              dbeta, dual_var, ddual_var, C):
        maskC = dual_var == C
        full_supp = np.logical_and(dual_var != 0, dual_var != C)
        if issparse(Xs):
            dryX = ddual_var[full_supp].T @ \
                (Xs[full_supp, :].T).multiply(ys[full_supp]).T
        else:
            dryX = ddual_var[full_supp].T @ (ys[full_supp] *
                                             Xs[full_supp, :].T).T
        quadratic_term = dryX.T @ dryX
        if np.any(maskC):
            if issparse(Xs):
                linear_term = dryX.T @ (Xs[maskC, :].T).multiply(ys[maskC]) @ \
                    dual_var[maskC]
            else:
                linear_term = dryX.T @ (ys[maskC] * Xs[maskC, :].T) @ \
                    dual_var[maskC]
        else:
            linear_term = 0
        res = quadratic_term + linear_term
        return norm(res)

    def _use_estimator(self, X, y, C, tol, max_iter):
        if self.estimator is None:
            raise ValueError("You did not pass a solver with sklearn API")
        self.estimator.set_params(tol=tol, C=C, max_iter=max_iter)
        self.estimator.fit(X, y)
        mask = self.estimator.coef_ != 0
        mask = mask[0, :]
        dense = (self.estimator.coef_)[0, :][mask]
        self.dual_var = np.abs(self.estimator.dual_coef_[0, :])
        return mask, dense, None

File Path: sparse_ho/models/svr.py
Content:
import numpy as np
from numpy.linalg import norm
from numba import njit
from scipy.sparse import issparse
import scipy.sparse.linalg as slinalg
from scipy.sparse.linalg import LinearOperator


from sparse_ho.models.base import BaseModel
from sparse_ho.utils import proj_box_svm, ind_box


@njit
def _compute_jac_aux(X, epsilon, dbeta, ddual_var, zj, L, C, j1, j2, sign):
    dF = sign * np.array([np.sum(dbeta[:, 0].T * X[j1, :]),
                          np.sum(dbeta[:, 1].T * X[j1, :])])
    dF[1] += epsilon
    ddual_var_old = ddual_var[j2, :].copy()
    dzj = ddual_var[j2, :] - dF / L[j1]
    ddual_var[j2, :] = ind_box(zj, C) * dzj
    ddual_var[j2, 0] += C * (C <= zj)
    dbeta[:, 0] += sign * (ddual_var[j2, 0] -
                           ddual_var_old[0]) * X[j1, :]
    dbeta[:, 1] += sign * (ddual_var[j2, 1] -
                           ddual_var_old[1]) * X[j1, :]


@njit
def _update_beta_jac_bcd_aux(X, y, epsilon, beta, dbeta, dual_var, ddual_var,
                             L, C, j1, j2, sign, compute_jac):
    F = sign * np.sum(beta * X[j1, :]) + epsilon - sign * y[j1]
    dual_var_old = dual_var[j2]
    zj = dual_var[j2] - F / L[j1]
    dual_var[j2] = proj_box_svm(zj, C)
    beta += sign * ((dual_var[j2] - dual_var_old) * X[j1, :])
    if compute_jac:
        _compute_jac_aux(X, epsilon, dbeta, ddual_var, zj, L, C, j1, j2, sign)


@njit
def _compute_jac_aux_sparse(
        data, indptr, indices, epsilon, dbeta, ddual_var, zj, L, C,
        j1, j2, sign):
    # get the i-st row of X in sparse format
    Xjs = data[indptr[j1]:indptr[j1+1]]
    # get the non zero indices
    idx_nz = indices[indptr[j1]:indptr[j1+1]]

    dF = sign * np.array([np.sum(dbeta[idx_nz, 0].T * Xjs),
                          np.sum(dbeta[idx_nz, 1].T * Xjs)])
    dF[1] += epsilon
    ddual_var_old = ddual_var[j2, :].copy()
    dzj = ddual_var[j2, :] - dF / L[j1]
    ddual_var[j2, :] = ind_box(zj, C) * dzj
    ddual_var[j2, 0] += C * (C <= zj)
    dbeta[idx_nz, 0] += sign * (ddual_var[j2, 0] -
                                ddual_var_old[0]) * Xjs
    dbeta[idx_nz, 1] += sign * (ddual_var[j2, 1] -
                                ddual_var_old[1]) * Xjs


@njit
def _update_beta_jac_bcd_aux_sparse(data, indptr, indices, y, epsilon, beta,
                                    dbeta, dual_var, ddual_var,
                                    L, C, j1, j2, sign, compute_jac):

    # get the i-st row of X in sparse format
    Xjs = data[indptr[j1]:indptr[j1+1]]
    # get the non zero indices
    idx_nz = indices[indptr[j1]:indptr[j1+1]]

    F = sign * np.sum(beta[idx_nz] * Xjs) + epsilon - sign * y[j1]
    dual_var_old = dual_var[j2]
    zj = dual_var[j2] - F / L[j1]
    dual_var[j2] = proj_box_svm(zj, C)
    beta[idx_nz] += sign * (dual_var[j2] - dual_var_old) * Xjs
    if compute_jac:
        _compute_jac_aux_sparse(
            data, indptr, indices, epsilon, dbeta, ddual_var, zj,
            L, C, j1, j2, sign)


class SVR(BaseModel):
    """The support vector regression without bias.

    The optimization problem is solved in the dual.

    Parameters
    ----------
    estimator: instance of ``sklearn.base.BaseEstimator``
        An estimator that follows the scikit-learn API.
    """

    def __init__(self, estimator=None):
        self.estimator = estimator
        self.dual = True  # solve the pb in the dual
        self.dual_var = None  # save the last dual_var variable for warm_start
        self.ddual_var = None  # save the last dual_var jacobian for warm_start

    def _init_dbeta_ddual_var(
            self, X, y, dense0=None, mask0=None, jac0=None, compute_jac=True):
        n_samples, n_features = X.shape
        ddual_var = np.zeros((2 * n_samples, 2))
        if jac0 is None or not compute_jac or self.ddual_var is None:
            dbeta = np.zeros((n_features, 2))
        else:
            if self.ddual_var.shape[0] != (2 * n_samples):
                dbeta = np.zeros((n_features, 2))
            else:
                ddual_var = self.ddual_var.copy()
                dbeta = X.T @ (
                    ddual_var[0:n_samples, :] -
                    ddual_var[n_samples:(2 * n_samples), :])
        return dbeta, ddual_var

    def _init_beta_dual_var(self, X, y, mask0, dense0):
        n_samples, n_features = X.shape
        dual_var = np.zeros(2 * n_samples)
        if mask0 is None or self.dual_var is None:
            beta = np.zeros(n_features)
        else:
            if self.dual_var.shape[0] != (2 * n_samples):
                beta = np.zeros(n_features)
            else:
                dual_var = self.dual_var
                beta = X.T @ (dual_var[0:n_samples] -
                              dual_var[n_samples:(2 * n_samples)])
        return beta, dual_var

    @staticmethod
    @njit
    def _update_beta_jac_bcd(
            X, y, beta, dbeta, dual_var, ddual_var,
            hyperparam, L, compute_jac=True):
        """
            beta : primal variable of the svr
            dual_var : dual used for cheap updates
            dbeta : jacobian of the primal variables
            ddual_var : jacobian of the dual variables
        """
        C = hyperparam[0]
        epsilon = hyperparam[1]
        n_samples = X.shape[0]

        for j in range(2 * n_samples):
            if j < n_samples:
                j1, j2, sign = j, j, 1
            else:  # j >= n_samples
                j1, j2, sign = j - n_samples, j, -1

            _update_beta_jac_bcd_aux(X, y, epsilon, beta, dbeta, dual_var,
                                     ddual_var, L, C, j1, j2, sign,
                                     compute_jac)

    @staticmethod
    @njit
    def _update_beta_jac_bcd_sparse(
            data, indptr, indices, y, n_samples, n_features, beta,
            dbeta, dual_var, ddual_var, hyperparam, L, compute_jac=True):
        C = hyperparam[0]
        epsilon = hyperparam[1]
        for j in range(2 * n_samples):
            if j < n_samples:
                j1, j2, sign = j, j, 1
            else:
                j1, j2, sign = j - n_samples, j, -1

            _update_beta_jac_bcd_aux_sparse(data, indptr, indices, y, epsilon,
                                            beta, dbeta, dual_var, ddual_var,
                                            L, C, j1, j2, sign, compute_jac)

    def _get_pobj0(self, dual_var, beta, hyperparam, y):
        n_samples = len(y)
        obj_prim = hyperparam[0] * np.sum(np.maximum(
            np.abs(y) - hyperparam[1], np.zeros(n_samples)))
        return obj_prim

    def _get_pobj(self, dual_var, X, beta, hyperparam, y):
        n_samples = X.shape[0]
        obj_prim = 0.5 * norm(beta) ** 2 + hyperparam[0] * np.sum(np.maximum(
            np.abs(X @ beta - y) - hyperparam[1], np.zeros(n_samples)))
        return obj_prim

    @staticmethod
    def _get_dobj(dual_var, X, beta, hyperparam, y):
        n_samples = X.shape[0]
        obj_dual = 0.5 * beta.T @ beta + hyperparam[1] * np.sum(dual_var)
        obj_dual -= np.sum(y * (dual_var[0:n_samples] -
                                dual_var[n_samples:(2 * n_samples)]))
        return -obj_dual

    @staticmethod
    def _get_jac(dbeta, mask):
        return dbeta[mask, :]

    def _init_dbeta0(self, mask, mask0, jac0):
        size_mat = mask.sum()
        if jac0 is not None:
            mask_both = np.logical_and(mask0, mask)
            size_mat = mask.sum()
            dbeta0_new = np.zeros((size_mat, 2))
            count = 0
            count_old = 0
            n_features = mask.shape[0]
            for j in range(n_features):
                if mask_both[j]:
                    dbeta0_new[count, :] = jac0[count_old, :]
                if mask0[j]:
                    count_old += 1
                if mask[j]:
                    count += 1
        else:
            dbeta0_new = np.zeros((size_mat, 2))
        return dbeta0_new

    def _init_dbeta(self, n_features):
        if self.dbeta is not None:
            return self.dbeta
        else:
            return np.zeros((n_features, 2))

    def _init_ddual_var(self, dbeta, X, y, sign_beta, hyperparam):
        dual_var = self.dual_var
        C = hyperparam[0]
        n_samples = X.shape[0]
        sign = np.zeros(dual_var.shape[0])
        sign[dual_var == 0.0] = -1.0
        sign[dual_var == C] = 1.0
        ddual_var = np.zeros((2 * X.shape[0], 2))
        if np.any(sign == 1.0):
            ddual_var[sign == 1.0, 0] = C
            ddual_var[sign == 1.0, 1] = 0
        self.ddual_var = ddual_var
        self.dbeta = X.T @ (
            ddual_var[0:n_samples, :] -
            ddual_var[n_samples:(2 * n_samples), :])
        return ddual_var

    @staticmethod
    @njit
    def _update_only_jac(X, y, dual_var, dbeta, ddual_var,
                         L, hyperparam, sign_beta):
        n_samples = L.shape[0]
        C = hyperparam[0]
        epsilon = hyperparam[1]
        gen_supp = np.zeros(dual_var.shape[0])
        gen_supp[dual_var == 0.0] = -1.0
        gen_supp[dual_var == C] = 1.0
        for j in np.arange(0, (2 * n_samples))[gen_supp == 0.0]:
            if j < n_samples:
                j1, j2, sign = j, j, 1
            else:
                j1, j2, sign = j - n_samples, j, -1

            _compute_jac_aux(
                X, epsilon, dbeta, ddual_var, dual_var[j2], L, C, j1, j2, sign)

    @staticmethod
    @njit
    def _update_only_jac_sparse(
            data, indptr, indices, y, n_samples, n_features,
            dbeta, dual_var, ddual_var, L, hyperparam, sign_beta):
        n_samples = L.shape[0]
        C = hyperparam[0]
        epsilon = hyperparam[1]
        # non_zeros = np.where(L != 0)[0]
        gen_supp = np.zeros(dual_var.shape[0])
        gen_supp[dual_var == 0.0] = -1.0
        gen_supp[dual_var == C] = 1.0

        for j in np.arange(0, (2 * n_samples))[gen_supp == 0.0]:
            if j < n_samples:
                j1, j2, sign = j, j, 1
            else:
                j1, j2, sign = j - n_samples, j, -1

            _compute_jac_aux_sparse(
                data, indptr, indices, epsilon, dbeta, ddual_var, dual_var[j2],
                L, C, j1, j2, sign)

    @staticmethod
    @njit
    def _reduce_alpha(alpha, mask):
        return alpha

    def get_L(self, X):
        """Compute Lipschitz constant of datafit.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.

        Returns
        -------
        L: float
            The Lipschitz constant.
        """
        if issparse(X):
            return slinalg.norm(X, axis=1) ** 2
        else:
            return norm(X, axis=1) ** 2

    @staticmethod
    def reduce_X(X, mask):
        """Reduce design matrix to generalized support.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        mask : ndarray, shape (n_features,)
            Generalized support.
        """
        return X[:, mask]

    @staticmethod
    def reduce_y(y, mask):
        """Reduce observation vector to generalized support.

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Observation vector.
        mask : ndarray, shape (n_features,)  TODO shape n_samples right?
            Generalized support.
        """
        return y

    def sign(self, beta, log_hyperparams):
        """Get sign of iterate.

        Parameters
        ----------
        beta : ndarray, shape TODO
        log_hyperparams : ndarray, shape TODO
        """
        # TODO harmonize with other models
        sign = np.zeros_like(beta)
        sign[np.isclose(beta, 0.0)] = -1.0
        sign[np.isclose(beta, np.exp(log_hyperparams[0]))] = 1.0
        return sign

    def get_jac_v(self, X, y, mask, dense, jac, v):
        """Compute hypergradient.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        jac: TODO
        v: TODO
        """
        return jac.T @ v(mask, dense)

    @staticmethod
    def get_full_jac_v(mask, jac_v, n_features):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        n_features: int
            Number of features.
        """
        return jac_v

    def get_mat_vec(self, X, y, mask, dense, log_alpha):
        """Returns a LinearOperator computing the matrix vector product
        with the Hessian of datafit. It is necessary to avoid storing a
        potentially large matrix, and keep advantage of the sparsity of X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        log_alpha: ndarray
            Logarithm of hyperparameter.
        """
        C = np.exp(log_alpha[0])
        n_samples = X.shape[0]
        dual_coef = self.dual_var[0:n_samples] - \
            self.dual_var[n_samples:(2 * n_samples)]
        full_supp = np.logical_and(
            np.logical_not(
                np.isclose(np.abs(dual_coef), 0)),
            np.logical_not(
                np.isclose(np.abs(dual_coef), C)))

        X_m = X[full_supp, :]
        size_supp = X_m.shape[0]

        def mv(v):
            return X_m @ (X_m.T @ v)
        return LinearOperator((size_supp, size_supp), matvec=mv)

    def get_dual_v(self, mask, dense, X, y, v, log_hyperparam):
        """TODO

        Parameters
        ----------
        mask: TODO
        dense: TODO
        X: TODO
        y: TODO
        v: TODO
        log_hyperparam: TODO
        """
        if v.shape[0] != 0:
            return X[:, mask] @ v
        else:
            return np.zeros(X.shape[0])

    def _get_grad(self, X, y, jac, mask, dense, hyperparam, v):
        C = hyperparam[0]
        n_samples = X.shape[0]
        epsilon = hyperparam[1]
        alpha = self.dual_var[0:n_samples] - \
            self.dual_var[n_samples:(2 * n_samples)]
        full_supp = np.logical_and(
            np.logical_not(
                np.isclose(np.abs(alpha), 0)),
            np.logical_not(
                np.isclose(np.abs(alpha), C)))
        maskC = np.isclose(np.abs(alpha), C)
        hessian = X[full_supp, :] @ X[maskC, :].T
        hessian_vec = hessian @ alpha[maskC]
        jac_t_v = hessian_vec.T @ jac
        jac_t_v += alpha[maskC].T @ v[maskC]
        jac_t_v2 = epsilon * np.sign(alpha[full_supp]) @ jac
        return np.array([jac_t_v, jac_t_v2])

    def generalized_supp(self, X, v, log_hyperparam):
        """Generalized support of iterate.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        v : TODO
        log_hyperparam : float TODO harmonize names
            Log of hyperparameter.

        Returns
        -------
        TODO
        """
        n_samples = int(self.dual_var.shape[0] / 2)
        C = np.exp(log_hyperparam[0])
        alpha = self.dual_var[0:n_samples] - \
            self.dual_var[n_samples:(2 * n_samples)]
        full_supp = np.logical_and(
            np.logical_not(
                np.isclose(np.abs(alpha), 0)),
            np.logical_not(
                np.isclose(np.abs(alpha), C)))
        return v[full_supp]

    def proj_hyperparam(self, X, y, log_alpha):
        """Project hyperparameter on an admissible range of values.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: ndarray, shape (2,)
            Logarithm of hyperparameter.

        Returns
        -------
        log_alpha: float
            Logarithm of projected hyperparameter.
        """
        return np.clip(log_alpha, -16, [5, 2])

    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta,
                              dbeta, dual_var, ddual_var, hyperparam):
        C = hyperparam[0]
        alpha = dual_var[0:n_samples] - dual_var[n_samples:(2 * n_samples)]
        dalpha = ddual_var[0:n_samples, 0] - \
            ddual_var[n_samples:(2 * n_samples), 0]

        maskC = np.isclose(np.abs(alpha), C)
        full_supp = np.logical_and(
            np.logical_not(
                np.isclose(np.abs(alpha), 0)),
            np.logical_not(
                np.isclose(np.abs(alpha), C)))

        alphaX = dalpha[full_supp].T @ Xs[full_supp, :]
        quadratic_term = alphaX.T @ alphaX

        linear_term = alphaX.T @ Xs[maskC, :].T @ alpha[maskC]
        return norm(quadratic_term + linear_term)

    def _use_estimator(self, X, y, hyperparam, tol):
        if self.estimator is None:
            raise ValueError("You did not pass a solver with sklearn API")
        self.estimator.set_params(
            epsilon=hyperparam[1], tol=tol, C=hyperparam[0],
            fit_intercept=False)
        self.estimator.fit(X, y)
        mask = self.estimator.coef_ != 0
        dense = self.estimator.coef_[mask]
        return mask, dense, None

File Path: sparse_ho/models/wlasso.py
Content:
import numpy as np
from numba import njit
from numpy.linalg import norm
from scipy.sparse import issparse
import scipy.sparse.linalg as slinalg
from scipy.sparse.linalg import LinearOperator

from sparse_ho.models.base import BaseModel
from sparse_ho.utils import ST, init_dbeta0_new_p


class WeightedLasso(BaseModel):
    r"""Linear Model trained with weighted L1 regularizer (aka weighted Lasso).

    The optimization objective for weighted Lasso is:

    ..math::

        ||y - Xw||^2_2 / (2 * n_samples) + \sum_i^{n_features} \alpha_i |wi|

    Parameters
    ----------
    estimator: instance of ``sklearn.base.BaseEstimator``
        An estimator that follows the scikit-learn API.
    """

    def __init__(self, estimator=None):
        self.estimator = estimator

    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,
                              dense0=None, compute_jac=True):
        n_samples, n_features = X.shape
        dbeta = np.zeros((n_features, n_features))
        ddual_var = np.zeros((n_samples, n_features))
        if jac0 is not None:
            dbeta[np.ix_(mask0, mask0)] = jac0.copy()
            ddual_var[:, mask0] = - X[:, mask0] @ jac0
        return dbeta, ddual_var

    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):
        beta = np.zeros(X.shape[1])
        if dense0 is None or len(dense0) == 0:
            dual_var = y.copy()
            dual_var = dual_var.astype(np.float)
        else:
            beta[mask0] = dense0.copy()
            dual_var = y - X[:, mask0] @ dense0
        return beta, dual_var

    @staticmethod
    @njit
    def _update_beta_jac_bcd(
            X, y, beta, dbeta, dual_var, ddual_var,
            alpha, L, compute_jac=True):
        n_samples, n_features = X.shape
        non_zeros = np.where(L != 0)[0]

        for j in non_zeros:
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j, :].copy()
            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)
            beta[j:j+1] = ST(zj, alpha[j] / L[j])
            if compute_jac:
                dzj = dbeta[j, :] + X[:, j] @ ddual_var / (L[j] * n_samples)
                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj
                dbeta[j:j+1, j] -= alpha[j] * np.sign(beta[j]) / L[j]
                # update residuals
                ddual_var -= np.outer(X[:, j], (dbeta[j, :] - dbeta_old))
            dual_var -= X[:, j] * (beta[j] - beta_old)

    @staticmethod
    @njit
    def _update_beta_jac_bcd_sparse(
            data, indptr, indices, y, n_samples, n_features, beta,
            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):
        non_zeros = np.where(L != 0)[0]

        for j in non_zeros:
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get non zero idices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            ###########################################
            beta_old = beta[j]
            if compute_jac:
                dbeta_old = dbeta[j, :].copy()
            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)
            beta[j:j+1] = ST(zj, alphas[j] / L[j])
            if compute_jac:
                dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \
                    (L[j] * n_samples)
                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj
                dbeta[j:j+1, j] -= alphas[j] * np.sign(beta[j]) / L[j]
                # update residuals
                ddual_var[idx_nz, :] -= np.outer(
                    Xjs, (dbeta[j, :] - dbeta_old))
            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)

    @staticmethod
    @njit
    def _update_bcd_jac_backward(
            X, alpha, jac_t_v, beta, v_, L):
        n_samples, n_features = X.shape
        sign_beta = np.sign(beta)
        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):
            jac_t_v[j] = jac_t_v[j] - (v_[j]) * alpha[j] * sign_beta[j] / L[j]
            v_[j] *= np.abs(sign_beta[j])
            v_ -= v_[j] / (L[j] * n_samples) * X[:, j] @ X
        return jac_t_v

    @staticmethod
    def _get_pobj(dual_var, X, beta, alphas, y=None):
        n_samples = dual_var.shape[0]
        return (
            norm(dual_var) ** 2 / (2 * n_samples) + norm(alphas * beta, 1))

    @staticmethod
    def _get_pobj0(dual_var, beta, alphas, y=None):
        n_samples = dual_var.shape[0]
        return norm(y) ** 2 / (2 * n_samples)

    @staticmethod
    def _get_jac(dbeta, mask):
        return dbeta[np.ix_(mask, mask)]

    @staticmethod
    def _init_dbeta0(mask, mask0, jac0):
        size_mat = mask.sum()
        if jac0 is None:
            dbeta0_new = np.zeros((size_mat, size_mat))
        else:
            dbeta0_new = init_dbeta0_new_p(jac0, mask, mask0)
        return dbeta0_new

    @staticmethod
    def _init_dbeta(n_features):
        dbeta = np.zeros((n_features, n_features))
        return dbeta

    @staticmethod
    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):
        return - X @ dbeta

    @staticmethod
    def _init_g_backward(jac_v0, n_features):
        if jac_v0 is None:
            return np.zeros(n_features)
        else:
            return jac_v0

    @staticmethod
    @njit
    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var, L,
                         alpha, sign_beta):
        n_samples, n_features = Xs.shape
        for j in range(n_features):
            dbeta_old = dbeta[j, :].copy()
            dbeta[j:j+1, :] = dbeta[j, :] + Xs[:, j] @ ddual_var / \
                (L[j] * n_samples)
            dbeta[j:j+1, j] -= alpha[j] * sign_beta[j] / L[j]
            # update residuals
            ddual_var -= np.outer(Xs[:, j], (dbeta[j, :] - dbeta_old))

    @staticmethod
    @njit
    def _update_only_jac_sparse(
            data, indptr, indices, y, n_samples, n_features, dbeta, dual_var,
            ddual_var, L, alpha, sign_beta):
        for j in range(n_features):
            # get the j-st column of X in sparse format
            Xjs = data[indptr[j]:indptr[j+1]]
            # get the non zero idices
            idx_nz = indices[indptr[j]:indptr[j+1]]
            # store old beta j for fast update
            dbeta_old = dbeta[j, :].copy()

            dbeta[j:j+1, :] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)
            dbeta[j, j] -= alpha[j] * sign_beta[j] / L[j]
            ddual_var[idx_nz] -= np.outer(Xjs, (dbeta[j] - dbeta_old))

    # @njit
    @staticmethod
    def _reduce_alpha(alpha, mask):
        return alpha[mask]

    @staticmethod
    def get_full_jac_v(mask, jac_v, n_features):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        n_features: int
            Number of features.
        """
        # MM sorry I don't get what this does
        # TODO n_features should be n_hyperparams, right ?
        res = np.zeros(n_features)
        res[mask] = jac_v
        return res

    @staticmethod
    def get_mask_jac_v(mask, jac_v):
        """TODO

        Parameters
        ----------
        mask: TODO
        jac_v: TODO
        """
        return jac_v[mask]

    @staticmethod
    def _get_grad(X, y, jac, mask, dense, alphas, v):
        size_supp = mask.sum()
        jac_t_v = np.zeros(size_supp)
        jac_t_v = alphas[mask] * np.sign(dense) * jac
        return jac_t_v

    def proj_hyperparam(self, X, y, log_alpha):
        """Project hyperparameter on an admissible range of values.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        log_alpha: ndarray, shape (n_features,)
            Logarithm of hyperparameter.

        Returns
        -------
        log_alpha: ndarray, shape (n_features,)
            Logarithm of projected hyperparameter.
        """
        if not hasattr(self, "log_alpha_max"):
            alpha_max = np.max(np.abs(X.T @ y)) / X.shape[0]
            self.log_alpha_max = np.log(alpha_max)
        log_alpha = np.clip(log_alpha, self.log_alpha_max - 5,
                            self.log_alpha_max + np.log(0.9))
        return log_alpha

    @staticmethod
    def get_L(X):
        """Compute Lipschitz constant of datafit.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.

        Returns
        -------
        L: float
            The Lipschitz constant.
        """
        if issparse(X):
            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])
        else:
            return norm(X, axis=0) ** 2 / (X.shape[0])

    @staticmethod
    def get_mat_vec(X, y, mask, dense, log_alpha):
        """Returns a LinearOperator computing the matrix vector product
        with the Hessian of datafit. It is necessary to avoid storing a
        potentially large matrix, and keep advantage of the sparsity of X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        log_alpha: ndarray
            Logarithm of hyperparameter.
        """
        X_m = X[:, mask]
        n_samples, size_supp = X_m.shape

        def mv(v):
            return X_m.T @ (X_m @ v) / n_samples
        return LinearOperator((size_supp, size_supp), matvec=mv)

    def _use_estimator(self, X, y, alpha, tol):
        self.estimator.set_params(tol=tol)
        self.estimator.weights = alpha
        self.estimator.fit(X, y)
        mask = self.estimator.coef_ != 0
        dense = (self.estimator.coef_)[mask]
        return mask, dense, None

    @staticmethod
    def reduce_X(X, mask):
        """Reduce design matrix to generalized support.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        mask : ndarray, shape (n_features,)
            Generalized support.
        """
        return X[:, mask]

    @staticmethod
    def reduce_y(y, mask):
        """Reduce observation vector to generalized support.

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Observation vector.
        mask : ndarray, shape (n_features,)  TODO shape n_samples right?
            Generalized support.
        """
        return y

    def sign(self, x, log_alpha):
        """Get sign of iterate.

        Parameters
        ----------
        x : ndarray, shape TODO
        log_alpha : ndarray, shape TODO
            Logarithm of hyperparameter.
        """
        return np.sign(x)

    def get_beta(self, X, y, mask, dense):
        """Return primal iterate.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        """
        # TODO what's the use of this function? it does nothing for all models
        return mask, dense

    def get_jac_v(self, X, y, mask, dense, jac, v):
        """Compute hypergradient.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Design matrix.
        y: ndarray, shape (n_samples,)
            Observation vector.
        mask: ndarray, shape (n_features,)
            Mask corresponding to non zero entries of beta.
        dense: ndarray, shape (mask.sum(),)
            Non zero entries of beta.
        jac: TODO
        v: TODO
        """
        # TODO this is the same for Lasso, Enet, Wlasso. Maybe inherit from
        # a common class, LinearModelPrimal or something?
        return jac.T @ v(mask, dense)

    def generalized_supp(self, X, v, log_alpha):
        """Generalized support of iterate.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Design matrix.
        v : TODO
        log_alpha : float
            Log of hyperparameter.

        Returns
        -------
        TODO
        """
        return v

    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta,
                              dbeta, dual_var, ddual_var, alpha):
        return(
            norm(ddual_var.T @ ddual_var +
                 n_samples * alpha * sign_beta @ dbeta))

File Path: sparse_ho/optimizers/__init__.py
Content:
from sparse_ho.optimizers.gradient_descent import GradientDescent
from sparse_ho.optimizers.adam import Adam
from sparse_ho.optimizers.line_search import LineSearch

__all__ = [
    'GradientDescent',
    'LineSearch']

File Path: sparse_ho/optimizers/adam.py
Content:
import numpy as np
from numpy.linalg import norm

from sparse_ho.optimizers.base import BaseOptimizer


class Adam(BaseOptimizer):
    """ADAM optimizer for the outer problem.

    This Adam code is taken from
    https://github.com/sagarvegad/Adam-optimizer/blob/master/Adam.py

    Parameters
    ----------
    n_outer: int, optional (default=100).
        Number of maximum updates of alpha.
    epsilon: float, optional (default=1e-3)
    lr: float, optional (default=1e-2)
        Learning rate
    beta_1: float, optional (default=0.9)
    beta_2: float, optional (default=0.999)
    verbose: bool, optional (default=False)
        Indicates whether information about hyperparameter
        optimization process is printed or not.
    tol : float, optional (default=1e-5)
        Tolerance for the inner optimization solver.
    t_max: float, optional (default=10_000)
        Maximum running time threshold in seconds.
    """

    def __init__(
            self, n_outer=100, epsilon=1e-3, lr=0.01, beta_1=0.9, beta_2=0.999,
            verbose=False, tol=1e-5, t_max=10000):
        self.n_outer = n_outer
        self.epsilon = epsilon
        self.lr = lr
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.verbose = verbose
        self.tol = tol
        self.t_max = t_max

    def _grad_search(
            self, _get_val_grad, proj_hyperparam, log_alpha0, monitor):

        log_alpha = log_alpha0
        # log_alpha0 = 0  # initialize the vector
        m_t = 0
        v_t = 0
        t = 0

        for i in range(self.n_outer):
            t += 1
            value_outer, grad = _get_val_grad(log_alpha, self.tol, monitor)

            if self.verbose:
                print(
                    "Iteration %i/%i || " % (i+1, self.n_outer) +
                    "Value outer criterion: %.2e || " % value_outer +
                    "norm grad %.2e" % norm(grad))

            if (i > 1) and (monitor.objs[-1] > monitor.objs[-2]):
                break
            # updates the moving averages of the gradient
            m_t = self.beta_1*m_t + (1 - self.beta_1) * grad
            # updates the moving averages of the squared gradient
            v_t = self.beta_2*v_t + (1 - self.beta_2) * (grad * grad)
            m_cap = m_t/(1-(self.beta_1**t))
            # calculates the bias-corrected estimates
            v_cap = v_t/(1-(self.beta_2**t))
            # calculates the bias-corrected estimates
            logh_alpha_prev = log_alpha
            # updates the parameters
            log_alpha = log_alpha - (self.lr*m_cap) / (
                np.sqrt(v_cap) + self.epsilon)
            # checks if it is converged or not
            if np.allclose(log_alpha, logh_alpha_prev):
                break

File Path: sparse_ho/optimizers/base.py
Content:
from abc import ABC, abstractmethod


class BaseOptimizer(ABC):

    @abstractmethod
    def __init__(cls):
        pass

    @abstractmethod
    def _grad_search(
            self, _get_val_grad, proj_hyperparam, log_alpha0, monitor):
        return NotImplemented

File Path: sparse_ho/optimizers/gradient_descent.py
Content:

import numpy as np
from numpy.linalg import norm

from sparse_ho.optimizers.base import BaseOptimizer


class GradientDescent(BaseOptimizer):
    """Gradient descent for the outer problem.
    This gradient descent scheme uses a (heuristic) adaptive stepsize:

    log_alphak = log_alphak - p_grad_norm * grad_outer / norm(grad_outer)

    Parameters
    ----------
    n_outer: int, optional (default=100).
        number of maximum updates of alpha.
    step_size: float
        stepsize of the gradient descent
    p_grad_norm: float
        Coefficient multiplying grad_outer / norm(grad_outer) in the gradient
        descent.
    verbose: bool, optional (default=False)
        Indicates whether information about hyperparameter
        optimization process is printed or not.
    tol : float, optional (default=1e-5)
        Tolerance for the inner optimization solver.
    tol_decrease: bool
        To use or not a tolerance decrease strategy in the gradient descent.
    t_max: float, optional (default=10000)
        Maximum running time threshold in seconds.
    """

    def __init__(
            self, n_outer=100, step_size=None, p_grad_norm=1,
            verbose=False, tol=1e-5, tol_decrease=None, t_max=10_000):
        self.n_outer = n_outer
        self.step_size = step_size
        self.verbose = verbose
        self.tol = tol
        self.t_max = t_max
        self.p_grad_norm = p_grad_norm
        self.has_gone_up = False
        self.tol_decrease = tol_decrease

    def _grad_search(
            self, _get_val_grad, proj_hyperparam, log_alpha0, monitor):
        is_multiparam = isinstance(log_alpha0, np.ndarray)
        if is_multiparam:
            log_alphak = log_alpha0.copy()
        else:
            log_alphak = log_alpha0

        if self.tol_decrease is not None:
            tols = np.geomspace(1e-2, self.tol, num=self.n_outer)
        else:
            tols = np.ones(self.n_outer) * self.tol

        for i, tol in enumerate(tols):
            value_outer, grad_outer = _get_val_grad(
                log_alphak, tol, monitor)
            if (self.step_size is None or i < 20) and not self.has_gone_up:
                self.step_size = self.p_grad_norm / (
                    np.linalg.norm(grad_outer) + 1e-12)
            log_alphak -= self.step_size * grad_outer

            if self.verbose:
                print(
                    "Iteration %i/%i ||" % (i+1, self.n_outer) +
                    "Value outer criterion: %.2e ||" % value_outer +
                    "norm grad %.2e" % norm(grad_outer))
            if len(monitor.times) > 0 and monitor.times[-1] > self.t_max:
                break

            if i > 0 and (monitor.objs[-1] > monitor.objs[-2]):
                self.step_size /= 10
                self.has_gone_up = True
        return log_alphak, value_outer, grad_outer

File Path: sparse_ho/optimizers/line_search.py
Content:
import numpy as np
from numpy.linalg import norm

from sparse_ho.optimizers.base import BaseOptimizer


class LineSearch(BaseOptimizer):
    """Gradient descent with line search for the outer problem.

    The code is taken from here:
    https://github.com/fabianp/hoag/blob/master/hoag/hoag.py

    Parameters
    ----------
    n_outer: int, optional (default=100).
        number of maximum updates of alpha.
    verbose: bool, optional (default=False)
        Verbosity.
    tolerance_decrease: string, optional (default="constant")
        Tolerance decrease strategy for approximate gradient.
    tol : float, optional (default=1e-5)
        Tolerance for the inner optimization solver.
    t_max: float, optional (default=10000)
        Maximum running time threshold in seconds.
    """

    def __init__(
            self, n_outer=100, verbose=False, tolerance_decrease='constant',
            tol=1e-5, t_max=10000):
        self.n_outer = n_outer
        self.verbose = verbose
        self.tolerance_decrease = tolerance_decrease
        self.tol = tol
        self.t_max = t_max

    def _grad_search(
            self, _get_val_grad, proj_hyperparam, log_alpha0, monitor):

        is_multiparam = isinstance(log_alpha0, np.ndarray)
        if is_multiparam:
            log_alphak = log_alpha0.copy()
            old_log_alphak = log_alphak.copy()
        else:
            log_alphak = log_alpha0
            old_log_alphak = log_alphak

        grad_norms = []

        L_log_alpha = None
        value_outer_old = np.inf

        if self.tolerance_decrease == 'exponential':
            seq_tol = np.geomspace(1e-2, self.tol, self.n_outer)
        else:
            seq_tol = self.tol * np.ones(self.n_outer)

        for i in range(self.n_outer):
            tol = seq_tol[i]
            try:
                old_tol = seq_tol[i - 1]
            except Exception:
                old_tol = seq_tol[0]
            value_outer, grad_outer = _get_val_grad(
                log_alphak, tol=tol, monitor=monitor)

            grad_norms.append(norm(grad_outer))
            if np.isnan(grad_norms[-1]):
                print("Nan present in gradient")
                break

            if L_log_alpha is None:
                if grad_norms[-1] > 1e-3:
                    # make sure we are not selecting a step size
                    # that is too small
                    if is_multiparam:
                        L_log_alpha = grad_norms[-1] / np.sqrt(len(log_alphak))
                    else:
                        L_log_alpha = grad_norms[-1]
                else:
                    L_log_alpha = 1
            step_size = (1. / L_log_alpha)
            try:
                old_log_alphak = log_alphak.copy()
            except Exception:
                old_log_alphak = log_alphak
            log_alphak -= step_size * grad_outer

            incr = norm(step_size * grad_outer)
            C = 0.25
            factor_L_log_alpha = 1.0
            if value_outer <= value_outer_old + C * tol + \
                    old_tol * (C + factor_L_log_alpha) * incr - \
                    factor_L_log_alpha * (L_log_alpha) * incr * incr:
                L_log_alpha *= 0.95
                if self.verbose > 1:
                    print('increased step size')
                log_alphak -= step_size * grad_outer

            elif value_outer >= 1.2 * value_outer_old:
                if self.verbose > 1:
                    print('decrease step size')
                # decrease step size
                L_log_alpha *= 2
                if is_multiparam:
                    log_alphak = old_log_alphak.copy()
                else:
                    log_alphak = old_log_alphak
                print('!!step size rejected!!', value_outer, value_outer_old)
                value_outer, grad_outer = _get_val_grad(
                    log_alphak, tol=tol, monitor=monitor)

                tol *= 0.5
            else:
                old_log_alphak = log_alphak.copy()
                log_alphak -= step_size * grad_outer

            log_alphak = proj_hyperparam(log_alphak)
            value_outer_old = value_outer

            if self.verbose:
                print(
                    "Iteration %i/%i || " % (i+1, self.n_outer) +
                    "Value outer criterion: %.2e || " % value_outer +
                    "norm grad %.2e" % norm(grad_outer))
            if monitor.times[-1] > self.t_max:
                break
        return log_alphak, value_outer, grad_outer

File Path: sparse_ho/tests/__init__.py
Content:

File Path: sparse_ho/tests/common.py
Content:
import itertools
import numpy as np
from scipy.sparse import csc_matrix
from sklearn import linear_model

import celer
from celer.datasets import make_correlated_data

from sparse_ho.models import (
    Lasso, ElasticNet, WeightedLasso, SparseLogreg, SVM, SVR, SimplexSVR)
from sparse_ho.tests.cvxpylayer import (
    enet_cvxpy, weighted_lasso_cvxpy, logreg_cvxpy, lasso_cvxpy,
    lasso_sure_cvxpy, svm_cvxpy, svr_cvxpy, ssvr_cvxpy)

# Generate data
n_samples, n_features = 10, 10
X, y, _ = make_correlated_data(
    n_samples, n_features, corr=0.1, snr=3, random_state=42)
sigma_star = 0.1

y = np.sign(y)
X_s = csc_matrix(X)
idx_train = np.arange(0, n_samples//2)
idx_val = np.arange(n_samples//2, n_features)

# Set alpha for the Lasso
alpha_max = (np.abs(X[idx_train, :].T @ y[idx_train])).max() / n_samples
p_alpha = 0.8
alpha = p_alpha * alpha_max
log_alpha = np.log(alpha)
log_alpha_max = np.log(alpha_max)

# Set alpha1 alpha2 for the enet
alpha_1 = p_alpha * alpha_max
alpha_2 = 0.1
log_alpha1 = np.log(alpha_1)
log_alpha2 = np.log(alpha_2)

dict_log_alpha = {}
dict_log_alpha["lasso"] = log_alpha
dict_log_alpha["enet"] = np.array([log_alpha1, log_alpha2])
tab = np.linspace(1, 1000, n_features)
dict_log_alpha["wLasso"] = log_alpha + np.log(tab / tab.max())
dict_log_alpha["logreg"] = (log_alpha - np.log(2))
dict_log_alpha["svm"] = 1e-4
dict_log_alpha["svr"] = np.log(np.array([1e-2, 1e-1]))
dict_log_alpha["ssvr"] = np.log(np.array([0.01, 0.1]))

# Set models to be tested
models = {}
models["lasso"] = Lasso(estimator=None)
models["enet"] = ElasticNet(estimator=None)
models["wLasso"] = WeightedLasso(estimator=None)
models["logreg"] = SparseLogreg(estimator=None)
models["svm"] = SVM(estimator=None)
models["svr"] = SVR(estimator=None)
models["ssvr"] = SimplexSVR(estimator=None)


custom_models = {}
custom_models["lasso"] = Lasso(estimator=celer.Lasso(
    warm_start=True, fit_intercept=False))
custom_models["enet"] = ElasticNet(
    estimator=linear_model.ElasticNet(warm_start=True, fit_intercept=False))
custom_models["logreg"] = SparseLogreg(
    estimator=celer.LogisticRegression(warm_start=True, fit_intercept=False))

# Compute "ground truth" with cvxpylayer
dict_cvxpy_func = {
    'lasso': lasso_cvxpy,
    'enet': enet_cvxpy,
    'wLasso': weighted_lasso_cvxpy,
    'logreg': logreg_cvxpy,
    'svm': svm_cvxpy,
    'svr': svr_cvxpy,
    'ssvr': ssvr_cvxpy
}

dict_vals_cvxpy = {}
dict_grads_cvxpy = {}
for model in models.keys():
    val_cvxpy, grad_cvxpy = dict_cvxpy_func[model](
        X, y, np.exp(dict_log_alpha[model]), idx_train, idx_val)
    dict_vals_cvxpy[model, 'MSE'] = val_cvxpy
    grad_cvxpy *= np.exp(dict_log_alpha[model])
    dict_grads_cvxpy[model, 'MSE'] = grad_cvxpy

val_cvxpy, grad_cvxpy = lasso_sure_cvxpy(
    X, y, np.exp(dict_log_alpha["lasso"]), sigma_star)
grad_cvxpy *= np.exp(dict_log_alpha["lasso"])
dict_vals_cvxpy["lasso", "SURE"] = val_cvxpy
dict_grads_cvxpy["lasso", "SURE"] = grad_cvxpy


# log alpha to be tested by checkgrad
dict_list_log_alphas = {}
dict_list_log_alphas["lasso"] = np.log(
    np.geomspace(alpha_max/2, alpha_max/5, num=5))
dict_list_log_alphas["wLasso"] = [
    log_alpha * np.ones(n_features) for log_alpha in
    dict_list_log_alphas["lasso"]]
dict_list_log_alphas["logreg"] = np.log(
    np.geomspace(alpha_max/5, alpha_max/40, num=5))
dict_list_log_alphas["enet"] = [np.array(i) for i in itertools.product(
    dict_list_log_alphas["lasso"], dict_list_log_alphas["lasso"])]
dict_list_log_alphas["svm"] = np.log(np.geomspace(1e-8, 1e-5, num=5))
dict_list_log_alphas["svr"] = [
    np.array(i) for i in itertools.product(
        np.log(np.geomspace(1e-2, 1e-1, num=5)),
        np.log(np.geomspace(1e-2, 1e-1, num=5)))]
dict_list_log_alphas["ssvr"] = [
    np.array(i) for i in itertools.product(
        np.log(np.geomspace(0.01, 0.1, num=5)),
        np.log(np.geomspace(0.01, 0.1, num=5)))]


def get_grad_outer(mask, dense):
    return 2 * (X[np.ix_(idx_val, mask)].T @ (
        X[np.ix_(idx_val, mask)] @ dense - y[idx_val])) / len(idx_val)


list_model_crit = [
    ('lasso', 'MSE'),
    ('enet', 'MSE'),
    ('wLasso', 'MSE'),
    ('lasso', 'SURE'),
    ('logreg', 'logistic'),
    ('svm', 'MSE'),
    ('svr', 'MSE'),
    ('ssvr', 'MSE')
]

list_model_names = ["lasso", "enet", "wLasso", "logreg", "svm", "svr", "ssvr"]

File Path: sparse_ho/tests/cvxpylayer.py
Content:
import cvxpy as cp
import numpy as np
import torch
from cvxpylayers.torch import CvxpyLayer
from sklearn.utils import check_random_state

torch.set_default_dtype(torch.double)


def lasso_cvxpy(X, y, lambd, idx_train, idx_val):
    val, grad = enet_cvxpy(X, y, [float(lambd), 0], idx_train, idx_val)
    return val, grad[0]


def enet_cvxpy(X, y, lambda_alpha, idx_train, idx_val):
    Xtrain, Xtest, ytrain, ytest = map(
        torch.from_numpy, [
            X[idx_train, :], X[idx_val], y[idx_train], y[idx_val]])

    n_samples_train, n_features = Xtrain.shape

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    lambda_cp = cp.Parameter(nonneg=True)
    alpha_cp = cp.Parameter(nonneg=True)

    # set up objective
    loss = ((1 / (2 * n_samples_train)) *
            cp.sum(cp.square(Xtrain @ beta_cp - ytrain)))
    reg = (lambda_cp * cp.norm1(beta_cp) +
           alpha_cp * cp.sum_squares(beta_cp) / 2)
    objective = loss + reg

    # define problem
    problem = cp.Problem(cp.Minimize(objective))
    assert problem.is_dpp()

    # solve problem
    layer = CvxpyLayer(problem, [lambda_cp, alpha_cp], [beta_cp])
    lambda_alpha_th = torch.tensor(lambda_alpha, requires_grad=True)
    beta_, = layer(lambda_alpha_th[0], lambda_alpha_th[1],
                   solver_args={'eps': 1e-6,  'max_iters': 2000})

    # get test loss and its gradient
    test_loss = (Xtest @ beta_ - ytest).pow(2).mean()
    test_loss.backward()

    val = test_loss.detach().numpy()
    grad = np.array(lambda_alpha_th.grad)
    return val, grad


def weighted_lasso_cvxpy(X, y, lambdas, idx_train, idx_val):
    Xtrain, Xtest, ytrain, ytest = map(
        torch.from_numpy, [
            X[idx_train, :], X[idx_val], y[idx_train], y[idx_val]])

    n_samples_train, n_features = Xtrain.shape

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    lambdas_cp = cp.Parameter(shape=n_features, nonneg=True)

    # set up objective
    loss = ((1 / (2 * n_samples_train)) *
            cp.sum(cp.square(Xtrain @ beta_cp - ytrain)))
    reg = lambdas_cp @ cp.abs(beta_cp)
    objective = loss + reg

    # define problem
    problem = cp.Problem(cp.Minimize(objective))
    assert problem.is_dpp()

    # solve problem
    layer = CvxpyLayer(problem, [lambdas_cp], [beta_cp])
    lambdas_th = torch.tensor(lambdas, requires_grad=True)
    beta_, = layer(lambdas_th)

    # get test loss and it's gradient
    test_loss = (Xtest @ beta_ - ytest).pow(2).mean()
    test_loss.backward()

    val = test_loss.detach().numpy()
    grad = np.array(lambdas_th.grad)
    return val, grad


def logreg_cvxpy(X, y, alpha, idx_train, idx_val):
    alpha = float(alpha)
    assert np.all(np.unique(y) == np.array([-1, 1]))
    Xtrain, Xtest, ytrain, ytest = map(
        torch.from_numpy, [
            X[idx_train, :], X[idx_val], y[idx_train], y[idx_val]])

    n_samples_train, n_features = Xtrain.shape

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    alpha_cp = cp.Parameter(nonneg=True)

    # set up objective
    loss = cp.sum(
        cp.logistic(cp.multiply(-ytrain, Xtrain @ beta_cp))) / n_samples_train
    reg = alpha_cp * cp.norm(beta_cp, 1)
    objective = loss + reg

    # define problem
    problem = cp.Problem(cp.Minimize(objective))
    assert problem.is_dpp()

    # solve problem
    layer = CvxpyLayer(problem, parameters=[alpha_cp], variables=[beta_cp])
    alpha_th = torch.tensor(alpha, requires_grad=True)
    beta_, = layer(alpha_th)

    # get test loss and it's gradient
    test_loss = torch.mean(torch.log(1 + torch.exp(-ytest * (Xtest @ beta_))))
    test_loss.backward()

    val = test_loss.detach().numpy()
    grad = np.array(alpha_th.grad)
    return val, grad


def lasso_sure_cvxpy(X, y, alpha, sigma, random_state=42):
    # lambda_alpha = [alpha, alpha]
    n_samples, n_features = X.shape
    epsilon = 2 * sigma / n_samples ** 0.3
    rng = check_random_state(random_state)
    delta = rng.randn(n_samples)

    y2 = y + epsilon * delta
    Xth, yth, y2th, deltath = map(torch.from_numpy, [X, y, y2, delta])

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    lambda_cp = cp.Parameter(nonneg=True)

    # set up objective
    loss = ((1 / (2 * n_samples)) * cp.sum(
        cp.square(Xth @ beta_cp - yth)))
    reg = lambda_cp * cp.norm1(beta_cp)
    objective = loss + reg

    # define problem
    problem1 = cp.Problem(cp.Minimize(objective))
    assert problem1.is_dpp()

    # solve problem1
    layer = CvxpyLayer(problem1, [lambda_cp], [beta_cp])
    alpha_th1 = torch.tensor(alpha, requires_grad=True)
    beta1, = layer(alpha_th1)

    # get test loss and it's gradient
    test_loss1 = (Xth @ beta1 - yth).pow(2).sum()
    test_loss1 -= 2 * sigma ** 2 / epsilon * (Xth @ beta1) @ deltath
    test_loss1.backward()
    val1 = test_loss1.detach().numpy()
    grad1 = np.array(alpha_th1.grad)

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    lambda_cp = cp.Parameter(nonneg=True)

    # set up objective
    loss = ((1 / (2 * n_samples)) * cp.sum(
        cp.square(Xth @ beta_cp - y2th)))
    reg = lambda_cp * cp.norm1(beta_cp)
    objective = loss + reg

    # define problem
    problem2 = cp.Problem(cp.Minimize(objective))
    assert problem2.is_dpp()

    # solve problem2
    layer = CvxpyLayer(problem2, [lambda_cp], [beta_cp])
    alpha_th2 = torch.tensor(alpha, requires_grad=True)
    beta2, = layer(alpha_th2)

    # get test loss and it's gradient
    test_loss2 = 2 * sigma ** 2 / epsilon * (Xth @ beta2) @ deltath
    test_loss2.backward()
    val2 = test_loss2.detach().numpy()
    grad2 = np.array(alpha_th2.grad)

    val = val1 + val2 - len(y) * sigma ** 2
    grad = grad1 + grad2
    return val, grad


def svm_cvxpy(X, y, C, idx_train, idx_val):
    C = float(C)
    Xtrain, Xtest, ytrain, ytest = map(
        torch.from_numpy, [
            X[idx_train, :], X[idx_val], y[idx_train], y[idx_val]])

    n_samples_train, n_features = Xtrain.shape

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    C_cp = cp.Parameter(nonneg=True)

    # set up objective
    loss = cp.sum_squares(beta_cp) / 2
    reg = C_cp * cp.sum(cp.pos(1 - cp.multiply(ytrain, Xtrain @ beta_cp)))
    objective = loss + reg

    # define problem
    problem = cp.Problem(cp.Minimize(objective))
    assert problem.is_dpp()

    # solve problem
    layer = CvxpyLayer(problem, parameters=[C_cp], variables=[beta_cp])
    C_th = torch.tensor(C, requires_grad=True)
    beta_, = layer(C_th)

    # get test loss and it's gradient
    test_loss = (Xtest @ beta_ - ytest).pow(2).mean()
    test_loss.backward()

    val = test_loss.detach().numpy()
    grad = np.array(C_th.grad)
    return val, grad


def svr_cvxpy(X, y, hyperparam, idx_train, idx_val):
    Xtrain, Xtest, ytrain, ytest = map(
        torch.from_numpy, [
            X[idx_train, :], X[idx_val], y[idx_train], y[idx_val]])

    n_samples_train, n_features = Xtrain.shape

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    xi_cp = cp.Variable(n_samples_train)
    xi_star_cp = cp.Variable(n_samples_train)
    C_cp = cp.Parameter(nonneg=True)
    epsilon_cp = cp.Parameter(nonneg=True)

    # set up objective
    loss = cp.sum_squares(beta_cp) / 2
    reg = C_cp * cp.sum(xi_cp + xi_star_cp)
    objective = loss + reg
    # define constraints
    constraints = [
        ytrain - Xtrain @ beta_cp <= epsilon_cp + xi_cp,
        Xtrain @ beta_cp - ytrain <= epsilon_cp + xi_star_cp,
        xi_cp >= 0.0, xi_star_cp >= 0.0]
    # define problem
    problem = cp.Problem(cp.Minimize(objective), constraints)
    assert problem.is_dpp()

    # solve problem
    layer = CvxpyLayer(
        problem, parameters=[C_cp, epsilon_cp], variables=[beta_cp])
    hyperparam_th = torch.tensor(hyperparam, requires_grad=True)
    beta_, = layer(hyperparam_th[0], hyperparam_th[1])

    # get test loss and it's gradient
    test_loss = (Xtest @ beta_ - ytest).pow(2).mean()
    test_loss.backward()

    val = test_loss.detach().numpy()
    grad = np.array(hyperparam_th.grad)
    return val, grad


def ssvr_cvxpy(X, y, hyperparam, idx_train, idx_val):
    Xtrain, Xtest, ytrain, ytest = map(
        torch.from_numpy, [
            X[idx_train, :], X[idx_val], y[idx_train], y[idx_val]])

    n_samples_train, n_features = Xtrain.shape

    # set up variables and parameters
    beta_cp = cp.Variable(n_features)
    xi_cp = cp.Variable(n_samples_train)
    xi_star_cp = cp.Variable(n_samples_train)
    C_cp = cp.Parameter(nonneg=True)
    epsilon_cp = cp.Parameter(nonneg=True)

    # set up objective
    loss = cp.sum_squares(beta_cp) / 2
    reg = C_cp / n_samples_train * cp.sum(xi_cp + xi_star_cp)
    objective = loss + reg
    # define constraints
    constraints = [ytrain - Xtrain @ beta_cp <= epsilon_cp + xi_cp,
                   Xtrain @ beta_cp - ytrain <= epsilon_cp + xi_star_cp,
                   xi_cp >= 0.0, xi_star_cp >= 0.0,
                   cp.sum(beta_cp) == 1, beta_cp >= 0.0]
    # define problem
    problem = cp.Problem(cp.Minimize(objective), constraints)
    assert problem.is_dpp()

    # solve problem
    layer = CvxpyLayer(problem, parameters=[C_cp, epsilon_cp],
                       variables=[beta_cp])
    hyperparam_th = torch.tensor(hyperparam, requires_grad=True)
    beta_, = layer(hyperparam_th[0], hyperparam_th[1])
    # get test loss and it's gradient
    test_loss = (Xtest @ beta_ - ytest).pow(2).mean()
    test_loss.backward()

    val = test_loss.detach().numpy()
    grad = np.array(hyperparam_th.grad)
    return val, grad

File Path: sparse_ho/tests/test_criterion.py
Content:
import pytest
import numpy as np

from sparse_ho.criterion import (
    HeldOutMSE, HeldOutLogistic, FiniteDiffMonteCarloSure)
from sparse_ho.utils import Monitor
from sparse_ho import Forward

from sparse_ho.tests.common import (
    X, y, sigma_star, idx_train, idx_val,
    models, dict_list_log_alphas)


list_model_crit = [
    ('lasso',  HeldOutMSE(idx_train, idx_val)),
    ('enet', HeldOutMSE(idx_train, idx_val)),
    ('wLasso', HeldOutMSE(idx_train, idx_val)),
    ('lasso', FiniteDiffMonteCarloSure(sigma_star)),
    ('logreg', HeldOutLogistic(idx_train, idx_val))]


tol = 1e-15


@pytest.mark.parametrize('model_name,criterion', list_model_crit)
def test_cross_val_criterion(model_name, criterion):
    # verify dtype from criterion, and the good shape
    algo = Forward()
    monitor_get_val = Monitor()
    monitor_get_val_grad = Monitor()

    model = models[model_name]
    for log_alpha in dict_list_log_alphas[model_name]:
        criterion.get_val(
            model, X, y, log_alpha, tol=tol, monitor=monitor_get_val)
    for log_alpha in dict_list_log_alphas[model_name]:
        criterion.get_val_grad(
            model, X, y, log_alpha, algo.compute_beta_grad,
            tol=tol, monitor=monitor_get_val_grad)

    obj_val = np.array(monitor_get_val.objs)
    obj_val_grad = np.array(monitor_get_val_grad.objs)

    np.testing.assert_allclose(obj_val, obj_val_grad)


if __name__ == '__main__':
    for model_name, criterion in list_model_crit:
        test_cross_val_criterion(model_name, criterion)

File Path: sparse_ho/tests/test_docstring_parameters.py
Content:
"""This is copied from sklearn."""
import inspect
import os.path as op
import re
import sys
from unittest import SkipTest
import warnings

from pkgutil import walk_packages
from inspect import getsource

import sparse_ho


def _get_args(function, varargs=False):
    params = inspect.signature(function).parameters
    args = [key for key, param in params.items()
            if param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)]
    if varargs:
        varargs = [param.name for param in params.values()
                   if param.kind == param.VAR_POSITIONAL]
        if len(varargs) == 0:
            varargs = None
        return args, varargs
    else:
        return args


public_modules = [
    # the list of modules users need to access for all functionality
    'sparse_ho',
    'sparse_ho.datasets',
    'sparse_ho.algo',
    'sparse_ho.criterion',
    'sparse_ho.optimizers',
    'sparse_ho.models'
]


def get_name(func):
    """Get the name."""
    parts = []
    module = inspect.getmodule(func)
    if module:
        parts.append(module.__name__)
    if hasattr(func, 'im_class'):
        parts.append(func.im_class.__name__)
    parts.append(func.__name__)
    return '.'.join(parts)


# functions to ignore args / docstring of
_docstring_ignores = [
    ".*compute_beta_grad$",
    ".*get_beta_jac$",
    ".*get_full_jac_obj$",
    ".*get_jac_residual_norm$",
]
_tab_ignores = []


def check_parameters_match(func, doc=None):
    """Check docstring, return list of incorrect results."""
    from numpydoc import docscrape
    incorrect = []
    name_ = get_name(func)
    if not name_.startswith('sparse_ho.'):
        return incorrect
    if inspect.isdatadescriptor(func):
        return incorrect
    args = _get_args(func)
    # drop self
    if len(args) > 0 and args[0] == 'self':
        args = args[1:]

    if doc is None:
        with warnings.catch_warnings(record=True) as w:
            try:
                doc = docscrape.FunctionDoc(func)
            except Exception as exp:
                incorrect += [name_ + ' parsing error: ' + str(exp)]
                return incorrect
        if len(w):
            raise RuntimeError('Error for %s:\n%s' % (name_, w[0]))
    # check set
    param_names = [name for name, _, _ in doc['Parameters']]
    # clean up some docscrape output:
    param_names = [name.split(':')[0].strip('` ') for name in param_names]
    param_names = [name for name in param_names if '*' not in name]
    if len(param_names) != len(args):
        bad = str(sorted(list(set(param_names) - set(args)) +
                         list(set(args) - set(param_names))))
        if not any(re.match(d, name_) for d in _docstring_ignores) and \
                'deprecation_wrapped' not in func.__code__.co_name:
            incorrect += [name_ + ' arg mismatch: ' + bad]
    else:
        for n1, n2 in zip(param_names, args):
            if n1 != n2:
                incorrect += [name_ + ' ' + n1 + ' != ' + n2]
    return incorrect


# TODO: readd numpydoc
# @requires_numpydoc
def test_docstring_parameters():
    """Test module docstring formatting."""
    from numpydoc import docscrape

    public_modules_ = public_modules[:]

    incorrect = []
    for name in public_modules_:
        with warnings.catch_warnings(record=True):  # traits warnings
            module = __import__(name, globals())
        for submod in name.split('.')[1:]:
            module = getattr(module, submod)
        classes = inspect.getmembers(module, inspect.isclass)
        for cname, cls in classes:
            if cname.startswith('_'):
                continue
            with warnings.catch_warnings(record=True) as w:
                cdoc = docscrape.ClassDoc(cls)
            if len(w):
                raise RuntimeError('Error for __init__ of %s in %s:\n%s'
                                   % (cls, name, w[0]))
            if hasattr(cls, '__init__'):
                incorrect += check_parameters_match(cls.__init__, cdoc)
            for method_name in cdoc.methods:
                method = getattr(cls, method_name)
                incorrect += check_parameters_match(method)
            if hasattr(cls, '__call__'):
                incorrect += check_parameters_match(cls.__call__)
        functions = inspect.getmembers(module, inspect.isfunction)
        for fname, func in functions:
            if fname.startswith('_'):
                continue
            incorrect += check_parameters_match(func)
    msg = '\n' + '\n'.join(sorted(list(set(incorrect))))
    if len(incorrect) > 0:
        raise AssertionError(msg)


def test_tabs():
    """Test that there are no tabs in our source files."""
    ignore = _tab_ignores[:]

    for importer, modname, ispkg in walk_packages(sparse_ho.__path__,
                                                  prefix='sparse_ho.'):
        if not ispkg and modname not in ignore:
            # mod = importlib.import_module(modname)  # not py26 compatible!
            try:
                with warnings.catch_warnings(record=True):  # traits
                    __import__(modname)
            except Exception:  # can't import properly
                continue
            mod = sys.modules[modname]
            try:
                source = getsource(mod)
            except IOError:  # user probably should have run "make clean"
                continue
            assert '\t' not in source, ('"%s" has tabs, please remove them '
                                        'or add it to the ignore list'
                                        % modname)


documented_ignored_mods = tuple()
documented_ignored_names = """
""".split('\n')


def test_documented():
    """Test that public functions and classes are documented."""
    public_modules_ = public_modules[:]

    doc_file = op.abspath(op.join(op.dirname(__file__), '..', '..', 'doc',
                                  'api.rst'))
    if not op.isfile(doc_file):
        raise SkipTest('Documentation file not found: %s' % doc_file)
    known_names = list()
    with open(doc_file, 'rb') as fid:
        for line in fid:
            line = line.decode('utf-8')
            if not line.startswith('  '):  # at least two spaces
                continue
            line = line.split()
            if len(line) == 1 and line[0] != ':':
                known_names.append(line[0].split('.')[-1])
    known_names = set(known_names)

    missing = []
    for name in public_modules_:
        with warnings.catch_warnings(record=True):  # traits warnings
            module = __import__(name, globals())
        for submod in name.split('.')[1:]:
            module = getattr(module, submod)
        classes = inspect.getmembers(module, inspect.isclass)
        functions = inspect.getmembers(module, inspect.isfunction)
        checks = list(classes) + list(functions)
        for name, cf in checks:
            if not name.startswith('_') and name not in known_names:
                from_mod = inspect.getmodule(cf).__name__
                if (from_mod.startswith('sparse_ho') and
                        from_mod not in documented_ignored_mods and
                        name not in documented_ignored_names):
                    missing.append('%s (%s.%s)' % (name, from_mod, name))
    if len(missing) > 0:
        raise AssertionError('\n\nFound new public members missing from '
                             'doc/python_reference.rst:\n\n* ' +
                             '\n* '.join(sorted(set(missing))))


if __name__ == "__main__":
    test_docstring_parameters()
    test_tabs()
    test_documented()

File Path: sparse_ho/tests/test_grid_search.py
Content:
import pytest
import numpy as np
from scipy.sparse import csc_matrix
import celer
from sklearn.model_selection import KFold

import sklearn.linear_model
from celer.datasets import make_correlated_data

from sparse_ho.utils import Monitor
from sparse_ho.models import Lasso
from sparse_ho.criterion import (
    HeldOutMSE, FiniteDiffMonteCarloSure, CrossVal, HeldOutLogistic)
from sparse_ho.grid_search import grid_search


n_samples = 100
n_features = 100
snr = 3
corr = 0.5

X, y, _ = make_correlated_data(
    n_samples, n_features, corr=corr, snr=snr, random_state=42)
sigma_star = 0.1
y = np.sign(y)
X_s = csc_matrix(X)

idx_train = np.arange(0, 50)
idx_val = np.arange(50, 100)

alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)

alphas = alpha_max * np.geomspace(1, 0.1)
alpha_min = 0.0001 * alpha_max

estimator = celer.Lasso(
    fit_intercept=False, max_iter=50, warm_start=True)
model = Lasso(estimator=estimator)

tol = 1e-8

# Set models to be tested
models = {}
models["lasso"] = Lasso(estimator=None)

models["lasso_custom"] = Lasso(estimator=celer.Lasso(
    warm_start=True, fit_intercept=False))


@pytest.mark.parametrize('model_name', list(models.keys()))
@pytest.mark.parametrize('XX', [X, X_s])
def test_cross_val_criterion(model_name, XX):
    model = models[model_name]
    alpha_min = alpha_max / 10
    max_iter = 10000
    n_alphas = 10
    kf = KFold(n_splits=5, shuffle=True, random_state=56)

    monitor_grid = Monitor()
    if model_name.startswith("lasso"):
        sub_crit = HeldOutMSE(None, None)
    else:
        sub_crit = HeldOutLogistic(None, None)
    criterion = CrossVal(sub_crit, cv=kf)
    grid_search(
        criterion, model, XX, y, alpha_min, alpha_max,
        monitor_grid, max_evals=n_alphas, tol=tol)

    if model_name.startswith("lasso"):
        reg = celer.LassoCV(
            cv=kf, verbose=True, tol=tol, fit_intercept=False,
            alphas=np.geomspace(alpha_max, alpha_min, num=n_alphas),
            max_iter=max_iter).fit(X, y)
    else:
        reg = sklearn.linear_model.LogisticRegressionCV(
            cv=kf, verbose=True, tol=tol, fit_intercept=False,
            Cs=len(idx_train) / np.geomspace(
                alpha_max, alpha_min, num=n_alphas),
            max_iter=max_iter, penalty='l1', solver='liblinear').fit(X, y)
    reg.score(XX, y)
    if model_name.startswith("lasso"):
        objs_grid_sk = reg.mse_path_.mean(axis=1)
    else:
        objs_grid_sk = reg.scores_[1.0].mean(axis=1)
    # these 2 value should be the same
    (objs_grid_sk - np.array(monitor_grid.objs))
    np.testing.assert_allclose(objs_grid_sk, monitor_grid.objs)


# TOD0 factorize this tests
def test_grid_search():
    max_evals = 5

    monitor_grid = Monitor()
    model = Lasso(estimator=estimator)
    criterion = HeldOutMSE(idx_train, idx_train)
    alpha_opt_grid, _ = grid_search(
        criterion, model, X, y, alpha_min, alpha_max,
        monitor_grid, max_evals=max_evals,
        tol=1e-5, samp="grid")

    monitor_random = Monitor()
    criterion = HeldOutMSE(idx_train, idx_val)
    alpha_opt_random, _ = grid_search(
        criterion, model, X, y, alpha_min, alpha_max,
        monitor_random,
        max_evals=max_evals, tol=1e-5, samp="random")

    np.testing.assert_allclose(monitor_random.alphas[
        np.argmin(monitor_random.objs)], alpha_opt_random)
    np.testing.assert_allclose(monitor_grid.alphas[
        np.argmin(monitor_grid.objs)], alpha_opt_grid)

    monitor_grid = Monitor()
    model = Lasso(estimator=estimator)

    criterion = FiniteDiffMonteCarloSure(sigma=sigma_star)
    alpha_opt_grid, _ = grid_search(
        criterion, model, X, y, alpha_min, alpha_max,
        monitor_grid, max_evals=max_evals,
        tol=1e-5, samp="grid")

    monitor_random = Monitor()
    criterion = FiniteDiffMonteCarloSure(sigma=sigma_star)
    alpha_opt_random, _ = grid_search(
        criterion, model, X, y, alpha_min, alpha_max,
        monitor_random,
        max_evals=max_evals, tol=1e-5, samp="random")

    np.testing.assert_allclose(monitor_random.alphas[
        np.argmin(monitor_random.objs)], alpha_opt_random)
    np.testing.assert_allclose(monitor_grid.alphas[
        np.argmin(monitor_grid.objs)], alpha_opt_grid)


if __name__ == '__main__':
    for model_name in models.keys():
        test_cross_val_criterion(model_name)
    # test_grid_search()

File Path: sparse_ho/tests/test_models.py
Content:
# TODO make Backward in the test
# TODO include tests for wLasso with custom solver
import pytest

import numpy as np
from scipy.optimize import check_grad

from sparse_ho import Forward, ImplicitForward, Implicit

from sparse_ho.algo.forward import compute_beta
from sparse_ho.algo.implicit_forward import (get_bet_jac_implicit_forward,
                                             get_only_jac)
from sparse_ho.algo.implicit import compute_beta_grad_implicit
from sparse_ho.criterion import (
    HeldOutMSE, FiniteDiffMonteCarloSure, HeldOutLogistic)

from sparse_ho.tests.common import (
    X, X_s, y, sigma_star, idx_train, idx_val,
    dict_log_alpha, models, custom_models, dict_cvxpy_func,
    dict_vals_cvxpy, dict_grads_cvxpy, dict_list_log_alphas, get_grad_outer,
    list_model_crit, list_model_names)

# list of algorithms to be tested
list_algos = [
    Forward(),
    ImplicitForward(tol_jac=1e-8, n_iter_jac=5000),
    Implicit()
    # Backward()  # XXX to fix
]

tol = 1e-14
X_r = X_s.tocsr()
X_c = X_s


@pytest.mark.parametrize('key', list(models.keys()))
def test_beta_jac(key):
    """Tests that algorithms computing the Jacobian return the same Jacobian"""
    if key == 'svm':
        return True
    if key == "svm" or key == "svr" or key == "ssvr":
        X_s = X_r
    else:
        X_s = X_c
    supp1, dense1, jac1 = compute_beta(
        X, y, dict_log_alpha[key], tol=tol, model=models[key])
    supp2, dense2, jac2 = get_bet_jac_implicit_forward(
        X, y, dict_log_alpha[key], tol=tol, model=models[key], tol_jac=tol)
    supp3, dense3, jac3 = compute_beta(
        X_s, y, dict_log_alpha[key], tol=tol,
        model=models[key])
    supp4, dense4, jac4 = get_bet_jac_implicit_forward(
        X_s, y, dict_log_alpha[key],
        tol=tol, model=models[key], tol_jac=tol)

    assert np.all(supp1 == supp2)
    assert np.allclose(dense1, dense2)
    assert np.allclose(jac1, jac2, atol=1e-6)

    assert np.all(supp2 == supp3)
    assert np.allclose(dense2, dense3)
    assert np.allclose(jac2, jac3, atol=1e-6)

    assert np.all(supp3 == supp4)
    assert np.allclose(dense3, dense4)
    assert np.allclose(jac3, jac4, atol=1e-6)

    compute_beta_grad_implicit(
        X, y, dict_log_alpha[key], get_grad_outer, model=models[key])


@pytest.mark.parametrize('model_name', list(custom_models.keys()))
def test_beta_jac_custom(model_name):
    """Check that using sk or celer yields the same solution as sparse ho"""
    if model_name in ("svm", "svr", "ssvr"):
        X_s = X_r
    else:
        X_s = X_c

    for log_alpha in dict_list_log_alphas[model_name]:
        supp, dense, jac = get_bet_jac_implicit_forward(
            X_s, y, log_alpha,
            tol=tol, model=models[model_name], tol_jac=tol)
        supp_custom, dense_custom, jac_custom = get_bet_jac_implicit_forward(
            X_s, y, log_alpha,
            tol=tol, model=custom_models[model_name], tol_jac=tol)
        assert np.all(supp == supp_custom)
        assert np.allclose(dense, dense_custom)
        assert np.allclose(jac, jac_custom)


@pytest.mark.parametrize('model_name', list(custom_models.keys()))
def test_warm_start(model_name):
    """Check that warm start leads to only 2 iterations
    in Jacobian computation"""
    if model_name in ("svm", "svr", "ssvr"):
        X_s = X_r
    else:
        X_s = X_c
    model = models[model_name]

    for log_alpha in dict_list_log_alphas[model_name]:
        mask, dense, jac = None, None, None
        for i in range(2):
            mask, dense, _ = compute_beta(
                X_s, y, log_alpha, tol=tol,
                mask0=mask, dense0=dense, jac0=jac,
                max_iter=5000, compute_jac=False, model=model)
            dbeta0_new = model._init_dbeta0(mask, mask, jac)
            reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)

            _, dual_var = model._init_beta_dual_var(X_s, y, mask, dense)
            jac = get_only_jac(
                model.reduce_X(X_s, mask), model.reduce_y(y, mask), dual_var,
                reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,
                niter_jac=5000, tol_jac=1e-13, model=model, mask=mask,
                dense=dense)
            if i == 0:
                np.testing.assert_array_less(2, get_only_jac.n_iter)
            else:
                assert get_only_jac.n_iter == 2


@pytest.mark.parametrize('model_name,criterion_name', list_model_crit)
@pytest.mark.parametrize('algo', list_algos)
def test_val_grad(model_name, criterion_name, algo):
    """Check that all methods return the same gradient, comparing to cvxpylayer
    """

    if criterion_name == 'logistic':
        pytest.xfail("cvxpylayer seems broken for logistic")

    if criterion_name == 'MSE':
        criterion = HeldOutMSE(idx_train, idx_val)
    elif criterion_name == 'logistic':
        criterion = HeldOutLogistic(idx_train, idx_val)
    elif criterion_name == 'SURE':
        criterion = FiniteDiffMonteCarloSure(sigma_star)

    log_alpha = dict_log_alpha[model_name]
    model = models[model_name]
    val, grad = criterion.get_val_grad(
        model, X, y, log_alpha, algo.compute_beta_grad, tol=tol)
    np.testing.assert_allclose(
        dict_vals_cvxpy[model_name, criterion_name], val, rtol=1e-3, atol=1e-3)
    np.testing.assert_allclose(
        dict_grads_cvxpy[model_name, criterion_name], grad,
        rtol=1e-3, atol=1e-3)


@pytest.mark.parametrize('model_name,criterion', list_model_crit)
@pytest.mark.parametrize('algo', list_algos)
def test_check_grad_sparse_ho(model_name, criterion, algo):
    """Check that all methods return a good gradient using check_grad"""
    if criterion == 'MSE':
        criterion = HeldOutMSE(idx_train, idx_val)
    elif criterion == 'SURE':
        criterion = FiniteDiffMonteCarloSure(sigma_star)
    elif criterion == 'logistic':
        criterion = HeldOutLogistic(idx_train, idx_val)

    model = models[model_name]
    log_alpha = dict_log_alpha[model_name]

    def get_val(log_alpha):
        val, _ = criterion.get_val_grad(
            model, X, y, np.squeeze(log_alpha), algo.compute_beta_grad,
            tol=tol)
        return val

    def get_grad(log_alpha):
        _, grad = criterion.get_val_grad(
            model, X, y, np.squeeze(log_alpha), algo.compute_beta_grad,
            tol=tol)
        return grad

    for log_alpha in dict_list_log_alphas[model_name]:
        grad_error = check_grad(get_val, get_grad, log_alpha)
        assert grad_error < 1e-1


@pytest.mark.parametrize('model_name', list_model_names)
def test_check_grad_logreg_cvxpy(model_name):

    pytest.xfail("cvxpylayer seems broken for logistic")
    cvxpy_func = dict_cvxpy_func[model_name]

    def get_val(log_alpha):
        val_cvxpy, _ = cvxpy_func(
            X, y, np.exp(log_alpha), idx_train, idx_val)
        return val_cvxpy

    def get_grad(log_alpha):
        _, grad_cvxpy = cvxpy_func(
            X, y, np.exp(log_alpha), idx_train, idx_val)
        grad_cvxpy *= np.exp(log_alpha)
        return grad_cvxpy

    for log_alpha in dict_list_log_alphas[model_name]:
        grad_error = check_grad(get_val, get_grad, log_alpha)
        assert grad_error < 1


if __name__ == "__main__":
    test_beta_jac_custom("logreg")
    print("#" * 30)
    for algo in list_algos:
        print("#" * 20)
        test_val_grad("lasso", "MSE", algo)
        test_check_grad_sparse_ho('lasso', 'MSE', algo)
        test_beta_jac('lasso')

File Path: sparse_ho/tests/test_multiclass.py
Content:
import numpy as np
import sklearn.linear_model

from libsvmdata.datasets import fetch_libsvm

from sparse_ho.models import SparseLogreg
from sparse_ho.criterion import LogisticMulticlass
from sparse_ho import ImplicitForward
from sparse_ho.utils import Monitor
from sparse_ho.datasets.utils_datasets import (
    alpha_max_multiclass, clean_dataset)


# load data
n_samples = 1000
n_features = 10
X, y = fetch_libsvm('mnist')
my_bool = np.logical_or(np.logical_or(y == 0, y == 1), y == 2)

X = X[my_bool, :]
y = y[my_bool]
# clean data and subsample
X, y = clean_dataset(X, y, n_samples, n_features)
idx_train = np.arange(len(y) // 2)
idx_val = np.arange(len(y) // 2, len(y))

alpha_max, n_classes = alpha_max_multiclass(X, y)
tol = 1e-8

n_classes = np.unique(y).shape[0]

max_iter = 10000
algo = ImplicitForward(n_iter_jac=1000)
estimator = sklearn.linear_model.LogisticRegression(
    solver='saga', penalty='l1', max_iter=max_iter,
    random_state=42, fit_intercept=False, warm_start=True)

model = SparseLogreg(estimator=estimator)
logit_multiclass = LogisticMulticlass(
    idx_train=idx_train, idx_val=idx_val, algo=algo)

n_alphas = 10
p_alphas = np.geomspace(1, 0.1, n_alphas)
p_alphas = np.tile(p_alphas, (n_classes, 1))


def test_our_vs_sklearn():
    monitor_grid = Monitor()
    monitor_grid_sk = Monitor()
    for i in range(n_alphas):
        # one versus all (ovr) logreg from scikit learn
        p_alpha = p_alphas[:, i]
        lr = sklearn.linear_model.LogisticRegression(
            solver='saga', multi_class='ovr', penalty='l1', max_iter=max_iter,
            random_state=42, fit_intercept=False, warm_start=True,
            C=1 / (alpha_max * p_alpha[0] * len(idx_train)), tol=tol)
        lr.fit(X[idx_train, :], y[idx_train])
        y_pred_val = lr.predict(X[idx_val, :])
        accuracy_val = sklearn.metrics.accuracy_score(y_pred_val, y[idx_val])
        print("accuracy validation (scikit) %f " % accuracy_val)

        monitor_grid_sk(None, None, acc_val=accuracy_val)
        log_alpha_i = np.log(alpha_max * p_alpha)
        # our one verus all
        val, grad = logit_multiclass.get_val_grad(
            model, X, y, log_alpha_i, None, monitor_grid, tol)
        print("accuracy validation (our) %f " % monitor_grid.acc_vals[-1])

    np.testing.assert_allclose(
        np.array(monitor_grid.acc_vals), np.array(monitor_grid_sk.acc_vals))


if __name__ == '__main__':
    test_our_vs_sklearn()

File Path: sparse_ho/tests/test_optimizers.py
Content:
import numpy as np
from scipy.sparse import csc_matrix
import pytest
import celer
from celer.datasets import make_correlated_data

from sparse_ho.utils import Monitor
from sparse_ho.models import Lasso

from sparse_ho import Forward
from sparse_ho import ImplicitForward
from sparse_ho import Implicit
from sparse_ho.criterion import HeldOutMSE
# XXX TODO test FiniteDiffMonteCarloSure crtiterion
from sparse_ho.ho import grad_search
from sparse_ho.optimizers import LineSearch, GradientDescent

n_samples = 100
n_features = 100
snr = 3
corr = 0.5

X, y, _ = make_correlated_data(
    n_samples, n_features, corr=corr, snr=snr, random_state=42)
sigma_star = 0.1
X_train_s = csc_matrix(X)

idx_train = np.arange(0, 50)
idx_val = np.arange(50, 100)

alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)
p_alpha = 0.7
alpha0 = p_alpha * alpha_max
# log_alpha = np.log(alpha)

log_alphas = np.log(alpha_max * np.geomspace(1, 0.1))
tol = 1e-16
max_iter = 1000

# dict_log_alpha0 = {}
# dict_log_alpha0["lasso"] = log_alpha
# tab = np.linspace(1, 1000, n_features)
# dict_log_alpha0["wlasso"] = log_alpha + np.log(tab / tab.max())

models = [
    Lasso(estimator=None),
]

estimator = celer.Lasso(
    fit_intercept=False, max_iter=1000, warm_start=True)
models_custom = [
    Lasso(estimator=estimator),
]

Optimizers = [LineSearch, GradientDescent]


@pytest.mark.parametrize('Optimizer', Optimizers)
@pytest.mark.parametrize('model', models)
@pytest.mark.parametrize('crit', ['MSE', 'sure'])
def test_grad_search(Optimizer, model, crit):
    """check that the paths are the same in the line search"""
    n_outer = 2

    criterion = HeldOutMSE(idx_train, idx_val)
    monitor1 = Monitor()
    algo = Forward()
    optimizer = Optimizer(n_outer=n_outer, tol=1e-16)
    grad_search(
        algo, criterion, model, optimizer, X, y, alpha0, monitor1)

    criterion = HeldOutMSE(idx_train, idx_val)
    monitor2 = Monitor()
    algo = Implicit()
    optimizer = Optimizer(n_outer=n_outer, tol=1e-16)
    grad_search(algo, criterion, model, optimizer, X, y, alpha0, monitor2)

    criterion = HeldOutMSE(idx_train, idx_val)
    monitor3 = Monitor()
    algo = ImplicitForward(tol_jac=1e-8, n_iter_jac=5000)
    optimizer = Optimizer(n_outer=n_outer, tol=1e-16)
    grad_search(algo, criterion, model, optimizer, X, y, alpha0, monitor3)

    np.testing.assert_allclose(
        np.array(monitor1.alphas), np.array(monitor3.alphas))
    np.testing.assert_allclose(
        np.array(monitor1.grads), np.array(monitor3.grads), rtol=1e-5)
    np.testing.assert_allclose(
        np.array(monitor1.objs), np.array(monitor3.objs))
    assert not np.allclose(
        np.array(monitor1.times), np.array(monitor3.times))


if __name__ == '__main__':
    models = [
        Lasso(estimator=None)]
    crits = ['sure']
    # crits = ['MSE']
    for model in models:
        for crit in crits:
            test_grad_search(model, crit)

File Path: sparse_ho/tests/test_utils.py
Content:
import numpy as np
from numpy.linalg import norm
from sklearn.model_selection import train_test_split
import celer
from celer.datasets import make_correlated_data

from sparse_ho.models import Lasso
from sparse_ho.criterion import HeldOutMSE
from sparse_ho import ImplicitForward
from sparse_ho.utils import Monitor
from sparse_ho import grad_search
from sparse_ho.optimizers import LineSearch


n_samples, n_features, corr, snr = 200, 70, 0.1, 5

X, y, _ = make_correlated_data(
    n_samples, n_features, corr=corr, snr=snr, random_state=42)

X, _, y, _ = train_test_split(X, y)

n_samples = X.shape[0]
idx_train = np.arange(0, n_samples // 2)
idx_val = np.arange(n_samples // 2, n_samples)

n_samples = len(y[idx_train])
alpha_max = np.max(
    np.abs(X[idx_train, :].T.dot(y[idx_train]))) / len(idx_train)
alpha0 = alpha_max / 10

tol = 1e-7
max_iter = 100_000


estimator = celer.Lasso(
    fit_intercept=False, max_iter=50, warm_start=True)


objs = []
X_val = X[idx_val]


def callback(val, grad, mask, dense, log_alpha):
    beta = np.zeros(len(mask))
    beta[mask] = dense
    objs.append(
        norm(X_val[:, mask] @ dense - y[idx_val]) ** 2 / len(idx_val))


def test_monitor():
    model = Lasso(estimator=estimator)
    criterion = HeldOutMSE(idx_train, idx_val)
    algo = ImplicitForward()
    monitor = Monitor(callback=callback)
    optimizer = LineSearch(n_outer=10, tol=tol)
    grad_search(algo, criterion, model, optimizer, X, y, alpha0, monitor)

    np.testing.assert_allclose(np.array(monitor.objs), np.array(objs))


if __name__ == '__main__':
    test_monitor()

File Path: sparse_ho/tests/test_utils_cross_entropy.py
Content:
import numpy as np

from scipy.optimize import check_grad
from scipy.sparse import csc_matrix
from sklearn.preprocessing import OneHotEncoder

from sparse_ho.utils_cross_entropy import cross_entropy, grad_cross_entropy


if __name__ == '__main__':
    rng = np.random.RandomState(42)
    K = 5
    X = rng.randn(120, 100)
    X = csc_matrix(X)
    y = rng.choice(range(K), size=X.shape[0])
    Y = OneHotEncoder().fit_transform(y[:, None]).toarray()
    betas = rng.randn(X.shape[1], K)

    def f(x):
        return cross_entropy(x.reshape(X.shape[1], K), X, Y)

    def gradf(x):
        return grad_cross_entropy(x.reshape(X.shape[1], K), X, Y).ravel()

    np.testing.assert_allclose(
        check_grad(f, gradf, x0=betas.ravel()), 0, atol=1e-5)

File Path: sparse_ho/utils.py
Content:
import time
import numpy as np
from numba import njit


@njit
def sparse_scalar_product(Xjs, idx_j, Xis, idx_i):
    product = 0
    if len(idx_j) != 0 and len(idx_i) != 0:
        cursor_j = 0
        cursor_i = 0
        for k in range(len(idx_j) + len(idx_i)):
            if idx_j[cursor_j] == idx_i[cursor_i]:
                product += Xjs[cursor_j] * Xis[cursor_i]
                cursor_i += 1
                cursor_j += 1

            elif idx_j[cursor_j] < idx_i[cursor_i]:
                cursor_j += 1
            else:
                cursor_i += 1
            if cursor_j >= (len(idx_j)) or cursor_i >= (len(idx_i)):
                break
        return product
    else:
        return 0.0


@njit
def ST(x, alpha):
    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0.)


@njit
def prox_elasticnet(x, alpha_1, alpha_2):
    return (1 / (1 + (alpha_2))) * ST(x, alpha_1)


@njit
def proj_box_svm(x, C):
    return min(max(0, x), C)


@njit
def compute_grad_proj(theta, F, C):
    if theta == 0:
        return min(F, 0)
    elif theta == C:
        return max(F, 0)
    else:
        return F


@njit
def ind_box(x, C):
    return np.logical_and((x > 0), (x < C))


@njit
def sigma(z):
    return 1 / (1 + np.exp(-z))


@njit
def xlogx(x):
    if x < 1e-10:
        return 0.
    else:
        return x * np.log(x)


@njit
def negative_ent(x):
    """
    Negative entropy:
    x * log(x) + (1 - x) * log(1 - x)
    """
    if 0. <= x <= 1.:
        return xlogx(x) + xlogx(1. - x)
    else:
        return np.inf


@njit
def dual_logreg(y, theta, alpha):
    d_obj = 0
    n_samples = len(y)
    for i in range(y.shape[0]):
        d_obj -= negative_ent(alpha * n_samples * y[i] * theta[i])
    d_obj /= n_samples
    return d_obj


def smooth_hinge(x):
    val = np.zeros(len(x))
    val[x <= 0.0] = 0.5 - x[x <= 0.0]
    boole = np.logical_and(x > 0.0, x <= 1)
    val[boole] = 0.5 * (1 - x[boole]) ** 2

    return val


def derivative_smooth_hinge(x):
    deriv = np.zeros(len(x))
    deriv[x <= 0.0] = -1.0
    boole = np.logical_and(x > 0.0, x <= 1)
    deriv[boole] = -1.0 + x[boole]
    return deriv


def smooth_hinge_loss(X, y, beta):
    n_samples, n_features = X.shape
    val = 0
    grad = np.zeros(n_features)
    for i in range(n_samples):
        val += smooth_hinge((X[i, :].T @ beta) * y[i])
        grad += derivative_smooth_hinge(
            (X[i, :].T @ beta) * y[i]) * X[i, :] * y[i]
    val /= X.shape[0]
    grad /= X.shape[0]
    return val, grad


@njit
def init_dbeta0_new_p(jac0, mask, mask_old):
    mask_both = np.logical_and(mask_old, mask)
    size_mat = mask.sum()
    dbeta0_new = np.zeros((size_mat, size_mat))
    count = 0
    count_old = 0
    n_features = mask.shape[0]
    for j in range(n_features):
        if mask_both[j]:
            dbeta0_new[count, :] = init_dbeta0_new(
                jac0[count_old, :], mask, mask_old)
        if mask_old[j]:
            count_old += 1
        if mask[j]:
            count += 1
    return dbeta0_new


@njit
def init_dbeta0_new(dbeta0, mask, mask_old):
    mask_both = np.logical_and(mask_old, mask)
    size_mat = mask.sum()
    dbeta0_new = np.zeros(size_mat)
    count = 0
    count_old = 0
    n_features = mask.shape[0]
    for j in range(n_features):
        if mask_both[j]:
            dbeta0_new[count] = dbeta0[count_old]
        if mask_old[j]:
            count_old += 1
        if mask[j]:
            count += 1
    return dbeta0_new


def iou(supp1, supp2):
    return np.logical_and(
        supp1, supp2).sum() / np.logical_or(supp1, supp2).sum()


class Monitor():
    """
    Class used to store computed metrics at each iteration of the outer loop.
    """

    def __init__(self, callback=None):
        self.t0 = time.time()
        self.objs = []   # TODO rename, use self.value_outer?
        self.times = []
        self.alphas = []
        self.grads = []
        self.callback = callback
        self.acc_vals = []
        self.all_betas = []

    def __call__(
            self, obj, grad, mask=None, dense=None, alpha=None,
            acc_val=None, acc_test=None):
        self.objs.append(obj)
        try:
            self.alphas.append(alpha.copy())
        except Exception:
            self.alphas.append(alpha)
        self.times.append(time.time() - self.t0)
        self.grads.append(grad)
        if self.callback is not None:
            self.callback(obj, grad, mask, dense, alpha)
        if acc_val is not None:
            self.acc_vals.append(acc_val)
        if acc_test is not None:
            self.acc_vals.append(acc_test)

File Path: sparse_ho/utils_cross_entropy.py
Content:
import numpy as np
import sklearn
from scipy.special import logsumexp


def softmax(Xbetas):
    # Stable softmax
    exp = np.exp(Xbetas - np.max(Xbetas, axis=1, keepdims=True))
    norms = np.sum(exp, axis=1)[:, np.newaxis]
    return exp / norms


def log_softmax(Xbetas):
    # Stable log softmax
    return Xbetas - logsumexp(Xbetas, axis=1, keepdims=True)


def cross_entropy(betas, X, Y):
    """cross-entropy"""
    n_samples, n_features = X.shape
    result = - np.sum(log_softmax(X @ betas) * Y) / n_samples
    if np.isnan(result):
        import ipdb
        ipdb.set_trace()
    return result


def accuracy(betas, X, Y):
    scores = X @ betas
    idx_max = np.argmax(scores, axis=1)
    idx_true = np.argmax(Y, axis=1)  # TODO to improve
    # acc = (idx_max == idx_true).mean()
    acc = sklearn.metrics.accuracy_score(idx_max, idx_true)
    return acc


def grad_cross_entropy(betas, X, Y):
    """Compute gradient of cross-entropy wrt betas
    betas: array of size (n_features, n_classes)
    X: {ndarray, sparse matrix} of (n_samples, n_features)
    Y: {ndarray, sparse matrix} of (n_samples, n_classes)
    """
    n_samples = X.shape[0]
    sm = softmax(X @ betas)
    weights = sm - Y
    grad = (X.T @ weights) / n_samples

    return grad


# def grad_cross_entropyk(betas, X, Y, k):
#     """Compute gradient of cross-entropy wrt betas
#     betas: array of size (n_features, n_classes)
#     X: {ndarray, sparse matrix} of (n_samples, n_features)
#     Y: {ndarray, sparse matrix} of (n_samples, n_classes)
#     """
#     n_samples = X.shape[0]
#     sm = softmax(X @ betas)
#     weights = sm[:, k] - Y[:, k]
#     gradk = (X.T @ weights) / n_samples

#     return gradk

File Path: sparse_ho/utils_plot.py
Content:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os


def configure_plt():
    params = {
        'axes.labelsize': 14,
        'font.size': 14,
        'legend.fontsize': 14,
        'xtick.labelsize': 14,
        'ytick.labelsize': 14,
        'text.usetex': True,
    }
    plt.rcParams.update(params)
    sns.set_palette("colorblind")
    sns.set_style("ticks")


def plot_legend_apart(ax, figname, ncol=None, figwidth=10.67, fontsize=18):
    """Do all your plots with fig, ax = plt.subplots(),
    don't call plt.legend() at the end but this instead"""
    if ncol is None:
        ncol = len(ax.lines)
    fig = plt.figure(figsize=(10.67, 3.5), constrained_layout=True)
    fig.legend(ax.lines, [line.get_label() for line in ax.lines], ncol=ncol,
               loc="upper center", fontsize=18)
    # fig = plt.figure(figsize=(30, 4), constrained_layout=True)
    # fig = plt.figure(figsize=(figwidth, 2), constrained_layout=True)
    # fig.legend(ax.lines, [line.get_label() for line in ax.lines], ncol=ncol,
    #           loc="upper center", fontsize=fontsize)
    fig.tight_layout()
    fig.savefig(figname)
    os.system("pdfcrop %s %s" % (figname, figname))
    return fig


def discrete_cmap(N, base_cmap=None):
    """Create an N-bin discrete colormap from the specified input map"""

    # Note that if base_cmap is a string or None, you can simply do
    #    return plt.cm.get_cmap(base_cmap, N)
    # The following works for string, None, or a colormap instance:

    base = plt.cm.get_cmap(base_cmap)
    color_list = base(np.linspace(1/3, 1, N))
    cmap_name = base.name + str(N)
    return base.from_list(cmap_name, color_list, N)


def discrete_color(N, base_cmap=None):
    """Create an N-bin discrete colormap from the specified input map"""

    # Note that if base_cmap is a string or None, you can simply do
    #    return plt.cm.get_cmap(base_cmap, N)
    # The following works for string, None, or a colormap instance:

    base = plt.cm.get_cmap(base_cmap)
    color_list = base(np.linspace(1/2, 1, N))
    cmap_name = base.name + str(N)
    cmap = base.from_list(cmap_name, color_list, N)
    return cmap(np.linspace(0, 1, N))


def round_down(n, decimals=0):
    multiplier = 10 ** decimals
    return np.floor(n * multiplier) / multiplier


def round_up(n, decimals=0):
    multiplier = 10 ** decimals
    return np.ceil(n * multiplier) / multiplier


dict_color_2Dplot = {
    'implicit_forward': 'Greens',
    'implicit_forward_approx': 'Greens',
    'grid_search': 'Oranges',
    'random': 'Purples',
    'bayesian': 'Blues'
    }


current_palette = sns.color_palette("colorblind")
dict_color = {}
dict_color["grid_search"] = current_palette[3]
dict_color["random"] = current_palette[5]
dict_color["bayesian"] = current_palette[0]
dict_color["implicit_forward"] = current_palette[2]
dict_color["implicit_forward_approx"] = current_palette[2]
dict_color["forward"] = current_palette[4]
dict_color["implicit"] = current_palette[1]

dict_method = {}
dict_method["forward"] = 'F. Iterdiff.'
dict_method["implicit_forward"] = '1st order'
dict_method["implicit_forward_approx"] = '1st order approx'
dict_method['implicit'] = 'Implicit'
dict_method['grid_search'] = 'Grid-search'
dict_method['bayesian'] = 'Bayesian'
dict_method['random'] = 'Random-search'
dict_method['hyperopt'] = 'Random-search'
dict_method['backward'] = 'B. Iterdiff.'

dict_markers = {}
dict_markers["implicit_forward"] = 'X'
dict_markers["implicit_forward_approx"] = 'x'
dict_markers['implicit'] = 'v'
dict_markers['grid_search'] = '3'
# dict_markers['grid_search'] = 'o'
dict_markers['bayesian'] = 'P'
dict_markers['random'] = '*'

dict_title = {}
dict_title["rcv1_train"] = "rcv1"
dict_title["news20"] = "news20"
dict_title["finance"] = "finance"
dict_title["kdda_train"] = "kdda"
dict_title["climate"] = "climate"
dict_title["leukemia"] = "leukemia"
dict_title["real-sim"] = "real-sim"

dict_n_features = {}
dict_n_features["rcv1_train"] = r"($p=19,959$)"
dict_n_features["real-sim"] = r"($p=20,958$)"
dict_n_features["news20"] = r"($p=632,982$)"
dict_n_features["finance"] = r"($p=1,668,737$)"
dict_n_features["leukemia"] = r"($p=7129$)"

Output:
{
    "experimental_code": "class ImplicitForward():\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    get_only_jac.n_iter = i\n\n    return dbeta\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n    alpha = np.exp(log_alpha)\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None\n\nclass Lasso(BaseModel):\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        return alphas[mask] * np.sign(dense) @ jac\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y))\n            alpha_max /= X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        return np.clip(log_alpha, self.log_alpha_max - 12,\n                       self.log_alpha_max + np.log(0.9))\n\nclass WeightedLasso(BaseModel):\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alpha[j] * np.sign(beta[j]) / L[j]\n                ddual_var -= np.outer(X[:, j], (dbeta[j, :] - dbeta_old))\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \\\n                    (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz, :] -= np.outer(\n                    Xjs, (dbeta[j, :] - dbeta_old))\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        size_supp = mask.sum()\n        jac_t_v = np.zeros(size_supp)\n        jac_t_v = alphas[mask] * np.sign(dense) * jac\n        return jac_t_v\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y)) / X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        log_alpha = np.clip(log_alpha, self.log_alpha_max - 5,\n                            self.log_alpha_max + np.log(0.9))\n        return log_alpha\n\nclass FiniteDiffMonteCarloSure(BaseCriterion):\n    def __init__(self, sigma, finite_difference_step=None,\n                 random_state=42):\n        self.sigma = sigma\n        self.random_state = random_state\n        self.finite_difference_step = finite_difference_step\n        self.init_delta_epsilon = False\n\n        self.mask0 = None\n        self.dense0 = None\n        self.quantity_to_warm_start = None\n\n        self.mask02 = None\n        self.dense02 = None\n        self.quantity_to_warm_start2 = None\n\n        self.rmse = None\n\n    def _init_delta_epsilon(self, X):\n        if self.finite_difference_step:\n            self.epsilon = self.finite_difference_step\n        else:\n            self.epsilon = 2.0 * self.sigma / (X.shape[0]) ** 0.3\n        rng = check_random_state(self.random_state)\n        self.delta = rng.randn(X.shape[0])\n        self.init_delta_epsilon = True\n\n    def get_val_grad(\n            self, model, X, y, log_alpha, compute_beta_grad, max_iter=1000,\n            tol=1e-3, monitor=None):\n        if not self.init_delta_epsilon:\n            self._init_delta_epsilon(X)\n\n        def v(mask, dense):\n            X_m = X[:, mask]\n            return (2 * X_m.T @ (\n                    X_m @ dense - y -\n                    self.delta * self.sigma ** 2 / self.epsilon))\n\n        def v2(mask, dense):\n            return ((2 * self.sigma ** 2 *\n                     X[:, mask].T @ self.delta / self.epsilon))\n\n        mask, dense, jac_v, quantity_to_warm_start = compute_beta_grad(\n            X, y, log_alpha, model, v,\n            mask0=self.mask0, dense0=self.dense0,\n            quantity_to_warm_start=self.quantity_to_warm_start,\n            max_iter=max_iter, tol=tol, full_jac_v=True)\n        mask2, dense2, jac_v2, quantity_to_warm_start2 = compute_beta_grad(\n            X, y + self.epsilon * self.delta,\n            log_alpha, model, v2, mask0=self.mask02,\n            dense0=self.dense02,\n            quantity_to_warm_start=self.quantity_to_warm_start2,\n            max_iter=max_iter, tol=tol, full_jac_v=True)\n        val = self.get_val_outer(X, y, mask, dense, mask2, dense2)\n        self.mask0 = mask\n        self.dense0 = dense\n        self.quantity_to_warm_start = quantity_to_warm_start\n\n        self.mask02 = mask2\n        self.dense02 = dense2\n        self.quantity_to_warm_start2 = quantity_to_warm_start2\n\n        if jac_v is not None and jac_v2 is not None:\n            grad = jac_v + jac_v2\n        else:\n            grad = None\n        if monitor is not None:\n            monitor(val, grad, mask, dense, alpha=np.exp(log_alpha))\n\n        return val, grad",
    "experimental_info": "The hyperparameter optimization problem is framed as a bi-level optimization. The inner loop computes Lasso-type regression coefficients (or Elastic Net, Sparse Logistic Regression, Weighted Lasso, SVM, SVR, SimplexSVR coefficients) using proximal coordinate descent (BCD) algorithms. The inner solver can be `celer.Lasso`, `sklearn.linear_model.LogisticRegression`, `celer.ElasticNet`, or `lightning.classification.LinearSVC` for the respective models. Inner problem parameters include `max_iter` (e.g., 50-10000) and `tol` (e.g., 1e-3 to 1e-8), and `warm_start=True` is often used.\n\n\n\nThe core innovation is the efficient computation of the weak Jacobian (∂β̂/∂λ) using an 'implicit forward iterative differentiation' algorithm (`sparse_ho.ImplicitForward`). This algorithm first computes the regression coefficients and identifies their support, then applies a modified forward differentiation recursion restricted to this support. This decouples computation and avoids large matrix inversions. Key parameters for Jacobian computation are `tol_jac` (e.g., 1e-3 to 1e-8) and `n_iter_jac` (e.g., 100-1000).\n\n\n\nHyperparameters (λ) are parametrized as `log_alpha` internally, and converted to `alpha = np.exp(log_alpha)` for the models, to handle positivity constraints and scaling. Initial hyperparameter values (`alpha0`) are typically set relative to `alpha_max` (e.g., `alpha_max / 10` or `0.1 * alpha_max`), where `alpha_max` is derived from the data (e.g., `np.max(np.abs(X.T @ y)) / n_samples`). A range of hyperparameters from `alpha_min` (e.g., `alpha_max / 100` or `1e-4 * alpha_max`) to `alpha_max` is explored.\n\n\n\nThe outer loop optimizes a criterion such as held-out loss or SURE. Different optimizers are used for the outer problem:\n- `GradientDescent`: `n_outer` (number of outer iterations, e.g., 10-100), `step_size`, `p_grad_norm` (e.g., 1 to 1.9 for adaptive step size).\n- `LineSearch`: `n_outer` (e.g., 10-30), `tolerance_decrease` (e.g., 'constant', 'exponential') for inner solver tolerance.\n- `Adam`: `n_outer` (e.g., 10), `lr` (learning rate, e.g., 0.11), `beta_1`, `beta_2`.\n\n\n\nVarious criteria are employed:\n- `HeldOutMSE`, `HeldOutLogistic`, `HeldOutSmoothedHinge`: Evaluated on a validation set (`idx_val`) after training on `idx_train`.\n- `CrossVal`: Uses `sklearn.model_selection.KFold` (e.g., 5-fold) for splitting data.\n- `FiniteDiffMonteCarloSure`: A weakly differentiable approximation using Finite Differences Monte-Carlo (dof_FDMC). It requires a `sigma` (noise level) and an optional `finite_difference_step`; otherwise, it uses a power law heuristic (`2.0 * sigma / (X.shape[0]) ** 0.3`).\n\n\n\nExperiments are conducted on datasets such as `rcv1.binary`, `rcv1`, `simu` (synthetic data from `make_classification` or `make_correlated_data`), `mnist`, `usps`, `sector_scale`, `aloi`, and real MEG data. The `sparse_ho.utils.Monitor` class is used to track objective values, computation times, hyperparameter values, gradients, and accuracy metrics (`acc_vals`, `acc_tests`) throughout the optimization process."
}
