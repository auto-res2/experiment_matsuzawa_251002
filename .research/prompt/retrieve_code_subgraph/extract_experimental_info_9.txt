
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
PASHA extends ASHA and is inspired by the 'doubling trick' concept. It starts with a small initial amount of resources and progressively increases them only if the ranking of configurations in the top two rungs (rounds of promotion) has not stabilized. This allows for early stopping when the relative performance of promising configurations becomes consistent. To handle noise in the training process, PASHA employs a 'soft ranking' approach where configurations are considered equivalent if their performance difference is below a threshold \u03b5. This \u03b5 value is automatically estimated by identifying pairs of configurations that repeatedly swap ranks across different resource levels (epochs/iterations), calculating it as the N-th percentile (default 90th) of the performance differences among these 'criss-crossing' configurations. The algorithm maintains a maximum resource 'safety net' to prevent indefinite resource increases.

# Repository Content
File Path: benchmarking/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/benchmark_loop/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/benchmark_loop/baselines.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers import HyperbandScheduler, FIFOScheduler

methods = {
    "RS": lambda config_space, metric, mode, random_seed, max_t, resource_attr: FIFOScheduler(
        config_space=config_space,
        searcher="random",
        metric=metric,
        mode=mode,
        random_seed=random_seed,
    ),
    "HB": lambda config_space, metric, mode, random_seed, max_t, resource_attr: HyperbandScheduler(
        config_space=config_space,
        searcher="random",
        search_options={"debug_log": False},
        mode=mode,
        metric=metric,
        max_t=max_t,
        resource_attr=resource_attr,
        random_seed=random_seed,
    ),
    "GP": lambda config_space, metric, mode, random_seed, max_t, resource_attr: FIFOScheduler(
        config_space,
        searcher="bayesopt",
        search_options={"debug_log": False},
        metric=metric,
        mode=mode,
        random_seed=random_seed,
    ),
    "MOBSTER": lambda config_space, metric, mode, random_seed, max_t, resource_attr: HyperbandScheduler(
        config_space,
        searcher="bayesopt",
        search_options={"debug_log": False},
        mode=mode,
        metric=metric,
        max_t=max_t,
        resource_attr=resource_attr,
        random_seed=random_seed,
    ),
}

File Path: benchmarking/benchmark_loop/benchmark_definitions.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass


@dataclass
class BenchmarkDefinition:
    max_wallclock_time: float
    n_workers: int
    elapsed_time_attr: str
    metric: str
    mode: str
    blackbox_name: str
    dataset_name: str


def fcnet_benchmark(dataset_name):
    return BenchmarkDefinition(
        max_wallclock_time=1200,
        n_workers=4,
        elapsed_time_attr="metric_elapsed_time",
        metric="metric_valid_loss",
        mode="min",
        blackbox_name="fcnet",
        dataset_name=dataset_name,
    )


def nas201_benchmark(dataset_name):
    return BenchmarkDefinition(
        max_wallclock_time=3600 * 4,
        n_workers=4,
        elapsed_time_attr="metric_elapsed_time",
        metric="metric_valid_error",
        mode="min",
        blackbox_name="nasbench201",
        dataset_name=dataset_name,
    )


benchmark_definitions = {
    "fcnet-protein": fcnet_benchmark("protein_structure"),
    "fcnet-naval": fcnet_benchmark("naval_propulsion"),
    "fcnet-parkinsons": fcnet_benchmark("parkinsons_telemonitoring"),
    "fcnet-slice": fcnet_benchmark("slice_localization"),
    "nas201-cifar10": nas201_benchmark("cifar10"),
    "nas201-cifar100": nas201_benchmark("cifar100"),
    "nas201-ImageNet16-120": nas201_benchmark("ImageNet16-120"),
}

File Path: benchmarking/benchmark_loop/benchmark_main.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import itertools
import logging
from argparse import ArgumentParser
from tqdm import tqdm

from benchmarking.benchmark_loop.baselines import methods
from benchmarking.benchmark_loop.benchmark_definitions import benchmark_definitions
from syne_tune.blackbox_repository import BlackboxRepositoryBackend

from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune import StoppingCriterion, Tuner
from coolname import generate_slug


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag", type=str, required=False, default=generate_slug(2)
    )
    parser.add_argument("--num_seeds", type=int, required=False, default=2)
    parser.add_argument("--method", type=str, required=False)
    parser.add_argument("--benchmark", type=str, required=False)
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    num_seeds = args.num_seeds
    method_names = [args.method] if args.method is not None else list(methods.keys())
    benchmark_names = (
        [args.benchmark]
        if args.benchmark is not None
        else list(benchmark_definitions.keys())
    )

    logging.getLogger("syne_tune.optimizer.schedulers").setLevel(logging.WARNING)
    logging.getLogger("syne_tune.backend").setLevel(logging.WARNING)
    logging.getLogger("syne_tune.backend.simulator_backend.simulator_backend").setLevel(
        logging.WARNING
    )

    combinations = list(
        itertools.product(method_names, range(num_seeds), benchmark_names)
    )

    print(combinations)
    for method, seed, benchmark_name in tqdm(combinations):
        np.random.seed(seed)
        benchmark = benchmark_definitions[benchmark_name]

        print(
            f"Starting experiment ({method}/{benchmark_name}/{seed}) of {experiment_tag}"
        )

        trial_backend = BlackboxRepositoryBackend(
            elapsed_time_attr=benchmark.elapsed_time_attr,
            blackbox_name=benchmark.blackbox_name,
            dataset=benchmark.dataset_name,
        )

        max_t = max(trial_backend.blackbox.fidelity_values)
        resource_attr = next(iter(trial_backend.blackbox.fidelity_space.keys()))
        scheduler = methods[method](
            config_space=trial_backend.blackbox.configuration_space,
            metric=benchmark.metric,
            mode=benchmark.mode,
            random_seed=seed,
            max_t=max_t,
            resource_attr=resource_attr,
        )

        stop_criterion = StoppingCriterion(
            max_wallclock_time=benchmark.max_wallclock_time
        )

        tuner = Tuner(
            trial_backend=trial_backend,
            scheduler=scheduler,
            stop_criterion=stop_criterion,
            n_workers=benchmark.n_workers,
            sleep_time=0,
            callbacks=[SimulatorCallback()],
            results_update_interval=600,
            print_update_interval=600,
            tuner_name=f"{experiment_tag}-{method}-{seed}-{benchmark_name}".replace(
                "_", "-"
            ),
            metadata={
                "seed": seed,
                "algorithm": method,
                "tag": experiment_tag,
                "benchmark": benchmark_name,
            },
        )
        tuner.run()

File Path: benchmarking/benchmark_loop/launch_remote.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from argparse import ArgumentParser
from pathlib import Path

from coolname import generate_slug
from sagemaker.pytorch import PyTorch

from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role
import syne_tune
import benchmarking
from syne_tune.util import s3_experiment_path

if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag", type=str, required=False, default=generate_slug(2)
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    print(experiment_tag)
    est = PyTorch(
        entry_point="benchmark_main.py",
        source_dir=str(Path(__file__).parent),
        # instance_type="local",
        checkpoint_s3_uri=s3_experiment_path(tuner_name=experiment_tag),
        instance_type="ml.c5.4xlarge",
        instance_count=1,
        py_version="py38",
        framework_version="1.10.0",
        role=get_execution_role(),
        dependencies=syne_tune.__path__ + benchmarking.__path__,
        disable_profiler=True,
        hyperparameters={"experiment_tag": experiment_tag, "num_seeds": 30},
    )
    est.fit(job_name=experiment_tag)

File Path: benchmarking/benchmark_loop/plot_results.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from argparse import ArgumentParser
from pathlib import Path
from typing import Dict

import sagemaker
from matplotlib import cm
import numpy as np

from syne_tune.constants import ST_TUNER_TIME, SYNE_TUNE_DEFAULT_FOLDER
from syne_tune.experiments import load_experiments_df
import matplotlib.pyplot as plt


def show_results(df_task, title: str, colors: Dict, show_seeds: bool = False):

    if len(df_task) > 0:
        metric = df_task.loc[:, "metric_names"].values[0]
        mode = df_task.loc[:, "metric_mode"].values[0]

        fig, ax = plt.subplots()

        for algorithm in sorted(df_task.algorithm.unique()):
            ts = []
            ys = []

            df_scheduler = df_task[df_task.algorithm == algorithm]
            for i, tuner_name in enumerate(df_scheduler.tuner_name.unique()):
                sub_df = df_scheduler[df_scheduler.tuner_name == tuner_name]
                sub_df = sub_df.sort_values(ST_TUNER_TIME)
                t = sub_df.loc[:, ST_TUNER_TIME].values
                y_best = (
                    sub_df.loc[:, metric].cummax().values
                    if mode == "max"
                    else sub_df.loc[:, metric].cummin().values
                )
                if show_seeds:
                    ax.plot(t, y_best, color=colors[algorithm], alpha=0.2)
                ts.append(t)
                ys.append(y_best)

            # compute the mean/std over time-series of different seeds at regular time-steps
            # start/stop at respectively first/last point available for all seeds
            t_min = max(tt[0] for tt in ts)
            t_max = min(tt[-1] for tt in ts)
            if t_min > t_max:
                continue
            t_range = np.linspace(t_min, t_max)

            # find the best value at each regularly spaced time-step from t_range
            y_ranges = []
            for t, y in zip(ts, ys):
                indices = np.searchsorted(t, t_range, side="left")
                y_range = y[indices]
                y_ranges.append(y_range)
            y_ranges = np.stack(y_ranges)

            mean = y_ranges.mean(axis=0)
            std = y_ranges.std(axis=0)
            ax.fill_between(
                t_range,
                mean - std,
                mean + std,
                color=colors[algorithm],
                alpha=0.1,
            )
            ax.plot(t_range, mean, color=colors[algorithm], label=algorithm)

        ax.set_xlabel("wallclock time")
        ax.set_ylabel(metric)
        ax.legend()
        ax.set_title(title)

    (Path(__file__).parent / "figures").mkdir(exist_ok=True)
    plt.savefig(f"figures/{title}.png")
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag",
        type=str,
        required=True,
        help="the experiment tag that was displayed when running launch_rl_benchmark.py",
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    logging.getLogger().setLevel(logging.INFO)

    print(
        f"In case you ran experiments remotely, we assume that you pulled your results by running in a terminal: \n"
        f"aws s3 sync s3://{sagemaker.Session().default_bucket()}/{SYNE_TUNE_DEFAULT_FOLDER}/{experiment_tag}/ ~/syne-tune/"
    )
    experiment_filter = lambda exp: exp.metadata.get("tag") == experiment_tag
    name_filter = lambda path: str(experiment_tag) in str(path)
    df = load_experiments_df(name_filter, experiment_filter)
    benchmarks = df.benchmark.unique()

    for benchmark in benchmarks:
        df_task = df.loc[df.benchmark == benchmark, :]
        cmap = cm.Set3
        colors = {
            algorithm: cmap(i) for i, algorithm in enumerate(df.algorithm.unique())
        }
        show_results(df_task=df_task, title=benchmark, colors=colors)

File Path: benchmarking/cli/benchmark_factory.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from benchmarking.definitions.definition_nasbench201 import (
    nasbench201_benchmark,
    nasbench201_default_params,
    DATASET_NAMES as NB201_DATASET_NAMES,
)
from benchmarking.definitions.definition_nashpobench import (
    nashpobench_benchmark,
    nashpobench_default_params,
    DATASET_NAMES as FCNET_DATASET_NAMES,
)
from benchmarking.definitions.definition_lcbench import (
    lcbench_benchmark,
    lcbench_default_params,
    DATASET_NAMES as LCBENCH_DATASET_NAMES,
)
from benchmarking.definitions.definition_mlp_on_fashion_mnist import (
    mlp_fashionmnist_benchmark,
    mlp_fashionmnist_default_params,
)
from benchmarking.definitions.definition_resnet_cifar10 import (
    resnet_cifar10_benchmark,
    resnet_cifar10_default_params,
)
from benchmarking.nursery.lstm_wikitext2.definition_lstm_wikitext2 import (
    lstm_wikitext2_benchmark,
    lstm_wikitext2_default_params,
)

logger = logging.getLogger(__name__)

__all__ = ["supported_benchmarks", "benchmark_factory"]


MULTI_DATASET_BENCHMARKS = [
    "nasbench201_",
    "nashpobench_",
    "lcbench_",
]


_NB201_BENCHMARKS = {
    f"nasbench201_{dataset}": (nasbench201_benchmark, nasbench201_default_params)
    for dataset in NB201_DATASET_NAMES
}

_FCNET_BENCHMARKS = {
    f"nashpobench_{dataset}": (nashpobench_benchmark, nashpobench_default_params)
    for dataset in FCNET_DATASET_NAMES
}

_LCBENCH_BENCHMARKS = {
    f"lcbench_{dataset}": (lcbench_benchmark, lcbench_default_params)
    for dataset in LCBENCH_DATASET_NAMES
}

BENCHMARKS = {
    **_NB201_BENCHMARKS,
    **_FCNET_BENCHMARKS,
    **_LCBENCH_BENCHMARKS,
    "mlp_fashionmnist": (mlp_fashionmnist_benchmark, mlp_fashionmnist_default_params),
    "resnet_cifar10": (resnet_cifar10_benchmark, resnet_cifar10_default_params),
    "lstm_wikitext2": (lstm_wikitext2_benchmark, lstm_wikitext2_default_params),
}


def supported_benchmarks():
    return BENCHMARKS.keys()


def benchmark_factory(params):
    name = params["benchmark_name"]
    assert (
        name in supported_benchmarks()
    ), f"benchmark_name = {name} not supported, choose from:\n{supported_benchmarks()}"

    for prefix in MULTI_DATASET_BENCHMARKS:
        if name.startswith(prefix):
            dataset_name = name[len(prefix) :]
            params["dataset_name"] = dataset_name

    benchmark, default_params = BENCHMARKS[name]
    # We want to use `default_params` of the benchmark as input if not in
    # `params`
    default_params = default_params(params)
    _params = default_params.copy()
    # Note: `_params.update(params)` does not work, because `params` contains
    # None values
    for k, v in params.items():
        if v is not None:
            _params[k] = v
    return benchmark(_params), default_params

File Path: benchmarking/cli/estimator_factory.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from sagemaker.pytorch import PyTorch
from sagemaker.huggingface import HuggingFace
from sagemaker.estimator import Framework

from syne_tune.backend.sagemaker_backend.custom_framework import CustomFramework
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)

logger = logging.getLogger(__name__)


def sagemaker_estimator_factory(
    entry_point: str,
    instance_type: str,
    framework: str = None,
    role: str = None,
    instance_count: int = 1,
    framework_version: str = None,
    py_version: str = None,
    dependencies: list = None,
    **kwargs,
) -> Framework:
    if role is None:
        role = get_execution_role()
    if py_version is None:
        py_version = "py3"
    common_kwargs = dict(
        kwargs,
        instance_type=instance_type,
        instance_count=instance_count,
        role=role,
        sagemaker_session=default_sagemaker_session(),
    )
    if dependencies is not None:
        common_kwargs["dependencies"] = dependencies
    if framework == "PyTorch":
        sm_estimator = PyTorch(
            entry_point,
            framework_version=framework_version,
            py_version=py_version,
            **common_kwargs,
        )
    elif framework == "HuggingFace":
        sm_estimator = HuggingFace(
            py_version,
            entry_point,
            **common_kwargs,
            transformers_version=framework_version,
        )
    else:
        if framework is not None:
            logger.info(
                f"framework = '{framework}' not supported, using " "CustomFramework"
            )
        assert (
            kwargs.get("image_uri") is not None
        ), "CustomFramework requires 'image_uri' to be specified"
        sm_estimator = CustomFramework(entry_point, **common_kwargs)
    return sm_estimator

File Path: benchmarking/cli/launch_hpo.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from pathlib import Path
import itertools
import copy
import numpy as np

from syne_tune.backend import LocalBackend, SageMakerBackend
from syne_tune.optimizer.schedulers.searchers.searcher_callback import (
    StoreResultsAndModelParamsCallback,
    SimulatorAndModelParamsCallback,
)
from syne_tune import StoppingCriterion
from syne_tune import Tuner
from syne_tune.remote.remote_launcher import RemoteLauncher
from syne_tune.util import s3_experiment_path, repository_root_path

from benchmarking.cli.estimator_factory import sagemaker_estimator_factory
from benchmarking.cli.launch_utils import parse_args
from benchmarking.cli.benchmark_factory import benchmark_factory
from benchmarking.cli.scheduler_factory import scheduler_factory
from benchmarking.utils import dict_get

logger = logging.getLogger(__name__)


if __name__ == "__main__":
    """
    Example for calling the CLI:
    
    python benchmarking/cli/launch_hpo.py --scheduler hyperband_stopping \
        --searcher bayesopt --benchmark_name mlp_fashionmnist \
        --scheduler_timeout 120 --local_tuner
    
    See also `docs/command_line.md`.

    This is launching a single experiment. You can launch several experiments
    by passing list values for certain arguments (most arguments support list
    values). If you do this, the combinatorial product of all combinations
    is iterated over.
    
    Example:
    
    python benchmarking/cli/launch_hpo.py --scheduler fifo hyperband_stopping \
        --searcher bayesopt random \
        --benchmark_name mlp_fashionmnist \
        --scheduler_timeout 120 --local_tuner

    is launching 4 experiments, with (scheduler, searcher) taking on values
    (fifo, bayesopt), (hyperband_stopping, bayesopt), (fifo, random),
    (hyperband_stopping, random).
    
    If you use --local_tuner, these experiments are run in sequence. If you drop
    --local_tuner, experiments are launched remotely as SageMaker jobs, so
    they can all run in parallel.

    Iterating over all combinations is not always what you want. You can use
    --argument_groups in order to group list value arguments together. The list
    values of arguments in the same group are iterated over jointly (so zip
    instead of product).
    
    Example:
    
    python benchmarking/cli/launch_hpo.py \
        --scheduler hyperband_stopping fifo \
        --searcher bayesopt random \
        --max_resource_level 9 27 81 \
        --benchmark_name mlp_fashionmnist \
        --scheduler_timeout 120 --local_tuner \
        --argument_groups "scheduler searcher"

    is launching 2 * 3 = 6 experiments, with (scheduler, searcher,
    max_resource_level) taking on values
    (hyperband_stopping, bayesopt, 9), (fifo, random, 9),
    (hyperband_stopping, bayesopt, 27), (fifo, random, 27),
    (hyperband_stopping, bayesopt, 81), (fifo, random, 81).
    
    Several groups can be formed.
    
    Example:
    
    python benchmarking/cli/launch_hpo.py \
        --scheduler hyperband_stopping fifo \
        --searcher bayesopt random \
        --benchmark_name mlp_fashionmnist \
        --max_resource_level 27 81 \
        --scheduler_timeout 120 240 --local_tuner \
        --argument_groups "scheduler searcher|max_resource_level scheduler_timeout"
    
    is launching 2 * 2 = 4 experiments, with (scheduler, searcher,
    max_resource_level, scheduler_timeout) taking on values
    (hyperband_stopping, bayesopt, 27, 120), (fifo, random, 27, 120),
    (hyperband_stopping, bayesopt, 81, 240), (fifo, random, 81, 240).
    
    A special argument with list values is --run_id, its values must be
    distinct nonnegative integers. Here, "--num_runs 5" is short for
    "--run_id 0 1 2 3 4". If --run_id is given, --num_runs is ignored.
    If neither of the two is given, the default is run_id = 0.

    When multiple experiments are launched in this way, you can use the
    --skip_initial_experiments argument in order to skip this number of initial
    experiments before launching the remaining ones. This is useful if a
    previous call failed to launch all intended experiments (e.g., because an
    AWS instance limit was reached). If the initial K experiments were in
    fact launched, a subsequent call with --skip_initial_experiments K will
    launch only the remaining ones.
    
    Note that --benchmark_name does not support list values. This is because
    we also support benchmark-specific command line arguments.

    """
    orig_params = parse_args(allow_lists_as_values=True)
    if orig_params["debug_log_level"] and orig_params["local_tuner"]:
        # For remote tuning, 'debug_log_level' concerns the remote tuning
        # job, not the local one here (where logging.DEBUG just pollutes
        # the output)
        log_level = logging.DEBUG
    else:
        log_level = logging.INFO
    logging.getLogger().setLevel(log_level)

    # Basic checks not done in `parse_args`
    run_id = orig_params.get("run_id")
    if run_id is not None:
        if not isinstance(run_id, list):
            run_id = [run_id]
        else:
            assert len(set(run_id)) == len(
                run_id
            ), f"run_id = {run_id} contains duplicate entries"
        for rid in run_id:
            assert rid >= 0, f"run_id contains negative entry {rid}"
    else:
        # Alternative to specify `run_id`
        num_runs = orig_params.get("num_runs")
        if num_runs is None:
            run_id = [0]
        else:
            assert num_runs >= 1, f"num_runs = {num_runs} must be positive"
            run_id = list(range(num_runs))
            del orig_params["num_runs"]
        orig_params["run_id"] = run_id

    # Master random seed is offset plus run_id, modulo 2 ** 32. The offset
    # is drawn at random and displayed if not specified.
    random_seed_offset = orig_params.get("random_seed_offset")
    if random_seed_offset is None:
        random_seed_offset = np.random.randint(0, 2**32)
        orig_params["random_seed_offset"] = random_seed_offset
    logger.info(f"Using random_seed_offset = {random_seed_offset}")

    # Split into params with list values and normal values
    params_listvals = dict()
    params_nolistvals = dict()
    keys_with_list = []
    for k, v in orig_params.items():
        if isinstance(v, list):
            params_listvals[k] = v
            if len(v) > 1:
                keys_with_list.append(k)
        else:
            params_nolistvals[k] = v
    argument_groups = orig_params.get("argument_groups")
    # Group list arguments together
    if argument_groups is not None:
        groups = [x.strip() for x in argument_groups.split("|")]
        for group in groups:
            list_lists = []
            keys = group.split()
            # Singleton groups can be ignored
            if len(keys) > 1:
                for k in keys:
                    v = params_listvals.get(k)
                    assert (
                        v is not None
                    ), f"{k} in argument_groups group {group} is not a list argument"
                    if list_lists:
                        assert len(v) == len(list_lists[0]), (
                            f"Lists value in group {group} must all have "
                            + "the same length"
                        )
                    list_lists.append(v)
                    del params_listvals[k]
                group_key = "|".join(keys)
                params_listvals[group_key] = list(zip(*list_lists))
    num_experiments = 1
    keys = []
    list_values = []
    for k, v in params_listvals.items():
        keys.append(k)
        list_values.append(v)
        num_experiments *= len(v)

    skip_initial_experiments = orig_params["skip_initial_experiments"]
    assert (
        skip_initial_experiments >= 0
    ), "--skip_initial_experiments must be nonnegative"
    if num_experiments > 1:
        msg = (
            f"The following arguments have list values: {keys_with_list}\n"
            + f"Total number of experiments: {num_experiments}"
        )
        if skip_initial_experiments > 0:
            msg += (
                f"\nSkipping {skip_initial_experiments}, launching "
                + f"{num_experiments - skip_initial_experiments}"
            )
        logger.info(msg)

    # Loop over all combinations
    experiment_name = dict_get(orig_params, "experiment_name", "stune")
    backend_name = orig_params["backend"]
    if backend_name == "sagemaker" or not orig_params["local_tuner"]:
        s3_path = s3_experiment_path(
            s3_bucket=orig_params.get("s3_bucket"),
            experiment_name=None
            if orig_params["no_experiment_subdirectory"]
            else experiment_name,
        )
    else:
        s3_path = None  # Not needed (avoid boto call)

    if not list_values:
        list_values = [None]
    first_tuner_name, last_tuner_name = None, None
    is_first_iteration = True
    for exp_id, values in enumerate(itertools.product(*list_values)):
        if exp_id < skip_initial_experiments:
            continue
        if num_experiments > 1:
            logger.info(
                "\n---------------------------------\n"
                f"Launching experiment {exp_id} of {num_experiments}\n"
                "---------------------------------"
            )
        if keys:
            extra_dict = dict(zip(keys, values))
            for k in keys:
                if "|" in k:
                    extra_dict.update(zip(k.split("|"), extra_dict[k]))
                    del extra_dict[k]
            params = dict(params_nolistvals, **extra_dict)
        else:
            # Single experiment only
            params = params_nolistvals

        # Select benchmark to run
        benchmark, default_params = benchmark_factory(params)

        # Create scheduler from parameters
        myscheduler, params = scheduler_factory(params, benchmark, default_params)

        # Create backend
        if backend_name == "local":
            logger.info(
                f"Using 'local' back-end with entry_point = {benchmark['script']}"
            )
            trial_backend = LocalBackend(
                entry_point=benchmark["script"], rotate_gpus=params["rotate_gpus"]
            )
        elif backend_name == "simulated":
            assert benchmark.get("supports_simulated", False), (
                f"Benchmark {params['benchmark_name']} does not support "
                + "the simulation back-end (has to be tabulated)"
            )
            blackbox_name = benchmark.get("blackbox_name")
            backend_kwargs = dict(
                elapsed_time_attr=benchmark["elapsed_time_attr"],
                tuner_sleep_time=params["tuner_sleep_time"],
                debug_resource_attr=benchmark["resource_attr"],
            )
            if blackbox_name is None:
                # Tabulated benchmark given by a script (special case)
                from syne_tune.backend.simulator_backend.simulator_backend import (
                    SimulatorBackend,
                )

                logger.info(
                    f"Using 'simulated' back-end with entry_point = {benchmark['script']}"
                )
                backend_kwargs["entry_point"] = benchmark["script"]
                trial_backend = SimulatorBackend(**backend_kwargs)
            else:
                from syne_tune.blackbox_repository import BlackboxRepositoryBackend

                # Tabulated benchmark from the blackbox repository (simulation
                # runs faster)
                logger.info(
                    f"Using 'simulated' back-end with blackbox_name = {blackbox_name}"
                )
                seed = params.get("blackbox_seed")
                if seed is not None:
                    logger.info(f"Using blackbox with blackbox_seed = {seed}")
                surrogate = benchmark.get("surrogate")
                if surrogate is not None:
                    # If a surrogate is given, it interpolates the tabulated
                    # blackbox to the configuration space of the benchmark,
                    # which often has numerical domains where the tabulated
                    # benchmark has categorical ones
                    config_space_surrogate = benchmark["config_space"]
                else:
                    config_space_surrogate = None
                backend_kwargs.update(
                    {
                        "blackbox_name": blackbox_name,
                        "dataset": params.get("dataset_name"),
                        "surrogate": surrogate,
                        "surrogate_kwargs": benchmark.get("surrogate_kwargs"),
                        "config_space_surrogate": config_space_surrogate,
                        "max_resource_attr": benchmark.get("max_resource_attr"),
                        "seed": seed,
                    }
                )
                trial_backend = BlackboxRepositoryBackend(**backend_kwargs)
        else:
            assert backend_name == "sagemaker"
            for k in ("instance_type",):
                assert (
                    params.get(k) is not None
                ), f"For 'sagemaker' backend, --{k} is needed"
            logger.info(
                f"Using 'sagemaker' back-end with entry_point = {benchmark['script']}"
            )
            script_path = Path(benchmark["script"])
            sm_estimator = sagemaker_estimator_factory(
                entry_point=script_path.name,
                instance_type=params["instance_type"],
                framework=params.get("framework"),
                role=params.get("sagemaker_execution_role"),
                dependencies=[str(repository_root_path() / "benchmarking/")],
                framework_version=params.get("framework_version"),
                pytorch_version=params.get("pytorch_version"),
                source_dir=str(script_path.parent),
                image_uri=params.get("image_uri"),
                disable_profiler=not params["enable_sagemaker_profiler"],
            )
            trial_backend = SageMakerBackend(
                sm_estimator=sm_estimator,
                metrics_names=[benchmark["metric"]],
                s3_path=s3_path,
            )

        # Stopping criterion
        num_trials = params.get("num_trials")
        scheduler_timeout = params.get("scheduler_timeout")
        assert not (
            num_trials is None and scheduler_timeout is None
        ), "One of --num_trials, --scheduler_timeout must be given"
        stop_criterion = StoppingCriterion(
            max_wallclock_time=scheduler_timeout, max_num_trials_completed=num_trials
        )
        if params["no_tuner_logging"]:
            # If the tuner does not log anything, we also do not have to
            # compute the status report. This is achieved by setting the
            # update interval large enough
            print_update_interval = (
                18000 if scheduler_timeout is None else scheduler_timeout + 100
            )
            params["print_update_interval"] = max(
                params["print_update_interval"], print_update_interval
            )

        # Put together meta-data. Here, we could also do more ...
        metadata = {k: v for k, v in params.items() if v is not None}
        for k in ("metric", "mode", "resource_attr", "elapsed_time_attr"):
            if k in benchmark:
                metadata[k] = benchmark[k]
        tuner_name = experiment_name

        try:
            import git

            repo = git.Repo(search_parent_directories=True)
            sha = repo.head.object.hexsha
            metadata["git_hash"] = sha
            o = repo.remote()
            urls = list(o.urls)
            metadata["git_urls"] = urls
        except Exception:
            pass

        tuner_sleep_time = (
            0 if backend_name == "simulated" else params["tuner_sleep_time"]
        )
        # These callbacks also store surrogate model parameters (only for
        # schedulers which support this)
        callbacks = (
            [SimulatorAndModelParamsCallback()]
            if backend_name == "simulated"
            else [StoreResultsAndModelParamsCallback()]
        )

        local_tuner = Tuner(
            trial_backend=trial_backend,
            scheduler=myscheduler,
            stop_criterion=stop_criterion,
            n_workers=params["num_workers"],
            sleep_time=tuner_sleep_time,
            results_update_interval=params["results_update_interval"],
            metadata=metadata,
            tuner_name=tuner_name,
            max_failures=params["max_failures"],
            asynchronous_scheduling=True,
            print_update_interval=params["print_update_interval"],
            callbacks=callbacks,
        )
        last_tuner_name = local_tuner.name
        if is_first_iteration:
            first_tuner_name = copy.copy(last_tuner_name)
            is_first_iteration = False

        if params["local_tuner"]:
            # Tuning experiment is run locally
            if params["no_tuner_logging"]:
                logging.getLogger("syne_tune.tuner").setLevel(logging.ERROR)
            local_tuner.run()
        else:
            if backend_name != "sagemaker":
                # Local backend: Configure SageMaker estimator to what the
                # benchmark needs
                instance_type = params["instance_type"]
            else:
                # Instance type for tuning, can be different from
                # instance type for workers
                instance_type = params["tuner_instance_type"]
            estimator_kwargs = {
                "disable_profiler": not params["enable_sagemaker_profiler"]
            }
            if scheduler_timeout is not None and scheduler_timeout > 12 * 60 * 60:
                # Make sure that the SageMaker training job running the tuning
                # loop is not stopped before `scheduler_timeout`
                estimator_kwargs["max_run"] = int(1.01 * scheduler_timeout)
            log_level = logging.DEBUG if params["debug_log_level"] else logging.INFO
            root_path = repository_root_path()
            dependencies = [str(root_path / "benchmarking")]
            tuner = RemoteLauncher(
                tuner=local_tuner,
                dependencies=dependencies,
                instance_type=instance_type,
                log_level=log_level,
                s3_path=s3_path,
                no_tuner_logging=params["no_tuner_logging"],
                **estimator_kwargs,
            )
            tuner.run(wait=False)

    logger.info(f"For the record:\n{first_tuner_name} .. {last_tuner_name}")

File Path: benchmarking/cli/launch_sample_searcher_states.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
This script launches an experiment for the purpose of sampling searcher
states, which can then be used in unit tests.
"""
import logging

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner

from benchmarking.definitions.definition_mlp_on_fashion_mnist import (
    mlp_fashionmnist_benchmark,
    mlp_fashionmnist_default_params,
)
from benchmarking.cli.searcher_state_callback import StoreSearcherStatesCallback


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)

    # We pick the MLP on FashionMNIST benchmark
    # The 'benchmark' dict contains arguments needed by scheduler and
    # searcher (e.g., 'mode', 'metric'), along with suggested default values
    # for other arguments (which you are free to override)
    random_seed = 31415927
    n_workers = 4
    # generate_data_for = 'test_expdecay_model'
    generate_data_for = "test_iss_model"

    default_params = mlp_fashionmnist_default_params()
    benchmark = mlp_fashionmnist_benchmark(default_params)
    mode = benchmark["mode"]
    metric = benchmark["metric"]
    config_space = benchmark["config_space"]

    # Local back-end
    trial_backend = LocalBackend(entry_point=benchmark["script"])

    # GP-based Bayesian optimization searcher
    searcher = "bayesopt"
    if generate_data_for == "test_expdecay_model":
        search_options = {
            "num_init_random": 6,  # Good value for 4 workers
            "model": "gp_multitask",
            "gp_resource_kernel": "freeze-thaw",
        }
    else:
        assert generate_data_for == "test_iss_model"
        search_options = {
            "num_init_random": 6,  # Good value for 4 workers
            "model": "gp_issm",
            "issm_gamma_one": False,
        }
    # Hyperband (or successive halving) scheduler of the stopping type.
    # Together with 'bayesopt', this selects the MOBSTER algorithm.
    # If you don't like the defaults suggested, just change them:
    scheduler = HyperbandScheduler(
        config_space,
        searcher=searcher,
        search_options=search_options,
        max_t=default_params["max_resource_level"],
        grace_period=default_params["grace_period"],
        reduction_factor=default_params["reduction_factor"],
        resource_attr=benchmark["resource_attr"],
        mode=mode,
        metric=metric,
        random_seed=random_seed,
        searcher_data="all",  # We need densely sampled data
    )

    callback = StoreSearcherStatesCallback()
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=lambda status: status.wallclock_time > 600,
        n_workers=n_workers,
        callbacks=[callback],
    )

    tuner.run()

    print(f"Number of searcher states logged: {len(callback.states)}")
    print("Here is code for them:")
    for pos in range(len(callback.states)):
        print(f"\nSearcher state {pos}")
        print(callback.searcher_state_as_code(pos, add_info=True))

File Path: benchmarking/cli/launch_utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import argparse
import logging

from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import (
    SUPPORTED_RESOURCE_FOR_ACQUISITION,
)
from benchmarking.cli.benchmark_factory import supported_benchmarks, benchmark_factory

logger = logging.getLogger(__name__)

__all__ = [
    "parse_args",
    "make_searcher_and_scheduler",
]


def parse_args(allow_lists_as_values=True):
    """
    Argument parser for CLI. Normally, this parameterizes a single experiment.
    But if `allow_lists_as_values == True`, certain arguments admit lists as
    values. In this case, experiments of all combinations of values (Cartesian
    product) are launched.

    :param allow_lists_as_values: See above
    :return: params dict. Note that if an argument added to the parser is not
        provided a value for, it is contained in the dict with value None

    """
    parser = argparse.ArgumentParser(
        description="Asynchronous Hyperparameter Optimization"
    )
    # We parse the CL args twice. The first pass parses all global arguments
    # (not specific to the benchmark). From that pass, we know what the
    # benchmark is. In a second pass, we parse additional benchmark-specific
    # arguments, as defined in the default_params for the benchmark.
    if allow_lists_as_values:
        allow_list = dict(nargs="+")
    else:
        allow_list = dict()

    if allow_lists_as_values:
        parser.add_argument(
            "--argument_groups",
            type=str,
            help="Specify groups of list arguments, separated "
            "by |. Arguments in a group are iterated "
            "over together",
        )
    # Note: The benchmark cannot be a list argument, since it can define its
    # own CL arguments
    parser.add_argument(
        "--benchmark_name",
        type=str,
        default="mlp_fashionmnist",
        choices=supported_benchmarks(),
        help="Benchmark to run experiment on",
    )
    parser.add_argument(
        "--skip_initial_experiments",
        type=int,
        default=0,
        help="When multiple experiments are launched (due to "
        "list arguments), this number of initial "
        "experiments are skipped",
    )
    parser.add_argument(
        "--backend",
        type=str,
        default="local",
        choices=("local", "sagemaker", "simulated"),
        help="Backend for training evaluations",
    )
    parser.add_argument(
        "--local_tuner",
        action="store_true",
        help="Run tuning experiment locally? Otherwise, it is "
        "run remotely (which allows to run multiple "
        "tuning experiments in parallel)",
    )
    parser.add_argument(
        "--run_id",
        type=int,
        help="Identifier to distinguish between runs " "(nonnegative integers)",
        **allow_list,
    )
    parser.add_argument(
        "--num_runs",
        type=int,
        help="Number of repetitions, with run_id 0, 1, ..."
        "Only if run_id not given (ignored otherwise)",
    )
    parser.add_argument(
        "--random_seed_offset",
        type=int,
        help="Master random seed is this plus run_id, modulo "
        "2 ** 32. Drawn at random if not given",
    )
    parser.add_argument(
        "--instance_type",
        type=str,
        help="SageMaker instance type for workers",
        **allow_list,
    )
    parser.add_argument(
        "--tuner_instance_type",
        type=str,
        default="ml.c5.xlarge",
        help="SageMaker instance type for tuner (only for "
        "sagemaker backend and remote tuning)",
        **allow_list,
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        help="Number of workers (parallel evaluations)",
        **allow_list,
    )
    parser.add_argument(
        "--image_uri", type=str, help="URI of Docker image (sagemaker backend)"
    )
    parser.add_argument(
        "--sagemaker_execution_role",
        type=str,
        help="SageMaker execution role (sagemaker backend)",
    )
    parser.add_argument(
        "--experiment_name",
        type=str,
        help="Experiment name (used as job_name_prefix in " "sagemaker backend)",
    )
    parser.add_argument(
        "--no_debug_log", action="store_true", help="Switch off verbose logging"
    )
    parser.add_argument(
        "--debug_log_level",
        action="store_true",
        help="Set logging level to DEBUG (default is INFO)",
    )
    parser.add_argument(
        "--no_tuner_logging",
        action="store_true",
        help="By default, the full tuning status is logged "
        "in the tuning loop every --print_update_interval"
        " secs. If this is set, this logging is suppressed",
    )
    parser.add_argument(
        "--enable_sagemaker_profiler",
        action="store_true",
        help="Enable SageMaker profiler (this needs one "
        "processing job for each training job",
    )
    parser.add_argument(
        "--no_experiment_subdirectory",
        action="store_true",
        help="When storing results, do not use subdirectory " "experiment_name",
    )
    parser.add_argument(
        "--cost_model_type",
        type=str,
        help="Selects cost model of benchmark",
        **allow_list,
    )
    parser.add_argument(
        "--scheduler", type=str, default="fifo", help="Scheduler name", **allow_list
    )
    parser.add_argument("--searcher", type=str, help="Searcher name", **allow_list)
    parser.add_argument(
        "--results_update_interval",
        type=int,
        default=300,
        help="Results and tuner state are stored every this " "many seconds",
    )
    parser.add_argument(
        "--print_update_interval",
        type=int,
        default=300,
        help="Tuner status printed every this many seconds",
    )
    parser.add_argument(
        "--tuner_sleep_time",
        type=float,
        default=5,
        help="Tuner tries to fetch new results every this " "many seconds",
    )
    parser.add_argument(
        "--max_resource_level",
        type=int,
        help="Largest resource level (e.g., epoch number) " "for training evaluations",
        **allow_list,
    )
    parser.add_argument(
        "--epochs",
        type=int,
        help="Deprecated: Use max_resource_level instead",
        **allow_list,
    )
    parser.add_argument(
        "--num_trials", type=int, help="Maximum number of trials", **allow_list
    )
    parser.add_argument(
        "--scheduler_timeout",
        type=int,
        help="Trials started until this cutoff time (in secs)",
        **allow_list,
    )
    parser.add_argument(
        "--max_failures",
        type=int,
        default=1,
        help="The tuning job terminates once this many " "training evaluations failed",
        **allow_list,
    )
    parser.add_argument(
        "--s3_bucket",
        type=str,
        help="S3 bucket to write checkpoints and results to. "
        "Defaults to default bucket of session",
    )
    parser.add_argument(
        "--no_gpu_rotation",
        action="store_true",
        help="For local back-end on a GPU instance: By "
        "default, trials are launched in parallel "
        "on different GPU cores (GPU rotation). If "
        "this is set, all GPU cores are used for a "
        "single evaluation",
    )
    parser.add_argument(
        "--blackbox_repo_s3_root",
        type=str,
        help="S3 root directory for blackbox repository. "
        "Defaults to default bucket of session",
    )
    parser.add_argument(
        "--blackbox_seed",
        type=int,
        help="Fixed seeds of blackbox queries to this value "
        "(0 is safe), so that they return the same "
        "metric values for the same config",
    )
    # Arguments for scheduler
    parser.add_argument(
        "--brackets",
        type=int,
        help="Number of brackets in HyperbandScheduler",
        **allow_list,
    )
    parser.add_argument(
        "--reduction_factor",
        type=float,
        help="Reduction factor in HyperbandScheduler",
        **allow_list,
    )
    parser.add_argument(
        "--grace_period",
        type=int,
        help="Minimum resource level (e.g., epoch number) " "in HyperbandScheduler",
        **allow_list,
    )
    parser.add_argument(
        "--rung_levels",
        type=str,
        help="List of resource levels to use for the rungs "
        "in HyperbandScheduler. Entries must be positive "
        "ints. Overrides --grace_period, "
        "--reduction_factor if given",
        **allow_list,
    )
    parser.add_argument(
        "--rung_system_per_bracket",
        action="store_true",
        help="Parameter of HyperbandScheduler",
    )
    parser.add_argument(
        "--searcher_data",
        type=str,
        help="Parameter of HyperbandScheduler",
        **allow_list,
    )
    parser.add_argument(
        "--register_pending_myopic",
        action="store_true",
        help="Parameter of HyperbandScheduler",
    )
    parser.add_argument(
        "--not_normalize_targets",
        action="store_true",
        help="Do not normalize targets to mean 0, variance 1"
        " before fitting surrogate model",
    )
    parser.add_argument(
        "--pasha_ranking_criterion",
        type=str,
        help="Parameter of PASHA scheduler",
        **allow_list,
    )
    parser.add_argument(
        "--pasha_epsilon", type=float, help="Parameter of PASHA scheduler", **allow_list
    )
    parser.add_argument(
        "--pasha_epsilon_scaling",
        type=str,
        help="Parameter of PASHA scheduler",
        **allow_list,
    )
    # Arguments for bayesopt searcher
    parser.add_argument(
        "--searcher_model",
        type=str,
        help="Surrogate model for bayesopt searcher with " "HyperbandScheduler",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_num_init_random",
        type=int,
        help="Number of initial trials not chosen by searcher",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_num_init_candidates",
        type=int,
        help="Number of random candidates scored to seed search",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_num_fantasy_samples",
        type=int,
        help="Number of fantasy samples",
        **allow_list,
    )
    help_str = (
        "Rule for resource level at which acquisition function is used "
        + f"[{SUPPORTED_RESOURCE_FOR_ACQUISITION}]"
    )
    parser.add_argument(
        "--searcher_resource_acq", type=str, help=help_str, **allow_list
    )
    parser.add_argument(
        "--searcher_resource_acq_bohb_threshold",
        type=int,
        help="Parameter for resource_acq == bohb",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_gp_resource_kernel",
        type=str,
        help="Multi-task kernel for HyperbandScheduler",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_opt_skip_period",
        type=int,
        help="Update GP hyperparameters only every (...) times",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_opt_skip_init_length",
        type=int,
        help="Update GP hyperparameters every time until "
        "(...) observations are done",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_opt_skip_num_max_resource",
        action="store_true",
        help="Update GP hyperparameters only when training " "runs reach max_t",
    )
    parser.add_argument(
        "--searcher_opt_nstarts",
        type=int,
        help="GP hyperparameter optimization restarted (...) " "times",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_opt_maxiter",
        type=int,
        help="Maximum number of iterations of GP " "hyperparameter optimization",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_initial_scoring",
        type=str,
        help="Scoring function to rank initial candidates "
        "for seeding search [thompson_indep, acq_func]",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_skip_local_optimization",
        action="store_true",
        help="Skip local optimization of acquisition function "
        "and just pick top-scorer of initial candidates",
    )
    parser.add_argument(
        "--searcher_issm_gamma_one",
        action="store_true",
        help="Fix gamma parameter of ISSM to one?",
    )
    parser.add_argument(
        "--searcher_exponent_cost",
        type=float,
        help="Exponent of cost term in cost-aware expected "
        "improvement acquisition function",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_expdecay_normalize_inputs",
        action="store_true",
        help="Normalize resource values to [0, 1] in "
        "GP-expdecay surrogate model (only if "
        "searcher_model = gp_expdecay)",
    )
    parser.add_argument(
        "--searcher_num_init_candidates_for_batch",
        type=int,
        help="Relevant for synchronous Hyperband with bayesopt "
        "searcher. If batch of size B is suggested, the "
        "first suggest uses searcher_num_init_candidates, "
        "the B-1 subsequent suggests use this value",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_no_fantasizing",
        action="store_true",
        help="Ignore pending evaluations, do not use fantasizing",
    )
    # Arguments for kde searcher
    parser.add_argument(
        "--searcher_num_min_data_points",
        type=int,
        help="KDE: Minimum number of datapoints needed to fit models",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_top_n_percent",
        type=int,
        help="KDE: Top (bottom) model fit on this top (bottom) fraction of data",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_min_bandwidth",
        type=float,
        help="KDE: Minimum bandwidth",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_num_candidates",
        type=int,
        help="KDE: Number of candidates that are sampled to optimize the acquisition function",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_bandwidth_factor",
        type=int,
        help="KDE: Parameter to scale bandwidth",
        **allow_list,
    )
    parser.add_argument(
        "--searcher_random_fraction",
        type=float,
        help="KDE: Fraction of configs suggested at random",
        **allow_list,
    )

    # First pass: All global arguments
    # Why do we parse all global args here, and not just benchmark_name?
    # This is to make sure that the help option of the parser lists all
    # global arguments and their help strings.
    _params = parser.parse_known_args()[0]
    benchmark_name = _params.benchmark_name

    # Add benchmark-specific CL args (if any)
    # These are the ones listed in benchmark['default_params'], minus args which
    # are already global (i.e., added above)
    _, default_params = benchmark_factory({"benchmark_name": benchmark_name})
    help_str = f"Additional parameter for {benchmark_name} benchmark"
    have_extra_args = False
    for name, value in default_params.items():
        try:
            # We don't need to set defaults here
            if value is None:
                _type = str
            else:
                _type = type(value)
            parser.add_argument("--" + name, type=_type, help=help_str)
            have_extra_args = True
        except argparse.ArgumentError:
            pass

    # Second pass: All args (global and benchmark-specific)
    if have_extra_args:
        params = vars(parser.parse_args())
    else:
        params = _params
    # Post-processing
    params["debug_log"] = not params["no_debug_log"]
    del params["no_debug_log"]
    params["rotate_gpus"] = not params["no_gpu_rotation"]
    del params["no_gpu_rotation"]
    epochs = params.get("epochs")
    if params.get("max_resource_level") is None:
        if epochs is not None:
            logger.info(
                "--epochs is deprecated, please use "
                "--max_resource_level in the future"
            )
            params["max_resource_level"] = epochs
    elif epochs is not None:
        logger.info(
            "Both --max_resource_level and the deprecated "
            "--epochs are set. The latter is ignored"
        )
    if "epochs" in params:
        del params["epochs"]
    params["normalize_targets"] = not params["not_normalize_targets"]
    del params["not_normalize_targets"]
    return params


def _enter_not_none(dct, key, val, tp=None):
    if tp is None:
        tp = str
    if val is not None:
        dct[key] = tp(val)


def make_searcher_and_scheduler(params) -> (dict, dict):
    scheduler = params["scheduler"]
    searcher = params["searcher"]
    # Options for searcher
    search_options = dict()
    _enter_not_none(search_options, "debug_log", params.get("debug_log"), tp=bool)
    _enter_not_none(
        search_options, "normalize_targets", params.get("normalize_targets"), tp=bool
    )
    model = params.get("searcher_model")
    _enter_not_none(search_options, "model", model)

    if searcher.startswith("bayesopt"):
        # Options for bayesopt searcher
        searcher_args = (
            ("num_init_random", int, False),
            ("num_init_candidates", int, False),
            ("num_fantasy_samples", int, False),
            ("resource_acq", str, True),
            ("resource_acq_bohb_threshold", int, True),
            ("gp_resource_kernel", str, True),
            ("opt_skip_period", int, False),
            ("opt_skip_init_length", int, False),
            ("opt_skip_num_max_resource", bool, False),
            ("opt_nstarts", int, False),
            ("opt_maxiter", int, False),
            ("initial_scoring", str, False),
            ("skip_local_optimization", bool, False),
            ("issm_gamma_one", bool, False),
            ("exponent_cost", float, False),
            ("expdecay_normalize_inputs", bool, False),
            ("num_init_candidates_for_batch", int, False),
            ("no_fantasizing", bool, False),
        )
        gp_add_models = {"gp_issm", "gp_expdecay"}
        for name, tp, warn in searcher_args:
            _enter_not_none(search_options, name, params.get("searcher_" + name), tp=tp)
            if warn and name in search_options and model in gp_add_models:
                logger.warning(f"{name} not used with searcher_model = {model}")
        if "issm_gamma_one" in search_options and model != "gp_issm":
            logger.warning(
                f"searcher_issm_gamma_one not used with searcher_model = {model}"
            )
        if "expdecay_normalize_inputs" in search_options and model != "gp_expdecay":
            logger.warning(
                "searcher_expdecay_normalize_inputs not used with searcher_model "
                f"= {model}"
            )
    elif searcher == "kde":
        # Options for kde searcher
        searcher_args = (
            ("num_min_data_points", int),
            ("top_n_percent", int),
            ("min_bandwidth", float),
            ("num_candidates", int),
            ("bandwidth_factor", int),
            ("random_fraction", float),
        )
        for name, tp in searcher_args:
            _enter_not_none(search_options, name, params.get("searcher_" + name), tp=tp)

    # Options for scheduler
    random_seed_offset = params.get("random_seed_offset")
    if random_seed_offset is None:
        random_seed_offset = 0
    random_seed = (random_seed_offset + params["run_id"]) % (2**32)
    scheduler_options = {"random_seed": random_seed}
    name = "max_resource_level" if scheduler == "hyperband_synchronous" else "max_t"
    _enter_not_none(scheduler_options, name, params.get("max_resource_level"), tp=int)
    scheduler_args = ()
    if scheduler != "fifo":
        # Only process these arguments for HyperbandScheduler
        prefix = "hyperband_"
        assert scheduler.startswith(prefix)
        scheduler_args = scheduler_args + (
            ("reduction_factor", int),
            ("grace_period", int),
            ("brackets", int),
        )
        if scheduler != "hyperband_synchronous":
            sch_type = scheduler[len(prefix) :]
            _enter_not_none(scheduler_options, "type", sch_type)
            rung_levels = params.get("rung_levels")
            if rung_levels is not None:
                scheduler_options["rung_levels"] = sorted(
                    [int(x) for x in rung_levels.split()]
                )
            scheduler_args = scheduler_args + (
                ("searcher_data", str),
                ("register_pending_myopic", bool),
                ("rung_system_per_bracket", bool),
            )
    for name, tp in scheduler_args:
        _enter_not_none(scheduler_options, name, params.get(name), tp=tp)

    # Special constraints
    if (
        scheduler != "fifo"
        and searcher.startswith("bayesopt")
        and model in gp_add_models
    ):
        searcher_data = scheduler_options.get("searcher_data")
        if searcher_data is not None and searcher_data != "all":
            logger.warning(
                f"searcher_model = '{model}' requires "
                f"searcher_data = 'all' (and not '{searcher_data}')"
            )
        scheduler_options["searcher_data"] = "all"

    return search_options, scheduler_options

File Path: benchmarking/cli/scheduler_factory.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.scheduler import TrialScheduler
from syne_tune.optimizer.schedulers import FIFOScheduler, HyperbandScheduler
from syne_tune.optimizer.schedulers.synchronous import (
    SynchronousGeometricHyperbandScheduler,
)
from syne_tune.optimizer.schedulers.multiobjective import MOASHA
from syne_tune.constants import ST_WORKER_TIME

from benchmarking.cli.launch_utils import make_searcher_and_scheduler
from benchmarking.utils import dict_get

__all__ = [
    "scheduler_factory",
    "supported_schedulers",
]


def _check_searcher(searcher, supported_searchers):
    assert searcher is not None, "searcher needs to be provided"
    assert (
        searcher in supported_searchers
    ), f"searcher = '{searcher}' not supported ({supported_searchers})"


supported_schedulers = {
    "fifo",
    "hyperband_stopping",
    "hyperband_promotion",
    "hyperband_cost_promotion",
    "hyperband_pasha",
    "hyperband_synchronous",
    "mo_asha",
    "raytune_fifo",
    "raytune_hyperband",
}


# Note: If schedulers are the same for async and sync, only the async
# names are listed here
schedulers_with_search_options = {
    "fifo": FIFOScheduler,
    "hyperband_stopping": HyperbandScheduler,
    "hyperband_promotion": HyperbandScheduler,
    "hyperband_cost_promotion": HyperbandScheduler,
    "hyperband_pasha": HyperbandScheduler,
    "hyperband_synchronous": SynchronousGeometricHyperbandScheduler,
}


def scheduler_factory(
    params: dict, benchmark: dict, default_params: dict
) -> (TrialScheduler, dict):
    """
    Creates scheduler from command line parameters and benchmark descriptor.
    We also return the CL parameters extended by benchmark-specific default
    values.

    :param params: CL parameters
    :param benchmark: Benchmark descriptor
    :param default_params: Default params for benchmark
    :return: scheduler, imputed_params

    """
    params = params.copy()
    config_space = benchmark["config_space"]

    scheduler = params["scheduler"]
    assert (
        scheduler in supported_schedulers
    ), f"scheduler = '{scheduler}' not supported ({supported_schedulers})"
    _default_params = dict(instance_type="ml.m4.xlarge", num_workers=4)
    _default_params.update(default_params)
    for k, v in _default_params.items():
        if params.get(k) is None:
            params[k] = v
    if params.get("searcher_num_init_random") is None:
        # The default value for this is num_workers + 2
        params["searcher_num_init_random"] = params["num_workers"] + 2

    if scheduler in schedulers_with_search_options:
        searcher = params.get("searcher")
        if searcher is None:
            searcher = "random"
            params["searcher"] = searcher
        else:
            supported_searchers = {"random", "bayesopt", "kde"}
            if scheduler == "fifo":
                supported_searchers.update(
                    {
                        "bayesopt_cost_coarse",
                        "bayesopt_cost_fine",
                        "bayesopt_constrained",
                    }
                )
            elif scheduler != "hyperband_synchronous":
                supported_searchers.add("bayesopt_cost")
            _check_searcher(searcher, supported_searchers)

        # Searcher and scheduler options from params
        search_options, scheduler_options = make_searcher_and_scheduler(params)
        for k in ("metric", "mode", "max_resource_attr"):
            if k in benchmark:
                scheduler_options[k] = benchmark[k]
        if scheduler.startswith("hyperband"):
            k = "resource_attr"
            if k in benchmark:
                scheduler_options[k] = benchmark[k]
        if scheduler == "hyperband_cost_promotion" or searcher.startswith(
            "bayesopt_cost"
        ):
            # Benchmark may define 'cost_attr'. If not, check for
            # 'elapsed_time_attr'
            cost_attr = None
            keys = ("cost_attr", "elapsed_time_attr")
            for k in keys:
                if k in benchmark:
                    cost_attr = benchmark[k]
                    break
            if cost_attr is not None:
                if scheduler.startswith("hyperband"):
                    scheduler_options["cost_attr"] = cost_attr
                if searcher.startswith("bayesopt_cost"):
                    search_options["cost_attr"] = cost_attr
        k = "points_to_evaluate"
        if k in params:
            scheduler_options[k] = params.get(k)
        # Transfer benchmark -> search_options
        k = "map_reward"
        if k in benchmark:
            search_options[k] = benchmark[k]
        if searcher == "bayesopt_cost_fine" or searcher == "bayesopt_cost":
            keys = ("cost_model", "resource_attr")
        elif searcher == "bayesopt_constrained":
            keys = ("constraint_attr",)
        else:
            keys = ()
        for k in keys:
            v = benchmark.get(k)
            assert v is not None, (
                f"searcher = '{searcher}': Need {k} to be defined for " + "benchmark"
            )
            search_options[k] = v
        if searcher.startswith("bayesopt_cost"):
            searcher = "bayesopt_cost"  # Internal name
        if scheduler == "hyperband_pasha":
            rung_system_kwargs = scheduler_options.get("rung_system_kwargs", dict())
            for name, tp in (
                ("ranking_criterion", str),
                ("epsilon", float),
                ("epsilon_scaling", float),
            ):
                name_cl = "pasha_" + name
                v = params.get(name_cl)
                if v is not None:
                    rung_system_kwargs[name] = tp(v)
            if rung_system_kwargs:
                scheduler_options["rung_system_kwargs"] = rung_system_kwargs
        # Build scheduler and searcher
        scheduler_cls = schedulers_with_search_options[scheduler]
        myscheduler = scheduler_cls(
            config_space,
            searcher=searcher,
            search_options=search_options,
            **scheduler_options,
        )
    elif scheduler == "mo_asha":
        # Use the mode for the first metric as given in the benchmark and
        # minimize time
        mode = [benchmark["mode"], "min"]
        metrics = [benchmark["metric"], ST_WORKER_TIME]
        myscheduler = MOASHA(
            config_space,
            mode=mode,
            metrics=metrics,
            max_t=params["max_resource_level"],
            time_attr=benchmark["resource_attr"],
        )
    else:
        from ray.tune.schedulers import AsyncHyperBandScheduler
        from ray.tune.schedulers import FIFOScheduler as RT_FIFOScheduler
        from ray.tune.suggest.skopt import SkOptSearch
        from syne_tune.optimizer.schedulers import RayTuneScheduler
        from syne_tune.optimizer.schedulers.searchers import impute_points_to_evaluate

        searcher = params.get("searcher")
        if searcher is None:
            searcher = "random"
            params["searcher"] = searcher
        else:
            _check_searcher(searcher, {"random", "bayesopt"})
        rt_searcher = None  # Defaults to random
        metric = benchmark["metric"]
        mode = benchmark["mode"]
        points_to_evaluate = impute_points_to_evaluate(
            params.get("points_to_evaluate"), config_space
        )
        if searcher == "bayesopt":
            rt_searcher = SkOptSearch(points_to_evaluate=points_to_evaluate)
            points_to_evaluate = None
            rt_searcher.set_search_properties(
                mode=mode,
                metric=metric,
                config=RayTuneScheduler.convert_config_space(config_space),
            )
        if scheduler == "raytune_hyperband":
            rt_scheduler = AsyncHyperBandScheduler(
                max_t=params["max_resource_level"],
                grace_period=dict_get(params, "grace_period", 1),
                reduction_factor=dict_get(params, "reduction_factor", 3),
                brackets=dict_get(params, "brackets", 1),
                time_attr=benchmark["resource_attr"],
                mode=mode,
                metric=metric,
            )
        else:
            rt_scheduler = RT_FIFOScheduler()
            rt_scheduler.set_search_properties(metric=metric, mode=mode)
        myscheduler = RayTuneScheduler(
            config_space=config_space,
            ray_scheduler=rt_scheduler,
            ray_searcher=rt_searcher,
            points_to_evaluate=points_to_evaluate,
        )

    return myscheduler, params

File Path: benchmarking/cli/searcher_state_callback.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import json
import time

from syne_tune.tuner_callback import TunerCallback
from syne_tune import Tuner
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.schedulers import FIFOScheduler
from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import ModelBasedSearcher


class StoreSearcherStatesCallback(TunerCallback):
    """
    Stores list of searcher states alongside a tuning run. The list
    is extended by a new state whenever the `TuningJobState` has changed
    compared to the last recently added one.

    This callback is useful to create meaningful unit tests, by sampling
    a given searcher alongside a realistic experiment.

    Works only for `ModelBasedSearcher` searchers. For other searchers, nothing
    is stored.

    """

    def __init__(self):
        super().__init__()
        self._states = []
        self._num_observations = None
        self._start_time = time.time()
        self._searcher = None

    def on_tuning_start(self, tuner: Tuner):
        scheduler = tuner.scheduler
        if isinstance(scheduler, FIFOScheduler):
            searcher = scheduler.searcher
            if isinstance(searcher, ModelBasedSearcher):
                self._searcher = searcher

    def on_trial_result(self, trial: Trial, status: str, result: Dict, decision: str):
        if self._searcher is not None:
            state = self._searcher.state_transformer.state
            num_observations = state.num_observed_cases()
            if (
                self._num_observations is None
                or num_observations != self._num_observations
            ):
                searcher_state = self._searcher.get_state()
                searcher_state["elapsed_time"] = time.time() - self._start_time
                searcher_state["num_observations"] = num_observations
                searcher_state["num_configs"] = len(state.candidate_evaluations)
                self._states.append(searcher_state)
                self._num_observations = num_observations

    @property
    def states(self):
        return self._states

    def searcher_state_as_code(self, pos: int, add_info: bool = False):
        assert 0 <= pos < len(self._states)
        searcher_state = self._states[pos]
        lines = []
        if add_info:
            lines.append(f"# elapsed_time = {searcher_state['elapsed_time']}")
            lines.append(f"# num_observations = {searcher_state['num_observations']}")
            lines.append(f"# num_configs = {searcher_state['num_configs']}")
        model_params = searcher_state["model_params"]
        lines.append(f"_model_params = '{json.dumps(model_params)}'")
        # lines.append("model_params = json.loads(_model_params)")
        state = searcher_state["state"]
        lines.append(f"_state = '{json.dumps(state)}'")
        # lines.append("state = decode_state(enc_state=json.loads(_state), hp_ranges=hp_ranges)")
        return "\n".join(lines)

File Path: benchmarking/definitions/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/definitions/definition_distilbert_on_imdb.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
DistilBERT fine-tuned on IMDB sentiment classification task
"""
from pathlib import Path

from benchmarking.training_scripts.distilbert_on_imdb.distilbert_on_imdb import (
    METRIC_ACCURACY,
    RESOURCE_ATTR,
    _config_space,
)


def distilbert_imdb_default_params(params=None):
    return {
        "max_resource_level": 15,
        "instance_type": "ml.g4dn.xlarge",
        "num_workers": 4,
        "framework": "HuggingFace",
        "framework_version": "4.4",
        "pytorch_version": "1.6",
        "dataset_path": "./",
    }


def distilbert_imdb_benchmark(params):
    config_space = dict(
        _config_space,
        dataset_path=params["dataset_path"],
        max_steps=params["max_resource_level"],
    )
    return {
        "script": Path(__file__).parent.parent
        / "training_scripts"
        / "distilbert_on_imdb"
        / "distilbert_on_imdb.py",
        "metric": METRIC_ACCURACY,
        "mode": "max",
        "resource_attr": RESOURCE_ATTR,
        "max_resource_attr": "epochs",
        "config_space": config_space,
    }

File Path: benchmarking/definitions/definition_lcbench.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.blackbox_repository.conversion_scripts.scripts.lcbench.lcbench import (
    METRIC_ELAPSED_TIME,
    METRIC_ACCURACY,
    BLACKBOX_NAME,
    MAX_RESOURCE_LEVEL,
    CONFIGURATION_SPACE,
)


DATASET_NAMES = [
    "APSFailure",
    "Amazon_employee_access",
    "Australian",
    "Fashion-MNIST",
    "KDDCup09_appetency",
    "MiniBooNE",
    "adult",
    "airlines",
    "albert",
    "bank-marketing",
    "blood-transfusion-service-center",
    "car",
    "christine",
    "cnae-9",
    "connect-4",
    "covertype",
    "credit-g",
    "dionis",
    "fabert",
    "helena",
    "higgs",
    "jannis",
    "jasmine",
    "jungle_chess_2pcs_raw_endgame_complete",
    "kc1",
    "kr-vs-kp",
    "mfeat-factors",
    "nomao",
    "numerai28.6",
    "phoneme",
    "segment",
    "shuttle",
    "sylvine",
    "vehicle",
    "volkert",
]


def lcbench_default_params(params=None):
    return {
        "max_resource_level": MAX_RESOURCE_LEVEL,
        "grace_period": 1,
        "reduction_factor": 3,
        "instance_type": "ml.m5.large",
        "num_workers": 4,
        "framework": "PyTorch",
        "framework_version": "1.6",
        "dataset_name": "Fashion-MNIST",
    }


def lcbench_benchmark(params):
    config_space = dict(
        CONFIGURATION_SPACE,
        epochs=params["max_resource_level"],
        dataset_name=params["dataset_name"],
    )
    return {
        "script": None,
        "metric": METRIC_ACCURACY,
        "mode": "max",
        "elapsed_time_attr": METRIC_ELAPSED_TIME,
        "max_resource_attr": "epochs",
        "config_space": config_space,
        "cost_model": None,
        "supports_simulated": True,
        "blackbox_name": BLACKBOX_NAME,
    }

File Path: benchmarking/definitions/definition_mlp_on_fashion_mnist.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Two-layer MLP trained on Fashion MNIST
"""
from pathlib import Path

from benchmarking.training_scripts.mlp_on_fashion_mnist.mlp_on_fashion_mnist import (
    NUM_UNITS_1,
    NUM_UNITS_2,
    _config_space,
    METRIC_NAME,
    RESOURCE_ATTR,
    ELAPSED_TIME_ATTR,
)


def mlp_fashionmnist_default_params(params=None):
    return {
        "max_resource_level": 81,
        "grace_period": 1,
        "reduction_factor": 3,
        "instance_type": "ml.c5.4xlarge",
        "num_workers": 4,
        "framework": "PyTorch",
        "framework_version": "1.6",
        "dataset_path": "./",
        "report_current_best": "False",
    }


def mlp_fashionmnist_benchmark(params):
    config_space = dict(
        _config_space,
        dataset_path=params["dataset_path"],
        epochs=params["max_resource_level"],
        report_current_best=params["report_current_best"],
    )
    return {
        "script": Path(__file__).parent.parent
        / "training_scripts"
        / "mlp_on_fashion_mnist"
        / "mlp_on_fashion_mnist.py",
        "metric": METRIC_NAME,
        "mode": "max",
        "resource_attr": RESOURCE_ATTR,
        "elapsed_time_attr": ELAPSED_TIME_ATTR,
        "max_resource_attr": "epochs",
        "map_reward": "1_minus_x",
        "config_space": config_space,
        "cost_model": get_cost_model(params),
    }


def get_cost_model(params):
    """
    This cost model ignores the batch size, but depends on the number of units
    in the two layers only.
    """
    try:
        from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.linear_cost_model import (
            FixedLayersMLPCostModel,
        )

        num_inputs = 28 * 28
        num_outputs = 10
        num_units_keys = [NUM_UNITS_1, NUM_UNITS_2]
        (
            expected_hidden_layer_width,
            exp_vals,
        ) = FixedLayersMLPCostModel.get_expected_hidden_layer_width(
            _config_space, num_units_keys
        )
        return FixedLayersMLPCostModel(
            num_inputs=num_inputs,
            num_outputs=num_outputs,
            num_units_keys=num_units_keys,
            expected_hidden_layer_width=expected_hidden_layer_width,
        )
    except Exception:
        return None

File Path: benchmarking/definitions/definition_nasbench201.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.config_space import choice
from syne_tune.blackbox_repository.conversion_scripts.scripts.nasbench201_import import (
    CONFIG_KEYS,
    METRIC_VALID_ERROR,
    RESOURCE_ATTR,
    BLACKBOX_NAME,
)


DATASET_NAMES = [
    "cifar10",
    "cifar100",
    "ImageNet16-120",
]


METRIC_ELAPSED_TIME = "metric_elapsed_time"


# First is default value
x_range = ["skip_connect", "none", "nor_conv_1x1", "nor_conv_3x3", "avg_pool_3x3"]


_config_space = {k: choice(x_range) for k in CONFIG_KEYS}


def nasbench201_default_params(params=None):
    dont_sleep = str(params is not None and params.get("backend") == "simulated")
    return {
        "max_resource_level": 200,
        "grace_period": 1,
        "reduction_factor": 3,
        "instance_type": "ml.m5.large",
        "num_workers": 4,
        "framework": "PyTorch",
        "framework_version": "1.6",
        "dataset_name": "cifar10",
        "dont_sleep": dont_sleep,
        "cost_model_type": "linear",
    }


def nasbench201_benchmark(params):
    config_space = dict(
        _config_space,
        epochs=params["max_resource_level"],
        dataset_name=params["dataset_name"],
        dont_sleep=params["dont_sleep"],
        blackbox_repo_s3_root=params.get("blackbox_repo_s3_root"),
    )
    return {
        "script": None,
        "metric": METRIC_VALID_ERROR,
        "mode": "min",
        "resource_attr": RESOURCE_ATTR,
        "elapsed_time_attr": METRIC_ELAPSED_TIME,
        "max_resource_attr": "epochs",
        "config_space": config_space,
        "cost_model": _get_cost_model(params),
        "supports_simulated": True,
        "blackbox_name": BLACKBOX_NAME,
    }


def _get_cost_model(params):
    try:
        cost_model_type = params.get("cost_model_type")
        if cost_model_type is None:
            cost_model_type = "linear"
        if cost_model_type.startswith("linear"):
            from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.linear_cost_model import (
                NASBench201LinearCostModel,
            )

            map_config_values = {
                "skip_connect": NASBench201LinearCostModel.Op.SKIP_CONNECT,
                "none": NASBench201LinearCostModel.Op.NONE,
                "nor_conv_1x1": NASBench201LinearCostModel.Op.NOR_CONV_1x1,
                "nor_conv_3x3": NASBench201LinearCostModel.Op.NOR_CONV_3x3,
                "avg_pool_3x3": NASBench201LinearCostModel.Op.AVG_POOL_3x3,
            }
            conv_separate_features = "cnvsep" in cost_model_type
            count_sum = "sum" in cost_model_type
            cost_model = NASBench201LinearCostModel(
                config_keys=CONFIG_KEYS,
                map_config_values=map_config_values,
                conv_separate_features=conv_separate_features,
                count_sum=count_sum,
            )
        else:
            from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.sklearn_cost_model import (
                ScikitLearnCostModel,
            )

            cost_model = ScikitLearnCostModel(cost_model_type)
        return cost_model
    except Exception:
        return None

File Path: benchmarking/definitions/definition_nashpobench.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.blackbox_repository.conversion_scripts.scripts.fcnet_import import (
    METRIC_ELAPSED_TIME,
    METRIC_VALID_LOSS,
    RESOURCE_ATTR,
    BLACKBOX_NAME,
    MAX_RESOURCE_LEVEL,
    CONFIGURATION_SPACE,
    NUM_UNITS_1,
    NUM_UNITS_2,
)

from syne_tune.config_space import choice, uniform, loguniform, lograndint


DATASET_NAMES = [
    "protein_structure",
    "naval_propulsion",
    "parkinsons_telemonitoring",
    "slice_localization",
]


def _config_space(interpolate_blackbox: bool) -> dict:
    """
    :param interpolate_blackbox: Are blackbox values interpolated by a
        surrogate model?
    :return: Configuration space to be used
    """
    if interpolate_blackbox:
        return dict(
            hp_activation_fn_1=choice(["tanh", "relu"]),
            hp_activation_fn_2=choice(["tanh", "relu"]),
            hp_lr_schedule=choice(["cosine", "const"]),
            hp_batch_size=lograndint(8, 64),
            hp_dropout_1=uniform(0.0, 0.6),
            hp_dropout_2=uniform(0.0, 0.6),
            hp_init_lr=loguniform(0.0005, 0.1),
            hp_n_units_1=lograndint(16, 512),
            hp_n_units_2=lograndint(16, 512),
        )
    else:
        return CONFIGURATION_SPACE


def nashpobench_default_params(params=None):
    return {
        "max_resource_level": MAX_RESOURCE_LEVEL,
        "grace_period": 1,
        "reduction_factor": 3,
        "instance_type": "ml.m5.large",
        "num_workers": 4,
        "framework": "PyTorch",
        "framework_version": "1.6",
        "dataset_name": "protein_structure",
        "interpolate_blackbox": False,
    }


def nashpobench_benchmark(params):
    """
    The underlying tabulated blackbox does not have an `elapsed_time_attr`,
    but only a `time_this_resource_attr`.

    The boolean parameter `interpolate_blackbox` decides whether the
    tabulated blackbox values are interpolated (using a random forest), in
    which case the hyperparameter ranges are intervals, or whether the
    ranges are only exactly covering the tabulated grid.
    Note that the latter leads to a larger encoded dimension (which can
    be a problem for Bayesian optimization), because one of the numerical
    parameters has to be encoded as categorical.

    """
    interpolate_blackbox = params["interpolate_blackbox"]
    config_space = dict(
        _config_space(interpolate_blackbox),
        epochs=params["max_resource_level"],
        dataset_name=params["dataset_name"],
    )
    if interpolate_blackbox:
        surrogate = "RandomForestRegressor"
        surrogate_kwargs = dict(max_samples=0.005, bootstrap=True)
    else:
        surrogate = None
        surrogate_kwargs = None
    return {
        "script": None,
        "metric": METRIC_VALID_LOSS,
        "mode": "min",
        "resource_attr": RESOURCE_ATTR,
        "elapsed_time_attr": METRIC_ELAPSED_TIME,
        "max_resource_attr": "epochs",
        "config_space": config_space,
        "cost_model": get_cost_model(params),
        "supports_simulated": True,
        "blackbox_name": BLACKBOX_NAME,
        "surrogate": surrogate,
        "surrogate_kwargs": surrogate_kwargs,
    }


# See Table 1 in https://arxiv.org/abs/1905.04970
_NUM_FEATURES = {
    "protein_structure": 9,
    "naval_propulsion": 15,
    "parkinsons_telemonitoring": 20,
    "slice_localization": 385,
}


def get_cost_model(params):
    """
    This cost model ignores the batch size, but depends on the number of units
    in the two layers only.
    """
    try:
        from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.linear_cost_model import (
            FixedLayersMLPCostModel,
        )

        num_inputs = _NUM_FEATURES[params["dataset_name"]]
        num_outputs = 1  # All benchmarks are regression problems
        num_units_keys = [NUM_UNITS_1, NUM_UNITS_2]
        (
            expected_hidden_layer_width,
            exp_vals,
        ) = FixedLayersMLPCostModel.get_expected_hidden_layer_width(
            CONFIGURATION_SPACE, num_units_keys
        )
        return FixedLayersMLPCostModel(
            num_inputs=num_inputs,
            num_outputs=num_outputs,
            num_units_keys=num_units_keys,
            expected_hidden_layer_width=expected_hidden_layer_width,
        )
    except Exception:
        return None

File Path: benchmarking/definitions/definition_resnet_cifar10.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path

from benchmarking.utils import get_cost_model_for_batch_size
from benchmarking.training_scripts.resnet_cifar10.resnet_cifar10 import (
    BATCH_SIZE_LOWER,
    BATCH_SIZE_UPPER,
    BATCH_SIZE_KEY,
    METRIC_NAME,
    RESOURCE_ATTR,
    ELAPSED_TIME_ATTR,
    _config_space,
)


def resnet_cifar10_default_params(params=None):
    if params is not None and params.get("backend") == "sagemaker":
        instance_type = "ml.g4dn.xlarge"
    else:
        # For local backend, GPU cores serve different workers, so we
        # need more memory
        instance_type = "ml.g4dn.12xlarge"
    return {
        "epochs": 27,
        "grace_period": 1,
        "reduction_factor": 3,
        "instance_type": instance_type,
        "num_workers": 4,
        "framework": "PyTorch",
        "framework_version": "1.6",
        "num_gpus": 1,
        "cost_model_type": "quadratic_spline",
    }


def resnet_cifar10_benchmark(params):
    config_space = dict(
        _config_space,
        epochs=params["max_resource_level"],
        dataset_path=params["dataset_path"],
        num_gpus=params["num_gpus"],
    )
    return {
        "script": Path(__file__).parent.parent
        / "training_scripts"
        / "resnet_cifar10"
        / "resnet_cifar10.py",
        "metric": METRIC_NAME,
        "mode": "max",
        "resource_attr": RESOURCE_ATTR,
        "elapsed_time_attr": ELAPSED_TIME_ATTR,
        "max_resource_attr": "epochs",
        "map_reward": "1_minus_x",
        "config_space": config_space,
        "cost_model": get_cost_model_for_batch_size(
            params,
            batch_size_key=BATCH_SIZE_KEY,
            batch_size_range=(BATCH_SIZE_LOWER, BATCH_SIZE_UPPER),
        ),
    }

File Path: benchmarking/nursery/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/nursery/benchmark_automl/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/nursery/benchmark_automl/baselines.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from typing import Dict, Optional

from syne_tune.blackbox_repository.simulated_tabular_backend import (
    BlackboxRepositoryBackend,
)
from syne_tune.optimizer.baselines import ZeroShotTransfer
from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.schedulers.median_stopping_rule import MedianStoppingRule
from syne_tune.optimizer.schedulers.transfer_learning import RUSHScheduler
from syne_tune.optimizer.schedulers.transfer_learning.bounding_box import BoundingBox
from syne_tune.optimizer.schedulers.searchers.regularized_evolution import (
    RegularizedEvolution,
)
from syne_tune.optimizer.schedulers.transfer_learning.quantile_based.quantile_based_searcher import (
    QuantileBasedSurrogateSearcher,
)


@dataclass
class MethodArguments:
    config_space: dict
    metric: str
    mode: str
    random_seed: int
    resource_attr: str
    max_t: Optional[int] = None
    max_resource_attr: Optional[str] = None
    transfer_learning_evaluations: Optional[Dict] = None
    use_surrogates: bool = False
    num_brackets: Optional[int] = None
    verbose: Optional[bool] = False


class Methods:
    RS = "RS"
    ASHA = "ASHA"
    MSR = "RS-MSR"
    ASHA_BB = "ASHA-BB"
    ASHA_CTS = "ASHA-CTS"
    GP = "GP"
    BOHB = "BOHB"
    REA = "REA"
    MOBSTER = "MOB"
    TPE = "TPE"
    BORE = "BORE"
    ZERO_SHOT = "ZS"
    RUSH = "RUSH"


def _max_resource_attr_or_max_t(
    args: MethodArguments, max_t_name: str = "max_t"
) -> dict:
    if args.max_resource_attr is not None:
        return {"max_resource_attr": args.max_resource_attr}
    else:
        assert args.max_t is not None
        return {max_t_name: args.max_t}


def search_options(args: MethodArguments) -> dict:
    if args.verbose:
        return {"debug_log": True}
    else:
        return {"debug_log": False}


methods = {
    Methods.RS: lambda method_arguments: FIFOScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.ASHA: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        type="promotion",
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        **_max_resource_attr_or_max_t(method_arguments),
    ),
    Methods.MSR: lambda method_arguments: MedianStoppingRule(
        scheduler=FIFOScheduler(
            config_space=method_arguments.config_space,
            searcher="random",
            metric=method_arguments.metric,
            mode=method_arguments.mode,
            random_seed=method_arguments.random_seed,
        ),
        resource_attr=method_arguments.resource_attr,
        running_average=False,
    ),
    Methods.ASHA_BB: lambda method_arguments: BoundingBox(
        scheduler_fun=lambda new_config_space, mode, metric: HyperbandScheduler(
            new_config_space,
            searcher="random",
            metric=metric,
            mode=mode,
            search_options=search_options(method_arguments),
            resource_attr=method_arguments.resource_attr,
            random_seed=method_arguments.random_seed,
            **_max_resource_attr_or_max_t(method_arguments),
        ),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        config_space=method_arguments.config_space,
        transfer_learning_evaluations=method_arguments.transfer_learning_evaluations,
        num_hyperparameters_per_task=10,
    ),
    Methods.ASHA_CTS: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher=QuantileBasedSurrogateSearcher(
            mode=method_arguments.mode,
            config_space=method_arguments.config_space,
            metric=method_arguments.metric,
            transfer_learning_evaluations=method_arguments.transfer_learning_evaluations,
            random_seed=method_arguments.random_seed,
        ),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        resource_attr=method_arguments.resource_attr,
        **_max_resource_attr_or_max_t(method_arguments),
    ),
    Methods.GP: lambda method_arguments: FIFOScheduler(
        method_arguments.config_space,
        searcher="bayesopt",
        search_options=search_options(method_arguments),
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.REA: lambda method_arguments: FIFOScheduler(
        config_space=method_arguments.config_space,
        searcher=RegularizedEvolution(
            config_space=method_arguments.config_space,
            metric=method_arguments.metric,
            mode=method_arguments.mode,
            random_seed=method_arguments.random_seed,
            population_size=10,
            sample_size=5,
        ),
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.BOHB: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="kde",
        search_options={"debug_log": False, "min_bandwidth": 0.1},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        **_max_resource_attr_or_max_t(method_arguments),
    ),
    Methods.TPE: lambda method_arguments: FIFOScheduler(
        config_space=method_arguments.config_space,
        searcher="kde",
        search_options={"debug_log": False, "min_bandwidth": 0.1},
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.BORE: lambda method_arguments: FIFOScheduler(
        config_space=method_arguments.config_space,
        searcher="bore",
        search_options={"classifier": "mlp"},
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.MOBSTER: lambda method_arguments: HyperbandScheduler(
        method_arguments.config_space,
        searcher="bayesopt",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        **_max_resource_attr_or_max_t(method_arguments),
    ),
    Methods.ZERO_SHOT: lambda method_arguments: ZeroShotTransfer(
        config_space=method_arguments.config_space,
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        transfer_learning_evaluations=method_arguments.transfer_learning_evaluations,
        use_surrogates=method_arguments.use_surrogates,
        random_seed=method_arguments.random_seed,
    ),
    Methods.RUSH: lambda method_arguments: RUSHScheduler(
        config_space=method_arguments.config_space,
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        transfer_learning_evaluations=method_arguments.transfer_learning_evaluations,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        **_max_resource_attr_or_max_t(method_arguments),
    ),
}


if __name__ == "__main__":
    # Run a loop that initializes all schedulers on all benchmark to see if they all work
    from benchmarking.nursery.benchmark_automl.benchmark_main import (
        get_transfer_learning_evaluations,
    )
    from benchmarking.nursery.benchmark_automl.benchmark_definitions import (
        benchmark_definitions,
    )

    benchmarks = ["fcnet-protein", "nas201-cifar10", "lcbench-Fashion-MNIST"]
    for benchmark_name in benchmarks:
        benchmark = benchmark_definitions[benchmark_name]
        backend = BlackboxRepositoryBackend(
            elapsed_time_attr=benchmark.elapsed_time_attr,
            blackbox_name=benchmark.blackbox_name,
            dataset=benchmark.dataset_name,
        )
        for method_name, method_fun in methods.items():
            print(f"checking initialization of: {method_name}, {benchmark_name}")
            scheduler = method_fun(
                MethodArguments(
                    config_space=backend.blackbox.configuration_space,
                    metric=benchmark.metric,
                    mode=benchmark.mode,
                    random_seed=0,
                    max_t=max(backend.blackbox.fidelity_values),
                    resource_attr=next(iter(backend.blackbox.fidelity_space.keys())),
                    transfer_learning_evaluations=get_transfer_learning_evaluations(
                        blackbox_name=benchmark.blackbox_name,
                        test_task=benchmark.dataset_name,
                        datasets=benchmark.datasets,
                    ),
                    use_surrogates=benchmark_name == "lcbench-Fashion-MNIST",
                )
            )
            scheduler.suggest(0)
            scheduler.suggest(1)

File Path: benchmarking/nursery/benchmark_automl/benchmark_definitions.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from typing import Optional, List


@dataclass
class BenchmarkDefinition:
    max_wallclock_time: float
    n_workers: int
    elapsed_time_attr: str
    metric: str
    mode: str
    blackbox_name: str
    dataset_name: str
    max_resource_attr: str
    max_num_evaluations: Optional[int] = None
    surrogate: Optional[str] = None
    surrogate_kwargs: Optional[dict] = None
    datasets: Optional[List[str]] = None


def fcnet_benchmark(dataset_name):
    return BenchmarkDefinition(
        max_wallclock_time=1200,
        n_workers=4,
        elapsed_time_attr="metric_elapsed_time",
        metric="metric_valid_loss",
        mode="min",
        blackbox_name="fcnet",
        dataset_name=dataset_name,
        max_resource_attr="epochs",
    )


def nas201_benchmark(dataset_name):
    return BenchmarkDefinition(
        max_wallclock_time=6 * 3600,
        n_workers=4,
        elapsed_time_attr="metric_elapsed_time",
        metric="metric_valid_error",
        mode="min",
        blackbox_name="nasbench201",
        dataset_name=dataset_name,
        max_resource_attr="epochs",
    )


def lcbench_benchmark(dataset_name, datasets):
    return BenchmarkDefinition(
        max_wallclock_time=7200,
        n_workers=4,
        elapsed_time_attr="time",
        metric="val_accuracy",
        mode="max",
        blackbox_name="lcbench",
        dataset_name=dataset_name,
        surrogate="KNeighborsRegressor",
        surrogate_kwargs={"n_neighbors": 1},
        max_num_evaluations=4000,
        datasets=datasets,
        max_resource_attr="epochs",
    )


benchmark_definitions = {
    "fcnet-protein": fcnet_benchmark("protein_structure"),
    "fcnet-naval": fcnet_benchmark("naval_propulsion"),
    "fcnet-parkinsons": fcnet_benchmark("parkinsons_telemonitoring"),
    "fcnet-slice": fcnet_benchmark("slice_localization"),
    "nas201-cifar10": nas201_benchmark("cifar10"),
    "nas201-cifar100": nas201_benchmark("cifar100"),
    "nas201-ImageNet16-120": nas201_benchmark("ImageNet16-120"),
}

# 5 most expensive lcbench datasets
lc_bench_datasets = [
    "Fashion-MNIST",
    "airlines",
    "albert",
    "covertype",
    "christine",
]
for task in lc_bench_datasets:
    benchmark_definitions[
        "lcbench-" + task.replace("_", "-").replace(".", "")
    ] = lcbench_benchmark(task, datasets=lc_bench_datasets)

File Path: benchmarking/nursery/benchmark_automl/benchmark_main.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List

import numpy as np
import itertools
import logging
from argparse import ArgumentParser
from tqdm import tqdm

from syne_tune.blackbox_repository import load_blackbox
from syne_tune.blackbox_repository.simulated_tabular_backend import (
    BlackboxRepositoryBackend,
)
from benchmarking.nursery.benchmark_automl.baselines import MethodArguments

from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
)
from syne_tune.stopping_criterion import StoppingCriterion
from syne_tune.tuner import Tuner


def get_transfer_learning_evaluations(
    blackbox_name: str,
    test_task: str,
    datasets: Optional[List[str]],
    n_evals: Optional[int] = None,
) -> dict:
    """
    :param blackbox_name:
    :param test_task: task where the performance would be tested, it is excluded from transfer-learning evaluations
    :param datasets: subset of datasets to consider, only evaluations from those datasets are provided to
    transfer-learning methods. If none, all datasets are used.
    :param n_evals: maximum number of evaluations to be returned
    :return:
    """
    task_to_evaluations = load_blackbox(blackbox_name)

    # todo retrieve right metric
    metric_index = 0
    transfer_learning_evaluations = {
        task: TransferLearningTaskEvaluations(
            configuration_space=bb.configuration_space,
            hyperparameters=bb.hyperparameters,
            objectives_evaluations=bb.objectives_evaluations[
                ..., metric_index : metric_index + 1
            ],
            objectives_names=[bb.objectives_names[metric_index]],
        )
        for task, bb in task_to_evaluations.items()
        if task != test_task and (datasets is None or task in datasets)
    }

    if n_evals is not None:
        # subsample n_evals / n_tasks of observations on each tasks
        def subsample(
            transfer_evaluations: TransferLearningTaskEvaluations, n: int
        ) -> TransferLearningTaskEvaluations:
            random_indices = np.random.permutation(
                len(transfer_evaluations.hyperparameters)
            )[:n]
            return TransferLearningTaskEvaluations(
                configuration_space=transfer_evaluations.configuration_space,
                hyperparameters=transfer_evaluations.hyperparameters.loc[
                    random_indices
                ].reset_index(drop=True),
                objectives_evaluations=transfer_evaluations.objectives_evaluations[
                    random_indices
                ],
                objectives_names=transfer_evaluations.objectives_names,
            )

        n = n_evals // len(transfer_learning_evaluations)
        transfer_learning_evaluations = {
            task: subsample(transfer_evaluations, n)
            for task, transfer_evaluations in transfer_learning_evaluations.items()
        }

    return transfer_learning_evaluations


def parse_args(methods: dict, benchmark_definitions: dict):
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--num_seeds",
        type=int,
        required=False,
        default=3,
        help="number of seeds to run",
    )
    parser.add_argument(
        "--run_all_seed",
        type=int,
        default=1,
        help="if 1 run all the seeds [0, `num_seeds`-1], otherwise run seed `num_seeds` only",
    )
    parser.add_argument(
        "--method", type=str, required=False, help="a method to run from baselines.py"
    )
    parser.add_argument(
        "--benchmark",
        type=str,
        required=False,
        help="a benchmark to run from benchmark_definitions.py",
    )
    parser.add_argument(
        "--verbose",
        type=int,
        default=0,
        help="verbose log output?",
    )
    parser.add_argument(
        "--num_brackets",
        type=int,
        required=False,
        help="number of brackets",
    )
    parser.add_argument(
        "--start_seed",
        type=int,
        default=0,
        help="first seed to run (if run_all_seed)",
    )
    args, _ = parser.parse_known_args()
    args.verbose = bool(args.verbose)
    if args.run_all_seed == 1:
        seeds = list(range(args.start_seed, args.num_seeds))
    else:
        seeds = [args.num_seeds]
    method_names = [args.method] if args.method is not None else list(methods.keys())
    benchmark_names = (
        [args.benchmark]
        if args.benchmark is not None
        else list(benchmark_definitions.keys())
    )
    return args, method_names, benchmark_names, seeds


def main(methods: dict, benchmark_definitions: dict):
    args, method_names, benchmark_names, seeds = parse_args(
        methods, benchmark_definitions
    )
    experiment_tag = args.experiment_tag

    if args.verbose:
        logging.getLogger().setLevel(logging.INFO)
    else:
        logging.getLogger("syne_tune.optimizer.schedulers").setLevel(logging.WARNING)
        logging.getLogger("syne_tune.backend").setLevel(logging.WARNING)
        logging.getLogger(
            "syne_tune.backend.simulator_backend.simulator_backend"
        ).setLevel(logging.WARNING)

    combinations = list(itertools.product(method_names, seeds, benchmark_names))
    print(combinations)
    for method, seed, benchmark_name in tqdm(combinations):
        np.random.seed(seed)
        benchmark = benchmark_definitions[benchmark_name]

        print(
            f"Starting experiment ({method}/{benchmark_name}/{seed}) of {experiment_tag}"
        )

        max_resource_attr = benchmark.max_resource_attr
        backend = BlackboxRepositoryBackend(
            elapsed_time_attr=benchmark.elapsed_time_attr,
            max_resource_attr=max_resource_attr,
            blackbox_name=benchmark.blackbox_name,
            dataset=benchmark.dataset_name,
            surrogate=benchmark.surrogate,
            surrogate_kwargs=benchmark.surrogate_kwargs,
        )

        resource_attr = next(iter(backend.blackbox.fidelity_space.keys()))
        max_resource_level = int(max(backend.blackbox.fidelity_values))
        if max_resource_attr is not None:
            config_space = dict(
                backend.blackbox.configuration_space,
                **{max_resource_attr: max_resource_level},
            )
            method_kwargs = {"max_resource_attr": max_resource_attr}
        else:
            config_space = backend.blackbox.configuration_space
            method_kwargs = {"max_t": max_resource_level}

        scheduler = methods[method](
            MethodArguments(
                config_space=config_space,
                metric=benchmark.metric,
                mode=benchmark.mode,
                random_seed=seed,
                resource_attr=resource_attr,
                verbose=args.verbose,
                transfer_learning_evaluations=get_transfer_learning_evaluations(
                    blackbox_name=benchmark.blackbox_name,
                    test_task=benchmark.dataset_name,
                    datasets=benchmark.datasets,
                ),
                num_brackets=args.num_brackets,
                use_surrogates="lcbench" in benchmark_name,
                **method_kwargs,
            )
        )

        stop_criterion = StoppingCriterion(
            max_wallclock_time=benchmark.max_wallclock_time,
            max_num_evaluations=benchmark.max_num_evaluations,
        )
        metadata = {
            "seed": seed,
            "algorithm": method,
            "tag": experiment_tag,
            "benchmark": benchmark_name,
        }
        if args.num_brackets is not None:
            metadata["num_brackets"] = args.num_brackets
        tuner = Tuner(
            trial_backend=backend,
            scheduler=scheduler,
            stop_criterion=stop_criterion,
            n_workers=benchmark.n_workers,
            sleep_time=0,
            callbacks=[SimulatorCallback()],
            results_update_interval=600,
            print_update_interval=600,
            tuner_name=experiment_tag,
            metadata=metadata,
        )
        tuner.run()


if __name__ == "__main__":
    from benchmarking.nursery.benchmark_automl.baselines import methods
    from benchmarking.nursery.benchmark_automl.benchmark_definitions import (
        benchmark_definitions,
    )

    main(methods, benchmark_definitions)

File Path: benchmarking/nursery/benchmark_automl/benchmark_nworkers.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import itertools
import logging
from argparse import ArgumentParser
from tqdm import tqdm

from syne_tune.blackbox_repository.simulated_tabular_backend import (
    BlackboxRepositoryBackend,
)
from benchmarking.nursery.benchmark_automl.baselines import (
    MethodArguments,
    methods,
    Methods,
)
from benchmarking.nursery.benchmark_automl.benchmark_definitions import (
    benchmark_definitions,
)

from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.stopping_criterion import StoppingCriterion
from syne_tune.tuner import Tuner
from coolname import generate_slug


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag", type=str, required=False, default=generate_slug(2)
    )
    parser.add_argument("--num_seeds", type=int, required=False, default=30)
    parser.add_argument("--method", type=str, required=False)

    args, _ = parser.parse_known_args()
    experiment_tag = "nworkers-" + args.experiment_tag
    num_seeds = args.num_seeds
    # method_names = ["RS", "HB"]
    method_names = [Methods.ASHA]
    benchmark_names = ["nas201-cifar100"]

    logging.getLogger("syne_tune.optimizer.schedulers").setLevel(logging.WARNING)
    logging.getLogger("syne_tune.backend").setLevel(logging.WARNING)
    logging.getLogger("syne_tune.backend.simulator_backend.simulator_backend").setLevel(
        logging.WARNING
    )
    n_workers = [1, 2, 4, 8]
    combinations = list(
        itertools.product(method_names, range(num_seeds), benchmark_names, n_workers)
    )

    for method, seed, benchmark_name, n_workers in tqdm(combinations):
        np.random.seed(seed)
        benchmark = benchmark_definitions[benchmark_name]

        print(
            f"Starting experiment ({method}/{benchmark_name}/{seed}/{n_workers}) of {experiment_tag}"
        )

        backend = BlackboxRepositoryBackend(
            elapsed_time_attr=benchmark.elapsed_time_attr,
            time_this_resource_attr=benchmark.time_this_resource_attr,
            blackbox_name=benchmark.blackbox_name,
            dataset=benchmark.dataset_name,
        )

        max_t = max(backend.blackbox.fidelity_values)
        resource_attr = next(iter(backend.blackbox.fidelity_space.keys()))

        scheduler = methods[method](
            MethodArguments(
                config_space=backend.blackbox.configuration_space,
                metric=benchmark.metric,
                mode=benchmark.mode,
                random_seed=seed,
                max_t=max_t,
                resource_attr=resource_attr,
            )
        )

        stop_criterion = StoppingCriterion(max_wallclock_time=25000)

        tuner = Tuner(
            trial_backend=backend,
            scheduler=scheduler,
            stop_criterion=stop_criterion,
            n_workers=n_workers,
            sleep_time=0,
            callbacks=[SimulatorCallback()],
            results_update_interval=600,
            print_update_interval=600,
            tuner_name=f"{experiment_tag}-{method}-{seed}-{benchmark_name}-{n_workers}".replace(
                "_", "-"
            ),
            metadata={
                "seed": seed,
                "algorithm": f"{method} ({n_workers} workers)",
                "tag": experiment_tag,
                "benchmark": benchmark_name,
                "n_workers": n_workers,
            },
        )
        tuner.run()

File Path: benchmarking/nursery/benchmark_automl/launch_remote.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from argparse import ArgumentParser
from pathlib import Path

from coolname import generate_slug
from sagemaker.pytorch import PyTorch

from benchmarking.nursery.benchmark_automl.baselines import methods, Methods
from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role
import syne_tune
import benchmarking
from syne_tune.util import s3_experiment_path, random_string

if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag", type=str, required=False, default=generate_slug(2)
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    hash = random_string(4)

    for method in methods.keys():
        sm_args = dict(
            entry_point="benchmark_main.py",
            source_dir=str(Path(__file__).parent),
            # instance_type="local",
            checkpoint_s3_uri=s3_experiment_path(
                tuner_name=method, experiment_name=experiment_tag
            ),
            instance_type="ml.c5.4xlarge",
            instance_count=1,
            py_version="py38",
            framework_version="1.10.0",
            max_run=3600 * 72,
            role=get_execution_role(),
            dependencies=syne_tune.__path__ + benchmarking.__path__,
            disable_profiler=True,
        )

        if method != Methods.MOBSTER:
            print(f"{experiment_tag}-{method}")
            sm_args["hyperparameters"] = {
                "experiment_tag": experiment_tag,
                "num_seeds": 30,
                "method": method,
            }
            est = PyTorch(**sm_args)
            est.fit(job_name=f"{experiment_tag}-{method}-{hash}", wait=False)
        else:
            # For mobster, we schedule one job per seed as the method takes much longer
            for seed in range(30):
                print(f"{experiment_tag}-{method}-{seed}")
                sm_args["hyperparameters"] = {
                    "experiment_tag": experiment_tag,
                    "num_seeds": seed,
                    "run_all_seed": 0,
                    "method": method,
                }
                est = PyTorch(**sm_args)
                est.fit(job_name=f"{experiment_tag}-{method}-{seed}-{hash}", wait=False)

File Path: benchmarking/nursery/benchmark_automl/results_analysis/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/nursery/benchmark_automl/results_analysis/show_results.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# %%
import logging
from argparse import ArgumentParser
import matplotlib.pyplot as plt
from benchmarking.nursery.benchmark_automl.baselines import Methods

from benchmarking.nursery.benchmark_automl.results_analysis.utils import (
    method_styles,
    load_and_cache,
    plot_results,
    print_rank_table,
)

if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag",
        type=str,
        required=False,
        default="purple-akita",
        help="the experiment tag that was displayed when running the experiment",
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    logging.getLogger().setLevel(logging.INFO)

    load_cache_if_exists = True

    # benchmarks_to_df = {bench: df[] for bench, df in benchmarks_to_df.items()}
    methods_to_show = list(method_styles.keys())
    benchmarks_to_df = load_and_cache(
        load_cache_if_exists=load_cache_if_exists,
        experiment_tag=experiment_tag,
        methods_to_show=methods_to_show,
    )

    for bench, df_ in benchmarks_to_df.items():
        df_methods = df_.algorithm.unique()
        for x in methods_to_show:
            if x not in df_methods:
                logging.warning(f"method {x} not found in {bench}")

    for benchmark in ["fcnet", "nas201"]:
        n = 0
        for key, df in benchmarks_to_df.items():
            if benchmark in key:
                n += len(df[df.algorithm == Methods.RS])
        print(f"number of hyperband evaluations for {benchmark}: {n}")

    methods_to_show = [
        Methods.RS,
        Methods.TPE,
        Methods.REA,
        # Methods.BORE,
        Methods.GP,
        Methods.MSR,
        Methods.ASHA,
        Methods.BOHB,
        Methods.MOBSTER,
        Methods.RUSH,
        Methods.ASHA_BB,
        Methods.ZERO_SHOT,
        Methods.ASHA_CTS,
    ]
    print_rank_table(benchmarks_to_df, methods_to_show)

    params = {
        "legend.fontsize": 18,
        "axes.labelsize": 22,
        "xtick.labelsize": 18,
        "ytick.labelsize": 18,
    }
    plt.rcParams.update(params)

    plot_results(benchmarks_to_df, method_styles, methods_to_show=methods_to_show)

File Path: benchmarking/nursery/benchmark_automl/results_analysis/show_results_workers.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from argparse import ArgumentParser
from pathlib import Path
from matplotlib import cm

from benchmarking.nursery.benchmark_automl.baselines import Methods
from benchmarking.nursery.benchmark_automl.results_analysis.utils import (
    MethodStyle,
    load_and_cache,
    plot_results,
)

show_seeds = False


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag",
        type=str,
        required=False,
        default="nworkers-rebel-ibex",
        help="the experiment tag that was displayed when running the experiment",
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    logging.getLogger().setLevel(logging.INFO)

    cmap = cm.get_cmap("viridis")
    method_styles = {
        f"{Methods.ASHA} (1 workers)": MethodStyle(cmap(0), "-"),
        f"{Methods.ASHA} (2 workers)": MethodStyle(cmap(0.25), "-"),
        f"{Methods.ASHA} (4 workers)": MethodStyle(cmap(0.5), "-"),
        f"{Methods.ASHA} (8 workers)": MethodStyle(cmap(1.0), "-"),
    }

    load_cache_if_exists = True
    methods_to_show = list(method_styles.keys())

    result_file = Path(f"~/Downloads/cached-results-{experiment_tag}.dill").expanduser()

    methods_to_show = list(method_styles.keys())
    benchmarks_to_df = load_and_cache(
        load_cache_if_exists=load_cache_if_exists,
        experiment_tag=experiment_tag,
        methods_to_show=methods_to_show,
    )

    for bench, df_ in benchmarks_to_df.items():
        df_methods = df_.algorithm.unique()
        for x in methods_to_show:
            if x not in df_methods:
                logging.warning(f"method {x} not found in {bench}")

    # benchmarks_to_df = {bench: df[] for bench, df in benchmarks_to_df.items()}

    plot_results(
        benchmarks_to_df,
        method_styles,
        prefix="number-workers-",
        title="Impact of parallelism on wallclock time",
    )

File Path: benchmarking/nursery/benchmark_automl/results_analysis/utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import os

import dill
from tqdm import tqdm

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Union
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

from benchmarking.nursery.benchmark_automl.baselines import Methods
from syne_tune.constants import ST_TUNER_TIME
from syne_tune.experiments import get_metadata, load_experiments_df

from syne_tune.util import catchtime

rs_color = "blue"
gp_color = "orange"
tpe_color = "red"
bore_color = "violet"
rea_color = "brown"
hb_bb_color = "green"
hb_ts_color = "yellow"
zs_color = "paleturquoise"
fifo_style = "solid"
multifidelity_style = "dashed"
multifidelity_style2 = "dashdot"
transfer_style = "dotted"


@dataclass
class MethodStyle:
    color: str
    linestyle: str
    marker: str = None


show_seeds = False
method_styles = {
    Methods.RS: MethodStyle(rs_color, fifo_style),
    Methods.TPE: MethodStyle(tpe_color, fifo_style),
    Methods.BORE: MethodStyle(bore_color, fifo_style),
    Methods.GP: MethodStyle(gp_color, fifo_style),
    Methods.REA: MethodStyle(rea_color, fifo_style),
    Methods.ASHA: MethodStyle(rs_color, multifidelity_style),
    Methods.MSR: MethodStyle(rs_color, multifidelity_style2),
    Methods.BOHB: MethodStyle(tpe_color, multifidelity_style),
    Methods.MOBSTER: MethodStyle(gp_color, multifidelity_style),
    # transfer learning
    Methods.ASHA_BB: MethodStyle(hb_bb_color, multifidelity_style, "."),
    Methods.ASHA_CTS: MethodStyle(hb_ts_color, multifidelity_style, "."),
    Methods.ZERO_SHOT: MethodStyle(zs_color, fifo_style, "."),
    Methods.RUSH: MethodStyle(rs_color, multifidelity_style, "."),
}


@dataclass
class PlotArgs:
    xmin: float = None
    xmax: float = None
    ymin: float = None
    ymax: float = None


plot_range = {
    "fcnet-naval": PlotArgs(50, 1200, 0.0, 4e-3),
    "fcnet-parkinsons": PlotArgs(0, 1200, 0.0, 0.1),
    "fcnet-protein": PlotArgs(xmin=0, xmax=1200, ymin=0.225, ymax=0.35),
    "fcnet-slice": PlotArgs(50, 1200, 0.0, 0.004),
    "nas201-ImageNet16-120": PlotArgs(1000, 21000, None, 0.8),
    "nas201-cifar10": PlotArgs(2000, 21000, 0.05, 0.15),
    "nas201-cifar100": PlotArgs(3000, 21000, 0.26, 0.35),
    "lcbench-bank-marketing": PlotArgs(0, 2000, 82, 89),
    "lcbench-KDDCup09-appetency": PlotArgs(0, 2000, 96, 100),
}


def generate_df_dict(
    tag=None, date_min=None, date_max=None, methods_to_show=None
) -> Dict[str, pd.DataFrame]:
    # todo load one df per task would be more efficient
    def metadata_filter(metadata, benchmark=None, tag=None):
        if methods_to_show is not None and not metadata["algorithm"] in methods_to_show:
            return False
        if benchmark is not None and metadata["benchmark"] != benchmark:
            return False
        if tag is not None:
            if not isinstance(tag, list):
                tag = [tag]
            if not metadata["tag"] in tag:
                return False
        if date_min is None or date_max is None:
            return True
        else:
            date_exp = datetime.fromtimestamp(metadata["st_tuner_creation_timestamp"])
            return date_min <= date_exp <= date_max

    metadatas = get_metadata()
    if tag is not None:
        if not isinstance(tag, list):
            tag = [tag]
        metadatas = {k: v for k, v in metadatas.items() if v.get("tag") in tag}
    # only select metadatas that contain the fields we are interested in
    metadatas = {
        k: v
        for k, v in metadatas.items()
        if all(
            key in v
            for key in ["algorithm", "benchmark", "tag", "st_tuner_creation_timestamp"]
        )
    }
    metadata_df = pd.DataFrame(metadatas.values())
    metadata_df["creation_date"] = metadata_df["st_tuner_creation_timestamp"].apply(
        lambda x: datetime.fromtimestamp(x)
    )
    metadata_df.sort_values(by="creation_date", ascending=False)
    metadata_df = metadata_df.drop_duplicates(["algorithm", "benchmark", "seed"])
    creation_dates_min_max = metadata_df.groupby(["algorithm"]).agg(["min", "max"])[
        "creation_date"
    ]
    print("creation date per method:\n" + creation_dates_min_max.to_string())

    count_per_seed = (
        metadata_df.groupby(["algorithm", "benchmark", "seed"]).count()["tag"].unstack()
    )
    print("num seeds per methods: \n" + count_per_seed.to_string())

    num_seed_per_method = (
        metadata_df.groupby(["algorithm", "benchmark"]).count()["tag"].unstack()
    )
    print("seeds present: \n" + num_seed_per_method.to_string())

    benchmarks = list(sorted(metadata_df.benchmark.dropna().unique()))

    benchmark_to_df = {}

    for benchmark in tqdm(benchmarks):
        valid_exps = set(
            [
                name
                for name, metadata in metadatas.items()
                if metadata_filter(metadata, benchmark, tag)
            ]
        )
        if len(valid_exps) > 0:

            def name_filter(path):
                tuner_name = Path(path).parent.stem
                return tuner_name in valid_exps

            df = load_experiments_df(name_filter)
            benchmark_to_df[benchmark] = df

    return benchmark_to_df


def plot_result_benchmark(
    df_task,
    title: str,
    show_seeds: bool = False,
    method_styles: Optional[Dict] = None,
    ax=None,
    methods_to_show: list = None,
):
    agg_results = {}
    if len(df_task) > 0:
        metric = df_task.loc[:, "metric_names"].values[0]
        mode = df_task.loc[:, "metric_mode"].values[0]

        if ax is None:
            fig, ax = plt.subplots()
        for algorithm, method_style in method_styles.items():
            if methods_to_show is not None and algorithm not in methods_to_show:
                continue
            ts = []
            ys = []

            df_scheduler = df_task[df_task.algorithm == algorithm]
            if len(df_scheduler) == 0:
                continue
            for i, tuner_name in enumerate(df_scheduler.tuner_name.unique()):
                sub_df = df_scheduler[df_scheduler.tuner_name == tuner_name]
                sub_df = sub_df.sort_values(ST_TUNER_TIME)
                t = sub_df.loc[:, ST_TUNER_TIME].values
                y_best = (
                    sub_df.loc[:, metric].cummax().values
                    if mode == "max"
                    else sub_df.loc[:, metric].cummin().values
                )
                if show_seeds:
                    ax.plot(
                        t,
                        y_best,
                        color=method_style.color,
                        linestyle=method_style.linestyle,
                        marker=method_style.marker,
                        alpha=0.2,
                    )
                ts.append(t)
                ys.append(y_best)

            # compute the mean/std over time-series of different seeds at regular time-steps
            # start/stop at respectively first/last point available for all seeds
            t_min = max(tt[0] for tt in ts)
            t_max = min(tt[-1] for tt in ts)
            if t_min > t_max:
                continue
            t_range = np.linspace(t_min, t_max)

            # find the best value at each regularly spaced time-step from t_range
            y_ranges = []
            for t, y in zip(ts, ys):
                indices = np.searchsorted(t, t_range, side="left")
                y_range = y[indices]
                y_ranges.append(y_range)
            y_ranges = np.stack(y_ranges)

            mean = y_ranges.mean(axis=0)
            std = y_ranges.std(axis=0)
            ax.fill_between(
                t_range,
                mean - std,
                mean + std,
                color=method_style.color,
                alpha=0.1,
            )
            ax.plot(
                t_range,
                mean,
                color=method_style.color,
                linestyle=method_style.linestyle,
                marker=method_style.marker,
                label=algorithm,
            )
            agg_results[algorithm] = mean

        ax.set_xlabel("wall clock time")
        y_label = None
        if metric == "metric_valid_loss":
            y_label = "mean squared error"
        elif metric == "val_accuracy":
            y_label = "classification accuracy"
        elif metric == "metric_valid_error":
            y_label = "classification error"
        ax.set_ylabel(y_label)
        ax.legend()
        # ax.set_title(title)
    return ax, t_range, agg_results


def plot_results(
    benchmarks_to_df,
    method_styles: Optional[Dict] = None,
    prefix: str = "",
    title: str = None,
    ax=None,
    methods_to_show: list = None,
):
    agg_results = {}

    for benchmark, df_task in benchmarks_to_df.items():
        ax, t_range, agg_result = plot_result_benchmark(
            df_task=df_task,
            title=benchmark,
            method_styles=method_styles,
            show_seeds=show_seeds,
            ax=ax,
            methods_to_show=methods_to_show,
        )
        if title is not None:
            ax.set_title(title)
        agg_results[benchmark] = agg_result
        if benchmark in plot_range:
            plotargs = plot_range[benchmark]
            ax.set_ylim([plotargs.ymin, plotargs.ymax])
            ax.set_xlim([plotargs.xmin, plotargs.xmax])

        if ax is not None:
            plt.tight_layout()
            os.makedirs("figures/", exist_ok=True)
            plt.savefig(f"figures/{prefix}{benchmark}.pdf")
        ax = None


def compute_best_value_over_time(benchmarks_to_df, methods_to_show):
    def get_results(df_task, methods_to_show):
        seed_results = {}
        if len(df_task) > 0:
            metric = df_task.loc[:, "metric_names"].values[0]
            mode = df_task.loc[:, "metric_mode"].values[0]

            t_max = df_task.loc[:, ST_TUNER_TIME].max()
            t_range = np.linspace(0, t_max, 10)

            for algorithm in methods_to_show:
                ts = []
                ys = []

                df_scheduler = df_task[df_task.algorithm == algorithm]
                for i, tuner_name in enumerate(df_scheduler.tuner_name.unique()):
                    sub_df = df_scheduler[df_scheduler.tuner_name == tuner_name]
                    sub_df = sub_df.sort_values(ST_TUNER_TIME)
                    t = sub_df.loc[:, ST_TUNER_TIME].values
                    y_best = (
                        sub_df.loc[:, metric].cummax().values
                        if mode == "max"
                        else sub_df.loc[:, metric].cummin().values
                    )
                    ts.append(t)
                    ys.append(y_best)

                # for each seed, find the best value at each regularly spaced time-step
                y_ranges = []
                for t, y in zip(ts, ys):
                    indices = np.searchsorted(t, t_range, side="left")
                    y_range = y[np.clip(indices, 0, len(y) - 1)]
                    y_ranges.append(y_range)

                # (num_seeds, num_time_steps)
                y_ranges = np.stack(y_ranges)

                seed_results[algorithm] = y_ranges

        # seed_results shape (num_seeds, num_time_steps)
        return t_range, seed_results

    benchmark_results = []
    for benchmark, df_task in tqdm(list(benchmarks_to_df.items())):
        # (num_seeds, num_time_steps)
        _, seed_results_dict = get_results(df_task, methods_to_show)

        shapes = [x.shape for x in seed_results_dict.values()]

        # take the minimum number of seeds in case some are missing
        min_num_seeds = min(num_seed for num_seed, num_time_steps in shapes)

        # (num_methods, num_min_seeds, num_time_steps)
        seed_results = np.stack([x[:min_num_seeds] for x in seed_results_dict.values()])

        benchmark_results.append(seed_results)

    # take the minimum number of seeds in case some are missing
    min_num_seeds = min([x.shape[1] for x in benchmark_results])
    benchmark_results = np.stack([b[:, :min_num_seeds, :] for b in benchmark_results])

    # (num_benchmarks, num_methods, num_min_seeds, num_time_steps)
    return methods_to_show, np.stack(benchmark_results)


def print_rank_table(benchmarks_to_df, methods_to_show: Optional[List[str]]):
    from sklearn.preprocessing import QuantileTransformer
    from benchmarking.nursery.benchmark_automl.results_analysis.utils import (
        compute_best_value_over_time,
    )
    import pandas as pd

    benchmarks = ["fcnet", "nas201", "lcbench"]

    rows = []
    for benchmark in benchmarks:
        benchmark_to_df = {k: v for k, v in benchmarks_to_df.items() if benchmark in k}
        if len(benchmark_to_df) == 0:
            print(f"did not find evaluations for {benchmark}")
            continue
        methods_present = next(iter(benchmark_to_df.values())).algorithm.unique()
        methods_to_show = [x for x in methods_to_show if x in methods_present]

        # (num_benchmarks, num_methods, num_min_seeds, num_time_steps)
        methods_to_show, benchmark_results = compute_best_value_over_time(
            benchmark_to_df, methods_to_show
        )

        for i, task in enumerate(benchmark_to_df.keys()):
            if "lcbench" in task:
                print(task)
                # lcbench do maximization instead of minimization, we should pass the mode instead of hardcoding this
                benchmark_results *= -1

        # (num_methods, num_benchmarks, num_min_seeds, num_time_steps)
        benchmark_results = benchmark_results.swapaxes(0, 1)

        # (num_methods, num_benchmarks * num_min_seeds * num_time_steps)
        ranks = QuantileTransformer().fit_transform(
            benchmark_results.reshape(len(benchmark_results), -1)
        )
        # ranks_std = ranks.std(axis=-1).mean(axis=0)
        row = {"benchmark": benchmark}
        row.update(dict(zip(methods_to_show, ranks.mean(axis=-1))))
        rows.append(row)
        print(row)
    df_ranks = pd.DataFrame(rows).set_index("benchmark")
    avg_row = dict(df_ranks.mean())
    avg_row["benchmark"] = "Average"
    df_ranks = pd.DataFrame(rows + [avg_row]).set_index("benchmark")
    benchmark_names = {
        "fcnet": "\\FCNet{}",
        "nas201": "\\NASBench{}",
        "lcbench": "\\LCBench{}",
    }
    df_ranks.index = df_ranks.index.map(lambda s: benchmark_names.get(s, s))
    df_ranks.columns = df_ranks.columns.map(lambda s: "\\" + s.replace("-", "") + "{}")
    print(df_ranks.to_string())
    print(df_ranks.to_latex(float_format="%.2f", na_rep="-", escape=False))


def load_and_cache(
    experiment_tag: Union[str, List[str]],
    load_cache_if_exists: bool = True,
    methods_to_show=None,
):

    result_file = Path(
        f"~/Downloads/cached-results-{str(experiment_tag)}.dill"
    ).expanduser()
    if load_cache_if_exists and result_file.exists():
        with catchtime(f"loading results from {result_file}"):
            with open(result_file, "rb") as f:
                benchmarks_to_df = dill.load(f)
    else:
        print(f"regenerating results to {result_file}")
        benchmarks_to_df = generate_df_dict(
            experiment_tag,
            date_min=None,
            date_max=None,
            methods_to_show=methods_to_show,
        )
        # metrics = df.metric_names
        with open(result_file, "wb") as f:
            dill.dump(benchmarks_to_df, f)

    return benchmarks_to_df

File Path: benchmarking/nursery/benchmark_dehb/baselines.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from benchmarking.nursery.benchmark_automl.baselines import (
    MethodArguments,
    search_options,
)
from syne_tune.blackbox_repository.simulated_tabular_backend import (
    BlackboxRepositoryBackend,
)
from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.synchronous import (
    SynchronousGeometricHyperbandScheduler,
    GeometricDifferentialEvolutionHyperbandScheduler,
)
from syne_tune.config_space import (
    Categorical,
    ordinal,
)


class Methods:
    ASHA = "ASHA"
    SYNCHB = "SYNCHB"
    DEHB = "DEHB"
    BOHB = "BOHB"
    ASHA_ORD = "ASHA-ORD"
    SYNCHB_ORD = "SYNCHB-ORD"
    DEHB_ORD = "DEHB-ORD"
    BOHB_ORD = "BOHB-ORD"
    ASHA_STOP = "ASHA-STOP"
    SYNCMOBSTER = "SYNCMOBSTER"


def _convert_categorical_to_ordinal(args: MethodArguments) -> dict:
    return {
        name: (
            ordinal(domain.categories) if isinstance(domain, Categorical) else domain
        )
        for name, domain in args.config_space.items()
    }


methods = {
    Methods.ASHA: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        type="promotion",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.SYNCHB: lambda method_arguments: SynchronousGeometricHyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.DEHB: lambda method_arguments: GeometricDifferentialEvolutionHyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="random_encoded",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.BOHB: lambda method_arguments: SynchronousGeometricHyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="kde",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.ASHA_ORD: lambda method_arguments: HyperbandScheduler(
        config_space=_convert_categorical_to_ordinal(method_arguments),
        searcher="random",
        type="promotion",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.SYNCHB_ORD: lambda method_arguments: SynchronousGeometricHyperbandScheduler(
        config_space=_convert_categorical_to_ordinal(method_arguments),
        searcher="random",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.DEHB_ORD: lambda method_arguments: GeometricDifferentialEvolutionHyperbandScheduler(
        config_space=_convert_categorical_to_ordinal(method_arguments),
        searcher="random_encoded",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.BOHB_ORD: lambda method_arguments: SynchronousGeometricHyperbandScheduler(
        config_space=_convert_categorical_to_ordinal(method_arguments),
        searcher="kde",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.ASHA_STOP: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        type="stopping",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
    Methods.SYNCMOBSTER: lambda method_arguments: SynchronousGeometricHyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="bayesopt",
        search_options=search_options(method_arguments),
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_resource_attr=method_arguments.max_resource_attr,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
        brackets=method_arguments.num_brackets,
    ),
}


if __name__ == "__main__":
    # Run a loop that initializes all schedulers on all benchmark to see if they all work
    from benchmarking.nursery.benchmark_automl.benchmark_definitions import (
        benchmark_definitions,
    )

    benchmarks = ["fcnet-protein", "nas201-cifar10", "lcbench-Fashion-MNIST"]
    for benchmark_name in benchmarks:
        benchmark = benchmark_definitions[benchmark_name]
        backend = BlackboxRepositoryBackend(
            elapsed_time_attr=benchmark.elapsed_time_attr,
            time_this_resource_attr=benchmark.time_this_resource_attr,
            blackbox_name=benchmark.blackbox_name,
            dataset=benchmark.dataset_name,
        )
        for method_name, method_fun in methods.items():
            print(f"checking initialization of: {method_name}, {benchmark_name}")
            scheduler = method_fun(
                MethodArguments(
                    config_space=backend.blackbox.configuration_space,
                    metric=benchmark.metric,
                    mode=benchmark.mode,
                    random_seed=0,
                    max_resource_attr=benchmark.max_resource_attr,
                    resource_attr=next(iter(backend.blackbox.fidelity_space.keys())),
                )
            )
            scheduler.suggest(0)
            scheduler.suggest(1)

File Path: benchmarking/nursery/benchmark_dehb/benchmark_definitions.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from benchmarking.nursery.benchmark_automl.benchmark_definitions import (
    BenchmarkDefinition,
    fcnet_benchmark,
    lcbench_benchmark,
)


NAS201_MAX_WALLCLOCK_TIME = {
    "cifar10": 5 * 3600,
    "cifar100": 6 * 3600,
    "ImageNet16-120": 8 * 3600,
}


NAS201_N_WORKERS = {
    "cifar10": 4,
    "cifar100": 4,
    "ImageNet16-120": 8,
}


def nas201_benchmark(dataset_name):
    return BenchmarkDefinition(
        max_wallclock_time=NAS201_MAX_WALLCLOCK_TIME[dataset_name],
        n_workers=NAS201_N_WORKERS[dataset_name],
        elapsed_time_attr="metric_elapsed_time",
        metric="metric_valid_error",
        mode="min",
        blackbox_name="nasbench201",
        dataset_name=dataset_name,
        max_resource_attr="epochs",
    )


benchmark_definitions = {
    "fcnet-protein": fcnet_benchmark("protein_structure"),
    "fcnet-naval": fcnet_benchmark("naval_propulsion"),
    "fcnet-parkinsons": fcnet_benchmark("parkinsons_telemonitoring"),
    "fcnet-slice": fcnet_benchmark("slice_localization"),
    "nas201-cifar10": nas201_benchmark("cifar10"),
    "nas201-cifar100": nas201_benchmark("cifar100"),
    "nas201-ImageNet16-120": nas201_benchmark("ImageNet16-120"),
}

# 5 most expensive lcbench datasets
lc_bench_datasets = [
    "Fashion-MNIST",
    "airlines",
    "albert",
    "covertype",
    "christine",
]
for task in lc_bench_datasets:
    benchmark_definitions[
        "lcbench-" + task.replace("_", "-").replace(".", "")
    ] = lcbench_benchmark(task, datasets=lc_bench_datasets)

File Path: benchmarking/nursery/benchmark_dehb/benchmark_main.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from benchmarking.nursery.benchmark_automl.benchmark_main import main


if __name__ == "__main__":
    from benchmarking.nursery.benchmark_dehb.baselines import methods
    from benchmarking.nursery.benchmark_dehb.benchmark_definitions import (
        benchmark_definitions,
    )

    main(methods, benchmark_definitions)

File Path: benchmarking/nursery/benchmark_dehb/launch_remote.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
from tqdm import tqdm

from sagemaker.pytorch import PyTorch

from benchmarking.nursery.benchmark_automl.benchmark_main import parse_args
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
)
import syne_tune
import benchmarking
from syne_tune.util import s3_experiment_path, random_string


if __name__ == "__main__":
    from benchmarking.nursery.benchmark_dehb.baselines import methods
    from benchmarking.nursery.benchmark_dehb.benchmark_definitions import (
        benchmark_definitions,
    )

    args, method_names, benchmark_names, _ = parse_args(
        methods,
        benchmark_definitions,
    )
    if len(benchmark_names) == 1:
        benchmark_name = benchmark_names[0]
    else:
        benchmark_name = None
    experiment_tag = args.experiment_tag
    suffix = random_string(4)

    for method in tqdm(method_names):
        checkpoint_s3_uri = s3_experiment_path(
            tuner_name=method, experiment_name=experiment_tag
        )
        sm_args = dict(
            entry_point="benchmark_main.py",
            source_dir=str(Path(__file__).parent),
            checkpoint_s3_uri=checkpoint_s3_uri,
            instance_type="ml.c5.4xlarge",
            instance_count=1,
            py_version="py38",
            framework_version="1.10.0",
            max_run=3600 * 72,
            role=get_execution_role(),
            dependencies=syne_tune.__path__ + benchmarking.__path__,
            disable_profiler=True,
        )

        hyperparameters = {
            "experiment_tag": experiment_tag,
            "num_seeds": args.num_seeds,
            "method": method,
        }
        if benchmark_name is not None:
            hyperparameters["benchmark"] = benchmark_name
        if args.num_brackets is not None:
            hyperparameters["num_brackets"] = args.num_brackets
        if args.start_seed is not None:
            hyperparameters["start_seed"] = args.start_seed
        print(
            f"{experiment_tag}-{method}\n"
            f"hyperparameters = {hyperparameters}\n"
            f"Results written to {checkpoint_s3_uri}"
        )
        sm_args["hyperparameters"] = hyperparameters
        est = PyTorch(**sm_args)
        est.fit(job_name=f"{experiment_tag}-{method}-{suffix}", wait=False)

    print(
        "\nLaunched all requested experiments. Once everything is done, use this "
        "command to sync result files from S3:\n"
        f"$ aws s3 sync {s3_experiment_path(experiment_name=experiment_tag)}/ "
        f'~/syne-tune/{experiment_tag}/ --exclude "*" '
        '--include "*metadata.json" --include "*results.csv.zip"'
    )

File Path: benchmarking/nursery/benchmark_neuralband/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/nursery/benchmark_neuralband/baselines.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from typing import Dict, Optional

from syne_tune.blackbox_repository.simulated_tabular_backend import (
    BlackboxRepositoryBackend,
)
from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.schedulers.neuralbands.neuralband import NeuralbandScheduler
from syne_tune.optimizer.schedulers.neuralbands.neuralband_supplement import (
    NeuralbandUCBScheduler,
    NeuralbandTSScheduler,
    NeuralbandEGreedyScheduler,
)


@dataclass
class MethodArguments:
    config_space: Dict
    metric: str
    mode: str
    random_seed: int
    max_t: int
    resource_attr: str
    transfer_learning_evaluations: Optional[Dict] = None
    use_surrogates: bool = False


class Methods:
    RS = "RS"
    ASHA = "ASHA"
    HP = "HP"
    GP = "GP"
    BOHB = "BOHB"
    MOBSTER = "MOB"
    TPE = "TPE"
    NeuralBandSH = "NeuralBandSH"
    NeuralBandHB = "NeuralBandHB"
    NeuralBand_UCB = "NeuralBandUCB"
    NeuralBand_TS = "NeuralBandTS"
    NeuralBandEpsilon = "NeuralBandEpsilon"


methods = {
    Methods.RS: lambda method_arguments: FIFOScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.ASHA: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.HP: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="random",
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        brackets=3,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.BOHB: lambda method_arguments: HyperbandScheduler(
        config_space=method_arguments.config_space,
        searcher="kde",
        search_options={"debug_log": False, "min_bandwidth": 0.1},
        mode=method_arguments.mode,
        brackets=3,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.TPE: lambda method_arguments: FIFOScheduler(
        config_space=method_arguments.config_space,
        searcher="kde",
        search_options={"debug_log": False, "min_bandwidth": 0.1},
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.GP: lambda method_arguments: FIFOScheduler(
        method_arguments.config_space,
        searcher="bayesopt",
        search_options={"debug_log": False},
        metric=method_arguments.metric,
        mode=method_arguments.mode,
        random_seed=method_arguments.random_seed,
    ),
    Methods.MOBSTER: lambda method_arguments: HyperbandScheduler(
        method_arguments.config_space,
        searcher="bayesopt",
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.NeuralBandSH: lambda method_arguments: NeuralbandScheduler(
        gamma=0.05,
        nu=0.02,
        max_while_loop=50,
        step_size=5,
        brackets=1,
        config_space=method_arguments.config_space,
        searcher="random",
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.NeuralBandHB: lambda method_arguments: NeuralbandScheduler(
        gamma=0.04,
        nu=0.02,
        max_while_loop=50,
        step_size=5,
        brackets=3,
        config_space=method_arguments.config_space,
        searcher="random",
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.NeuralBand_UCB: lambda method_arguments: NeuralbandUCBScheduler(
        lamdba=0.1,
        nu=0.001,
        max_while_loop=50,
        step_size=5,
        config_space=method_arguments.config_space,
        searcher="random",
        brackets=3,
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.NeuralBand_TS: lambda method_arguments: NeuralbandTSScheduler(
        lamdba=0.1,
        nu=0.001,
        max_while_loop=50,
        step_size=5,
        config_space=method_arguments.config_space,
        searcher="random",
        brackets=3,
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
    Methods.NeuralBandEpsilon: lambda method_arguments: NeuralbandEGreedyScheduler(
        epsilon=0.1,
        max_while_loop=1000,
        step_size=5,
        config_space=method_arguments.config_space,
        searcher="random",
        brackets=3,
        search_options={"debug_log": False},
        mode=method_arguments.mode,
        metric=method_arguments.metric,
        max_t=method_arguments.max_t,
        resource_attr=method_arguments.resource_attr,
        random_seed=method_arguments.random_seed,
    ),
}


if __name__ == "__main__":
    # Run a loop that initializes all schedulers on all benchmark to see if they all work
    from benchmarking.nursery.benchmark_automl.benchmark_main import (
        get_transfer_learning_evaluations,
    )
    from benchmarking.nursery.benchmark_automl.benchmark_definitions import (
        benchmark_definitions,
    )

    benchmarks = ["fcnet-protein", "nas201-cifar10", "lcbench-Fashion-MNIST"]
    for benchmark_name in benchmarks:
        benchmark = benchmark_definitions[benchmark_name]
        backend = BlackboxRepositoryBackend(
            elapsed_time_attr=benchmark.elapsed_time_attr,
            time_this_resource_attr=benchmark.time_this_resource_attr,
            blackbox_name=benchmark.blackbox_name,
            dataset=benchmark.dataset_name,
        )
        for method_name, method_fun in methods.items():
            print(f"checking initialization of: {method_name}, {benchmark_name}")
            scheduler = method_fun(
                MethodArguments(
                    config_space=backend.blackbox.configuration_space,
                    metric=benchmark.metric,
                    mode=benchmark.mode,
                    random_seed=0,
                    max_t=max(backend.blackbox.fidelity_values),
                    resource_attr=next(iter(backend.blackbox.fidelity_space.keys())),
                    transfer_learning_evaluations=get_transfer_learning_evaluations(
                        blackbox_name=benchmark.blackbox_name,
                        test_task=benchmark.dataset_name,
                        datasets=benchmark.datasets,
                    ),
                    use_surrogates=benchmark_name == "lcbench-Fashion-MNIST",
                )
            )
            scheduler.suggest(0)
            scheduler.suggest(1)

File Path: benchmarking/nursery/benchmark_neuralband/benchmark_main.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, List
import numpy as np
import itertools
import logging
from argparse import ArgumentParser
from tqdm import tqdm

from syne_tune.blackbox_repository import load
from syne_tune.blackbox_repository.simulated_tabular_backend import (
    BlackboxRepositoryBackend,
)
from benchmarking.nursery.benchmark_neuralband.baselines import MethodArguments, methods
from benchmarking.nursery.benchmark_automl.benchmark_definitions import (
    benchmark_definitions,
)

from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
)
from syne_tune.stopping_criterion import StoppingCriterion
from syne_tune.tuner import Tuner
from coolname import generate_slug


def get_transfer_learning_evaluations(
    blackbox_name: str,
    test_task: str,
    datasets: Optional[List[str]],
    n_evals: Optional[int] = None,
) -> Dict:
    """
    :param blackbox_name:
    :param test_task: task where the performance would be tested, it is excluded from transfer-learning evaluations
    :param datasets: subset of datasets to consider, only evaluations from those datasets are provided to
    transfer-learning methods. If none, all datasets are used.
    :param n_evals: maximum number of evaluations to be returned
    :return:
    """
    task_to_evaluations = load(blackbox_name)

    # todo retrieve right metric
    metric_index = 0
    transfer_learning_evaluations = {
        task: TransferLearningTaskEvaluations(
            configuration_space=bb.configuration_space,
            hyperparameters=bb.hyperparameters,
            objectives_evaluations=bb.objectives_evaluations[
                ..., metric_index : metric_index + 1
            ],
            objectives_names=[bb.objectives_names[metric_index]],
        )
        for task, bb in task_to_evaluations.items()
        if task != test_task and (datasets is None or task in datasets)
    }

    if n_evals is not None:
        # subsample n_evals / n_tasks of observations on each tasks
        def subsample(
            transfer_evaluations: TransferLearningTaskEvaluations, n: int
        ) -> TransferLearningTaskEvaluations:
            random_indices = np.random.permutation(
                len(transfer_evaluations.hyperparameters)
            )[:n]
            return TransferLearningTaskEvaluations(
                configuration_space=transfer_evaluations.configuration_space,
                hyperparameters=transfer_evaluations.hyperparameters.loc[
                    random_indices
                ].reset_index(drop=True),
                objectives_evaluations=transfer_evaluations.objectives_evaluations[
                    random_indices
                ],
                objectives_names=transfer_evaluations.objectives_names,
            )

        n = n_evals // len(transfer_learning_evaluations)
        transfer_learning_evaluations = {
            task: subsample(transfer_evaluations, n)
            for task, transfer_evaluations in transfer_learning_evaluations.items()
        }

    return transfer_learning_evaluations


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag", type=str, required=False, default=generate_slug(2)
    )
    parser.add_argument(
        "--num_seeds",
        type=int,
        required=False,
        default=1,
        help="number of seeds to run",
    )
    parser.add_argument(
        "--run_all_seed",
        type=int,
        default=1,
        help="if 1 run only `seed=num_seeds`, otherwise runs all the seeds [0, `num_seeds`-1]",
    )
    parser.add_argument(
        "--method", type=str, required=False, help="a method to run from baselines.py"
    )
    parser.add_argument(
        "--benchmark",
        type=str,
        required=False,
        help="a benchmark to run from benchmark_definitions.py",
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag

    if args.run_all_seed == 1:
        seeds = list(range(args.num_seeds))
    else:
        seeds = [args.num_seeds]

    method_names = [args.method] if args.method is not None else list(methods.keys())
    benchmark_names = (
        [args.benchmark]
        if args.benchmark is not None
        else list(benchmark_definitions.keys())
    )
    logging.getLogger("syne_tune.optimizer.schedulers").setLevel(logging.WARNING)
    logging.getLogger("syne_tune.backend").setLevel(logging.WARNING)
    logging.getLogger("syne_tune.backend.simulator_backend.simulator_backend").setLevel(
        logging.WARNING
    )

    combinations = list(itertools.product(method_names, seeds, benchmark_names))
    for method, seed, benchmark_name in tqdm(combinations):
        np.random.seed(seed)
        benchmark = benchmark_definitions[benchmark_name]

        print(
            f"Starting experiment ({method}/{benchmark_name}/{seed}) of {experiment_tag}"
        )

        backend = BlackboxRepositoryBackend(
            elapsed_time_attr=benchmark.elapsed_time_attr,
            time_this_resource_attr=benchmark.time_this_resource_attr,
            blackbox_name=benchmark.blackbox_name,
            dataset=benchmark.dataset_name,
            surrogate=benchmark.surrogate,
        )

        # todo move into benchmark definition
        max_t = max(backend.blackbox.fidelity_values)
        resource_attr = next(iter(backend.blackbox.fidelity_space.keys()))

        scheduler = methods[method](
            MethodArguments(
                config_space=backend.blackbox.configuration_space,
                metric=benchmark.metric,
                mode=benchmark.mode,
                random_seed=seed,
                max_t=max_t,
                resource_attr=resource_attr,
                transfer_learning_evaluations=get_transfer_learning_evaluations(
                    blackbox_name=benchmark.blackbox_name,
                    test_task=benchmark.dataset_name,
                    datasets=benchmark.datasets,
                ),
                use_surrogates="lcbench" in benchmark_name,
            )
        )
        stop_criterion = StoppingCriterion(
            max_wallclock_time=benchmark.max_wallclock_time,
        )
        tuner = Tuner(
            trial_backend=backend,
            scheduler=scheduler,
            stop_criterion=stop_criterion,
            n_workers=benchmark.n_workers,
            sleep_time=0,
            callbacks=[SimulatorCallback()],
            results_update_interval=600,
            print_update_interval=600,
            tuner_name=f"{experiment_tag}-{method}-{seed}-{benchmark_name}".replace(
                "_", "-"
            ),
            metadata={
                "seed": seed,
                "algorithm": method,
                "tag": experiment_tag,
                "benchmark": benchmark_name,
            },
        )
        tuner.run()

File Path: benchmarking/nursery/benchmark_neuralband/launch_remote.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from argparse import ArgumentParser
from pathlib import Path

from coolname import generate_slug
from sagemaker.pytorch import PyTorch

from benchmarking.nursery.benchmark_neuralband.baselines import methods, Methods
from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role
import syne_tune
import benchmarking
from syne_tune.util import s3_experiment_path, random_string


benchmark_names = [
    "fcnet-protein",
    "fcnet-naval",
    "fcnet-parkinsons",
    "fcnet-slice",
    "nas201-cifar10",
    "nas201-cifar100",
    "nas201-ImageNet16-120",
    "lcbench-APSFailure",
    "lcbench-Amazon-employee-access",
    "lcbench-Australian",
    "lcbench-Fashion-MNIST",
    "lcbench-KDDCup09-appetency",
    "lcbench-MiniBooNE",
    "lcbench-adult",
    "lcbench-airlines",
    "lcbench-albert",
    "lcbench-bank-marketing",
    "lcbench-car",
    "lcbench-christine",
    "lcbench-cnae-9",
    "lcbench-connect-4",
    "lcbench-covertype",
    "lcbench-credit-g",
    "lcbench-dionis",
    "lcbench-fabert",
    "lcbench-helena",
    "lcbench-higgs",
    "lcbench-jannis",
    "lcbench-jasmine",
    "lcbench-kc1",
    "lcbench-kr-vs-kp",
    "lcbench-mfeat-factors",
    "lcbench-nomao",
    "lcbench-numerai286",
    "lcbench-phoneme",
    "lcbench-segment",
    "lcbench-shuttle",
    "lcbench-sylvine",
    "lcbench-vehicle",
    "lcbench-volkert",
]


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag", type=str, required=False, default=generate_slug(2)
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    hash = random_string(4)
    num_seeds = 5
    for method in methods.keys():
        sm_args = dict(
            entry_point="benchmark_main.py",
            source_dir=str(Path(__file__).parent),
            checkpoint_s3_uri=s3_experiment_path(
                tuner_name=method, experiment_name=experiment_tag
            ),
            instance_type="ml.c5.4xlarge",
            instance_count=1,
            py_version="py38",
            framework_version="1.10.0",
            max_run=3600 * 72,
            role=get_execution_role(),
            dependencies=syne_tune.__path__ + benchmarking.__path__,
            disable_profiler=True,
        )

        if (
            method == Methods.NeuralBandSH
            or method == Methods.NeuralBandHB
            or method == Methods.MOBSTER
        ):
            for seed in range(num_seeds):
                for benchm in benchmark_names:
                    print(f"{experiment_tag}-{method}-{benchm}-{seed}")
                    sm_args["hyperparameters"] = {
                        "experiment_tag": experiment_tag,
                        "num_seeds": seed,
                        "run_all_seed": 0,
                        "method": method,
                        "benchmark": benchm,
                    }
                    est = PyTorch(**sm_args)
                    est.fit(
                        job_name=f"{experiment_tag}-{method}-{benchm}-{seed}-{hash}",
                        wait=False,
                    )

        elif (
            method == Methods.NeuralBand_UCB
            or method == Methods.NeuralBand_TS
            or method == Methods.NeuralBandEpsilon
        ):
            for seed in range(num_seeds):
                for benchm in benchmark_names:
                    print(f"{experiment_tag}-{method}-{benchm}-{seed}")
                    sm_args["hyperparameters"] = {
                        "experiment_tag": experiment_tag,
                        "num_seeds": seed,
                        "run_all_seed": 0,
                        "method": method,
                        "benchmark": benchm,
                    }
                    est = PyTorch(**sm_args)
                    est.fit(
                        job_name=f"{experiment_tag}-{method}-{benchm}-{seed}-{hash}",
                        wait=False,
                    )

        elif method == Methods.RS:
            print(f"{experiment_tag}-{method}")
            sm_args["hyperparameters"] = {
                "experiment_tag": experiment_tag,
                "num_seeds": num_seeds,
                "run_all_seed": 1,
                "method": method,
            }
            est = PyTorch(**sm_args)
            est.fit(job_name=f"{experiment_tag}-{method}-{hash}", wait=False)
        else:
            for seed in range(num_seeds):
                print(f"{experiment_tag}-{method}-{seed}")
                sm_args["hyperparameters"] = {
                    "experiment_tag": experiment_tag,
                    "num_seeds": seed,
                    "run_all_seed": 0,
                    "method": method,
                }
                est = PyTorch(**sm_args)
                est.fit(job_name=f"{experiment_tag}-{method}-{seed}-{hash}", wait=False)

File Path: benchmarking/nursery/fine_tuning_transformer_glue/hpo_main.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import argparse
from pathlib import Path

from syne_tune.config_space import uniform, loguniform, choice, randint
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.baselines import (
    ASHA,
    MOBSTER,
    BayesianOptimization,
    RandomSearch,
)
from syne_tune import (
    Tuner,
    StoppingCriterion,
)


# Different GLUE tasks and their metric names
TASK2METRICSMODE = {
    "cola": {"metric": "matthews_correlation", "mode": "max"},
    "mnli": {"metric": "accuracy", "mode": "max"},
    "mrpc": {"metric": "f1", "mode": "max"},
    "qnli": {"metric": "accuracy", "mode": "max"},
    "qqp": {"metric": "f1", "mode": "max"},
    "rte": {"metric": "accuracy", "mode": "max"},
    "sst2": {"metric": "accuracy", "mode": "max"},
    "stsb": {"metric": "spearmanr", "mode": "max"},
    "wnli": {"metric": "accuracy", "mode": "max"},
}


# Pre-trained models from HuggingFace zoo considered here
PRETRAINED_MODELS = [
    "bert-base-cased",
    "bert-base-uncased",
    "distilbert-base-uncased",
    "distilbert-base-cased",
    "roberta-base",
    "albert-base-v2",
    "distilroberta-base",
    "xlnet-base-cased",
    "albert-base-v1",
]


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--run_id", type=int, default=0)
    parser.add_argument(
        "--dataset", type=str, default="rte", choices=list(TASK2METRICSMODE.keys())
    )
    parser.add_argument(
        "--model_type", type=str, default="bert-base-cased", choices=PRETRAINED_MODELS
    )
    parser.add_argument("--max_runtime", type=int, default=1800)
    parser.add_argument(
        "--num_train_epochs",
        type=int,
        default=3,
        help="number of epochs to train the networks",
    )
    parser.add_argument("--n_workers", type=int, default=4)
    parser.add_argument(
        "--optimizer", type=str, default="asha", choices=("rs", "bo", "asha", "mobster")
    )
    parser.add_argument(
        "--experiment_name",
        type=str,
        help="experiment name",
    )
    parser.add_argument("--choose_model", type=int, default=0)
    parser.add_argument("--seed", type=int)
    parser.add_argument("--train_valid_fraction", type=float, default=0.7)
    parser.add_argument("--store_logs_checkpoints_to_s3", type=int, default=0)

    args, _ = parser.parse_known_args()
    args.choose_model = bool(args.choose_model)
    args.store_logs_checkpoints_to_s3 = bool(args.store_logs_checkpoints_to_s3)
    return args


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    args = parse_args()
    dataset = args.dataset
    model_type = args.model_type
    num_train_epochs = args.num_train_epochs
    seed = args.seed
    optimizer = args.optimizer

    # Path to training script. We also need to specify the names of metrics reported
    # back from this script
    entry_point = "./run_glue_modified.py"
    metric = "eval_" + TASK2METRICSMODE[dataset]["metric"]
    mode = TASK2METRICSMODE[dataset]["mode"]
    resource_attribute = "epoch"

    # The configuration space contains all hyperparameters we would like to optimize,
    # and their search ranges.
    hyperparameter_space = {
        "learning_rate": loguniform(1e-6, 1e-4),
        "per_device_train_batch_size": randint(16, 48),
        "warmup_ratio": uniform(0, 0.5),
    }

    # Additionally, it contains fixed parameters passed to the training script.
    # We use `save_strategy="no"` in order to save disk space. This must be
    # changed to "epoch" for HPO methods which require checkpointing.
    fixed_parameters = {
        "num_train_epochs": num_train_epochs,
        "model_name_or_path": model_type,
        "task_name": dataset,
        "train_valid_fraction": args.train_valid_fraction,
        "seed": seed,
        "do_train": True,
        "max_seq_length": 128,
        "output_dir": "tmp/" + dataset,
        "evaluation_strategy": "epoch",
        "save_strategy": "no",  # change to "epoch" if checkpoints are needed!
        "save_total_limit": 1,
    }

    config_space = {**hyperparameter_space, **fixed_parameters}

    # This is the default configuration provided by Hugging Face. It will always
    # be evaluated first
    default_configuration = {
        "learning_rate": 2e-5,
        "per_device_train_batch_size": 32,
        "warmup_ratio": 0.0,
    }

    # Combine HPO with model selection:
    # Just another categorical hyperparameter
    if args.choose_model:
        config_space["model_name_or_path"] = choice(PRETRAINED_MODELS)
        default_configuration["model_name_or_path"] = model_type

    # The backend is responsible to start and stop training evaluations. Here, we
    # use the local backend, which runs on a single instance
    backend = LocalBackend(entry_point=entry_point)

    # HPO algorithm
    # We can choose from these optimizers:
    schedulers = {
        "rs": RandomSearch,
        "bo": BayesianOptimization,
        "asha": ASHA,
        "mobster": MOBSTER,
    }
    scheduler_kwargs = dict(
        metric=metric,
        mode=mode,
        random_seed=seed,  # same seed passed to training function
        points_to_evaluate=[default_configuration],  # evaluate this one first
    )
    if optimizer in {"asha", "mobster"}:
        # The multi-fidelity methods need extra information
        scheduler_kwargs["resource_attr"] = resource_attribute
        # Maximum resource level information in `config_space`:
        scheduler_kwargs["max_resource_attr"] = "num_train_epochs"
    scheduler = schedulers[optimizer](config_space, **scheduler_kwargs)

    # All parts come together in the tuner, which runs the experiment
    stop_criterion = StoppingCriterion(max_wallclock_time=args.max_runtime)
    tuner = Tuner(
        trial_backend=backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=args.n_workers,
        metadata=vars(args),  # metadata is stored along with results
        tuner_name=args.experiment_name,
    )

    # Set path for logs and checkpoints
    if args.store_logs_checkpoints_to_s3:
        backend.set_path(results_root=tuner.tuner_path)
    else:
        backend.set_path(
            results_root=str(Path("~/").expanduser()), tuner_name=tuner.name
        )

    tuner.run()  # off we go!

File Path: benchmarking/nursery/fine_tuning_transformer_glue/launch_remote.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
import itertools
from tqdm import tqdm

from sagemaker.pytorch import PyTorch
import syne_tune

from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
)
from syne_tune.util import s3_experiment_path, random_string


if __name__ == "__main__":
    experiment_name = "glue-4"
    random_seed_offset = 31415627

    dataset_name = "rte"  # GLUE task
    model_type = "bert-base-cased"  # Default model used if not selected
    num_train_epochs = 3  # Maximum number of epochs
    max_runtime = 1800  # Each experiment runs for 30 mins
    instance_type = "ml.g4dn.xlarge"
    # instance_type = "ml.g4dn.12xlarge"
    # Useful if not all experiments could be started:
    skip_initial_experiments = 0

    # Compare selecting the model to fixing it to a default choice:
    model_selection = [False, True]
    # Compare 4 different HPO algorithms (2 multi-fidelity):
    optimizers = ["rs", "bo", "asha", "mobster"]
    # Each setup is repeated 10 times:
    num_runs = 10
    run_ids = list(range(num_runs))
    num_experiments = len(model_selection) * len(optimizers) * len(run_ids)
    # We need 1 GPU for each worker:
    if instance_type == "ml.g4dn.12xlarge":
        n_workers = 4
    else:
        n_workers = 1

    # Loop over all combinations and repetitions
    suffix = random_string(4)
    combinations = list(itertools.product(model_selection, optimizers, run_ids))
    for exp_id, (choose_model, optimizer, run_id) in tqdm(enumerate(combinations)):
        if exp_id < skip_initial_experiments:
            continue
        print(f"Experiment {exp_id} (of {num_experiments})")
        # Make sure that results (on S3) are written to different subdirectories.
        # Otherwise, the SM training job will download many previous results at
        # start
        tuner_name = f"{optimizer}-{choose_model}-{run_id}"
        # Results written to S3 under this path
        checkpoint_s3_uri = s3_experiment_path(
            experiment_name=experiment_name, tuner_name=tuner_name
        )
        print(f"Results stored to {checkpoint_s3_uri}")
        # We use a different seed for each `run_id`
        seed = (random_seed_offset + run_id) % (2**32)

        # Each experiment run is executed as SageMaker training job
        hyperparameters = {
            "run_id": run_id,
            "dataset": dataset_name,
            "model_type": model_type,
            "max_runtime": max_runtime,
            "num_train_epochs": num_train_epochs,
            "n_workers": n_workers,
            "optimizer": optimizer,
            "experiment_name": experiment_name,
            "choose_model": int(choose_model),
            "seed": seed,
        }

        # Pass Syne Tune sources as dependencies
        source_dir = str(Path(__file__).parent)
        entry_point = "hpo_main.py"
        dependencies = syne_tune.__path__ + [source_dir]
        # Latest PyTorch version (1.10):
        est = PyTorch(
            entry_point=entry_point,
            source_dir=source_dir,
            checkpoint_s3_uri=checkpoint_s3_uri,
            instance_type=instance_type,
            instance_count=1,
            py_version="py38",
            framework_version="1.10.0",
            volume_size=125,
            max_run=int(1.25 * max_runtime),
            role=get_execution_role(),
            dependencies=dependencies,
            disable_profiler=True,
            hyperparameters=hyperparameters,
        )

        job_name = f"{experiment_name}-{tuner_name}-{suffix}"
        print(f"Launching {job_name}")
        est.fit(wait=False, job_name=job_name)

    print(
        "\nLaunched all requested experiments. Once everything is done, use this "
        "command to sync result files from S3:\n"
        f"$ aws s3 sync {s3_experiment_path(experiment_name=experiment_name)}/ "
        f'~/syne-tune/{experiment_name}/ --exclude "*" '
        '--include "*metadata.json" --include "*results.csv.zip"'
    )

File Path: benchmarking/nursery/fine_tuning_transformer_glue/run_glue_modified.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2020 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" Finetuning the library models for sequence classification on GLUE."""

import logging
import os
import random
import sys
from dataclasses import dataclass, field
from typing import Optional

import datasets
import numpy as np
from datasets import load_dataset, load_metric

import transformers
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    EvalPrediction,
    HfArgumentParser,
    PretrainedConfig,
    Trainer,
    TrainingArguments,
    default_data_collator,
    set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils.versions import require_version

from syne_tune.report import Reporter  # Syne Tune: Reporting


# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
# check_min_version("4.17.0.dev0")

require_version(
    "datasets>=1.8.0",
    "To fix: pip install -r examples/pytorch/text-classification/requirements.txt",
)

task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}

logger = logging.getLogger(__name__)


# ************** ADDITIONAL CODE TO SUPPORT SYNE TUNE USE CASES ***************

# We start from the Hugging Face example script
#
#     `examples/pytorch/text-classification/run_glue.py`
#
# which fine-tunes library models for text classification on GLUE. We make the
# following modifications:
#
# [ 1] We split the training dataset into a part for training and one for
#      validation. The evaluation dataset (if given) is used as test set. This
#      means the metrics 'eval_*' are computed on the validation split, not on
#      the test set. This is what HPO operates on.
#      If the evaluation dataset is given, we also compute test set metrics
#      along with the others and report them as 'test_*'. This is for results
#      analysis only, HPO must not access them.
#
# [ 2] We use a custom `TrainerCallback` in order to report metrics to Syne Tune,
#      using the `on_evaluate` method. This needs `evaluation_strategy` to be set
#      to 'epoch', which we do here.
#      For convenience, test metrics are computed and reported alongside.
#
# [ 3] Optionally, we estimate prediction latency by running a few forward
#      passes. This is done at the start, and the same metrics are reported with
#      each `on_evaluate`.
#
# [ 4] If Syne Tune passes a checkpoint directory, this overwrites `output_dir`,
#      where checkpoints are written. Note that checkpointing is done only if
#      `training_args.save_strategy != "no"` (and it must be set to "epoch"
#      then). If the HPO method does not require checkpoints, we recommend
#      switching checkpointing off, in order to save local disk space.


@dataclass
class SyneTuneArguments:
    """
    Syne Tune is passing additional arguments. In `HfArgumentParser`, they can be
    kept separately from the ones in the default script.
    """

    st_checkpoint_dir: Optional[str] = field(
        default=None,
        metadata={
            "help": "The checkpoint directory for SyneTune. Overwrites `output_dir`."
        },
    )

    train_valid_fraction: float = field(
        default=0.7,
        metadata={
            "help": "Fraction of training set used for training, rest used for evaluation."
        },
    )

    latency_attribute: Optional[str] = field(
        default=None,
        metadata={"help": "If given, prediction latency is estimated and reported"},
    )

    num_model_params_attribute: Optional[int] = field(
        default=None,
        metadata={"help": "If given, number of model parameters is reported"},
    )

    # The following arguments are passed by Syne Tune, but not needed

    trial_id: Optional[int] = field(
        default=None,
        metadata={"help": ""},
    )

    st_instance_type: Optional[str] = field(
        default=None,
        metadata={"help": ""},
    )

    st_instance_count: Optional[int] = field(
        default=None,
        metadata={"help": ""},
    )


# [ 2]
class ReportBackMetrics(transformers.trainer_callback.TrainerCallback):
    """
    This callback is used in order to report metrics back to Syne Tune, using a
    `Reporter` object.

    If `test_dataset` is given, we also compute and report test set metrics here.
    These are just for final evaluations. HPO must use validation metrics (in
    `metrics` passed to `on_evaluate`).

    If `additional_info` is given, it is a static dict reported with each call.
    """

    def __init__(self, trainer, test_dataset, additional_info=None):
        self.trainer = trainer
        if test_dataset is not None:
            self.test_dataloader = self.trainer.get_test_dataloader(test_dataset)
        else:
            self.test_dataloader = None
        self.report = Reporter()
        if additional_info is None:
            additional_info = dict()
        self.additional_info = additional_info

    def on_evaluate(self, args, state, control, **kwargs):
        # Metrics on train and validation set:
        results = kwargs["metrics"].copy()
        results["step"] = state.global_step
        results["epoch"] = int(state.epoch)
        # Compute metrics on test set (if given)
        if self.test_dataloader is not None:
            output = self.trainer.evaluation_loop(
                self.test_dataloader,
                "Testing",
                prediction_loss_only=True
                if self.trainer.compute_metrics is None
                else None,
                metric_key_prefix="test",
            )
            results.update(output.metrics)
        # Append additional info
        results.update(self.additional_info)
        # Report results back to Syne Tune
        self.report(**results)


# [ 3]
def additional_syne_tune_metrics(
    model, tokenizer, train_dataset, data_args, syne_tune_args
) -> dict:
    """
    Additional metrics are total number of trainable model parameters, and
    prediction latency of the model. The latter is estimated by running a
    forward pass on a single sentence, averaging this over 300 repeats.

    These additional metrics are used by the multi-objective instance type
    tuning example.

    """
    additional_metrics = dict()
    if syne_tune_args.num_model_params_attribute is not None:
        # Total number of parameters
        n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        additional_metrics[syne_tune_args.num_model_params_attribute] = n_params
    if syne_tune_args.latency_attribute is not None:
        # Compute inference time (i.e latency)
        import torch

        device = torch.device("cuda")
        model.to(device)
        # init loggers
        starter = torch.cuda.Event(enable_timing=True)
        ender = torch.cuda.Event(enable_timing=True)
        repetitions = 300
        timings = []
        sentence1_key, _ = task_to_keys[data_args.task_name]
        # warm-up GPU
        single_input = train_dataset[0][sentence1_key]
        for _ in range(10):
            _ = model(**tokenizer(single_input, return_tensors="pt").to(device))
        # measure latency
        with torch.no_grad():
            for _ in range(repetitions):
                starter.record()
                _ = model(**tokenizer(single_input, return_tensors="pt").to(device))
                ender.record()
                # synchronize GPU
                torch.cuda.synchronize()
                curr_time = starter.elapsed_time(ender)
                timings.append(curr_time)
        additional_metrics[syne_tune_args.latency_attribute] = np.mean(timings)
    return additional_metrics


# *** END SYNE TUNE INSERT ***


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.

    Using `HfArgumentParser` we can turn this class
    into argparse arguments to be able to specify them on
    the command line.
    """

    task_name: Optional[str] = field(
        default=None,
        metadata={
            "help": "The name of the task to train on: "
            + ", ".join(task_to_keys.keys())
        },
    )
    dataset_name: Optional[str] = field(
        default=None,
        metadata={"help": "The name of the dataset to use (via the datasets library)."},
    )
    dataset_config_name: Optional[str] = field(
        default=None,
        metadata={
            "help": "The configuration name of the dataset to use (via the datasets library)."
        },
    )
    max_seq_length: int = field(
        default=128,
        metadata={
            "help": "The maximum total input sequence length after tokenization. Sequences longer "
            "than this will be truncated, sequences shorter will be padded."
        },
    )
    overwrite_cache: bool = field(
        default=False,
        metadata={"help": "Overwrite the cached preprocessed datasets or not."},
    )
    pad_to_max_length: bool = field(
        default=True,
        metadata={
            "help": "Whether to pad all samples to `max_seq_length`. "
            "If False, will pad the samples dynamically when batching to the maximum length in the batch."
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
            "value if set."
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
            "value if set."
        },
    )
    max_predict_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of prediction examples to this "
            "value if set."
        },
    )
    train_file: Optional[str] = field(
        default=None,
        metadata={"help": "A csv or a json file containing the training data."},
    )
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "A csv or a json file containing the validation data."},
    )
    test_file: Optional[str] = field(
        default=None,
        metadata={"help": "A csv or a json file containing the test data."},
    )

    def __post_init__(self):
        if self.task_name is not None:
            self.task_name = self.task_name.lower()
            if self.task_name not in task_to_keys.keys():
                raise ValueError(
                    "Unknown task, you should pick one in "
                    + ",".join(task_to_keys.keys())
                )
        elif self.dataset_name is not None:
            pass
        elif self.train_file is None or self.validation_file is None:
            raise ValueError(
                "Need either a GLUE task, a training/validation file or a dataset name."
            )
        else:
            train_extension = self.train_file.split(".")[-1]
            assert train_extension in [
                "csv",
                "json",
            ], "`train_file` should be a csv or a json file."
            validation_extension = self.validation_file.split(".")[-1]
            assert (
                validation_extension == train_extension
            ), "`validation_file` should have the same extension (csv or json) as `train_file`."


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """

    model_name_or_path: str = field(
        metadata={
            "help": "Path to pretrained model or model identifier from huggingface.co/models"
        }
    )
    config_name: Optional[str] = field(
        default=None,
        metadata={
            "help": "Pretrained config name or path if not the same as model_name"
        },
    )
    tokenizer_name: Optional[str] = field(
        default=None,
        metadata={
            "help": "Pretrained tokenizer name or path if not the same as model_name"
        },
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={
            "help": "Where do you want to store the pretrained models downloaded from huggingface.co"
        },
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={
            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."
        },
    )
    model_revision: str = field(
        default="main",
        metadata={
            "help": "The specific model version to use (can be a branch name, tag name or commit id)."
        },
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
            "with private models)."
        },
    )


def main():
    # See all possible arguments in src/transformers/training_args.py
    # or by passing the --help flag to this script.
    # We now keep distinct sets of args, for a cleaner separation of concerns.

    parser = HfArgumentParser(
        (ModelArguments, DataTrainingArguments, TrainingArguments, SyneTuneArguments)
    )
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments.
        model_args, data_args, training_args, syne_tune_args = parser.parse_json_file(
            json_file=os.path.abspath(sys.argv[1])
        )
    else:
        (
            model_args,
            data_args,
            training_args,
            syne_tune_args,
        ) = parser.parse_args_into_dataclasses()

    # *** SYNE TUNE INSERT [ 4] ***

    assert (
        training_args.do_train
    ), "Need do_train=True in order to be used with Syne Tune"

    # Syne Tune overwrites checkpoint directory
    st_checkpoint_dir = syne_tune_args.st_checkpoint_dir
    if st_checkpoint_dir is not None and training_args.save_strategy != "no":
        training_args.output_dir = st_checkpoint_dir

    # *** END SYNE TUNE INSERT ***

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )
    logger.info(f"Training/evaluation parameters {training_args}")

    # Detecting last checkpoint.
    last_checkpoint = None
    if (
        os.path.isdir(training_args.output_dir)
        and training_args.do_train
        and not training_args.overwrite_output_dir
    ):
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif (
            last_checkpoint is not None and training_args.resume_from_checkpoint is None
        ):
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

    # Set seed before initializing model.
    set_seed(training_args.seed)

    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)
    # or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).
    #
    # For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the
    # sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named
    # label if at least two columns are provided.
    #
    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this
    # single column. You can easily tweak this behavior (see below)
    #
    # In distributed training, the load_dataset function guarantee that only one local process can concurrently
    # download the dataset.
    if data_args.task_name is not None:
        # Downloading and loading a dataset from the hub.
        raw_datasets = load_dataset(
            "glue", data_args.task_name, cache_dir=model_args.cache_dir
        )
    elif data_args.dataset_name is not None:
        # Downloading and loading a dataset from the hub.
        raw_datasets = load_dataset(
            data_args.dataset_name,
            data_args.dataset_config_name,
            cache_dir=model_args.cache_dir,
        )
    else:
        # Loading a dataset from your local files.
        # CSV/JSON training and evaluation files are needed.
        data_files = {
            "train": data_args.train_file,
            "validation": data_args.validation_file,
        }

        # Get the test dataset: you can provide your own CSV/JSON test file (see below)
        # when you use `do_predict` without specifying a GLUE benchmark task.
        if training_args.do_predict:
            if data_args.test_file is not None:
                train_extension = data_args.train_file.split(".")[-1]
                test_extension = data_args.test_file.split(".")[-1]
                assert (
                    test_extension == train_extension
                ), "`test_file` should have the same extension (csv or json) as `train_file`."
                data_files["test"] = data_args.test_file
            else:
                raise ValueError(
                    "Need either a GLUE task or a test file for `do_predict`."
                )

        for key in data_files.keys():
            logger.info(f"load a local file for {key}: {data_files[key]}")

        if data_args.train_file.endswith(".csv"):
            # Loading a dataset from local csv files
            raw_datasets = load_dataset(
                "csv", data_files=data_files, cache_dir=model_args.cache_dir
            )
        else:
            # Loading a dataset from local json files
            raw_datasets = load_dataset(
                "json", data_files=data_files, cache_dir=model_args.cache_dir
            )
    # See more about loading any type of standard or custom dataset at
    # https://huggingface.co/docs/datasets/loading_datasets.html.

    # Labels
    if data_args.task_name is not None:
        is_regression = data_args.task_name == "stsb"
        if not is_regression:
            label_list = raw_datasets["train"].features["label"].names
            num_labels = len(label_list)
        else:
            num_labels = 1
    else:
        # Trying to have good defaults here, don't hesitate to tweak to your needs.
        is_regression = raw_datasets["train"].features["label"].dtype in [
            "float32",
            "float64",
        ]
        if is_regression:
            num_labels = 1
        else:
            # A useful fast method:
            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique
            label_list = raw_datasets["train"].unique("label")
            label_list.sort()  # Let's sort it for determinism
            num_labels = len(label_list)

    # Load pretrained model and tokenizer
    #
    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
    # download model & vocab.
    config = AutoConfig.from_pretrained(
        model_args.config_name
        if model_args.config_name
        else model_args.model_name_or_path,
        num_labels=num_labels,
        finetuning_task=data_args.task_name,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name
        if model_args.tokenizer_name
        else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        use_fast=model_args.use_fast_tokenizer,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    model = AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )

    # Preprocessing the raw_datasets
    if data_args.task_name is not None:
        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]
    else:
        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.
        non_label_column_names = [
            name for name in raw_datasets["train"].column_names if name != "label"
        ]
        if (
            "sentence1" in non_label_column_names
            and "sentence2" in non_label_column_names
        ):
            sentence1_key, sentence2_key = "sentence1", "sentence2"
        else:
            if len(non_label_column_names) >= 2:
                sentence1_key, sentence2_key = non_label_column_names[:2]
            else:
                sentence1_key, sentence2_key = non_label_column_names[0], None

    # Padding strategy
    if data_args.pad_to_max_length:
        padding = "max_length"
    else:
        # We will pad later, dynamically at batch creation, to the max sequence length in each batch
        padding = False

    # Some models have set the order of the labels to use, so let's make sure we do use it.
    label_to_id = None
    if (
        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
        and data_args.task_name is not None
        and not is_regression
    ):
        # Some have all caps in their config, some don't.
        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):
            label_to_id = {
                i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)
            }
        else:
            logger.warning(
                "Your model seems to have been trained with labels, but they don't match the dataset: ",
                f"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}."
                "\nIgnoring the model labels as a result.",
            )
    elif data_args.task_name is None and not is_regression:
        label_to_id = {v: i for i, v in enumerate(label_list)}

    if label_to_id is not None:
        model.config.label2id = label_to_id
        model.config.id2label = {id: label for label, id in config.label2id.items()}
    elif data_args.task_name is not None and not is_regression:
        model.config.label2id = {l: i for i, l in enumerate(label_list)}
        model.config.id2label = {id: label for label, id in config.label2id.items()}

    if data_args.max_seq_length > tokenizer.model_max_length:
        logger.warning(
            f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the"
            f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
        )
    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

    def preprocess_function(examples):
        # Tokenize the texts
        args = (
            (examples[sentence1_key],)
            if sentence2_key is None
            else (examples[sentence1_key], examples[sentence2_key])
        )
        result = tokenizer(
            *args, padding=padding, max_length=max_seq_length, truncation=True
        )

        # Map labels to IDs (not necessary for GLUE tasks)
        if label_to_id is not None and "label" in examples:
            result["label"] = [
                (label_to_id[l] if l != -1 else -1) for l in examples["label"]
            ]
        return result

    with training_args.main_process_first(desc="dataset map pre-processing"):
        raw_datasets = raw_datasets.map(
            preprocess_function,
            batched=True,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Running tokenizer on dataset",
        )
    if training_args.do_train:
        if "train" not in raw_datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = raw_datasets["train"]
        if data_args.max_train_samples is not None:
            train_dataset = train_dataset.select(range(data_args.max_train_samples))

    if training_args.do_eval:
        if (
            "validation" not in raw_datasets
            and "validation_matched" not in raw_datasets
        ):
            raise ValueError("--do_eval requires a validation dataset")
        eval_dataset = raw_datasets[
            "validation_matched" if data_args.task_name == "mnli" else "validation"
        ]
        if data_args.max_eval_samples is not None:
            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))

    if (
        training_args.do_predict
        or data_args.task_name is not None
        or data_args.test_file is not None
    ):
        if "test" not in raw_datasets and "test_matched" not in raw_datasets:
            raise ValueError("--do_predict requires a test dataset")
        predict_dataset = raw_datasets[
            "test_matched" if data_args.task_name == "mnli" else "test"
        ]
        if data_args.max_predict_samples is not None:
            predict_dataset = predict_dataset.select(
                range(data_args.max_predict_samples)
            )

    # Log a few random samples from the training set:
    if training_args.do_train:
        for index in random.sample(range(len(train_dataset)), 3):
            logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

    # Get the metric function
    if data_args.task_name is not None:
        metric = load_metric("glue", data_args.task_name)
    else:
        metric = load_metric("accuracy")

    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a
    # predictions and label_ids field) and has to return a dictionary string to float.
    def compute_metrics(p: EvalPrediction):
        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
        preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)
        if data_args.task_name is not None:
            result = metric.compute(predictions=preds, references=p.label_ids)
            if len(result) > 1:
                result["combined_score"] = np.mean(list(result.values())).item()
            return result
        elif is_regression:
            return {"mse": ((preds - p.label_ids) ** 2).mean().item()}
        else:
            return {"accuracy": (preds == p.label_ids).astype(np.float32).mean().item()}

    # Data collator will default to DataCollatorWithPadding when the tokenizer is passed to Trainer, so we change it if
    # we already did the padding.
    if data_args.pad_to_max_length:
        data_collator = default_data_collator
    elif training_args.fp16:
        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
    else:
        data_collator = None

    # *** SYNE TUNE INSERT [ 1] ***

    # Split `train_dataset` into training part (new `train_dataset`) and
    # validation part `valid_dataset`. These two are used for HPO, while
    # `eval_dataset` is used as test set.

    split = train_dataset.train_test_split(
        train_size=syne_tune_args.train_valid_fraction,
        seed=training_args.seed,
    )
    train_dataset = split["train"]
    valid_dataset = split["test"]

    # *** END SYNE TUNE INSERT ***

    # Initialize our Trainer
    # SYNE TUNE: `eval_dataset` is set to `valid_dataset`, whereas `eval_dataset`
    # is used in the original script
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=valid_dataset,  # SYNE TUNE INSERT [ 1]
        compute_metrics=compute_metrics,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    # *** SYNE TUNE INSERT [ 2] ***

    # Compute additional metrics (optional)
    additional_info = additional_syne_tune_metrics(
        model, tokenizer, train_dataset, data_args, syne_tune_args
    )

    # Callback to report metrics back to Syne Tune at the end of each epoch
    trainer.add_callback(
        ReportBackMetrics(
            trainer=trainer,
            test_dataset=eval_dataset if training_args.do_eval else None,
            additional_info=additional_info,
        )
    )

    # *** END SYNE TUNE INSERT ***

    # Training
    if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
        metrics = train_result.metrics
        max_train_samples = (
            data_args.max_train_samples
            if data_args.max_train_samples is not None
            else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.save_model()  # Saves the tokenizer too for easy upload

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()

    # Evaluation
    if training_args.do_eval:
        logger.info("*** Evaluate ***")

        # Loop to handle MNLI double evaluation (matched, mis-matched)
        tasks = [data_args.task_name]
        eval_datasets = [eval_dataset]
        if data_args.task_name == "mnli":
            tasks.append("mnli-mm")
            eval_datasets.append(raw_datasets["validation_mismatched"])

        for eval_dataset, task in zip(eval_datasets, tasks):
            metrics = trainer.evaluate(eval_dataset=eval_dataset)

            max_eval_samples = (
                data_args.max_eval_samples
                if data_args.max_eval_samples is not None
                else len(eval_dataset)
            )
            metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

            trainer.log_metrics("eval", metrics)
            trainer.save_metrics("eval", metrics)

    if training_args.do_predict:
        logger.info("*** Predict ***")

        # Loop to handle MNLI double evaluation (matched, mis-matched)
        tasks = [data_args.task_name]
        predict_datasets = [predict_dataset]
        if data_args.task_name == "mnli":
            tasks.append("mnli-mm")
            predict_datasets.append(raw_datasets["test_mismatched"])

        for predict_dataset, task in zip(predict_datasets, tasks):
            # Removing the `label` columns because it contains -1 and Trainer won't like that.
            predict_dataset = predict_dataset.remove_columns("label")
            predictions = trainer.predict(
                predict_dataset, metric_key_prefix="predict"
            ).predictions
            predictions = (
                np.squeeze(predictions)
                if is_regression
                else np.argmax(predictions, axis=1)
            )

            output_predict_file = os.path.join(
                training_args.output_dir, f"predict_results_{task}.txt"
            )
            if trainer.is_world_process_zero():
                with open(output_predict_file, "w") as writer:
                    logger.info(f"***** Predict results {task} *****")
                    writer.write("index\tprediction\n")
                    for index, item in enumerate(predictions):
                        if is_regression:
                            writer.write(f"{index}\t{item:3.3f}\n")
                        else:
                            item = label_list[item]
                            writer.write(f"{index}\t{item}\n")

    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "tasks": "text-classification",
    }
    if data_args.task_name is not None:
        kwargs["language"] = "en"
        kwargs["dataset_tags"] = "glue"
        kwargs["dataset_args"] = data_args.task_name
        kwargs["dataset"] = f"GLUE {data_args.task_name.upper()}"

    if training_args.push_to_hub:
        trainer.push_to_hub(**kwargs)
    else:
        trainer.create_model_card(**kwargs)


def _mp_fn(index):
    # For xla_spawn (TPUs)
    main()


if __name__ == "__main__":
    main()

File Path: benchmarking/nursery/lstm_wikitext2/definition_lstm_wikitext2.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example that reproduces the LSTM on WikiText2 benchmark from AutoGluonExperiments repo
"""
from pathlib import Path

from benchmarking.utils import get_cost_model_for_batch_size
from benchmarking.nursery.lstm_wikitext2.lstm_wikitext2 import (
    BATCH_SIZE_LOWER,
    BATCH_SIZE_UPPER,
    BATCH_SIZE_KEY,
    _config_space,
    METRIC_NAME,
    RESOURCE_ATTR,
    ELAPSED_TIME_ATTR,
)


def lstm_wikitext2_default_params(params=None):
    if params is not None and params.get("backend") == "sagemaker":
        instance_type = "ml.g4dn.xlarge"
        num_workers = 8
    else:
        # For local backend, GPU cores serve different workers, so we
        # need more memory
        instance_type = "ml.g4dn.12xlarge"
        num_workers = 4
    return {
        "max_resource_level": 81,
        "grace_period": 1,
        "reduction_factor": 3,
        "instance_type": instance_type,
        "num_workers": num_workers,
        "framework": "PyTorch",
        "framework_version": "1.6",
        "report_current_best": "False",
        "dataset_path": "./",
        "cost_model_type": "quadratic_spline",
    }


def lstm_wikitext2_benchmark(params):
    config_space = dict(
        _config_space,
        dataset_path=params["dataset_path"],
        epochs=params["max_resource_level"],
        report_current_best=params["report_current_best"],
    )
    return {
        "script": Path(__file__).parent / "lstm_wikitext2.py",
        "metric": METRIC_NAME,
        "mode": "max",
        "resource_attr": RESOURCE_ATTR,
        "elapsed_time_attr": ELAPSED_TIME_ATTR,
        "max_resource_attr": "epochs",
        "map_reward": "minus_x",
        "config_space": config_space,
        "cost_model": get_cost_model_for_batch_size(
            params,
            batch_size_key=BATCH_SIZE_KEY,
            batch_size_range=(BATCH_SIZE_LOWER, BATCH_SIZE_UPPER),
        ),
    }

File Path: benchmarking/nursery/lstm_wikitext2/lstm_wikitext2.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example that reproduces the LSTM on WikiText2 benchmark from AutoGluonExperiments repo
"""
import os
import argparse
import logging
import time
import math

from syne_tune import Reporter
from syne_tune.config_space import randint, uniform, loguniform, add_to_argparse
from benchmarking.utils import (
    resume_from_checkpointed_model,
    checkpoint_model_at_rung_level,
    add_checkpointing_to_argparse,
    pytorch_load_save_functions,
    parse_bool,
)


BATCH_SIZE_LOWER = 8

BATCH_SIZE_UPPER = 256

BATCH_SIZE_KEY = "batch_size"

METRIC_NAME = "objective"

RESOURCE_ATTR = "epoch"

ELAPSED_TIME_ATTR = "elapsed_time"


_config_space = {
    "lr": loguniform(1, 50),
    "dropout": uniform(0, 0.99),
    BATCH_SIZE_KEY: randint(BATCH_SIZE_LOWER, BATCH_SIZE_UPPER),
    "clip": uniform(0.1, 2),
    "lr_factor": loguniform(1, 100),
}


DATASET_PATH = "https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/"


def download_data(root):
    import urllib

    path = os.path.join(root, "wikitext-2")
    for fname in ("train.txt", "valid.txt"):
        fh = os.path.join(path, fname)
        if not os.path.exists(fh):
            os.makedirs(path, exist_ok=True)
            urllib.request.urlretrieve(DATASET_PATH + fname, fh)


class Dictionary:
    def __init__(self):
        self.word2idx = {}
        self.idx2word = []

    def add_word(self, word):
        if word not in self.word2idx:
            self.idx2word.append(word)
            self.word2idx[word] = len(self.idx2word) - 1
        return self.word2idx[word]

    def __len__(self):
        return len(self.idx2word)


class Corpus:
    def __init__(self, root):
        self.dictionary = Dictionary()
        # Make sure files are present locally
        download_data(root)
        path = os.path.join(root, "wikitext-2")
        self.train = self.tokenize(path, "train.txt")
        self.valid = self.tokenize(path, "valid.txt")
        # self.test = self.tokenize(path, 'test.txt')

    def tokenize(self, path, fname):
        """Tokenizes a text file."""
        assert fname in {"train.txt", "valid.txt", "test.txt"}
        fh = os.path.join(path, fname)
        # Add words to the dictionary
        with open(fh, "r", encoding="utf8") as f:
            for line in f:
                words = line.split() + ["<eos>"]
                for word in words:
                    self.dictionary.add_word(word)
        # Tokenize file content
        with open(fh, "r", encoding="utf8") as f:
            idss = []
            for line in f:
                words = line.split() + ["<eos>"]
                ids = []
                for word in words:
                    ids.append(self.dictionary.word2idx[word])
                idss.append(torch.tensor(ids).type(torch.int64))
            ids = torch.cat(idss)
        return ids


def repackage_hidden(h):
    """Wraps hidden states in new Tensors, to detach them from their history."""

    if isinstance(h, torch.Tensor):
        return h.detach()
    else:
        return tuple(repackage_hidden(v) for v in h)


def objective(config):
    # print(args)
    model_type = "rnn"
    emsize = 200
    nhid = emsize
    nlayers = 2
    eval_batch_size = 10
    bptt = 35
    tied = True
    seed = np.random.randint(10000)
    # log_interval = 200
    # save = "./model.pt"
    nhead = 2
    dropout = config["dropout"]
    batch_size = config["batch_size"]
    clip = config["clip"]
    lr_factor = config["lr_factor"]
    report_current_best = parse_bool(config["report_current_best"])
    trial_id = config.get("trial_id")
    debug_log = trial_id is not None
    if debug_log:
        print("Trial {}: Starting evaluation".format(trial_id), flush=True)

    torch.manual_seed(seed)
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    #######################################################################
    # Load data
    #######################################################################
    path = config["dataset_path"]
    os.makedirs(path, exist_ok=True)
    # Lock protection is needed for backends which run multiple worker
    # processes on the same instance
    lock_path = os.path.join(path, "lock")
    lock = SoftFileLock(lock_path)
    try:
        with lock.acquire(timeout=120, poll_intervall=1):
            corpus = Corpus(config["dataset_path"])
    except Timeout:
        print(
            "WARNING: Could not obtain lock for dataset files. Trying anyway...",
            flush=True,
        )
        corpus = Corpus(config["dataset_path"])

    # Do not want to count the time to download the dataset, which can be
    # substantial the first time
    ts_start = time.time()
    report = Reporter()

    def batchify(data, bsz):
        # Work out how cleanly we can divide the dataset into bsz parts.
        nbatch = data.size(0) // bsz
        # Trim off any extra elements that wouldn't cleanly fit (remainders).
        data = data.narrow(0, 0, nbatch * bsz)
        # Evenly divide the data across the bsz batches.
        data = data.view(bsz, -1).t().contiguous()
        return data.to(device)

    train_data = batchify(corpus.train, batch_size)
    val_data = batchify(corpus.valid, eval_batch_size)
    # test_data = batchify(corpus.test, eval_batch_size)

    #######################################################################
    # Build the model
    #######################################################################
    ntokens = len(corpus.dictionary)
    if model_type == "transformer":
        model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(
            device
        )
    else:
        model = RNNModel("LSTM", ntokens, emsize, nhid, nlayers, dropout, tied).to(
            device
        )
    criterion = nn.CrossEntropyLoss()

    def get_batch(source, i):
        seq_len = min(bptt, len(source) - 1 - i)
        data = source[i : i + seq_len]
        target = source[i + 1 : i + 1 + seq_len].view(-1)
        return data, target

    def evaluate(model, corpus, criterion, data_source):
        # Turn on evaluation mode which disables dropout.
        model.eval()
        total_loss = 0.0
        ntokens = len(corpus.dictionary)
        if model_type != "transformer":
            hidden = model.init_hidden(eval_batch_size)
        with torch.no_grad():
            for i in range(0, data_source.size(0) - 1, bptt):
                data, targets = get_batch(data_source, i)
                if model_type == "transformer":
                    output = model(data)
                else:
                    output, hidden = model(data, hidden)
                    hidden = repackage_hidden(hidden)
                output_flat = output.view(-1, ntokens)
                total_loss += len(data) * criterion(output_flat, targets).item()
        return total_loss / (len(data_source) - 1)

    def train(model, corpus, criterion, train_data, lr, batch_size, clip):
        # Turn on training mode which enables dropout.
        model.train()
        # total_loss = 0.
        # start_time = time.time()
        ntokens = len(corpus.dictionary)
        if model_type != "transformer":
            hidden = model.init_hidden(batch_size)
        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):
            data, targets = get_batch(train_data, i)
            # Starting each batch, we detach the hidden state from how it was previously produced.
            # If we didn't, the model would try backpropagating all the way to start of the dataset.
            model.zero_grad()
            if model_type == "transformer":
                output = model(data)
            else:
                hidden = repackage_hidden(hidden)
                output, hidden = model(data, hidden)
            loss = criterion(output.view(-1, ntokens), targets)
            loss.backward()

            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            for p in model.parameters():
                p.data.add_(p.grad.data, alpha=-lr)

            # total_loss += loss.item()
            # if batch % log_interval == 0 and batch > 0:
            #    cur_loss = total_loss / log_interval
            #    elapsed = time.time() - start_time
            #    print('| {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.2f} | '
            #          'loss {:5.4f} | ppl {:8.2f}'.format(
            #        batch, len(train_data) // bptt, lr,
            #        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))
            #    total_loss = 0
            #    start_time = time.time()

    # Checkpointing
    # Note that `lr` and `best_val_loss` are also part of the state to be
    # checkpointed. In order for things to work out, we keep them in a
    # dict (otherwise, they'd not be mutable in `load_model_fn`,
    # `save_model_fn`.
    mutable_state = {"lr": config["lr"], "best_val_loss": None}

    load_model_fn, save_model_fn = pytorch_load_save_functions(
        {"model": model}, mutable_state
    )

    # Resume from checkpoint (optional)
    resume_from = resume_from_checkpointed_model(config, load_model_fn)

    # Loop over epochs.
    for epoch in range(resume_from + 1, config["epochs"] + 1):
        train(
            model, corpus, criterion, train_data, mutable_state["lr"], batch_size, clip
        )
        val_loss = evaluate(model, corpus, criterion, val_data)

        val_loss = np.clip(val_loss, 1e-10, 10)
        # print('-' * 89)
        # print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
        #      'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
        #                                 val_loss, math.exp(val_loss)))
        # print('-' * 89)
        elapsed_time = time.time() - ts_start

        if not np.isfinite(val_loss):
            val_loss = 7

        best_val_loss = mutable_state["best_val_loss"]
        if not best_val_loss or val_loss < best_val_loss:
            best_val_loss = val_loss
            mutable_state["best_val_loss"] = val_loss
        else:
            # Anneal the learning rate if no improvement has been seen in the validation dataset.
            mutable_state["lr"] /= lr_factor

        # Feed the score back back to Tune.
        _loss = best_val_loss if report_current_best else val_loss
        objective = -math.exp(_loss)
        report(
            **{
                RESOURCE_ATTR: epoch,
                METRIC_NAME: objective,
                ELAPSED_TIME_ATTR: elapsed_time,
            }
        )

        # Write checkpoint (optional)
        checkpoint_model_at_rung_level(config, save_model_fn, epoch)

        if debug_log:
            print(
                "Trial {}: epoch = {}, objective = {:.3f}, elapsed_time = {:.2f}".format(
                    trial_id, epoch, objective, elapsed_time
                ),
                flush=True,
            )


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from io import open
    import numpy as np
    from filelock import SoftFileLock, Timeout
    import torch
    import torch.nn as nn
    import torch.nn.functional as F

    # References to superclasses require torch and torch.nn to be defined here

    # Temporarily leave PositionalEncoding module here. Will be moved somewhere else.
    class PositionalEncoding(nn.Module):
        r"""Inject some information about the relative or absolute position of the tokens
            in the sequence. The positional encodings have the same dimension as
            the embeddings, so that the two can be summed. Here, we use sine and cosine
            functions of different frequencies.
        .. math::
            \text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))
            \text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))
            \text{where pos is the word position and i is the embed idx)
        Args:
            d_model: the embed dim (required).
            dropout: the dropout value (default=0.1).
            max_len: the max. length of the incoming sequence (default=5000).
        Examples:
            >>> pos_encoder = PositionalEncoding(d_model)
        """

        def __init__(self, d_model, dropout=0.1, max_len=5000):
            super(PositionalEncoding, self).__init__()
            self.dropout = nn.Dropout(p=dropout)
            pe = torch.zeros(max_len, d_model)
            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
            div_term = torch.exp(
                torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
            )
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            pe = pe.unsqueeze(0).transpose(0, 1)
            self.register_buffer("pe", pe)

        def forward(self, x):
            r"""Inputs of forward function
            Args:
                x: the sequence fed to the positional encoder model (required).
            Shape:
                x: [sequence length, batch size, embed dim]
                output: [sequence length, batch size, embed dim]
            Examples:
                >>> output = pos_encoder(x)
            """
            x = x + self.pe[: x.size(0), :]
            return self.dropout(x)

    class TransformerModel(nn.Module):
        """Container module with an encoder, a recurrent or transformer module, and a decoder."""

        def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):
            super(TransformerModel, self).__init__()
            try:
                from torch.nn import TransformerEncoder, TransformerEncoderLayer
            except:
                raise ImportError(
                    "TransformerEncoder module does not exist in PyTorch 1.1 or lower."
                )
            self.model_type = "Transformer"
            self.src_mask = None
            self.pos_encoder = PositionalEncoding(ninp, dropout)
            encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)
            self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)
            self.encoder = nn.Embedding(ntoken, ninp)
            self.ninp = ninp
            self.decoder = nn.Linear(ninp, ntoken)
            self.init_weights()

        def _generate_square_subsequent_mask(self, sz):
            mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
            mask = (
                mask.float()
                .masked_fill(mask == 0, float("-inf"))
                .masked_fill(mask == 1, float(0.0))
            )
            return mask

        def init_weights(self):
            initrange = 0.1
            self.encoder.weight.data.uniform_(-initrange, initrange)
            self.decoder.bias.data.zero_()
            self.decoder.weight.data.uniform_(-initrange, initrange)

        def forward(self, src, has_mask=True):
            if has_mask:
                device = src.device
                if self.src_mask is None or self.src_mask.size(0) != len(src):
                    mask = self._generate_square_subsequent_mask(len(src)).to(device)
                    self.src_mask = mask
            else:
                self.src_mask = None
            src = self.encoder(src) * math.sqrt(self.ninp)
            src = self.pos_encoder(src)
            output = self.transformer_encoder(src, self.src_mask)
            output = self.decoder(output)
            return F.log_softmax(output, dim=-1)

    class RNNModel(nn.Module):
        """Container module with an encoder, a recurrent module, and a decoder."""

        def __init__(
            self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False
        ):
            super(RNNModel, self).__init__()
            self.drop = nn.Dropout(dropout)
            self.encoder = nn.Embedding(ntoken, ninp)
            if rnn_type in ["LSTM", "GRU"]:
                self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)
            else:
                try:
                    nonlinearity = {"RNN_TANH": "tanh", "RNN_RELU": "relu"}[rnn_type]
                except KeyError:
                    raise ValueError(
                        """An invalid option for `--model` was supplied,
                                     options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']"""
                    )
                self.rnn = nn.RNN(
                    ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout
                )
            self.decoder = nn.Linear(nhid, ntoken)

            # Optionally tie weights as in:
            # "Using the Output Embedding to Improve Language Models" (Press & Wolf 2016)
            # https://arxiv.org/abs/1608.05859
            # and
            # "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling" (Inan et al. 2016)
            # https://arxiv.org/abs/1611.01462
            if tie_weights:
                if nhid != ninp:
                    raise ValueError(
                        "When using the tied flag, nhid must be equal to emsize"
                    )
                self.decoder.weight = self.encoder.weight
            self.init_weights()
            self.rnn_type = rnn_type
            self.nhid = nhid
            self.nlayers = nlayers

        def init_weights(self):
            initrange = 0.1
            self.encoder.weight.data.uniform_(-initrange, initrange)
            self.decoder.bias.data.zero_()
            self.decoder.weight.data.uniform_(-initrange, initrange)

        def forward(self, input, hidden):
            emb = self.drop(self.encoder(input))
            output, hidden = self.rnn(emb, hidden)
            output = self.drop(output)
            decoded = self.decoder(output)
            return decoded, hidden

        def init_hidden(self, bsz):
            weight = next(self.parameters())
            if self.rnn_type == "LSTM":
                return (
                    weight.new_zeros(self.nlayers, bsz, self.nhid),
                    weight.new_zeros(self.nlayers, bsz, self.nhid),
                )
            else:
                return weight.new_zeros(self.nlayers, bsz, self.nhid)

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_path", type=str, required=True)
    parser.add_argument("--report_current_best", type=str, default="False")
    parser.add_argument("--trial_id", type=str)
    add_to_argparse(parser, _config_space)
    add_checkpointing_to_argparse(parser)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: benchmarking/nursery/remote_sm_backend/launch_experiment.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from argparse import ArgumentParser

from sagemaker.pytorch import PyTorch

from syne_tune.backend import SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.baselines import ASHA
from syne_tune.stopping_criterion import StoppingCriterion
from syne_tune.tuner import Tuner
from benchmarking.definitions.definition_resnet_cifar10 import (
    resnet_cifar10_benchmark,
)
from syne_tune.util import repository_root_path


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=0,
        help="seed (for repetitions)",
    )
    parser.add_argument(
        "--n_workers",
        type=int,
        default=4,
        help="number of parallel workers",
    )
    parser.add_argument(
        "--max_wallclock_time",
        type=int,
        default=3 * 3600,
        help="maximum wallclock time of experiment",
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag

    params = {
        "backend": "sagemaker",
        "dataset_path": "./",
        "num_gpus": 1,
        "max_resource_level": 27,
        "instance_type": "ml.g4dn.xlarge",
    }
    benchmark = resnet_cifar10_benchmark(params)
    logging.getLogger().setLevel(logging.INFO)

    print(f"Starting experiment ({args.seed}) of {experiment_tag}")

    script_path = benchmark["script"]
    trial_backend = SageMakerBackend(
        # we tune a PyTorch Framework from Sagemaker
        sm_estimator=PyTorch(
            entry_point=script_path.name,
            source_dir=str(script_path.parent),
            instance_type=params["instance_type"],
            instance_count=1,
            role=get_execution_role(),
            framework_version="1.7.1",
            py_version="py3",
            max_run=2 * args.max_wallclock_time,
            dependencies=[str(repository_root_path() / "benchmarking/")],
            disable_profiler=True,
            sagemaker_session=default_sagemaker_session(),
        ),
        # names of metrics to track. Each metric will be detected by Sagemaker if it is written in the
        # following form: "[RMSE]: 1.2", see in train_main_example how metrics are logged for an example
        metrics_names=[benchmark["metric"]],
    )

    scheduler = ASHA(
        benchmark["config_space"],
        type="stopping",
        search_options={"debug_log": True},
        metric=benchmark["metric"],
        mode=benchmark["mode"],
        resource_attr=benchmark["resource_attr"],
        max_resource_attr=benchmark["max_resource_attr"],
        random_seed=args.seed,
    )

    stop_criterion = StoppingCriterion(
        max_wallclock_time=args.max_wallclock_time,
    )
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=args.n_workers,
        sleep_time=5.0,
        tuner_name=experiment_tag,
        metadata={
            "seed": args.seed,
            "algorithm": "ASHA",
            "type": "stopping",
            "tag": experiment_tag,
            "benchmark": "resnet_cifar10",
            "n_workers": args.n_workers,
            "max_wallclock_time": args.max_wallclock_time,
        },
    )

    tuner.run()

File Path: benchmarking/nursery/remote_sm_backend/launch_remote.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
from tqdm import tqdm
from argparse import ArgumentParser
import os
import boto3

from sagemaker.pytorch import PyTorch

from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
)
import syne_tune
import benchmarking
from syne_tune.util import s3_experiment_path, random_string


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--experiment_tag",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--num_seeds",
        type=int,
        required=True,
        help="number of seeds to run",
    )
    parser.add_argument(
        "--start_seed",
        type=int,
        default=0,
        help="first seed to run",
    )
    parser.add_argument(
        "--n_workers",
        type=int,
        default=4,
        help="number of parallel workers",
    )
    parser.add_argument(
        "--max_wallclock_time",
        type=int,
        default=3 * 3600,
        help="maximum wallclock time of experiment",
    )
    args, _ = parser.parse_known_args()
    experiment_tag = args.experiment_tag
    suffix = random_string(4)

    if boto3.Session().region_name is None:
        os.environ["AWS_DEFAULT_REGION"] = "us-west-2"
    environment = {"AWS_DEFAULT_REGION": boto3.Session().region_name}

    for seed in tqdm(range(args.start_seed, args.num_seeds)):
        checkpoint_s3_uri = s3_experiment_path(
            tuner_name=str(seed), experiment_name=experiment_tag
        )
        sm_args = dict(
            entry_point="launch_experiment.py",
            source_dir=str(Path(__file__).parent),
            checkpoint_s3_uri=checkpoint_s3_uri,
            instance_type="ml.c5.4xlarge",
            instance_count=1,
            py_version="py3",
            framework_version="1.7.1",
            max_run=3600 * 72,
            role=get_execution_role(),
            dependencies=syne_tune.__path__ + benchmarking.__path__,
            disable_profiler=True,
            environment=environment,
        )

        sm_args["hyperparameters"] = {
            "experiment_tag": experiment_tag,
            "seed": seed,
            "n_workers": args.n_workers,
            "max_wallclock_time": args.max_wallclock_time,
        }
        print(
            f"{experiment_tag}-{seed}\n"
            f"hyperparameters = {sm_args['hyperparameters']}\n"
            f"Results written to {checkpoint_s3_uri}"
        )
        est = PyTorch(**sm_args)
        est.fit(job_name=f"{experiment_tag}-{seed}-{suffix}", wait=False)

    print(
        "\nLaunched all requested experiments. Once everything is done, use this "
        "command to sync result files from S3:\n"
        f"$ aws s3 sync {s3_experiment_path(experiment_name=experiment_tag)}/ "
        f'~/syne-tune/{experiment_tag}/ --exclude "*" '
        '--include "*metadata.json" --include "*results.csv.zip"'
    )

File Path: benchmarking/nursery/tuning_with_hyperparameter_file/training_script.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import json
import logging
from pathlib import Path

from syne_tune import Reporter
from argparse import ArgumentParser

report = Reporter()


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = ArgumentParser()
    parser.add_argument(f"--st_checkpoint_dir", type=str)
    args, _ = parser.parse_known_args()

    # gets hyperparameters that are written into {trial_path}/config.json
    # note: only works with LocalBackend for now.
    trial_path = Path(args.st_checkpoint_dir).parent
    with open(Path(args.st_checkpoint_dir).parent / "config.json", "r") as f:
        hyperparameters = json.load(f)

    report(error=hyperparameters["x"] ** 2)

File Path: benchmarking/nursery/tuning_with_hyperparameter_file/tuning_example.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example showing how to tune given a script ("training_script.py") that takes input hyperparameters
as a file rather than command line arguments.
Note that this approach only works with `LocalBackend` at the moment.
"""
from pathlib import Path

from syne_tune import Tuner, StoppingCriterion
from syne_tune.backend import LocalBackend
from syne_tune.config_space import randint
from syne_tune.optimizer.baselines import RandomSearch

if __name__ == "__main__":
    config_space = {"x": randint(0, 10)}
    tuner = Tuner(
        scheduler=RandomSearch(config_space=config_space, metric="error"),
        trial_backend=LocalBackend(
            entry_point=str(Path(__file__).parent / "training_script.py")
        ),
        stop_criterion=StoppingCriterion(max_wallclock_time=20),
        n_workers=2,
    )
    tuner.run()

File Path: benchmarking/training_scripts/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/training_scripts/distilbert_on_imdb/distilbert_on_imdb.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
DistilBERT fine-tuned on IMDB sentiment classification task
"""
import argparse
import logging
import time

from syne_tune import Reporter
from syne_tune.config_space import loguniform, add_to_argparse


METRIC_ACCURACY = "accuracy"

RESOURCE_ATTR = "step"

_config_space = {
    "learning_rate": loguniform(1e-6, 1e-4),
    "weight_decay": loguniform(1e-6, 1e-4),
}


def download_data(config):
    train_dataset, eval_dataset = load_dataset(
        "imdb", split=["train", "test"], cache_dir=config["dataset_path"]
    )
    return train_dataset, eval_dataset


def prepare_data(config, train_dataset, eval_dataset, seed=42):
    # Subsample data
    train_dataset = train_dataset.shuffle(seed=seed).select(
        range(config["n_train_data"])
    )
    eval_dataset = eval_dataset.shuffle(seed=seed).select(range(config["n_eval_data"]))

    # Tokenize data
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

    def tokenize(batch):
        return tokenizer(batch["text"], padding="max_length", truncation=True)

    train_dataset = train_dataset.map(tokenize, batched=True)
    eval_dataset = eval_dataset.map(tokenize, batched=True)

    return train_dataset, eval_dataset


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    metric = load_metric("accuracy")
    return metric.compute(predictions=predictions, references=labels)


def objective(config):
    trial_id = config.get("trial_id")
    debug_log = trial_id is not None

    # Download and prepare data
    train_dataset, eval_dataset = download_data(config)
    train_dataset, eval_dataset = prepare_data(config, train_dataset, eval_dataset)

    report = Reporter()

    # Do not want to count the time to download the dataset, which can be
    # substantial the first time
    ts_start = time.time()

    # Download model from Hugging Face model hub
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased", num_labels=2
    )

    # Define training args
    training_args = TrainingArguments(
        output_dir="./",
        num_train_epochs=config["epochs"],
        per_device_train_batch_size=config["train_batch_size"],
        per_device_eval_batch_size=config["eval_batch_size"],
        evaluation_strategy="steps",
        eval_steps=config["eval_interval"] // config["train_batch_size"],
        learning_rate=float(config["learning_rate"]),
        weight_decay=float(config["weight_decay"]),
        load_best_model_at_end=True,
        metric_for_best_model="eval_accuracy",
        greater_is_better=True,
        # avoid filling disk
        save_strategy="no",
    )

    # Create Trainer instance
    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # add a callback so that accuracy is sent to Syne Tune whenever it is computed
    class Callback(TrainerCallback):
        def __init__(self):
            self.step = 1

        def on_evaluate(self, args, state, control, metrics, **kwargs):
            # Feed the validation accuracy back to Tune
            report_dct = {
                RESOURCE_ATTR: self.step,
                METRIC_ACCURACY: metrics["eval_accuracy"],
            }
            report(**report_dct)
            self.step += 1

    trainer.add_callback(Callback())

    # Train model
    trainer.train()

    # Evaluate model
    eval_result = trainer.evaluate(eval_dataset=eval_dataset)
    eval_accuracy = eval_result["eval_accuracy"]

    elapsed_time = time.time() - ts_start

    if debug_log:
        print(
            "Trial {}: accuracy = {:.3f}, elapsed_time = {:.2f}".format(
                trial_id, eval_accuracy, elapsed_time
            ),
            flush=True,
        )


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from transformers import (
        AutoModelForSequenceClassification,
        Trainer,
        TrainingArguments,
        AutoTokenizer,
    )
    from transformers import TrainerCallback
    from datasets import load_dataset, load_metric

    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_path", type=str, required=True)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--train_batch_size", type=int, default=16)
    parser.add_argument("--eval_batch_size", type=int, default=32)
    parser.add_argument("--n_train_data", type=int, default=25000)
    parser.add_argument("--n_eval_data", type=int, default=5000)
    parser.add_argument("--eval_interval", type=int, default=5000)
    parser.add_argument("--trial_id", type=str)
    add_to_argparse(parser, _config_space)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: benchmarking/training_scripts/mlp_on_fashion_mnist/mlp_on_fashion_mnist.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Two-layer MLP trained on Fashion MNIST
"""
import os
import argparse
import logging
import time

from syne_tune import Reporter
from syne_tune.config_space import randint, uniform, loguniform, add_to_argparse
from benchmarking.utils import (
    resume_from_checkpointed_model,
    checkpoint_model_at_rung_level,
    add_checkpointing_to_argparse,
    pytorch_load_save_functions,
    parse_bool,
)


NUM_UNITS_1 = "n_units_1"

NUM_UNITS_2 = "n_units_2"

METRIC_NAME = "accuracy"

RESOURCE_ATTR = "epoch"

ELAPSED_TIME_ATTR = "elapsed_time"


_config_space = {
    NUM_UNITS_1: randint(4, 1024),
    NUM_UNITS_2: randint(4, 1024),
    "batch_size": randint(8, 128),
    "dropout_1": uniform(0, 0.99),
    "dropout_2": uniform(0, 0.99),
    "learning_rate": loguniform(1e-6, 1),
    "weight_decay": loguniform(1e-8, 1),
}


# Boilerplate for objective


def download_data(config):
    path = os.path.join(config["dataset_path"], "FashionMNIST")
    os.makedirs(path, exist_ok=True)
    # Lock protection is needed for backends which run multiple worker
    # processes on the same instance
    lock_path = os.path.join(path, "lock")
    lock = SoftFileLock(lock_path)
    try:
        with lock.acquire(timeout=120, poll_intervall=1):
            data_train = datasets.FashionMNIST(
                root=path, train=True, download=True, transform=transforms.ToTensor()
            )
    except Timeout:
        print(
            "WARNING: Could not obtain lock for dataset files. Trying anyway...",
            flush=True,
        )
        data_train = datasets.FashionMNIST(
            root=path, train=True, download=True, transform=transforms.ToTensor()
        )
    return data_train


def split_data(config, data_train):
    # We use 50000 samples for training and 10000 samples for validation
    indices = list(range(data_train.data.shape[0]))
    train_idx, valid_idx = indices[:50000], indices[50000:]
    train_sampler = SubsetRandomSampler(train_idx)
    valid_sampler = SubsetRandomSampler(valid_idx)
    batch_size = config["batch_size"]
    train_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=train_sampler, drop_last=True
    )
    valid_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=valid_sampler, drop_last=True
    )
    return train_loader, valid_loader


def model_and_optimizer(config):
    n_units_1 = config["n_units_1"]
    n_units_2 = config["n_units_2"]
    dropout_1 = config["dropout_1"]
    dropout_2 = config["dropout_2"]
    learning_rate = config["learning_rate"]
    weight_decay = config["weight_decay"]
    # Define the network architecture
    comp_list = [
        nn.Linear(28 * 28, n_units_1),
        nn.Dropout(p=dropout_1),
        nn.ReLU(),
        nn.Linear(n_units_1, n_units_2),
        nn.Dropout(p=dropout_2),
        nn.ReLU(),
        nn.Linear(n_units_2, 10),
    ]
    model = nn.Sequential(*comp_list)
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=weight_decay
    )
    criterion = nn.CrossEntropyLoss()
    return {"model": model, "optimizer": optimizer, "criterion": criterion}


def train_model(config, state, train_loader):
    model = state["model"]
    optimizer = state["optimizer"]
    criterion = state["criterion"]
    batch_size = config["batch_size"]
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data.view(batch_size, -1))
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()


def validate_model(config, state, valid_loader):
    batch_size = config["batch_size"]
    model = state["model"]
    model.eval()
    correct = 0
    total = 0
    for data, target in valid_loader:
        output = model(data.view(batch_size, -1))
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
    return correct / total  # Validation accuracy


def objective(config):
    report_current_best = parse_bool(config["report_current_best"])

    data_train = download_data(config)

    # Do not want to count the time to download the dataset, which can be
    # substantial the first time
    ts_start = time.time()
    report = Reporter()

    train_loader, valid_loader = split_data(config, data_train)

    state = model_and_optimizer(config)

    # Checkpointing
    load_model_fn, save_model_fn = pytorch_load_save_functions(
        {"model": state["model"], "optimizer": state["optimizer"]}
    )
    # Resume from checkpoint (optional)
    resume_from = resume_from_checkpointed_model(config, load_model_fn)

    current_best = None
    for epoch in range(resume_from + 1, config["epochs"] + 1):
        train_model(config, state, train_loader)
        accuracy = validate_model(config, state, valid_loader)
        elapsed_time = time.time() - ts_start
        if current_best is None or accuracy > current_best:
            current_best = accuracy
        # Feed the score back to Tune.
        objective = current_best if report_current_best else accuracy
        report(
            **{
                RESOURCE_ATTR: epoch,
                METRIC_NAME: objective,
                ELAPSED_TIME_ATTR: elapsed_time,
            }
        )
        # Write checkpoint (optional)
        checkpoint_model_at_rung_level(config, save_model_fn, epoch)


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from filelock import SoftFileLock, Timeout
    import torch
    import torch.nn as nn
    from torch.utils.data.sampler import SubsetRandomSampler
    from torchvision import datasets
    from torchvision import transforms

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_path", type=str, required=True)
    parser.add_argument("--report_current_best", type=str, default="False")
    add_to_argparse(parser, _config_space)
    add_checkpointing_to_argparse(parser)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: benchmarking/training_scripts/resnet_cifar10/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: benchmarking/training_scripts/resnet_cifar10/resnet_cifar10.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import os
import argparse
import logging
import time

from syne_tune import Reporter
from syne_tune.config_space import randint, uniform, loguniform, add_to_argparse
from benchmarking.utils import (
    resume_from_checkpointed_model,
    checkpoint_model_at_rung_level,
    add_checkpointing_to_argparse,
    pytorch_load_save_functions,
)


BATCH_SIZE_LOWER = 8

BATCH_SIZE_UPPER = 256

BATCH_SIZE_KEY = "batch_size"

METRIC_NAME = "objective"

RESOURCE_ATTR = "epoch"

ELAPSED_TIME_ATTR = "elapsed_time"


_config_space = {
    BATCH_SIZE_KEY: randint(BATCH_SIZE_LOWER, BATCH_SIZE_UPPER),
    "momentum": uniform(0, 0.99),
    "weight_decay": loguniform(1e-5, 1e-3),
    "lr": loguniform(1e-3, 0.1),
}


# ATTENTION: train_dataset, valid_dataset are both based on the CIFAR10
# training set, but train_dataset uses data augmentation. Make sure to
# only use disjoint parts for training and validation further down.
def get_CIFAR10(root):
    input_size = 32
    num_classes = 10
    normalize = [(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)]
    train_transform = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(*normalize),
        ]
    )
    local_path = os.path.join(root, "CIFAR10")
    train_dataset = datasets.CIFAR10(
        local_path, train=True, transform=train_transform, download=True
    )

    valid_transform = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize(*normalize),
        ]
    )
    valid_dataset = datasets.CIFAR10(
        local_path, train=True, transform=valid_transform, download=True
    )

    return input_size, num_classes, train_dataset, valid_dataset


def train(model, train_loader, optimizer):
    model.train()
    total_loss = []
    for data, target in tqdm(train_loader):
        if torch.cuda.is_available():
            data = data.cuda()
            target = target.cuda()
        optimizer.zero_grad()
        prediction = model(data)
        loss = F.nll_loss(prediction, target)
        loss.backward()
        optimizer.step()
        total_loss.append(loss.item())
    avg_loss = sum(total_loss) / len(total_loss)
    # print(f"Epoch: {epoch}:")
    # print(f"Train Set: Average Loss: {avg_loss:.2f}")


def valid(model, valid_loader):
    model.eval()
    loss = 0
    correct = 0
    for data, target in valid_loader:
        with torch.no_grad():
            if torch.cuda.is_available():
                data = data.cuda()
                target = target.cuda()
            prediction = model(data)
            loss += F.nll_loss(prediction, target, reduction="sum")
            prediction = prediction.max(1)[1]
            correct += prediction.eq(target.view_as(prediction)).sum().item()
    n_valid = len(valid_loader.sampler)
    loss /= n_valid
    percentage_correct = 100.0 * correct / n_valid
    # print(
    #    "Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)".format(
    #        loss, correct, len(valid_loader.sampler), percentage_correct
    #    )
    # )
    return loss, percentage_correct / 100


def objective(config):
    torch.manual_seed(np.random.randint(10000))
    batch_size = config["batch_size"]
    lr = config["lr"]
    momentum = config["momentum"]
    weight_decay = config["weight_decay"]
    num_gpus = config.get("num_gpus")
    if num_gpus is None:
        num_gpus = 1
    trial_id = config.get("trial_id")
    debug_log = trial_id is not None
    if debug_log:
        print("Trial {}: Starting evaluation".format(trial_id), flush=True)

    path = config["dataset_path"]
    os.makedirs(path, exist_ok=True)
    # Lock protection is needed for backends which run multiple worker
    # processes on the same instance
    lock_path = os.path.join(path, "lock")
    lock = SoftFileLock(lock_path)
    try:
        with lock.acquire(timeout=120, poll_intervall=1):
            input_size, num_classes, train_dataset, valid_dataset = get_CIFAR10(
                root=path
            )
    except Timeout:
        print(
            "WARNING: Could not obtain lock for dataset files. Trying anyway...",
            flush=True,
        )
        input_size, num_classes, train_dataset, valid_dataset = get_CIFAR10(root=path)

    # Do not want to count the time to download the dataset, which can be
    # substantial the first time
    ts_start = time.time()
    report = Reporter()

    indices = list(range(train_dataset.data.shape[0]))
    train_idx, valid_idx = indices[:40000], indices[40000:]
    train_sampler = SubsetRandomSampler(train_idx)
    valid_sampler = SubsetRandomSampler(valid_idx)
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        # shuffle=True,
        num_workers=0,
        sampler=train_sampler,
        pin_memory=True,
    )
    valid_loader = torch.utils.data.DataLoader(
        valid_dataset,
        batch_size=128,
        # shuffle=False,
        num_workers=0,
        sampler=valid_sampler,
        pin_memory=True,
    )

    model = Model()
    if torch.cuda.is_available():
        model = model.cuda()
        device = torch.device("cuda")
        # print(device)
        model = torch.nn.DataParallel(
            model, device_ids=[i for i in range(num_gpus)]
        ).to(device)
    milestones = [25, 40]
    optimizer = torch.optim.SGD(
        model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay
    )
    scheduler = torch.optim.lr_scheduler.MultiStepLR(
        optimizer, milestones=milestones, gamma=0.1
    )

    # Checkpointing
    load_model_fn, save_model_fn = pytorch_load_save_functions(
        {"model": model, "optimizer": optimizer, "lr_scheduler": scheduler}
    )
    # Resume from checkpoint (optional)
    resume_from = resume_from_checkpointed_model(config, load_model_fn)

    for epoch in range(resume_from + 1, config["epochs"] + 1):
        train(model, train_loader, optimizer)
        loss, y = valid(model, valid_loader)
        scheduler.step()
        elapsed_time = time.time() - ts_start

        # Feed the score back back to Tune.
        report(
            **{RESOURCE_ATTR: epoch, METRIC_NAME: y, ELAPSED_TIME_ATTR: elapsed_time}
        )

        # Write checkpoint (optional)
        checkpoint_model_at_rung_level(config, save_model_fn, epoch)

        if debug_log:
            print(
                "Trial {}: epoch = {}, objective = {:.3f}, elapsed_time = {:.2f}".format(
                    trial_id, epoch, y, elapsed_time
                ),
                flush=True,
            )


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from filelock import SoftFileLock, Timeout
    import numpy as np
    from tqdm import tqdm
    import torch
    import torch.nn.functional as F
    from torch.utils.data.sampler import SubsetRandomSampler
    from torchvision import datasets, transforms
    from torchvision.models import resnet18

    # Superclass reference torch.nn.Module requires torch to be defined
    class Model(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.resnet = resnet18(pretrained=False, num_classes=10)
            self.resnet.conv1 = torch.nn.Conv2d(
                3, 64, kernel_size=3, stride=1, padding=1, bias=False
            )
            self.resnet.maxpool = torch.nn.Identity()

        def forward(self, x):
            x = self.resnet(x)
            x = F.log_softmax(x, dim=1)
            return x

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_path", type=str, required=True)
    parser.add_argument("--num_gpus", type=int)
    parser.add_argument("--trial_id", type=str)
    add_to_argparse(parser, _config_space)
    add_checkpointing_to_argparse(parser)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: benchmarking/utils/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from benchmarking.utils.checkpoint import *  # noqa: F401
from benchmarking.utils.parse_bool import *  # noqa: F401
from benchmarking.utils.dict_get import *  # noqa: F401
from benchmarking.utils.get_cost_model import *  # noqa: F401

File Path: benchmarking/utils/checkpoint.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Callable, Any, Optional
import argparse
import os

from syne_tune.constants import ST_CHECKPOINT_DIR

__all__ = [
    "add_checkpointing_to_argparse",
    "resume_from_checkpointed_model",
    "checkpoint_model_at_rung_level",
    "pytorch_load_save_functions",
]


def add_checkpointing_to_argparse(parser: argparse.ArgumentParser):
    """
    To be called for the argument parser in the endpoint script.
    Arguments added here are optional. If checkpointing is not supported,
    they are simply not parsed.

    :param parser:
    """
    parser.add_argument(f"--{ST_CHECKPOINT_DIR}", type=str)


def resume_from_checkpointed_model(
    config: Dict, load_model_fn: Callable[[str], int]
) -> int:
    """
    Checks whether there is a checkpoint to be resumed from. If so, the
    checkpoint is loaded by calling `load_model_fn`. This function takes
    a local pathname (to which it appends a filename). It returns
    resume_from, the resource value (e.g., epoch) the checkpoint was written
    at. If it fails to load the checkpoint, it may return 0. This skips
    resuming from a checkpoint. This resume_from value is returned.

    If checkpointing is not supported in `config`, or no checkpoint is
    found, resume_from = 0 is returned.

    :param config:
    :param load_model_fn:
    :return: resume_from (0 if no checkpoint has been loaded)
    """
    resume_from = 0
    local_path = config.get(ST_CHECKPOINT_DIR)
    if local_path is not None and os.path.exists(local_path):
        resume_from = load_model_fn(local_path)
        trial_id = config.get("trial_id")
        if trial_id is not None:
            print(
                f"Trial {trial_id}: Loading checkpoint [resume_from = "
                f"{resume_from}, local_path = {local_path}]"
            )
    return resume_from


def checkpoint_model_at_rung_level(
    config: Dict, save_model_fn: Callable[[str, int], Any], resource: int
):
    """
    If checkpointing is supported, checks whether a checkpoint is to be
    written. This is the case if the checkpoint dir is set in `config`.
    A checkpoint is written by calling `save_model_fn`, passing the
    local pathname and resource.

    Note: Why is `resource` passed here? In the future, we want to support
    writing checkpoints only for certain resource levels. This is useful if
    writing the checkpoint is expensive compared to the time needed to
    run one resource unit.

    :param config:
    :param save_model_fn:
    :param resource:
    """
    local_path = config.get(ST_CHECKPOINT_DIR)
    if local_path is not None:
        save_model_fn(local_path, resource)
        trial_id = config.get("trial_id")
        if trial_id is not None:
            print(
                f"Trial {trial_id}: Saving checkpoint [resource = "
                f"{resource}, local_path = {local_path}]"
            )


RESOURCE_NAME = "st_resource"

STATE_DICT_PREFIX = "st_state_dict_"

MUTABLE_STATE_PREFIX = "st_mutable_"


def pytorch_load_save_functions(
    state_dict_objects: dict,
    mutable_state: Optional[dict] = None,
    fname: str = "checkpoint.json",
):
    """
    Provides default `load_model_fn`, `save_model_fn` functions for standard
    PyTorch models (arguments to `resume_from_checkpointed_model`,
    `checkpoint_model_at_rung_level`.

    :param state_dict_objects: Dict of PyTorch objects implementing `state_dict`
        and `load_state_dict`
    :param mutable_state: Optional. Additional dict with elementary value
        types
    :param fname: Name of local file (path is taken from config)
    :return: load_model_fn, save_model_fn
    """
    import torch

    def load_model_fn(local_path: str) -> int:
        _mutable_state, local_filename = _common_init(local_path)
        try:
            checkpoint = torch.load(local_filename)
            resume_from = int(checkpoint[RESOURCE_NAME])
            for k, v in state_dict_objects.items():
                v.load_state_dict(checkpoint[STATE_DICT_PREFIX + k])
            for k in _mutable_state:
                v = checkpoint[MUTABLE_STATE_PREFIX + k]
                v_old = _mutable_state.get(k)
                if v_old is not None:
                    v = type(v_old)(v)
                _mutable_state[k] = v
        except Exception:
            resume_from = 0
        return resume_from

    def save_model_fn(local_path: str, resource: int):
        os.makedirs(local_path, exist_ok=True)
        _mutable_state, local_filename = _common_init(local_path)
        local_filename = os.path.join(local_path, fname)
        checkpoint = {
            STATE_DICT_PREFIX + k: v.state_dict() for k, v in state_dict_objects.items()
        }
        checkpoint[RESOURCE_NAME] = resource
        for k, v in _mutable_state.items():
            checkpoint[MUTABLE_STATE_PREFIX + k] = v
        torch.save(checkpoint, local_filename)

    def _common_init(local_path: str) -> (dict, str):
        if mutable_state is None:
            _mutable_state = dict()
        else:
            _mutable_state = mutable_state
        local_filename = os.path.join(local_path, fname)
        return _mutable_state, local_filename

    return load_model_fn, save_model_fn

File Path: benchmarking/utils/dict_get.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

__all__ = ["dict_get"]


def dict_get(params: dict, key: str, default):
    """
    Returns `params[key]` if this exists and is not None, and `default` otherwise.
    Note that this is not the same as `params.get(key, default)`. Namely, if `params[key]`
    is equal to None, this would return None, but this method returns `default`.

    This function is particularly helpful when dealing with a dict returned by
    :class:`argparse.ArgumentParser`. Whenever `key` is added as argument to the parser,
    but a value is not provided, this leads to `params[key] = None`.

    """
    v = params.get(key)
    return default if v is None else v

File Path: benchmarking/utils/get_cost_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Tuple

__all__ = ["get_cost_model_for_batch_size"]


def get_cost_model_for_batch_size(
    params: dict, batch_size_key: str, batch_size_range: Tuple[int, int]
):
    """
    Returns cost model depending on the batch size only.

    :param params: Command line arguments
    :param batch_size_key: Name of batch size entry in config
    :param batch_size_range: (lower, upper) for batch size, both sides are
        inclusive
    :return: Cost model (or None if dependencies cannot be imported)

    """
    try:
        cost_model_type = params.get("cost_model_type")
        if cost_model_type is None:
            cost_model_type = "quadratic_spline"
        if cost_model_type == "biasonly":
            from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.linear_cost_model import (
                BiasOnlyLinearCostModel,
            )

            cost_model = BiasOnlyLinearCostModel()
        else:
            from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.sklearn_cost_model import (
                UnivariateSplineCostModel,
            )

            def scalar_attribute(config_dct):
                return float(config_dct[batch_size_key])

            assert cost_model_type in {
                "quadratic_spline",
                "cubic_spline",
            }, f"cost_model_type = '{cost_model_type}' is not supported"
            cost_model = UnivariateSplineCostModel(
                scalar_attribute=scalar_attribute,
                input_range=batch_size_range,
                spline_degree=(2 if cost_model_type[0] == "q" else 3),
            )
        return cost_model
    except Exception:
        return None

File Path: benchmarking/utils/parse_bool.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

__all__ = ["parse_bool"]


def parse_bool(val: str) -> bool:
    val = val.upper()
    if val == "TRUE":
        return True
    else:
        assert val == "FALSE", f"val = '{val}' is not a boolean value"
        return False

File Path: docs/tutorials/basics/scripts/launch_asha_promotion.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 4
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    # - Metrics need to be reported after each epoch, `resource_attr` must match
    #   what is reported in the training script
    entry_point = str(Path(__file__).parent / "traincode_report_withcheckpointing.py")
    mode = "max"
    metric = "accuracy"
    resource_attr = "epoch"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

    # Additional fixed parameters
    config_space.update(
        {
            max_resource_attr: max_resource_level,
            "dataset_path": "./",
        }
    )

    # Local back-end: Responsible for scheduling trials
    # The local back-end runs trials as sub-processes on a single instance
    trial_backend = LocalBackend(entry_point=entry_point)

    # Scheduler:
    # 'HyperbandScheduler' runs asynchronous successive halving, or Hyperband.
    # It starts a trial whenever a worker is free.
    # The 'promotion' variant pauses each trial at certain resource levels
    # (called rungs). Trials which outperform others at the same rung, are
    # promoted later on, to run to the next higher rung.
    # We configure this scheduler with random search: configurations for new
    # trials are drawn at random
    searcher = "random"
    scheduler = HyperbandScheduler(
        config_space,
        type="promotion",
        searcher=searcher,
        grace_period=1,
        reduction_factor=3,
        resource_attr=resource_attr,
        max_resource_attr=max_resource_attr,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    # The experiment is stopped after `max_wallclock_time` seconds
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

    # Everything comes together in the tuner
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: docs/tutorials/basics/scripts/launch_asha_stopping.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 4
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    # - Metrics need to be reported after each epoch, `resource_attr` must match
    #   what is reported in the training script
    # [1]
    entry_point = str(Path(__file__).parent / "traincode_report_eachepoch.py")
    mode = "max"
    metric = "accuracy"
    resource_attr = "epoch"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

    # Additional fixed parameters
    config_space.update(
        {
            max_resource_attr: max_resource_level,
            "dataset_path": "./",
        }
    )

    # Local back-end: Responsible for scheduling trials
    # The local back-end runs trials as sub-processes on a single instance
    trial_backend = LocalBackend(entry_point=entry_point)

    # Scheduler:
    # 'HyperbandScheduler' runs asynchronous successive halving, or Hyperband.
    # It starts a trial whenever a worker is free.
    # The 'stopping' variant stops trials which underperform compared to others
    # at certain resource levels (called rungs).
    # We configure this scheduler with random search: configurations for new
    # trials are drawn at random
    # [2]
    searcher = "random"
    scheduler = HyperbandScheduler(
        config_space,
        type="stopping",
        searcher=searcher,
        grace_period=1,  # [3]
        reduction_factor=3,
        resource_attr=resource_attr,
        max_resource_attr=max_resource_attr,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    # The experiment is stopped after `max_wallclock_time` seconds
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

    # Everything comes together in the tuner
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: docs/tutorials/basics/scripts/launch_bayesopt.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import FIFOScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 4
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    entry_point = str(Path(__file__).parent / "traincode_report_end.py")
    mode = "max"
    metric = "accuracy"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

    # Additional fixed parameters
    config_space.update(
        {
            max_resource_attr: max_resource_level,
            "dataset_path": "./",
        }
    )

    # Local back-end: Responsible for scheduling trials
    # The local back-end runs trials as sub-processes on a single instance
    trial_backend = LocalBackend(entry_point=entry_point)

    # Scheduler:
    # The `FIFOScheduler` starts a trial whenever a worker is free. It does
    # not stop or pause trials, they always run to the end.
    # We configure this scheduler with Bayesian optimization: configurations
    # for new trials are selected by optimizing an acquisition function based
    # on a Gaussian process surrogate model
    # [1]
    searcher = "bayesopt"
    search_options = {"num_init_random": n_workers + 2}
    scheduler = FIFOScheduler(
        config_space,
        searcher=searcher,
        search_options=search_options,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    # The experiment is stopped after `max_wallclock_time` seconds
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

    # Everything comes together in the tuner
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: docs/tutorials/basics/scripts/launch_configspace_only.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from syne_tune.config_space import randint, uniform, loguniform


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    # [1]
    random_seed = 31415927
    n_workers = 4
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    # [2]
    entry_point = str(Path(__file__).parent / "traincode_report_end.py")
    mode = "max"
    metric = "accuracy"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    # [3]
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

File Path: docs/tutorials/basics/scripts/launch_mobster_promotion.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 4
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    # - Metrics need to be reported after each epoch, `resource_attr` must match
    #   what is reported in the training script
    entry_point = str(Path(__file__).parent / "traincode_report_withcheckpointing.py")
    mode = "max"
    metric = "accuracy"
    resource_attr = "epoch"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

    # Additional fixed parameters
    config_space.update(
        {
            max_resource_attr: max_resource_level,
            "dataset_path": "./",
        }
    )

    # Local back-end: Responsible for scheduling trials
    # The local back-end runs trials as sub-processes on a single instance
    trial_backend = LocalBackend(entry_point=entry_point)

    # Scheduler:
    # 'HyperbandScheduler' runs asynchronous successive halving, or Hyperband.
    # It starts a trial whenever a worker is free.
    # The 'promotion' variant pauses each trial at certain resource levels
    # (called rungs). Trials which outperform others at the same rung, are
    # promoted later on, to run to the next higher rung.
    # We configure this scheduler with Bayesian optimization: configurations
    # for new trials are selected by optimizing an acquisition function based
    # on a Gaussian process surrogate model. The latter models learning curves
    # f(x, r), x the configuration, r the number of epochs done, not just final
    # values f(x).
    searcher = "bayesopt"
    search_options = {
        "num_init_random": n_workers + 2,
        "gp_resource_kernel": "exp-decay-sum",  # GP surrogate model
    }
    scheduler = HyperbandScheduler(
        config_space,
        type="promotion",
        searcher=searcher,
        search_options=search_options,
        grace_period=1,
        reduction_factor=3,
        resource_attr=resource_attr,
        max_resource_attr=max_resource_attr,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    # The experiment is stopped after `max_wallclock_time` seconds
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

    # Everything comes together in the tuner
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: docs/tutorials/basics/scripts/launch_mobster_stopping.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 4
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    # - Metrics need to be reported after each epoch, `resource_attr` must match
    #   what is reported in the training script
    entry_point = str(Path(__file__).parent / "traincode_report_eachepoch.py")
    mode = "max"
    metric = "accuracy"
    resource_attr = "epoch"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

    # Additional fixed parameters
    config_space.update(
        {
            max_resource_attr: max_resource_level,
            "dataset_path": "./",
        }
    )

    # Local back-end: Responsible for scheduling trials
    # The local back-end runs trials as sub-processes on a single instance
    trial_backend = LocalBackend(entry_point=entry_point)

    # Scheduler:
    # 'HyperbandScheduler' runs asynchronous successive halving, or Hyperband.
    # It starts a trial whenever a worker is free.
    # The 'stopping' variant stops trials which underperform compared to others
    # at certain resource levels (called rungs).
    # We configure this scheduler with Bayesian optimization: configurations
    # for new trials are selected by optimizing an acquisition function based
    # on a Gaussian process surrogate model. The latter models learning curves
    # f(x, r), x the configuration, r the number of epochs done, not just final
    # values f(x).
    # [1]
    searcher = "bayesopt"
    search_options = {
        "num_init_random": n_workers + 2,
        "gp_resource_kernel": "exp-decay-sum",  # GP surrogate model
    }
    scheduler = HyperbandScheduler(
        config_space,
        type="stopping",
        searcher=searcher,
        search_options=search_options,
        grace_period=1,
        reduction_factor=3,
        resource_attr=resource_attr,
        max_resource_attr=max_resource_attr,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    # The experiment is stopped after `max_wallclock_time` seconds
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

    # Everything comes together in the tuner
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: docs/tutorials/basics/scripts/launch_randomsearch.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import FIFOScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 4
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    entry_point = str(Path(__file__).parent / "traincode_report_end.py")
    mode = "max"
    metric = "accuracy"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

    # Additional fixed parameters
    # [1]
    config_space.update(
        {
            max_resource_attr: max_resource_level,
            "dataset_path": "./",
        }
    )

    # Local back-end: Responsible for scheduling trials
    # The local back-end runs trials as sub-processes on a single instance
    # [2]
    trial_backend = LocalBackend(entry_point=entry_point)

    # Scheduler:
    # The `FIFOScheduler` starts a trial whenever a worker is free. It does
    # not stop or pause trials, they always run to the end.
    # We configure this scheduler with random search: configurations for new
    # trials are drawn at random
    # [3]
    searcher = "random"
    scheduler = FIFOScheduler(
        config_space,
        searcher=searcher,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    # The experiment is stopped after `max_wallclock_time` seconds
    # [4]
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

    # Everything comes together in the tuner
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: docs/tutorials/basics/scripts/launch_sagemaker_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import logging
from pathlib import Path

from sagemaker.pytorch import PyTorch

from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.backend import SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion
from syne_tune.util import repository_root_path


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 8
    max_wallclock_time = 3 * 3600  # Run for 3 hours
    max_resource_level = 81  # Maximum number of training epochs

    # Here, we specify the training script we want to tune
    # - `mode` and `metric` must match what is reported in the training script
    # - Metrics need to be reported after each epoch, `resource_attr` must match
    #   what is reported in the training script
    entry_point = str(Path(__file__).parent / "traincode_report_withcheckpointing.py")
    mode = "max"
    metric = "accuracy"
    resource_attr = "epoch"
    max_resource_attr = "epochs"

    # Search space (or configuration space)
    # For each tunable parameter, need to define type, range, and encoding
    # (linear, logarithmic)
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "weight_decay": loguniform(1e-8, 1),
    }

    # Additional fixed parameters
    config_space.update(
        {
            max_resource_attr: max_resource_level,
            "dataset_path": "./",
        }
    )

    # SageMaker back-end: Responsible for scheduling trials
    # Each trial is run as a separate SageMaker training job. This is useful for
    # expensive workloads, where all resources of an instance (or several ones)
    # are used for training. On the other hand, training job start-up overhead
    # is incurred for every trial.
    # [1]
    trial_backend = SageMakerBackend(
        # we tune a PyTorch Framework from Sagemaker
        sm_estimator=PyTorch(
            entry_point=entry_point,
            instance_type="ml.m4.xlarge",
            instance_count=1,
            role=get_execution_role(),
            dependencies=[str(repository_root_path() / "benchmarking")],
            max_run=int(1.05 * max_wallclock_time),
            framework_version="1.7.1",
            py_version="py3",
            disable_profiler=True,
            sagemaker_session=default_sagemaker_session(),
        ),
        metrics_names=[metric],
    )

    # Scheduler:
    # 'HyperbandScheduler' runs asynchronous successive halving, or Hyperband.
    # It starts a trial whenever a worker is free.
    # We configure this scheduler with Bayesian optimization: configurations
    # for new trials are selected by optimizing an acquisition function based
    # on a Gaussian process surrogate model. The latter models learning curves
    # f(x, r), x the configuration, r the number of epochs done, not just final
    # values f(x).
    searcher = "bayesopt"
    search_options = {
        "num_init_random": n_workers + 2,
        "gp_resource_kernel": "exp-decay-sum",  # GP surrogate model
    }
    scheduler = HyperbandScheduler(
        config_space,
        type="stopping",
        searcher=searcher,
        search_options=search_options,
        grace_period=1,
        reduction_factor=3,
        resource_attr=resource_attr,
        max_resource_attr=max_resource_attr,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    # The experiment is stopped after `max_wallclock_time` seconds
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

    # Everything comes together in the tuner
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: docs/tutorials/basics/scripts/traincode_notune.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import os
import argparse
import logging


# Boilerplate for objective


def download_data(config):
    path = os.path.join(config["dataset_path"], "FashionMNIST")
    os.makedirs(path, exist_ok=True)
    # Lock protection is needed for backends which run multiple worker
    # processes on the same instance
    lock_path = os.path.join(path, "lock")
    lock = SoftFileLock(lock_path)
    try:
        with lock.acquire(timeout=120, poll_intervall=1):
            data_train = datasets.FashionMNIST(
                root=path, train=True, download=True, transform=transforms.ToTensor()
            )
    except Timeout:
        print(
            "WARNING: Could not obtain lock for dataset files. Trying anyway...",
            flush=True,
        )
        data_train = datasets.FashionMNIST(
            root=path, train=True, download=True, transform=transforms.ToTensor()
        )
    return data_train


def split_data(config, data_train):
    # We use 50000 samples for training and 10000 samples for validation
    indices = list(range(data_train.data.shape[0]))
    train_idx, valid_idx = indices[:50000], indices[50000:]
    train_sampler = SubsetRandomSampler(train_idx)
    valid_sampler = SubsetRandomSampler(valid_idx)
    batch_size = config["batch_size"]
    train_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=train_sampler, drop_last=True
    )
    valid_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=valid_sampler, drop_last=True
    )
    return train_loader, valid_loader


# [3]
def model_and_optimizer(config):
    n_units_1 = config["n_units_1"]
    n_units_2 = config["n_units_2"]
    dropout_1 = config["dropout_1"]
    dropout_2 = config["dropout_2"]
    learning_rate = config["learning_rate"]
    weight_decay = config["weight_decay"]
    # Define the network architecture
    comp_list = [
        nn.Linear(28 * 28, n_units_1),
        nn.Dropout(p=dropout_1),
        nn.ReLU(),
        nn.Linear(n_units_1, n_units_2),
        nn.Dropout(p=dropout_2),
        nn.ReLU(),
        nn.Linear(n_units_2, 10),
    ]
    model = nn.Sequential(*comp_list)
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=weight_decay
    )
    criterion = nn.CrossEntropyLoss()
    return {"model": model, "optimizer": optimizer, "criterion": criterion}


def train_model(config, state, train_loader):
    model = state["model"]
    optimizer = state["optimizer"]
    criterion = state["criterion"]
    batch_size = config["batch_size"]
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data.view(batch_size, -1))
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()


def validate_model(config, state, valid_loader):
    batch_size = config["batch_size"]
    model = state["model"]
    model.eval()
    correct = 0
    total = 0
    for data, target in valid_loader:
        output = model(data.view(batch_size, -1))
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
    return correct / total  # Validation accuracy


# [1]
def objective(config):
    # Download data
    data_train = download_data(config)

    # Split into training and validation set
    train_loader, valid_loader = split_data(config, data_train)

    # Create model and optimizer
    state = model_and_optimizer(config)

    # Training loop
    for epoch in range(1, config["epochs"] + 1):
        train_model(config, state, train_loader)

    accuracy = validate_model(config, state, valid_loader)
    print(
        f"Model trained for {config['epochs']} epochs:\n"
        f"Validation accuracy = {accuracy}"
    )


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from filelock import SoftFileLock, Timeout
    import torch
    import torch.nn as nn
    from torch.utils.data.sampler import SubsetRandomSampler
    from torchvision import datasets
    from torchvision import transforms

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_path", type=str, required=True)
    # [2]
    # Hyperparameters
    parser.add_argument("--n_units_1", type=int, required=True)
    parser.add_argument("--n_units_2", type=int, required=True)
    parser.add_argument("--batch_size", type=int, required=True)
    parser.add_argument("--dropout_1", type=float, required=True)
    parser.add_argument("--dropout_2", type=float, required=True)
    parser.add_argument("--learning_rate", type=float, required=True)
    parser.add_argument("--weight_decay", type=float, required=True)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: docs/tutorials/basics/scripts/traincode_report_eachepoch.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import os
import argparse
import logging

from syne_tune import Reporter


# Boilerplate for objective


def download_data(config):
    path = os.path.join(config["dataset_path"], "FashionMNIST")
    os.makedirs(path, exist_ok=True)
    # Lock protection is needed for backends which run multiple worker
    # processes on the same instance
    lock_path = os.path.join(path, "lock")
    lock = SoftFileLock(lock_path)
    try:
        with lock.acquire(timeout=120, poll_intervall=1):
            data_train = datasets.FashionMNIST(
                root=path, train=True, download=True, transform=transforms.ToTensor()
            )
    except Timeout:
        print(
            "WARNING: Could not obtain lock for dataset files. Trying anyway...",
            flush=True,
        )
        data_train = datasets.FashionMNIST(
            root=path, train=True, download=True, transform=transforms.ToTensor()
        )
    return data_train


def split_data(config, data_train):
    # We use 50000 samples for training and 10000 samples for validation
    indices = list(range(data_train.data.shape[0]))
    train_idx, valid_idx = indices[:50000], indices[50000:]
    train_sampler = SubsetRandomSampler(train_idx)
    valid_sampler = SubsetRandomSampler(valid_idx)
    batch_size = config["batch_size"]
    train_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=train_sampler, drop_last=True
    )
    valid_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=valid_sampler, drop_last=True
    )
    return train_loader, valid_loader


def model_and_optimizer(config):
    n_units_1 = config["n_units_1"]
    n_units_2 = config["n_units_2"]
    dropout_1 = config["dropout_1"]
    dropout_2 = config["dropout_2"]
    learning_rate = config["learning_rate"]
    weight_decay = config["weight_decay"]
    # Define the network architecture
    comp_list = [
        nn.Linear(28 * 28, n_units_1),
        nn.Dropout(p=dropout_1),
        nn.ReLU(),
        nn.Linear(n_units_1, n_units_2),
        nn.Dropout(p=dropout_2),
        nn.ReLU(),
        nn.Linear(n_units_2, 10),
    ]
    model = nn.Sequential(*comp_list)
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=weight_decay
    )
    criterion = nn.CrossEntropyLoss()
    return {"model": model, "optimizer": optimizer, "criterion": criterion}


def train_model(config, state, train_loader):
    model = state["model"]
    optimizer = state["optimizer"]
    criterion = state["criterion"]
    batch_size = config["batch_size"]
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data.view(batch_size, -1))
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()


def validate_model(config, state, valid_loader):
    batch_size = config["batch_size"]
    model = state["model"]
    model.eval()
    correct = 0
    total = 0
    for data, target in valid_loader:
        output = model(data.view(batch_size, -1))
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
    return correct / total  # Validation accuracy


def objective(config):
    # Download data
    data_train = download_data(config)

    # Report results to Syne Tune
    report = Reporter()

    # Split into training and validation set
    train_loader, valid_loader = split_data(config, data_train)

    # Create model and optimizer
    state = model_and_optimizer(config)

    # Training loop
    for epoch in range(1, config["epochs"] + 1):
        train_model(config, state, train_loader)
        accuracy = validate_model(config, state, valid_loader)
        # Report validation accuracy to Syne Tune
        # [1]
        report(epoch=epoch, accuracy=accuracy)


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from filelock import SoftFileLock, Timeout
    import torch
    import torch.nn as nn
    from torch.utils.data.sampler import SubsetRandomSampler
    from torchvision import datasets
    from torchvision import transforms

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_path", type=str, required=True)
    # Hyperparameters
    parser.add_argument("--n_units_1", type=int, required=True)
    parser.add_argument("--n_units_2", type=int, required=True)
    parser.add_argument("--batch_size", type=int, required=True)
    parser.add_argument("--dropout_1", type=float, required=True)
    parser.add_argument("--dropout_2", type=float, required=True)
    parser.add_argument("--learning_rate", type=float, required=True)
    parser.add_argument("--weight_decay", type=float, required=True)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: docs/tutorials/basics/scripts/traincode_report_end.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import os
import argparse
import logging

from syne_tune import Reporter


# Boilerplate for objective


def download_data(config):
    path = os.path.join(config["dataset_path"], "FashionMNIST")
    os.makedirs(path, exist_ok=True)
    # Lock protection is needed for backends which run multiple worker
    # processes on the same instance
    lock_path = os.path.join(path, "lock")
    lock = SoftFileLock(lock_path)
    try:
        with lock.acquire(timeout=120, poll_intervall=1):
            data_train = datasets.FashionMNIST(
                root=path, train=True, download=True, transform=transforms.ToTensor()
            )
    except Timeout:
        print(
            "WARNING: Could not obtain lock for dataset files. Trying anyway...",
            flush=True,
        )
        data_train = datasets.FashionMNIST(
            root=path, train=True, download=True, transform=transforms.ToTensor()
        )
    return data_train


def split_data(config, data_train):
    # We use 50000 samples for training and 10000 samples for validation
    indices = list(range(data_train.data.shape[0]))
    train_idx, valid_idx = indices[:50000], indices[50000:]
    train_sampler = SubsetRandomSampler(train_idx)
    valid_sampler = SubsetRandomSampler(valid_idx)
    batch_size = config["batch_size"]
    train_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=train_sampler, drop_last=True
    )
    valid_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=valid_sampler, drop_last=True
    )
    return train_loader, valid_loader


def model_and_optimizer(config):
    n_units_1 = config["n_units_1"]
    n_units_2 = config["n_units_2"]
    dropout_1 = config["dropout_1"]
    dropout_2 = config["dropout_2"]
    learning_rate = config["learning_rate"]
    weight_decay = config["weight_decay"]
    # Define the network architecture
    comp_list = [
        nn.Linear(28 * 28, n_units_1),
        nn.Dropout(p=dropout_1),
        nn.ReLU(),
        nn.Linear(n_units_1, n_units_2),
        nn.Dropout(p=dropout_2),
        nn.ReLU(),
        nn.Linear(n_units_2, 10),
    ]
    model = nn.Sequential(*comp_list)
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=weight_decay
    )
    criterion = nn.CrossEntropyLoss()
    return {"model": model, "optimizer": optimizer, "criterion": criterion}


def train_model(config, state, train_loader):
    model = state["model"]
    optimizer = state["optimizer"]
    criterion = state["criterion"]
    batch_size = config["batch_size"]
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data.view(batch_size, -1))
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()


def validate_model(config, state, valid_loader):
    batch_size = config["batch_size"]
    model = state["model"]
    model.eval()
    correct = 0
    total = 0
    for data, target in valid_loader:
        output = model(data.view(batch_size, -1))
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
    return correct / total  # Validation accuracy


def objective(config):
    # Download data
    data_train = download_data(config)

    # Report results to Syne Tune
    report = Reporter()

    # Split into training and validation set
    train_loader, valid_loader = split_data(config, data_train)

    # Create model and optimizer
    state = model_and_optimizer(config)

    # Training loop
    for epoch in range(1, config["epochs"] + 1):
        train_model(config, state, train_loader)

    # Report validation accuracy to Syne Tune
    # [1]
    accuracy = validate_model(config, state, valid_loader)
    report(accuracy=accuracy)


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from filelock import SoftFileLock, Timeout
    import torch
    import torch.nn as nn
    from torch.utils.data.sampler import SubsetRandomSampler
    from torchvision import datasets
    from torchvision import transforms

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_path", type=str, required=True)
    # Hyperparameters
    parser.add_argument("--n_units_1", type=int, required=True)
    parser.add_argument("--n_units_2", type=int, required=True)
    parser.add_argument("--batch_size", type=int, required=True)
    parser.add_argument("--dropout_1", type=float, required=True)
    parser.add_argument("--dropout_2", type=float, required=True)
    parser.add_argument("--learning_rate", type=float, required=True)
    parser.add_argument("--weight_decay", type=float, required=True)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: docs/tutorials/basics/scripts/traincode_report_withcheckpointing.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# If you like to run the code linked in this tutorial, please make sure to use
# the current `PyPI` release. If you cloned the source repository, this is
# obtained as follows:
#
# ```bash
# git checkout -b basic_tutorial v0.11
# ```
#
# This gives you a local branch `basic_tutorial`, in which you can play around
# with the code.
import os
import argparse
import logging

from syne_tune import Reporter
from benchmarking.utils import (
    resume_from_checkpointed_model,
    checkpoint_model_at_rung_level,
    add_checkpointing_to_argparse,
    pytorch_load_save_functions,
)


# Boilerplate for objective


def download_data(config):
    path = os.path.join(config["dataset_path"], "FashionMNIST")
    os.makedirs(path, exist_ok=True)
    # Lock protection is needed for backends which run multiple worker
    # processes on the same instance
    lock_path = os.path.join(path, "lock")
    lock = SoftFileLock(lock_path)
    try:
        with lock.acquire(timeout=120, poll_intervall=1):
            data_train = datasets.FashionMNIST(
                root=path, train=True, download=True, transform=transforms.ToTensor()
            )
    except Timeout:
        print(
            "WARNING: Could not obtain lock for dataset files. Trying anyway...",
            flush=True,
        )
        data_train = datasets.FashionMNIST(
            root=path, train=True, download=True, transform=transforms.ToTensor()
        )
    return data_train


def split_data(config, data_train):
    # We use 50000 samples for training and 10000 samples for validation
    indices = list(range(data_train.data.shape[0]))
    train_idx, valid_idx = indices[:50000], indices[50000:]
    train_sampler = SubsetRandomSampler(train_idx)
    valid_sampler = SubsetRandomSampler(valid_idx)
    batch_size = config["batch_size"]
    train_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=train_sampler, drop_last=True
    )
    valid_loader = torch.utils.data.DataLoader(
        data_train, batch_size=batch_size, sampler=valid_sampler, drop_last=True
    )
    return train_loader, valid_loader


def model_and_optimizer(config):
    n_units_1 = config["n_units_1"]
    n_units_2 = config["n_units_2"]
    dropout_1 = config["dropout_1"]
    dropout_2 = config["dropout_2"]
    learning_rate = config["learning_rate"]
    weight_decay = config["weight_decay"]
    # Define the network architecture
    comp_list = [
        nn.Linear(28 * 28, n_units_1),
        nn.Dropout(p=dropout_1),
        nn.ReLU(),
        nn.Linear(n_units_1, n_units_2),
        nn.Dropout(p=dropout_2),
        nn.ReLU(),
        nn.Linear(n_units_2, 10),
    ]
    model = nn.Sequential(*comp_list)
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=weight_decay
    )
    criterion = nn.CrossEntropyLoss()
    return {"model": model, "optimizer": optimizer, "criterion": criterion}


def train_model(config, state, train_loader):
    model = state["model"]
    optimizer = state["optimizer"]
    criterion = state["criterion"]
    batch_size = config["batch_size"]
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data.view(batch_size, -1))
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()


def validate_model(config, state, valid_loader):
    batch_size = config["batch_size"]
    model = state["model"]
    model.eval()
    correct = 0
    total = 0
    for data, target in valid_loader:
        output = model(data.view(batch_size, -1))
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
    return correct / total  # Validation accuracy


def objective(config):
    # Download data
    data_train = download_data(config)

    # Report results to Syne Tune
    report = Reporter()

    # Split into training and validation set
    train_loader, valid_loader = split_data(config, data_train)

    # Create model and optimizer
    state = model_and_optimizer(config)

    # Checkpointing
    # [4]
    load_model_fn, save_model_fn = pytorch_load_save_functions(
        {"model": state["model"], "optimizer": state["optimizer"]}
    )
    # Resume from checkpoint (optional)
    # [2]
    resume_from = resume_from_checkpointed_model(config, load_model_fn)

    # Training loop
    for epoch in range(resume_from + 1, config["epochs"] + 1):
        train_model(config, state, train_loader)
        accuracy = validate_model(config, state, valid_loader)
        # Report validation accuracy to Syne Tune
        report(epoch=epoch, accuracy=accuracy)
        # Write checkpoint (optional)
        # [1]
        checkpoint_model_at_rung_level(config, save_model_fn, epoch)


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    from filelock import SoftFileLock, Timeout
    import torch
    import torch.nn as nn
    from torch.utils.data.sampler import SubsetRandomSampler
    from torchvision import datasets
    from torchvision import transforms

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_path", type=str, required=True)
    # Hyperparameters
    parser.add_argument("--n_units_1", type=int, required=True)
    parser.add_argument("--n_units_2", type=int, required=True)
    parser.add_argument("--batch_size", type=int, required=True)
    parser.add_argument("--dropout_1", type=float, required=True)
    parser.add_argument("--dropout_2", type=float, required=True)
    parser.add_argument("--learning_rate", type=float, required=True)
    parser.add_argument("--weight_decay", type=float, required=True)
    # [3]
    add_checkpointing_to_argparse(parser)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: examples/launch_asha_yahpo.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example for running ASHA with 4 workers with the simulator back-end based on three Yahpo surrogate benchmarks.
"""
import logging
from dataclasses import dataclass

import matplotlib.pyplot as plt

from syne_tune.blackbox_repository import BlackboxRepositoryBackend
from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.experiments import load_experiment
from syne_tune.optimizer.baselines import ASHA
from syne_tune import Tuner, StoppingCriterion


def plot_yahpo_learning_curves(
    trial_backend, benchmark: str, time_col: str, metric_col: str
):
    bb = trial_backend.blackbox
    plt.figure()
    plt.title(
        f"Learning curves from Yahpo {benchmark} for 10 different hyperparameters."
    )
    for i in range(10):
        config = {k: v.sample() for k, v in bb.configuration_space.items()}
        evals = bb(config)
        time_index = next(
            i for i, name in enumerate(bb.objectives_names) if name == time_col
        )
        accuracy_index = next(
            i for i, name in enumerate(bb.objectives_names) if name == metric_col
        )
        import numpy as np

        if np.diff(evals[:, time_index]).min() < 0:
            print("negative time between two different steps...")
        plt.plot(evals[:, time_index], evals[:, accuracy_index])
    plt.xlabel(time_col)
    plt.ylabel(metric_col)
    plt.show()


@dataclass
class BenchmarkInfo:
    blackbox_name: str
    elapsed_time_attr: str
    metric: str
    dataset: str
    mode: str
    max_t: int
    resource_attr: str


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    benchmark_infos = {
        "nb301": BenchmarkInfo(
            elapsed_time_attr="runtime",
            metric="val_accuracy",
            blackbox_name="yahpo-nb301",
            dataset="CIFAR10",
            mode="max",
            max_t=97,
            resource_attr="epoch",
        ),
        "lcbench": BenchmarkInfo(
            elapsed_time_attr="time",
            metric="val_accuracy",
            blackbox_name="yahpo-lcbench",
            dataset="3945",
            mode="max",
            max_t=51,
            resource_attr="epoch",
        ),
        "fcnet": BenchmarkInfo(
            elapsed_time_attr="runtime",
            metric="valid_mse",
            blackbox_name="yahpo-fcnet",
            dataset="fcnet_naval_propulsion",
            mode="min",
            max_t=99,
            resource_attr="epoch",
        ),
    }
    for benchmark in ["nb301", "lcbench", "fcnet"]:
        benchmark_info = benchmark_infos[benchmark]

        trial_backend = BlackboxRepositoryBackend(
            blackbox_name=benchmark_info.blackbox_name,
            elapsed_time_attr=benchmark_info.elapsed_time_attr,
            dataset=benchmark_info.dataset,
        )

        plot_yahpo_learning_curves(
            trial_backend,
            benchmark=benchmark,
            time_col=benchmark_info.elapsed_time_attr,
            metric_col=benchmark_info.metric,
        )

        scheduler = ASHA(
            config_space=trial_backend.blackbox.configuration_space,
            max_t=benchmark_info.max_t,
            resource_attr=benchmark_info.resource_attr,
            mode=benchmark_info.mode,
            metric=benchmark_info.metric,
        )

        stop_criterion = StoppingCriterion(max_num_trials_started=200)

        tuner = Tuner(
            trial_backend=trial_backend,
            scheduler=scheduler,
            stop_criterion=stop_criterion,
            n_workers=4,
            sleep_time=0,
            print_update_interval=10,
            callbacks=[SimulatorCallback()],
            tuner_name=f"ASHA-Yahpo-{benchmark}",
        )
        tuner.run()

        tuning_experiment = load_experiment(tuner.name)
        tuning_experiment.plot()

File Path: examples/launch_bayesopt_constrained.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example for running constrained Bayesian optimization on a toy example
"""
import logging
from pathlib import Path

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import FIFOScheduler
from syne_tune.config_space import uniform
from syne_tune import StoppingCriterion, Tuner


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 2

    config_space = {
        "x1": uniform(-5, 10),
        "x2": uniform(0, 15),
        "constraint_offset": 1.0,  # the lower, the stricter
    }

    entry_point = str(
        Path(__file__).parent
        / "training_scripts"
        / "constrained_hpo"
        / "train_constrained_example.py"
    )
    mode = "max"
    metric = "objective"
    constraint_attr = "my_constraint_metric"

    # Local back-end
    trial_backend = LocalBackend(entry_point=entry_point)

    # Bayesian constrained optimization:
    #   max_x f(x)   s.t. c(x) <= 0
    # Here, `metric` represents f(x), `constraint_attr` represents c(x).
    search_options = {
        "num_init_random": n_workers,
        "constraint_attr": constraint_attr,
    }
    scheduler = FIFOScheduler(
        config_space,
        searcher="bayesopt_constrained",
        search_options=search_options,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=30)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: examples/launch_fashionmnist.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example for how to tune one of the benchmarks.
"""
import logging

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion

from benchmarking.definitions.definition_mlp_on_fashion_mnist import (
    mlp_fashionmnist_benchmark,
    mlp_fashionmnist_default_params,
)


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)

    # We pick the MLP on FashionMNIST benchmark
    # The 'benchmark' dict contains arguments needed by scheduler and
    # searcher (e.g., 'mode', 'metric'), along with suggested default values
    # for other arguments (which you are free to override)
    random_seed = 31415927
    n_workers = 4
    default_params = mlp_fashionmnist_default_params()
    benchmark = mlp_fashionmnist_benchmark(default_params)
    mode = benchmark["mode"]
    metric = benchmark["metric"]

    # If you don't like the default config_space, change it here. But let
    # us use the default
    config_space = benchmark["config_space"]

    # Local back-end
    trial_backend = LocalBackend(entry_point=benchmark["script"])

    # GP-based Bayesian optimization searcher. Many options can be specified
    # via `search_options`, but let's use the defaults
    searcher = "bayesopt"
    search_options = {"num_init_random": n_workers + 2}
    # Hyperband (or successive halving) scheduler of the stopping type.
    # Together with 'bayesopt', this selects the MOBSTER algorithm.
    # If you don't like the defaults suggested, just change them:
    scheduler = HyperbandScheduler(
        config_space,
        searcher=searcher,
        search_options=search_options,
        max_t=default_params["max_resource_level"],
        grace_period=default_params["grace_period"],
        reduction_factor=default_params["reduction_factor"],
        resource_attr=benchmark["resource_attr"],
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=120)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: examples/launch_fashionmnist_costaware.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example for cost-aware promotion-based Hyperband
"""
import logging

from benchmarking.definitions.definition_mlp_on_fashion_mnist import (
    mlp_fashionmnist_default_params,
    mlp_fashionmnist_benchmark,
)
from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)
    # logging.getLogger().setLevel(logging.INFO)

    # We pick the MLP on FashionMNIST benchmark
    # The 'benchmark' dict contains arguments needed by scheduler and
    # searcher (e.g., 'mode', 'metric'), along with suggested default values
    # for other arguments (which you are free to override)
    random_seed = 31415927
    n_workers = 4
    default_params = mlp_fashionmnist_default_params()
    benchmark = mlp_fashionmnist_benchmark(default_params)
    mode = benchmark["mode"]
    metric = benchmark["metric"]

    # If you don't like the default config_space, change it here. But let
    # us use the default
    config_space = benchmark["config_space"]

    # Local back-end
    trial_backend = LocalBackend(entry_point=benchmark["script"])

    # Cost-aware variant of ASHA, using a random searcher
    scheduler = HyperbandScheduler(
        config_space,
        searcher="random",
        max_t=default_params["max_resource_level"],
        grace_period=default_params["grace_period"],
        reduction_factor=default_params["reduction_factor"],
        resource_attr=benchmark["resource_attr"],
        mode=mode,
        metric=metric,
        type="cost_promotion",
        rung_system_kwargs={"cost_attr": benchmark["elapsed_time_attr"]},
        random_seed=random_seed,
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=120)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: examples/launch_height_baselines.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from pathlib import Path

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.baselines import (
    RandomSearch,
    ASHA,
)

# from syne_tune.optimizer.baselines import PASHA, BORE  # noqa: F401
# from syne_tune.optimizer.schedulers.synchronous import \
#    SynchronousGeometricHyperbandScheduler  # noqa: F401
# from syne_tune.optimizer.schedulers import FIFOScheduler  # noqa: F401
# from syne_tune.optimizer.schedulers.botorch.botorch_searcher import BotorchSearcher  # noqa: F401
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import randint
from syne_tune.try_import import try_import_gpsearchers_message


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = (
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    mode = "min"
    metric = "mean_loss"

    schedulers = [
        RandomSearch(config_space, metric=metric, mode=mode),
        ASHA(
            config_space,
            metric=metric,
            resource_attr="epoch",
            max_t=max_steps,
            mode=mode,
        ),
        # Commented as needs extra libraries or to save CI testing time. Since we are testing those baselines
        # in our baseline, we keep the uncommented list of schedulers to a small number.
        # PASHA(config_space, metric=metric, resource_attr='epoch', max_t=max_steps, mode=mode),
        # BORE(config_space, metric=metric, mode=mode),
        # SynchronousGeometricHyperbandScheduler(
        #     config_space,
        #     max_resource_level=max_steps,
        #     brackets=3,
        #     max_resource_attr='steps',
        #     resource_attr='epoch',
        #     batch_size=n_workers,
        #     mode=mode,
        #     metric=metric,
        # ),
        # FIFOScheduler(
        #     config_space,
        #     searcher=BotorchSearcher(config_space=config_space, metric=metric, mode='min'),
        #     metric=metric
        # ),
    ]
    try:
        from syne_tune.optimizer.baselines import BayesianOptimization

        # example of setting additional kwargs arguments
        schedulers.append(
            BayesianOptimization(
                config_space,
                metric=metric,
                mode=mode,
                search_options={"num_init_random": n_workers + 2},
            )
        )
        from syne_tune.optimizer.baselines import MOBSTER

        schedulers.append(
            MOBSTER(
                config_space,
                metric=metric,
                resource_attr="epoch",
                max_t=max_steps,
                mode=mode,
            )
        )
    except Exception:
        logging.info(try_import_gpsearchers_message())

    for scheduler in schedulers:
        logging.info(f"\n*** running scheduler {scheduler} ***\n")

        trial_backend = LocalBackend(entry_point=str(entry_point))

        stop_criterion = StoppingCriterion(
            max_wallclock_time=5, min_metric_value={"mean_loss": -6.0}
        )
        tuner = Tuner(
            trial_backend=trial_backend,
            scheduler=scheduler,
            stop_criterion=stop_criterion,
            n_workers=n_workers,
        )

        tuner.run()

File Path: examples/launch_height_moasha.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example showing how to tune multiple objectives at once of an artificial function.
"""
import logging
from pathlib import Path

import numpy as np

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers.multiobjective import MOASHA
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import uniform


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)
    np.random.seed(0)

    max_steps = 27
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "theta": uniform(0, np.pi / 2),
        "sleep_time": 0.01,
    }
    entry_point = (
        Path(__file__).parent
        / "training_scripts"
        / "mo_artificial"
        / "mo_artificial.py"
    )
    mode = "min"

    np.random.seed(0)
    scheduler = MOASHA(
        max_t=max_steps,
        time_attr="step",
        mode=mode,
        metrics=["y1", "y2"],
        config_space=config_space,
    )
    trial_backend = LocalBackend(entry_point=str(entry_point))

    stop_criterion = StoppingCriterion(max_wallclock_time=30)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        sleep_time=0.5,
    )
    tuner.run()

File Path: examples/launch_height_python_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
An example showing to launch a tuning of a python function `train_height`.
"""

from syne_tune import Tuner, StoppingCriterion
from syne_tune.backend.python_backend import PythonBackend
from syne_tune.config_space import randint
from syne_tune.optimizer.baselines import ASHA


def train_height(steps: int, width: float, height: float):
    """
    The function to be tuned, note that import must be in PythonBackend and no global variable are allowed,
    more details on requirements of tuned functions can be found in `PythonBackend`.
    """
    import logging
    from syne_tune import Reporter
    import time

    root = logging.getLogger()
    root.setLevel(logging.INFO)
    reporter = Reporter()
    for step in range(steps):
        dummy_score = (0.1 + width * step / 100) ** (-1) + height * 0.1
        # Feed the score back to Syne Tune.
        reporter(step=step, mean_loss=dummy_score, epoch=step + 1)
        time.sleep(0.1)


if __name__ == "__main__":
    import logging

    root = logging.getLogger()
    root.setLevel(logging.DEBUG)

    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }

    scheduler = ASHA(
        config_space,
        metric="mean_loss",
        resource_attr="epoch",
        max_t=max_steps,
        mode="min",
    )

    trial_backend = PythonBackend(tune_function=train_height, config_space=config_space)

    stop_criterion = StoppingCriterion(
        max_wallclock_time=10, min_metric_value={"mean_loss": -6.0}
    )
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )
    tuner.run()

File Path: examples/launch_height_ray.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from pathlib import Path

from ray.tune.schedulers import AsyncHyperBandScheduler
from ray.tune.suggest.skopt import SkOptSearch
import numpy as np

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import RayTuneScheduler
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import randint

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)

    random_seed = 31415927
    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = str(
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    mode = "min"
    metric = "mean_loss"

    # Local back-end
    trial_backend = LocalBackend(entry_point=entry_point)

    # Hyperband scheduler with SkOpt searcher
    np.random.seed(random_seed)
    ray_searcher = SkOptSearch()
    ray_searcher.set_search_properties(
        mode=mode,
        metric=metric,
        config=RayTuneScheduler.convert_config_space(config_space),
    )

    ray_scheduler = AsyncHyperBandScheduler(
        max_t=max_steps, time_attr="step", mode=mode, metric=metric
    )

    scheduler = RayTuneScheduler(
        config_space=config_space,
        ray_scheduler=ray_scheduler,
        ray_searcher=ray_searcher,
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=30)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: examples/launch_height_sagemaker.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example showing how to run on Sagemaker with a Sagemaker Framework.
"""
import logging
from pathlib import Path

from sagemaker.pytorch import PyTorch

from syne_tune.backend import SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import randint


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = (
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    mode = "min"
    metric = "mean_loss"

    # Random search without stopping
    scheduler = RandomSearch(
        config_space, mode=mode, metric=metric, random_seed=random_seed
    )

    trial_backend = SageMakerBackend(
        # we tune a PyTorch Framework from Sagemaker
        sm_estimator=PyTorch(
            entry_point=str(entry_point),
            instance_type="ml.m5.large",
            instance_count=1,
            role=get_execution_role(),
            max_run=10 * 60,
            framework_version="1.7.1",
            py_version="py3",
            sagemaker_session=default_sagemaker_session(),
        ),
        # names of metrics to track. Each metric will be detected by Sagemaker if it is written in the
        # following form: "[RMSE]: 1.2", see in train_main_example how metrics are logged for an example
        metrics_names=[metric],
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=600)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        sleep_time=5.0,
        tuner_name="hpo-hyperband",
    )

    tuner.run()

File Path: examples/launch_height_sagemaker_custom_image.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example showing how to run on Sagemaker with a custom docker image.
"""
import logging
from pathlib import Path

from syne_tune.backend.sagemaker_backend.custom_framework import CustomFramework
from syne_tune.backend import SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import randint


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = str(
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    mode = "min"
    metric = "mean_loss"

    # Random search without stopping
    scheduler = RandomSearch(
        config_space, mode=mode, metric=metric, random_seed=random_seed
    )

    # indicate here an image_uri that is available in ecr, something like that "XXXXXXXXXXXX.dkr.ecr.us-west-2.amazonaws.com/my_image:latest"
    image_uri = ...

    trial_backend = SageMakerBackend(
        sm_estimator=CustomFramework(
            entry_point=entry_point,
            instance_type="ml.m5.large",
            instance_count=1,
            role=get_execution_role(),
            image_uri=image_uri,
            max_run=10 * 60,
            job_name_prefix="hpo-hyperband",
            sagemaker_session=default_sagemaker_session(),
        ),
        # names of metrics to track. Each metric will be detected by Sagemaker if it is written in the
        # following form: "[RMSE]: 1.2", see in train_main_example how metrics are logged for an example
        metrics_names=[metric],
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=600)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        sleep_time=5.0,
    )

    tuner.run()

File Path: examples/launch_height_sagemaker_remotely.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
This example show how to launch a tuning job that will be executed on Sagemaker rather than on your local machine.
"""
import logging

from pathlib import Path

from sagemaker.pytorch import PyTorch

from syne_tune.backend import LocalBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune.remote.remote_launcher import RemoteLauncher
from syne_tune.backend import SageMakerBackend
from syne_tune.config_space import randint
from syne_tune import StoppingCriterion, Tuner

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = str(
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    mode = "min"
    metric = "mean_loss"

    # We can use the local or sagemaker backend when tuning remotely.
    # Using the local backend means that the remote instance will evaluate the trials locally.
    # Using the sagemaker backend means the remote instance will launch one sagemaker job per trial.
    distribute_trials_on_sagemaker = False
    if distribute_trials_on_sagemaker:
        trial_backend = SageMakerBackend(
            # we tune a PyTorch Framework from Sagemaker
            sm_estimator=PyTorch(
                entry_point=entry_point,
                instance_type="ml.m5.xlarge",
                instance_count=1,
                role=get_execution_role(),
                max_run=10 * 60,
                framework_version="1.6",
                py_version="py3",
                base_job_name="hpo-height",
                sagemaker_session=default_sagemaker_session(),
            ),
        )
    else:
        trial_backend = LocalBackend(entry_point=entry_point)

    for seed in range(2):
        # Random search without stopping
        scheduler = RandomSearch(
            config_space, mode=mode, metric=metric, random_seed=seed
        )

        tuner = RemoteLauncher(
            tuner=Tuner(
                trial_backend=trial_backend,
                scheduler=scheduler,
                n_workers=n_workers,
                tuner_name="height-tuning",
                stop_criterion=StoppingCriterion(max_wallclock_time=600),
            ),
            # Extra arguments describing the resource of the remote tuning instance and whether we want to wait
            # the tuning to finish. The instance-type where the tuning job runs can be different than the
            # instance-type used for evaluating the training jobs.
            instance_type="ml.m5.large",
        )

        tuner.run(wait=False)

File Path: examples/launch_height_standalone_scheduler.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example showing how to implement a new Scheduler.
"""
import logging
from pathlib import Path
from typing import Optional, Dict, List

import numpy as np

from syne_tune.backend import LocalBackend
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import (
    TrialScheduler,
    SchedulerDecision,
    TrialSuggestion,
)
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import randint


class SimpleScheduler(TrialScheduler):
    def __init__(self, config_space: Dict, metric: str, mode: Optional[str] = None):
        super(SimpleScheduler, self).__init__(config_space=config_space)
        self.metric = metric
        self.mode = mode if mode is not None else "min"
        self.sorted_results = []

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        # Called when a slot exists to run a trial, here we simply draw a
        # random candidate.
        config = {
            k: v.sample() if hasattr(v, "sample") else v
            for k, v in self.config_space.items()
        }
        return TrialSuggestion.start_suggestion(config)

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        # Given a new result, we decide whether the trial should stop or continue.
        # In this case, we implement a naive strategy that stops if the result is worse than 80% of previous results.
        # This is a naive strategy as we do not account for the fact that trial improves with more steps.

        new_metric = result[self.metric]

        # insert new metric in sorted results
        index = np.searchsorted(self.sorted_results, new_metric)
        self.sorted_results = np.insert(self.sorted_results, index, new_metric)
        normalized_rank = index / float(len(self.sorted_results))

        if self.mode == "max":
            normalized_rank = 1 - normalized_rank

        if normalized_rank < 0.8:
            return SchedulerDecision.CONTINUE
        else:
            logging.info(
                f"see new results {new_metric} which rank {normalized_rank * 100}%, "
                f"stopping it as it does not rank on the top 80%"
            )
            return SchedulerDecision.STOP

    def metric_names(self) -> List[str]:
        return [self.metric]


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)

    random_seed = 31415927
    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = str(
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    metric = "mean_loss"

    # Local back-end
    trial_backend = LocalBackend(entry_point=entry_point)

    np.random.seed(random_seed)
    scheduler = SimpleScheduler(config_space=config_space, metric=metric)

    stop_criterion = StoppingCriterion(max_wallclock_time=30)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: examples/launch_huggingface_classification.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example for how to fine-tune a DistilBERT model on the IMDB sentiment classification task using the Hugging Face SageMaker Framework.
"""
import logging
from pathlib import Path

from sagemaker.huggingface import HuggingFace

import syne_tune
from syne_tune.backend import SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune import Tuner, StoppingCriterion

from benchmarking.definitions.definition_distilbert_on_imdb import (
    distilbert_imdb_benchmark,
    distilbert_imdb_default_params,
)

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    # We pick the DistilBERT on IMDB benchmark
    # The 'benchmark' dict contains arguments needed by scheduler and
    # searcher (e.g., 'mode', 'metric'), along with suggested default values
    # for other arguments (which you are free to override)
    random_seed = 31415927
    n_workers = 4
    default_params = distilbert_imdb_default_params()
    benchmark = distilbert_imdb_benchmark(default_params)
    mode = benchmark["mode"]
    metric = benchmark["metric"]
    config_space = benchmark["config_space"]

    # Define Hugging Face SageMaker estimator
    root = Path(syne_tune.__path__[0]).parent
    huggingface_estimator = HuggingFace(
        entry_point=benchmark["script"],
        base_job_name="hpo-transformer",
        instance_type=default_params["instance_type"],
        instance_count=1,
        transformers_version="4.4",
        pytorch_version="1.6",
        py_version="py36",
        role=get_execution_role(),
        dependencies=[root / "benchmarking"],
        sagemaker_session=default_sagemaker_session(),
    )

    # SageMaker backend
    trial_backend = SageMakerBackend(
        sm_estimator=huggingface_estimator,
        metrics_names=[metric],
    )

    # Random search without stopping
    scheduler = RandomSearch(
        config_space, mode=mode, metric=metric, random_seed=random_seed
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=3600)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
    )

    tuner.run()

File Path: examples/launch_moasha_instance_tuning.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example showing how to tune instance types and hyperparameters with a Sagemaker Framework.
"""
import logging
from pathlib import Path

from sagemaker.huggingface import HuggingFace

from syne_tune.backend.sagemaker_backend.instance_info import select_instance_type
from syne_tune.backend import SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.constants import ST_WORKER_TIME, ST_WORKER_COST
from syne_tune.optimizer.schedulers.multiobjective import MOASHA
from syne_tune.remote.remote_launcher import RemoteLauncher
from syne_tune import StoppingCriterion, Tuner
from syne_tune.config_space import loguniform, choice

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    n_workers = 2
    epochs = 4

    # Select the instance types that are searched.
    # Alternatively, you can define the instance list explicitly: `instance_types = ['ml.c5.xlarge', 'ml.m5.2xlarge']`
    instance_types = select_instance_type(min_gpu=1, max_cost_per_hour=5.0)

    print(f"tuning over hyperparameters and instance types: {instance_types}")

    # define a search space that contains hyperparameters (learning-rate, weight-decay) and instance-type.
    config_space = {
        "st_instance_type": choice(instance_types),
        "learning_rate": loguniform(1e-6, 1e-4),
        "weight_decay": loguniform(1e-5, 1e-2),
        "epochs": epochs,
        "dataset_path": "./",
    }
    entry_point = (
        Path(__file__).parent
        / "training_scripts"
        / "distilbert_on_imdb"
        / "distilbert_on_imdb.py"
    )
    metric = "accuracy"

    # Define a MOASHA scheduler that searches over the config space to maximise accuracy and minimize cost and time.
    scheduler = MOASHA(
        max_t=epochs,
        time_attr="step",
        metrics=[metric, ST_WORKER_COST, ST_WORKER_TIME],
        mode=["max", "min", "min"],
        config_space=config_space,
    )

    # Define the training function to be tuned, use the Sagemaker backend to execute trials as separate training job
    # (since they are quite expensive).
    trial_backend = SageMakerBackend(
        sm_estimator=HuggingFace(
            entry_point=str(entry_point),
            base_job_name="hpo-transformer",
            # instance-type given here are override by Syne Tune with values sampled from `st_instance_type`.
            instance_type="ml.m5.large",
            instance_count=1,
            transformers_version="4.4",
            pytorch_version="1.6",
            py_version="py36",
            max_run=3600,
            role=get_execution_role(),
            dependencies=[str(Path(__file__).parent.parent / "benchmarking")],
            sagemaker_session=default_sagemaker_session(),
        ),
    )

    remote_launcher = RemoteLauncher(
        tuner=Tuner(
            trial_backend=trial_backend,
            scheduler=scheduler,
            stop_criterion=StoppingCriterion(max_wallclock_time=3600, max_cost=10.0),
            n_workers=n_workers,
            sleep_time=5.0,
        ),
        dependencies=[str(Path(__file__).parent.parent / "benchmarking")],
    )

    remote_launcher.run(wait=False)

File Path: examples/launch_nas201_transfer_learning.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict

from syne_tune.blackbox_repository import load_blackbox, BlackboxRepositoryBackend
from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.experiments import load_experiment
from syne_tune.optimizer.schedulers import FIFOScheduler
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
    BoundingBox,
)
from syne_tune import StoppingCriterion, Tuner


def load_transfer_learning_evaluations(
    blackbox_name: str, test_task: str, metric: str
) -> Dict[str, TransferLearningTaskEvaluations]:
    bb_dict = load_blackbox(blackbox_name)
    metric_index = [
        i
        for i, name in enumerate(bb_dict[test_task].objectives_names)
        if name == metric
    ][0]
    transfer_learning_evaluations = {
        task: TransferLearningTaskEvaluations(
            hyperparameters=bb.hyperparameters,
            configuration_space=bb.configuration_space,
            objectives_evaluations=bb.objectives_evaluations[
                ..., metric_index : metric_index + 1
            ],
            objectives_names=[metric],
        )
        for task, bb in bb_dict.items()
        if task != test_task
    }
    return transfer_learning_evaluations


if __name__ == "__main__":
    blackbox_name = "nasbench201"
    test_task = "cifar100"
    elapsed_time_attr = "metric_elapsed_time"
    metric = "metric_valid_error"

    bb_dict = load_blackbox(blackbox_name)
    transfer_learning_evaluations = load_transfer_learning_evaluations(
        blackbox_name, test_task, metric
    )

    scheduler = BoundingBox(
        scheduler_fun=lambda new_config_space, mode, metric: FIFOScheduler(
            new_config_space,
            points_to_evaluate=[],
            searcher="random",
            metric=metric,
            mode=mode,
        ),
        mode="min",
        config_space=bb_dict[test_task].configuration_space,
        metric=metric,
        num_hyperparameters_per_task=10,
        transfer_learning_evaluations=transfer_learning_evaluations,
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=7200)

    trial_backend = BlackboxRepositoryBackend(
        blackbox_name=blackbox_name,
        elapsed_time_attr=elapsed_time_attr,
        dataset=test_task,
    )

    # It is important to set `sleep_time` to 0 here (mandatory for simulator backend)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=4,
        sleep_time=0,
        # This callback is required in order to make things work with the
        # simulator callback. It makes sure that results are stored with
        # simulated time (rather than real time), and that the time_keeper
        # is advanced properly whenever the tuner loop sleeps
        callbacks=[SimulatorCallback()],
    )
    tuner.run()

    tuning_experiment = load_experiment(tuner.name)
    print(tuning_experiment)

    print(f"best result found: {tuning_experiment.best_config()}")

    tuning_experiment.plot()

File Path: examples/launch_nasbench201_simulated.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example for running the simulator back-end on a tabulated benchmark
"""
import logging

from benchmarking.definitions.definition_nasbench201 import (
    nasbench201_default_params,
    nasbench201_benchmark,
)
from syne_tune.blackbox_repository import BlackboxRepositoryBackend
from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.optimizer.schedulers import HyperbandScheduler
from syne_tune import Tuner, StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927
    n_workers = 4
    default_params = nasbench201_default_params({"backend": "simulated"})
    benchmark = nasbench201_benchmark(default_params)
    # Benchmark must be tabulated to support simulation:
    assert benchmark.get("supports_simulated", False)
    mode = benchmark["mode"]
    metric = benchmark["metric"]
    blackbox_name = benchmark.get("blackbox_name")
    # NASBench201 is a blackbox from the repository
    assert blackbox_name is not None
    dataset_name = "cifar100"

    # If you don't like the default config_space, change it here. But let
    # us use the default
    config_space = benchmark["config_space"]

    # Simulator back-end specialized to tabulated blackboxes
    trial_backend = BlackboxRepositoryBackend(
        blackbox_name=blackbox_name,
        elapsed_time_attr=benchmark["elapsed_time_attr"],
        dataset=dataset_name,
    )

    searcher = "random"
    # Hyperband (or successive halving) scheduler of the stopping type.
    scheduler = HyperbandScheduler(
        config_space,
        searcher=searcher,
        max_t=default_params["max_resource_level"],
        grace_period=default_params["grace_period"],
        reduction_factor=default_params["reduction_factor"],
        resource_attr=benchmark["resource_attr"],
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )

    max_wallclock_time = 600
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)
    # Printing the status during tuning takes a lot of time, and so does
    # storing results.
    print_update_interval = 700
    results_update_interval = 300
    # It is important to set `sleep_time` to 0 here (mandatory for simulator
    # backend)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        sleep_time=0,
        results_update_interval=results_update_interval,
        print_update_interval=print_update_interval,
        # This callback is required in order to make things work with the
        # simulator callback. It makes sure that results are stored with
        # simulated time (rather than real time), and that the time_keeper
        # is advanced properly whenever the tuner loop sleeps
        callbacks=[SimulatorCallback()],
    )
    tuner.run()

File Path: examples/launch_pbt.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from pathlib import Path

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers import PopulationBasedTraining
from syne_tune import Tuner
from syne_tune.config_space import loguniform
from syne_tune import StoppingCriterion


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)

    max_trials = 100

    config_space = {
        "lr": loguniform(0.0001, 0.02),
    }

    entry_point = (
        Path(__file__).parent / "training_scripts" / "pbt_example" / "pbt_example.py"
    )
    trial_backend = LocalBackend(entry_point=str(entry_point))

    mode = "max"
    metric = "mean_accuracy"
    time_attr = "training_iteration"
    population_size = 2

    pbt = PopulationBasedTraining(
        config_space=config_space,
        metric=metric,
        resource_attr=time_attr,
        population_size=population_size,
        mode=mode,
        max_t=200,
        perturbation_interval=1,
    )

    local_tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=pbt,
        stop_criterion=StoppingCriterion(max_wallclock_time=20),
        n_workers=population_size,
        results_update_interval=1,
    )

    local_tuner.run()

File Path: examples/launch_plot_results.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from pathlib import Path

from syne_tune.backend import LocalBackend
from syne_tune.experiments import load_experiment
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import randint


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)

    random_seed = 31415927
    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = str(
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    mode = "min"
    metric = "mean_loss"

    trial_backend = LocalBackend(entry_point=entry_point)

    # Random search without stopping
    scheduler = RandomSearch(
        config_space, mode=mode, metric=metric, random_seed=random_seed
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=20)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        n_workers=n_workers,
        stop_criterion=stop_criterion,
        results_update_interval=5,
        tuner_name="plot-results-demo",
        metadata={"description": "just an example"},
    )

    tuner.run()

    tuning_experiment = load_experiment(tuner.name)
    print(tuning_experiment)

    print(f"best result found: {tuning_experiment.best_config()}")

    tuning_experiment.plot()

File Path: examples/launch_rl_tuning.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
This launches a local HPO tuning the discount factor of PPO on cartpole.
To run this example, you should have installed dependencies in `requirements.txt`.
"""
import logging
from pathlib import Path

import numpy as np

from syne_tune.backend import LocalBackend
from syne_tune.experiments import load_experiment
from syne_tune.optimizer.baselines import ASHA
import syne_tune.config_space as sp
from syne_tune import Tuner, StoppingCriterion

if __name__ == "__main__":

    logging.getLogger().setLevel(logging.DEBUG)
    np.random.seed(0)
    max_steps = 100
    trial_backend = LocalBackend(
        entry_point=Path(__file__).parent
        / "training_scripts"
        / "rl_cartpole"
        / "train_cartpole.py"
    )

    scheduler = ASHA(
        config_space={
            "gamma": sp.uniform(0.5, 0.99),
            "lr": sp.loguniform(1e-6, 1e-3),
        },
        metric="episode_reward_mean",
        mode="max",
        max_t=100,
        resource_attr="training_iter",
        search_options={"debug_log": False},
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=60)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        # tune for 3 minutes
        stop_criterion=stop_criterion,
        n_workers=2,
    )

    tuner.run()

    tuning_experiment = load_experiment(tuner.name)

    print(f"best result found: {tuning_experiment.best_config()}")

    tuning_experiment.plot()

File Path: examples/launch_simulated_benchmark.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import numpy as np
import pandas as pd
import syne_tune.config_space as sp

from syne_tune.blackbox_repository import (
    load_blackbox,
    add_surrogate,
    BlackboxRepositoryBackend,
    UserBlackboxBackend,
)
from syne_tune.blackbox_repository.blackbox_tabular import BlackboxTabular

from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.optimizer.baselines import ASHA
from syne_tune import StoppingCriterion, Tuner


def example_blackbox():
    n = 100
    x1 = np.arange(n)
    x2 = np.arange(n)[::-1]
    hyperparameters = pd.DataFrame(
        data=np.stack([x1, x2]).T, columns=["hp_x1", "hp_x2"]
    )
    config_space = {
        "hp_x1": sp.randint(0, n),
        "hp_x2": sp.randint(0, n),
    }
    n_epochs = 100
    cs_fidelity = {
        "hp_epoch": sp.randint(0, n_epochs),
    }
    num_seeds = 1
    num_objectives = 3

    objectives_evaluations = np.random.rand(
        len(hyperparameters), num_seeds, n_epochs, num_objectives
    )
    # dummy runtime
    for t in range(0, n_epochs):
        objectives_evaluations[:, :, t, 1] = 60 * (t + 1)
    return add_surrogate(
        BlackboxTabular(
            hyperparameters=hyperparameters,
            configuration_space=config_space,
            fidelity_space=cs_fidelity,
            objectives_evaluations=objectives_evaluations,
            objectives_names=["metric_error", "runtime", "gpu_usage"],
        )
    )


def simulate_benchmark(blackbox, trial_backend, metric):
    # Random search without stopping
    scheduler = ASHA(
        blackbox.configuration_space,
        max_t=max(blackbox.fidelity_values),
        resource_attr=next(iter(blackbox.fidelity_space.keys())),
        mode="min",
        metric=metric,
        random_seed=31415927,
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=7200)

    # It is important to set `sleep_time` to 0 here (mandatory for simulator backend)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        sleep_time=0,
        # This callback is required in order to make things work with the
        # simulator callback. It makes sure that results are stored with
        # simulated time (rather than real time), and that the time_keeper
        # is advanced properly whenever the tuner loop sleeps
        callbacks=[SimulatorCallback()],
    )
    tuner.run()


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    n_workers = 4

    ## example of loading nasbench201 and then simulating tuning
    blackbox_name, dataset, metric = "nasbench201", "cifar100", "metric_valid_error"
    elapsed_time_attr = "metric_elapsed_time"
    blackbox = load_blackbox(blackbox_name)[dataset]
    trial_backend = BlackboxRepositoryBackend(
        blackbox_name=blackbox_name,
        dataset=dataset,
        elapsed_time_attr=elapsed_time_attr,
    )
    simulate_benchmark(blackbox=blackbox, trial_backend=trial_backend, metric=metric)

    ## example of loading a blackbox with custom code and then simulating tuning
    metric = "metric_error"
    blackbox = example_blackbox()
    trial_backend = UserBlackboxBackend(
        blackbox=blackbox,
        elapsed_time_attr="runtime",
    )
    simulate_benchmark(blackbox=blackbox, trial_backend=trial_backend, metric=metric)

File Path: examples/launch_standalone_bayesian_optimization.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.config_space import uniform, randint, choice

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    dictionarize_objective,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_tuning_job_state,
)
from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import GPFIFOSearcher
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import encode_state

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    random_seed = 31415927

    # toy example of 3 hp's
    config_space = {
        "hp_1": uniform(-5.0, 5.0),
        "hp_2": randint(-5, 5),
        "hp_3": choice(["a", "b", "c"]),
    }
    hp_ranges = make_hyperparameter_ranges(config_space)
    batch_size = 16
    num_init_candidates_for_batch = 10
    state = create_tuning_job_state(
        hp_ranges=hp_ranges,
        cand_tuples=[
            (-3.0, -4, "a"),
            (2.2, -3, "b"),
            (-4.9, -1, "b"),
            (-1.9, -1, "c"),
            (-3.5, 3, "a"),
        ],
        metrics=[dictionarize_objective(x) for x in (15.0, 27.0, 13.0, 39.0, 35.0)],
    )

    gp_searcher = GPFIFOSearcher(
        state.hp_ranges.config_space,
        points_to_evaluate=None,
        random_seed=random_seed,
        metric="objective",
        debug_log=False,
    )
    gp_searcher_state = gp_searcher.get_state()
    gp_searcher_state["state"] = encode_state(state)
    gp_searcher = gp_searcher.clone_from_state(gp_searcher_state)

    next_candidate_list = gp_searcher.get_batch_configs(
        batch_size=batch_size,
        num_init_candidates_for_batch=num_init_candidates_for_batch,
    )

    assert len(next_candidate_list) == batch_size

File Path: examples/launch_tensorboard_example.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

"""
Example showing how to visualize the HPO process of Syne Tune with Tensorboard. Results will be stored
in ~/syne-tune/{tuner_name}/tensoboard_output. To start tensorboard, execute in a separate shell:

>  tensorboard --logdir  /~/syne-tune/{tuner_name}/tensorboard_output

Open the displayed URL in the browser.

Note that, to use this functionality you need to install tensorboardX: pip install tensorboardX
"""

import logging
from pathlib import Path

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import randint
from syne_tune.tuner_callback import TensorboardCallback

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)

    random_seed = 31415927
    max_steps = 100
    n_workers = 4

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
    }
    entry_point = str(
        Path(__file__).parent
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    mode = "min"
    metric = "mean_loss"

    trial_backend = LocalBackend(entry_point=entry_point)

    # Random search without stopping
    scheduler = RandomSearch(
        config_space, mode=mode, metric=metric, random_seed=random_seed
    )

    stop_criterion = StoppingCriterion(max_wallclock_time=20)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        n_workers=n_workers,
        stop_criterion=stop_criterion,
        results_update_interval=5,
        callbacks=[TensorboardCallback(target_metric=metric, mode=mode)],
        tuner_name="tensorboardx-demo",
        metadata={"description": "just an example"},
    )

    tuner.run()

File Path: examples/launch_tuning_gluonts.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
This launches an HPO tuning several hyperparameters of a gluonts model.
To run this example locally, you need to have installed dependencies in `requirements.txt` in your current interpreter.
"""
import logging
from pathlib import Path

import numpy as np

from sagemaker.mxnet import MXNet

from syne_tune.backend import LocalBackend, SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.baselines import ASHA
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import loguniform, lograndint


if __name__ == "__main__":

    logging.getLogger().setLevel(logging.INFO)
    np.random.seed(0)
    epochs = 50

    config_space = {
        "lr": loguniform(1e-4, 1e-1),
        "epochs": epochs,
        "num_cells": lograndint(lower=1, upper=80),
        "num_layers": lograndint(lower=1, upper=10),
        "dataset": "electricity"
        # "dataset": "m4_hourly"
    }

    mode = "min"
    metric = "mean_wQuantileLoss"
    entry_point = (
        Path(__file__).parent / "training_scripts" / "gluonts" / "train_gluonts.py"
    )

    evaluate_trials_on_sagemaker = False

    if evaluate_trials_on_sagemaker:
        # evaluate trials on Sagemaker
        trial_backend = SageMakerBackend(
            sm_estimator=MXNet(
                entry_point=entry_point.name,
                source_dir=str(entry_point.parent),
                instance_type="ml.c5.2xlarge",
                instance_count=1,
                role=get_execution_role(),
                max_run=10 * 60,
                framework_version="1.7",
                py_version="py3",
                base_job_name="hpo-gluonts",
                sagemaker_session=default_sagemaker_session(),
            ),
            # names of metrics to track. Each metric will be detected by Sagemaker if it is written in the
            # following form: "[RMSE]: 1.2", see in train_main_example how metrics are logged for an example
            metrics_names=[metric],
        )
    else:
        # evaluate trials locally, replace with SageMakerBackend to evaluate trials on Sagemaker
        trial_backend = LocalBackend(entry_point=str(entry_point))

    # see examples to see other schedulers, mobster, Raytune, multiobjective, etc...
    scheduler = ASHA(
        config_space, max_t=epochs, resource_attr="epoch_no", mode="min", metric=metric
    )

    wallclock_time_budget = 3600 if evaluate_trials_on_sagemaker else 600
    dollar_cost_budget = 20.0

    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        # stops if wallclock time or dollar-cost exceeds budget,
        # dollar-cost is only available when running on Sagemaker
        stop_criterion=StoppingCriterion(
            max_wallclock_time=wallclock_time_budget, max_cost=dollar_cost_budget
        ),
        n_workers=4,
        # some failures may happen when SGD diverges with NaNs
        max_failures=10,
    )

    # launch the tuning
    tuner.run()

File Path: examples/training_scripts/checkpoint_example/checkpoint_example.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import argparse
import json
import logging
import os
import time
from pathlib import Path

from syne_tune import Reporter


report = Reporter()


def load_checkpoint(checkpoint_path: Path):
    with open(checkpoint_path, "r") as f:
        return json.load(f)


def save_checkpoint(checkpoint_path: Path, epoch: int, value: float):
    os.makedirs(checkpoint_path.parent, exist_ok=True)
    with open(checkpoint_path, "w") as f:
        json.dump({"last_epoch": epoch, "last_value": value}, f)


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--num-epochs", type=int, required=True)
    parser.add_argument("--multiplier", type=float, default=1)
    parser.add_argument("--sleep-time", type=float, default=0.1)

    # convention the path where to serialize and deserialize is given as st_checkpoint_dir
    parser.add_argument("--st_checkpoint_dir", type=str)

    args, _ = parser.parse_known_args()

    num_epochs = args.num_epochs
    checkpoint_path = None
    start_epoch = 0
    current_value = 0
    if args.st_checkpoint_dir is not None:
        checkpoint_path = Path(args.st_checkpoint_dir) / "checkpoint.json"
        if checkpoint_path.exists():
            state = load_checkpoint(checkpoint_path)
            logging.info(f"resuming from previous checkpoint {state}")
            start_epoch = state["last_epoch"] + 1
            current_value = state["last_value"]

    # write dumb values for loss to illustrate sagemaker ability to retrieve metrics
    # should be replaced by your algorithm
    for current_epoch in range(start_epoch, num_epochs):
        current_value = (current_value + 1) * args.multiplier
        report(train_acc=current_value, step=current_epoch)
        if checkpoint_path is not None:
            save_checkpoint(checkpoint_path, current_epoch, current_value)
        time.sleep(args.sleep_time)

File Path: examples/training_scripts/constrained_hpo/train_constrained_example.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import numpy as np

from syne_tune import Reporter
from argparse import ArgumentParser


report = Reporter()


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.DEBUG)

    parser = ArgumentParser()
    parser.add_argument("--x1", type=float)
    parser.add_argument("--x2", type=float)
    parser.add_argument("--constraint_offset", type=float)

    args, _ = parser.parse_known_args()

    x1 = args.x1
    x2 = args.x2
    constraint_offset = args.constraint_offset
    r = 6
    objective_value = (
        (x2 - (5.1 / (4 * np.pi**2)) * x1**2 + (5 / np.pi) * x1 - r) ** 2
        + 10 * (1 - 1 / (8 * np.pi)) * np.cos(x1)
        + 10
    )
    constraint_value = (
        x1 * 2.0 - constraint_offset
    )  # feasible iff x1 <= 0.5 * constraint_offset
    report(objective=-objective_value, my_constraint_metric=constraint_value)

File Path: examples/training_scripts/constrained_hpo/train_fairhpo_tutorial.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import numpy as np
import pickle as pkl
import random
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


from syne_tune import Reporter
from argparse import ArgumentParser


report = Reporter()


def statistical_disparity(model, X, Y, groups):
    """
    :param model: the trained model
    :param X: the input dataset with n observations
    :param Y: binary labels associated to the n observations (1 = positive)
    :param groups: a list of n values binary values defining two different subgroups of the populations
    """
    fY = model.predict(X)
    sp = [0, 0]
    sp[0] = float(
        len([1 for idx, fy in enumerate(fY) if fy == 1 and groups[idx] == 0])
    ) / len([1 for idx, fy in enumerate(fY) if groups[idx] == 0])
    sp[1] = float(
        len([1 for idx, fy in enumerate(fY) if fy == 1 and groups[idx] == 1])
    ) / len([1 for idx, fy in enumerate(fY) if groups[idx] == 1])
    return abs(sp[0] - sp[1])


if __name__ == "__main__":

    root = logging.getLogger()
    root.setLevel(logging.DEBUG)

    p = ArgumentParser()
    p.add_argument("--max_depth", default=10, type=int)
    p.add_argument("--min_samples_split", default=0.5, type=float)
    p.add_argument("--criterion", type=str, default="gini")
    p.add_argument("--data_path", type=str, default="../notebooks/fairhpo_data.pickle")
    p.add_argument("--fairness_threshold", default=0.01, type=float)
    p.add_argument("--seed", default=10, type=int)
    args, _ = p.parse_known_args()

    random.seed(args.seed)
    np.random.seed(args.seed)
    with open(args.data_path, "rb") as handle:
        data_dict = pkl.load(handle)

    classifier = RandomForestClassifier(
        max_depth=args.max_depth,
        min_samples_split=args.min_samples_split,
        criterion=args.criterion,
    )
    classifier.fit(data_dict["X_train"], data_dict["Y_train"])
    dsp_foreign_worker = statistical_disparity(
        classifier, data_dict["X_test"], data_dict["Y_test"], data_dict["is_foreign"]
    )
    y_pred = classifier.predict(data_dict["X_test"])
    accuracy = accuracy_score(y_pred, data_dict["Y_test"])
    objective_value = accuracy
    constraint_value = (
        dsp_foreign_worker - args.fairness_threshold
    )  # If DSP < fairness threshold,
    # then we consider the model fair
    report(objective=objective_value, my_constraint_metric=constraint_value)

File Path: examples/training_scripts/cost_aware_hpo/train_cost_aware_example.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import numpy as np

from syne_tune import Reporter
from argparse import ArgumentParser


report = Reporter()


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.DEBUG)

    parser = ArgumentParser()
    parser.add_argument("--x1", type=float)
    parser.add_argument("--x2", type=float)
    parser.add_argument("--cost", type=float)

    args, _ = parser.parse_known_args()

    x1 = args.x1
    x2 = args.x2
    cost = args.cost
    r = 6
    objective_value = (
        (x2 - (5.1 / (4 * np.pi**2)) * x1**2 + (5 / np.pi) * x1 - r) ** 2
        + 10 * (1 - 1 / (8 * np.pi)) * np.cos(x1)
        + 10
    )
    cost_value = x2**cost  # the larger x2, the more costly the evaluation
    report(objective=-objective_value, elapsed_time=cost_value)

File Path: examples/training_scripts/gluonts/train_gluonts.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

import logging

from gluonts.evaluation import make_evaluation_predictions, Evaluator

from syne_tune import Reporter
from argparse import ArgumentParser
from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator
from gluonts.mx.trainer import Trainer
from gluonts.mx.trainer.callback import Callback
from gluonts.core.component import validated
from gluonts.dataset.repository.datasets import get_dataset


class GluontsTuneReporter(Callback):
    @validated()
    def __init__(self, validation_data):
        self.reporter = Reporter()
        self.val_dataset = validation_data
        # number of samples used in evaluation
        self.num_samples = 10

    def set_estimator(self, estimator):
        # since the callback does not provide all information to compute forecasting metrics, we set the estimator
        # in order to have the transformation.
        self.estimator = estimator

    def compute_metrics(self, predictor, dataset):
        forecast_it, ts_it = make_evaluation_predictions(
            dataset, predictor=predictor, num_samples=self.num_samples
        )
        # adding more than one worker throws an error, not sure why
        agg_metrics, item_metrics = Evaluator(num_workers=0)(
            ts_it,
            forecast_it,
            num_series=len(dataset),
        )
        return agg_metrics

    def on_validation_epoch_end(
        self, epoch_no: int, epoch_loss: float, training_network, trainer
    ) -> bool:
        metrics = {
            "epoch_no": epoch_no + 1,
            "epoch_loss": epoch_loss,
        }
        predictor = self.estimator.create_predictor(
            transformation=self.estimator.create_transformation(),
            trained_network=training_network,
        )
        metrics["mean_wQuantileLoss"] = self.compute_metrics(
            predictor, self.val_dataset
        )["mean_wQuantileLoss"]
        self.reporter(**metrics)
        return True


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = ArgumentParser()
    parser.add_argument("--lr", type=float, default=0.001)
    parser.add_argument("--num_cells", type=int, default=40)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--dataset", type=str, default="m4_hourly")

    args, _ = parser.parse_known_args()

    dataset = get_dataset(args.dataset, regenerate=False)
    prediction_length = dataset.metadata.prediction_length
    freq = dataset.metadata.freq

    # TODO, we should provide a validation split in all our datasets
    #  for now we use the test as the validation.
    validation_data = dataset.test
    reporter = GluontsTuneReporter(validation_data=validation_data)
    trainer = Trainer(
        learning_rate=args.lr,
        epochs=args.epochs,
        num_batches_per_epoch=500,
        callbacks=[reporter],
    )
    estimator = SimpleFeedForwardEstimator(
        num_hidden_dimensions=args.num_layers * [args.num_cells],
        prediction_length=prediction_length,
        freq=freq,
        trainer=trainer,
    )
    # required to pass additional context so that the callback can compute forecasting metrics
    reporter.set_estimator(estimator)

    predictor = estimator.train(
        dataset.train, validation_data=validation_data, num_workers=None
    )

File Path: examples/training_scripts/height_example/train_height.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example similar to Raytune, https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/skopt_example.py
"""
import logging
import time

from syne_tune import Reporter
from argparse import ArgumentParser


report = Reporter()


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = ArgumentParser()
    parser.add_argument("--steps", type=int)
    parser.add_argument("--width", type=float)
    parser.add_argument("--height", type=float)
    parser.add_argument("--sleep_time", type=float, default=0.1)

    args, _ = parser.parse_known_args()

    width = args.width
    height = args.height
    for step in range(args.steps):
        dummy_score = (0.1 + width * step / 100) ** (-1) + height * 0.1
        # Feed the score back to Syne Tune.
        report(step=step, mean_loss=dummy_score, epoch=step + 1)
        time.sleep(args.sleep_time)

File Path: examples/training_scripts/height_with_cost/train_height_with_cost.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Derived from `train_height.py`, but add variable cost (elapsed time).
"""
import os
import argparse
import logging
import time
import math

from syne_tune import Reporter
from syne_tune.config_space import randint, add_to_argparse
from benchmarking.utils import (
    resume_from_checkpointed_model,
    checkpoint_model_at_rung_level,
    add_checkpointing_to_argparse,
    parse_bool,
)


_config_space = {
    "width": randint(0, 20),
    "height": randint(-100, 100),
}


def height_with_cost_default_params(params=None):
    dont_sleep = str(params is not None and params.get("backend") == "simulated")
    return {
        "max_resource_level": 100,
        "grace_period": 1,
        "reduction_factor": 3,
        "instance_type": "ml.m5.large",
        "num_workers": 4,
        "framework": "PyTorch",
        "framework_version": "1.6",
        "dont_sleep": dont_sleep,
    }


def height_with_cost_benchmark(params):
    config_space = dict(
        _config_space,
        epochs=params["max_resource_level"],
        dont_sleep=params["dont_sleep"],
    )
    return {
        "script": __file__,
        "metric": "mean_loss",
        "mode": "min",
        "resource_attr": "epoch",
        "elapsed_time_attr": "elapsed_time",
        "max_resource_attr": "epochs",
        "config_space": config_space,
        "supports_simulated": True,
    }


def objective(config):
    dont_sleep = parse_bool(config["dont_sleep"])
    width = config["width"]
    height = config["height"]

    ts_start = time.time()
    report = Reporter()

    # Checkpointing
    # Since this is a tabular benchmark, checkpointing is not really needed.
    # Still, we use a "checkpoint" file in order to store the epoch at which
    # the evaluation was paused, since this information is not passed

    def load_model_fn(local_path: str) -> int:
        local_filename = os.path.join(local_path, "checkpoint.json")
        try:
            with open(local_filename, "r") as f:
                data = json.load(f)
                resume_from = int(data["epoch"])
        except Exception:
            resume_from = 0
        return resume_from

    def save_model_fn(local_path: str, epoch: int):
        os.makedirs(local_path, exist_ok=True)
        local_filename = os.path.join(local_path, "checkpoint.json")
        with open(local_filename, "w") as f:
            json.dump({"epoch": str(epoch)}, f)

    resume_from = resume_from_checkpointed_model(config, load_model_fn)

    # Loop over epochs
    cost_epoch = 0.1 + 0.05 * math.sin(width * height)
    elapsed_time_raw = 0
    for epoch in range(resume_from + 1, config["epochs"] + 1):
        mean_loss = 1.0 / (0.1 + width * epoch / 100) + 0.1 * height

        if dont_sleep:
            elapsed_time_raw += cost_epoch
        else:
            time.sleep(cost_epoch)
        elapsed_time = time.time() - ts_start + elapsed_time_raw

        report(epoch=epoch, mean_loss=mean_loss, elapsed_time=elapsed_time)

        # Write checkpoint (optional)
        if epoch == config["epochs"]:
            checkpoint_model_at_rung_level(config, save_model_fn, epoch)


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    import json

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dont_sleep", type=str, required=True)
    add_to_argparse(parser, _config_space)
    add_checkpointing_to_argparse(parser)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: examples/training_scripts/mo_artificial/mo_artificial.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import time
from argparse import ArgumentParser

import numpy as np

from syne_tune import Reporter


def f(t, theta):
    # Function drawing upper-right circles with radius set to `t` and with center set at
    # (-t, -t). `t` is interpreted as a fidelity and larger `t` corresponds to larger radius and better candidates.
    # The optimal multiobjective solution should select theta uniformly from [0, pi/2].
    return {
        "y1": -t + t * np.cos(theta),
        "y2": -t + t * np.sin(theta),
    }


def plot_function():
    import matplotlib.pyplot as plt

    ts = np.linspace(0, 27, num=5)
    thetas = np.linspace(0, 1) * np.pi / 2
    y1s = []
    y2s = []
    for t in ts:
        for theta in thetas:
            res = f(t, theta)
            y1s.append(res["y1"])
            y2s.append(res["y2"])
    plt.scatter(y1s, y2s)
    plt.show()


if __name__ == "__main__":
    # plot_function()
    parser = ArgumentParser()
    parser.add_argument("--steps", type=int, required=True)
    parser.add_argument("--theta", type=float, required=True)
    parser.add_argument("--sleep_time", type=float, required=False, default=0.1)
    args, _ = parser.parse_known_args()

    assert 0 <= args.theta < np.pi / 2
    reporter = Reporter()
    for step in range(args.steps):
        y = f(t=step, theta=args.theta)
        reporter(step=step, **y)
        time.sleep(args.sleep_time)

File Path: examples/training_scripts/nasbench201/nasbench201.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Reproduces NASBench201 benchmark from AutoGluonExperiments repo
"""
import os
import argparse
import logging
import time

from syne_tune import Reporter
from benchmarking.utils import (
    resume_from_checkpointed_model,
    checkpoint_model_at_rung_level,
    add_checkpointing_to_argparse,
    parse_bool,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts.nasbench201_import import (
    CONFIG_KEYS,
    METRIC_VALID_ERROR,
    RESOURCE_ATTR,
    BLACKBOX_NAME,
)


METRIC_ELAPSED_TIME = "metric_elapsed_time"


def objective(config):
    dont_sleep = parse_bool(config["dont_sleep"])

    ts_start = time.time()
    s3_root = config.get("blackbox_repo_s3_root")
    blackbox = load_blackbox(BLACKBOX_NAME, s3_root=s3_root)[config["dataset_name"]]
    # We load metric values for all epochs required here
    essential_config = {k: config[k] for k in CONFIG_KEYS}
    fidelity_range = (1, config["epochs"])
    all_metrics = metrics_for_configuration(
        blackbox=blackbox,
        config=essential_config,
        resource_attr=RESOURCE_ATTR,
        fidelity_range=fidelity_range,
    )
    startup_overhead = time.time() - ts_start

    report = Reporter()

    # Checkpointing
    # Since this is a tabular benchmark, checkpointing is not really needed.
    # Still, we use a "checkpoint" file in order to store the epoch at which
    # the evaluation was paused, since this information is not passed

    def load_model_fn(local_path: str) -> int:
        local_filename = os.path.join(local_path, "checkpoint.json")
        try:
            with open(local_filename, "r") as f:
                data = json.load(f)
                resume_from = int(data["epoch"])
        except Exception:
            resume_from = 0
        return resume_from

    def save_model_fn(local_path: str, epoch: int):
        os.makedirs(local_path, exist_ok=True)
        local_filename = os.path.join(local_path, "checkpoint.json")
        with open(local_filename, "w") as f:
            json.dump({"epoch": str(epoch)}, f)

    resume_from = resume_from_checkpointed_model(config, load_model_fn)

    # Loop over epochs
    prev_elapsed_time = 0
    for epoch in range(resume_from + 1, config["epochs"] + 1):
        metrics_this_epoch = all_metrics[epoch - 1]
        elapsed_time = metrics_this_epoch[METRIC_ELAPSED_TIME]
        valid_error = metrics_this_epoch[METRIC_VALID_ERROR]

        if not dont_sleep:
            if epoch == resume_from + 1:
                # Subtract startup overhead of loading the table
                time_this_epoch = max(
                    elapsed_time - prev_elapsed_time - startup_overhead, 0.0
                )
            time.sleep(time_this_epoch)

        report_dict = {
            RESOURCE_ATTR: epoch,
            METRIC_VALID_ERROR: valid_error,
            METRIC_ELAPSED_TIME: elapsed_time,
        }
        report(**report_dict)

        # Write checkpoint (optional)
        if (not dont_sleep) or epoch == config["epochs"]:
            checkpoint_model_at_rung_level(config, save_model_fn, epoch)


if __name__ == "__main__":
    # Benchmark-specific imports are done here, in order to avoid import
    # errors if the dependencies are not installed (such errors should happen
    # only when the code is really called)
    import json

    from syne_tune.blackbox_repository import load_blackbox
    from syne_tune.blackbox_repository.utils import metrics_for_configuration

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, required=True)
    parser.add_argument("--dataset_name", type=str, required=True)
    parser.add_argument("--dont_sleep", type=str, required=True)
    parser.add_argument("--blackbox_repo_s3_root", type=str)
    for name in CONFIG_KEYS:
        parser.add_argument(f"--{name}", type=str, required=True)
    add_checkpointing_to_argparse(parser)

    args, _ = parser.parse_known_args()

    objective(config=vars(args))

File Path: examples/training_scripts/pbt_example/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: examples/training_scripts/pbt_example/pbt_example.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import argparse
import logging
import json
import os
import random
import time

from syne_tune import Reporter
from syne_tune.constants import ST_CHECKPOINT_DIR

report = Reporter()


def pbt_function(config):
    """Toy PBT problem for benchmarking adaptive learning rate.

    The goal is to optimize this trainable's accuracy. The accuracy increases
    fastest at the optimal lr, which is a function of the current accuracy.

    The optimal lr schedule for this problem is the triangle wave as follows.
    Note that many lr schedules for real models also follow this shape:

     best lr
      ^
      |    /\
      |   /  \
      |  /    \
      | /      \
      ------------> accuracy

    In this problem, using PBT with a population of 2-4 is sufficient to
    roughly approximate this lr schedule. Higher population sizes will yield
    faster convergence. Training will not converge without PBT.
    """
    lr = config["lr"]
    checkpoint_dir = config.get("st_checkpoint_dir")
    accuracy = 0.0  # end = 1000
    start = 1
    if checkpoint_dir and os.path.isdir(checkpoint_dir):
        with open(os.path.join(checkpoint_dir, "checkpoint.json"), "r") as f:
            state = json.loads(f.read())
            accuracy = state["acc"]
            start = state["step"]

    midpoint = 100  # lr starts decreasing after acc > midpoint
    q_tolerance = 3  # penalize exceeding lr by more than this multiple
    noise_level = 2  # add gaussian noise to the acc increase
    # triangle wave:
    #  - start at 0.001 @ t=0,
    #  - peak at 0.01 @ t=midpoint,
    #  - end at 0.001 @ t=midpoint * 2,
    for step in range(start, 200):
        if accuracy < midpoint:
            optimal_lr = 0.01 * accuracy / midpoint
        else:
            optimal_lr = 0.01 - 0.01 * (accuracy - midpoint) / midpoint
        optimal_lr = min(0.01, max(0.001, optimal_lr))

        # compute accuracy increase
        q_err = max(lr, optimal_lr) / min(lr, optimal_lr)
        if q_err < q_tolerance:
            accuracy += (1.0 / q_err) * random.random()
        elif lr > optimal_lr:
            accuracy -= (q_err - q_tolerance) * random.random()
        accuracy += noise_level * np.random.normal()
        accuracy = max(0, accuracy)

        # if step % 3 == 0:

        if checkpoint_dir is not None:
            os.makedirs(os.path.join(checkpoint_dir), exist_ok=True)

            path = os.path.join(checkpoint_dir, "checkpoint.json")
            with open(path, "w") as f:
                f.write(json.dumps({"acc": accuracy, "step": step}))

        report(
            mean_accuracy=accuracy,
            cur_lr=lr,
            training_iteration=step,
            optimal_lr=optimal_lr,  # for debugging
            q_err=q_err,  # for debugging
            # done=accuracy > midpoint * 2  # this stops the training process
        )
        time.sleep(2)


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--lr", type=float)
    parser.add_argument(f"--{ST_CHECKPOINT_DIR}", type=str)

    args, _ = parser.parse_known_args()

    params = vars(args)
    pbt_function(params)

File Path: examples/training_scripts/rl_cartpole/train_cartpole.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Adapts the introductionary example of rllib that trains a Cartpole with PPO.
https://docs.ray.io/en/master/rllib/index.html
The input arguments learning-rate and gamma discount factor can be tuned for maximizing the episode mean reward.
"""
from argparse import ArgumentParser
from syne_tune import Reporter
from ray.rllib.agents.ppo import PPOTrainer

if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("--max_training_steps", type=int, default=100)
    parser.add_argument("--lr", type=float, default=5e-5)
    parser.add_argument("--gamma", type=float, default=0.99)
    args, _ = parser.parse_known_args()

    # Configure the algorithm.
    config = {
        # Environment (RLlib understands openAI gym registered strings).
        "env": "CartPole-v0",
        "num_workers": 2,
        # Change this to "framework: torch", if you are using PyTorch.
        # Also, use "framework: tf2" for tf2.x eager execution.
        "framework": "tf",
        "gamma": args.gamma,
        "lr": args.lr,
    }

    trainer = PPOTrainer(config=config)

    reporter = Reporter()
    # Run it for n max_training_steps iterations. A training iteration includes
    # parallel sample collection by the environment workers as well as
    # loss calculation on the collected batch and a model update.
    # Episode reward mean is reported each time.
    for i in range(args.max_training_steps):
        results = trainer.train()
        reporter(
            training_iter=i + 1,
            episode_reward_mean=results["episode_reward_mean"],
        )

File Path: notebooks/run_bo_experiments.py
Content:
import json
import logging
import numpy as np
import pandas as pd
import random
import string
from argparse import ArgumentParser

from syne_tune.blackbox_repository import load_blackbox
from syne_tune.blackbox_repository.simulated_tabular_backend import BlackboxRepositoryBackend
from benchmarking.definitions.definition_nasbench201 import (
    nasbench201_benchmark,
    nasbench201_default_params
)
from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.baselines import baselines_dict
from syne_tune.tuner import Tuner
from syne_tune.stopping_criterion import StoppingCriterion
from syne_tune.experiments import load_experiment


def run_experiment(dataset_name, random_seed, benchmark_random_seed, hpo_approach, reduction_factor=None, rung_system_kwargs={'ranking_criterion': 'soft_ranking', 'epsilon': 0.025}, benchmark_metric='metric_valid_error', benchmark='nasbench201', benchmark_mode=None):
    """
    Function to run an experiment. It is similar to the NASBench201 example script
    in syne-tune but extended to make it simple to run our experiments.
    
    When describing the following parameters we say what values we use, but feel free to also use other values.
    
    :param dataset_name: one of 'cifar10', 'cifar100', 'ImageNet16-120'
    :param random_seed: one of 31415927, 0, 1234, 3458, 7685
    :param benchmark_random_seed: one of 0, 1, 2 for nasbench201
    :param hpo_approach: one of 'pasha', 'asha', 'pasha-bo', 'asha-bo'
    :param reduction_factor: by default None (resulting in using the default value 3) or 2, 4
    :param rung_system_kwargs: dictionary of ranking criterion (str) and epsilon or epsilon scaling (both float)
    :return: tuner.name
    
    """

    # this function is similar to the NASBench201 example script
    logging.getLogger().setLevel(logging.WARNING)

    default_params = nasbench201_default_params({'backend': 'simulated'})
    benchmark = nasbench201_benchmark(default_params)
    resource_attr = benchmark['resource_attr']
    max_t = default_params['max_resource_level']
    blackbox_name = dataset_name
    # NASBench201 is a blackbox from the repository
    assert blackbox_name is not None
    elapsed_time_attr = 'elapsed_time'
    surrogate = None

    # benchmark must be tabulated to support simulation
    assert benchmark.get('supports_simulated', False)
    if benchmark_mode:
        mode = benchmark_mode
    else:
        mode = benchmark['mode']

    metric = benchmark_metric  # benchmark['metric']

    config_space = benchmark['config_space']
    config_space['dataset_name'] = dataset_name

    # simulator back-end specialized to tabulated blackboxes
    trial_backend = BlackboxRepositoryBackend(
        blackbox_name=blackbox_name,
        elapsed_time_attr=elapsed_time_attr,
        dataset=dataset_name,
        seed=benchmark_random_seed,
        surrogate=surrogate)

    # set logging of the simulator backend to WARNING level
    logging.getLogger(
        'syne_tune.backend.simulator_backend.simulator_backend').setLevel(logging.WARNING)

    if not reduction_factor:
        reduction_factor = default_params['reduction_factor']

    # we support various schedulers within the function
    # NOTE: previously we used resource_attr instead of max_resource_attr
    if hpo_approach == 'pasha':
        scheduler = baselines_dict['PASHA'](
            config_space,
            max_t=max_t,
            grace_period=default_params['grace_period'],
            reduction_factor=reduction_factor,
            resource_attr=resource_attr,
            mode=mode,
            metric=metric,
            random_seed=random_seed,
            rung_system_kwargs=rung_system_kwargs)
    elif hpo_approach == 'asha':
        scheduler = baselines_dict['ASHA'](
            config_space,
            max_t=max_t,
            grace_period=default_params['grace_period'],
            reduction_factor=reduction_factor,
            resource_attr=resource_attr,
            mode=mode,
            type='promotion',
            metric=metric,
            random_seed=random_seed)
    elif hpo_approach == 'pasha-bo':
        scheduler = HyperbandScheduler(
            config_space,
            max_t=max_t,
            grace_period=default_params['grace_period'],
            reduction_factor=reduction_factor,
            resource_attr=resource_attr,
            mode=mode,
            searcher='bayesopt',
            type='pasha',
            metric=metric,
            random_seed=random_seed,
            rung_system_kwargs=rung_system_kwargs)
    elif hpo_approach == 'asha-bo':
        scheduler = HyperbandScheduler(
            config_space,
            max_t=max_t,
            grace_period=default_params['grace_period'],
            reduction_factor=reduction_factor,
            resource_attr=resource_attr,
            mode=mode,
            searcher='bayesopt',
            type='promotion',
            metric=metric,
            random_seed=random_seed)
    else:
        raise ValueError('The selected scheduler is not implemented')

    stop_criterion = StoppingCriterion(max_num_trials_started=256)
    # printing the status during tuning takes a lot of time, and so does
    # storing results
    print_update_interval = 7000
    results_update_interval = 3000
    # it is important to set `sleep_time` to 0 here (mandatory for simulator
    # backend)

    tuner_name = 'nb201bo-' + ''.join(random.choice(string.ascii_lowercase) for _ in range(5))

    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        sleep_time=0,
        results_update_interval=results_update_interval,
        print_update_interval=print_update_interval,
        # this callback is required in order to make things work with the
        # simulator callback. It makes sure that results are stored with
        # simulated time (rather than real time), and that the time_keeper
        # is advanced properly whenever the tuner loop sleeps
        callbacks=[SimulatorCallback()],
        tuner_name=tuner_name
    )

    tuner.run()

    return tuner.name


if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument("--dataset_name", type=str,
                        required=True)
    parser.add_argument("--nb201_random_seed", type=int,
                        required=True)
    parser.add_argument("--random_seed", type=int,
                        required=True)
    parser.add_argument("--scheduler", type=str,
                        required=True)
    parser.add_argument("--experiment_type", type=str,
                        required=True)
    # scheduler is pasha-bo or asha-bo
    args, _ = parser.parse_known_args()


    # store the information
    metric_valid_error_dim = 0
    metric_runtime_dim = 8
    dataset_names = ['cifar10', 'cifar100', 'ImageNet16-120']
    epoch_names = ['val_acc_epoch_' + str(e) for e in range(200)]
    random_seeds = [31415927, 0, 1234, 3458, 7685]
    nb201_random_seeds = [0, 1, 2]
    n_workers = 4

    bb_dict = {}
    for dataset in dataset_names:
        bb_dict[dataset] = load_blackbox(dataset)[dataset]

    df_dict = {}

    for seed in nb201_random_seeds:
        df_dict[seed] = {}
        for dataset in dataset_names:
            # create a dataframe with the validation accuracies for various epochs
            df_val_acc = pd.DataFrame((1.0-bb_dict[dataset].objectives_evaluations[:, seed, :, metric_valid_error_dim])
                                    * 100, columns=['val_acc_epoch_' + str(e) for e in range(200)])

            # add a new column with the best validation accuracy
            df_val_acc['val_acc_best'] = df_val_acc[epoch_names].max(axis=1)
            # create a dataframe with the hyperparameter values
            df_hp = bb_dict[dataset].hyperparameters
            # create a dataframe with the times it takes to run an epoch
            df_time = pd.DataFrame(bb_dict[dataset].objectives_evaluations[:, seed, :, metric_runtime_dim][:, -1], columns=['eval_time_epoch'])    
            # combine all smaller dataframes into one dataframe for each NASBench201 random seed and dataset
            df_dict[seed][dataset] = pd.concat([df_hp, df_val_acc, df_tcal, df_time], axis=1)

    # we need to specify the following:

    benchmark_metric = 'metric_valid_error'
    rung_system_kwargs = {'ranking_criterion': 'soft_ranking_auto', 'epsilon': 0.0}

    experiment_name = run_experiment(
        args.dataset_name, args.random_seed, args.nb201_random_seed, args.scheduler, benchmark_metric=benchmark_metric, rung_system_kwargs=rung_system_kwargs)

    experiment_dict = {'experiment_name': experiment_name,
                       'dataset_name': args.dataset_name,
                       'random_seed': args.random_seed,
                       'nb201_random_seed': args.nb201_random_seed,
                       'scheduler': args.scheduler,
                       'experiment_type': args.experiment_type}

    # now we need to store the experiment_name and also the values of the arguments
    with open('bo_experiment_details_' + args.experiment_type + '.json', 'r') as f:
        current_list = json.load(f)

    current_list.append(experiment_dict)
    with open('bo_experiment_details_' + args.experiment_type + '.json', 'w') as f:
        json.dump(current_list, f)

File Path: setup.py
Content:
from syne_tune import read_version
from setuptools import setup, find_packages
from pathlib import Path


def load_requirements(filename):
    with open(filename) as f:
        return f.read().splitlines()


def load_benchmark_requirements():
    # the requirements of benchmarks are placed into the same directory as the examples script
    res = set()
    for fname in Path(__file__).parent.glob(
        "benchmarking/training_scripts/*/requirements.txt"
    ):
        res.update(load_requirements(fname))
    # gluon-ts is not added as the git dependency does not work with setup.py
    k = "git+https://github.com/awslabs/gluon-ts.git"
    if k in res:
        res.remove(k)
    return list(res)


required_core = load_requirements("requirements.txt")
required_ray = load_requirements("requirements-ray.txt")
required_gpsearchers = load_requirements("requirements-gpsearchers.txt")
required_bore = load_requirements("requirements-bore.txt")
required_botorch = load_requirements("requirements-botorch.txt")
required_kde = load_requirements("requirements-kde.txt")
required_blackbox_repository = load_requirements(
    "syne_tune/blackbox_repository/requirements.txt"
)
required_yahpo = load_requirements(
    "syne_tune/blackbox_repository/conversion_scripts/scripts/requirements-yahpo.txt"
)
required_benchmarks = load_benchmark_requirements()
required_dev = load_requirements("requirements-dev.txt")
required_aws = load_requirements("requirements-aws.txt")

long_description = (Path(__file__).parent / "README.md").read_text()
required_extra = (
    required_ray
    + required_gpsearchers
    + required_benchmarks
    + required_blackbox_repository
    + required_kde
    + required_botorch
    + required_dev
    + required_aws
    + required_yahpo
)
setup(
    name="syne_tune",
    version=read_version(),
    description="Distributed Hyperparameter Optimization on SageMaker",
    long_description=long_description,
    long_description_content_type="text/markdown",
    author="AWS",
    packages=find_packages(
        include=[
            "syne_tune",
            "syne_tune.*",
        ]
    ),
    extras_require={
        "raytune": required_ray,
        "bore": required_bore,
        "kde": required_kde,
        "gpsearchers": required_gpsearchers,
        "benchmarks": required_benchmarks,
        "blackbox-repository": required_blackbox_repository,
        "aws": required_aws,
        "yahpo": required_yahpo,
        "extra": required_extra,
    },
    install_requires=required_core,
    include_package_data=True,
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "License :: OSI Approved :: Apache Software License",
        "Programming Language :: Python :: 3.8",
    ],
)

File Path: syne_tune/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path

try:
    # The reason for conditional imports is that `read_version` is called
    # by `setup.py` before any dependencies are installed
    from syne_tune.stopping_criterion import StoppingCriterion  # noqa: F401
    from syne_tune.report import Reporter  # noqa: F401
    from syne_tune.tuner import Tuner  # noqa: F401

    __all__ = ["StoppingCriterion", "Tuner", "Reporter"]
except ImportError:
    __all__ = []


def read_version():
    with open(Path(__file__).parent / "version", "r") as f:
        return f.readline().strip().replace('"', "")


__version__ = read_version()

File Path: syne_tune/backend/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

__all__ = []

import logging

from syne_tune.try_import import try_import_backend_message

try:
    from syne_tune.backend.local_backend import LocalBackend  # noqa: F401

    __all__.append("LocalBackend")
except ImportError:
    logging.info(try_import_backend_message("LocalBackend"))

try:
    from syne_tune.backend.python_backend.python_backend import (  # noqa: F401
        PythonBackend,
    )

    __all__.append("PythonBackend")
except ImportError:
    logging.info(try_import_backend_message("PythonBackend"))

try:
    from syne_tune.backend.sagemaker_backend.sagemaker_backend import (  # noqa: F401
        SageMakerBackend,
    )

    __all__.append("SageMakerBackend")
except ImportError:
    logging.info(try_import_backend_message("SageMakerBackend"))

File Path: syne_tune/backend/local_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import json
import logging
import os
import shutil
import sys

import numpy as np
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from syne_tune.backend.trial_backend import TrialBackend
from syne_tune.num_gpu import get_num_gpus
from syne_tune.report import retrieve
from syne_tune.backend.trial_status import TrialResult, Status
from syne_tune.constants import ST_CHECKPOINT_DIR
from syne_tune.util import experiment_path, random_string

logger = logging.getLogger(__name__)


if "OMP_NUM_THREADS" not in os.environ:
    logger.debug(
        "OMP_NUM_THREADS is not set, it is going to be set to 1 to avoid performance issues in case of many "
        "workers are used locally. Overrides this behavior by setting a custom value."
    )
    os.environ["OMP_NUM_THREADS"] = "1"


class LocalBackend(TrialBackend):
    def __init__(
        self,
        entry_point: str,
        rotate_gpus: bool = True,
        delete_checkpoints: bool = False,
    ):
        """
        A backend running locally by spawning sub-process concurrently.
        Note that no resource management is done so the concurrent number of
        trials should be adjusted to the machine capacity.

        :param entry_point: python main file to be tuned
        :param rotate_gpus: in case several GPUs are present, each trial is
            scheduled on a different GPU. A new trial is preferentially
            scheduled on a free GPU, and otherwise the GPU with least prior
            assignments is chosen. If False, then all GPUs are used at the same
            time for all trials.
        :param delete_checkpoints: If True, checkpoints of stopped or completed
            trials are deleted

        """
        super(LocalBackend, self).__init__(delete_checkpoints)

        assert Path(
            entry_point
        ).exists(), f"the script provided to tune does not exist ({entry_point})"
        self.entry_point = entry_point

        self.trial_subprocess = {}

        # GPU rotation
        # Note that the initialization is delayed until first used, so we can
        # be sure it happens on the instance running the training evaluations
        self.rotate_gpus = rotate_gpus
        self.num_gpus = None
        self.trial_gpu = None
        self.gpu_times_assigned = None

        # sets the path where to write files, can be overidden later by Tuner.
        self.set_path(Path(experiment_path(tuner_name=random_string(length=10))))

    def trial_path(self, trial_id: int) -> Path:
        return self.local_path / str(trial_id)

    def _checkpoint_trial_path(self, trial_id: int):
        return self.trial_path(trial_id) / "checkpoints"

    def copy_checkpoint(self, src_trial_id: int, tgt_trial_id: int):
        src_checkpoint_path = self._checkpoint_trial_path(src_trial_id)
        tgt_checkpoint_path = self._checkpoint_trial_path(tgt_trial_id)
        shutil.copytree(src_checkpoint_path, tgt_checkpoint_path)

    def delete_checkpoint(self, trial_id: int):
        checkpoint_path = self._checkpoint_trial_path(trial_id)
        shutil.rmtree(checkpoint_path, ignore_errors=True)

    def _prepare_for_schedule(self, num_gpus=None):
        """
        Called at the start of each `_schedule`.
        In particular, we initialize variables related to GPU scheduling, if
        `rotate_gpus' is set. This is done before the first call of `_schedule`,
        so we can be sure it runs on the target instance.

        """
        if self.rotate_gpus and self.num_gpus is None:
            if num_gpus is None:
                self.num_gpus = get_num_gpus()
            else:
                self.num_gpus = num_gpus
            logging.info(f"Detected {self.num_gpus} GPUs")
            if self.num_gpus > 1:
                self.trial_gpu = dict()  # Maps running trials to GPUs
                # To break ties among GPUs (free ones have precedence)
                self.gpu_times_assigned = [0] * self.num_gpus
            else:
                # Nothing to rotate over
                self.rotate_gpus = False

    def _gpu_for_new_trial(self) -> int:
        """
        Selects GPU for trial to be scheduled on. GPUs not assigned to other
        running trials have precedence. Ties are resolved by selecting a GPU
        with the least number of previous assignments.
        The number of assignments is incremented for the GPU returned.

        """
        assert self.rotate_gpus
        free_gpus = set(range(self.num_gpus)).difference(self.trial_gpu.values())
        if free_gpus:
            eligible_gpus = free_gpus
            logging.debug(f"Free GPUs: {free_gpus}")
        else:
            eligible_gpus = range(self.num_gpus)
        # We select the GPU which has the least prior assignments. Selection
        # over all GPUs currently free. If all GPUs are currently assigned,
        # selection is over all GPUs. In this case, the assignment will go to
        # a GPU currently occupied (this happens if the number of GPUs is
        # smaller than the number of workers).
        res_gpu, _ = min(
            ((gpu, self.gpu_times_assigned[gpu]) for gpu in eligible_gpus),
            key=lambda x: x[1],
        )
        self.gpu_times_assigned[res_gpu] += 1
        return res_gpu

    def _schedule(self, trial_id: int, config: Dict):
        self._prepare_for_schedule()
        trial_path = self.trial_path(trial_id)
        os.makedirs(trial_path, exist_ok=True)
        with open(trial_path / "std.out", "a") as stdout:
            with open(trial_path / "std.err", "a") as stderr:
                logging.debug(
                    f"scheduling {trial_id}, {self.entry_point}, {config}, logging into {trial_path}"
                )
                config_copy = config.copy()
                config_copy[ST_CHECKPOINT_DIR] = str(trial_path / "checkpoints")
                config_str = " ".join(
                    [f"--{key} {value}" for key, value in config_copy.items()]
                )

                def np_encoder(object):
                    if isinstance(object, np.generic):
                        return object.item()

                with open(trial_path / "config.json", "w") as f:
                    # the encoder fixes json error "TypeError: Object of type 'int64' is not JSON serializable"
                    json.dump(config, f, default=np_encoder)

                cmd = f"{sys.executable} {self.entry_point} {config_str}"

                env = dict(os.environ)
                self._allocate_gpu(trial_id, env)

                logging.info(f"running subprocess with command: {cmd}")

                self.trial_subprocess[trial_id] = subprocess.Popen(
                    cmd.split(" "), stdout=stdout, stderr=stderr, env=env
                )

    def _allocate_gpu(self, trial_id: int, env: dict):
        if self.rotate_gpus:
            gpu = self._gpu_for_new_trial()
            env["CUDA_VISIBLE_DEVICES"] = str(gpu)
            self.trial_gpu[trial_id] = gpu
            logging.debug(f"Assigned GPU {gpu} to trial_id {trial_id}")

    def _deallocate_gpu(self, trial_id: int):
        if self.rotate_gpus and trial_id in self.trial_gpu:
            del self.trial_gpu[trial_id]

    def _all_trial_results(self, trial_ids: List[int]) -> List[TrialResult]:
        """
        :param trial_ids: list of trial-ids whose status must be retrieved
        :return:
        """
        res = []
        for trial_id in trial_ids:
            trial_path = self.trial_path(trial_id)
            status = self._read_status(trial_id)

            if status != Status.in_progress:
                # Trial completed or failed: Deallocate GPU
                self._deallocate_gpu(trial_id)

            # If the job has finished, we read its end-time in a time-stamp.
            # If the time-stamp does not exist and the job finished, we create it. As a consequence the end-time is
            # an (over)-approximation. It is not clear how to avoid this without running the command in shell mode
            # (which allows to write a time-stamp when the process finishes) but it is probably OK if all_results
            # is called every few seconds.
            if os.path.exists(trial_path / "end"):
                training_end_time = self._read_time_stamp(trial_id=trial_id, name="end")
            else:
                training_end_time = datetime.now()

                # if the time-stamp is not present, we check whether the job has finished,
                # if this is the case we create a time-stamp to mark now as the end-time.
                if self._is_process_done(trial_id=trial_id):
                    self._write_time_stamp(trial_id=trial_id, name="end")

            metrics = retrieve(log_lines=self.stdout(trial_id=trial_id))
            trial_results = self._trial_dict[trial_id].add_results(
                metrics=metrics,
                status=status,
                training_end_time=training_end_time,
            )
            res.append(trial_results)
        return res

    def _pause_trial(self, trial_id: int, result: Optional[dict]):
        self._file_path(trial_id=trial_id, filename="pause").touch()
        self._kill_process(trial_id)
        self._deallocate_gpu(trial_id)

    def _resume_trial(self, trial_id: int):
        pause_path = self._file_path(trial_id=trial_id, filename="pause")
        try:
            pause_path.unlink()
        except FileNotFoundError:
            logger.info(f"Pause lock file {str(pause_path)} not found")

    def _stop_trial(self, trial_id: int, result: Optional[dict]):
        self._file_path(trial_id=trial_id, filename="stop").touch()
        self._kill_process(trial_id)
        self._deallocate_gpu(trial_id)

    def _kill_process(self, trial_id: int):
        # send a kill process to the process
        process = self.trial_subprocess[trial_id]
        try:
            process.kill()
        except ProcessLookupError as e:
            pass

    def _file_path(self, trial_id: int, filename: str):
        return Path(self.trial_path(trial_id=trial_id) / filename)

    def _write_time_stamp(self, trial_id: int, name: str):
        time_stamp_path = self._file_path(trial_id=trial_id, filename=name)
        with open(time_stamp_path, "w") as f:
            f.write(str(datetime.now().timestamp()))

    def _read_time_stamp(self, trial_id: int, name: str):
        time_stamp_path = self._file_path(trial_id=trial_id, filename=name)
        with open(time_stamp_path, "r") as f:
            return datetime.fromtimestamp(float(f.readline()))

    def _is_process_done(self, trial_id: int) -> bool:
        return self.trial_subprocess[trial_id].poll() is not None

    def _read_status(self, trial_id: int):
        if self._file_path(trial_id=trial_id, filename="stop").exists():
            return Status.stopped
        elif self._file_path(trial_id=trial_id, filename="pause").exists():
            return Status.paused
        else:
            code = self.trial_subprocess[trial_id].poll()
            if code is None:
                return Status.in_progress
            else:
                if code == 0:
                    return Status.completed
                else:
                    return Status.failed

    def stdout(self, trial_id: int) -> List[str]:
        with open(self.trial_path(trial_id=trial_id) / "std.out", "r") as f:
            return f.readlines()

    def stderr(self, trial_id: int) -> List[str]:
        with open(self.trial_path(trial_id=trial_id) / "std.err", "r") as f:
            return f.readlines()

    def set_path(
        self, results_root: Optional[str] = None, tuner_name: Optional[str] = None
    ):
        self.local_path = Path(results_root)

    def entrypoint_path(self) -> Path:
        return Path(self.entry_point)

    def set_entrypoint(self, entry_point: str):
        self.entry_point = entry_point

    def __str__(self):
        return f"local entry_point {Path(self.entry_point).name}"

File Path: syne_tune/backend/python_backend/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
__all__ = ["PythonBackend"]

from syne_tune.backend.python_backend.python_backend import PythonBackend

File Path: syne_tune/backend/python_backend/python_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import hashlib
import json
import logging
import types
from pathlib import Path
from typing import Dict, Callable, Optional

import dill

from syne_tune.backend import LocalBackend
from syne_tune.config_space import to_dict, Domain


def file_md5(filename: str) -> str:
    hash_md5 = hashlib.md5()
    with open(filename, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


class PythonBackend(LocalBackend):
    def __init__(
        self,
        tune_function: Callable,
        config_space: Dict[str, object],
        rotate_gpus: bool = True,
        delete_checkpoints: bool = False,
    ):
        """
        A backend that supports the tuning of Python functions (if you rather want to tune an endpoint script such as
        "train.py", then you should rather use `LocalBackend`). The function `tune_function` should be serializable,
        should not reference any global variable or module and should have as arguments all the keys of `config_space`.
        When deserializing, a md5 is checked to ensure consistency.

        For instance, the following function is a valid way of defining a backend on top of simple function:

        ```python
        def f(x):
            import logging
            import time
            from syne_tune import Reporter
            root = logging.getLogger()
            root.setLevel(logging.DEBUG)
            reporter = Reporter()
            for i in range(5):
                reporter(step=i + 1, y=x + i)


        from syne_tune.backend.python_backend.python_backend import PythonBackend
        from syne_tune.config_space import uniform

        config_space = {"x": uniform(-10, 10)}
        backend = PythonBackend(tune_function=f, config_space=config_space)
        ```
        See `examples/launch_height_python_backend.py` for a full example.
        :param tune_function: a python function to be tuned, the function should call Syne Tune reporter to report
        metrics and be serializable, imports should be performed inside the function body.
        :param config_space: config_space used to in Syne Tune, it must corresponds to key words arguments of
        `tune_function`.
        :param rotate_gpus: in case several GPUs are present, each trial is
            scheduled on a different GPU. A new trial is preferentially
            scheduled on a free GPU, and otherwise the GPU with least prior
            assignments is chosen. If False, then all GPUs are used at the same
            time for all trials.
        :param delete_checkpoints: If True, checkpoints of stopped or completed
            trials are deleted
        """
        super(PythonBackend, self).__init__(
            entry_point=str(Path(__file__).parent / "python_entrypoint.py"),
            rotate_gpus=rotate_gpus,
            delete_checkpoints=delete_checkpoints,
        )
        self.config_space = config_space
        # save function without reference to global variables or modules
        self.tune_function = types.FunctionType(tune_function.__code__, {})
        self.tune_function_path = self.local_path / "tune_function"

    def set_path(
        self, results_root: Optional[str] = None, tuner_name: Optional[str] = None
    ):
        super(PythonBackend, self).set_path(
            results_root=results_root, tuner_name=tuner_name
        )
        if self.local_path.exists():
            logging.error(
                f"path {self.local_path} already exists, make sure you have a unique tuner name."
            )
        self.tune_function_path = self.local_path / "tune_function"

    def _schedule(self, trial_id: int, config: Dict):
        if not (self.tune_function_path / "tune_function.dill").exists():
            self.save_tune_function(self.tune_function)
        config = config.copy()
        config["tune_function_root"] = str(self.tune_function_path)
        # to detect if the serialized function is the same as the one passed by the user, we pass the md5 to the
        # endpoint script. The hash is checked before executing the function.
        config["tune_function_hash"] = file_md5(
            self.tune_function_path / "tune_function.dill"
        )
        super(PythonBackend, self)._schedule(trial_id=trial_id, config=config)

    def save_tune_function(self, tune_function):
        self.tune_function_path.mkdir(parents=True, exist_ok=True)
        with open(self.tune_function_path / "tune_function.dill", "wb") as file:
            dill.dump(tune_function, file)
        with open(self.tune_function_path / "configspace.json", "w") as file:
            json.dump(
                {
                    k: to_dict(v) if isinstance(v, Domain) else v
                    for k, v in self.config_space.items()
                },
                file,
            )

File Path: syne_tune/backend/python_backend/python_entrypoint.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
An entry point that loads a serialized function from `PythonBackend` and executes it with the provided hyperparameter.
The md5 hash of the file is first checked before executing the deserialized function.
"""
import json
import logging
from argparse import ArgumentParser
from pathlib import Path
from typing import Dict

import dill

from syne_tune.backend.python_backend.python_backend import file_md5
from syne_tune.config_space import add_to_argparse, from_dict

if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = ArgumentParser()
    parser.add_argument(f"--tune_function_root", type=str)
    parser.add_argument(f"--tune_function_hash", type=str)
    args, _ = parser.parse_known_args()

    # first parse args to get where the function and config space were saved and
    # check the md5 of the serialized function is the same
    # then parse args again with parameters defined in the config space
    assert args.tune_function_root
    assert args.tune_function_hash
    root = Path(args.tune_function_root)
    assert (
        file_md5(root / "tune_function.dill") == args.tune_function_hash
    ), "The hash of the tuned function should match the hash obtained when serializing in Syne Tune."
    with open(root / "tune_function.dill", "rb") as file:
        tuned_function = dill.load(file)

    with open(root / "configspace.json", "r") as file:
        config_space = json.load(file)
        config_space = {
            k: from_dict(v) if isinstance(v, Dict) else v
            for k, v in config_space.items()
        }

    add_to_argparse(parser, config_space)

    args, _ = parser.parse_known_args()

    hps = {k: v for k, v in args.__dict__.items() if k in config_space}
    tuned_function(**hps)

File Path: syne_tune/backend/sagemaker_backend/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/backend/sagemaker_backend/custom_framework.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from sagemaker.estimator import Framework
from sagemaker.vpc_utils import VPC_CONFIG_DEFAULT

logger = logging.getLogger(__name__)


class CustomFramework(Framework):

    __framework_name__ = "customframework"

    LATEST_VERSION = "0.1"

    def __init__(
        self,
        entry_point,
        image_uri: str,
        source_dir=None,
        hyperparameters=None,
        **kwargs
    ) -> None:
        super(CustomFramework, self).__init__(
            str(entry_point), source_dir, hyperparameters, image_uri=image_uri, **kwargs
        )

    def create_model(
        self,
        model_server_workers=None,
        role=None,
        vpc_config_override=VPC_CONFIG_DEFAULT,
    ):
        # required to allow this object instantiation
        raise NotImplementedError()

File Path: syne_tune/backend/sagemaker_backend/instance_info.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, List

import pandas as pd


@dataclass
class InstanceInfo:
    name: str
    num_cpu: int
    num_gpu: int
    cost_per_hour: float


class InstanceInfos:
    """
    Utility to get information of an instance type (num cpu/gpu, cost per hour).
    """

    def __init__(self):
        # TODO right now, we use a static file but some services are available to get updated information
        root = Path(__file__).parent
        self.df_instances = pd.read_csv(
            root / "instance-types-cost.csv", delimiter=";"
        ).sort_values(by="price")
        self.instances = list(self.df_instances.instance.unique())

    def __call__(self, instance_type: str) -> InstanceInfo:
        row = self.df_instances.loc[self.df_instances.instance == instance_type]
        return InstanceInfo(
            name=row["instance"].values[0],
            num_cpu=row["vCPU"].values[0],
            num_gpu=row["GPU"].values[0],
            cost_per_hour=row["price"].values[0],
        )


def select_instance_type(
    min_gpu: int = 0,
    max_gpu: int = 16,
    min_cost_per_hour: Optional[float] = None,
    max_cost_per_hour: Optional[float] = None,
) -> List[str]:
    """
    :param min_gpu:
    :param max_gpu:
    :param min_cost_per_hour:
    :param max_cost_per_hour:
    :return: a list of instance type that met the required constrain on minimum/maximum number of GPU and
    minimum/maximum cost per hour.
    """
    res = []
    instance_infos = InstanceInfos()
    for instance in instance_infos.instances:
        instance_info = instance_infos(instance)
        if instance_info.num_gpu < min_gpu or instance_info.num_gpu > max_gpu:
            continue
        if (
            min_cost_per_hour is not None
            and instance_info.cost_per_hour <= min_cost_per_hour
        ):
            continue
        if (
            max_cost_per_hour is not None
            and instance_info.cost_per_hour >= max_cost_per_hour
        ):
            continue
        res.append(instance)
    return res


if __name__ == "__main__":
    info = InstanceInfos()

    for instance in info.instances:
        print(instance, info(instance))

File Path: syne_tune/backend/sagemaker_backend/sagemaker_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import json
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional
import boto3
from botocore.exceptions import ClientError
import numpy as np

from sagemaker import LocalSession
from sagemaker.estimator import Framework

from syne_tune.backend.trial_backend import TrialBackend
from syne_tune.constants import ST_INSTANCE_TYPE, ST_INSTANCE_COUNT, ST_CHECKPOINT_DIR
from syne_tune.util import s3_experiment_path
from syne_tune.backend.trial_status import TrialResult, Status
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    sagemaker_search,
    get_log,
    sagemaker_fit,
    metric_definitions_from_names,
    add_syne_tune_dependency,
    map_identifier_limited_length,
    s3_copy_files_recursively,
    s3_delete_files_recursively,
    default_config,
    default_sagemaker_session,
)


logger = logging.getLogger(__name__)


class SageMakerBackend(TrialBackend):
    def __init__(
        self,
        sm_estimator: Framework,
        metrics_names: Optional[List[str]] = None,
        s3_path: Optional[str] = None,
        delete_checkpoints: bool = False,
        *args,
        **sagemaker_fit_kwargs,
    ):
        """
        :param sm_estimator: sagemaker estimator to be fitted
        :param metrics_names: name of metrics passed to `report`, used to plot live curve in sagemaker (optional, only
        used for visualization purpose)
        :param s3_path: S3 base path used for checkpointing. The full path
            also involves the tuner name and the trial_id
        :param sagemaker_fit_kwargs: extra arguments that are passed to sagemaker.estimator.Framework when fitting the
        job, for instance `{'train': 's3://my-data-bucket/path/to/my/training/data'}`
        """
        assert (
            not delete_checkpoints
        ), "delete_checkpoints=True not yet supported for SageMaker backend"
        super(SageMakerBackend, self).__init__()
        self.sm_estimator = sm_estimator

        # edit the sagemaker estimator so that metrics of the user can be plotted over time by sagemaker and so that
        # the report.py code is available
        if metrics_names is None:
            metrics_names = []
        self.metrics_names = metrics_names
        self.add_metric_definitions_to_sagemaker_estimator(metrics_names)

        st_prefix = "st-"
        if self.sm_estimator.base_job_name is None:
            base_job_name = st_prefix
        else:
            base_job_name = st_prefix + self.sm_estimator.base_job_name
        # Make sure len(base_job_name) <= 63
        self.sm_estimator.base_job_name = map_identifier_limited_length(base_job_name)

        add_syne_tune_dependency(self.sm_estimator)

        self.job_id_mapping = {}
        self.sagemaker_fit_kwargs = sagemaker_fit_kwargs

        # we keep the list of jobs that were paused/stopped as Sagemaker training job status is not immediately changed
        # after stopping a job.
        self.paused_jobs = set()
        self.stopped_jobs = set()
        # Counts how often a trial has been resumed
        self.resumed_counter = dict()
        if s3_path is None:
            s3_path = s3_experiment_path()
        self.s3_path = s3_path.rstrip("/")
        self.tuner_name = None

    @property
    def sm_client(self):
        return boto3.client(service_name="sagemaker", config=default_config())

    def add_metric_definitions_to_sagemaker_estimator(self, metrics_names: List[str]):
        # We add metric definitions corresponding to the metrics passed by `report` that the user wants to track
        # this allows to plot live learning curves of metrics in Sagemaker console.
        # The reason why we ask to the user metric names is that they are required to be known before hand so that live
        # plotting works.
        if self.sm_estimator.metric_definitions is None:
            self.sm_estimator.metric_definitions = metric_definitions_from_names(
                metrics_names
            )
        else:
            self.sm_estimator.metric_definitions = (
                self.sm_estimator.metric_definitions
                + metric_definitions_from_names(self.metrics_names)
            )
        if len(self.sm_estimator.metric_definitions) > 40:
            logger.warning(
                "Sagemaker only supports 40 metrics for learning curve visualization, keeping only the first 40"
            )
            self.sm_estimator.metric_definitions = self.sm_estimator.metric_definitions[
                :40
            ]

    def _all_trial_results(self, trial_ids: List[int]) -> List[TrialResult]:
        res = sagemaker_search(
            trial_ids_and_names=[
                (jobid, self.job_id_mapping[jobid]) for jobid in trial_ids
            ],
            sm_client=self.sm_client,
        )

        # overrides the status return by Sagemaker as the stopping decision may not have been propagated yet.
        for trial_res in res:
            if trial_res.trial_id in self.paused_jobs:
                trial_res.status = Status.paused
            if trial_res.trial_id in self.stopped_jobs:
                trial_res.status = Status.stopped
        return res

    @staticmethod
    def _numpy_serialize(dict):
        def np_encoder(object):
            if isinstance(object, np.generic):
                return object.item()

        return json.loads(json.dumps(dict, default=np_encoder))

    def _checkpoint_s3_uri_for_trial(self, trial_id: int) -> str:
        res_path = self.s3_path
        if self.tuner_name is not None:
            res_path = f"{res_path}/{self.tuner_name}"
        return f"{res_path}/{str(trial_id)}/checkpoints/"

    def _schedule(self, trial_id: int, config: Dict):
        config[ST_CHECKPOINT_DIR] = "/opt/ml/checkpoints"
        hyperparameters = config.copy()

        # This passes the instance type and instance count to the training function in Sagemaker as hyperparameters
        # with reserved names `st_instance_type` and `st_instance_count`.
        # We pass them as hyperparameters as it is not easy to get efficiently from inside Sagemaker training script
        # (this information is not given for instance as Sagemaker environment variables).
        # This allows to: 1) measure cost in the worker 2) tune instance_type and instance_count by having
        # `st_instance_type` or `st_instance_count` in the config space.
        # TODO once we have a multiobjective scheduler, we should add an example on how to tune instance-type/count.
        if ST_INSTANCE_TYPE not in hyperparameters:
            hyperparameters[ST_INSTANCE_TYPE] = self.sm_estimator.instance_type
        else:
            self.sm_estimator.instance_type = hyperparameters[ST_INSTANCE_TYPE]
        if ST_INSTANCE_COUNT not in hyperparameters:
            hyperparameters[ST_INSTANCE_COUNT] = self.sm_estimator.instance_count
        else:
            self.sm_estimator.instance_count = hyperparameters[ST_INSTANCE_COUNT]

        if self.sm_estimator.instance_type != "local":
            checkpoint_s3_uri = self._checkpoint_s3_uri_for_trial(trial_id)
            logging.info(
                f"Trial {trial_id} will checkpoint results to {checkpoint_s3_uri}."
            )
        else:
            # checkpointing is not supported in local mode. When using local mode with remote tuner (for instance for
            # debugging), results are not stored.
            checkpoint_s3_uri = None

        # Once a trial gets resumed, the running job number has to feature in
        # the SM job_name
        job_name = f"{self.tuner_name}-{trial_id}"
        job_running_number = self.resumed_counter.get(trial_id, 0)
        if job_running_number > 0:
            job_name += f"-{job_running_number}"
        jobname = sagemaker_fit(
            sm_estimator=self.sm_estimator,
            # the encoder fixes json error "TypeError: Object of type 'int64' is not JSON serializable"
            hyperparameters=self._numpy_serialize(hyperparameters),
            checkpoint_s3_uri=checkpoint_s3_uri,
            job_name=job_name,
            **self.sagemaker_fit_kwargs,
        )
        logger.info(f"scheduled {jobname} for trial-id {trial_id}")
        self.job_id_mapping[trial_id] = jobname

    def _pause_trial(self, trial_id: int, result: Optional[dict]):
        self._stop_trial_job(trial_id)
        self.paused_jobs.add(trial_id)

    def _stop_trial(self, trial_id: int, result: Optional[dict]):
        training_job_name = self.job_id_mapping[trial_id]
        logger.info(f"stopping {trial_id} ({training_job_name})")
        self._stop_trial_job(trial_id)
        self.stopped_jobs.add(trial_id)

    def _stop_trial_job(self, trial_id: int):
        training_job_name = self.job_id_mapping[trial_id]
        try:
            self.sm_client.stop_training_job(TrainingJobName=training_job_name)
        except ClientError:
            # the scheduler may have decided to stop a job that finished already
            pass

    def _resume_trial(self, trial_id: int):
        assert (
            trial_id in self.paused_jobs
        ), f"Try to resume trial {trial_id} that was not paused before."
        self.paused_jobs.remove(trial_id)
        if trial_id in self.resumed_counter:
            self.resumed_counter[trial_id] += 1
        else:
            self.resumed_counter[trial_id] = 1

    def stdout(self, trial_id: int) -> List[str]:
        return get_log(self.job_id_mapping[trial_id])

    def stderr(self, trial_id: int) -> List[str]:
        return get_log(self.job_id_mapping[trial_id])

    @property
    def source_dir(self) -> Optional[str]:
        return self.sm_estimator.source_dir

    def set_entrypoint(self, entry_point: str):
        self.sm_estimator.entry_point = entry_point

    def entrypoint_path(self) -> Path:
        return Path(self.sm_estimator.entry_point)

    def __getstate__(self):
        # dont store sagemaker client that cannot be serialized, we could remove it by changing our interface
        # and having kwargs/args of SagemakerFramework in the constructor of this class (that would be serializable)
        # plus the class (for instance PyTorch)
        self.sm_estimator.sagemaker_session = None
        self.sm_estimator.latest_training_job = None
        self.sm_estimator.jobs = []
        return self.__dict__

    def __setstate__(self, state):
        self.__dict__ = state
        self.initialize_sagemaker_session()

        # adjust the dependencies when running Sagemaker backend on sagemaker with remote launcher
        # since they are in a different path
        is_running_on_sagemaker = "SM_OUTPUT_DIR" in os.environ
        if is_running_on_sagemaker:
            # todo support dependencies on Sagemaker estimator, one way would be to ship them with the remote
            #  dependencies
            self.sm_estimator.dependencies = [
                Path(dep).name for dep in self.sm_estimator.dependencies
            ]

    def initialize_sagemaker_session(self):
        if boto3.Session().region_name is None:
            # avoids error "Must setup local AWS configuration with a region supported by SageMaker."
            # in case no region is explicitely configured
            os.environ["AWS_DEFAULT_REGION"] = "us-west-2"

        if self.sm_estimator.instance_type in ("local", "local_gpu"):
            if (
                self.sm_estimator.instance_type == "local_gpu"
                and self.sm_estimator.instance_count > 1
            ):
                raise RuntimeError("Distributed Training in Local GPU is not supported")
            self.sm_estimator.sagemaker_session = LocalSession()
        else:
            # Use SageMaker boto3 client with default config. This is important
            # to configure automatic retry options properly
            self.sm_estimator.sagemaker_session = default_sagemaker_session()

    def copy_checkpoint(self, src_trial_id: int, tgt_trial_id: int):
        s3_source_path = self._checkpoint_s3_uri_for_trial(src_trial_id)
        s3_target_path = self._checkpoint_s3_uri_for_trial(tgt_trial_id)
        logger.info(
            f"Copying checkpoint files from {s3_source_path} to " + s3_target_path
        )
        result = s3_copy_files_recursively(s3_source_path, s3_target_path)
        num_action_calls = result["num_action_calls"]
        if num_action_calls == 0:
            logger.info(f"No checkpoint files found at {s3_source_path}")
        else:
            num_successful_action_calls = result["num_successful_action_calls"]
            assert num_successful_action_calls == num_action_calls, (
                f"{num_successful_action_calls} files copied successfully, "
                + f"{num_action_calls - num_successful_action_calls} failures. "
                + "Error:\n"
                + result["first_error_message"]
            )

    def delete_checkpoint(self, trial_id: int):
        s3_path = self._checkpoint_s3_uri_for_trial(trial_id)
        result = s3_delete_files_recursively(s3_path)
        num_action_calls = result["num_action_calls"]
        if num_action_calls > 0:
            num_successful_action_calls = result["num_successful_action_calls"]
            if num_successful_action_calls == num_action_calls:
                logger.info(
                    f"Deleted {num_action_calls} checkpoint files from {s3_path}"
                )
            else:
                logger.warning(
                    f"Successfully deleted {num_successful_action_calls} "
                    f"checkpoint files from {s3_path}, but failed to delete "
                    f"{num_action_calls - num_successful_action_calls} files. "
                    "Error:\n" + result["first_error_message"]
                )

    def set_path(
        self, results_root: Optional[str] = None, tuner_name: Optional[str] = None
    ):
        # we use the tuner-name to set the checkpoint directory
        self.tuner_name = tuner_name

    def on_tuner_save(self):
        # Re-initialize the session after `Tuner` is serialized
        self.initialize_sagemaker_session()

File Path: syne_tune/backend/sagemaker_backend/sagemaker_utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import functools
import logging
import os
import re
import subprocess
import tarfile
from ast import literal_eval
from pathlib import Path
from typing import List, Tuple, Dict, Optional

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError
from sagemaker.estimator import Framework
from sagemaker import Session

import syne_tune
from syne_tune.backend.trial_status import TrialResult
from syne_tune.report import retrieve
from syne_tune.util import experiment_path, random_string, s3_experiment_path

logger = logging.getLogger(__name__)


def default_config() -> Config:
    # a default config that avoid throttling
    return Config(retries={"max_attempts": 10, "mode": "standard"})


def default_sagemaker_session():
    sagemaker_client = boto3.client(service_name="sagemaker", config=default_config())
    return Session(sagemaker_client=sagemaker_client)


def get_log(jobname: str, log_client=None) -> List[str]:
    """
    :param jobname: name of a sagemaker training job
    :param log_client: a log client, for instance `boto3.client('logs')` if None, the client is instantiated with the
    default AWS configuration
    :return: lines appearing in the log of the Sagemaker training job
    """
    if log_client is None:
        log_client = boto3.client("logs", config=default_config())
    streams = log_client.describe_log_streams(
        logGroupName="/aws/sagemaker/TrainingJobs", logStreamNamePrefix=jobname
    )
    res = []

    for stream in streams["logStreams"]:
        get_response = functools.partial(
            log_client.get_log_events,
            logGroupName="/aws/sagemaker/TrainingJobs",
            logStreamName=stream["logStreamName"],
            startFromHead=True,
        )
        response = get_response()
        for event in response["events"]:
            res.append(event["message"])
        next_token = None
        while (
            "nextForwardToken" in response
            and next_token != response["nextForwardToken"]
        ):
            next_token = response["nextForwardToken"]
            response = get_response(nextToken=next_token)
            for event in response["events"]:
                res.append(event["message"])
    return res


def decode_sagemaker_hyperparameter(hp: str):
    # Sagemaker encodes hyperparameters as literals which are compatible with Python, except for true and false
    # that are respectively encoded as 'true' and 'false'.
    if hp == "true":
        return True
    elif hp == "false":
        return False
    return literal_eval(hp)


def sagemaker_search(
    trial_ids_and_names: List[Tuple[int, str]],
    sm_client=None,
) -> List[TrialResult]:
    """
    :param trial_ids_and_names: Trial ids and sagemaker jobnames to retrieve information from
    :param sm_client:
    :return: list of dictionary containing job information (status, creation-time, metrics, hyperparameters etc).
    In term of speed around 100 jobs can be retrieved per second.
    """
    if sm_client is None:
        sm_client = boto3.client(service_name="sagemaker", config=default_config())

    if len(trial_ids_and_names) == 0:
        return []

    trial_dict = {}

    # Sagemaker Search has a maximum length for filters of 20, hence we call search with 20 jobs at once
    bucket_limit = 20

    def chunks(lst, n):
        for i in range(0, len(lst), n):
            yield lst[i : i + n]

    # the results of Sagemaker search are sorted by last modified time,
    # we use this dictionary to return results sorted by trial-id
    name_to_trialid_dict = {name: trialid for trialid, name in trial_ids_and_names}

    for job_chunk in chunks(trial_ids_and_names, bucket_limit):
        search_params = {
            "MaxResults": bucket_limit,
            "Resource": "TrainingJob",
            "SearchExpression": {
                "Filters": [
                    {"Name": "TrainingJobName", "Operator": "Equals", "Value": name}
                    for (trial_id, name) in job_chunk
                ],
                "Operator": "Or",
            },
        }
        search_results = sm_client.search(**search_params)["Results"]

        for results in search_results:
            job_info = results["TrainingJob"]
            name = job_info["TrainingJobName"]

            # remove sagemaker specific stuff such as container_log_level from hyperparameters
            hps = {
                k: v
                for k, v in job_info["HyperParameters"].items()
                if not k.startswith("sagemaker_")
            }

            # Sagemaker encodes hyperparameters as literals, we evaluate them to retrieve the original type
            hps = {k: decode_sagemaker_hyperparameter(v) for k, v in hps.items()}

            metrics = retrieve(log_lines=get_log(name))

            trial_id = name_to_trialid_dict[name]

            trial_dict[trial_id] = TrialResult(
                trial_id=trial_id,
                config=hps,
                metrics=metrics,
                status=job_info["TrainingJobStatus"],
                creation_time=job_info["CreationTime"],
                training_end_time=job_info.get("TrainingEndTime", None),
            )

    # Sagemaker Search returns results sorted by last modified time, we reorder the results so that they are returned
    # with the same order as the trial-ids passed
    sorted_res = [
        trial_dict[trial_id]
        for trial_id, _ in trial_ids_and_names
        if trial_id in trial_dict
    ]
    return sorted_res


def metric_definitions_from_names(metrics_names):
    """
    :param metrics_names: names of the metrics present in the log.
    Metrics must be written in the log as [metric-name]: value, for instance [accuracy]: 0.23
    :return: a list of metric dictionaries that can be passed to sagemaker so that metrics are parsed from logs, the
    list can be passed to `metric_definitions` in sagemaker.
    """

    def metric_dict(metric_name):
        """
        :param metric_name:
        :return: a sagemaker metric definition to enable Sagemaker to interpret metrics from logs
        """
        regex = rf".*[tune-metric].*\"{re.escape(metric_name)}\": ([-+]?\d\.?\d*)"
        return {"Name": metric_name, "Regex": regex}

    return [metric_dict(m) for m in metrics_names]


def add_syne_tune_dependency(sm_estimator):
    # adds code of syne tune to the estimator to be sent with the estimator dependencies so that report.py or
    # other functions of syne tune can be found
    sm_estimator.dependencies = sm_estimator.dependencies + [
        str(Path(syne_tune.__path__[0]))
    ]


def sagemaker_fit(
    sm_estimator: Framework,
    hyperparameters: Dict[str, object],
    checkpoint_s3_uri: Optional[str] = None,
    wait: bool = False,
    job_name: Optional[str] = None,
    *sagemaker_fit_args,
    **sagemaker_fit_kwargs,
):
    """
    :param sm_estimator: sagemaker estimator to be fitted
    :param hyperparameters: dictionary of hyperparameters that are passed to `entry_point_script`
    :param checkpoint_s3_uri: checkpoint_s3_uri of Sagemaker Estimator
    :param wait: whether to wait for job completion
    :param metrics_names: names of metrics to track reported with `report.py`. In case those metrics are passed, their
    learning curves will be shown in Sagemaker console.
    :return: name of sagemaker job
    """
    experiment = sm_estimator
    experiment._hyperparameters = hyperparameters
    experiment.checkpoint_s3_uri = checkpoint_s3_uri

    experiment.fit(
        wait=wait, job_name=job_name, *sagemaker_fit_args, **sagemaker_fit_kwargs
    )

    return experiment.latest_training_job.job_name


def get_execution_role():
    """
    :return: sagemaker execution role that is specified with the environment variable `AWS_ROLE`, if not specified then
    we infer it by searching for the role associated to Sagemaker. Note that
    `import sagemaker; sagemaker.get_execution_role()`
    does not return the right role outside of a Sagemaker notebook.
    """
    if "AWS_ROLE" in os.environ:
        aws_role = os.environ["AWS_ROLE"]
        logger.info(
            f"Using Sagemaker role {aws_role} passed set as environment variable $AWS_ROLE"
        )
        return aws_role
    else:
        logger.info(
            f"No Sagemaker role passed as environment variable $AWS_ROLE, inferring it."
        )
        client = boto3.client("iam", config=default_config())
        sm_roles = client.list_roles(PathPrefix="/service-role/")["Roles"]
        for role in sm_roles:
            if "AmazonSageMaker-ExecutionRole" in role["RoleName"]:
                return role["Arn"]
        raise Exception(
            "Could not infer Sagemaker role, specify it by specifying `AWS_ROLE` environement variable "
            "or refer to https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html to create a new one"
        )


def untar(filename: Path):
    if str(filename).endswith("tar.gz"):
        tar = tarfile.open(filename, "r:gz")
        tar.extractall(path=filename.parent)
        tar.close()


def download_sagemaker_results(s3_path: Optional[str] = None):
    """
    Download results obtained after running tuning remotely on Sagemaker,
    e.g. when using `RemoteLauncher`.
    """
    if s3_path is None:
        s3_path = s3_experiment_path()
    tgt_dir = str(experiment_path())
    cmd = f"aws s3 sync {s3_path} {tgt_dir}"
    logger.info(f"downloading sagemaker results to {tgt_dir} with command {cmd}")
    subprocess.run(cmd.split(" "))


def map_identifier_limited_length(
    name: str, max_length: int = 63, rnd_digits: int = 4
) -> str:
    """
    If `name` is longer than 'max_length` characters, it is mapped to a new
    identifier of length `max_length`, being the concatenation of the first
    `max_length - rnd_digits` characters of `name`, followed by a random
    string of length `hash_digits`.

    :param name: Identifier to be limited in length
    :param max_length: Maximum length for output
    :param rnd_digits: See above
    :return: See above

    """
    orig_length = len(name)
    if orig_length <= max_length:
        return name
    else:
        assert 1 < rnd_digits < max_length
        postfix = random_string(rnd_digits)
        return name[: (max_length - rnd_digits)] + postfix


def _s3_traverse_recursively(s3_client, action, bucket: str, prefix: str) -> dict:
    """
    Traverses directory from root `prefix`. The function `action` is applied
    to all objects encountered, the signature is

        `action(s3_client, bucket, object_key)`

    'action' returns None if successful, otherwise an error message.
    We return a dict with 'num_action_calls', 'num_successful_action_calls',
    'first_error_message' (the error message for the first failed `action` call
    encountered).

    :param s3_client: S3 client
    :param action: See above
    :param bucket: S3 bucket name
    :param prefix: Prefix from where to traverse, must end with '/'
    :return: See above
    """
    more_objects = True
    continuation_kwargs = dict()
    list_objects_kwargs = dict(Bucket=bucket, Prefix=prefix, Delimiter="/")
    all_next_prefixes = []
    num_action_calls = 0
    num_successful_action_calls = 0
    first_error_message = None
    while more_objects:
        response = s3_client.list_objects_v2(
            **list_objects_kwargs, **continuation_kwargs
        )
        # Subdirectories
        for next_prefix in response.get("CommonPrefixes", []):
            all_next_prefixes.append(next_prefix["Prefix"])
        # Objects
        for source in response.get("Contents", []):
            ret_msg = action(s3_client, bucket, source["Key"])
            num_action_calls += 1
            if ret_msg is None:
                num_successful_action_calls += 1
            elif first_error_message is None:
                first_error_message = ret_msg
        more_objects = "NextContinuationToken" in response
        if more_objects:
            continuation_kwargs = {
                "ContinuationToken": response["NextContinuationToken"]
            }
    # Recursive calls
    for next_prefix in all_next_prefixes:
        result = _s3_traverse_recursively(s3_client, action, bucket, prefix=next_prefix)
        num_action_calls += result["num_action_calls"]
        num_successful_action_calls += result["num_successful_action_calls"]
        if first_error_message is None:
            first_error_message = result["first_error_message"]
    return dict(
        num_action_calls=num_action_calls,
        num_successful_action_calls=num_successful_action_calls,
        first_error_message=first_error_message,
    )


def _split_bucket_prefix(s3_path: str) -> (str, str):
    assert s3_path[:5] == "s3://", s3_path
    parts = s3_path[5:].split("/")
    bucket = parts[0]
    prefix = "/".join(parts[1:])
    if prefix[-1] != "/":
        prefix += "/"
    return bucket, prefix


def s3_copy_files_recursively(s3_source_path: str, s3_target_path: str) -> dict:
    """
    Recursively copies files from `s3_source_path` to `s3_target_path`.

    We return a dict with 'num_action_calls', 'num_successful_action_calls',
    'first_error_message' (the error message for the first failed `action` call
    encountered).

    :param s3_source_path:
    :param s3_target_path:
    :return: See above
    """
    src_bucket, src_prefix = _split_bucket_prefix(s3_source_path)
    trg_bucket, trg_prefix = _split_bucket_prefix(s3_target_path)

    def copy_action(s3_client, bucket: str, object_key: str) -> Optional[str]:
        assert object_key.startswith(
            src_prefix
        ), f"object_key = {object_key} must start with {src_prefix}"
        target_key = trg_prefix + object_key[len(src_prefix) :]
        copy_source = dict(Bucket=bucket, Key=object_key)
        ret_msg = None
        try:
            s3_client.copy_object(
                CopySource=copy_source, Bucket=trg_bucket, Key=target_key
            )
            logger.debug(
                f"Copied s3://{bucket}/{object_key}   to   s3://{trg_bucket}/{target_key}"
            )
        except ClientError as ex:
            ret_msg = str(ex)
        return ret_msg

    s3 = boto3.client("s3")
    return _s3_traverse_recursively(
        s3_client=s3, action=copy_action, bucket=src_bucket, prefix=src_prefix
    )


def s3_delete_files_recursively(s3_path: str) -> dict:
    """
    Recursively deletes files from `s3_path`.

    We return a dict with 'num_action_calls', 'num_successful_action_calls',
    'first_error_message' (the error message for the first failed `action` call
    encountered).

    :param s3_path:
    :return: See above
    """
    bucket_name, prefix = _split_bucket_prefix(s3_path)

    def delete_action(s3_client, bucket: str, object_key: str) -> Optional[str]:
        ret_msg = None
        try:
            s3_client.delete_object(Bucket=bucket, Key=object_key)
            logger.debug(f"Deleted s3://{bucket}/{object_key}")
        except ClientError as ex:
            ret_msg = str(ex)
        return ret_msg

    s3 = boto3.client("s3")
    return _s3_traverse_recursively(
        s3_client=s3, action=delete_action, bucket=bucket_name, prefix=prefix
    )

File Path: syne_tune/backend/simulator_backend/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/backend/simulator_backend/events.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from typing import List, Tuple, Optional
import heapq


@dataclass
class Event:
    """
    Base class for events dealt with in the simulator.

    """

    trial_id: int


@dataclass
class StartEvent(Event):
    """
    Start training evaluation function for `trial_id`. In fact, the function
    is run completely, and `OnTrialResultEvent` events and one `CompleteEvent`
    are generated.

    """


@dataclass
class CompleteEvent(Event):
    """
    Job for trial `trial_id` completes with status `status`. This is registered
    at the back-end.

    """

    status: str


@dataclass
class StopEvent(Event):
    """
    Job for trial `trial_id` is stopped. This leads to all later events for
    `trial_id` to be deleted, and a new `CompleteEvent`.

    """


@dataclass
class OnTrialResultEvent(Event):
    """
    Result reported by some worker arrives at the back-end and is registered
    there.

    """

    result: dict


EventHeapType = List[Tuple[float, int, Event]]


class SimulatorState:
    """
    Maintains the state of the simulator, in particular the event heap.

    `event_heap` is the priority queue for events, the key being `(time, cnt)`,
    where `time` is the event time, and `cnt` is a non-negative int used to
    break ties. When an event is added, the `cnt` value is taken from
    `events_added`. This means that ties are broken first_in_first_out.

    """

    def __init__(
        self, event_heap: Optional[EventHeapType] = None, events_added: int = 0
    ):
        if event_heap is None:
            event_heap = []
        self.event_heap = event_heap
        self.events_added = events_added

    def push(self, event: Event, event_time: float):
        """
        Push new event onto heap

        :param event:
        :param event_time:
        """
        heapq.heappush(self.event_heap, (event_time, self.events_added, event))
        self.events_added += 1

    def remove_events(self, trial_id: int):
        """
        Remove all events with trial_id equal to `trial_id`.

        :param trial_id:
        """
        self.event_heap = [
            elem for elem in self.event_heap if elem[2].trial_id != trial_id
        ]
        heapq.heapify(self.event_heap)

    def next_until(self, time_until: float) -> Optional[Tuple[float, Event]]:
        """
        Returns (and pops) event on top of heap, if event time is <=
        `time_until`. Otherwise, returns None.

        :param time_until:
        :return:
        """
        result = None
        if self.event_heap:
            top_time, _, top_event = self.event_heap[0]
            if top_time <= time_until:
                heapq.heappop(self.event_heap)
                result = (top_time, top_event)
        return result

File Path: syne_tune/backend/simulator_backend/simulator_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import json
import logging
import os
from datetime import timedelta
import copy
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

import numpy as np
import subprocess

from syne_tune.report import retrieve
from syne_tune.backend import LocalBackend
from syne_tune.backend.trial_status import TrialResult, Status, Trial
from syne_tune.backend.simulator_backend.time_keeper import SimulatedTimeKeeper
from syne_tune.backend.simulator_backend.events import (
    SimulatorState,
    StartEvent,
    CompleteEvent,
    StopEvent,
    OnTrialResultEvent,
)
from syne_tune.constants import ST_CHECKPOINT_DIR, ST_WORKER_TIMESTAMP, ST_TUNER_TIME
from syne_tune.tuner import DEFAULT_SLEEP_TIME

logger = logging.getLogger(__name__)


DEFAULT_DELAY = 0.05


@dataclass
class SimulatorConfig:
    """
    Configures the simulator:

    delay_on_trial_result:
        Time from `report` called on worker to result registered at back-end
    delay_complete_after_final_report:
        Time from final `report` called on worker to job completion being
        registered at back-end
    delay_complete_after_stop:
        Time from stop signal received at worker to job completion being
        registered at back-end
    delay_start:
        Time from start command being sent at back-end and job starting on
        the worker (which is free)
    delay_stop:
        Time from stop signal being sent at back-end to signal received at
        worker (which is running)
    """

    delay_on_trial_result: float = DEFAULT_DELAY
    delay_complete_after_final_report: float = DEFAULT_DELAY
    delay_complete_after_stop: float = DEFAULT_DELAY
    delay_start: float = DEFAULT_DELAY
    delay_stop: float = DEFAULT_DELAY

    def __post_init__(self):
        assert self.delay_on_trial_result >= 0
        assert self.delay_complete_after_final_report >= 0
        assert self.delay_complete_after_stop >= 0
        assert self.delay_start >= 0
        assert self.delay_stop >= 0
        # Otherwise, the final result may arrive after the job completion
        assert self.delay_on_trial_result <= self.delay_complete_after_final_report


class SimulatorBackend(LocalBackend):
    def __init__(
        self,
        entry_point: str,
        elapsed_time_attr: str,
        simulator_config: Optional[SimulatorConfig] = None,
        tuner_sleep_time: float = DEFAULT_SLEEP_TIME,
        debug_resource_attr: Optional[str] = None,
    ):
        """
        This simulator back-end drives experiments with tabulated training
        evaluation functions, which return their computation time rather than
        spend it. To this end, time (on the tuning instance) is simulated using
        a `time_keeper` and an event priority queue in `_simulator_state`.

        Time is advanced both by `Tuner.run` waiting, and by non-negligible
        computations during the tuning loop (in particular, we take care of
        `scheduler.suggest` and `scheduler.on_trial_result` there).

        When the `entry_point` script is executed, we wait for all results to
        be returned. In each result, the value for key `elapsed_time_attr`
        contains the time since start of the script. These values are used
        to place worker events on the simulated time line (represented by
        `simulator_state`).
        NOTE: If a trial is resumed, the elapsed_time value contains the time
        since start of the last recent resume, NOT the cumulative time used by
        the trial.

        Each method call starts by advancing time by what was spent outside,
        since the last recent call to the back-end. Then, all events in
        `simulator_state` are processed whose time is before the current time
        in `time_keeper`. The method ends by `time_keeper.mark_exit()`.

        Note: In this basic version of the simulator back-end, we still call a
        Python main function as a subprocess, which returns the requested
        metrics by looking them up or running a surrogate. This is flexible,
        but has the overhead of loading a table at every call. For faster
        simulations, use :class:`BlackboxRepositoryBackend` after bringing your
        tabulated data or surrogate benchmark into the blackbox repository.

        :param entry_point: Python main file to be tuned
        :param elapsed_time_attr: See above
        :param simulator_config: Parameters for simulator
        :param tuner_sleep_time: Effective sleep time in `Tuner.run`. This
            information is needed in `SimulatorCallback`

        """
        super().__init__(entry_point=entry_point, rotate_gpus=False)
        self.elapsed_time_attr = elapsed_time_attr
        if simulator_config is None:
            self.simulator_config = SimulatorConfig()
        else:
            self.simulator_config = simulator_config
        self.tuner_sleep_time = tuner_sleep_time
        self._debug_resource_attr = debug_resource_attr
        # Start with empty event queue
        self._simulator_state = SimulatorState()
        self._time_keeper = SimulatedTimeKeeper()
        self._next_results_to_fetch = dict()
        logger.setLevel(logging.INFO)  # Suppress DEBUG for this class

    @property
    def time_keeper(self) -> SimulatedTimeKeeper:
        return self._time_keeper

    @staticmethod
    def _debug_message(
        event_name: str, time: float, trial_id: int, pushed: bool = False, **kwargs
    ):
        msg_part = "push " if pushed else ""
        msg = f"[{msg_part}{event_name}:"
        parts = [f"time = {time:.2f}", f"trial_id = {trial_id}"] + [
            f"{k} = {v}" for k, v in kwargs.items()
        ]
        msg += ", ".join(parts) + "]"
        logger.debug(msg)

    def start_trial(
        self, config: Dict, checkpoint_trial_id: Optional[int] = None
    ) -> Trial:
        # Overwritten to record the correct `creation_time`
        trial_id = self.new_trial_id()
        if checkpoint_trial_id is not None:
            self.copy_checkpoint(
                src_trial_id=checkpoint_trial_id, tgt_trial_id=trial_id
            )
        self.trial_ids.append(trial_id)
        self._schedule(trial_id=trial_id, config=config)
        now = self._time_keeper.time_stamp()
        trial = Trial(
            trial_id=trial_id,
            config=config,
            creation_time=now,
        )
        self._trial_dict[trial_id] = trial

        return trial

    def _process_events_until_now(self):
        """
        We process all events in the queue with times before
        `time_keeper.time()`.
        """
        time_now = self._time_keeper.time()
        next_event = self._simulator_state.next_until(time_now)
        while next_event is not None:
            time_event, event = next_event
            trial_id = event.trial_id
            if isinstance(event, StartEvent):
                self._debug_message("StartEvent", time=time_event, trial_id=trial_id)
                # Run training script and push event for each result
                self._process_start_event(trial_id=trial_id, time_event=time_event)
            elif isinstance(event, CompleteEvent):
                trial_result = self._trial_dict[trial_id]
                status = event.status
                self._debug_message(
                    "CompleteEvent", time=time_event, trial_id=trial_id, status=status
                )
                training_end_time = self._time_keeper.start_time_stamp + timedelta(
                    seconds=time_event
                )
                if isinstance(trial_result, TrialResult):
                    trial_result.status = status
                    trial_result.training_end_time = training_end_time
                else:
                    # No results reported for the trial. This can happen if
                    # the trial failed
                    self._trial_dict[trial_id] = trial_result.add_results(
                        metrics=[], status=status, training_end_time=training_end_time
                    )
            elif isinstance(event, StopEvent):
                self._debug_message("StopEvent", time=time_event, trial_id=trial_id)
                # Remove all remaining events for `trial_id`. This includes
                # the `CompleteEvent` pushed with `StartEvent`, so there can
                # be no confusion with the 2nd `CompleteEvent` pushed by
                # `_stop_trial`.
                self._simulator_state.remove_events(trial_id)
            elif isinstance(event, OnTrialResultEvent):
                result = copy.copy(event.result)
                if self._debug_resource_attr:
                    k = self._debug_resource_attr
                    debug_kwargs = {k: result.get(k)}
                else:
                    debug_kwargs = dict()
                self._debug_message(
                    "OnTrialResultEvent",
                    time=time_event,
                    trial_id=trial_id,
                    **debug_kwargs,
                )
                # Append timestamps to `result`. This is done here, but not in
                # the other back-ends, for which timestamps are only added when
                # results are written out.
                result[ST_TUNER_TIME] = time_event
                if trial_id in self._next_results_to_fetch:
                    self._next_results_to_fetch[trial_id].append(result)
                else:
                    self._next_results_to_fetch[trial_id] = [result]
                trial_result = self._trial_dict[trial_id]
                if isinstance(trial_result, TrialResult):
                    trial_result.metrics.append(result)
                else:
                    self._trial_dict[trial_id] = trial_result.add_results(
                        metrics=[result],
                        status=Status.in_progress,
                        training_end_time=None,
                    )
            else:
                raise TypeError(f"Event at time {time_event} of unknown type: {event}")
            next_event = self._simulator_state.next_until(time_now)

    def _process_start_event(
        self, trial_id: int, time_event: float, config: Optional[dict] = None
    ):
        # Run training script and record results
        status, results = self._run_job_and_collect_results(trial_id, config=config)
        time_final_result = time_event
        deb_it = 0  # DEBUG
        for i, result in enumerate(results):
            elapsed_time = result.get(self.elapsed_time_attr)
            assert elapsed_time is not None, (
                f"Result for trial_id = {trial_id} does not contain "
                + f"{self.elapsed_time_attr} entry. Your code needs "
                + "to report elapsed time, and the attribute name "
                + "must be set as elapsed_time_attr here."
            )
            _time_result = time_event + float(elapsed_time)
            time_result = _time_result + self.simulator_config.delay_on_trial_result
            self._simulator_state.push(
                OnTrialResultEvent(trial_id=trial_id, result=result),
                event_time=time_result,
            )
            time_final_result = max(time_final_result, _time_result)
            # DEBUG:
            if deb_it < 10:
                if self._debug_resource_attr:
                    k = self._debug_resource_attr
                    debug_kwargs = {k: result.get(k)}
                else:
                    debug_kwargs = dict()
                self._debug_message(
                    "OnTrialResultEvent",
                    time=time_result,
                    trial_id=trial_id,
                    pushed=True,
                    **debug_kwargs,
                )
                deb_it += 1
        time_complete = (
            time_final_result + self.simulator_config.delay_complete_after_final_report
        )
        self._simulator_state.push(
            CompleteEvent(trial_id=trial_id, status=status), event_time=time_complete
        )
        self._debug_message(
            "CompleteEvent", time=time_complete, trial_id=trial_id, pushed=True
        )

    def _advance_by_outside_time(self):
        self._time_keeper.advance(self._time_keeper.real_time_since_last_recent_exit())

    def fetch_status_results(
        self, trial_ids: List[int]
    ) -> Tuple[Dict[int, Tuple[Trial, str]], List[Tuple[int, Dict]]]:
        self._advance_by_outside_time()
        # Process all events in the past
        self._process_events_until_now()
        # Results are collected in `_next_results_to_fetch`
        results = []
        for trial_id in trial_ids:
            result_list = self._next_results_to_fetch.get(trial_id)
            if result_list is not None:
                results.extend((trial_id, result) for result in result_list)
                self._last_metric_seen_index[trial_id] += len(result_list)
                del self._next_results_to_fetch[trial_id]
        if self._next_results_to_fetch:
            # Note: This tends to happen regularly with fast-running
            # benchmarks
            warn_msg = [
                "The following trials reported results, but are not covered "
                "by trial_ids. These results will be ignored:"
            ]
            for trial_id, result_list in self._next_results_to_fetch.items():
                status = self._trial_dict[trial_id].status
                msg_line = f"  trial_id {trial_id}: status = {status}, "
                if self._debug_resource_attr is None:
                    msg_line += f"num_results = {len(result_list)}"
                else:
                    resources = [
                        result[self._debug_resource_attr] for result in result_list
                    ]
                    msg_line += f"resources = {resources}"
                warn_msg.append(msg_line)
                self._last_metric_seen_index[trial_id] += len(result_list)
            logger.debug("\n".join(warn_msg))
            self._next_results_to_fetch = dict()

        if len(results) > 0 and ST_WORKER_TIMESTAMP in results[0]:
            results = sorted(results, key=lambda result: result[1][ST_WORKER_TIMESTAMP])

        trial_status_dict = dict()
        for trial_id in trial_ids:
            trial_result = self._trial_dict[trial_id]
            status = (
                trial_result.status
                if isinstance(trial_result, TrialResult)
                else Status.in_progress
            )
            trial = Trial(
                trial_id=trial_result.trial_id,
                config=trial_result.config,
                creation_time=trial_result.creation_time,
            )
            trial_status_dict[trial_id] = (trial, status)

        self._time_keeper.mark_exit()
        return trial_status_dict, results

    def _schedule(self, trial_id: int, config: Dict):
        """
        This is called by `start_trial` or `resume_trial`. We register a start
        event here. `config` can be ignored, it will be in
        `trial(trial_id).config` once the start event is executed.

        Note: This call is "non-blocking": The start event is registered
        here (in the future), but is not yet processed.
        """
        self._advance_by_outside_time()
        # Process all events in the past
        self._process_events_until_now()
        _time_start = self._time_keeper.time()
        time_start = _time_start + self.simulator_config.delay_start
        self._simulator_state.push(StartEvent(trial_id=trial_id), event_time=time_start)
        self._debug_message(
            "StartEvent", time=time_start, trial_id=trial_id, pushed=True
        )
        logger.debug(f"Simulated time since start: {_time_start:.2f} secs")
        self._time_keeper.mark_exit()

    def _all_trial_results(self, trial_ids: List[int]) -> List[TrialResult]:
        """
        Note: Since this is not used anymore in `fetch_results`, it can
        simply just return all registered trials which already have some
        results.
        This will not return trials which have just been started, but did not
        report any results yet.
        """
        results = []
        for trial_id in trial_ids:
            trial_result = self._trial_dict[trial_id]
            # Filter out entries which have not obtained any results
            if isinstance(trial_result, TrialResult):
                results.append(trial_result)
        return results

    def _pause_trial(self, trial_id: int, result: Optional[dict]):
        self._stop_or_pause_trial(trial_id, status=Status.paused)

    def _resume_trial(self, trial_id: int):
        pass

    def _stop_trial(self, trial_id: int, result: Optional[dict]):
        self._stop_or_pause_trial(trial_id, status=Status.stopped)

    def _stop_or_pause_trial(self, trial_id: int, status: str):
        """
        This is called by `stop_trial` or `pause_trial`.

        Note: This call is "blocking": Stop and complete events
        are not just registered here, but also processed.
        """
        self._advance_by_outside_time()
        time_stop = self._time_keeper.time() + self.simulator_config.delay_stop
        self._simulator_state.push(StopEvent(trial_id=trial_id), event_time=time_stop)
        self._debug_message("StopEvent", time=time_stop, trial_id=trial_id, pushed=True)
        # Note: We need to call `_process_events_until_now` twice. If we first
        # pushed the final `CompleteEvent`, it would be removed by the
        # `StopEvent`.
        self._time_keeper.advance_to(time_stop + 1e-3)
        # Process events up to and including `StopEvent`
        self._process_events_until_now()
        time_complete = (
            self._time_keeper.time() + self.simulator_config.delay_complete_after_stop
        )
        self._simulator_state.push(
            CompleteEvent(trial_id=trial_id, status=status), event_time=time_complete
        )
        self._debug_message(
            "CompleteEvent", time=time_complete, trial_id=trial_id, pushed=True
        )
        # Process final `CompleteEvent`
        self._time_keeper.advance_to(time_complete + 1e-3)
        self._process_events_until_now()
        self._time_keeper.mark_exit()

    def _run_job_and_collect_results(
        self, trial_id: int, config: Optional[dict] = None
    ) -> (str, List[dict]):
        """
        Runs training evaluation script for trial `trial_id`, using the config
        `trial(trial_id).config`. This is a blocking call, we wait for the
        script to finish, then parse all its results and return them.

        :param trial_id:
        :return: (final status, list of all results reported)
        """
        assert (
            trial_id in self._trial_dict
        ), f"Trial with trial_id = {trial_id} not registered with back-end"
        if config is None:
            config = self._trial_dict[trial_id].config

        # Run training script and fetch all results
        trial_path = self.trial_path(trial_id)
        os.makedirs(trial_path, exist_ok=True)
        config_copy = config.copy()
        config_copy[ST_CHECKPOINT_DIR] = str(trial_path / "checkpoints")
        config_str = " ".join(
            [f"--{key} {value}" for key, value in config_copy.items()]
        )

        def np_encoder(obj):
            if isinstance(obj, np.generic):
                return obj.item()

        with open(trial_path / "config.json", "w") as f:
            # the encoder fixes json error "TypeError: Object of type 'int64' is not JSON serializable"
            json.dump(config, f, default=np_encoder)
        cmd = f"python {self.entry_point} {config_str}"
        env = dict(os.environ)
        logging.info(f"running script with command: {cmd}")
        with open(trial_path / "std.out", "a") as stdout:
            with open(trial_path / "std.err", "a") as stderr:
                return_status = subprocess.run(
                    cmd.split(" "), stdout=stdout, stderr=stderr, env=env
                )
        if return_status.returncode == 0:
            status = Status.completed
        else:
            status = Status.failed
        # Read all reported results
        # Results are also read if the process failed
        # Note that `retrieve` returns all results, even those already
        # received before (in case the trial is resumed at least once).
        all_results = retrieve(log_lines=self.stdout(trial_id=trial_id))
        num_already_before = self._last_metric_seen_index[trial_id]
        assert num_already_before <= len(all_results), (
            f"Found {len(all_results)} total results, but have already "
            + f"processed {num_already_before} before!"
        )
        results = all_results[num_already_before:]

        return status, results

File Path: syne_tune/backend/simulator_backend/simulator_callback.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.tuner_callback import StoreResultsCallback
from syne_tune.backend.simulator_backend.simulator_backend import SimulatorBackend
from syne_tune import Tuner
from syne_tune.constants import ST_TUNER_TIME
from syne_tune import StoppingCriterion
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler

logger = logging.getLogger(__name__)


class SimulatorCallback(StoreResultsCallback):
    """
    Callback to be used in `Tuner.run` in order to support the
    :class:`SimulatorBackend`.

    This is doing two things. First, `on_tuning_sleep` is advancing the
    `time_keeper` of the simulator back-end by `tuner_sleep_time` (also
    defined in the back-end). The real sleep time in `Tuner` must be 0.

    Second, we need to make sure that results written out are annotated by
    simulated time, not real time. This is already catered for by
    `SimulatorBackend` adding ST_TUNER_TIME entries to each result it
    receives.

    Third (and most subtle), we need to make sure the stop criterion in
    `Tuner.run` is using simulated time instead of real time when making
    a decision based on `max_wallclock_time`. By default, `StoppingCriterion`
    takes `TuningStatus` as an input, which counts real time and knows nothing
    about simulated time. To this end, we modify `stop_criterion` of the tuner
    to instead depend on the ST_TUNER_TIME fields in the results received.
    This allows us to keep both `Tuner` and `TuningStatus` independent of the
    time keeper.

    """

    def __init__(self):
        # Note: `results_update_interval` is w.r.t. real time, not
        # simulated time. Storing results intermediately is not important for
        # the simulator back-end, so the default is larger
        super().__init__(add_wallclock_time=True)
        self._tuner_sleep_time = None
        self._time_keeper = None
        self._tuner = None
        self._backup_stop_criterion = None

    def _modify_stop_criterion(self, tuner: "Tuner"):
        stop_criterion = tuner.stop_criterion
        if not isinstance(stop_criterion, StoppingCriterion):
            # Note: We could raise an exception here ...
            logger.warning(
                "The stop_criterion argument to Tuner is not of type "
                + "StoppingCriterion. This can be problematic when using "
                + "the SimulatorBackend. If your stop_criterion depends on "
                + "wallclock time, you'll get wrong behaviour. It is highly "
                + "recommended to use StoppingCriterion!"
            )
        elif stop_criterion.max_wallclock_time is not None:
            # Since `TuningStatus` is measuring real time, not simulated time,
            # we need to replace the `max_wallclock_time` part of this criterion
            # by `max_metric_value` w.r.t. ST_TUNER_TIME. Note that
            # `SimulatorBackend` is adding ST_TUNER_TIME to any result it
            # receives
            self._backup_stop_criterion = stop_criterion
            max_wallclock_time = stop_criterion.max_wallclock_time
            new_stop_criterion = StoppingCriterion(
                max_num_trials_started=stop_criterion.max_num_trials_started,
                max_num_trials_completed=stop_criterion.max_num_trials_completed,
                max_cost=stop_criterion.max_cost,
                max_num_trials_finished=stop_criterion.max_num_trials_finished,
                max_metric_value={ST_TUNER_TIME: max_wallclock_time},
                max_num_evaluations=stop_criterion.max_num_evaluations,
            )
            tuner.stop_criterion = new_stop_criterion

    def on_tuning_start(self, tuner: "Tuner"):
        super(SimulatorCallback, self).on_tuning_start(tuner=tuner)
        if tuner.sleep_time != 0:
            logger.warning(
                "Setting sleep time of tuner to 0 as it is required for simulations."
            )
            tuner.sleep_time = 0
        backend = tuner.trial_backend
        assert isinstance(
            backend, SimulatorBackend
        ), "Use SimulatorCallback only together with SimulatorBackend"
        assert (
            tuner.sleep_time == 0
        ), "Initialize Tuner with sleep_time = 0 if you use the SimulatorBackend"
        self._time_keeper = backend.time_keeper
        scheduler = tuner.scheduler
        if isinstance(scheduler, FIFOScheduler):
            # Assign backend.time_keeper. It is important to do this here,
            # just at the start of an experiment, and not already at
            # construction of backend and scheduler. Otherwise, the way in
            # which backend and scheduler are serialized and deserialized for
            # remote tuning, leads to issues (in particular, the backend and its
            # time_keeper are recreated, so the scheduler refers to the wrong
            # time_keeper object then).
            scheduler.set_time_keeper(self._time_keeper)
        self._time_keeper.start_of_time()
        self._tuner_sleep_time = backend.tuner_sleep_time
        # Modify `tuner.stop_criterion` in case it depends on wallclock time
        self._modify_stop_criterion(tuner)
        self._tuner = tuner

    def on_tuning_sleep(self, sleep_time: float):
        self._time_keeper.advance(self._tuner_sleep_time)

    def on_tuning_end(self):
        super().on_tuning_end()
        # Restore `stop_criterion`
        self._tuner.stop_criterion = self._backup_stop_criterion
        self._tuner = None

File Path: syne_tune/backend/simulator_backend/time_keeper.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import time
from datetime import datetime, timedelta

from syne_tune.backend.time_keeper import TimeKeeper


class SimulatedTimeKeeper(TimeKeeper):
    """
    Here, time is simulated. It needs to be advanced explicitly.

    In addition, `mark_exit` and `real_time_since_last_recent_exit` are used to
    measure real time spent outside the back-end (i.e., in the tuner loop and
    scheduler). Namely, every method of :class:`SimulatorBackend` calls
    `mark_exit` before leaving, and `real_time_since_last_recent_exit` at the
    start, advancing the time counter accordingly.

    """

    def __init__(self):
        self._current_time = None
        self._start_time_stamp = None
        self._last_recent_exit = None

    @property
    def start_time_stamp(self) -> datetime:
        """
        :return: Time stamp (datetime) of (last recent) call of
        `start_of_time`
        """
        self._assert_has_started()
        return self._start_time_stamp

    def start_of_time(self):
        # This can be called multiple times, if multiple experiments are
        # run in sequence
        self._current_time = 0
        self._start_time_stamp = datetime.now()
        self.mark_exit()

    def _assert_has_started(self):
        assert (
            self._current_time is not None
        ), "RealTimeKeeper needs to be started, by calling start_of_time"

    def time(self) -> float:
        self._assert_has_started()
        return self._current_time

    def time_stamp(self) -> datetime:
        self._assert_has_started()
        return self._start_time_stamp + timedelta(seconds=self._current_time)

    def advance(self, step: float):
        self._assert_has_started()
        assert step >= 0
        self._current_time += step

    def advance_to(self, to_time: float):
        self._assert_has_started()
        self._current_time = max(to_time, self._current_time)

    def mark_exit(self):
        self._last_recent_exit = time.time()

    def real_time_since_last_recent_exit(self) -> float:
        self._assert_has_started()
        return time.time() - self._last_recent_exit

File Path: syne_tune/backend/time_keeper.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import time
from datetime import datetime


class TimeKeeper:
    """
    To be used by tuner, backend, and scheduler to measure time differences
    and wait for a specified amount of time. By centralizing this
    functionality here, we can support simulating experiments much faster than
    real time if the training evaluation function corresponds to a tabulated
    benchmark.

    """

    def start_of_time(self):
        """
        Called at the start of the experiment. Can be called multiple times
        if several experiments are run in sequence.
        """
        raise NotImplementedError

    def time(self) -> float:
        """
        :return: Time elapsed since the start of the experiment
        """
        raise NotImplementedError

    def time_stamp(self) -> datetime:
        """
        :return: Timestamp (datetime) corresponding to `time()`
        """
        raise NotImplementedError

    def advance(self, step: float):
        """
        Advance time by `step`. For real time, this means we sleep for
        `step`.
        """
        raise NotImplementedError


class RealTimeKeeper(TimeKeeper):
    def __init__(self):
        self._start_time = None

    def start_of_time(self):
        # This can be called multiple times, if multiple experiments are
        # run in sequence
        self._start_time = time.time()

    def _assert_has_started(self):
        assert (
            self._start_time is not None
        ), "RealTimeKeeper needs to be started, by calling start_of_time"

    def time(self) -> float:
        self._assert_has_started()
        return time.time() - self._start_time

    def time_stamp(self) -> datetime:
        self._assert_has_started()
        return datetime.now()

    def advance(self, step: float):
        self._assert_has_started()
        assert step >= 0
        time.sleep(step)

File Path: syne_tune/backend/trial_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from collections import defaultdict

from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging

from syne_tune.backend.trial_status import TrialResult, Trial, Status
from syne_tune.constants import ST_WORKER_TIMESTAMP

logger = logging.getLogger(__name__)


class TrialBackend:
    def __init__(self, delete_checkpoints: bool = False):
        """
        If `delete_checkpoints` is True, the checkpoints written by a trial are
        deleted once the trial is stopped or is registered as completed. Also,
        as part of `stop_all` called at the end of the tuning loop, all remaining
        checkpoints are deleted.

        :param delete_checkpoints: See above

        """
        self.delete_checkpoints = delete_checkpoints
        self.trial_ids = []
        self._trial_dict = {}

        # index of the last metric that was seen for each trial-id
        self._last_metric_seen_index = defaultdict(lambda: 0)

    def start_trial(
        self, config: dict, checkpoint_trial_id: Optional[int] = None
    ) -> TrialResult:
        """
        :param config: program arguments of `script`
        :param checkpoint_trial_id: id of a trial to be resumed, if given the checkpoint of this trial-id is be copied
        to the checkpoint of the new trial-id.
        """
        trial_id = self.new_trial_id()
        if checkpoint_trial_id is not None:
            self.copy_checkpoint(
                src_trial_id=checkpoint_trial_id, tgt_trial_id=trial_id
            )
        self.trial_ids.append(trial_id)
        self._schedule(trial_id=trial_id, config=config)
        now = datetime.now()
        trial = TrialResult(
            trial_id=trial_id,
            config=config,
            creation_time=now,
            status=Status.in_progress,
            metrics=[],
        )
        self._trial_dict[trial_id] = trial

        return trial

    def copy_checkpoint(self, src_trial_id: int, tgt_trial_id: int):
        """
        Copy the checkpoint folder from one trial to the other.

        :param src_trial_id:
        :param tgt_trial_id:
        """
        raise NotImplementedError()

    def delete_checkpoint(self, trial_id: int):
        """
        Removes checkpoint folder for a trial. It is OK for the folder not to
        exist.

        :param trial_id:
        """
        raise NotImplementedError()

    def resume_trial(self, trial_id: int, new_config: Optional[dict] = None):
        """
        :param trial_id: id of the trial to be resumed
        :param new_config: If given, the config maintained in trial.config is
            replaced by new_config
        :return:
        """
        assert trial_id < len(
            self.trial_ids
        ), "cannot resume a trial id that is not present"
        # todo assert that status is not running
        trial = self._trial_dict[trial_id]
        self._resume_trial(trial_id)
        if new_config is not None:
            trial.config = new_config
        self._schedule(
            trial_id=trial_id,
            config=trial.config,
        )

    def _resume_trial(self, trial_id: int):
        """
        update internal backend information when a trial gets resumed
        """
        raise NotImplementedError()

    def pause_trial(self, trial_id: int, result: Optional[dict] = None):
        """
        Checks that the operation is valid and call backend internal implementation to actually pause the trial.
        If the status is queried after this function, it should be `paused`.

        :param trial_id: ID of trial to pause
        :param result: Result dict based on which scheduler decided to pause the
            trial
        """
        # todo assert trial_id is valid
        # todo assert trial_id has not been stopped or paused before
        self._pause_trial(trial_id=trial_id, result=result)

    def _pause_trial(self, trial_id: int, result: Optional[dict]):
        """
        Backend specific operation that pauses the trial.
        """
        raise NotImplementedError()

    def stop_trial(self, trial_id: int, result: Optional[dict] = None):
        """
        Checks that the operation is valid and call backend internal implementation to actually stop the trial.
        If the status is queried after this function, it should be `stopped`.
        :param trial_id: ID of trial to stop
        :param result: Result dict based on which scheduler decided to stop the
            trial
        """
        # todo assert trial_id is valid
        # todo assert trial_id has not been stopped or paused before
        self._stop_trial(trial_id=trial_id, result=result)
        if self.delete_checkpoints:
            logger.info(f"Removing checkpoints for trial_id = {trial_id}")
            self.delete_checkpoint(trial_id=trial_id)  # checkpoint not needed anymore

    def _stop_trial(self, trial_id: int, result: Optional[dict]):
        """
        Backend specific operation that stops the trial.
        """
        raise NotImplementedError()

    def new_trial_id(self) -> int:
        return len(self.trial_ids)

    def _schedule(self, trial_id: int, config: dict):
        raise NotImplementedError()

    def _all_trial_results(self, trial_ids: List[int]) -> List[TrialResult]:
        """
        :param trial_ids:
        :return: list of results corresponding to the trial-id passed, contains all the results obtained since the start
        of the trial.
        """
        pass

    def fetch_status_results(
        self, trial_ids: List[int]
    ) -> Tuple[Dict[int, Tuple[Trial, str]], List[Tuple[int, dict]]]:
        """
        :param trial_ids: trials whose information should be fetch.
        :return: A tuple containing 1) a dictionary from trial-id to Trial and status information 2) list of
        trial-id/results pair for each new result that was emitted since the last call. The list of results is sorted
         by the worker time-stamp (last time-stamp appears last).
        """
        all_trial_results = self._all_trial_results(trial_ids)
        results = []
        for trial_result in all_trial_results:
            trial_id = trial_result.trial_id
            self._trial_dict[trial_id] = trial_result
            if len(trial_result.metrics) > 0:
                if trial_result.status in [
                    Status.paused,
                    Status.stopping,
                    Status.stopped,
                ]:
                    # metrics obtained after a stopping decision from a scheduler are hidden.
                    new_metrics = []
                else:
                    # we return the list of all new metrics, which may be empty if no new metrics were generated.
                    position_last_seen = self._last_metric_seen_index[trial_id]
                    new_metrics = trial_result.metrics[position_last_seen:]
                    self._last_metric_seen_index[trial_id] += len(new_metrics)
                    if (
                        self.delete_checkpoints
                        and trial_result.status == Status.completed
                    ):
                        logger.info(f"Removing checkpoints for trial_id = {trial_id}")
                        self.delete_checkpoint(trial_id=trial_id)
                for new_metric in new_metrics:
                    results.append((trial_id, new_metric))

        trial_status_dict = {}
        for trial_id in trial_ids:
            trial_result = self._trial_dict[trial_id]
            # we cast TrialResult to Trial to avoid downstream code depending on TrialResult which we should ultimately
            # remove (since it duplicates several information such as status or list of results)
            trial = Trial(
                trial_id=trial_result.trial_id,
                config=trial_result.config,
                creation_time=trial_result.creation_time,
            )
            trial_status_dict[trial_id] = (trial, trial_result.status)
        results = sorted(results, key=lambda result: result[1][ST_WORKER_TIMESTAMP])
        return trial_status_dict, results

    def stdout(self, trial_id: int) -> List[str]:
        """
        :param trial_id:
        :return: lines of the log of the trial (stdout)
        """
        raise NotImplementedError()

    def stderr(self, trial_id: int) -> List[str]:
        """
        :param trial_id:
        :return: lines of the log of the trial (stderr)
        """
        raise NotImplementedError()

    def stop_all(self):
        trial_results = self._all_trial_results(self.trial_ids)
        for trial in trial_results:
            if trial.status == Status.in_progress:
                self.stop_trial(trial_id=trial.trial_id)
        if self.delete_checkpoints:
            # Delete all remaining checkpoints (e.g., of paused trials)
            logger.info("Removing all remaining checkpoints of trials")
            for trial_id in self.trial_ids:
                self.delete_checkpoint(trial_id=trial_id)

    def set_path(
        self, results_root: Optional[str] = None, tuner_name: Optional[str] = None
    ):
        """
        :param results_root: the local folder that should contains the results of the tuning experiment.
        Used by Tuner to indicate a desired path where the results should be written to. This is used
         to unify the location of backend files and Tuner results when possible (in the local backend).
         By default, the backend does not do anything since not all backends may be able to unify their files
         locations.
        :param tuner_name: name of the tuner can be used for instance to save checkpoints on remote storage.
        """
        pass

    def entrypoint_path(self) -> Path:
        """
        :return: the path of the entrypoint to be executed
        """
        pass

    def set_entrypoint(self, entry_point: str):
        """
        Update the entrypoint to point path.
        :param entry_point: new path of the entrypoint.
        :return:
        """
        pass

    def on_tuner_save(self):
        """
        Called by :class:`Tuner` at the end of `save`
        """
        pass

File Path: syne_tune/backend/trial_status.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# requires python 3.7
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Optional, List

try:
    from typing_extensions import Literal
except ImportError:
    from typing import Literal


from syne_tune.constants import ST_WORKER_COST


class Status:
    completed: str = "Completed"
    in_progress: str = "InProgress"
    failed: str = "Failed"
    paused: str = "Paused"
    stopped: str = "Stopped"
    stopping: str = "Stopping"


@dataclass
class Trial:
    trial_id: int
    config: Dict[str, object]
    creation_time: datetime

    def add_results(self, metrics, status, training_end_time):
        return TrialResult(
            metrics=metrics,
            status=status,
            training_end_time=training_end_time,
            trial_id=self.trial_id,
            config=self.config,
            creation_time=self.creation_time,
        )


@dataclass
class TrialResult(Trial):
    # Metrics recorded for each call of `report`. Each metric is a dictionary from metric name to value (
    # could be numeric or string, the only constrain is that it must be compatible with json).
    metrics: List[Dict[str, object]]
    status: Literal[
        Status.completed,
        Status.in_progress,
        Status.failed,
        Status.stopped,
        Status.stopping,
    ]

    training_end_time: Optional[datetime] = None

    @property
    def seconds(self):
        # todo the robustness of this logic could be improved. Currently, if the job is still running the runtime is the
        #  difference between training end and start time. If the job is still running, it is the difference between now
        #  and start time. However, it should be the difference between the last updated time and start time.
        end_time = (
            datetime.now() if self.training_end_time is None else self.training_end_time
        )
        return (
            end_time.replace(tzinfo=None) - self.creation_time.replace(tzinfo=None)
        ).seconds

    @property
    def cost(self):
        if len(self.metrics) > 0:
            return self.metrics[-1].get(ST_WORKER_COST, None)
        else:
            return None

File Path: syne_tune/blackbox_repository/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.blackbox_repository.blackbox_offline import (  # noqa: F401
    BlackboxOffline,
    deserialize,
)
from syne_tune.blackbox_repository.repository import (  # noqa: F401
    load_blackbox,
    blackbox_list,
)
from syne_tune.blackbox_repository.blackbox_surrogate import add_surrogate  # noqa: F401
from syne_tune.blackbox_repository.simulated_tabular_backend import (  # noqa: F401
    BlackboxRepositoryBackend,
    UserBlackboxBackend,
)

File Path: syne_tune/blackbox_repository/blackbox.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from numbers import Number

import pandas as pd
from typing import Dict, Optional, Callable, List, Tuple, Union
import numpy as np


class Blackbox:
    def __init__(
        self,
        configuration_space: Dict,
        fidelity_space: Optional[Dict] = None,
        objectives_names: Optional[List[str]] = None,
    ):
        """
        Interface aiming at following [HPOBench](https://github.com/automl/HPOBench) for compatibility.
        """
        self.configuration_space = configuration_space
        self.fidelity_space = fidelity_space
        self.objectives_names = objectives_names

    def objective_function(
        self,
        configuration: Dict,
        fidelity: Union[Dict, Number] = None,
        seed: Optional[int] = None,
    ) -> Dict:
        """
        Returns an evaluation of the blackbox, first perform data check and then call `_objective_function` that should
        be overriden in the child class.
        :param configuration: configuration to be evaluated, should belong to `self.configuration_space`
        :param fidelity: not passing a fidelity is possible if either the blackbox does not have a fidelity space
        or if it has a single fidelity in its fidelity space. In the latter case, all fidelities are returned in form
        of a tensor with shape (num_fidelities, num_objectives).
        :param seed:
        :return: dictionary of objectives evaluated or tensor with shape (num_fidelities, num_objectives) if no fidelity
        was given.
        """
        if self.fidelity_space is None:
            assert fidelity is None
        else:
            if fidelity is None:
                assert (
                    len(self.fidelity_space) == 1
                ), "not passing a fidelity is only supported when only one fidelity is present."

        if isinstance(fidelity, Number):
            # allows to call
            # `objective_function(configuration=..., fidelity=2)`
            # instead of
            # `objective_function(configuration=..., {'num_epochs': 2})`
            fidelity_names = list(self.fidelity_space.keys())
            assert (
                len(fidelity_names) == 1
            ), "passing numeric value is only possible when there is a single fidelity in the fidelity space."
            fidelity = {fidelity_names[0]: fidelity}

        # todo check configuration/fidelity matches their space
        return self._objective_function(
            configuration=configuration,
            fidelity=fidelity,
            seed=seed,
        )

    def _objective_function(
        self,
        configuration: Dict,
        fidelity: Optional[Dict] = None,
        seed: Optional[int] = None,
    ) -> Dict:
        """
        Override this function to provide your benchmark function.
        """
        pass

    def __call__(self, *args, **kwargs) -> Dict:
        """
        Allows to call blackbox directly as a function rather than having to call the specific method.
        :return:
        """
        return self.objective_function(*args, **kwargs)

    def hyperparameter_objectives_values(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        :return: a tuple of two dataframes, the first one contains hyperparameters values and the second
        one contains objective values, this is used when fitting a surrogate model.
        """
        pass

    @property
    def fidelity_values(self) -> Optional[np.array]:
        """
        :return: Fidelity values; or None if the blackbox has none
        """
        return None


def from_function(
    configuration_space: Dict,
    eval_fun: Callable,
    fidelity_space: Optional[Dict] = None,
    objectives_names: Optional[List[str]] = None,
):
    """
    Helper to create a blackbox from a function, useful for test or to wrap-up real blackbox functions.
    :param configuration_space:
    :param eval_fun: function that returns dictionary of objectives given configuration and fidelity
    :param fidelity_space:
    :return:
    """

    class BB(Blackbox):
        def __init__(self):
            super(BB, self).__init__(
                configuration_space=configuration_space,
                fidelity_space=fidelity_space,
                objectives_names=objectives_names,
            )

        def objective_function(
            self,
            configuration: Dict,
            fidelity: Optional[Dict] = None,
            seed: Optional[int] = None,
        ) -> Dict:
            return eval_fun(configuration, fidelity, seed)

    return BB()

File Path: syne_tune/blackbox_repository/blackbox_offline.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
from typing import Dict, List, Optional, Union

import pandas as pd

from syne_tune.blackbox_repository.blackbox import Blackbox
from syne_tune.blackbox_repository.serialize import (
    serialize_configspace,
    deserialize_configspace,
    serialize_metadata,
    deserialize_metadata,
)


class BlackboxOffline(Blackbox):
    def __init__(
        self,
        df_evaluations: pd.DataFrame,
        configuration_space: Dict,
        fidelity_space: Optional[Dict] = None,
        objectives_names: Optional[List[str]] = None,
        seed_col: Optional[str] = None,
    ):
        """
        A blackbox obtained given offline evaluations each row of the dataframe should contain one evaluation given a
        fixed configuration, fidelity and seed. The columns must corresponds the provided configuration and fidelity
        space, by default all columns that are prefixed by "metric_" are assumed to be metrics but this can be overrided
        by providing metric columns.
        :param df_evaluations:
        :param configuration_space:
        :param fidelity_space:
        :param objectives_names: names of the metrics, by default consider all metrics prefixed by "metric_" to be metrics
        :param seed_col: optional, can be used when multiple seeds are recorded
        """
        if objectives_names is not None:
            self.metric_cols = objectives_names
            for col in objectives_names:
                assert (
                    col in df_evaluations.columns
                ), f"column {col} from metric columns not found in dataframe"
        else:
            self.metric_cols = [
                col for col in df_evaluations.columns if col.startswith("metric_")
            ]

        super(BlackboxOffline, self).__init__(
            configuration_space=configuration_space,
            fidelity_space=fidelity_space,
            objectives_names=objectives_names,
        )

        hp_names = list(configuration_space.keys())
        self.index_cols = hp_names
        if fidelity_space is not None:
            fidelity_names = list(fidelity_space.keys())
            assert len(set(fidelity_names).intersection(hp_names)) == 0
            self.index_cols += fidelity_names

        self.seed_col = seed_col
        if seed_col is not None:
            assert seed_col not in self.index_cols, f"column {seed_col} duplicated"
            self.index_cols.append(seed_col)

        for col in self.index_cols:
            assert (
                col in df_evaluations.columns
            ), f"column {col} from configuration or fidelity space not found in dataframe"

        self.df = df_evaluations.set_index(self.index_cols)

    def hyperparameter_objectives_values(self):
        columns = self.index_cols
        if self.seed_col is not None:
            columns.remove(self.seed_col)
        X = self.df.reset_index().loc[:, columns]
        y = self.df.loc[:, self.metric_cols]
        return X, y

    def _objective_function(
        self,
        configuration: Dict,
        fidelity: Optional[Dict] = None,
        seed: Optional[int] = None,
    ) -> Dict[str, float]:
        """
        Return the dictionary of objectives for a configuration/fidelity/seed.
        :param configuration:
        :param fidelity:
        :param seed:
        :return:
        """
        # todo: we should check range configuration with configspaces
        # query the configuration in the list of available ones
        key_dict = configuration
        if self.seed_col is not None:
            key_dict[self.seed_col] = seed
        if self.fidelity_space is not None and fidelity is not None:
            key_dict.update(fidelity)
        if self.fidelity_space is not None and fidelity is None:
            keys = tuple(set(self.index_cols) - set(self.fidelity_space.keys()))
        else:
            keys = self.index_cols
        output = self.df.xs(tuple(key_dict[col] for col in keys), level=keys).loc[
            :, self.metric_cols
        ]
        if len(output) == 0:
            raise ValueError(
                f"the hyperparameter {configuration} is not present in available evaluations. Use `add_surrogate(blackbox)` if"
                f" you want to add interpolation or a surrogate model that support querying any configuration."
            )
        if fidelity is not None or self.fidelity_space is None:
            return output.iloc[0].to_dict()
        else:
            # TODO select only the fidelity values in the self.fidelity_space, since it might be the case there are more
            #  values in the dataframe. Then the output tensor has larger number of elements than expected num_fidelities.
            return output.to_numpy()

    def __str__(self):
        stats = {
            "total evaluations": len(self.df),
            "objectives": self.objectives_names,
            "hyperparameters": self.configuration_space.get_hyperparameter_names(),
        }
        stats_str = ", ".join([f"{k}: {v}" for k, v in stats.items()])
        return f"offline blackbox: {stats_str}"


def serialize(
    bb_dict: Dict[str, BlackboxOffline], path: str, categorical_cols: List[str] = []
):
    """
    :param bb_dict:
    :param path:
    :param categorical_cols: optional, allow to retrieve columns as categories, lower drastically the memory
     footprint when few values are present
    :return:
    """
    if isinstance(bb_dict, BlackboxOffline):
        # todo hack that allows to call `serialize(bb)` instead of `serialize({"dummy-task-name": bb})`
        # not sure if we should keep it
        bb_dict = {path.stem, bb_dict}

    # check all blackboxes share the same search space and have evaluated the same hyperparameters
    bb_first = next(iter(bb_dict.values()))
    for bb in bb_dict.values():
        assert bb.configuration_space == bb_first.configuration_space
        assert bb.fidelity_space == bb_first.fidelity_space
        assert bb.objectives_names == bb_first.objectives_names

    path = Path(path)
    path.mkdir(exist_ok=True)

    serialize_configspace(
        path=path,
        configuration_space=bb_first.configuration_space,
        fidelity_space=bb_first.fidelity_space,
    )

    for name, bb in bb_dict.items():
        df = bb.df
        df["task"] = name
        # we use gzip as snappy is not supported for fastparquet engine compression
        # gzip is slower than the default snappy but more compact
        df.reset_index().to_parquet(
            path / f"data-{name}.parquet",
            index=False,
            compression="gzip",
            engine="fastparquet",
        )

    serialize_metadata(
        path=path,
        metadata={
            "objectives_names": bb_first.objectives_names,
            "task_names": list(bb_dict.keys()),
            "seed_col": bb_first.seed_col,
            "categorical_cols": categorical_cols,
        },
    )


def deserialize(path: str) -> Union[Dict[str, BlackboxOffline], BlackboxOffline]:
    """
    :param path: where to find blackbox serialized information (at least data.csv.zip and configspace.json)
    :param groupby_col: separate evaluations into a list of blackbox with different task if the column is provided
    :return: list of blackboxes per task, or single blackbox in the case of a single task
    """
    configuration_space, fidelity_space = deserialize_configspace(path)

    assert (
        configuration_space is not None
    ), f"configspace.json could not be found in {path}"

    metadata = deserialize_metadata(path)
    metric_cols = metadata["objectives_names"]
    seed_col = metadata["seed_col"]
    cat_cols = metadata.get("categorical_cols")  # optional
    task_names = metadata.get("task_names")

    # need to specify columns to have categorical encoding of columns (rather than int or float)
    # this is required as it has a massive effect on memory usage; we use fastparquet for the engine as pyarrow does
    # not handle categorization of int/float columns
    df_tasks = {
        task: pd.read_parquet(
            Path(path) / f"data-{task}.parquet",
            categories=cat_cols,
            engine="fastparquet",
        )
        for task in task_names
    }

    return {
        task: BlackboxOffline(
            df_evaluations=df,
            configuration_space=configuration_space,
            fidelity_space=fidelity_space,
            objectives_names=metric_cols,
            seed_col=seed_col,
        )
        for task, df in df_tasks.items()
    }

File Path: syne_tune/blackbox_repository/blackbox_surrogate.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, Dict
import pandas as pd
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np

from syne_tune.config_space import Categorical
from syne_tune.blackbox_repository.blackbox import Blackbox


class Columns(BaseEstimator, TransformerMixin):
    def __init__(self, names=None):
        self.names = names

    def fit(self, *args, **kwargs):
        return self

    def transform(self, X):
        return X[self.names]


def _default_surrogate(surrogate):
    if surrogate is not None:
        return surrogate
    else:
        return KNeighborsRegressor(n_neighbors=1)


class BlackboxSurrogate(Blackbox):
    def __init__(
        self,
        X: pd.DataFrame,
        y: pd.DataFrame,
        configuration_space: Dict,
        fidelity_space: Optional[Dict] = None,
        fidelity_values: Optional[np.array] = None,
        surrogate=None,
        max_fit_samples: Optional[int] = None,
        name: Optional[str] = None,
    ):
        """
        Fits a blackbox surrogates that can be evaluated anywhere, which can be useful for supporting
        interpolation/extrapolation. To wrap an existing blackbox with a surrogate estimator, use `add_surrogate`
        which automatically extract X, y matrices from available blackbox evaluations.
        :param X: dataframe containing hyperparameters values, columns should be the ones in configuration_space
        and fidelity_space
        :param y: dataframe containing objectives values
        :param configuration_space:
        :param fidelity_space:
        :param surrogate: the model that is fitted to predict objectives given any configuration, default to
        KNeighborsRegressor(n_neighbors=1).
        Possible examples: KNeighborsRegressor(n_neighbors=1), MLPRegressor() or any estimator obeying Scikit-learn API.
        The model is fit on top of pipeline that applies basic feature-processing to convert rows in X to vectors.
        We use the configuration_space hyperparameters types to deduce the types of columns in X (for instance
        CategoricalHyperparameter are one-hot encoded).
        :param max_fit_samples: maximum number of samples to be fed to the surrogate estimator, if the more data points
        than this number are passed, then they are subsampled without replacement.
        :param name:
        """
        super(BlackboxSurrogate, self).__init__(
            configuration_space=configuration_space,
            fidelity_space=fidelity_space,
            objectives_names=y.columns,
        )
        assert len(X) == len(y)
        # todo other types of assert with configuration_space, objective_names, ...
        self.surrogate = _default_surrogate(surrogate)
        self.max_fit_samples = max_fit_samples
        self.fit_surrogate(
            X=X, y=y, surrogate=surrogate, max_samples=self.max_fit_samples
        )
        self.name = name
        self._fidelity_values = fidelity_values

    @property
    def fidelity_values(self) -> np.array:
        return self._fidelity_values

    @staticmethod
    def make_model_pipeline(configuration_space, fidelity_space, model):
        # gets hyperparameters types, categorical for CategoricalHyperparameter, numeric for everything else
        numeric = []
        categorical = []

        if fidelity_space is not None:
            surrogate_hps = dict()
            surrogate_hps.update(configuration_space)
            surrogate_hps.update(fidelity_space)
        else:
            surrogate_hps = configuration_space

        for hp_name, hp in surrogate_hps.items():
            if isinstance(hp, Categorical):
                categorical.append(hp_name)
            else:
                numeric.append(hp_name)

        # builds a pipeline that standardize numeric features and one-hot categorical ones before applying
        # the surrogate model
        features_union = []
        if len(categorical) > 0:
            features_union.append(
                (
                    "categorical",
                    make_pipeline(
                        Columns(names=categorical),
                        OneHotEncoder(sparse=False, handle_unknown="ignore"),
                    ),
                )
            )
        if len(numeric) > 0:
            features_union.append(
                ("numeric", make_pipeline(Columns(names=numeric), StandardScaler()))
            )

        return Pipeline(
            [
                ("features", FeatureUnion(features_union)),
                ("standard scaler", StandardScaler(with_mean=False)),
                ("model", model),
            ]
        )

    def fit_surrogate(
        self, X, y, surrogate=None, max_samples: Optional[int] = None
    ) -> Blackbox:
        """
        Fits a surrogate model to a blackbox.
        :param surrogate: fits the model and apply the model transformation when evaluating a
        blackbox configuration. Possible example: KNeighborsRegressor(n_neighbors=1), MLPRegressor() or any estimator
        obeying Scikit-learn API.
        """
        self.surrogate = _default_surrogate(surrogate)

        self.surrogate_pipeline = self.make_model_pipeline(
            configuration_space=self.configuration_space,
            fidelity_space=self.fidelity_space,
            model=surrogate,
        )
        # todo would be nicer to have this in the feature pipeline
        if max_samples is not None and max_samples < len(X):
            random_indices = np.random.permutation(len(X))[:max_samples]
            self.surrogate_pipeline.fit(
                X=X.loc[random_indices], y=y.loc[random_indices]
            )
        else:
            self.surrogate_pipeline.fit(X=X, y=y)
        return self

    def _objective_function(
        self,
        configuration: Dict,
        fidelity: Optional[Dict] = None,
        seed: Optional[int] = None,
    ) -> Dict[str, float]:
        surrogate_input = configuration.copy()
        if fidelity is not None or self.fidelity_values is None:
            if fidelity is not None:
                surrogate_input.update(fidelity)
            # use the surrogate model for prediction
            prediction = self.surrogate_pipeline.predict(
                pd.DataFrame([surrogate_input])
            )

            # converts the returned nd-array with shape (1, num_metrics) to the list of objectives values
            prediction = prediction.reshape(-1).tolist()

            # convert prediction to dictionary
            return dict(zip(self.objectives_names, prediction))
        else:
            # when no fidelity is given and a fidelity space exists, we return all fidelities
            # we construct a input dataframe with all fidelity for the configuration given to call the transformer
            # at once which is more efficient due to vectorization
            surrogate_input_df = pd.DataFrame(
                [surrogate_input] * len(self.fidelity_values)
            )
            surrogate_input_df[
                next(iter(self.fidelity_space.keys()))
            ] = self.fidelity_values
            objectives_values = self.surrogate_pipeline.predict(surrogate_input_df)
            return objectives_values


def add_surrogate(blackbox: Blackbox, surrogate=None, configuration_space=None):
    """
    Fits a blackbox surrogates that can be evaluated anywhere, which can be useful
    for supporting interpolation/extrapolation.
    :param blackbox: the blackbox must implement `hyperparameter_objectives_values`
        so that input/output are passed to estimate the model, see `BlackboxOffline`
        or `BlackboxTabular
    :param surrogate: the model that is fitted to predict objectives given any
        configuration. Possible examples: `KNeighborsRegressor(n_neighbors=1)`,
        `MLPRegressor()` or any estimator obeying Scikit-learn API.
        The model is fit on top of pipeline that applies basic feature-processing
        to convert rows in X to vectors. We use `config_space` to deduce the types
        of columns in X (categorical parameters are 1-hot encoded).
    :param configuration_space: configuration space for the resulting blackbox surrogate.
        The default is `blackbox.configuration_space`. But note that if `blackbox`
        is tabular, the domains in `blackbox.configuration_space` are typically
        categorical even for numerical parameters.
    :return: a blackbox where the output is obtained through the fitted surrogate
    """
    surrogate = _default_surrogate(surrogate)
    if configuration_space is None:
        configuration_space = blackbox.configuration_space
    X, y = blackbox.hyperparameter_objectives_values()
    return BlackboxSurrogate(
        X=X,
        y=y,
        configuration_space=configuration_space,
        fidelity_space=blackbox.fidelity_space,
        fidelity_values=blackbox.fidelity_values,
        surrogate=surrogate,
    )

File Path: syne_tune/blackbox_repository/blackbox_tabular.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
import pandas as pd
import numpy as np

from syne_tune.blackbox_repository.blackbox import Blackbox
from syne_tune.blackbox_repository.serialize import (
    serialize_configspace,
    deserialize_configspace,
    deserialize_metadata,
    serialize_metadata,
)


class BlackboxTabular(Blackbox):
    def __init__(
        self,
        hyperparameters: pd.DataFrame,
        configuration_space: Dict,
        fidelity_space: Dict,
        objectives_evaluations: np.array,
        fidelity_values: Optional[np.array] = None,
        objectives_names: Optional[List[str]] = None,
    ):
        """
        Blackbox that contains tabular evaluations (e.g. all hyperparameters evaluated on all fidelities).
        We use a separate class as BlackboxOffline as performance improvement can be made by avoiding to repeat
        hyperparameters and by storing all evaluations in a single table.
        :param hyperparameters: dataframe of hyperparameters, shape (num_evals, num_hps), columns must match
        hyperparameter names of configuration_space
        :param configuration_space:
        :param fidelity_space: only single fidelity supported for now
        :param objectives_evaluations: values of recorded objectives, must have shape
        (num_evals, num_seeds, num_fidelities, num_objectives)
        :param fidelity_values: values of the `num_fidelities` fidelities, default to [1, ..., `num_fidelities`]
        :param objectives_names:
        """
        super(BlackboxTabular, self).__init__(
            configuration_space=configuration_space,
            fidelity_space=fidelity_space,
            objectives_names=objectives_names,
        )
        # todo missing-value support, should boils down to droping nans in `hyperparameter_objectives_values`
        num_hps = len(hyperparameters.columns)

        assert objectives_evaluations.ndim == 4
        (
            num_evals,
            num_seeds,
            num_fidelities,
            num_objectives,
        ) = objectives_evaluations.shape

        self.num_seeds = num_seeds
        self.num_fidelities = num_fidelities
        if fidelity_values is None:
            self._fidelity_values = np.arange(num_fidelities) + 1
        else:
            # assert sorted(fidelity_values.tolist()) == fidelity_values
            self._fidelity_values = fidelity_values

        # allows to retrieve the index in the objectives_evaluations of a given fidelity
        self.fidelity_map = {
            value: index for index, value in enumerate(self._fidelity_values)
        }
        self.hyperparameters = hyperparameters

        # builds a dataframe to retrieve in O(1) index given a hyperparameter, we could have use a dict but chose a
        # dataframe instead as 1) it is easier since the hyperparameters are itself given in a dataframe (otherwise
        # we would need to have hashable type from the dataframe value) 2) we can support in the future querying
        # multiple results at once efficiently
        self._hp_cols = list(hyperparameters.columns.values)
        self.hyperparameters_index = hyperparameters.copy()
        self.hyperparameters_index["index"] = hyperparameters.index
        self.hyperparameters_index.set_index(self._hp_cols, inplace=True)

        self.objectives_evaluations = objectives_evaluations
        if objectives_names is None:
            self.objectives_names = [f"y{i}" for i in range(num_objectives)]

        assert len(self.objectives_evaluations) == len(hyperparameters)
        assert len(fidelity_space) == 1, "only support single fidelity for now"
        assert (
            max(self._fidelity_values) <= list(fidelity_space.values())[0].upper
        ), f"{max(self._fidelity_values)}, {max(next(iter(fidelity_space.values())).upper)}"
        assert len(hyperparameters) == len(
            hyperparameters.drop_duplicates()
        ), "some hps are duplicated, use a seed column"
        assert len(configuration_space) == num_hps
        for name in configuration_space.keys():
            assert name in hyperparameters.columns

        assert len(self.objectives_names) == num_objectives

    def _objective_function(
        self,
        configuration: Union[Dict, int],
        fidelity: Optional[Dict] = None,
        seed: Optional[int] = None,
    ) -> Dict:
        if seed is not None:
            assert 0 <= seed < self.num_seeds
        else:
            seed = np.random.randint(0, self.num_seeds)
        if not isinstance(configuration, dict):
            objectives_values = self.objectives_evaluations[configuration, seed, :, :]
            return objectives_values
        try:
            key = tuple(configuration[key] for key in self._hp_cols)
            matching_index = self.hyperparameters_index.loc[key].values
        except KeyError:
            raise ValueError(
                f"the hyperparameter {configuration} is not present in available evaluations. Use `add_surrogate(blackbox)` if"
                f" you want to add interpolation or a surrogate model that support querying any configuration."
            )

        df_found = self.hyperparameters.loc[matching_index]
        assert len(df_found) == 1
        index = df_found.index.values[0]

        if fidelity is None:
            # returns all fidelities
            objectives_values = self.objectives_evaluations[index, seed, :, :]
            return objectives_values
        else:
            fidelity_index = self.fidelity_map[list(fidelity.values())[0]]
            objectives_values = self.objectives_evaluations[
                index, seed, fidelity_index, :
            ]
            return dict(zip(self.objectives_names, objectives_values))

    @property
    def fidelity_values(self) -> np.array:
        return self._fidelity_values

    def _impute_objectives_values(self) -> Tuple[pd.DataFrame, np.array]:
        """Replaces nan values in objectives with first previous non-nan value.

        Time objective should be cumulative, otherwise each step will consume additional time.
        """
        # Replace nan with previous value. Assumes that elapsed time is cumulative.
        objectives_evaluations = self.objectives_evaluations.copy()
        hyperparameters = self.hyperparameters.copy()
        (
            num_configs,
            num_seeds,
            num_fidelities,
            num_objectives,
        ) = objectives_evaluations.shape
        for config_idx in range(num_configs):
            for seed_idx in range(num_seeds):
                for fidelity_idx in range(num_fidelities):
                    for objective_idx in range(num_objectives):
                        if np.isnan(
                            objectives_evaluations[config_idx][seed_idx][fidelity_idx][
                                objective_idx
                            ]
                        ):
                            objectives_evaluations[config_idx][seed_idx][fidelity_idx][
                                objective_idx
                            ] = objectives_evaluations[config_idx][seed_idx][
                                fidelity_idx - 1
                            ][
                                objective_idx
                            ]
        # Drop all hyperparameters with all nan objectives.
        nan_mask = np.isnan(objectives_evaluations).any((1, 2, 3))
        hyperparameters = hyperparameters[~nan_mask]
        objectives_evaluations = objectives_evaluations[~nan_mask]
        return hyperparameters, objectives_evaluations

    def hyperparameter_objectives_values(self):
        """
        :return: X, y of shape (num_evals * num_seeds * num_fidelities, num_hps)
        and (num_evals * num_seeds * num_fidelities, num_objectives)
        """
        objectives_evaluations = self.objectives_evaluations
        hyperparameters = self.hyperparameters
        if np.isnan(np.sum(objectives_evaluations)):
            hyperparameters, objectives_evaluations = self._impute_objectives_values()

        Xs = []
        ys = []
        for fidelity_index, fidelity_value in enumerate(self.fidelity_values):
            X = hyperparameters.copy()
            X[list(self.fidelity_space.keys())[0]] = fidelity_value
            for seed in range(self.num_seeds):
                Xs.append(X)
                # (num_evals, num_objectives)
                ys.append(objectives_evaluations[:, seed, fidelity_index, :])
        X = pd.concat(Xs, ignore_index=True)
        y = pd.DataFrame(data=np.vstack(ys), columns=self.objectives_names)
        return X, y

    def rename_objectives(self, objective_name_mapping: Dict[str, str]):
        """
        :param objective_name_mapping: dictionary from old objective name to new one, old objective name must be present
        in the blackbox
        :return: a blackbox with as many objectives as objective_name_mapping
        """
        # todo add test
        for old_name in objective_name_mapping.keys():
            assert old_name in self.objectives_names
        objective_indices = dict(
            zip(self.objectives_names, range(len(self.objectives_names)))
        )
        new_objectives_indices = [
            objective_indices[old_obj_name]
            for old_obj_name in objective_name_mapping.keys()
        ]
        return BlackboxTabular(
            hyperparameters=self.hyperparameters,
            configuration_space=self.configuration_space,
            fidelity_space=self.fidelity_space,
            objectives_evaluations=self.objectives_evaluations[
                :, :, :, new_objectives_indices
            ],
            fidelity_values=self._fidelity_values,
            objectives_names=list(objective_name_mapping.values()),
        )

    def __str__(self):
        (
            num_evals,
            num_seeds,
            num_fidelities,
            num_objectives,
        ) = self.objectives_evaluations.shape
        stats = {
            "total evaluations": self.objectives_evaluations.size // num_fidelities,
            "num fidelities": num_fidelities,
            "evaluated hps": num_evals,
            "seeds": num_seeds,
            "fidelities": num_fidelities,
            "objectives": self.objectives_names,
            "hyperparameter": list(self.configuration_space.keys()),
        }
        stats_str = ", ".join([f"{k}: {v}" for k, v in stats.items()])
        return f"tabular blackbox: {stats_str}"


def serialize(
    bb_dict: Dict[str, BlackboxTabular], path: str, metadata: Optional[Dict] = None
):
    # check all blackboxes share the same search space and have evaluated the same hyperparameters
    # pick an arbitrary blackbox
    bb_first = next(iter(bb_dict.values()))
    for bb in bb_dict.values():
        pd.testing.assert_frame_equal(bb.hyperparameters, bb_first.hyperparameters)
        # assert bb.configuration_space == bb_first.configuration_space
        # assert bb.fidelity_space == bb_first.fidelity_space
        assert np.all(bb.fidelity_values == bb_first.fidelity_values)
        assert bb.objectives_names == bb_first.objectives_names
        assert bb.objectives_evaluations.shape == bb_first.objectives_evaluations.shape

    path = Path(path)

    path.mkdir(exist_ok=True)

    serialize_configspace(
        path=path,
        configuration_space=bb_first.configuration_space,
        fidelity_space=bb_first.fidelity_space,
    )

    # we use gzip as snappy is not supported for fastparquet engine compression
    # gzip is slower than the default snappy but more compact
    bb_first.hyperparameters.to_parquet(
        path / "hyperparameters.parquet",
        index=False,
        compression="gzip",
        engine="fastparquet",
    )

    with open(path / "objectives_evaluations.npy", "wb") as f:
        # (num_tasks, num_hps, num_seeds, num_fidelities, num_objectives)
        objectives = np.stack(
            [bb_dict[task].objectives_evaluations for task in bb_dict.keys()]
        )
        np.save(f, objectives.astype(np.float32), allow_pickle=False)

    with open(path / "fidelities_values.npy", "wb") as f:
        np.save(f, bb_first.fidelity_values, allow_pickle=False)

    metadata = metadata.copy() if metadata else {}
    metadata.update(
        {
            "objectives_names": bb_first.objectives_names,
            "task_names": list(bb_dict.keys()),
        }
    )
    serialize_metadata(
        path=path,
        metadata=metadata,
    )


def deserialize(path: str) -> Dict[str, BlackboxTabular]:
    """
    Deserialize blackboxes contained in a path that were saved with `serialize` above.
    TODO: the API is currently dissonant with `serialize`, `deserialize` for BlackboxOffline as `serialize` is there a member.
    A possible way to unify is to have serialize also be a free function for BlackboxOffline.
    :param path: a path that contains blackboxes that were saved with `serialize`
    :return: a dictionary from task name to blackbox
    """
    path = Path(path)

    configuration_space, fidelity_space = deserialize_configspace(path)
    hyperparameters = pd.read_parquet(
        Path(path) / "hyperparameters.parquet", engine="fastparquet"
    )

    metadata = deserialize_metadata(path)
    objectives_names = metadata["objectives_names"]
    task_names = metadata["task_names"]

    with open(path / "fidelities_values.npy", "rb") as f:
        fidelity_values = np.load(f)

    # possibly we could use memmap to avoid memory use or speed-up loading times
    with open(path / "objectives_evaluations.npy", "rb") as f:
        objectives_evaluations = np.load(f)

    return {
        task: BlackboxTabular(
            hyperparameters=hyperparameters,
            configuration_space=configuration_space,
            fidelity_space=fidelity_space,
            objectives_evaluations=objectives_evaluations[i],
            fidelity_values=fidelity_values,
            objectives_names=objectives_names,
        )
        for i, task in enumerate(task_names)
    }

File Path: syne_tune/blackbox_repository/conversion_scripts/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/blackbox_repository/conversion_scripts/blackbox_recipe.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Optional

from syne_tune.util import catchtime


class BlackboxRecipe:
    def __init__(self, name: str, cite_reference: str):
        """
        Parent class for a blackbox recipe that allows to generate the blackbox data on disk, see `FCNETRecipe` or
        `LCBenchRecipe` classes for example on how to add a new benchmark.
        :param name: name of the blackbox
        :param cite_reference: name of the paper to be referenced. A message is prompted when generating the blackbox
        to ask the user to cite the relevant paper.
        """
        self.name = name
        self.cite_reference = cite_reference

    def generate(self, s3_root: Optional[str] = None):
        """
        Generates the blackbox on disk then upload it on s3 if AWS is available.
        :param s3_root: s3 root where to upload to s3, default to s3://{sagemaker-bucket}/blackbox-repository.
        If AWS is not available, this step is skipped and the dataset is just persisted locally.
        :return:
        """
        message = (
            f"Generating {self.name} blackbox locally, if you use this dataset in a publication, please cite "
            f'the following paper: "{self.cite_reference}"'
        )
        logging.info(message)
        self._generate_on_disk()

        with catchtime("uploading to s3"):
            from syne_tune.blackbox_repository.conversion_scripts.utils import (
                upload_blackbox,
            )

            upload_blackbox(name=self.name, s3_root=s3_root)

    def _generate_on_disk(self):
        """
        Method to be overloaded by the child class that should generate the blackbox on disk (handling the donwloading
        and reformatting of external files).
        :return:
        """
        raise NotImplementedError()

File Path: syne_tune/blackbox_repository/conversion_scripts/recipes.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.blackbox_repository.conversion_scripts.scripts.icml2020_import import (
    DeepARRecipe,
    XGBoostRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts.lcbench.lcbench import (
    LCBenchRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts.nasbench201_import import (
    NASBench201Recipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts.fcnet_import import (
    FCNETRecipe,
)

# from syne_tune.blackbox_repository.conversion_scripts.scripts.yahpo_import import (
#     YAHPORecipe,
#     yahpo_scenarios,
# )

# add a blackbox recipe here to expose it in Syne Tune
from syne_tune.blackbox_repository.conversion_scripts.scripts.pd1_import import (
    PD1Recipe,
)

recipes = [
    DeepARRecipe(),
    XGBoostRecipe(),
    NASBench201Recipe(),
    FCNETRecipe(),
    LCBenchRecipe(),
    PD1Recipe(),
]

# for scenario in yahpo_scenarios:
#     recipes.append(YAHPORecipe("yahpo-" + scenario))


generate_blackbox_recipes = {recipe.name: recipe for recipe in recipes}

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
metric_elapsed_time = "metric_elapsed_time"
default_metric = "metric_default"
resource_attr = "resource_attr"

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/fcnet_import.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Convert tabular data from
 Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization
 Aaron Klein Frank Hutter
 https://arxiv.org/pdf/1905.04970.pdf.
"""
import urllib
import tarfile

from pathlib import Path
import pandas as pd
import numpy as np
import ast

try:
    import h5py
except ImportError:
    print("Cannot import h5py. Use 'pip install h5py'")

from syne_tune.blackbox_repository.blackbox_tabular import serialize, BlackboxTabular
from syne_tune.blackbox_repository.conversion_scripts.blackbox_recipe import (
    BlackboxRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts import (
    metric_elapsed_time,
    default_metric,
    resource_attr,
)
from syne_tune.blackbox_repository.conversion_scripts.utils import repository_path

from syne_tune.util import catchtime
from syne_tune.config_space import choice, logfinrange, finrange, randint

BLACKBOX_NAME = "fcnet"

METRIC_VALID_LOSS = "metric_valid_loss"

METRIC_ELAPSED_TIME = "metric_elapsed_time"

RESOURCE_ATTR = "hp_epoch"

MAX_RESOURCE_LEVEL = 100

NUM_UNITS_1 = "hp_n_units_1"

NUM_UNITS_2 = "hp_n_units_2"

CONFIGURATION_SPACE = {
    "hp_activation_fn_1": choice(["tanh", "relu"]),
    "hp_activation_fn_2": choice(["tanh", "relu"]),
    "hp_batch_size": logfinrange(8, 64, 4, cast_int=True),
    "hp_dropout_1": finrange(0.0, 0.6, 3),
    "hp_dropout_2": finrange(0.0, 0.6, 3),
    "hp_init_lr": choice([0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]),
    "hp_lr_schedule": choice(["cosine", "const"]),
    NUM_UNITS_1: logfinrange(16, 512, 6, cast_int=True),
    NUM_UNITS_2: logfinrange(16, 512, 6, cast_int=True),
}


def convert_dataset(dataset_path: Path, max_rows: int = None):
    data = h5py.File(dataset_path, "r")
    keys = data.keys()
    if max_rows is not None:
        keys = list(keys)[:max_rows]

    hyperparameters = pd.DataFrame(ast.literal_eval(key) for key in keys)
    hyperparameters.rename(
        columns={col: "hp_" + col for col in hyperparameters.columns}, inplace=True
    )

    objective_names = [
        "valid_loss",
        "train_loss",
        "final_test_error",
        "n_params",
        "elapsed_time",
    ]

    # todo for now only full metrics
    fidelity_values = np.arange(1, MAX_RESOURCE_LEVEL + 1)
    n_fidelities = len(fidelity_values)
    n_objectives = len(objective_names)
    n_seeds = 4
    n_hps = len(keys)

    objective_evaluations = np.empty(
        (n_hps, n_seeds, n_fidelities, n_objectives)
    ).astype("float32")

    def save_objective_values_helper(name, values):
        assert values.shape == (n_hps, n_seeds, n_fidelities)

        name_index = dict(zip(objective_names, range(len(objective_names))))
        objective_evaluations[..., name_index[name]] = values

    # (n_hps, n_seeds,)
    final_test_error = np.stack(
        [data[key]["final_test_error"][:].astype("float32") for key in keys]
    )

    # (n_hps, n_seeds, n_fidelities)
    final_test_error = np.repeat(
        np.expand_dims(final_test_error, axis=-1), n_fidelities, axis=-1
    )
    save_objective_values_helper("final_test_error", final_test_error)

    # (n_hps, n_seeds,)
    n_params = np.stack([data[key]["n_params"][:].astype("float32") for key in keys])

    # (n_hps, n_seeds, n_fidelities)
    n_params = np.repeat(np.expand_dims(n_params, axis=-1), n_fidelities, axis=-1)
    save_objective_values_helper("n_params", n_params)

    # (n_hps, n_seeds,)
    runtime = np.stack([data[key]["runtime"][:].astype("float32") for key in keys])

    # linear interpolation to go from total training time to training time per epoch as in fcnet code
    # (n_hps, n_seeds, n_epochs)
    # todo utilize expand dim instead of reshape
    epochs = np.repeat(fidelity_values.reshape(1, -1), n_hps * n_seeds, axis=0).reshape(
        n_hps, n_seeds, -1
    )
    elapsed_time = (epochs / MAX_RESOURCE_LEVEL) * runtime.reshape((n_hps, n_seeds, 1))

    save_objective_values_helper("elapsed_time", elapsed_time)

    # metrics that are fully observed, only use train/valid loss as mse are the same numbers
    # for m in ['train_loss', 'train_mse', 'valid_loss', 'valid_mse']:
    for m in ["train_loss", "valid_loss"]:
        save_objective_values_helper(
            m, np.stack([data[key][m][:].astype("float32") for key in keys])
        )

    fidelity_space = {RESOURCE_ATTR: randint(lower=1, upper=MAX_RESOURCE_LEVEL)}

    objective_names = [f"metric_{m}" for m in objective_names]
    # Sanity checks:
    assert objective_names[0] == METRIC_VALID_LOSS
    assert objective_names[4] == METRIC_ELAPSED_TIME
    return BlackboxTabular(
        hyperparameters=hyperparameters,
        configuration_space=CONFIGURATION_SPACE,
        fidelity_space=fidelity_space,
        objectives_evaluations=objective_evaluations,
        fidelity_values=fidelity_values,
        objectives_names=objective_names,
    )


def generate_fcnet():
    blackbox_name = BLACKBOX_NAME
    fcnet_file = repository_path / "fcnet_tabular_benchmarks.tar.gz"
    if not fcnet_file.exists():
        src = "http://ml4aad.org/wp-content/uploads/2019/01/fcnet_tabular_benchmarks.tar.gz"
        print(f"did not find {fcnet_file}, downloading {src}")
        urllib.request.urlretrieve(src, fcnet_file)

    with tarfile.open(fcnet_file) as f:
        f.extractall(path=repository_path)

    with catchtime("converting"):
        bb_dict = {}
        for dataset in [
            "protein_structure",
            "naval_propulsion",
            "parkinsons_telemonitoring",
            "slice_localization",
        ]:
            print(f"converting {dataset}")
            dataset_path = (
                repository_path
                / "fcnet_tabular_benchmarks"
                / f"fcnet_{dataset}_data.hdf5"
            )
            bb_dict[dataset] = convert_dataset(dataset_path=dataset_path)

    with catchtime("saving to disk"):
        serialize(
            bb_dict=bb_dict,
            path=repository_path / blackbox_name,
            metadata={
                metric_elapsed_time: METRIC_ELAPSED_TIME,
                default_metric: METRIC_VALID_LOSS,
                resource_attr: RESOURCE_ATTR,
            },
        )


def plot_learning_curves():
    import matplotlib.pyplot as plt
    from syne_tune.blackbox_repository.repository import load_blackbox

    # plot one learning-curve for sanity-check
    bb_dict = load_blackbox(BLACKBOX_NAME)

    b = bb_dict["naval_propulsion"]
    configuration = {k: v.sample() for k, v in b.configuration_space.items()}
    print(configuration)
    errors = []
    for i in range(1, MAX_RESOURCE_LEVEL + 1):
        res = b.objective_function(configuration=configuration, fidelity={"epochs": i})
        errors.append(res[METRIC_VALID_LOSS])
    plt.plot(errors)


class FCNETRecipe(BlackboxRecipe):
    def __init__(self):
        super(FCNETRecipe, self).__init__(
            name=BLACKBOX_NAME,
            cite_reference="Tabular benchmarks for joint architecture and hyperparameter optimization. "
            "Klein, A. and Hutter, F. 2019.",
        )

    def _generate_on_disk(self):
        generate_fcnet()


if __name__ == "__main__":
    FCNETRecipe().generate()

    # plot_learning_curves()

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/icml2020_import.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Convert evaluations from
 A Quantile-based Approach for Hyperparameter Transfer Learning
 David Salinas Huibin Shen Valerio Perrone
 http://proceedings.mlr.press/v119/salinas20a/salinas20a.pdf
"""
import pandas as pd
import numpy as np
from syne_tune.blackbox_repository.blackbox_offline import serialize, BlackboxOffline
from syne_tune.blackbox_repository.conversion_scripts.blackbox_recipe import (
    BlackboxRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.utils import repository_path
import syne_tune.config_space as sp


def download(blackbox: str):
    import urllib

    root = "https://github.com/geoalgo/A-Quantile-based-Approach-for-Hyperparameter-Transfer-Learning/blob/master/src/blackbox/offline_evaluations/"
    urllib.request.urlretrieve(
        root + f"{blackbox}.csv.zip?raw=true", repository_path / f"{blackbox}.csv.zip"
    )


def serialize_deepar():
    blackbox = "DeepAR"
    download(blackbox=blackbox)
    df = pd.read_csv(repository_path / f"{blackbox}.csv.zip")

    df["hp_num_layers"] = df.hp_num_layers.apply(np.exp)
    df["hp_num_cells"] = df.hp_num_cells.apply(np.exp)
    df["hp_dropout_rate"] = df.hp_dropout_rate_log.apply(np.exp)
    df["hp_learning_rate"] = df.hp_learning_rate_log.apply(np.exp)
    df["hp_num_batches_per_epoch"] = df.hp_num_batches_per_epoch_log.apply(np.exp)
    df["hp_context_length_ratio"] = df.hp_context_length_ratio_log.apply(np.exp)

    df = df[[col for col in df.columns if not col.endswith("_log")]]
    configuration_space = {
        "hp_num_layers": sp.randint(lower=2, upper=4),
        "hp_num_cells": sp.randint(lower=30, upper=120),
        "hp_dropout_rate": sp.uniform(lower=0.01, upper=0.51),
        "hp_learning_rate": sp.loguniform(lower=1e-4, upper=1e-2),
        "hp_num_cells": sp.lograndint(lower=10, upper=10000),
        "hp_context_length_ratio": sp.loguniform(lower=0.05, upper=4),
    }

    serialize(
        {
            task: BlackboxOffline(
                df_evaluations=df.loc[df.task == task, :],
                configuration_space=configuration_space,
                objectives_names=[
                    col for col in df.columns if col.startswith("metric_")
                ],
            )
            for task in df.task.unique()
        },
        path=repository_path / "icml-deepar",
    )


def serialize_xgboost():
    """
    'hp_log2_min_child_weight', 'hp_subsample', 'hp_colsample_bytree',
    'hp_log2_gamma', 'hp_log2_lambda', 'hp_eta', 'hp_max_depth_index',
    'hp_log2_alpha', 'metric_error', 'blackbox', 'task'
    """
    blackbox = "XGBoost"
    download(blackbox=blackbox)
    df = pd.read_csv(repository_path / f"{blackbox}.csv.zip")

    for hp in [
        "hp_log2_min_child_weight",
        "hp_log2_gamma",
        "hp_log2_lambda",
        "hp_log2_alpha",
    ]:
        df[hp.replace("_log2", "")] = df[hp].apply(np.exp2)

    df = df[[col for col in df.columns if not "_log2" in col]]

    configuration_space = {
        "hp_subsample": sp.uniform(lower=0.5, upper=1.0),
        "hp_colsample_bytree": sp.uniform(
            lower=0.3,
            upper=1.0,
        ),
        "hp_eta": sp.uniform(lower=0.0, upper=1.0),
        "hp_max_depth_index": sp.uniform(
            lower=0.0,
            upper=12.0,
        ),
        "hp_min_child_weight": sp.loguniform(lower=1e-5, upper=64.0),
        "hp_gamma": sp.loguniform(lower=1e-5, upper=64),
        "hp_lambda": sp.loguniform(lower=1e-5, upper=256),
        "hp_alpha": sp.loguniform(lower=1e-5, upper=256),
    }

    serialize(
        {
            task: BlackboxOffline(
                df_evaluations=df.loc[df.task == task, :],
                configuration_space=configuration_space,
                objectives_names=[
                    col for col in df.columns if col.startswith("metric_")
                ],
            )
            for task in df.task.unique()
        },
        path=repository_path / "icml-xgboost",
    )


class XGBoostRecipe(BlackboxRecipe):
    def __init__(self):
        super(XGBoostRecipe, self).__init__(
            name="icml-xgboost",
            cite_reference="A quantile-based approach for hyperparameter transfer learning."
            "Salinas, D., Shen, H., and Perrone, V. 2021.",
        )

    def _generate_on_disk(self):
        serialize_xgboost()


class DeepARRecipe(BlackboxRecipe):
    def __init__(self):
        super(DeepARRecipe, self).__init__(
            name="icml-deepar",
            cite_reference="A quantile-based approach for hyperparameter transfer learning."
            "Salinas, D., Shen, H., and Perrone, V. 2021.",
        )

    def _generate_on_disk(self):
        serialize_deepar()


if __name__ == "__main__":
    DeepARRecipe().generate()
    XGBoostRecipe().generate()

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/lcbench/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/lcbench/api.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# File taken from LCBench to avoid having to install a directory manually
# https://github.com/automl/LCBench

import os as os
import numpy as np
import json
import pickle
import gzip


class Benchmark:
    """API for TabularBench."""

    def __init__(self, data_dir, cache=False, cache_dir="cached/"):
        """Initialize dataset (will take a few seconds-minutes).

        Keyword arguments:
        bench_data -- str, the raw benchmark data directory
        """
        if not os.path.isfile(data_dir) or not data_dir.endswith(".json"):
            raise ValueError("Please specify path to the bench json file.")

        self.data_dir = data_dir
        self.cache_dir = cache_dir
        self.cache = cache

        print("==> Loading data...")
        self.data = self._read_data(data_dir)
        self.dataset_names = list(self.data.keys())
        print("==> Done.")

    def query(self, dataset_name, tag, config_id):
        """Query a run.

        Keyword arguments:
        dataset_name -- str, the name of the dataset in the benchmark
        tag -- str, the tag you want to query
        config_id -- int, an identifier for which run you want to query, if too large will query the last run
        """
        config_id = str(config_id)
        if dataset_name not in self.dataset_names:
            raise ValueError("Dataset name not found.")

        if config_id not in self.data[dataset_name].keys():
            raise ValueError(
                "Config nr %s not found for dataset %s." % (config_id, dataset_name)
            )

        if tag in self.data[dataset_name][config_id]["log"].keys():
            return self.data[dataset_name][config_id]["log"][tag]

        if tag in self.data[dataset_name][config_id]["results"].keys():
            return self.data[dataset_name][config_id]["results"][tag]

        if tag in self.data[dataset_name][config_id]["config"].keys():
            return self.data[dataset_name][config_id]["config"][tag]

        if tag == "config":
            return self.data[dataset_name][config_id]["config"]

        raise ValueError(
            "Tag %s not found for config %s for dataset %s"
            % (tag, config_id, dataset_name)
        )

    def query_best(self, dataset_name, tag, criterion, position=0):
        """Query the n-th best run. "Best" here means achieving the largest value at any epoch/step,

        Keyword arguments:
        dataset_name -- str, the name of the dataset in the benchmark
        tag -- str, the tag you want to query
        criterion -- str, the tag you want to use for the ranking
        position -- int, an identifier for which position in the ranking you want to query
        """
        performances = []
        for config_id in self.data[dataset_name].keys():
            performances.append(
                (config_id, max(self.query(dataset_name, criterion, config_id)))
            )

        performances.sort(key=lambda x: x[1] * 1000, reverse=True)
        desired_position = performances[position][0]

        return self.query(dataset_name, tag, desired_position)

    def get_queriable_tags(self, dataset_name=None, config_id=None):
        """Returns a list of all queriable tags"""
        if dataset_name is None or config_id is None:
            dataset_name = list(self.data.keys())[0]
            config_id = list(self.data[dataset_name].keys())[0]
        else:
            config_id = str(config_id)
        log_tags = list(self.data[dataset_name][config_id]["log"].keys())
        result_tags = list(self.data[dataset_name][config_id]["results"].keys())
        config_tags = list(self.data[dataset_name][config_id]["config"].keys())
        additional = ["config"]
        return log_tags + result_tags + config_tags + additional

    def get_dataset_names(self):
        """Returns a list of all availabe dataset names like defined on openml"""
        return self.dataset_names

    def get_openml_task_ids(self):
        """Returns a list of openml task ids"""
        task_ids = []
        for dataset_name in self.dataset_names:
            task_ids.append(self.query(dataset_name, "OpenML_task_id", 1))
        return task_ids

    def get_number_of_configs(self, dataset_name):
        """Returns the number of configurations for a dataset"""
        if dataset_name not in self.dataset_names:
            raise ValueError("Dataset name not found.")
        return len(self.data[dataset_name].keys())

    def get_config(self, dataset_name, config_id):
        """Returns the configuration of a run specified by dataset name and config id"""
        if dataset_name not in self.dataset_names:
            raise ValueError("Dataset name not found.")
        return self.data[dataset_name][config_id]["config"]

    def plot_by_name(
        self,
        dataset_names,
        x_col,
        y_col,
        n_configs=10,
        show_best=False,
        xscale="linear",
        yscale="linear",
        criterion=None,
    ):
        """Plot multiple datasets and multiple runs.

        Keyword arguments:
        dataset_names -- list
        x_col -- str, tag to plot on x-axis
        y_col -- str, tag to plot on y-axis
        n_configs -- int, number of configs to plot for each dataset
        show_best -- bool, weather to show the n_configs best (according to query_best())
        xscale -- str, set xscale, options as in matplotlib: "linear", "log", "symlog", "logit", ...
        yscale -- str, set yscale, options as in matplotlib: "linear", "log", "symlog", "logit", ...
        criterion -- str, tag used as criterion for query_best()
        """
        import matplotlib.pyplot as plt

        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        if not isinstance(dataset_names, (list, np.ndarray)):
            raise ValueError(
                "Please specify a dataset name or a list list of dataset names."
            )

        n_rows = len(dataset_names)
        fig, axes = plt.subplots(
            n_rows, 1, sharex=False, sharey=False, figsize=(10, 7 * n_rows)
        )

        if criterion is None:
            criterion = y_col

        loop_arg = enumerate(axes.flatten()) if len(dataset_names) > 1 else [(0, axes)]

        for ind_ax, ax in loop_arg:
            for ind in range(n_configs):
                try:
                    if ind == 0:
                        instances = int(
                            self.query(dataset_names[ind_ax], "instances", 0)
                        )
                        classes = int(self.query(dataset_names[ind_ax], "classes", 0))
                        features = int(self.query(dataset_names[ind_ax], "features", 0))

                    if show_best:
                        x = self.query_best(
                            dataset_names[ind_ax], x_col, criterion, ind
                        )
                        y = self.query_best(
                            dataset_names[ind_ax], y_col, criterion, ind
                        )
                    else:
                        x = self.query(dataset_names[ind_ax], x_col, ind + 1)
                        y = self.query(dataset_names[ind_ax], y_col, ind + 1)

                    ax.plot(x, y, "p-")
                    ax.set_xscale(xscale)
                    ax.set_yscale(yscale)
                    ax.set(xlabel="step", ylabel=y_col)
                    title_str = ", ".join(
                        [
                            dataset_names[ind_ax],
                            "features: " + str(features),
                            "classes: " + str(classes),
                            "instances: " + str(instances),
                        ]
                    )
                    ax.title.set_text(title_str)
                except ValueError:
                    print(
                        "Run %i not found for dataset %s" % (ind, dataset_names[ind_ax])
                    )
                except Exception as e:
                    raise e

    def _cache_data(self, data, cache_file):
        os.makedirs(self.cache_dir, exist_ok=True)
        with gzip.open(cache_file, "wb") as f:
            pickle.dump(data, f)

    def _read_cached_data(self, cache_file):
        with gzip.open(cache_file, "rb") as f:
            data = pickle.load(f)
        return data

    def _read_file_string(self, path):
        """Reads a large json string from path. Python file handler has issues with large files so it has to be chunked."""
        # Shoutout to https://stackoverflow.com/questions/48122798/oserror-errno-22-invalid-argument-when-reading-a-huge-file
        file_str = ""
        with open(path, "r") as f:
            while True:
                block = f.read(64 * (1 << 20))  # Read 64 MB at a time
                if not block:  # Reached EOF
                    break
                file_str += block
        return file_str

    def _read_data(self, path):
        """Reads cached data if available. If not, reads json and caches the data as .pkl.gz"""
        cache_file = os.path.join(
            self.cache_dir, os.path.basename(self.data_dir).replace(".json", ".pkl.gz")
        )
        if os.path.exists(cache_file) and self.cache:
            print("==> Found cached data, loading...")
            data = self._read_cached_data(cache_file)
        else:
            print("==> No cached data found or cache set to False.")
            print("==> Reading json data...")
            data = json.loads(self._read_file_string(path))
            if self.cache:
                print("==> Caching data...")
                self._cache_data(data, cache_file)
        return data

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/lcbench/lcbench.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import zipfile
import urllib

import pandas as pd
import numpy as np

from syne_tune.blackbox_repository.blackbox_tabular import serialize, BlackboxTabular
from syne_tune.blackbox_repository.conversion_scripts.blackbox_recipe import (
    BlackboxRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts import (
    metric_elapsed_time,
    default_metric,
    resource_attr,
)
from syne_tune.config_space import randint, lograndint, uniform, loguniform
from syne_tune.util import catchtime
from syne_tune.blackbox_repository.conversion_scripts.scripts.lcbench.api import (
    Benchmark,
)
from syne_tune.blackbox_repository.conversion_scripts.utils import repository_path


BLACKBOX_NAME = "lcbench"

METRIC_ACCURACY = "val_accuracy"

METRIC_ELAPSED_TIME = "time"

RESOURCE_ATTR = "epoch"

MAX_RESOURCE_LEVEL = 50

CONFIGURATION_SPACE = {
    "num_layers": randint(1, 5),
    "max_units": lograndint(64, 1024),
    "batch_size": lograndint(16, 512),
    "learning_rate": loguniform(1e-4, 1e-1),
    "weight_decay": uniform(1e-5, 1e-1),
    "momentum": uniform(0.1, 0.99),
    "max_dropout": uniform(0.0, 1.0),
}


def convert_task(bench, dataset_name):
    n_config = 2000
    configs = [
        bench.query(dataset_name=dataset_name, tag="config", config_id=i)
        for i in range(n_config)
    ]
    hyperparameters = pd.DataFrame(configs)
    # remove constant columns
    hyperparameters = hyperparameters.loc[
        :, (hyperparameters != hyperparameters.iloc[0]).any()
    ]
    objectives = [
        "Train/val_accuracy",
        "Train/val_balanced_accuracy",
        "Train/val_cross_entropy",
        "Train/test_result",
        "Train/test_balanced_accuracy",
        "Train/test_cross_entropy",
        "time",
    ]
    objective_evaluations = np.zeros((n_config, 1, MAX_RESOURCE_LEVEL, len(objectives)))

    fidelity_space = {RESOURCE_ATTR: randint(lower=1, upper=MAX_RESOURCE_LEVEL)}
    for j, tag in enumerate(objectives):
        for i in range(n_config):
            # Drop first evaluation (before training) as well as last.
            raw_objective_evaluations = bench.query(
                dataset_name=dataset_name, tag=tag, config_id=i
            )
            objective_evaluations[i, 0, :, j] = raw_objective_evaluations[1:-1]
            if tag == "time":
                # Remove time for scoring the model before training it
                objective_evaluations[i, 0, :, j] -= raw_objective_evaluations[0]
    return BlackboxTabular(
        hyperparameters=hyperparameters,
        configuration_space=CONFIGURATION_SPACE,
        fidelity_space=fidelity_space,
        objectives_evaluations=objective_evaluations,
        fidelity_values=np.arange(1, MAX_RESOURCE_LEVEL + 1),
        objectives_names=[
            "val_accuracy",
            "val_balanced_accuracy",
            "val_cross_entropy",
            "test_accuracy",
            "test_balanced_accuracy",
            "test_cross_entropy",
            "time",
        ],
    )


class LCBenchRecipe(BlackboxRecipe):
    def __init__(self):
        super(LCBenchRecipe, self).__init__(
            name=BLACKBOX_NAME,
            cite_reference="Auto-PyTorch: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL. "
            "Lucas Zimmer, Marius Lindauer, Frank Hutter. 2020.",
        )

    def _generate_on_disk(self):
        data_file = repository_path / "data_2k_lw.zip"
        if not data_file.exists():
            src = "https://figshare.com/ndownloader/files/21188598"
            print(f"did not find {data_file}, downloading {src}")
            urllib.request.urlretrieve(src, data_file)

        with zipfile.ZipFile(data_file, "r") as zip_ref:
            zip_ref.extractall(repository_path)

        with catchtime("converting"):
            bench = Benchmark(str(repository_path / "data_2k_lw.json"), cache=False)
            bb_dict = {
                task: convert_task(bench, task) for task in bench.get_dataset_names()
            }

        with catchtime("saving to disk"):
            serialize(
                bb_dict=bb_dict,
                path=repository_path / self.name,
                metadata={
                    metric_elapsed_time: METRIC_ELAPSED_TIME,
                    default_metric: METRIC_ACCURACY,
                    resource_attr: RESOURCE_ATTR,
                },
            )


if __name__ == "__main__":
    LCBenchRecipe().generate()

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/nasbench201_import.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import bz2
import pickle
import pandas as pd
import numpy as np
import logging

from syne_tune.blackbox_repository.blackbox_tabular import serialize, BlackboxTabular
from syne_tune.blackbox_repository.conversion_scripts.blackbox_recipe import (
    BlackboxRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts import (
    metric_elapsed_time,
    default_metric,
    resource_attr,
)
from syne_tune.blackbox_repository.conversion_scripts.utils import repository_path

from syne_tune.config_space import randint, choice
from syne_tune.util import catchtime

logger = logging.getLogger(__name__)


BLACKBOX_NAME = "nasbench201"

CONFIG_KEYS = ("hp_x0", "hp_x1", "hp_x2", "hp_x3", "hp_x4", "hp_x5")

METRIC_VALID_ERROR = "metric_valid_error"

# This is time required for the given epoch, not time elapsed
# since start of training
METRIC_TIME_THIS_RESOURCE = "metric_runtime"

RESOURCE_ATTR = "hp_epoch"


def str_to_list(arch_str):
    node_strs = arch_str.split("+")
    config = []
    for i, node_str in enumerate(node_strs):
        inputs = [x for x in node_str.split("|") if x != ""]
        for xinput in inputs:
            assert len(xinput.split("~")) == 2, "invalid input length : {:}".format(
                xinput
            )
        inputs = (xi.split("~") for xi in inputs)

        config.extend(op for (op, idx) in inputs)

    return config


def convert_dataset(data, dataset):
    hp_cols = list(CONFIG_KEYS)

    hps = dict()

    for h in hp_cols:
        hps[h] = []

    n_hps = data["total_archs"]

    for i in range(n_hps):
        config = str_to_list(data["arch2infos"][i]["200"]["arch_str"])

        for j, hp in enumerate(config):
            hps[CONFIG_KEYS[j]].append(hp)

    hyperparameters = pd.DataFrame(data=hps, columns=hp_cols)

    objective_names = [
        "valid_error",
        "train_error",
        "runtime",
        "elapsed_time",
        "latency",
        "flops",
        "params",
    ]

    fidelity_values = np.arange(1, 201)
    n_fidelities = len(fidelity_values)
    n_objectives = len(objective_names)
    n_seeds = 3

    objective_evaluations = np.empty(
        (n_hps, n_seeds, n_fidelities, n_objectives)
    ).astype("float32")
    name_index = {name: i for i, name in enumerate(objective_names)}

    def save_objective_values_helper(name, values):
        assert values.shape == (n_hps, n_seeds, n_fidelities)

        objective_evaluations[..., name_index[name]] = values

    ve = np.empty((n_hps, n_seeds, n_fidelities)).astype("float32")
    te = np.empty((n_hps, n_seeds, n_fidelities)).astype("float32")
    rt = np.empty((n_hps, n_seeds, n_fidelities)).astype("float32")

    for ai in range(n_hps):
        for si, seed in enumerate([777, 888, 999]):

            try:
                entry = data["arch2infos"][ai]["200"]["all_results"][(dataset, seed)]
                validation_error = [
                    1 - entry["eval_acc1es"]["ori-test@%d" % ei] / 100
                    for ei in range(n_fidelities)
                ]
                train_error = [
                    1 - entry["train_acc1es"][ei] / 100 for ei in range(n_fidelities)
                ]
                # runtime measure the time for a single epoch
                runtime = [
                    entry["train_times"][ei] +
                    entry["eval_times"]["ori-test@%d" % ei]
                    for ei in range(n_fidelities)
                ]

            except KeyError:
                validation_error = [np.nan] * n_fidelities
                train_error = [np.nan] * n_fidelities
                runtime = [np.nan] * n_fidelities
            ve[ai, si, :] = validation_error
            te[ai, si, :] = train_error
            rt[ai, si, :] = runtime

    def impute(values):
        idx = np.isnan(values)
        a, s, e = np.where(idx == True)
        for ai, si, ei in zip(a, s, e):
            l = values[ai, :, ei]
            m = np.mean(np.delete(l, si))
            values[ai, si, ei] = m
        return values

    # The original data contains missing values, since not all architectures were evaluated for all three seeds
    # We impute these missing values by taking the average of the available datapoints for the corresponding
    # architecture and time step

    save_objective_values_helper("valid_error", impute(ve))
    save_objective_values_helper("train_error", impute(te))
    save_objective_values_helper("runtime", impute(rt))
    save_objective_values_helper(
        "elapsed_time", np.cumsum(impute(rt), axis=-1))

    latency = np.array(
        [
            data["arch2infos"][ai]["200"]["all_results"][(
                dataset, 777)]["latency"][0]
            for ai in range(n_hps)
        ]
    )
    latency = np.repeat(np.expand_dims(latency, axis=-1), n_seeds, axis=-1)
    latency = np.repeat(np.expand_dims(
        latency, axis=-1), n_fidelities, axis=-1)
    save_objective_values_helper("latency", latency)

    flops = np.array(
        [
            data["arch2infos"][ai]["200"]["all_results"][(
                dataset, 777)]["flop"]
            for ai in range(n_hps)
        ]
    )
    flops = np.repeat(np.expand_dims(flops, axis=-1), n_seeds, axis=-1)
    flops = np.repeat(np.expand_dims(flops, axis=-1), n_fidelities, axis=-1)
    save_objective_values_helper("flops", flops)

    params = np.array(
        [
            data["arch2infos"][ai]["200"]["all_results"][(
                dataset, 777)]["params"]
            for ai in range(n_hps)
        ]
    )
    params = np.repeat(np.expand_dims(params, axis=-1), n_seeds, axis=-1)
    params = np.repeat(np.expand_dims(params, axis=-1), n_fidelities, axis=-1)
    save_objective_values_helper("params", params)

    configuration_space = {
        node: choice(
            ["avg_pool_3x3", "nor_conv_3x3", "skip_connect", "nor_conv_1x1", "none"]
        )
        for node in hp_cols
    }

    fidelity_space = {RESOURCE_ATTR: randint(lower=1, upper=201)}

    objective_names = [f"metric_{m}" for m in objective_names]
    # Sanity checks:
    assert objective_names[0] == METRIC_VALID_ERROR
    assert objective_names[2] == METRIC_TIME_THIS_RESOURCE
    assert objective_names[3] == "metric_elapsed_time"
    return BlackboxTabular(
        hyperparameters=hyperparameters,
        configuration_space=configuration_space,
        fidelity_space=fidelity_space,
        objectives_evaluations=objective_evaluations,
        fidelity_values=fidelity_values,
        objectives_names=objective_names,
    )


class NASBench201Recipe(BlackboxRecipe):
    def __init__(self):
        super(NASBench201Recipe, self).__init__(
            name="nasbench201",
            cite_reference="NAS-Bench-201: Extending the scope of reproducible neural architecture search. "
            "Dong, X. and Yang, Y. 2020.",
        )

    def _generate_on_disk(self):
        logger.info(
            "\nGenerating NASBench201 blackbox from sources and persisting to S3:\n"
            "This takes quite some time, a substantial amount of memory, and about "
            "1.8 GB of local disk space.\n"
            "If this procedure fails, please re-run it on a machine with sufficient resources"
        )
        file_name = repository_path / "NATS-tss-v1_0-3ffb9.pickle.pbz2"
        if not file_name.exists():
            logger.info(f"did not find {file_name}, downloading")
            with catchtime("downloading compressed file"):
                import requests

                def download_file_from_google_drive(id, destination):
                    URL = "https://docs.google.com/uc?export=download"

                    session = requests.Session()

                    params = {"id": id, "confirm": True}
                    response = session.get(URL, params=params, stream=True)

                    save_response_content(response, destination)

                def save_response_content(response, destination):
                    CHUNK_SIZE = 32768

                    with open(destination, "wb") as f:
                        for chunk in response.iter_content(CHUNK_SIZE):
                            if chunk:  # filter out keep-alive new chunks
                                f.write(chunk)

                download_file_from_google_drive(
                    "1vzyK0UVH2D3fTpa1_dSWnp1gvGpAxRul", file_name
                )
        else:
            logger.info(f"found {file_name} locally, will use that one")

        with catchtime("uncompressing and loading"):
            f = bz2.BZ2File(file_name, "rb")
            data = pickle.load(f)

        for dataset in ["cifar10", "cifar100", "ImageNet16-120"]:
            bb_dict = {}
            with catchtime(f"converting {dataset}"):
                bb_dict[dataset] = convert_dataset(data, dataset)

            with catchtime("saving to disk"):
                serialize(
                    bb_dict=bb_dict,
                    path=repository_path / dataset,
                    metadata={
                        metric_elapsed_time: "elapsed_time",
                        default_metric: METRIC_VALID_ERROR,
                        resource_attr: RESOURCE_ATTR,
                    },
                )

        # with catchtime("saving to disk"):
        #     serialize(
        #         bb_dict=bb_dict,
        #         path=repository_path / BLACKBOX_NAME,
        #         metadata={
        #             metric_elapsed_time: "elapsed_time",
        #             default_metric: METRIC_VALID_ERROR,
        #             resource_attr: RESOURCE_ATTR,
        #         },
        #     )


if __name__ == "__main__":
    NASBench201Recipe().generate()

    # plot one learning-curve for sanity-check
    from syne_tune.blackbox_repository import load_blackbox

    bb_dict = load_blackbox(BLACKBOX_NAME)

    b = bb_dict["cifar10"]
    configuration = {k: v.sample() for k, v in b.configuration_space.items()}
    errors = []
    runtime = []

    import matplotlib.pyplot as plt

    for i in range(1, 201):
        res = b.objective_function(
            configuration=configuration, fidelity={"epochs": i})
        errors.append(res[METRIC_VALID_ERROR])
        runtime.append(res[METRIC_TIME_THIS_RESOURCE])

    plt.plot(np.cumsum(runtime), errors)
    plt.show()

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/pd1_import.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import json
import logging
import tarfile
from pathlib import Path
from typing import Dict, Optional

import numpy as np
import pandas as pd

from syne_tune import config_space
from syne_tune.blackbox_repository.blackbox_tabular import BlackboxTabular
from syne_tune.blackbox_repository.conversion_scripts.blackbox_recipe import (
    BlackboxRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts import (
    default_metric,
    metric_elapsed_time,
    resource_attr,
)
from syne_tune.blackbox_repository.conversion_scripts.utils import (
    repository_path,
    download_file,
)
from syne_tune.blackbox_repository.serialize import (
    deserialize_configspace,
    deserialize_metadata,
    serialize_configspace,
    serialize_metadata,
)
from syne_tune.config_space import loguniform, randint, uniform
from syne_tune.util import catchtime

logger = logging.getLogger(__name__)

BLACKBOX_NAME = "pd1"

METRIC_VALID_ERROR = "metric_valid_error_rate"

METRIC_ELAPSED_TIME = "metric_elapsed_time"

RESOURCE_ATTR = "global_step"

CONFIGURATION_SPACE = {
    "lr_initial_value": loguniform(1e-5, 10),
    "lr_power": uniform(0.1, 2.0),
    "lr_decay_steps_factor": uniform(0.01, 0.99),
    "one_minus_momentum": loguniform(1e-3, 1.0),
}

COLUMN_RENAMING = {
    "hps.lr_hparams.initial_value": "lr_initial_value",
    "hps.lr_hparams.power": "lr_power",
    "hps.lr_hparams.decay_steps_factor": "lr_decay_steps_factor",
    "hps.opt_hparams.momentum": "one_minus_momentum",
    "valid/ce_loss": "metric_valid_ce_loss",
    "valid/error_rate": METRIC_VALID_ERROR,
    "epoch": "epoch",
    "eval_time": METRIC_ELAPSED_TIME,
    "global_step": RESOURCE_ATTR,
}


def convert_task(task_data):
    hyperparameters = task_data[list(CONFIGURATION_SPACE.keys())]

    objective_names = [
        "metric_valid_error_rate",
        "metric_valid_ce_loss",
        METRIC_ELAPSED_TIME,
    ]
    available_objectives = [
        objective_name
        for objective_name, is_not_available in task_data[objective_names]
        .isnull()
        .all()
        .to_dict()
        .items()
        if not is_not_available
    ]
    task_data.insert(
        0,
        "num_steps",
        task_data[available_objectives[0]].map(lambda x: 0 if x is None else len(x)),
    )
    learning_curve_length = task_data["num_steps"].max()

    def pad_with_nans(learning_curve, length):
        if learning_curve is None:
            return length * [np.nan]
        return learning_curve + (length - len(learning_curve)) * [np.nan]

    objectives_evaluations = list()
    for o in available_objectives:
        task_data[o] = task_data[o].apply(pad_with_nans, args=(learning_curve_length,))
        objectives_evaluations.append(np.array(task_data[o].to_list()))
    objectives_evaluations = np.expand_dims(
        np.stack(objectives_evaluations, axis=-1), 1
    )

    fidelity_space = {RESOURCE_ATTR: randint(lower=1, upper=learning_curve_length)}

    return BlackboxTabular(
        hyperparameters=hyperparameters,
        configuration_space=CONFIGURATION_SPACE,
        fidelity_space=fidelity_space,
        objectives_evaluations=objectives_evaluations,
        fidelity_values=np.arange(1, learning_curve_length + 1),
        objectives_names=available_objectives,
    )


class PD1Recipe(BlackboxRecipe):
    def __init__(self):
        super(PD1Recipe, self).__init__(
            name=BLACKBOX_NAME,
            cite_reference="Pre-trained Gaussian processes for Bayesian optimization. "
            "Wang, Z. and Dahl G. and Swersky K. and Lee C. and Mariet Z. and Nado Z. and Gilmer J. and Snoek J. and "
            "Ghahramani Z. 2021.",
        )

    def _download_data(self):
        file_name = repository_path / f"{BLACKBOX_NAME}.tar.gz"
        if not file_name.exists():
            logger.info(f"Did not find {file_name}. Starting download.")
            download_file(
                "http://storage.googleapis.com/gresearch/pint/pd1.tar.gz", file_name
            )
        else:
            logger.info(f"Skip downloading since {file_name} is available locally.")

    def _convert_data(self) -> Dict[str, BlackboxTabular]:
        with tarfile.open(repository_path / f"{BLACKBOX_NAME}.tar.gz") as f:
            f.extractall(path=repository_path)
        data = []
        for matched in ["matched", "unmatched"]:
            path = (
                repository_path
                / BLACKBOX_NAME
                / f"pd1_{matched}_phase1_results.jsonl.gz"
            )
            with open(path, "rb") as fin:
                data.append(
                    pd.read_json(fin, orient="records", lines=True, compression="gzip")
                )
        df = pd.concat(data)
        df["eval_time"] = df["eval_time"].apply(
            lambda x: None if x is None else np.cumsum(x).tolist()
        )

        tasks = df[
            ["dataset", "model", "hps.batch_size", "hps.activation_fn"]
        ].drop_duplicates()
        bb_dict = {}
        for _, task in tasks.iterrows():
            activation_name = (
                ""
                if task["hps.activation_fn"] is None
                else f"_{task['hps.activation_fn']}"
            )
            task_name = "{}_{}{}_batch_size_{}".format(
                task["dataset"],
                task["model"],
                activation_name,
                task["hps.batch_size"],
            )
            task_data = df[
                (df["dataset"] == task["dataset"])
                & (df["model"] == task["model"])
                & (df["hps.batch_size"] == task["hps.batch_size"])
            ]
            if task["hps.activation_fn"] is not None:
                task_data = task_data[
                    task_data["hps.activation_fn"] == task["hps.activation_fn"]
                ]
            task_data = task_data.reset_index()
            task_data = task_data[list(COLUMN_RENAMING)]
            task_data.columns = list(COLUMN_RENAMING.values())
            with catchtime(f"converting task {task_name}"):
                bb_dict[task_name] = convert_task(task_data)
        return bb_dict

    def _save_data(self, bb_dict: Dict[str, BlackboxTabular]) -> None:
        with catchtime("saving to disk"):
            serialize(
                bb_dict=bb_dict,
                path=repository_path / BLACKBOX_NAME,
                metadata={
                    metric_elapsed_time: METRIC_ELAPSED_TIME,
                    default_metric: METRIC_VALID_ERROR,
                    resource_attr: RESOURCE_ATTR,
                },
            )

    def _generate_on_disk(self):
        matched_file = (
            repository_path / BLACKBOX_NAME / "pd1_matched_phase1_results.jsonl.gz"
        )
        unmatched_file = (
            repository_path / BLACKBOX_NAME / "pd1_unmatched_phase1_results.jsonl.gz"
        )
        if matched_file.exists() and unmatched_file.exists():
            return
        self._download_data()
        bb_dict = self._convert_data()
        self._save_data(bb_dict)


def serialize(
    bb_dict: Dict[str, BlackboxTabular], path: str, metadata: Optional[Dict] = None
):
    # check all blackboxes share the objectives
    bb_first = next(iter(bb_dict.values()))
    for bb in bb_dict.values():
        assert bb.objectives_names == bb_first.objectives_names

    path = Path(path)
    path.mkdir(exist_ok=True)

    serialize_configspace(
        path=path,
        configuration_space=bb_first.configuration_space,
    )

    for task, bb in bb_dict.items():
        bb.hyperparameters.to_parquet(
            path / f"{task}-hyperparameters.parquet",
            index=False,
            compression="gzip",
            engine="fastparquet",
        )

        with open(path / f"{task}-fidelity_space.json", "w") as f:
            json.dump(
                {
                    k: config_space.to_dict(v)
                    for k, v in bb_dict[task].fidelity_space.items()
                },
                f,
            )

        with open(path / f"{task}-objectives_evaluations.npy", "wb") as f:
            np.save(
                f,
                bb_dict[task].objectives_evaluations.astype(np.float32),
                allow_pickle=False,
            )

        with open(path / f"{task}-fidelity_values.npy", "wb") as f:
            np.save(f, bb_dict[task].fidelity_values, allow_pickle=False)

    metadata = metadata.copy() if metadata else {}
    metadata.update(
        {
            "objectives_names": bb_first.objectives_names,
            "task_names": list(bb_dict.keys()),
        }
    )
    serialize_metadata(
        path=path,
        metadata=metadata,
    )


def deserialize(path: str) -> Dict[str, BlackboxTabular]:
    """
    Deserialize blackboxes contained in a path that were saved with `serialize` above.
    TODO: the API is currently dissonant with `serialize`, `deserialize` for BlackboxOffline as `serialize` is there a member.
    A possible way to unify is to have serialize also be a free function for BlackboxOffline.
    :param path: a path that contains blackboxes that were saved with `serialize`
    :return: a dictionary from task name to blackbox
    """
    path = Path(path)
    configuration_space, _ = deserialize_configspace(path)
    metadata = deserialize_metadata(path)
    objectives_names = metadata["objectives_names"]
    task_names = metadata["task_names"]

    bb_dict = {}
    for task in task_names:
        hyperparameters = pd.read_parquet(
            Path(path) / f"{task}-hyperparameters.parquet", engine="fastparquet"
        )
        with open(path / f"{task}-fidelity_space.json", "r") as file:
            fidelity_space = {
                k: config_space.from_dict(v) for k, v in json.load(file).items()
            }

        with open(path / f"{task}-fidelity_values.npy", "rb") as f:
            fidelity_values = np.load(f)

        with open(path / f"{task}-objectives_evaluations.npy", "rb") as f:
            objectives_evaluations = np.load(f)

        bb_dict[task] = BlackboxTabular(
            hyperparameters=hyperparameters,
            configuration_space=configuration_space,
            fidelity_space=fidelity_space,
            objectives_evaluations=objectives_evaluations,
            fidelity_values=fidelity_values,
            objectives_names=objectives_names,
        )
    return bb_dict


if __name__ == "__main__":
    PD1Recipe().generate()

File Path: syne_tune/blackbox_repository/conversion_scripts/scripts/yahpo_import.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

"""
Wrap Surrogates from 
YAHPO Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization
Florian Pfisterer, Lennart Schneider, Julia Moosbauer, Martin Binder, Bernd Bischl
"""
import logging
import shutil

from yahpo_gym import benchmark_set
import numpy as np
import zipfile
from pathlib import Path

from syne_tune.blackbox_repository.conversion_scripts.blackbox_recipe import (
    BlackboxRecipe,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts import (
    default_metric,
    metric_elapsed_time,
    resource_attr,
)
from syne_tune.blackbox_repository.conversion_scripts.utils import repository_path
from syne_tune.blackbox_repository.serialize import (
    serialize_metadata,
)
import syne_tune.config_space as cs
from syne_tune.blackbox_repository.blackbox import Blackbox
from typing import Dict, Optional


import ConfigSpace
from yahpo_gym.benchmark_set import BenchmarkSet
from yahpo_gym.configuration import list_scenarios
from yahpo_gym import local_config

from syne_tune.constants import ST_WORKER_ITER


def download(target_path: Path, version: str):
    import urllib

    root = "https://github.com/slds-lmu/yahpo_data/archive/refs/tags/"

    target_file = target_path / f"yahpo_data-{version}"
    if not target_file.exists():
        logging.info(f"File {target_file} not found redownloading it.")
        urllib.request.urlretrieve(root + f"v{version}.zip", str(target_path) + ".zip")
        with zipfile.ZipFile(str(target_path) + ".zip", "r") as zip_ref:
            zip_ref.extractall(target_path)
    else:
        logging.info(f"File {target_file} found, skipping download.")


class BlackBoxYAHPO(Blackbox):
    """
    A wrapper that allows putting a 'YAHPO' BenchmarkInstance into a Blackbox.
    """

    def __init__(self, benchmark):
        self.benchmark = benchmark
        super(BlackBoxYAHPO, self).__init__(
            configuration_space=cs_to_synetune(
                self.benchmark.get_opt_space(drop_fidelity_params=True)
            ),
            fidelity_space=cs_to_synetune(self.benchmark.get_fidelity_space()),
            objectives_names=self.benchmark.config.y_names,
        )

    def _objective_function(
        self,
        configuration: Dict,
        fidelity: Optional[Dict] = None,
        seed: Optional[int] = None,
    ) -> Dict:
        if fidelity is not None:
            configuration.update(fidelity)
            return self.benchmark.objective_function(configuration, seed)[0]
        else:
            """
            copying the parent comment of the parent class:
            "not passing a fidelity is possible if either the blackbox does not have a fidelity space
            or if it has a single fidelity in its fidelity space. In the latter case, all fidelities are returned in form
            of a tensor with shape (num_fidelities, num_objectives)."
            This is used for efficiency (it is much faster to retrieve a full row in an array in term of read time).
            """
            # returns a tensor of shape (num_fidelities, num_objectives)
            num_fidelities = len(self.fidelity_values)
            num_objectives = len(self.objectives_names)
            result = np.empty((num_fidelities, num_objectives))
            fidelity_name = next(iter(self.fidelity_space.keys()))
            configs = []
            for fidelity in self.fidelity_values:
                config_with_fidelity = configuration.copy()
                config_with_fidelity[fidelity_name] = fidelity
                configs.append(config_with_fidelity)
            result_dicts = self.benchmark.objective_function(configs, seed=seed)

            for i, fidelity in enumerate(self.fidelity_values):
                result[i] = [
                    result_dicts[i][objective] for objective in self.objectives_names
                ]

            return result

    def set_instance(self, instance):
        """
        Set an instance for the underlying YAHPO Benchmark.
        """
        # Set the instance in the benchmark
        self.benchmark.set_instance(instance)
        # Update the configspace with the fixed instance
        if self.benchmark.config.instance_names:
            instance_names = self.benchmark.config.instance_names
        else:
            instance_names = "instance-names"
        self.configuration_space[instance_names] = cs.choice([instance])
        return self

    @property
    def instances(self) -> np.array:
        return self.benchmark.instances

    @property
    def fidelity_values(self) -> np.array:
        fids = next(iter(self.fidelity_space.values()))
        return np.arange(fids.lower, fids.upper)

    @property
    def time_attribute(self) -> str:
        """Name of the time column"""
        return self.benchmark.config.runtime_name


def cs_to_synetune(config_space):
    """
    Convert ConfigSpace.ConfigSpace to a synetune configspace.

    TODO cover all possible hyperparameters of ConfigSpace.ConfigSpace, right now we only convert the one we need.
    """
    hps = config_space.get_hyperparameters()

    keys = []
    vals = []
    for a in hps:
        keys += [a.name]
        if isinstance(a, ConfigSpace.hyperparameters.CategoricalHyperparameter):
            vals += [cs.choice(a.choices)]
        elif isinstance(a, ConfigSpace.hyperparameters.Constant):
            vals += [cs.choice([a.value])]
        elif isinstance(a, ConfigSpace.hyperparameters.UniformIntegerHyperparameter):
            if a.log:
                vals += [cs.lograndint(a.lower, a.upper)]
            else:
                vals += [cs.randint(a.lower, a.upper)]
        elif isinstance(a, ConfigSpace.hyperparameters.UniformFloatHyperparameter):
            if a.log:
                vals += [cs.loguniform(a.lower, a.upper)]
            else:
                vals += [cs.uniform(a.lower, a.upper)]
        else:
            raise ValueError(
                f"Hyperparameter {a.name} has type {type(a)} which is not supported in this converter."
            )
    # FIXME: This should also handle dependencies between hyperparameters.
    return dict(zip(keys, vals))


def instantiate_yahpo(scenario: str):
    assert scenario.startswith("yahpo")
    scenario = scenario[6:]

    local_config.init_config()
    local_config.set_data_path(str(repository_path / "yahpo"))

    # Select a Benchmark, active_session False because the ONNX session can not be serialized.
    bench = benchmark_set.BenchmarkSet(scenario, active_session=False)

    return {
        instance: BlackBoxYAHPO(
            BenchmarkSet(scenario, active_session=False, check=False)
        ).set_instance(instance)
        for instance in bench.instances
    }


def serialize_yahpo(scenario: str, version: str = "1.0"):
    """
    Serialize YAHPO (Metadata only for now)
    """
    assert scenario.startswith("yahpo-")
    scenario = scenario[6:]

    # download yahpo metadata and surrogate
    download(version=version, target_path=repository_path)

    target_path = repository_path / f"yahpo" / scenario

    # copy files to yahpo-scenario
    if target_path.exists():
        shutil.rmtree(target_path)
    shutil.copytree(
        str(repository_path / f"yahpo_data-{version}" / scenario), str(target_path)
    )

    # For now we only serialize metadata because everything else can be obtained from YAHPO.
    serialize_metadata(
        path=target_path,
        metadata={
            metric_elapsed_time: "time",
            default_metric: "val_accuracy",
            resource_attr: ST_WORKER_ITER,  # TODO, ressource not present, we can use ST_WORKER_ITER
        },
    )


class YAHPORecipe(BlackboxRecipe):
    def __init__(self, name: str):
        self.scenario = name
        super(YAHPORecipe, self).__init__(
            name=name,
            cite_reference="YAHPO Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization. "
            "Pfisterer F., Schneider S., Moosbauer J., Binder M., Bischl B., 2022",
        )

    def _generate_on_disk(self):
        serialize_yahpo(self.scenario)


yahpo_scenarios = list_scenarios()


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    scenario = "lcbench"

    YAHPORecipe(f"yahpo-{scenario}").generate()

    # plot one learning-curve for sanity-check
    from syne_tune.blackbox_repository import load_blackbox

    bb_dict = load_blackbox(f"yahpo-{scenario}", skip_if_present=False)
    first_task = next(iter(bb_dict.keys()))
    b = bb_dict[first_task]
    configuration = {k: v.sample() for k, v in b.configuration_space.items()}
    errors = []
    runtime = []

    import matplotlib.pyplot as plt

    for i in range(1, 52):
        res = b.objective_function(configuration=configuration, fidelity={"epoch": i})
        errors.append(res["val_accuracy"])
        runtime.append(res["time"])

    plt.plot(np.cumsum(runtime), errors)
    plt.show()

File Path: syne_tune/blackbox_repository/conversion_scripts/utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional
import os
import logging
from functools import lru_cache
from pathlib import Path

from syne_tune.try_import import try_import_aws_message

try:
    import s3fs
    import sagemaker
    from botocore.exceptions import NoCredentialsError
except ImportError:
    print(try_import_aws_message())


@lru_cache(maxsize=1)
def s3_blackbox_folder(s3_root: Optional[str] = None):
    if s3_root is None:
        if "AWS_DEFAULT_REGION" not in os.environ:
            # avoids error "Must setup local AWS configuration with a region supported by SageMaker."
            # in case no region is explicitely configured
            os.environ["AWS_DEFAULT_REGION"] = "us-west-2"
        s3_root = sagemaker.Session().default_bucket()
    return f"{s3_root}/blackbox-repository"


repository_path = Path("~/.blackbox-repository/").expanduser()


def upload_blackbox(name: str, s3_root: Optional[str] = None):
    """
    Uploads a blackbox locally present in repository_path to S3.
    :param name: folder must be available in repository_path/name
    """
    try:
        fs = s3fs.S3FileSystem()
        for src in Path(repository_path / name).glob("*"):
            tgt = f"s3://{s3_blackbox_folder(s3_root)}/{name}/{src.name}"
            logging.info(f"copy {src} to {tgt}")
            fs.put(str(src), tgt)
    except NoCredentialsError:
        logging.warning(
            "Unable to locate credentials. Blackbox won't be uploaded to S3."
        )


def download_file(source: str, destination: str):
    import shutil
    import requests
    from syne_tune.util import catchtime

    with catchtime("Downloading file."):
        with requests.get(source, stream=True) as r:
            with open(destination, "wb") as f:
                shutil.copyfileobj(r.raw, f)

File Path: syne_tune/blackbox_repository/repository.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from pathlib import Path
from typing import List, Union, Dict, Optional

from syne_tune.try_import import try_import_aws_message

try:
    import s3fs as s3fs
    from botocore.exceptions import NoCredentialsError
except ImportError:
    print(try_import_aws_message())

from syne_tune.blackbox_repository.blackbox import Blackbox
from syne_tune.blackbox_repository.blackbox_offline import (
    deserialize as deserialize_offline,
)
from syne_tune.blackbox_repository.blackbox_tabular import (
    deserialize as deserialize_tabular,
)
from syne_tune.blackbox_repository.conversion_scripts.scripts.pd1_import import (
    deserialize as deserialize_pd1,
)

# from syne_tune.blackbox_repository.conversion_scripts.scripts.yahpo_import import (
#     instantiate_yahpo,
# )

# where the blackbox repository is stored on s3
from syne_tune.blackbox_repository.conversion_scripts.recipes import (
    generate_blackbox_recipes,
)
from syne_tune.blackbox_repository.conversion_scripts.utils import (
    repository_path,
    s3_blackbox_folder,
)


def blackbox_list() -> List[str]:
    """
    :return: list of blackboxes available
    """
    return list(generate_blackbox_recipes.keys())


def load_blackbox(
    name: str,
    skip_if_present: bool = True,
    s3_root: Optional[str] = None,
    generate_if_not_found: bool = True,
) -> Union[Dict[str, Blackbox], Blackbox]:
    """
    :param name: name of a blackbox present in the repository, see blackbox_list() to get list of available blackboxes.
    Syne Tune currently provides the following blackboxes evaluations:
    * "nasbench201": 15625 multi-fidelity configurations of computer vision architectures evaluated on 3 datasets.
    NAS-Bench-201: Extending the scope of reproducible neural architecture search.
    Dong, X. and Yang, Y. 2020.
    * "fcnet": 62208 multi-fidelity configurations of MLP evaluated on 4 datasets.
    Tabular benchmarks for joint architecture and hyperparameter optimization.
    Klein, A. and Hutter, F. 2019.
    * "lcbench": 2000 multi-fidelity Pytorch model configurations evaluated on many datasets.
    Reference: Auto-PyTorch: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL.
    Lucas Zimmer, Marius Lindauer, Frank Hutter. 2020.
    * "icml-deepar": 2420 single-fidelity configurations of DeepAR forecasting algorithm evaluated on 10 datasets.
    A quantile-based approach for hyperparameter transfer learning.
    Salinas, D., Shen, H., and Perrone, V. 2021.
    * "icml-xgboost": 5O00 single-fidelity configurations of XGBoost evaluated on 9 datasets.
    A quantile-based approach for hyperparameter transfer learning.
    Salinas, D., Shen, H., and Perrone, V. 2021.
    :param skip_if_present: skip the download if the file locally exists
    :param s3_root: S3 root directory for blackbox repository. Defaults to
        S3 bucket name of SageMaker session
    :param generate_if_not_found: If the blackbox file is not present locally
        or on S3, should it be generated using its conversion script?
    :return: blackbox with the given name, download it if not present.
    """
    tgt_folder = Path(repository_path) / name
    if (
        tgt_folder.exists()
        and (tgt_folder / "metadata.json").exists()
        and skip_if_present
    ):
        logging.info(
            f"skipping download of {name} as {tgt_folder} already exists, change skip_if_present to redownload"
        )
    else:
        tgt_folder.mkdir(exist_ok=True, parents=True)
        try:
            s3_folder = s3_blackbox_folder(s3_root)
            fs = s3fs.S3FileSystem()
            data_on_s3 = fs.exists(f"{s3_folder}/{name}/metadata.json")
        except NoCredentialsError:
            data_on_s3 = False
        if data_on_s3:
            logging.info("found blackbox on S3, copying it locally")
            # download files from s3 to repository_path
            for src in fs.glob(f"{s3_folder}/{name}/*"):
                tgt = tgt_folder / Path(src).name
                logging.info(f"copying {src} to {tgt}")
                fs.get(src, str(tgt))
        else:
            assert generate_if_not_found, (
                "Blackbox files do not exist locally or on S3. If you have "
                + f"write permissions to {s3_folder}, you can set "
                + "generate_if_not_found=True in order to generate and persist them"
            )
            logging.info(
                "did not find blackbox files locally nor on S3, regenerating it locally and persisting it on S3."
            )
            generate_blackbox_recipes[name].generate(s3_root=s3_root)

    # if name.startswith("yahpo"):
    #     return instantiate_yahpo(name)
    if name.startswith("pd1"):
        return deserialize_pd1(tgt_folder)
    if (tgt_folder / "hyperparameters.parquet").exists():
        return deserialize_tabular(tgt_folder)
    else:
        return deserialize_offline(tgt_folder)


if __name__ == "__main__":
    # list all blackboxes available
    blackboxes = blackbox_list()
    print(blackboxes)

    for bb in blackboxes:
        print(bb)
        # download an existing blackbox
        blackbox = load_blackbox(bb)
        print(blackbox)

File Path: syne_tune/blackbox_repository/serialize.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
from typing import Optional, Dict
import json
import syne_tune.config_space as sp


def serialize_configspace(
    path: str, configuration_space: Dict, fidelity_space: Optional[Dict] = None
):
    path = Path(path)
    with open(path / "configspace.json", "w") as f:
        json.dump({k: sp.to_dict(v) for k, v in configuration_space.items()}, f)

    if fidelity_space is not None:
        with open(path / "fidelityspace.json", "w") as f:
            json.dump({k: sp.to_dict(v) for k, v in fidelity_space.items()}, f)


def deserialize_configspace(path: str):
    def open_if_exists(name):
        config_path = Path(path) / name
        if config_path.exists():
            with open(config_path, "r") as file:
                cs_space = json.load(file)
                return {k: sp.from_dict(v) for k, v in cs_space.items()}
        else:
            return None

    configuration_space = open_if_exists("configspace.json")
    fidelity_space = open_if_exists("fidelityspace.json")
    return configuration_space, fidelity_space


def serialize_metadata(path: str, metadata):
    with open(path / "metadata.json", "w") as f:
        json.dump(metadata, f)


def deserialize_metadata(path: str):
    with open(Path(path) / "metadata.json", "r") as f:
        metadata = json.load(f)
        return metadata

File Path: syne_tune/blackbox_repository/simulated_tabular_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
from typing import List, Optional
import logging
import numpy as np

from syne_tune.blackbox_repository import load_blackbox, add_surrogate
from syne_tune.blackbox_repository.blackbox import Blackbox
from syne_tune.blackbox_repository.blackbox_tabular import BlackboxTabular
from syne_tune.blackbox_repository.utils import metrics_for_configuration

from syne_tune.backend.simulator_backend.simulator_backend import SimulatorBackend
from syne_tune.backend.trial_status import Status
from syne_tune.config_space import to_dict, from_dict, Domain

logger = logging.getLogger(__name__)


class _BlackboxSimulatorBackend(SimulatorBackend):
    def __init__(
        self,
        elapsed_time_attr: str,
        max_resource_attr: Optional[str] = None,
        seed: Optional[int] = None,
        **simulatorbackend_kwargs,
    ):
        """
        Allows to simulate any blackbox from blackbox-repository, can be either a blackbox from a registered
        tabulated benchmark (in this case, you should use `BlackboxRepositoryBackend`) or a blackbox given from custom
        code (in this case, you should use `UserBlackboxBackend`), see `examples/launch_simulated_benchmark.py` for
        an example on how to use.

        In each result reported to the simulator back-end, the value for key
        `elapsed_time_attr` must be the time since the start of the
        evaluation. For example, if resource (or fidelity) equates to epochs
        trained, this would be the time from start of training until the end
        of the epoch. If the blackbox contains this information in a column,
        `elapsed_time_attr` should be its key.

        If this backend is used with pause-and-resume multi-fidelity
        scheduling, it needs to track at which resource level each trial is
        paused. Namely, once a trial is resumed, all results for resources
        smaller or equal to that level are ignored, which simulates the
        situation that training is resumed from a checkpoint. This feature
        relies on `result` to be passed to `pause_trial`. If this is not
        done, the backend cannot know from which resource level to resume
        a trial, so it starts the trial from scratch (which is equivalent to
        no checkpointing).

        ATTENTION: If the blackbox maintains cumulative time (elapsed_time),
        this is different from what :class:`SimulatorBackend` requires for
        `elapsed_time_attr`, if a pause-and-resume scheduler is used.
        Namely, the back-end requires the time since the start of the last recent
        resume. This conversion is done here internally in
        `_run_job_and_collect_results`, which is called for each resume. This
        means that the field `elapsed_time_attr` is not what is received from
        the blackbox table, but instead what the back-end needs.

        `max_resource_attr` plays the same role as in :class:`HyperbandScheduler`.
        If given, it is the key in a configuration `config` for the maximum
        resource. This is used by schedulers which limit each evaluation by
        setting this argument (e.g., promotion-based Hyperband).

        If `seed` is given, entries of the blackbox are queried for this
        seed. Otherwise, a seed is drawn at random for every trial, but the
        same seed is used for all `_run_job_and_collect_results` calls for the
        same trial. This is important for pause and resume scheduling.

        :param elapsed_time_attr: See above
        :param max_resource_attr: See above
        :param seed: See above
        """
        super().__init__(
            # TODO we feed a dummy value for entry_point since they are not required
            entry_point=str(Path(__file__)),
            elapsed_time_attr=elapsed_time_attr,
            **simulatorbackend_kwargs,
        )
        self._max_resource_attr = max_resource_attr
        self.simulatorbackend_kwargs = simulatorbackend_kwargs
        self._seed = seed
        self._seed_for_trial = dict()
        self._resource_paused_for_trial = dict()

    @property
    def blackbox(self) -> Blackbox:
        raise NotImplementedError()

    @property
    def resource_attr(self):
        return next(iter(self.blackbox.fidelity_space.keys()))

    def _pause_trial(self, trial_id: int, result: Optional[dict]):
        """
        From `result`, we obtain the resource level at which the trial is
        paused by the scheduler. This is required in order to properly
        resume the trial in `_run_job_and_collect_results`.
        """
        super()._pause_trial(trial_id, result)
        resource_attr = self.resource_attr
        if result is not None and resource_attr in result:
            resource = int(result[resource_attr])
            self._resource_paused_for_trial[trial_id] = resource

    def config_objectives(self, config: dict, seed: int) -> List[dict]:
        if self._max_resource_attr is not None and self._max_resource_attr in config:
            max_resource = int(config[self._max_resource_attr])
            fidelity_range = (min(self.blackbox.fidelity_values), max_resource)
        else:
            fidelity_range = None  # All fidelity values
        return metrics_for_configuration(
            blackbox=self.blackbox,
            config=config,
            resource_attr=self.resource_attr,
            fidelity_range=fidelity_range,
            seed=seed,
        )

    def _run_job_and_collect_results(
        self, trial_id: int, config: Optional[dict] = None
    ) -> (str, List[dict]):
        """
        :param trial_id:
        :return: (final status, list of all results reported)
        """
        assert (
            trial_id in self._trial_dict
        ), f"Trial with trial_id = {trial_id} not registered with back-end"
        if config is None:
            config = self._trial_dict[trial_id].config

        # Seed for query to blackbox. It is important to use the same
        # seed for all queries for the same `trial_id`
        seed = None
        if self._seed is not None:
            seed = self._seed
        elif isinstance(self.blackbox, BlackboxTabular):
            seed = self._seed_for_trial.get(trial_id)
            if seed is None:
                seed = np.random.randint(0, self.blackbox.num_seeds)
                self._seed_for_trial[trial_id] = seed

        # Fetch all results for this trial from the table
        all_results = self.config_objectives(config, seed=seed)

        status = Status.completed
        resource_paused = self._resource_paused_for_trial.get(trial_id)
        if resource_paused is not None:
            resource_attr = self.resource_attr
            elapsed_time_offset = 0
            results = []
            for result in all_results:
                resource = int(result[resource_attr])
                if resource > resource_paused:
                    results.append(result)
                elif resource == resource_paused:
                    elapsed_time_offset = result[self.elapsed_time_attr]
            for result in results:
                result[self.elapsed_time_attr] -= elapsed_time_offset
        else:
            results = all_results

        # Makes sure that time is monotonically increasing which may not be the case due to numerical errors or due to
        # the use of a surrogate
        for i in range(1, len(results)):
            results[i][self.elapsed_time_attr] = max(
                results[i][self.elapsed_time_attr],
                results[i - 1][self.elapsed_time_attr] + 0.001,
            )

        return status, results


def make_surrogate(
    surrogate: Optional[str] = None, surrogate_kwargs: Optional[dict] = None
):
    """
    :param surrogate: optionally, a model that is fitted to predict objectives given any configuration.
    Possible examples: "KNeighborsRegressor" or "MLPRegressor" or "XGBRegressor" which would enable using
    the corresponding Scikit-learn estimator.
    The model is fit on top of pipeline that applies basic feature-processing to convert hyperparameters
    rows in X to vectors. The configuration_space hyperparameters types are used to deduce the types of columns in
     X (for instance CategoricalHyperparameter are one-hot encoded).
    :param surrogate_kwargs: arguments for the scikit-learn estimator, for instance {"n_neighbors": 1} can be used
    if `surrogate="KNeighborsRegressor"` is chosen.
    :return:
    """
    if surrogate is None:
        return None
    else:
        from sklearn.neighbors import KNeighborsRegressor
        from sklearn.neural_network import MLPRegressor
        from sklearn.ensemble import RandomForestRegressor
        import xgboost

        surrogate_dict = {
            "KNeighborsRegressor": KNeighborsRegressor,
            "MLPRegressor": MLPRegressor,
            "XGBRegressor": xgboost.XGBRegressor,
            "RandomForestRegressor": RandomForestRegressor,
        }
        assert surrogate in surrogate_dict, (
            f"surrogate passed {surrogate} is not supported, "
            f"only {list(surrogate_dict.keys())} are available"
        )
        if surrogate_kwargs is None:
            surrogate_kwargs = dict()
        return surrogate_dict[surrogate](**surrogate_kwargs)


class BlackboxRepositoryBackend(_BlackboxSimulatorBackend):
    def __init__(
        self,
        blackbox_name: str,
        elapsed_time_attr: str,
        max_resource_attr: Optional[str] = None,
        seed: Optional[int] = None,
        dataset: Optional[str] = None,
        surrogate: Optional[str] = None,
        surrogate_kwargs: Optional[dict] = None,
        config_space_surrogate: Optional[dict] = None,
        **simulatorbackend_kwargs,
    ):
        """
        Backend for evaluations from the blackbox-repository, name of the blackbox and dataset should be present in the
        repository. See `examples/launch_simulated_benchmark.py` for an example on how to use.
        If you want to add a new dataset, see the section `Adding a new dataset section` of
        `blackbox_repository/README.md`.

        :param blackbox_name: name of a blackbox, should have been registered in blackbox repository.
        :param elapsed_time_attr: name of the column containing cumulative time
        :param max_resource_attr:
        :param dataset: Selects different versions of the blackbox
        :param surrogate: optionally, a model that is fitted to predict objectives given any configuration.
        Possible examples: "KNeighborsRegressor" or "MLPRegressor" or "XGBRegressor" which would enable using
        the corresponding Scikit-learn estimator.
        The model is fit on top of pipeline that applies basic feature-processing to convert hyperparameters
        rows in X to vectors. The configuration_space hyperparameters types are used to deduce the types of columns in
         X (for instance CategoricalHyperparameter are one-hot encoded).
        :param surrogate_kwargs: arguments for the scikit-learn estimator, for instance {"n_neighbors": 1} can be used
        if `surrogate="KNeighborsRegressor"` is chosen.
        :param config_space_surrogate: if `surrogate` is given, this is the
            configuration space for the surrogate blackbox. If not given, the
            space of the original blackbox is used. However, if this is a tabular
            blackbox, its numerical parameters have categorical domains, which is
            usually not what we want for a surrogate.
        """
        assert (
            config_space_surrogate is None or surrogate is not None
        ), "config_space_surrogate only together with surrogate"
        super().__init__(
            elapsed_time_attr=elapsed_time_attr,
            max_resource_attr=max_resource_attr,
            seed=seed,
            **simulatorbackend_kwargs,
        )
        self.blackbox_name = blackbox_name
        self.dataset = dataset
        self._blackbox = None
        if surrogate is not None:
            # makes sure the surrogate can be constructed
            make_surrogate(surrogate=surrogate, surrogate_kwargs=surrogate_kwargs)
        self._surrogate = surrogate
        self._surrogate_kwargs = (
            surrogate_kwargs if surrogate_kwargs is not None else {}
        )
        if config_space_surrogate is not None:
            self._config_space_surrogate = {
                k: v for k, v in config_space_surrogate.items() if isinstance(v, Domain)
            }
        else:
            self._config_space_surrogate = None

    @property
    def blackbox(self) -> Blackbox:
        if self._blackbox is None:
            if self.dataset is None:
                self._blackbox = load_blackbox(self.blackbox_name)
                # TODO: This could fail earlier
                assert not isinstance(self._blackbox, dict), (
                    f"blackbox_name = '{self.blackbox_name}' maps to a dict, "
                    + "dataset argument must be given"
                )
            else:
                self._blackbox = load_blackbox(self.blackbox_name)[self.dataset]
            if self._surrogate is not None:
                surrogate = make_surrogate(
                    surrogate=self._surrogate, surrogate_kwargs=self._surrogate_kwargs
                )
                self._blackbox = add_surrogate(
                    blackbox=self._blackbox,
                    surrogate=surrogate,
                    configuration_space=self._config_space_surrogate,
                )

        return self._blackbox

    def __getstate__(self):
        # we serialize only required metadata information since the blackbox data is contained in the repository and
        # its raw data does not need to be saved.
        state = {
            "elapsed_time_attr": self.elapsed_time_attr,
            "max_resource_attr": self._max_resource_attr,
            "seed": self._seed,
            "seed_for_trial": self._seed_for_trial,
            "simulatorbackend_kwargs": self.simulatorbackend_kwargs,
            "blackbox_name": self.blackbox_name,
            "dataset": self.dataset,
            "surrogate": self._surrogate,
            "surrogate_kwargs": self._surrogate_kwargs,
        }
        if self._config_space_surrogate is not None:
            state["config_space_surrogate"] = {
                k: to_dict(v) for k, v in self._config_space_surrogate.items()
            }
        return state

    def __setstate__(self, state):
        super().__init__(
            elapsed_time_attr=state["elapsed_time_attr"],
            max_resource_attr=state["max_resource_attr"],
            seed=state["seed"],
            **state["simulatorbackend_kwargs"],
        )
        self._seed_for_trial = state["seed_for_trial"]
        self.blackbox_name = state["blackbox_name"]
        self.dataset = state["dataset"]
        self._surrogate = state["surrogate"]
        self._surrogate_kwargs = state["surrogate_kwargs"]
        self._blackbox = None
        if "config_space_surrogate" in state:
            self._config_space_surrogate = {
                k: from_dict(v) for k, v in state["config_space_surrogate"].items()
            }
        else:
            self._config_space_surrogate = None


class UserBlackboxBackend(_BlackboxSimulatorBackend):
    def __init__(
        self,
        blackbox: Blackbox,
        elapsed_time_attr: str,
        max_resource_attr: Optional[str] = None,
        seed: Optional[int] = None,
        **simulatorbackend_kwargs,
    ):
        """
        Backend to run simulation from a user blackbox.

        :param blackbox: blackbox to be used for simulation, see `examples/launch_simulated_benchmark.py` for an example
            on how to use.
        :param elapsed_time_attr:
        :param max_resource_attr:
        """
        super().__init__(
            elapsed_time_attr=elapsed_time_attr,
            max_resource_attr=max_resource_attr,
            seed=seed,
            **simulatorbackend_kwargs,
        )
        self._blackbox = blackbox

    @property
    def blackbox(self) -> Blackbox:
        return self._blackbox

File Path: syne_tune/blackbox_repository/utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Optional, Tuple

from syne_tune.blackbox_repository.blackbox import Blackbox


def metrics_for_configuration(
    blackbox: Blackbox,
    config: dict,
    resource_attr: str,
    fidelity_range: Optional[Tuple[float, float]] = None,
    seed: Optional[int] = None,
) -> List[dict]:
    """
    Returns all results for configuration `config` at fidelities in range
    `fidelity_range`.

    :param blackbox: Blackbox
    :param config: Configuration
    :param resource_attr: Name of resource attribute
    :param fidelity_range: Range [min_f, max_f], only fidelities in this range
        (both ends inclusive) are returned. Default is no filtering
    :param seed: Seed for queries to blackbox. Drawn at random if not
        given
    :return: List of result dicts

    """
    all_fidelities = blackbox.fidelity_values
    assert all_fidelities is not None, "Blackbox must come with fidelities"
    res = []
    if fidelity_range is None:
        fidelity_range = (min(all_fidelities), max(all_fidelities))
    else:
        assert (
            len(fidelity_range) == 2 and fidelity_range[0] <= fidelity_range[1]
        ), f"fidelity_range = {fidelity_range} must be tuple (min, max), min <= max"
    objective_values = blackbox._objective_function(config, seed=seed)
    for fidelity, value in enumerate(all_fidelities):
        if value >= fidelity_range[0] and value <= fidelity_range[1]:
            res_dict = dict(zip(blackbox.objectives_names, objective_values[fidelity]))
            res_dict[resource_attr] = value
            res.append(res_dict)
    return res

File Path: syne_tune/config_space.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

# This file has been taken from Ray. The reason for reusing the file is to be able to support the same API when
# defining search space while avoiding to have Ray as a required dependency. We may want to add functionality in the
# future.
import logging
from copy import copy
from inspect import signature
from math import isclose
import sys
from typing import Any, Callable, Dict, List, Optional, Sequence, Union
import argparse

import numpy as np

logger = logging.getLogger(__name__)


class Domain:
    """Base class to specify a type and valid range to sample parameters from.

    This base class is implemented by parameter spaces, like float ranges
    (``Float``), integer ranges (``Integer``), or categorical variables
    (``Categorical``). The ``Domain`` object contains information about
    valid values (e.g. minimum and maximum values), and exposes methods that
    allow specification of specific samplers (e.g. ``uniform()`` or
    ``loguniform()``).

    """

    sampler = None
    default_sampler_cls = None

    @property
    def value_type(self):
        raise NotImplementedError

    def cast(self, value):
        """Cast value to domain type"""
        return self.value_type(value)

    def set_sampler(self, sampler, allow_override=False):
        if self.sampler and not allow_override:
            raise ValueError(
                "You can only choose one sampler for parameter "
                "domains. Existing sampler for parameter {}: "
                "{}. Tried to add {}".format(
                    self.__class__.__name__, self.sampler, sampler
                )
            )
        self.sampler = sampler

    def get_sampler(self) -> "Sampler":
        sampler = self.sampler
        if not sampler:
            sampler = self.default_sampler_cls()
        return sampler

    def sample(self, spec=None, size=1, random_state=None):
        """
        :param size: Number of values to sample
        :param random_state: PRN generator
        :return: Single value (`size == 1`) or list (`size > 1`)
        """
        sampler = self.get_sampler()
        return sampler.sample(self, spec=spec, size=size, random_state=random_state)

    def is_grid(self):
        return isinstance(self.sampler, Grid)

    def is_function(self):
        return False

    def is_valid(self, value: Any):
        """Returns True if `value` is a valid value in this domain."""
        raise NotImplementedError

    @property
    def domain_str(self):
        return "(unknown)"

    def __len__(self):
        """
        :return: Size of domain (number of distinct elements), or 0 if size
            is infinite
        """
        raise NotImplementedError

    def match_string(self, value) -> str:
        """
        Returns string representation of `value` (which must be of domain type)
        which is to match configurations for (approximate) equality.
        For discrete types (e.g., `Integer`, `Categorical`), this matches for
        exact equality.

        :param value: Value of domain type (use `cast` to be safe)
        :return: String representation useful for matching
        """
        raise NotImplementedError

    def __eq__(self, other) -> bool:
        if self.sampler is None:
            return other.sampler is None
        else:
            return self.sampler == other.sampler


class Sampler:
    def sample(
        self,
        domain: Domain,
        spec: Optional[Union[List[Dict], Dict]] = None,
        size: int = 1,
        random_state: Optional[np.random.RandomState] = None,
    ):
        raise NotImplementedError

    def __eq__(self, other) -> bool:
        raise NotImplementedError


class BaseSampler(Sampler):
    def __str__(self):
        return "Base"


class Uniform(Sampler):
    def __str__(self):
        return "Uniform"

    def __eq__(self, other) -> bool:
        return isinstance(other, Uniform)


EXP_ONE = np.exp(1.0)


class LogUniform(Sampler):
    """
    Note: We keep the argument `base` for compatibility with Ray Tune.
    Since `base` has no effect on the distribution, we don't use it
    internally.

    """

    def __init__(self, base: float = EXP_ONE):
        assert base > 0, "Base has to be strictly greater than 0"
        self.base = base  # Not really used internally

    def __str__(self):
        return "LogUniform"

    def __eq__(self, other) -> bool:
        return isinstance(other, LogUniform) and self.base == other.base


class Normal(Sampler):
    def __init__(self, mean: float = 0.0, sd: float = 0.0):
        self.mean = mean
        self.sd = sd

        assert self.sd > 0, "SD has to be strictly greater than 0"

    def __str__(self):
        return "Normal"

    def __eq__(self, other) -> bool:
        return (
            isinstance(other, Normal)
            and np.isclose(self.mean, other.mean)
            and np.isclose(self.sd, other.sd)
        )


class Grid(Sampler):
    """Dummy sampler used for grid search"""

    def sample(
        self,
        domain: Domain,
        spec: Optional[Union[List[Dict], Dict]] = None,
        size: int = 1,
        random_state: Optional[np.random.RandomState] = None,
    ):
        return RuntimeError("Do not call `sample()` on grid.")

    def __eq__(self, other) -> bool:
        return isinstance(other, Grid)


def _sanitize_sample_result(items, domain: Domain):
    if len(items) > 1:
        return [domain.cast(x) for x in items]
    else:
        return domain.cast(items[0])


class Float(Domain):
    class _Uniform(Uniform):
        def sample(
            self,
            domain: "Float",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            assert domain.lower > float("-inf"), "Uniform needs a lower bound"
            assert domain.upper < float("inf"), "Uniform needs a upper bound"
            if random_state is None:
                random_state = np.random
            items = random_state.uniform(domain.lower, domain.upper, size=size)
            return _sanitize_sample_result(items, domain)

    class _LogUniform(LogUniform):
        def sample(
            self,
            domain: "Float",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            assert domain.lower > 0, "LogUniform needs a lower bound greater than 0"
            assert (
                0 < domain.upper < float("inf")
            ), "LogUniform needs a upper bound greater than 0"
            # Note: We don't use `self.base` here, because it does not make a
            # difference
            logmin = np.log(domain.lower)
            logmax = np.log(domain.upper)
            if random_state is None:
                random_state = np.random
            log_items = random_state.uniform(logmin, logmax, size=size)
            items = np.exp(log_items)
            return _sanitize_sample_result(items, domain)

    # Transform is -log(1 - x)
    class _ReverseLogUniform(LogUniform):
        def sample(
            self,
            domain: "Float",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            assert 0 <= domain.lower <= domain.upper < 1
            logmin = -np.log1p(-domain.lower)
            logmax = -np.log1p(-domain.upper)
            if random_state is None:
                random_state = np.random
            log_items = random_state.uniform(logmin, logmax, size=size)
            items = -np.expm1(-log_items)
            return _sanitize_sample_result(items, domain)

    class _Normal(Normal):
        def sample(
            self,
            domain: "Float",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            assert not domain.lower or domain.lower == float(
                "-inf"
            ), "Normal sampling does not allow a lower value bound."
            assert not domain.upper or domain.upper == float(
                "inf"
            ), "Normal sampling does not allow a upper value bound."
            if random_state is None:
                random_state = np.random
            items = random_state.normal(self.mean, self.sd, size=size)
            return _sanitize_sample_result(items, domain)

    default_sampler_cls = _Uniform

    def __init__(self, lower: Optional[float], upper: Optional[float]):
        # Need to explicitly check for None
        self.lower = lower if lower is not None else float("-inf")
        self.upper = upper if upper is not None else float("inf")

    @property
    def value_type(self):
        return float

    def uniform(self):
        if not self.lower > float("-inf"):
            raise ValueError(
                "Uniform requires a lower bound. Make sure to set the "
                "`lower` parameter of `Float()`."
            )
        if not self.upper < float("inf"):
            raise ValueError(
                "Uniform requires a upper bound. Make sure to set the "
                "`upper` parameter of `Float()`."
            )
        new = copy(self)
        new.set_sampler(self._Uniform())
        return new

    def loguniform(self):
        if not self.lower > 0:
            raise ValueError(
                "LogUniform requires a lower bound greater than 0."
                f"Got: {self.lower}. Did you pass a variable that has "
                "been log-transformed? If so, pass the non-transformed value "
                "instead."
            )
        if not 0 < self.upper < float("inf"):
            raise ValueError(
                "LogUniform requires a upper bound greater than 0. "
                f"Got: {self.lower}. Did you pass a variable that has "
                "been log-transformed? If so, pass the non-transformed value "
                "instead."
            )
        new = copy(self)
        new.set_sampler(self._LogUniform())
        return new

    def reverseloguniform(self):
        if not (0 <= self.lower <= self.upper < 1):
            raise ValueError(
                "ReverseLogUniform requires 0 <= lower <= upper < 1."
                f"Got: lower={self.lower}, upper={self.upper}. Did you "
                "pass a variable that has been transformed as -log(1 - x)?"
                "If so, pass the non-transformed values instead."
            )
        new = copy(self)
        new.set_sampler(self._ReverseLogUniform())
        return new

    def normal(self, mean=0.0, sd=1.0):
        new = copy(self)
        new.set_sampler(self._Normal(mean, sd))
        return new

    def quantized(self, q: float):
        if self.lower > float("-inf") and not isclose(
            self.lower / q, round(self.lower / q)
        ):
            raise ValueError(
                f"Your lower variable bound {self.lower} is not divisible by "
                f"quantization factor {q}."
            )
        if self.upper < float("inf") and not isclose(
            self.upper / q, round(self.upper / q)
        ):
            raise ValueError(
                f"Your upper variable bound {self.upper} is not divisible by "
                f"quantization factor {q}."
            )

        new = copy(self)
        new.set_sampler(Quantized(new.get_sampler(), q), allow_override=True)
        return new

    def is_valid(self, value: float):
        return self.lower <= value <= self.upper

    @property
    def domain_str(self):
        return f"({self.lower}, {self.upper})"

    def __len__(self):
        if self.lower < self.upper:
            return 0
        else:
            return 1

    def match_string(self, value) -> str:
        return f"{value:.7e}"

    def __eq__(self, other) -> bool:
        return (
            isinstance(other, Float)
            and super(Float, self).__eq__(other)
            and np.isclose(self.lower, other.lower)
            and np.isclose(self.upper, other.upper)
        )


class Integer(Domain):
    class _Uniform(Uniform):
        def sample(
            self,
            domain: "Integer",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            if random_state is None:
                random_state = np.random
            # Note: domain.upper is inclusive here, but exclusive in
            # `np.random.randint`.
            items = random_state.randint(domain.lower, domain.upper + 1, size=size)
            return _sanitize_sample_result(items, domain)

    class _LogUniform(LogUniform):
        def sample(
            self,
            domain: "Integer",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            assert domain.lower > 0, "LogUniform needs a lower bound greater than 0"
            assert (
                0 < domain.upper < float("inf")
            ), "LogUniform needs a upper bound greater than 0"
            # Note: We don't use `self.base` here, because it does not make a
            # difference
            logmin = np.log(domain.lower)
            logmax = np.log(domain.upper)
            if random_state is None:
                random_state = np.random
            log_items = random_state.uniform(logmin, logmax, size=size)
            items = np.exp(log_items)
            items = np.round(items).astype(int)
            return _sanitize_sample_result(items, domain)

    default_sampler_cls = _Uniform

    def __init__(self, lower, upper):
        self.lower = self.cast(lower)
        self.upper = self.cast(upper)

    @property
    def value_type(self):
        return int

    def cast(self, value):
        return int(round(value))

    def quantized(self, q: int):
        new = copy(self)
        new.set_sampler(Quantized(new.get_sampler(), q), allow_override=True)
        return new

    def uniform(self):
        new = copy(self)
        new.set_sampler(self._Uniform())
        return new

    def loguniform(self):
        if not self.lower > 0:
            raise ValueError(
                "LogUniform requires a lower bound greater than 0."
                f"Got: {self.lower}. Did you pass a variable that has "
                "been log-transformed? If so, pass the non-transformed value "
                "instead."
            )
        if not 0 < self.upper < float("inf"):
            raise ValueError(
                "LogUniform requires a upper bound greater than 0. "
                f"Got: {self.lower}. Did you pass a variable that has "
                "been log-transformed? If so, pass the non-transformed value "
                "instead."
            )
        new = copy(self)
        new.set_sampler(self._LogUniform())
        return new

    def is_valid(self, value: int):
        return self.lower <= value <= self.upper

    @property
    def domain_str(self):
        return f"({self.lower}, {self.upper})"

    def __len__(self):
        return self.upper - self.lower + 1

    def match_string(self, value) -> str:
        return str(value)

    def __eq__(self, other) -> bool:
        return (
            isinstance(other, Integer)
            and super(Integer, self).__eq__(other)
            and self.lower == other.lower
            and self.upper == other.upper
        )


class Categorical(Domain):
    class _Uniform(Uniform):
        def sample(
            self,
            domain: "Categorical",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            if random_state is None:
                random_state = np.random
            categories = domain.categories
            items = [
                categories[i] for i in random_state.choice(len(categories), size=size)
            ]
            return _sanitize_sample_result(items, domain)

    default_sampler_cls = _Uniform

    def __init__(self, categories: Sequence):
        assert len(categories) > 0
        self.categories = list(categories)
        value_type = self.value_type
        assert all(
            type(x) == value_type for x in self.categories
        ), f"All entries in categories = {self.categories} must have the same type"
        if isinstance(self.value_type, float):
            logger.warning(
                "The configuration space contains a categorical value with float type. "
                "When performing remote execution, floats are converted to string which can cause rounding "
                "issues. In case of problem, consider using string to represent the float."
            )

    def uniform(self):
        new = copy(self)
        new.set_sampler(self._Uniform())
        return new

    def grid(self):
        new = copy(self)
        new.set_sampler(Grid())
        return new

    def __len__(self):
        return len(self.categories)

    def __getitem__(self, item):
        return self.categories[item]

    def is_valid(self, value: Any):
        return value in self.categories

    @property
    def value_type(self):
        return type(self.categories[0])

    @property
    def domain_str(self):
        return f"{self.categories}"

    def cast(self, value):
        value = self.value_type(value)
        if value not in self.categories:
            assert isinstance(
                value, float
            ), f"value = {value} not contained in categories = {self.categories}"
            # For value type float, we do nearest neighbor matching, in order to
            # avoid meaningless mistakes due to round-off or conversions from
            # string and back
            categ_arr = np.array(self.categories)
            distances = np.abs(categ_arr - value)
            minind = np.argmin(distances)
            assert distances[minind] < 0.01 * abs(
                categ_arr[minind]
            ), f"value = {value} not contained or close to any in categories = {self.categories}"
            value = self.categories[minind]
        return value

    def match_string(self, value) -> str:
        return str(self.categories.index(value))

    def __repr__(self):
        return f"choice({self.categories})"

    def __eq__(self, other) -> bool:
        return (
            isinstance(other, Categorical)
            and super(Categorical, self).__eq__(other)
            and self.categories == other.categories
        )


class Ordinal(Categorical):
    """
    Represents an ordered set. As far as random sampling is concerned, this
    type is equivalent to :class:`Categorical`, but when used in methods
    that require encodings (or distances), nearby values have closer
    encodings.
    """

    def __init__(self, categories: Sequence):
        super().__init__(categories)

    def __repr__(self):
        return f"ordinal({self.categories})"

    def __eq__(self, other) -> bool:
        return (
            isinstance(other, Ordinal)
            and super(Ordinal, self).__eq__(other)
            and self.categories == other.categories
        )


class Function(Domain):
    class _CallSampler(BaseSampler):
        def sample(
            self,
            domain: "Function",
            spec: Optional[Union[List[Dict], Dict]] = None,
            size: int = 1,
            random_state: Optional[np.random.RandomState] = None,
        ):
            if random_state is not None:
                raise NotImplementedError()
            if domain.pass_spec:
                items = [
                    domain.func(spec[i] if isinstance(spec, list) else spec)
                    for i in range(size)
                ]
            else:
                items = [domain.func() for i in range(size)]

            return _sanitize_sample_result(items, domain)

    default_sampler_cls = _CallSampler

    def __init__(self, func: Callable):
        sig = signature(func)

        pass_spec = True  # whether we should pass `spec` when calling `func`
        try:
            sig.bind({})
        except TypeError:
            pass_spec = False

        if not pass_spec:
            try:
                sig.bind()
            except TypeError as exc:
                raise ValueError(
                    "The function passed to a `Function` parameter must be "
                    "callable with either 0 or 1 parameters."
                ) from exc

        self.pass_spec = pass_spec
        self.func = func

    def is_function(self):
        return True

    def is_valid(self, value: Any):
        return True  # This is user-defined, so lets not assume anything

    @property
    def domain_str(self):
        return f"{self.func}()"

    def __len__(self):
        return 0


class Quantized(Sampler):
    def __init__(self, sampler: Sampler, q: Union[float, int]):
        self.sampler = sampler
        self.q = q

        assert self.sampler, "Quantized() expects a sampler instance"

    def get_sampler(self):
        return self.sampler

    def sample(
        self,
        domain: Domain,
        spec: Optional[Union[List[Dict], Dict]] = None,
        size: int = 1,
        random_state: Optional[np.random.RandomState] = None,
    ):
        values = self.sampler.sample(domain, spec, size, random_state)
        quantized = np.round(np.divide(values, self.q)) * self.q
        if not isinstance(quantized, np.ndarray):
            return domain.cast(quantized)
        return list(quantized)

    def __eq__(self, other) -> bool:
        return (
            isinstance(other, Quantized)
            and self.q == other.q
            and self.sampler == other.sampler
        )


class FiniteRange(Domain):
    """
    Represents a finite range `[lower, ..., upper]` with `size` values
    equally spaced in linear or log domain.
    If `cast_int`, the value type is int (rounding after the transform).
    """

    def __init__(
        self,
        lower: float,
        upper: float,
        size: int,
        log_scale: bool = False,
        cast_int: bool = False,
    ):
        assert lower <= upper
        assert size >= 1
        if log_scale:
            assert lower > 0.0
        self._uniform_int = randint(0, size - 1)
        self.lower = lower
        self.upper = upper
        self.log_scale = log_scale
        self.cast_int = cast_int
        self.size = size
        if not log_scale:
            self._lower_internal = lower
            self._step_internal = (upper - lower) / (size - 1) if size > 1 else 0
        else:
            self._lower_internal = np.log(lower)
            upper_internal = np.log(upper)
            self._step_internal = (
                (upper_internal - self._lower_internal) / (size - 1) if size > 1 else 0
            )
        self._values = [self._map_from_int(x) for x in range(self.size)]

    @property
    def values(self):
        return self._values

    def _map_from_int(self, x: int) -> Union[float, int]:
        y = x * self._step_internal + self._lower_internal
        if self.log_scale:
            y = np.exp(y)
        res = float(np.clip(y, self.lower, self.upper))
        if self.cast_int:
            res = int(np.rint(res))
        return res

    def __repr__(self):
        values_str = ",".join([str(x) for x in self._values])
        return f"finite-range([{values_str}])"

    @property
    def value_type(self):
        return float if not self.cast_int else int

    def _map_to_int(self, value) -> int:
        if self._step_internal == 0:
            return 0
        else:
            int_value = np.clip(value, self.lower, self.upper)
            if self.log_scale:
                int_value = np.log(int_value)
            sz = len(self._uniform_int)
            return int(
                np.clip(
                    round((int_value - self._lower_internal) / self._step_internal),
                    0,
                    sz - 1,
                )
            )

    def cast(self, value):
        return self._values[self._map_to_int(value)]

    def set_sampler(self, sampler, allow_override=False):
        raise NotImplementedError()

    def get_sampler(self):
        return None

    def sample(self, spec=None, size=1, random_state=None):
        int_sample = self._uniform_int.sample(spec, size, random_state)
        if size > 1:
            return [self._values[x] for x in int_sample]
        else:
            return self._values[int_sample]

    @property
    def domain_str(self):
        return f"({self.lower}, {self.upper}, {self.__len__()})"

    def __len__(self):
        return len(self._uniform_int)

    def match_string(self, value) -> str:
        return str(self._map_to_int(value))

    def __eq__(self, other) -> bool:
        return (
            isinstance(other, FiniteRange)
            and np.isclose(self.lower, other.lower)
            and np.isclose(self.upper, other.upper)
            and self.log_scale == other.log_scale
            and self.cast_int == other.cast_int
        )


def sample_from(func: Callable[[Dict], Any]):
    """Specify that tune should sample configuration values from this function.

    Arguments:
        func: An callable function to draw a sample from.
    """
    return Function(func)


def uniform(lower: float, upper: float):
    """Sample a float value uniformly between ``lower`` and ``upper``.

    Sampling from ``tune.uniform(1, 10)`` is equivalent to sampling from
    ``np.random.uniform(1, 10))``

    """
    return Float(lower, upper).uniform()


def quniform(lower: float, upper: float, q: float):
    """Sample a quantized float value uniformly between ``lower`` and ``upper``.

    Sampling from ``tune.uniform(1, 10)`` is equivalent to sampling from
    ``np.random.uniform(1, 10))``

    The value will be quantized, i.e. rounded to an integer increment of ``q``.
    Quantization makes the upper bound inclusive.

    """
    return Float(lower, upper).uniform().quantized(q)


def loguniform(lower: float, upper: float):
    """Sugar for sampling in different orders of magnitude.

    Note: Ray Tune has an argument `base` here, but since this does not
    affect the distribution, we drop it.

    Args:
        lower (float): Lower boundary of the output interval (e.g. 1e-4)
        upper (float): Upper boundary of the output interval (e.g. 1e-2)

    """
    return Float(lower, upper).loguniform()


def reverseloguniform(lower: float, upper: float):
    """Values 0 <= x < 1, internally represented as -log(1 - x)

    Args:
        lower (float): Lower boundary of the output interval (e.g. 0.99)
        upper (float): Upper boundary of the output interval (e.g. 0.9999)

    """
    return Float(lower, upper).reverseloguniform()


def qloguniform(lower: float, upper: float, q: float):
    """Sugar for sampling in different orders of magnitude.

    The value will be quantized, i.e. rounded to an integer increment of ``q``.

    Quantization makes the upper bound inclusive.

    Args:
        lower (float): Lower boundary of the output interval (e.g. 1e-4)
        upper (float): Upper boundary of the output interval (e.g. 1e-2)
        q (float): Quantization number. The result will be rounded to an
            integer increment of this value.

    """
    return Float(lower, upper).loguniform().quantized(q)


def choice(categories: List):
    """Sample a categorical value.

    Sampling from ``tune.choice([1, 2])`` is equivalent to sampling from
    ``random.choice([1, 2])``

    """
    return Categorical(categories).uniform()


def ordinal(categories: List):
    """Sample an ordinal value."""
    return Ordinal(categories).uniform()


def randint(lower: int, upper: int):
    """Sample an integer value uniformly between ``lower`` and ``upper``.

    ``lower`` and ``upper`` are inclusive. This is a difference to Ray Tune,
    where ``upper`` is exclusive. However, both `lograndint` and `qrandint`
    have inclusive ``upper`` in Ray Tune, so we fix this inconsistency here.

    Sampling from ``tune.randint(10)`` is equivalent to sampling from
    ``np.random.randint(10 + 1)``.

    """
    return Integer(lower, upper).uniform()


def lograndint(lower: int, upper: int):
    """Sample an integer value log-uniformly between ``lower`` and ``upper``

    ``lower`` and ``upper` are inclusive.

    Note: Ray Tune has an argument `base` here, but since this does not
    affect the distribution, we drop it.
    """
    return Integer(lower, upper).loguniform()


def qrandint(lower: int, upper: int, q: int = 1):
    """Sample an integer value uniformly between ``lower`` and ``upper``.

    ``lower`` is inclusive, ``upper`` is also inclusive (!).

    The value will be quantized, i.e. rounded to an integer increment of ``q``.
    Quantization makes the upper bound inclusive.

    """
    return Integer(lower, upper).uniform().quantized(q)


def qlograndint(lower: int, upper: int, q: int):
    """Sample an integer value log-uniformly between ``lower`` and ``upper``

    ``lower`` is inclusive, ``upper`` is also inclusive (!).

    The value will be quantized, i.e. rounded to an integer increment of ``q``.
    Quantization makes the upper bound inclusive.

    """
    return Integer(lower, upper).loguniform().quantized(q)


def randn(mean: float = 0.0, sd: float = 1.0):
    """Sample a float value normally with ``mean`` and ``sd``.

    Args:
        mean (float): Mean of the normal distribution. Defaults to 0.
        sd (float): SD of the normal distribution. Defaults to 1.

    """
    return Float(None, None).normal(mean, sd)


def qrandn(mean: float, sd: float, q: float):
    """Sample a float value normally with ``mean`` and ``sd``.

    The value will be quantized, i.e. rounded to an integer increment of ``q``.

    Args:
        mean (float): Mean of the normal distribution.
        sd (float): SD of the normal distribution.
        q (float): Quantization number. The result will be rounded to an
            integer increment of this value.

    """
    return Float(None, None).normal(mean, sd).quantized(q)


def finrange(lower: float, upper: float, size: int, cast_int: bool = False):
    """
    Finite range `[lower, ..., upper]` with `size` entries, which are
    equi-spaced. Finite alternative to `uniform`.

    :param lower: Smallest feasible value
    :param upper: Largest feasible value
    :param size: Size of (finite) domain, must be >= 2
    :param cast_int: Values rounded to int?
    """
    return FiniteRange(lower, upper, size, log_scale=False, cast_int=cast_int)


def logfinrange(lower: float, upper: float, size: int, cast_int: bool = False):
    """
    Finite range `[lower, ..., upper]` with `size` entries, which are
    equi-spaced in the log domain. Finite alternative to `loguniform`.

    :param lower: Smallest feasible value (positive)
    :param upper: Largest feasible value (positive)
    :param size: Size of (finite) domain, must be >= 2
    :param cast_int: Values rounded to int?
    """
    return FiniteRange(lower, upper, size, log_scale=True, cast_int=cast_int)


def is_log_space(domain: Domain) -> bool:
    if isinstance(domain, FiniteRange):
        return domain.log_scale
    else:
        sampler = domain.get_sampler()
        return isinstance(sampler, Float._LogUniform) or isinstance(
            sampler, Integer._LogUniform
        )


def is_reverse_log_space(domain: Domain) -> bool:
    return isinstance(domain, Float) and isinstance(
        domain.get_sampler(), Float._ReverseLogUniform
    )


def is_uniform_space(domain: Domain) -> bool:
    if isinstance(domain, FiniteRange):
        return not domain.log_scale
    else:
        sampler = domain.get_sampler()
        return isinstance(sampler, Float._Uniform) or isinstance(
            sampler, Integer._Uniform
        )


def add_to_argparse(parser: argparse.ArgumentParser, config_space: Dict):
    """
    Use this to prepare argument parser in endpoint script, for the
    non-fixed parameters in `config_space`.

    :param parser:
    :param config_space:
    :return:
    """
    for name, domain in config_space.items():
        tp = domain.value_type if isinstance(domain, Domain) else type(domain)
        parser.add_argument(f"--{name}", type=tp, required=True)


def cast_config_values(config: Dict, config_space: Dict) -> Dict:
    """
    Returns config with keys, values of `config`, but values are casted to
    their specific types.

    :param config: Config whose values are to be casted
    :param config_space:
    :return: New config with values casted to correct types
    """
    return {
        name: domain.cast(config[name]) if isinstance(domain, Domain) else config[name]
        for name, domain in config_space.items()
        if name in config
    }


def non_constant_hyperparameter_keys(config_space: Dict) -> List[str]:
    """
    :param config_space:
    :return: Keys corresponding to (non-fixed) hyperparameters
    """
    return [name for name, domain in config_space.items() if isinstance(domain, Domain)]


def config_space_size(config_space: Dict, upper_limit: int = 2**20) -> Optional[int]:
    """
    Counts the number of distinct configurations in the configuration space
    `config_space`. If this is infinite (due to real-valued parameters) or
    larger than `upper_limit`, None is returned.
    """
    assert upper_limit > 1
    size = 1
    for name, domain in config_space.items():
        if isinstance(domain, Domain):
            domain_size = len(domain)
            if domain_size == 0 or domain_size > upper_limit:
                return None  # Try to avoid overflow
            size *= domain_size
            if size > upper_limit:
                return None
    return size


def config_to_match_string(config: Dict, config_space: Dict, keys: List[str]) -> str:
    """
    Maps configuration to a match string, which can be used to compare configs
    for (approximate) equality. Only keys in `keys` are used, in that ordering.

    :param config: Configuration to be encoded in match string
    :param config_space: Configuration space
    :param keys: Keys of parameters to be encoded
    :return: Match string
    """
    parts = []
    for key in keys:
        domain = config_space[key]
        value = config[key]
        parts.append(f"{key}:{domain.match_string(value)}")
    return ",".join(parts)


def to_dict(x: "Domain") -> Dict:
    """
    We assume that for each `Domain` subclass, the `__init__` kwargs are
    also members, and all other members start with `_`.

    """
    domain_kwargs = {
        k: v for k, v in x.__dict__.items() if k != "sampler" and not k.startswith("_")
    }
    result = {
        "domain_cls": x.__class__.__name__,
        "domain_kwargs": domain_kwargs,
    }
    sampler = x.get_sampler()
    if sampler is not None:
        result.update({"sampler_cls": str(sampler), "sampler_kwargs": sampler.__dict__})
    return result


def from_dict(d: Dict) -> Domain:
    domain_cls = getattr(sys.modules[__name__], d["domain_cls"])
    domain_kwargs = d["domain_kwargs"]
    domain = domain_cls(**domain_kwargs)
    if "sampler_cls" in d:
        sampler_cls = getattr(domain_cls, "_" + d["sampler_cls"])
        sampler_kwargs = d["sampler_kwargs"]
        sampler = sampler_cls(**sampler_kwargs)
        domain.set_sampler(sampler)
    return domain


def restrict_domain(numerical_domain: Domain, lower: float, upper: float) -> Domain:
    """
    Restricts a numerical domain to be in the range [lower, upper]
    :return:
    """
    assert hasattr(numerical_domain, "lower") and hasattr(numerical_domain, "upper")
    lower = numerical_domain.cast(lower)
    upper = numerical_domain.cast(upper)
    assert lower <= upper
    if not isinstance(numerical_domain, FiniteRange):
        # domain is numerical, set new lower and upper ranges with bounding-box values
        new_domain_dict = to_dict(numerical_domain)
        new_domain_dict["domain_kwargs"]["lower"] = lower
        new_domain_dict["domain_kwargs"]["upper"] = upper
        return from_dict(new_domain_dict)
    else:
        values = numerical_domain.values
        assert lower <= max(numerical_domain._values)
        assert upper >= min(numerical_domain._values)
        i = 0
        while values[i] < lower and i < len(values) - 1:
            i += 1
        new_lower = values[i]

        j = len(values) - 1
        while upper < values[j] and i < j:
            j -= 1
        new_upper = values[j]
        return FiniteRange(
            lower=new_lower,
            upper=new_upper,
            size=j - i + 1,
            cast_int=numerical_domain.cast_int,
            log_scale=numerical_domain.log_scale,
        )

File Path: syne_tune/constants.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Collects constants to be shared between core code and tuning scripts or
benchmarks.
"""

SYNE_TUNE_ENV_FOLDER = "SYNETUNE_FOLDER"  # environment variable that allows to overides default library folder
SYNE_TUNE_DEFAULT_FOLDER = "syne-tune"  # name of default library folder used if the env variable is not defined

ST_TUNER_CREATION_TIMESTAMP = "st_tuner_creation_timestamp"
ST_TUNER_START_TIMESTAMP = "st_tuner_start_timestamp"

# constants of keys that are written by `report`
ST_WORKER_ITER = "st_worker_iter"
ST_WORKER_TIMESTAMP = "st_worker_timestamp"
ST_WORKER_TIME = "st_worker_time"
ST_WORKER_COST = "st_worker_cost"
ST_INSTANCE_TYPE = "st_instance_type"
ST_INSTANCE_COUNT = "st_instance_count"

# constants for tuner results
ST_TRIAL_ID = "trial_id"
ST_TUNER_TIMESTAMP = "st_tuner_timestamp"
ST_TUNER_TIME = "st_tuner_time"
ST_DECISION = "st_decision"
ST_STATUS = "st_status"

# constant for the hyperparameter name that contains the checkpoint directory
ST_CHECKPOINT_DIR = "st_checkpoint_dir"

# Name for `upload_dir` in `RemoteTuner`
ST_REMOTE_UPLOAD_DIR_NAME = "tuner"

File Path: syne_tune/experiments.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import json
import logging
from datetime import datetime
from json.decoder import JSONDecodeError
from typing import List, Dict, Callable, Optional
import pandas as pd
from dataclasses import dataclass

from syne_tune.constants import ST_TUNER_TIME, ST_TUNER_CREATION_TIMESTAMP
from syne_tune import Tuner
from syne_tune.util import experiment_path, s3_experiment_path
from syne_tune.try_import import try_import_aws_message

try:
    import boto3
    from botocore.exceptions import ClientError
except ImportError:
    print(try_import_aws_message())


@dataclass
class ExperimentResult:
    name: str
    results: pd.DataFrame
    metadata: Dict
    tuner: Tuner

    def __str__(self):
        res = f"Experiment {self.name}"
        if self.results is not None:
            res += f" contains {len(self.results)} evaluations from {len(self.results.trial_id.unique())} trials"
        return res

    def creation_date(self):
        return datetime.fromtimestamp(self.metadata[ST_TUNER_CREATION_TIMESTAMP])

    def plot(self, **plt_kwargs):
        import matplotlib.pyplot as plt

        metric = self.metric_names()[0]
        df = self.results
        if df is not None and len(df) > 0:
            df = df.sort_values(ST_TUNER_TIME)
            x = df.loc[:, ST_TUNER_TIME]
            y = (
                df.loc[:, metric].cummax()
                if self.metric_mode() == "max"
                else df.loc[:, metric].cummin()
            )
            plt.plot(x, y, **plt_kwargs)
            plt.xlabel("wallclock time")
            plt.ylabel(metric)
            plt.title(f"Best result over time {self.name}")
            plt.legend()
            plt.show()

    def metric_mode(self) -> str:
        return self.metadata["metric_mode"]

    def metric_names(self) -> List[str]:
        return self.metadata["metric_names"]

    def entrypoint_name(self) -> str:
        return self.metadata["entrypoint"]

    def best_config(self) -> Dict:
        """
        Return the best config found for the first metric defined in the scheduler.
        :param self:
        :return:
        """
        metric_names = self.metric_names()
        metric_mode = self.metric_mode()

        if len(metric_names) > 1:
            logging.warning(
                "Several metrics exists so the best is not defined, this will return the best other the"
                f"first metric {metric_names}."
            )
        metric_name = metric_names[0]

        # locate best result
        if metric_mode == "min":
            best_index = self.results.loc[:, metric_name].argmin()
        else:
            best_index = self.results.loc[:, metric_name].argmax()
        res = dict(self.results.loc[best_index])

        # dont include internal fields
        return {k: v for k, v in res.items() if not k.startswith("st_")}


def download_single_experiment(
    tuner_name: str,
    s3_bucket: Optional[str] = None,
    experiment_name: Optional[str] = None,
):
    """
    Downloads results from s3 of a tuning experiment previously run with remote launcher.
    :param tuner_name: named of the tuner to be retrieved.
    :param s3_bucket: If not given, the default bucket for the SageMaker session is used
    :param experiment_name: If given, this is used as first directory.
    :return:
    """
    s3_path = s3_experiment_path(
        s3_bucket=s3_bucket, tuner_name=tuner_name, experiment_name=experiment_name
    )
    tgt_dir = experiment_path(tuner_name=tuner_name)
    tgt_dir.mkdir(exist_ok=True, parents=True)
    s3 = boto3.client("s3")
    parts_path = s3_path.replace("s3://", "").split("/")
    s3_bucket = parts_path[0]
    s3_key = "/".join(parts_path[1:])
    for file in ["metadata.json", "results.csv.zip", "tuner.dill"]:
        try:
            logging.info(f"downloading {file} on {s3_path}")
            s3.download_file(s3_bucket, f"{s3_key}/{file}", str(tgt_dir / file))
        except ClientError as e:
            logging.info(f"could not find {file} on {s3_path}")


def load_experiment(
    tuner_name: str,
    download_if_not_found: bool = True,
    load_tuner: bool = False,
    local_path: Optional[str] = None,
) -> ExperimentResult:
    """
    :param tuner_name: name of a tuning experiment previously run
    :param download_if_not_found: whether to fetch the experiment from s3 if not found locally
    :param load_tuner: whether to load the tuner in addition to metadata and results
    :param local_path: path containing the experiment to load if not specified, then `~/{SYNE_TUNE_FOLDER}/` is used.
    :return:
    """
    path = experiment_path(tuner_name, local_path)

    metadata_path = path / "metadata.json"
    if not (metadata_path.exists()) and download_if_not_found:
        logging.info(
            f"experiment {tuner_name} not found locally, trying to get it from s3."
        )
        download_single_experiment(tuner_name=tuner_name)
    try:
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
    except FileNotFoundError:
        metadata = None
    try:
        if (path / "results.csv.zip").exists():
            results = pd.read_csv(path / "results.csv.zip")
        else:
            results = pd.read_csv(path / "results.csv")
    except Exception:
        results = None
    if load_tuner:
        try:
            tuner = Tuner.load(path)
        except FileNotFoundError:
            tuner = None
        except Exception:
            tuner = None
    else:
        tuner = None
    return ExperimentResult(
        name=tuner.name if tuner is not None else path.stem,
        results=results,
        tuner=tuner,
        metadata=metadata,
    )


def get_metadata(
    name_filter: Callable[[str], bool] = None, root=experiment_path()
) -> Dict[str, Dict]:
    """
    :param name_filter: if passed then only experiments whose path matching the filter are kept. This allows
    rapid filtering in the presence of many experiments.
    :return: dictionary from tuner name to metadata dict
    """
    res = {}
    for metadata_path in root.glob("**/metadata.json"):
        path = metadata_path.parent
        if name_filter is None or name_filter(str(path)):
            try:
                tuner_name = path.name
                with open(metadata_path, "r") as f:
                    metadata = json.load(f)
                    # we check that the metadata is valid by verifying that is a dict containing Syne Tune time-stamp
                    if (
                        isinstance(metadata, Dict)
                        and ST_TUNER_CREATION_TIMESTAMP in metadata
                    ):
                        metadata["path"] = str(path.parent)
                        res[tuner_name] = metadata
            except JSONDecodeError as e:
                print(f"could not read {path}")
                pass
    return res


def list_experiments(
    path_filter: Callable[[str], bool] = None,
    experiment_filter: Callable[[ExperimentResult], bool] = None,
    load_tuner: bool = False,
) -> List[ExperimentResult]:
    res = []
    for metadata_path in experiment_path().glob("**/metadata.json"):
        path = metadata_path.parent
        tuner_name = path.name
        if path_filter is None or path_filter(metadata_path):
            exp = load_experiment(tuner_name, load_tuner, local_path=path.parent)
            if experiment_filter is None or experiment_filter(exp):
                if exp.results is not None and exp.metadata is not None:
                    res.append(exp)
    return sorted(
        res,
        key=lambda exp: exp.metadata.get(ST_TUNER_CREATION_TIMESTAMP, 0),
        reverse=True,
    )


def load_experiments_df(
    path_filter: Callable[[str], bool] = None,
    experiment_filter: Callable[[ExperimentResult], bool] = None,
    load_tuner: bool = False,
) -> pd.DataFrame:
    """
    :param: name_filter: if specified, only experiment whose path name matches the filter will be kept.
    :param experiment_filter: only experiment where the filter is True are kept, default to None and returns everything.
    :return: a dataframe that contains all evaluations reported by tuners according to the filter given.
    The columns contains trial-id, hyperparameter evaluated, metrics observed by `report`:
     metrics collected automatically by syne-tune:
     `st_worker_time` (indicating time spent in the worker when report was seen)
     `time` (indicating wallclock time measured by the tuner)
     `decision` decision taken by the scheduler when observing the result
     `status` status of the trial that was shown to the tuner
     `config_{xx}` configuration value for the hyperparameter {xx}
     `tuner_name` named passed when instantiating the Tuner
     `entry_point_name`/`entry_point_path` name and path of the entry point that was tuned
    """
    dfs = []
    for experiment in list_experiments(
        path_filter=path_filter,
        experiment_filter=experiment_filter,
        load_tuner=load_tuner,
    ):
        assert experiment.results is not None
        assert experiment.metadata is not None

        df = experiment.results
        df["tuner_name"] = experiment.name
        for k, v in experiment.metadata.items():
            if isinstance(v, List):
                if len(v) > 1:
                    for i, x in enumerate(v):
                        df[f"{k}-{i}"] = x
                else:
                    df[k] = v[0]
            else:
                df[k] = v
        dfs.append(df)
    return pd.concat(dfs, ignore_index=True)


if __name__ == "__main__":
    for exp in list_experiments():
        if exp.results is not None:
            print(exp)

File Path: syne_tune/num_gpu.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Adapted from to not run in Shell mode which is unsecure.
https://github.com/aws/sagemaker-rl-container/blob/master/src/vw-serving/src/vw_serving/sagemaker/gpu.py
"""

import logging
import subprocess
import time

_num_gpus = None


def get_num_gpus() -> int:
    """
    Returns the number of available GPUs based on configuration parameters and available hardware GPU devices.
    Gpus are detected by running "nvidia-smi --list-gpus" as a subprocess.
    :return: (int) number of GPUs
    """
    global _num_gpus
    if _num_gpus is None:
        try:
            cmd = "nvidia-smi --list-gpus"
            with open("std.out", "w") as stdout:
                proc = subprocess.Popen(cmd.split(" "), shell=False, stdout=stdout)
            max_trials = 0
            while proc.poll() is None and max_trials < 100:
                time.sleep(0.1)
                max_trials += 1

            if proc.poll() is None:
                raise ValueError("nvidia-smi timed out after 10 secs.")

            with open("std.out", "r") as stdout:
                _num_gpus = len(stdout.readlines())
            return _num_gpus

        except (OSError, FileNotFoundError):
            logging.info(
                "Error launching /usr/bin/nvidia-smi, no GPU could be detected."
            )
            _num_gpus = 0
            return 0
    else:
        return _num_gpus

File Path: syne_tune/optimizer/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/baselines.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional
import numpy as np
import logging

from syne_tune.optimizer.schedulers import (
    FIFOScheduler,
    HyperbandScheduler,
    PopulationBasedTraining,
)
from syne_tune.optimizer.schedulers.multiobjective import MOASHA
from syne_tune.optimizer.schedulers.searchers.regularized_evolution import (
    RegularizedEvolution,
)
from syne_tune.optimizer.schedulers.synchronous import (
    SynchronousGeometricHyperbandScheduler,
    GeometricDifferentialEvolutionHyperbandScheduler,
)
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
)
from syne_tune.try_import import (
    try_import_blackbox_repository_message,
    try_import_bore_message,
)


class RandomSearch(FIFOScheduler):
    def __init__(self, config_space: Dict, metric: str, **kwargs):
        super(RandomSearch, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="random",
            **kwargs,
        )


class GridSearch(FIFOScheduler):
    def __init__(self, config_space: Dict, metric: str, **kwargs):
        super(GridSearch, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="grid",
            **kwargs,
        )


class BayesianOptimization(FIFOScheduler):
    def __init__(self, config_space: Dict, metric: str, **kwargs):
        super(BayesianOptimization, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="bayesopt",
            **kwargs,
        )


def _assert_max_resource_args(kwargs: dict, name: str = "max_t"):
    need_one = {name, "max_resource_attr"}
    assert need_one.intersection(kwargs.keys()), f"Need one of these: {need_one}"


class ASHA(HyperbandScheduler):
    def __init__(self, config_space: Dict, metric: str, resource_attr: str, **kwargs):
        """
        One of `max_t`, `max_resource_attr` needs to be in `kwargs`. For
        `type='promotion'`, the latter is more useful, see also
        :class:`HyperbandScheduler`.

        """
        _assert_max_resource_args(kwargs)
        super(ASHA, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="random",
            resource_attr=resource_attr,
            **kwargs,
        )


class MOBSTER(HyperbandScheduler):
    def __init__(self, config_space: Dict, metric: str, resource_attr: str, **kwargs):
        """
        One of `max_t`, `max_resource_attr` needs to be in `kwargs`. For
        `type='promotion'`, the latter is more useful, see also
        :class:`HyperbandScheduler`.

        """
        _assert_max_resource_args(kwargs)
        super(MOBSTER, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="bayesopt",
            resource_attr=resource_attr,
            **kwargs,
        )


class PASHA(HyperbandScheduler):
    def __init__(self, config_space: Dict, metric: str, resource_attr: str, **kwargs):
        """
        One of `max_t`, `max_resource_attr` needs to be in `kwargs`. The
        latter is more useful, see also :class:`HyperbandScheduler`.

        """
        _assert_max_resource_args(kwargs)
        super(PASHA, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="random",
            resource_attr=resource_attr,
            type="pasha",
            **kwargs,
        )


class SyncHyperband(SynchronousGeometricHyperbandScheduler):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        resource_attr: str,
        **kwargs,
    ):
        """
        One of `max_resource_level`, `max_resource_attr` needs to be in
        `kwargs`. The latter is more useful, see also
        :class:`HyperbandScheduler`.

        """
        _assert_max_resource_args(kwargs, name="max_resource_level")
        super(SyncHyperband, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="random",
            resource_attr=resource_attr,
            **kwargs,
        )


class SyncBOHB(SynchronousGeometricHyperbandScheduler):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        resource_attr: str,
        **kwargs,
    ):
        """
        One of `max_resource_level`, `max_resource_attr` needs to be in
        `kwargs`. The latter is more useful, see also
        :class:`HyperbandScheduler`.

        """
        _assert_max_resource_args(kwargs, name="max_resource_level")
        super(SyncBOHB, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="kde",
            resource_attr=resource_attr,
            **kwargs,
        )


class DEHB(GeometricDifferentialEvolutionHyperbandScheduler):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        resource_attr: str,
        **kwargs,
    ):
        """
        One of `max_resource_level`, `max_resource_attr` needs to be in
        `kwargs`. The latter is more useful, see also
        :class:`HyperbandScheduler`.

        """
        _assert_max_resource_args(kwargs, name="max_resource_level")
        super(DEHB, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="random",
            resource_attr=resource_attr,
            **kwargs,
        )


class SyncMOBSTER(SynchronousGeometricHyperbandScheduler):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        resource_attr: str,
        **kwargs,
    ):
        """
        One of `max_resource_level`, `max_resource_attr` needs to be in
        `kwargs`. The latter is more useful, see also
        :class:`HyperbandScheduler`.

        The default surrogate model is "gp_independent", different to async
        MOBSTER.

        """
        _assert_max_resource_args(kwargs, name="max_resource_level")
        search_options = kwargs.get("search_options", dict())
        if "model" not in search_options:
            search_options["model"] = "gp_independent"
        kwargs["search_options"] = search_options
        super(SyncMOBSTER, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="bayesopt",
            resource_attr=resource_attr,
            **kwargs,
        )


class BORE(FIFOScheduler):
    def __init__(self, config_space: Dict, metric: str, mode: str, **kwargs):
        try:
            from syne_tune.optimizer.schedulers.searchers.bore import Bore
        except ImportError:
            logging.info(try_import_bore_message())
            raise

        super(BORE, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher=Bore(config_space=config_space, metric=metric, mode=mode),
            mode=mode,
            **kwargs,
        )


class REA(FIFOScheduler):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        population_size: int = 100,
        sample_size: int = 10,
        **kwargs,
    ):
        super(REA, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher=RegularizedEvolution(
                config_space=config_space,
                metric=metric,
                population_size=population_size,
                sample_size=sample_size,
                **kwargs,
            ),
            **kwargs,
        )


class ConstrainedBayesianOptimization(FIFOScheduler):
    def __init__(self, config_space: Dict, metric: str, constraint_attr: str, **kwargs):
        search_options = kwargs.get("search_options", dict())
        kwargs["search_options"] = dict(search_options, constraint_attr=constraint_attr)
        super(ConstrainedBayesianOptimization, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher="bayesopt_constrained",
            **kwargs,
        )


class ZeroShotTransfer(FIFOScheduler):
    def __init__(
        self,
        config_space: Dict,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        metric: str,
        mode: str = "min",
        sort_transfer_learning_evaluations: bool = True,
        use_surrogates: bool = False,
        random_seed: Optional[int] = None,
        **kwargs,
    ):
        """
        A zero-shot transfer hyperparameter optimization method which jointly selects configurations that minimize the
        average rank obtained on historic metadata (transfer_learning_evaluations).

        Reference: Sequential Model-Free Hyperparameter Tuning.
        Martin Wistuba, Nicolas Schilling, Lars Schmidt-Thieme.
        IEEE International Conference on Data Mining (ICDM) 2015.

        :param config_space: Configuration space for trial evaluation function.
        :param transfer_learning_evaluations: Dictionary from task name to offline evaluations.
        :param metric: Objective name to optimize, must be present in transfer learning evaluations.
        :param mode: Whether to minimize (min) or maximize (max)
        :param sort_transfer_learning_evaluations: Use False if the hyperparameters for each task in
        transfer_learning_evaluations Are already in the same order. If set to True, hyperparameters are sorted.
        :param use_surrogates: If the same configuration is not evaluated on all tasks, set this to true. This will
        generate a set of configurations and will impute their performance using surrogate models.
        :param random_seed: Used for randomly sampling candidates. Only used if use_surrogate is True.
        """
        try:
            from syne_tune.optimizer.schedulers.transfer_learning import zero_shot
        except ImportError:
            logging.info(try_import_blackbox_repository_message())
            raise

        super(ZeroShotTransfer, self).__init__(
            config_space=config_space,
            metric=metric,
            searcher=zero_shot.ZeroShotTransfer(
                config_space=config_space,
                metric=metric,
                mode=mode,
                sort_transfer_learning_evaluations=sort_transfer_learning_evaluations,
                random_seed=random_seed,
                transfer_learning_evaluations=transfer_learning_evaluations,
                use_surrogates=use_surrogates,
            ),
            mode=mode,
            **kwargs,
        )


class ASHACTS(HyperbandScheduler):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        resource_attr: str,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        mode: str = "min",
        random_seed: Optional[int] = None,
        **kwargs,
    ):
        """
        Runs ASHA where the searcher is done with the transfer-learning method:
        A Quantile-based Approach for Hyperparameter Transfer Learning.
        David Salinas, Huibin Shen, Valerio Perrone. ICML 2020.
        This is the Copula Thompson Sampling approach described in the paper where a surrogate is fitted on the
        transfer learning data to predict mean/variance of configuration performance given a hyperparameter.
        The surrogate is then sampled from and the best configurations are returned as next candidate to evaluate.
        :param config_space:
        :param metric:
        :param resource_attr:
        :param transfer_learning_evaluations:
        :param mode:
        :param random_seed:
        :param kwargs:
        """
        try:
            from syne_tune.optimizer.schedulers.transfer_learning.quantile_based.quantile_based_searcher import (
                QuantileBasedSurrogateSearcher,
            )
        except ImportError:
            logging.info(try_import_blackbox_repository_message())
            raise

        super(ASHACTS, self).__init__(
            config_space=config_space,
            searcher=QuantileBasedSurrogateSearcher(
                mode=mode,
                config_space=config_space,
                metric=metric,
                transfer_learning_evaluations=transfer_learning_evaluations,
                random_seed=random_seed
                if random_seed
                else np.random.randint(0, 2**32),
            ),
            mode=mode,
            metric=metric,
            resource_attr=resource_attr,
            **kwargs,
        )


# dictionary that allows to also list baselines who don't need a wrapper class such as PBT.
baselines_dict = {
    "Random Search": RandomSearch,
    "Grid Search": GridSearch,
    "Bayesian Optimization": BayesianOptimization,
    "ASHA": ASHA,
    "MOBSTER": MOBSTER,
    "PASHA": PASHA,
    "MOASHA": MOASHA,
    "PBT": PopulationBasedTraining,
    "BORE": BORE,
    "REA": REA,
    "SyncHyperband": SyncHyperband,
    "SyncBOHB": SyncBOHB,
    "DEHB": DEHB,
    "SyncMOBSTER": SyncMOBSTER,
    "ConstrainedBayesianOptimization": ConstrainedBayesianOptimization,
    "ZeroShotTransfer": ZeroShotTransfer,
    "ASHACTS": ASHACTS,
}

File Path: syne_tune/optimizer/scheduler.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from typing import Optional, Dict, List
import logging

from syne_tune.backend.trial_status import Trial
from syne_tune.config_space import non_constant_hyperparameter_keys, cast_config_values

logger = logging.getLogger(__name__)


class SchedulerDecision:
    CONTINUE = "CONTINUE"  #: Status for continuing trial execution
    PAUSE = "PAUSE"  #: Status for pausing trial execution
    STOP = "STOP"  #: Status for stopping trial execution


@dataclass
class TrialSuggestion:
    """Suggestion returned by a scheduler.
    :param spawn_new_trial_id: whether a new trial-id should be used.
    :param checkpoint_trial_id: the checkpoint of the trial-id that should be used.
    If `spawn_new_trial_id` is False, then the trial `checkpoint_trial_id` is resumed with its previous checkpoint.
    :param config: the configuration that should be evaluated.
    """

    spawn_new_trial_id: bool = True
    checkpoint_trial_id: Optional[int] = None
    config: Optional[Dict] = None

    def __post_init__(self):
        if self.spawn_new_trial_id:
            assert (
                self.checkpoint_trial_id is not None or self.config is not None
            ), "Cannot start a new trial without specifying a checkpoint or a config."
        else:
            assert (
                self.checkpoint_trial_id is not None
            ), "A trial-id must be passed to resume a trial."

    @staticmethod
    def start_suggestion(
        config: Dict, checkpoint_trial_id: Optional[int] = None
    ) -> "TrialSuggestion":
        """
        :param config: configuration to use for the new trial.
        :param checkpoint_trial_id: if given, then the checkpoint folder of the corresponding trial is used when
        starting the new trial.
        :return: a trial decision that consists in starting a new trial (which would receive a new trial-id).
        """
        return TrialSuggestion(
            spawn_new_trial_id=True,
            config=config,
            checkpoint_trial_id=checkpoint_trial_id,
        )

    @staticmethod
    def resume_suggestion(
        trial_id: int, config: Optional[Dict] = None
    ) -> "TrialSuggestion":
        """
        :param trial_id:
        :param config:
        :return: a trial decision that consists in resuming trial `trial-id` with `config` if provided or the previous
        configuration used if not provided.
        """
        return TrialSuggestion(
            spawn_new_trial_id=False,
            config=config,
            checkpoint_trial_id=trial_id,
        )

    def __str__(self):
        res = f"config {self.config}"
        if self.checkpoint_trial_id is not None:
            res += f" using from trial's checkpoint {self.checkpoint_trial_id}"
        return res


class TrialScheduler:
    """
    Schedulers maintain and drive the logic of an experiment, making decisions
    which configs to evaluate in new trials, and which trials to stop early.

    Some schedulers support pausing and resuming trials. In this case, they
    also drive the decision when to restart a paused trial.

    Note that Ray Tune distributes these decisions between schedulers and
    searchers (see :class:`RayTuneScheduler`).
    """

    def __init__(self, config_space: Dict):
        self.config_space = config_space
        self._hyperparameter_keys = set(non_constant_hyperparameter_keys(config_space))

    def suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        """Returns a suggestion for a new trial, or one to be resumed

        This method returns `suggestion` of type `TrialSuggestion` (unless
        there is no config left to explore, and None is returned).

        If `suggestion.spawn_new_trial_id` is True, a new trial is to be
        started with config `suggestion.config`. Typically, this new trial
        is started from scratch. But if `suggestion.checkpoint_trial_id` is
        given, the trial is to be (warm)started from the checkpoint written
        for the trial with this ID. The new trial has ID `trial_id`.

        If `suggestion.spawn_new_trial_id` is False, an existing and currently
        paused trial is to be resumed, whose ID is
        `suggestion.checkpoint_trial_id`. If this trial has a checkpoint, we
        start from there. In this case, `suggestion.config` is optional. If not
        given (default), the config of the resumed trial does not change.
        Otherwise, its config is overwritten by `suggestion.config` (see
        :class:`HyperbandScheduler` with type 'promotion' for an example why
        this can be useful).

        Apart from the HP config, additional fields can be appended to the
        dict, these are passed to the trial function as well.

        :param trial_id: ID for new trial to be started (ignored if existing
            trial to be resumed)
        :return: Suggestion for a trial to be started or to be resumed, see
            above
        """
        ret_val = self._suggest(trial_id)
        if ret_val is not None:
            assert isinstance(ret_val, TrialSuggestion)
            if ret_val.config is not None:
                ret_val = TrialSuggestion(
                    spawn_new_trial_id=ret_val.spawn_new_trial_id,
                    checkpoint_trial_id=ret_val.checkpoint_trial_id,
                    config=self._postprocess_config(ret_val.config),
                )
        return ret_val

    def _postprocess_config(self, config: Dict) -> Dict:
        """
        Post-processes a config as returned by a searcher. This involves:
        - Adding parameters which are constant, therefore do not feature
            in the config space of the searcher
        - Casting values to types (float, int, str) according to config_space
            value types

        :param config: Config returned by searcher
        :return: Post-processed config
        """
        new_config = self.config_space.copy()
        new_config.update(cast_config_values(config, config_space=self.config_space))
        return new_config

    def _preprocess_config(self, config: Dict) -> Dict:
        """
        Pre-processes a config before passing it to a searcher. This involves:
        - Removing parameters which are constant in the config space (these do
            not feature in the config space used by the searcher)
        - Casting values to types (float, int, str) according to config_space
            value types

        :param config:
        :return: Pre-processed config, can be passed to searcher
        """
        return cast_config_values(
            {k: v for k, v in config.items() if k in self._hyperparameter_keys},
            config_space=self.config_space,
        )

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        """
        Implements `suggest`, except for basic postprocessing of config.
        Note that the config returned here may also contain values for constant
        parameters in the config space. If so, these values take precedence.
        See :class:`HyperbandScheduler` with `type = 'promotion'` for an
        example how this is used.
        """
        raise NotImplementedError()

    def on_trial_add(self, trial: Trial):
        """Called when a new trial is added to the trial runner.

        Additions are normally triggered by `suggest`.
        """
        pass

    def on_trial_error(self, trial: Trial):
        """Notification for the error of trial."""
        pass

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        """Called on each intermediate result returned by a trial.

        At this point, the trial scheduler can make a decision by returning
        one of CONTINUE, PAUSE, and STOP. This will only be called when the
        trial is in the RUNNING state.

        :param trial:
        :param result:
        :return: trial_decision
        """
        return SchedulerDecision.CONTINUE

    def on_trial_complete(self, trial: Trial, result: Dict):
        """Notification for the completion of trial."""
        pass

    def on_trial_remove(self, trial: Trial):
        """Called to remove trial.
        This is called when the trial is in PAUSED or PENDING state. Otherwise,
        call `on_trial_complete`."""
        pass

    def metric_names(self) -> List[str]:
        """
        :return: List of metric names. The first one is the target
            metric optimized over
        """
        raise NotImplementedError()

    def metric_mode(self) -> str:
        """
        :return: 'min' if target metric is minimized, otherwise 'max', 'min' is the default in all schedulers.
        """
        if hasattr(self, "mode"):
            return self.mode
        else:
            raise NotImplementedError()

File Path: syne_tune/optimizer/schedulers/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

__all__ = [
    "FIFOScheduler",
    "HyperbandScheduler",
    "MedianStoppingRule",
    "PopulationBasedTraining",
]

import logging

from syne_tune.try_import import try_import_raytune_message
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.median_stopping_rule import MedianStoppingRule
from syne_tune.optimizer.schedulers.pbt import PopulationBasedTraining

try:
    from syne_tune.optimizer.schedulers.ray_scheduler import (  # noqa: F401
        RayTuneScheduler,
    )

    __all__.append("RayTuneScheduler")
except ImportError:
    logging.info(try_import_raytune_message())

File Path: syne_tune/optimizer/schedulers/botorch/botorch_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, List
import logging

import numpy as np
import torch
from botorch.models import SingleTaskGP
from botorch.fit import fit_gpytorch_model
from botorch.models.transforms import Warp
from botorch.utils import standardize
from botorch.utils.transforms import normalize
from gpytorch.mlls import ExactMarginalLogLikelihood
from botorch.acquisition import qExpectedImprovement
from botorch.optim import optimize_acqf
from gpytorch.utils.errors import NotPSDError

import syne_tune.config_space as cs
from syne_tune.optimizer.schedulers.searchers import SearcherWithRandomSeed

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)


logger = logging.getLogger(__name__)


NOISE_LEVEL = 1e-3


class BotorchSearcher(SearcherWithRandomSeed):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        num_init_random_draws: int = 3,
        mode: str = "min",
        points_to_evaluate: Optional[List[Dict]] = None,
        fantasising: bool = True,
        max_num_observations: Optional[int] = 200,
        input_warping: bool = True,
        **kwargs,
    ):
        """
        A searcher that suggest configurations using BOTORCH to build GP surrogate and optimize acquisition function.
        `qExpectedImprovement is used for the acquisition function given that it supports pending evaluations.
        :param config_space: configuration space to optimize
        :param metric: metric to optimize, should be present in reported results.
        :param num_init_random_draws: number of initial random draws, after this number the suggestion are obtained
        from the GP surrogate model.
        :param mode: 'min' or 'max'
        :param points_to_evaluate: if passed, those configurations are evaluated first
        :param fantasising: whether to fantasize pending evaluations by sampling from the GP posterior
        :param max_num_observations: maximum number of observation to use when fitting the GP, if the number of
        observations gets larger than this number, then data is subsampled. If None, then all data is used to fit the GP.
        :param input_warping: whether to apply input warping when fitting the GP.
        :param kwargs: additional arguments of SearcherWithRandomSeed
        """
        super(BotorchSearcher, self).__init__(
            config_space, metric, points_to_evaluate=points_to_evaluate, **kwargs
        )
        assert num_init_random_draws >= 2
        assert mode in ["min", "max"]
        self.hp_ranges = make_hyperparameter_ranges(config_space=config_space)
        self.mode = mode
        self.metric_name = metric
        self.num_minimum_observations = num_init_random_draws
        self.points_to_evaluate = points_to_evaluate
        self.config_seen = set()
        self.fantasising = fantasising
        self.max_num_observations = max_num_observations
        self.input_warping = input_warping
        self.trial_configs = {}
        self.pending_trials = set()
        self.trial_observations = {}

    def _update(self, trial_id: str, config: Dict, result: Dict):
        trial_id = int(trial_id)
        self.trial_observations[trial_id] = result[self.metric_name]
        self.pending_trials.remove(trial_id)

    def clone_from_state(self, state):
        pass

    def num_suggestions(self):
        return len(self.trial_configs)

    def get_config(self, trial_id: str, **kwargs):
        trial_id = int(trial_id)
        config_suggested = self._next_initial_config()

        if config_suggested is None:
            if self.points_to_evaluate is not None and self.num_suggestions() < len(
                self.points_to_evaluate
            ):
                # if we are not done yet with points_to_evaluate, we pick the next one from this list
                config_suggested = self.points_to_evaluate[self.num_suggestions()]
            else:
                not_enough_suggestion = (
                    len(self.objectives()) < self.num_minimum_observations
                )
                if not_enough_suggestion:
                    config_suggested = self._sample_random()
                else:
                    config_suggested = self._sample_next_candidate()

        self.pending_trials.add(trial_id)
        self.trial_configs[trial_id] = config_suggested
        self.config_seen.add(tuple(config_suggested.values()))

        return config_suggested

    def _sample_next_candidate(self) -> Dict:
        """
        :return: a next candidate to evaluate, if possible it is obtained by fitting a GP on past data and maximizing EI
        if this fails because of numerical difficulties with non PSD matrices, then the candidate is sampled at random.
        """
        try:
            X = np.array(self._config_to_feature_matrix(self._configs_with_results()))
            y = self.objectives()

            if (
                self.max_num_observations is not None
                and len(X) >= self.max_num_observations
            ):
                perm = self.random_state.permutation(len(X))[
                    : self.max_num_observations
                ]
                X = X[perm]
                y = y[perm]
                subsample = True
            else:
                subsample = False

            X_tensor = torch.Tensor(X)
            Y_tensor = standardize(torch.Tensor(y).reshape(-1, 1))
            gp = self._make_gp(X_tensor=X_tensor, Y_tensor=Y_tensor)
            mll = ExactMarginalLogLikelihood(gp.likelihood, gp)
            fit_gpytorch_model(mll, max_retries=0)

            if self.pending_trials and self.fantasising and not subsample:
                X_pending = self._config_to_feature_matrix(self._configs_pending())
            else:
                X_pending = None

            acq = qExpectedImprovement(
                model=gp,
                best_f=Y_tensor.min().item()
                if self.mode == "min"
                else Y_tensor.max().item(),
                maximize=self.mode == "max",
                X_pending=X_pending,
            )

            candidate, acq_value = optimize_acqf(
                acq,
                bounds=torch.Tensor(self.hp_ranges.get_ndarray_bounds()).T,
                q=1,
                num_restarts=3,
                raw_samples=100,
            )

            candidate = candidate.detach().numpy()[0]
            config = self.hp_ranges.from_ndarray(candidate)
            if not self._is_config_already_seen(config):
                return config
            else:
                logger.warning(
                    "Optimization of the acquisition function yielded a config that was already seen."
                )
                return self._sample_and_pick_acq_best(acq)
        except NotPSDError as _:
            logging.warning("Chlolesky inversion failed, sampling randomly.")
            return self._sample_random()

    def _make_gp(self, X_tensor: torch.Tensor, Y_tensor: torch.Tensor) -> SingleTaskGP:
        double_precision = False
        if double_precision:
            X_tensor = X_tensor.double()
            Y_tensor = Y_tensor.double()

        noise_std = NOISE_LEVEL
        Y_tensor += noise_std * torch.randn_like(Y_tensor)

        if self.input_warping:
            warp_tf = Warp(indices=list(range(X_tensor.shape[-1])))
        else:
            warp_tf = None
        return SingleTaskGP(X_tensor, Y_tensor, input_transform=warp_tf)

    def _config_to_feature_matrix(self, configs: List[Dict]) -> torch.Tensor:
        bounds = torch.Tensor(self.hp_ranges.get_ndarray_bounds()).T
        X = torch.Tensor([self.hp_ranges.to_ndarray(config) for config in configs])
        return normalize(X, bounds)

    def objectives(self):
        return np.array(list(self.trial_observations.values()))

    def _sample_and_pick_acq_best(self, acq, num_samples: int = 100) -> Dict:
        """
        :param acq:
        :param num_samples:
        :return: Samples `num_samples` candidates and return the one maximizing the acquisitition function `acq` that
        was not seen earlier, if all samples were seen, return a random sample instead.
        """
        configs_candidates = [self._sample_random() for _ in range(num_samples)]
        configs_candidates = [
            x for x in configs_candidates if not self._is_config_already_seen(x)
        ]
        logger.debug(f"Sampling among {len(configs_candidates)} unseen configs")
        if configs_candidates:
            X_tensor = self._config_to_feature_matrix(configs_candidates)
            ei = acq(X_tensor.unsqueeze(dim=-2))
            return configs_candidates[ei.argmax()]
        else:
            return self._sample_random()

    def _is_config_already_seen(self, config) -> bool:
        return tuple(config.values()) in self.config_seen

    def _sample_random(self) -> Dict:
        return {
            k: v.sample(random_state=self.random_state)
            if isinstance(v, cs.Domain)
            else v
            for k, v in self.config_space.items()
        }

    def _configs_with_results(self) -> List[Dict]:
        return [
            config
            for trial, config in self.trial_configs.items()
            if not trial in self.pending_trials
        ]

    def _configs_pending(self) -> List[Dict]:
        return [
            config
            for trial, config in self.trial_configs.items()
            if trial in self.pending_trials
        ]

    def metric_names(self) -> List[str]:
        return [self.metric_name]

    def metric_mode(self) -> str:
        return self.mode

File Path: syne_tune/optimizer/schedulers/fifo.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, List
import logging
import os
import numpy as np

from syne_tune.optimizer.schedulers.searchers.searcher import BaseSearcher
from syne_tune.optimizer.schedulers.searchers.searcher_factory import searcher_factory
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
    Categorical,
    String,
    Boolean,
    assert_no_invalid_options,
    Integer,
)
from syne_tune.optimizer.schedulers.random_seeds import RandomSeedGenerator
from syne_tune.optimizer.scheduler import (
    TrialScheduler,
    SchedulerDecision,
    TrialSuggestion,
)
from syne_tune.backend.time_keeper import TimeKeeper, RealTimeKeeper
from syne_tune.backend.trial_status import Trial
from syne_tune.config_space import cast_config_values

__all__ = ["FIFOScheduler", "ResourceLevelsScheduler"]

logger = logging.getLogger(__name__)


_ARGUMENT_KEYS = {
    "searcher",
    "search_options",
    "checkpoint",
    "resume",
    "metric",
    "mode",
    "points_to_evaluate",
    "random_seed",
    "max_t",
    "max_resource_attr",
    "time_keeper",
}

_DEFAULT_OPTIONS = {
    "searcher": "random",
    "resume": False,
    "mode": "min",
}

_CONSTRAINTS = {
    "checkpoint": String(),
    "resume": Boolean(),
    "metric": String(),
    "mode": Categorical(choices=("min", "max")),
    "random_seed": Integer(0, 2**32 - 1),
    "max_t": Integer(1, None),
    "max_resource_attr": String(),
}


class ResourceLevelsScheduler(TrialScheduler):
    def _infer_max_resource_level_getval(self, name):
        if name in self.config_space and name not in self._hyperparameter_keys:
            return self.config_space[name]
        else:
            return None

    def _infer_max_resource_level(
        self, max_resource_level: Optional[int], max_resource_attr: Optional[str]
    ):
        """
        Helper to infer `max_resource_level` if not explicitly given.

        :param max_resource_level: Value explicitly provided, or None
        :param max_resource_attr: Name of max resource attribute in
            `config_space` (optional)
        :return:
        """
        inferred_max_t = None
        names = ("epochs", "max_t", "max_epochs")
        if max_resource_attr is not None:
            names = (max_resource_attr,) + names
        for name in names:
            inferred_max_t = self._infer_max_resource_level_getval(name)
            if inferred_max_t is not None:
                break
        if max_resource_level is not None:
            if inferred_max_t is not None and max_resource_level != inferred_max_t:
                logger.warning(
                    f"max_resource_level = {max_resource_level} is different "
                    f"from the value {inferred_max_t} inferred from "
                    "config_space"
                )
        else:
            # It is OK if max_resource_level cannot be inferred
            if inferred_max_t is not None:
                logger.info(
                    f"max_resource_level = {inferred_max_t}, as inferred "
                    "from config_space"
                )
            max_resource_level = inferred_max_t
        return max_resource_level


class FIFOScheduler(ResourceLevelsScheduler):
    r"""Simple scheduler that just runs trials in submission order.

    Parameters
    ----------
    config_space: dict
        Configuration space for trial evaluation function
    searcher : str or BaseSearcher
        Searcher (get_config decisions). If str, this is passed to
        searcher_factory along with search_options.
    search_options : dict
        If searcher is str, these arguments are passed to searcher_factory.
    checkpoint : str
        If filename given here, a checkpoint of scheduler (and searcher) state
        is written to file every time a job finishes.
        Note: May not be fully supported by all searchers.
    resume : bool
        If True, scheduler state is loaded from checkpoint, and experiment
        starts from there.
        Note: May not be fully supported by all searchers.
    metric : str
        Name of metric to optimize, key in result's obtained via
        `on_trial_result`
    mode : str
        Mode to use for the metric given, can be 'min' or 'max', default to 'min'.
    points_to_evaluate: list[dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified, or even be an
        empty dict. For each hyperparameter not specified, the default value
        is determined using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
        Note: If `searcher` is BaseSearcher, points_to_evaluate must be set
        there.
    random_seed : int
        Master random seed. Generators used in the scheduler or searcher are
        seeded using `RandomSeedGenerator`. If not given, the master random
        seed is drawn at random here.
    time_keeper : TimeKeeper
        If passed, this will be used for timing here (see `_elapsed_time`). The
        time keeper has to be started at the beginning of the experiment. If not
        given, we use a local time keeper here, which is started with the first
        call to `_suggest`.
        Can also be set after construction, with `set_time_keeper`.
        NOTE: If you use :class:`SimulatorBackend`, you need to pass its
        `time_keeper` here.
    max_t : int (optional)
        Maximum resource (see resource_attr) to be used for a job. Mandatory
        for multi-fidelity scheduling, and for fine-grained cost-aware
        searchers.
        Note: If this is not given, we try to infer its value from `config_space`,
        checking `config_space['epochs']`, `config_space['max-t']`, and
        `config_space['max-epochs']`. If `max_resource_attr` is given, we use
        the value `config_space[max_resource_attr]`. But if `max_t` is given
        here, it takes precedence.
    max_resource_attr : str (optional)
        Key name in config for fixed attribute containing the maximum resource.
        Mandatory for promotion-based multi-fidelity scheduling (see
        :class:`HyperbandScheduler`, type 'promotion'). If given here, it is
        used to infer `max_t` if not given.

    """

    def __init__(self, config_space: Dict, **kwargs):
        super().__init__(config_space)
        # Check values and impute default values
        assert_no_invalid_options(kwargs, _ARGUMENT_KEYS, name="FIFOScheduler")
        kwargs = check_and_merge_defaults(
            kwargs, set(), _DEFAULT_OPTIONS, _CONSTRAINTS, dict_name="scheduler_options"
        )
        metric = kwargs.get("metric")
        assert metric is not None, (
            "Argument 'metric' is mandatory. Pass the name of the metric "
            + "reported by your training script, which you'd like to "
            + "optimize, and use 'mode' to specify whether it should "
            + "be minimized or maximized"
        )
        self.metric = metric
        self.mode = kwargs["mode"]
        self.max_resource_attr = kwargs.get("max_resource_attr")
        # Setting max_t (if not provided as argument -> self.max_t)
        # This value can often be obtained from config_space. We check these
        # attributes (in order): epochs, max_t, max_epochs.
        # In any case, the max_t argument takes precedence. If it is None, we use
        # the one inferred from config_space.
        self.max_t = self._infer_max_resource_level(
            kwargs.get("max_t"), self.max_resource_attr
        )
        # Generator for random seeds
        random_seed = kwargs.get("random_seed")
        if random_seed is None:
            random_seed = np.random.randint(0, 2**32)
        logger.info(f"Master random_seed = {random_seed}")
        self.random_seed_generator = RandomSeedGenerator(random_seed)
        # Generate searcher
        searcher = kwargs["searcher"]
        if isinstance(searcher, str):
            search_options = kwargs.get("search_options")
            if search_options is None:
                search_options = dict()
            else:
                search_options = search_options.copy()
            search_options.update(
                {
                    "config_space": self.config_space.copy(),
                    "metric": self.metric,
                    "points_to_evaluate": kwargs.get("points_to_evaluate"),
                    "scheduler_mode": kwargs["mode"],
                    "mode": kwargs["mode"],
                    "random_seed_generator": self.random_seed_generator,
                }
            )
            if self.max_t is not None:
                search_options["max_epochs"] = self.max_t
            # Subclasses may extend `search_options`
            search_options = self._extend_search_options(search_options)
            # Adjoin scheduler info to search_options, if not already done by
            # subclass (via `_extend_search_options`)
            if "scheduler" not in search_options:
                search_options["scheduler"] = "fifo"
            self.searcher: BaseSearcher = searcher_factory(searcher, **search_options)
        else:
            assert isinstance(searcher, BaseSearcher)
            self.searcher: BaseSearcher = searcher

        checkpoint = kwargs.get("checkpoint")
        self._checkpoint = checkpoint
        self._start_time = None  # Will be set at first `suggest`
        self._searcher_initialized = False
        # Resume experiment from checkpoint?
        if kwargs["resume"]:
            assert checkpoint is not None, "Need checkpoint to be set if resume = True"
            if os.path.isfile(checkpoint):
                raise NotImplementedError()
                # TODO: Need load
                # self.load_state_dict(load(checkpoint))
            else:
                msg = f"checkpoint path {checkpoint} is not available for resume."
                logger.exception(msg)
                raise FileExistsError(msg)
        # Time keeper
        time_keeper = kwargs.get("time_keeper")
        if time_keeper is not None:
            self.set_time_keeper(time_keeper)
        else:
            self.time_keeper = None

    def set_time_keeper(self, time_keeper: TimeKeeper):
        """
        Allows to assign the time keeper after instruction. This is possible
        only if it was not assigned there already, and the experiment has
        not yet started.
        """
        assert self.time_keeper is None, "Time keeper has already been assigned"
        assert isinstance(
            time_keeper, TimeKeeper
        ), "Argument must be of type TimeKeeper"
        self.time_keeper = time_keeper

    def _extend_search_options(self, search_options: Dict) -> Dict:
        return search_options

    def _initialize_searcher(self):
        if not self._searcher_initialized:
            self.searcher.configure_scheduler(self)
            self._searcher_initialized = True

    def save(self, checkpoint=None):
        """Save Checkpoint"""
        if checkpoint is None:
            checkpoint = self._checkpoint
        if checkpoint is not None:
            raise NotImplementedError()
            # TODO: Need mkdir, save
            # mkdir(os.path.dirname(checkpoint))
            # save(self.state_dict(), checkpoint)

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        self._initialize_searcher()
        # If no time keeper was provided at construction, we use a local
        # one which is started here
        if self.time_keeper is None:
            self.time_keeper = RealTimeKeeper()
            self.time_keeper.start_of_time()
        # For pause/resume schedulers: Can a paused trial be promoted?
        promote_trial_id, extra_kwargs = self._promote_trial()
        if promote_trial_id is not None:
            promote_trial_id = int(promote_trial_id)
            return TrialSuggestion.resume_suggestion(
                trial_id=promote_trial_id, config=extra_kwargs
            )
        # Ask searcher for config of new trial to start
        extra_kwargs["elapsed_time"] = self._elapsed_time()
        trial_id = str(trial_id)
        config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
        if config is not None:
            config = cast_config_values(config, self.config_space)
            config = self._on_config_suggest(config, trial_id, **extra_kwargs)
            config = TrialSuggestion.start_suggestion(config)
        return config

    def _on_config_suggest(self, config: Dict, trial_id: str, **kwargs) -> Dict:
        # We register the config here, not in `on_trial_add`. While this risks
        # registering a config which is not successfully started, this is the
        # right thing to do for batch suggestions. There, `suggest` is called
        # multiple times in a row, and the batch trials are started together.
        # If we did not register pending configs after being suggested (but
        # before getting started), fantasizing would not be used for them.
        self.searcher.register_pending(trial_id=trial_id, config=config)
        if self.searcher.debug_log is not None:
            # For log outputs:
            config = dict(config, trial_id=trial_id)
        return config

    def _promote_trial(self) -> (Optional[str], Optional[Dict]):
        """
        Has to be implemented by pause/resume schedulers.
        If a trial can be promoted, its trial_id is returned, otherwise None.

        The second return argument, extra_kwargs, plays different roles
        depending on the first return argument:
        - If trial_id = None (no promotion): extra_kwargs are args to be
            passed to `get_config` call of searcher.
        - If trial_id not None (promotion): extra_kwargs may be None or a dict.
            If a dict, extra_kwargs is used to update the config of the
            trial to be promoted. In this case, `FIFOScheduler.suggest` will
            return the tuple (trial_id, extra_kwargs).

        :return: trial_id, extra_kwargs
        """
        return None, dict()

    def _elapsed_time(self):
        """
        :return: Time elapsed since start of experiment (see 'run')
        """
        assert self.time_keeper is not None, "Experiment has not been started yet"
        return self.time_keeper.time()

    def on_trial_error(self, trial: Trial):
        self._initialize_searcher()
        trial_id = str(trial.trial_id)
        self.searcher.evaluation_failed(trial_id)
        if self.searcher.debug_log is not None:
            logger.info(f"trial_id {trial_id}: Evaluation failed!")

    def _check_key_of_result(self, result: Dict, key: str):
        assert key in result, (
            "Your training evaluation function needs to report values "
            + f"for the key {key}:\n   report({key}=..., ...)"
        )

    def _check_result(self, result: Dict):
        self._check_key_of_result(result, self.metric)

    # Not doing much. Note the result at the end of the trial run is
    # passed to `on_trial_complete`
    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        self._check_result(result)
        trial_id = str(trial.trial_id)
        trial_decision = SchedulerDecision.CONTINUE
        if len(result) == 0:
            # An empty dict should just be skipped
            if self.searcher.debug_log is not None:
                logger.info(f"trial_id {trial_id}: Skipping empty result")
        else:
            config = self._preprocess_config(trial.config)
            self.searcher.on_trial_result(trial_id, config, result=result, update=False)
            # Extra info in debug mode
            log_msg = f"trial_id {trial_id} (metric = {result[self.metric]:.3f}"
            for k, is_float in (("epoch", False), ("elapsed_time", True)):
                if k in result:
                    if is_float:
                        log_msg += f", {k} = {result[k]:.2f}"
                    else:
                        log_msg += f", {k} = {result[k]}"
            log_msg += f"): decision = {trial_decision}"
            logger.debug(log_msg)
        return trial_decision

    def on_trial_complete(self, trial: Trial, result: Dict):
        if len(result) > 0:
            self._initialize_searcher()
            config = self._preprocess_config(trial.config)
            self.searcher.on_trial_result(
                str(trial.trial_id), config, result=result, update=True
            )

    def metric_names(self) -> List[str]:
        return [self.metric]

    def metric_mode(self) -> str:
        return self.mode

File Path: syne_tune/optimizer/schedulers/hyperband.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import copy
import logging
import os
from dataclasses import dataclass
from typing import Optional, List

import numpy as np

from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import SchedulerDecision
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.schedulers.hyperband_cost_promotion import (
    CostPromotionRungSystem,
)
from syne_tune.optimizer.schedulers.hyperband_pasha import PASHARungSystem
from syne_tune.optimizer.schedulers.hyperband_promotion import PromotionRungSystem
from syne_tune.optimizer.schedulers.hyperband_rush import (
    RUSHPromotionRungSystem,
    RUSHStoppingRungSystem,
)
from syne_tune.optimizer.schedulers.hyperband_stopping import StoppingRungSystem
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
    Integer,
    Boolean,
    Categorical,
    filter_by_key,
    String,
    Dictionary,
    Float,
)
from syne_tune.optimizer.schedulers.searchers.bracket_distribution import (
    DefaultHyperbandBracketDistribution,
)

__all__ = [
    "HyperbandScheduler",
    "HyperbandBracketManager",
    "hyperband_rung_levels",
]

logger = logging.getLogger(__name__)


_ARGUMENT_KEYS = {
    "resource_attr",
    "grace_period",
    "reduction_factor",
    "brackets",
    "type",
    "searcher_data",
    "cost_attr",
    "register_pending_myopic",
    "do_snapshots",
    "rung_system_per_bracket",
    "rung_levels",
    "rung_system_kwargs",
    "bracket_distribution",
}

_DEFAULT_OPTIONS = {
    "resource_attr": "epoch",
    "resume": False,
    "grace_period": 1,
    "reduction_factor": 3,
    "brackets": 1,
    "type": "stopping",
    "searcher_data": "rungs",
    "register_pending_myopic": False,
    "do_snapshots": False,
    "rung_system_per_bracket": False,
    "rung_system_kwargs": {
        "ranking_criterion": "soft_ranking",
        "epsilon": 1.0,
        "epsilon_scaling": 1.0,
    },
}

_CONSTRAINTS = {
    "resource_attr": String(),
    "resume": Boolean(),
    "grace_period": Integer(1, None),
    "reduction_factor": Float(2, None),
    "brackets": Integer(1, None),
    "type": Categorical(
        (
            "stopping",
            "promotion",
            "cost_promotion",
            "pasha",
            "rush_promotion",
            "rush_stopping",
        )
    ),
    "searcher_data": Categorical(("rungs", "all", "rungs_and_last")),
    "cost_attr": String(),
    "register_pending_myopic": Boolean(),
    "do_snapshots": Boolean(),
    "rung_system_per_bracket": Boolean(),
    "rung_system_kwargs": Dictionary(),
}


def is_continue_decision(trial_decision: str) -> bool:
    return trial_decision == SchedulerDecision.CONTINUE


@dataclass
class TrialInformation:
    """
    The scheduler maintains information about all trials it has been dealing
    with so far. `trial_decision` is the current status of the trial.
    `keep_case` is relevant only if `searcher_data == 'rungs_and_last'`.
    `largest_update_resource` is the largest resource level for which the
    searcher was updated, or None.
    `reported_result` caontains the last recent reported result, or None
    (task was started, but did not report anything yet). Only contains
    attributes `self.metric` and `self._resource_attr`.
    """

    config: dict
    time_stamp: float
    bracket: int
    keep_case: bool
    trial_decision: str
    reported_result: Optional[dict] = None
    largest_update_resource: Optional[int] = None

    def restart(self, time_stamp: float):
        self.time_stamp = time_stamp
        self.reported_result = None
        self.keep_case = False
        self.trial_decision = SchedulerDecision.CONTINUE


class HyperbandScheduler(FIFOScheduler):
    r"""Implements different variants of asynchronous Hyperband

    See 'type' for the different variants. One implementation detail is when
    using multiple brackets, task allocation to bracket is done randomly,
    based on a distribution inspired by the synchronous Hyperband case.

    For definitions of concepts (bracket, rung, milestone), see

        Li, Jamieson, Rostamizadeh, Gonina, Hardt, Recht, Talwalkar (2018)
        A System for Massively Parallel Hyperparameter Tuning
        https://arxiv.org/abs/1810.05934

    or

        Tiao, Klein, Lienart, Archambeau, Seeger (2020)
        Model-based Asynchronous Hyperparameter and Neural Architecture Search
        https://arxiv.org/abs/2003.10865

    Note: This scheduler requires both `metric` and `resource_attr` to be
    returned by the reporter. Here, resource values must be positive int. If
    resource_attr == 'epoch', this should be the number of epochs done,
    starting from 1 (not the epoch number, starting from 0).

    Rung levels and promotion quantiles:

    Rung levels are values of the resource attribute at which stop/go decisions
    are made for jobs, comparing their metric against others at the same level.
    These rung levels (positive, strictly increasing) can be specified via
    `rung_levels`, the largest must be `<= max_t`.
    If `rung_levels` is not given, rung levels are specified by `grace_period`
    and `reduction_factor`:

        [round(grace_period * (reduction_factor ** j))], j = 0, 1, ...

    This is the default choice for successive halving (Hyperband).
    Note: If `rung_levels` is given, then `grace_period`, `reduction_factor`
    are ignored. If they are given, a warning is logged.

    The rung levels determine the quantiles to be used in the stop/go
    decisions. If rung levels are r_0, r_1, ..., define

        q_j = r_j / r_{j+1}

    q_j is the promotion quantile at rung level r_j. On average, a fraction
    of q_j jobs can continue, the remaining ones are stopped (or paused).
    In the default successive halving case:

        q_j = 1 / reduction_factor    for all j

    Cost-aware schedulers or searchers:

    Some schedulers (e.g., type == 'cost_promotion') or searchers may depend
    on cost values (with key `cost_attr`) reported alongside the target metric.
    For promotion-based scheduling, a trial may pause and resume several times.
    The cost received in `on_trial_result` only counts the cost since the last
    resume. We maintain the sum of such costs in `_cost_offset`, and append
    a new entry to `result` in `on_trial_result` with the total cost.
    If the evaluation function does not implement checkpointing, once a trial
    is resumed, it has to start from scratch. We detect this in
    `on_trial_result` and reset the cost offset to 0 (if the trial runs from
    scratch, the cost reported needs no offset added).
    NOTE: This process requires `cost_attr` to be set!

    Pending evaluations:

    The searcher is notified. by `searcher.register_pending` calls, of
    (trial, resource) pairs for which evaluations are running, and a result
    is expected in the future. These pending evaluations can be used by the
    searcher in order to direct sampling elsewhere.
    The choice of pending evaluations depends on `searcher_data`. If equal
    to `'rungs'`, pending evaluations sit only at rung levels, because
    observations are only used there. In the other cases, pending evaluations
    sit at all resource levels for which observations are obtained. For
    example, if a trial is at rung level `r` and continues towards the next
    rung level `r_next`, if `searcher_data == 'rungs'`,
    `searcher.register_pending` is called for `r_next` only, while for other
    `searcher_data` values, pending evaluations are registered for
    `r + 1, r + 2, ..., r_next`.
    However, if in this case, `register_pending_myopic` is True, we instead
    call `searcher.register_pending` for `r + 1` when each observation is
    obtained (not just at a rung level). This leads to less pending
    evaluations at any one time. On the other hand, when a trial is continued
    at a rung level, we already know it will emit observations up to the next
    rung level, so it seems more "correct" to register all these pending
    evaluations in one go.

    Parameters
    ----------
    config_space: dict
        Configuration space for trial evaluation function
    searcher : str or BaseSearcher
        Searcher (get_config decisions). If str, this is passed to
        searcher_factory along with search_options.
    search_options : dict
        If searcher is str, these arguments are passed to searcher_factory.
    checkpoint : str
        If filename given here, a checkpoint of scheduler (and searcher) state
        is written to file every time a job finishes.
        Note: May not be fully supported by all searchers.
    resume : bool
        If True, scheduler state is loaded from checkpoint, and experiment
        starts from there.
        Note: May not be fully supported by all searchers.
    metric : str
        Name of metric to optimize, key in result's obtained via
        `on_trial_result`
    mode : str
        See :class:`TrialScheduler`
    resource_attr : str
        Name of resource attribute in result's obtained via `on_trial_result`.
        Note: The type of resource must be int.
    points_to_evaluate : List[dict] or None
        See :class:`FIFOScheduler`
    max_t : int
        See :class:`FIFOScheduler`. This is mandatory here. If not given, we
        try to infer it.
    grace_period : int
        Minimum resource to be used for a job. Ignored if `rung_levels` is
        given.
    reduction_factor : float (>= 2)
        Parameter to determine rung levels in successive halving (Hyperband).
        Ignored if `rung_levels` is given.
    rung_levels: list of int
        If given, prescribes the set of rung levels to be used. Must contain
        positive integers, strictly increasing. This information overrides
        `grace_period` and `reduction_factor`, but not `max_t`.
        Note that the stop/promote rule in the successive halving scheduler is
        set based on the ratio of successive rung levels.
    brackets : int
        Number of brackets to be used in Hyperband. Each bracket has a different
        grace period, all share max_t and reduction_factor.
        If brackets == 1, we run successive halving.
    extra_searcher_info : bool
        If True, information about the current state of the searcher returned
        in `on_trial_result`. This info includes in particular the current
        hyperparameters of the surrogate model of the searcher, as well as the
        dataset size.
    type : str
        Type of Hyperband scheduler:
            stopping:
                A config eval is executed by a single task. The task is stopped
                at a milestone if its metric is worse than a fraction of those
                who reached the milestone earlier, otherwise it continues.
                As implemented in Ray/Tune:
                https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband
                See :class:`StoppingRungSystem`.
            promotion:
                A config eval may be associated with multiple tasks over its
                lifetime. It is never terminated, but may be paused. Whenever a
                task becomes available, it may promote a config to the next
                milestone, if better than a fraction of others who reached the
                milestone. If no config can be promoted, a new one is chosen.
                This variant may benefit from pause&resume, which is not directly
                supported here. As proposed in this paper (termed ASHA):
                https://arxiv.org/abs/1810.05934
                See :class:`PromotionRungSystem`.
            cost_promotion:
                This is a cost-aware variant of 'promotion', see
                :class:`CostPromotionRungSystem` for details. In this case,
                costs must be reported under the name
                `rung_system_kwargs['cost_attr']` in results.
            pasha:
                Similar to promotion type Hyperband, but it progressively
                expands the available resources until the ranking
                of configurations stabilizes.
            rush_stopping:
                A variation of the stopping scheduler which requires passing rung_system_kwargs
                (see num_threshold_candidates) and points_to_evaluate. The first num_threshold_candidates of
                points_to_evaluate will enforce stricter rules on which task is continued.
                See :class:`RUSHScheduler`.
            rush_promotion:
                Same as rush_stopping but for promotion.
    cost_attr : str
        Required if the scheduler itself uses a cost metric (i.e.,
        `type='cost_promotion'`), or if the searcher uses a cost metric.
        See also header comment.
    searcher_data : str
        Relevant only if a model-based searcher is used.
        Example: For NN tuning and `resource_attr == epoch', we receive a
        result for each epoch, but not all epoch values are also rung levels.
        searcher_data determines which of these results are passed to the
        searcher. As a rule, the more data the searcher receives, the better
        its fit, but also the more expensive get_config may become. Choices:
        - 'rungs' (default): Only results at rung levels. Cheapest
        - 'all': All results. Most expensive
        - 'rungs_and_last': Results at rung levels, plus the most recent
            result. This means that in between rung levels, only the most
            recent result is used by the searcher. This is in between
        Note: For a Gaussian additive learning curve surrogate model, this
        has to be set to 'all'.
    register_pending_myopic : bool
        See above. Used only if `searcher_data != 'rungs'`.
    rung_system_per_bracket : bool
        This concerns Hyperband with `brackets > 1`. When starting a job for a
        new config, it is assigned a randomly sampled bracket. The larger the
        bracket, the larger the grace period for the config. If
        `rung_system_per_bracket = True`, we maintain separate rung level
        systems for each bracket, so that configs only compete with others
        started in the same bracket.
        If `rung_system_per_bracket = False`, we use a single rung level system,
        so that all configs compete with each other. In this case, the bracket
        of a config only determines the initial grace period, i.e. the first
        milestone at which it starts competing with others. This is the
        default.
        The concept of brackets in Hyperband is meant to hedge against overly
        aggressive filtering in successive halving, based on low fidelity
        criteria. In practice, successive halving (i.e., `brackets = 1`) often
        works best in the asynchronous case (as implemented here). If
        `brackets > 1`, the hedging is stronger if `rung_system_per_bracket`
        is True.
    do_snapshots : bool
        Support snapshots? If True, a snapshot of all running tasks and rung
        levels is returned by _promote_config. This snapshot is passed to the
        searcher in get_config.
        Note: Currently, only the stopping variant supports snapshots.
    max_resource_attr : str
        Optional. Relevant only for type 'promotion'. Key name in config for
        fixed attribute containing the maximum resource. The training
        evaluation function runs a loop over 1, ..., config[max_resource_attr],
        or starts from a resource > 1 if a checkpoint can be loaded.
        Whenever a trial is started or resumed here, this value in the config
        is set to the next rung level this trial will reach. As it will pause
        there in any case, this precludes the training code to continue until a
        stop signal is received.
        If given, `max_resource_attr` is also used in the mechanism to infer
        `max_t` (if not given).
    rung_system_kwargs : dict
        Arguments passed to the rung system:
            ranking_criterion : str
                Used if `type == 'pasha'`. Specifies what strategy to use
                for deciding if the ranking is stable and if to increase the resource.
                Available options are soft_ranking, soft_ranking_std,
                soft_ranking_median_dst and soft_ranking_mean_dst. The simplest
                soft_ranking accepts a manually specified value of epsilon and
                groups configurations with similar performance within the given range
                of objective values. The other strategies calculate the value of epsilon
                automatically, with the option to rescale the it using epsilon_scaling.
            epsilon : float
                Used if `type == 'pasha'`. Parameter for soft ranking in PASHA
                to say which configurations should be group together based on the
                similarity of their performance.
            epsilon_scaling : float
                Used if `type == 'pasha'`. When epsilon for soft ranking in
                PASHA is calculated automatically, it is possible to rescale it
                using epsilon_scaling.
            num_threshold_candidates : int
                Used if `type in ['rush_promotion', 'rush_stopping']`. The first num_threshold_candidates in
                points_to_evaluate enforce stricter requirements to the continuation of training tasks.
                See :class:`RUSHScheduler`.
    bracket_distribution : BracketDistribution
        Helper object for distribution the bracket for a new trial is sampled
        from. The default is :class:`DefaultHyperbandBracketDistribution`.

    See Also
    --------
    HyperbandBracketManager
    """

    def __init__(self, config_space, **kwargs):
        # Before we can call the superclass constructor, we need to set a few
        # members (see also `_extend_search_options`).
        # To do this properly, we first check values and impute defaults for
        # `kwargs`.
        kwargs = check_and_merge_defaults(
            kwargs, set(), _DEFAULT_OPTIONS, _CONSTRAINTS, dict_name="scheduler_options"
        )
        scheduler_type = kwargs["type"]
        self.scheduler_type = scheduler_type
        self._resource_attr = kwargs["resource_attr"]
        self._rung_system_kwargs = kwargs["rung_system_kwargs"]
        self._cost_attr = kwargs.get("cost_attr")
        self._num_brackets = kwargs["brackets"]
        assert not (
            scheduler_type == "cost_promotion" and self._cost_attr is None
        ), "cost_attr must be given if type='cost_promotion'"
        self.bracket_distribution = kwargs.get("bracket_distribution")
        if self.bracket_distribution is None:
            self.bracket_distribution = DefaultHyperbandBracketDistribution()
        # Superclass constructor
        resume = kwargs["resume"]
        kwargs["resume"] = False  # Cannot be done in superclass
        super().__init__(config_space, **filter_by_key(kwargs, _ARGUMENT_KEYS))
        assert self.max_t is not None, (
            "Either max_t must be specified, or it has to be specified as "
            + "config_space['epochs'], config_space['max_t'], "
            + "config_space['max_epochs']"
        )

        # If rung_levels is given, grace_period and reduction_factor are ignored
        rung_levels = kwargs.get("rung_levels")
        if rung_levels is not None:
            assert isinstance(rung_levels, list)
            if ("grace_period" in kwargs) or ("reduction_factor" in kwargs):
                logger.warning(
                    "Since rung_levels is given, the values grace_period = "
                    f"{kwargs.get('grace_period')} and reduction_factor = "
                    f"{kwargs.get('reduction_factor')} are ignored!"
                )
        rung_levels = hyperband_rung_levels(
            rung_levels,
            grace_period=kwargs["grace_period"],
            reduction_factor=kwargs["reduction_factor"],
            max_t=self.max_t,
        )
        do_snapshots = kwargs["do_snapshots"]
        assert (not do_snapshots) or (
            scheduler_type == "stopping"
        ), "Snapshots are supported only for type = 'stopping'"
        rung_system_per_bracket = kwargs["rung_system_per_bracket"]

        self.terminator = HyperbandBracketManager(
            scheduler_type,
            self._resource_attr,
            self.metric,
            self.mode,
            self.max_t,
            rung_levels,
            self._num_brackets,
            rung_system_per_bracket,
            cost_attr=self._total_cost_attr(),
            random_seed=self.random_seed_generator(),
            rung_system_kwargs=self._rung_system_kwargs,
            scheduler=self,
        )
        self.do_snapshots = do_snapshots
        self.searcher_data = kwargs["searcher_data"]
        self._register_pending_myopic = kwargs["register_pending_myopic"]
        # _active_trials:
        # Maintains information for all tasks (running, paused, or stopped).
        # Maps trial_id to `TrialInformation`.
        self._active_trials = dict()
        # _cost_offset:
        # Is used for promotion-based (pause/resume) scheduling if the eval
        # function reports cost values. For a trial which has been paused at
        # least once, this records the sum of costs for reaching its last
        # recent milestone.
        self._cost_offset = dict()
        if resume:
            checkpoint = kwargs.get("checkpoint")
            assert checkpoint is not None, "Need checkpoint to be set if resume = True"
            if os.path.isfile(checkpoint):
                raise NotImplementedError()
                # TODO! Need load
                # self.load_state_dict(load(checkpoint))
            else:
                msg = f"checkpoint path {checkpoint} is not available for resume."
                logger.exception(msg)
                raise FileExistsError(msg)

    def does_pause_resume(self) -> bool:
        """
        :return: Is this variant doing pause and resume scheduling, in the
            sense that trials can be paused and resumed later?
        """
        return self.scheduler_type != "stopping"

    @property
    def rung_levels(self) -> List[int]:
        return self.terminator.rung_levels

    def _initialize_searcher(self):
        if not self._searcher_initialized:
            super()._initialize_searcher()
            self.bracket_distribution.configure(self)

    def _extend_search_options(self, search_options: dict) -> dict:
        # Note: Needs self.scheduler_type to be set
        scheduler = "hyperband_{}".format(self.scheduler_type)
        result = dict(
            search_options, scheduler=scheduler, resource_attr=self._resource_attr
        )
        # Cost attribute: For promotion-based, cost needs to be accumulated
        # for each trial
        cost_attr = self._total_cost_attr()
        if cost_attr is not None:
            result["cost_attr"] = cost_attr
        return result

    def _total_cost_attr(self) -> Optional[str]:
        if self._cost_attr is None:
            return None
        elif self.does_pause_resume():
            return "total_" + self._cost_attr
        else:
            return self._cost_attr

    def _on_config_suggest(self, config: dict, trial_id: str, **kwargs) -> dict:
        """
        `kwargs` being used here:
        - elapsed_time: Time from start of experiment, set in
            `FIFOScheduler._suggest`
        - bracket: Bracket in which new trial is started, set in
            `HyperbandScheduler._promote_trial`
        - milestone: First milestone the new trial will reach, set in
            `HyperbandScheduler._promote_trial`
        """
        assert trial_id not in self._active_trials, f"Trial {trial_id} already exists"
        # See `FIFOScheduler._on_config_suggest` for why we register the task
        # and pending evaluation here, and not later in `on_task_add`.
        debug_log = self.searcher.debug_log
        # Register new task
        first_milestone = self.terminator.on_task_add(
            trial_id, bracket=kwargs["bracket"], new_config=True
        )[-1]
        if debug_log is not None:
            logger.info(
                f"trial_id {trial_id} starts (first milestone = " f"{first_milestone})"
            )
        # Register pending evaluation with searcher
        if self.searcher_data == "rungs":
            pending_resources = [first_milestone]
        elif self._register_pending_myopic:
            pending_resources = [1]
        else:
            pending_resources = list(range(1, first_milestone + 1))
        for resource in pending_resources:
            self.searcher.register_pending(
                trial_id=trial_id, config=config, milestone=resource
            )
        # Extra fields in `config`
        if debug_log is not None:
            # For log outputs:
            config["trial_id"] = trial_id
        if self.does_pause_resume() and self.max_resource_attr is not None:
            # The new trial should only run until the next milestone.
            # This needs its config to be modified accordingly.
            config[self.max_resource_attr] = kwargs["milestone"]

        self._active_trials[trial_id] = TrialInformation(
            config=copy.copy(config),
            time_stamp=kwargs["elapsed_time"],
            bracket=kwargs["bracket"],
            keep_case=False,
            trial_decision=SchedulerDecision.CONTINUE,
            largest_update_resource=None,
        )

        return config

    # Snapshot (in extra_kwargs['snapshot']):
    # - max_resource
    # - reduction_factor
    # - tasks: Info about running tasks in bracket bracket_id (or, if
    #   brackets share the same rung level system, all running tasks):
    #   dict(task_id) -> dict:
    #   - config: config as dict
    #   - time: Time when task was started, or when last recent result was
    #     reported
    #   - level: Level of last recent result report, or 0 if no reports yet
    # - rungs: Metric values at rung levels in bracket bracket_id:
    #   List of (rung_level, metric_dict), where metric_dict has entries
    #   task_id: metric_value. Note that entries are sorted in decreasing order
    #   w.r.t. rung_level.
    def _promote_trial(self) -> (Optional[str], Optional[dict]):
        trial_id, extra_kwargs = self.terminator.on_task_schedule()
        if trial_id is None:
            # No trial to be promoted
            if self.do_snapshots:
                # Append snapshot
                bracket_id = extra_kwargs["bracket"]
                extra_kwargs["snapshot"] = {
                    "tasks": self._snapshot_tasks(bracket_id),
                    "rungs": self.terminator.snapshot_rungs(bracket_id),
                    "max_resource": self.max_t,
                }
        else:
            # At this point, we can assume the trial will be resumed
            extra_kwargs["new_config"] = False
            self.terminator.on_task_add(trial_id, **extra_kwargs)
            # Update information (note that 'time_stamp' is not exactly
            # correct, since the task may get started a little later)
            assert (
                trial_id in self._active_trials
            ), f"Paused trial {trial_id} must be in _active_trials"
            record = self._active_trials[trial_id]
            assert not is_continue_decision(
                record.trial_decision
            ), f"Paused trial {trial_id} marked as running in _active_trials"
            record.restart(time_stamp=self._elapsed_time())
            # Register pending evaluation(s) with searcher
            next_milestone = extra_kwargs["milestone"]
            resume_from = extra_kwargs["resume_from"]
            if self.searcher_data == "rungs":
                pending_resources = [next_milestone]
            elif self._register_pending_myopic:
                pending_resources = [resume_from + 1]
            else:
                pending_resources = list(range(resume_from + 1, next_milestone + 1))
            for resource in pending_resources:
                self.searcher.register_pending(trial_id=trial_id, milestone=resource)
            if self.searcher.debug_log is not None:
                logger.info(
                    f"trial_id {trial_id}: Promotion from "
                    f"{resume_from} to {next_milestone}"
                )
            # In the case of a promoted trial, extra_kwargs plays a different
            # role
            if self.does_pause_resume() and self.max_resource_attr is not None:
                # The promoted trial should only run until the next milestone.
                # This needs its config to be modified accordingly
                extra_kwargs = record.config.copy()
                extra_kwargs[self.max_resource_attr] = next_milestone
            else:
                extra_kwargs = None
        return trial_id, extra_kwargs

    def _snapshot_tasks(self, bracket_id):
        # If all brackets share a single rung level system, then all
        # running jobs have to be taken into account, otherwise only
        # those jobs running in the same bracket
        all_running = not self.terminator._rung_system_per_bracket
        tasks = dict()
        for k, v in self._active_trials.items():
            if is_continue_decision(v.trial_decision) and (
                all_running or v.bracket == bracket_id
            ):
                reported_result = v.reported_result
                level = (
                    0
                    if reported_result is None
                    else reported_result[self._resource_attr]
                )
                # It is possible to have tasks in _active_trials which have
                # reached self.max_t. These must not end up in the snapshot
                if level < self.max_t:
                    tasks[k] = {
                        "config": v.config,
                        "time": v.time_stamp,
                        "level": level,
                    }
        return tasks

    def _cleanup_trial(self, trial_id: str, trial_decision: str):
        """
        Called for trials which are stopped or paused. The trial is still kept
        in the records.

        :param trial_id:
        """
        self.terminator.on_task_remove(trial_id)
        if trial_id in self._active_trials:
            # We do not remove stopped trials
            self._active_trials[trial_id].trial_decision = trial_decision

    def on_trial_error(self, trial: Trial):
        super().on_trial_error(trial)
        self._cleanup_trial(str(trial.trial_id), trial_decision=SchedulerDecision.STOP)

    def _update_searcher_internal(self, trial_id: str, config: dict, result: dict):
        if self.searcher_data == "rungs_and_last":
            # Remove last recently added result for this task. This is not
            # done if it fell on a rung level (i.e., `keep_case` is True)
            record = self._active_trials[trial_id]
            rem_result = record.reported_result
            if (rem_result is not None) and (not record.keep_case):
                self.searcher.remove_case(trial_id, **rem_result)

    def _update_searcher(
        self, trial_id: str, config: dict, result: dict, task_info: dict
    ):
        """
        Updates searcher with `result` (depending on `searcher_data`), and
        registers pending config with searcher.

        :param trial_id:
        :param config:
        :param result: Record obtained from `on_trial_result`
        :param task_info: Info from `self.terminator.on_task_report`
        :return: Should searcher be updated?
        """
        task_continues = task_info["task_continues"]
        milestone_reached = task_info["milestone_reached"]
        next_milestone = task_info.get("next_milestone")
        do_update = False
        pending_resources = []
        if self.searcher_data == "rungs":
            resource = result[self._resource_attr]
            if resource in self.rung_levels or resource == self.max_t:
                # Update searcher with intermediate result
                # Note: This condition is weaker than `milestone_reached` if
                # more than one bracket is used
                do_update = True
                if task_continues and milestone_reached and next_milestone is not None:
                    pending_resources = [next_milestone]
        elif not task_info.get("ignore_data", False):
            # All results are reported to the searcher, except if
            # task_info['ignore_data'] is True. The latter happens only for
            # tasks running promoted configs. In this case, we may receive
            # reports before the first milestone is reached, which should not
            # be passed to the searcher (they'd duplicate earlier
            # datapoints).
            # See also header comment of PromotionRungSystem.
            do_update = True
            if task_continues:
                resource = int(result[self._resource_attr])
                if self._register_pending_myopic or next_milestone is None:
                    pending_resources = [resource + 1]
                elif milestone_reached:
                    # Register pending evaluations for all resources up to
                    # `next_milestone`
                    pending_resources = list(range(resource + 1, next_milestone + 1))
        # Update searcher
        if do_update:
            self._update_searcher_internal(trial_id, config, result)
        # Register pending evaluations
        for resource in pending_resources:
            self.searcher.register_pending(
                trial_id=trial_id, config=config, milestone=resource
            )
        return do_update

    def _check_result(self, result: dict):
        super()._check_result(result)
        self._check_key_of_result(result, self._resource_attr)
        if self.scheduler_type == "cost_promotion":
            self._check_key_of_result(result, self._cost_attr)
        resource = result[self._resource_attr]
        assert 1 <= resource == round(resource), (
            "Your training evaluation function needs to report positive "
            + f"integer values for key {self._resource_attr}. Obtained "
            + f"value {resource}, which is not permitted"
        )

    def on_trial_result(self, trial: Trial, result: dict) -> str:
        self._check_result(result)
        trial_id = str(trial.trial_id)
        debug_log = self.searcher.debug_log
        trial_decision = SchedulerDecision.CONTINUE
        if len(result) == 0:
            # An empty dict should just be skipped
            if debug_log is not None:
                logger.info(
                    f"trial_id {trial_id}: Skipping empty dict received "
                    "from reporter"
                )
        else:
            # Time since start of experiment
            time_since_start = self._elapsed_time()
            do_update = False
            config = self._preprocess_config(trial.config)
            cost_and_promotion = (
                self._cost_attr is not None
                and self._cost_attr in result
                and self.does_pause_resume()
            )
            if cost_and_promotion:
                # Trial may have paused/resumed before, so need to add cost
                # offset from these
                cost_offset = self._cost_offset.get(trial_id, 0)
                result[self._total_cost_attr()] = result[self._cost_attr] + cost_offset
            # We may receive a report from a trial which has been stopped or
            # paused before. In such a case, we override trial_decision to be
            # STOP or PAUSE as before, so the report is not taken into account
            # by the scheduler. The report is sent to the searcher, but with
            # update=False. This means that the report is registered, but cannot
            # influence any decisions.
            record = self._active_trials[trial_id]
            trial_decision = record.trial_decision
            if trial_decision != SchedulerDecision.CONTINUE:
                logger.warning(
                    f"trial_id {trial_id}: {trial_decision}, but receives "
                    f"another report {result}\nThis report is ignored"
                )
            else:
                task_info = self.terminator.on_task_report(trial_id, result)
                task_continues = task_info["task_continues"]
                milestone_reached = task_info["milestone_reached"]
                if cost_and_promotion:
                    if milestone_reached:
                        # Trial reached milestone and will pause there: Update
                        # cost offset
                        if self._cost_attr is not None:
                            self._cost_offset[trial_id] = result[
                                self._total_cost_attr()
                            ]
                    elif task_info.get("ignore_data", False):
                        # For a resumed trial, the report is for resource <=
                        # resume_from, where resume_from < milestone. This
                        # happens if checkpointing is not implemented and a
                        # resumed trial has to start from scratch, publishing
                        # results all the way up to resume_from. In this case,
                        # we can erase the `_cost_offset` entry, since the
                        # instantaneous cost reported by the trial does not
                        # have any offset.
                        if self._cost_offset[trial_id] > 0:
                            logger.info(
                                f"trial_id {trial_id}: Resumed trial seems to have been "
                                + "started from scratch (no checkpointing?), so we erase "
                                + "the cost offset."
                            )
                        self._cost_offset[trial_id] = 0

                # Update searcher and register pending
                do_update = self._update_searcher(trial_id, config, result, task_info)
                # Change snapshot entry for task
                # Note: This must not be done above, because what _update_searcher
                # is doing, depends on the entry *before* its update here.
                # Note: result may contain all sorts of extra info.
                # All we need to maintain in the snapshot are metric and
                # resource level.
                # 'keep_case' entry (only used if searcher_data ==
                # 'rungs_and_last'): The result is kept in the dataset iff
                # milestone_reached == True (i.e., we are at a rung level).
                # Otherwise, it is removed once _update_searcher is called for
                # the next recent result.
                resource = int(result[self._resource_attr])
                record.time_stamp = time_since_start
                record.reported_result = {
                    self.metric: result[self.metric],
                    self._resource_attr: resource,
                }
                record.keep_case = milestone_reached
                if do_update:
                    largest_update_resource = record.largest_update_resource
                    if largest_update_resource is None:
                        largest_update_resource = resource - 1
                    assert largest_update_resource <= resource, (
                        f"Internal error (trial_id {trial_id}): "
                        + f"on_trial_result called with resource = {resource}, "
                        + f"but largest_update_resource = {largest_update_resource}"
                    )
                    if resource == largest_update_resource:
                        do_update = False  # Do not update again
                    else:
                        record.largest_update_resource = resource
                if not task_continues:
                    if (not self.does_pause_resume()) or resource >= self.max_t:
                        trial_decision = SchedulerDecision.STOP
                        act_str = "Terminating"
                    else:
                        trial_decision = SchedulerDecision.PAUSE
                        act_str = "Pausing"
                    self._cleanup_trial(trial_id, trial_decision=trial_decision)
                if debug_log is not None:
                    if not task_continues:
                        logger.info(
                            f"trial_id {trial_id}: {act_str} evaluation "
                            f"at {resource}"
                        )
                    elif milestone_reached:
                        msg = f"trial_id {trial_id}: Reaches {resource}, continues"
                        next_milestone = task_info.get("next_milestone")
                        if next_milestone is not None:
                            msg += f" to {next_milestone}"
                        logger.info(msg)
            self.searcher.on_trial_result(
                trial_id, config, result=result, update=do_update
            )
        # Extra info in debug mode
        log_msg = f"trial_id {trial_id} (metric = {result[self.metric]:.3f}"
        for k, is_float in ((self._resource_attr, False), ("elapsed_time", True)):
            if k in result:
                if is_float:
                    log_msg += f", {k} = {result[k]:.2f}"
                else:
                    log_msg += f", {k} = {result[k]}"
        log_msg += f"): decision = {trial_decision}"
        logger.debug(log_msg)
        return trial_decision

    def on_trial_remove(self, trial: Trial):
        self._cleanup_trial(str(trial.trial_id), trial_decision=SchedulerDecision.PAUSE)

    def on_trial_complete(self, trial: Trial, result: dict):
        # Check whether searcher was already updated based on `result`
        trial_id = str(trial.trial_id)
        largest_update_resource = self._active_trials[trial_id].largest_update_resource
        if largest_update_resource is not None:
            resource = int(result[self._resource_attr])
            if resource > largest_update_resource:
                super().on_trial_complete(trial, result)
        # Remove pending evaluations, in case there are still some
        self.searcher.cleanup_pending(trial_id)
        self._cleanup_trial(trial_id, trial_decision=SchedulerDecision.STOP)


def _is_positive_int(x):
    return int(x) == x and x >= 1


def hyperband_rung_levels(rung_levels, grace_period, reduction_factor, max_t):
    if rung_levels is not None:
        assert (
            isinstance(rung_levels, list) and len(rung_levels) > 1
        ), "rung_levels must be list of size >= 2"
        assert all(
            _is_positive_int(x) for x in rung_levels
        ), "rung_levels must be list of positive integers"
        rung_levels = [int(x) for x in rung_levels]
        assert all(
            x < y for x, y in zip(rung_levels, rung_levels[1:])
        ), "rung_levels must be strictly increasing sequence"
        assert (
            rung_levels[-1] <= max_t
        ), f"Last entry of rung_levels ({rung_levels[-1]}) must be <= max_t ({max_t})"
    else:
        # Rung levels given by grace_period, reduction_factor, max_t
        assert _is_positive_int(grace_period)
        assert reduction_factor >= 2
        assert _is_positive_int(max_t)
        assert (
            max_t > grace_period
        ), f"max_t ({max_t}) must be greater than grace_period ({grace_period})"
        rf = reduction_factor
        min_t = grace_period
        max_rungs = int(np.log(max_t / min_t) / np.log(rf) + 1)
        rung_levels = [int(round(min_t * np.power(rf, k))) for k in range(max_rungs)]
        assert rung_levels[-1] <= max_t  # Sanity check
        assert len(rung_levels) >= 2, (
            f"grace_period = {grace_period}, reduction_factor = "
            + f"{reduction_factor}, max_t = {max_t} leads to single rung level only"
        )
    return rung_levels


class HyperbandBracketManager:
    """Hyperband Manager

    Maintains rung level systems for range of brackets. Differences depending
    on `scheduler_type` ('stopping', 'promotion') manifest themselves mostly
    at the level of the rung level system itself.

    For `scheduler_type` == 'stopping', see :class:`StoppingRungSystem`.
    For `scheduler_type` == 'promotion', see :class:`PromotionRungSystem`.

    Args:
        scheduler_type : str
            See :class:`HyperbandScheduler`.
        resource_attr : str
            See :class:`HyperbandScheduler`.
        metric : str
            See :class:`HyperbandScheduler`.
        mode : str
            See :class:`HyperbandScheduler`.
        max_t : int
            See :class:`HyperbandScheduler`.
        rung_levels : list[int]
            See :class:`HyperbandScheduler`. If `rung_levels` is not given
            there, the default rung levels based on `grace_period` and
            `reduction_factor` are used.
        brackets : int
            See :class:`HyperbandScheduler`.
        rung_system_per_bracket : bool
            See :class:`HyperbandScheduler`.
        cost_attr : str
            Overrides entry in `rung_system_kwargs`
        random_seed : int
            Random seed for bracket sampling
        rung_system_kwargs : dict
            Dictionary of arguments passed to the rung system
        scheduler : HyperbandScheduler
            The scheduler is needed in order to sample a bracket
    """

    def __init__(
        self,
        scheduler_type,
        resource_attr,
        metric,
        mode,
        max_t,
        rung_levels,
        brackets,
        rung_system_per_bracket,
        cost_attr,
        random_seed,
        rung_system_kwargs,
        scheduler,
    ):
        self._scheduler_type = scheduler_type
        self._resource_attr = resource_attr
        self._max_t = max_t
        self.rung_levels = copy.copy(rung_levels)
        self._rung_system_per_bracket = rung_system_per_bracket
        self._scheduler = scheduler
        # Maps trial_id -> bracket_id
        self._task_info = dict()
        max_num_brackets = len(rung_levels)
        self.num_brackets = min(brackets, max_num_brackets)
        num_systems = self.num_brackets if rung_system_per_bracket else 1
        rung_levels_plus_maxt = rung_levels[1:] + [max_t]
        # Promotion quantiles: q_j = r_j / r_{j+1}
        promote_quantiles = [x / y for x, y in zip(rung_levels, rung_levels_plus_maxt)]
        kwargs = dict(metric=metric, mode=mode, resource_attr=resource_attr)
        if scheduler_type == "stopping":
            rs_type = StoppingRungSystem
        elif scheduler_type == "pasha":
            kwargs["max_t"] = max_t
            kwargs["ranking_criterion"] = rung_system_kwargs["ranking_criterion"]
            kwargs["epsilon"] = rung_system_kwargs["epsilon"]
            kwargs["epsilon_scaling"] = rung_system_kwargs["epsilon_scaling"]
            rs_type = PASHARungSystem
        elif scheduler_type in ["rush_promotion", "rush_stopping"]:
            kwargs["num_threshold_candidates"] = rung_system_kwargs.get(
                "num_threshold_candidates", 0
            )
            if scheduler_type == "rush_stopping":
                rs_type = RUSHStoppingRungSystem
            else:
                kwargs["max_t"] = max_t
                rs_type = RUSHPromotionRungSystem
        else:
            kwargs["max_t"] = max_t
            if scheduler_type == "promotion":
                rs_type = PromotionRungSystem
            else:
                kwargs["cost_attr"] = cost_attr
                rs_type = CostPromotionRungSystem
        self._rung_systems = [
            rs_type(
                rung_levels=rung_levels[s:],
                promote_quantiles=promote_quantiles[s:],
                **kwargs,
            )
            for s in range(num_systems)
        ]
        self.random_state = np.random.RandomState(random_seed)

    def _get_rung_system_for_bracket_id(self, bracket_id: int):
        if self._rung_system_per_bracket:
            sys_id = bracket_id
            skip_rungs = 0
        else:
            sys_id = 0
            skip_rungs = bracket_id
        return self._rung_systems[sys_id], skip_rungs

    def _get_rung_system(self, trial_id: str):
        bracket_id = self._task_info[trial_id]
        rung_sys, skip_rungs = self._get_rung_system_for_bracket_id(bracket_id)
        return rung_sys, bracket_id, skip_rungs

    def on_task_add(self, trial_id: str, **kwargs) -> List[int]:
        """
        Called when new task is started (can be new trial or trial being
        resumed).

        Since the bracket has already been sampled, not much is done here.
        We return the list of milestones for this bracket in reverse
        (decreasing) order. The first entry is max_t, even if it is
        not a milestone in the bracket. This list contains the resource
        levels the task would reach if it ran to max_t without being stopped.

        :param trial_id:
        :param kwargs:
        :return: List of milestones in decreasing order, where max_t is first
        """
        assert "bracket" in kwargs
        bracket_id = kwargs["bracket"]
        self._task_info[trial_id] = bracket_id
        rung_sys, skip_rungs = self._get_rung_system_for_bracket_id(bracket_id)
        rung_sys.on_task_add(trial_id, skip_rungs=skip_rungs, **kwargs)
        milestones = rung_sys.get_milestones(skip_rungs)
        if milestones[0] < self._max_t:
            milestones.insert(0, self._max_t)
        return milestones

    def on_task_report(self, trial_id: str, result: dict):
        """
        This method is called by the reporter thread whenever a new metric
        value is received. It returns a dictionary with all the information
        needed for making decisions (e.g., stop / continue task, update
        model, etc)
        - task_continues: Should task continue or stop/pause?
        - milestone_reached: True if rung level (or max_t) is hit
        - next_milestone: If hit rung level < max_t, this is the subsequent
          rung level (otherwise: None)
        - bracket_id: Bracket in which the task is running

        :param trial_id:
        :param result:
        :return: See above
        """
        rung_sys, bracket_id, skip_rungs = self._get_rung_system(trial_id)
        ret_dict = {
            "bracket_id": bracket_id,
            "task_continues": False,
            "milestone_reached": True,
            "next_milestone": None,
        }
        if self._scheduler_type != "stopping":
            ret_dict["ignore_data"] = False
        if result[self._resource_attr] < self._max_t:
            ret_dict.update(
                rung_sys.on_task_report(trial_id, result, skip_rungs=skip_rungs)
            )
            # Special case: If config just reached the last milestone in
            # the bracket and survived, next_milestone is equal to max_t
            if (
                ret_dict["task_continues"]
                and ret_dict["milestone_reached"]
                and (ret_dict["next_milestone"] is None)
            ):
                ret_dict["next_milestone"] = self._max_t
        return ret_dict

    def on_task_remove(self, trial_id):
        if trial_id in self._task_info:
            rung_sys, _, _ = self._get_rung_system(trial_id)
            rung_sys.on_task_remove(trial_id)
            del self._task_info[trial_id]

    def _sample_bracket(self) -> int:
        distribution = self._scheduler.bracket_distribution()
        return self.random_state.choice(a=distribution.size, p=distribution)

    def on_task_schedule(self) -> (Optional[str], dict):
        """
        Samples bracket for task to be scheduled. Check whether any paused
        trial in that bracket can be promoted. If so, its trial_id is
        returned. We also return extra_kwargs to be used in `_promote_trial`.
        """
        # Sample bracket for task to be scheduled
        bracket_id = self._sample_bracket()
        rung_sys, skip_rungs = self._get_rung_system_for_bracket_id(bracket_id)
        extra_kwargs = {"bracket": bracket_id}
        # Check whether config can be promoted
        ret_dict = rung_sys.on_task_schedule()
        trial_id = ret_dict.get("trial_id")
        if trial_id is not None:
            for k in ("milestone", "resume_from"):
                extra_kwargs[k] = ret_dict[k]
        else:
            # First milestone the new config will get to
            extra_kwargs["milestone"] = rung_sys.get_first_milestone(skip_rungs)
        return trial_id, extra_kwargs

    def snapshot_rungs(self, bracket_id):
        rung_sys, skip_rungs = self._get_rung_system_for_bracket_id(bracket_id)
        return rung_sys.snapshot_rungs(skip_rungs)

File Path: syne_tune/optimizer/schedulers/hyperband_cost_promotion.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.optimizer.schedulers.hyperband_promotion import PromotionRungSystem

logger = logging.getLogger(__name__)


class CostPromotionRungSystem(PromotionRungSystem):
    """
    Cost-aware extension of promotion-based asynchronous Hyperband (ASHA).

    This code is equivalent with base :class:`PromotionRungSystem`, except
    the "promotable" condition in `_find_promotable_config` is replaced.

    When a config x reaches rung level r, the result includes a metric
    m(x, r), but also a cost c(x, r). The latter is the cost (e.g., training
    time) spent to reach level r.

    Consider all trials who reached rung level r (whether promoted from there
    or still paused there), ordered w.r.t. m(x, r), best first, and let their
    number be N. Define

        C(r, k) = sum( c(x_i, r) | i <= k)

    For a promotion quantile q, define

        K = max_k [ C(r, k) <= q * C(r, N) ]

    Any trial not yet promoted and ranked <= K can be promoted.

    As usual, we scan rungs from the top. If several trials are promotable,
    the one with the best metric value is promoted.

    Note that costs c(x, r) reported via `cost_attr` need to be total costs of
    a trial. If the trial is paused and resumed, partial costs have to be added
    up. See :class:`HyperbandScheduler` for how this works.

    """

    def __init__(
        self,
        rung_levels,
        promote_quantiles,
        metric,
        mode,
        resource_attr,
        cost_attr,
        max_t,
    ):
        super().__init__(
            rung_levels, promote_quantiles, metric, mode, resource_attr, max_t
        )
        self._cost_attr = cost_attr
        # Note: The data entry in _rungs is now a dict mapping trial_id to
        # (metric_value, cost_value, was_promoted), where metric_value is
        # m(x, r), cost value is c(x, r).

    def _find_promotable_trial(self, recorded, prom_quant, resource):
        """
        Check whether any not yet promoted entry in `recorded` is
        promotable (see header comment). If there are several such, the one
        with the best value is chosen.

        :param recorded: Dict to scan
        :param prom_quant: Quantile for promotion
        :param resource: Amount of resources spent on the rung.
        :return: trial_id if found, otherwise None
        """
        ret_id = None
        if len(recorded) > 1:
            sign = 2 * (self._mode == "min") - 1
            # Sort best-first
            sorted_record = sorted(
                ((k,) + v for k, v in recorded.items()), key=lambda x: x[1] * sign
            )
            cost_threshold = sum(x[2] for x in sorted_record) * prom_quant
            sum_costs = 0
            # DEBUG
            log_msg = (
                f"q = {prom_quant:.2f}, threshold = {cost_threshold:.2f}\n"
                + ", ".join(
                    [
                        f"{x[0]}:{x[2]:.2f}({x[1]:.3f},{int(x[3])})"
                        for x in sorted_record
                    ]
                )
            )
            for id, _, cost, was_promoted in sorted_record:
                sum_costs += cost
                if sum_costs > cost_threshold:
                    log_msg += "\nNothing to promote"
                    break
                if not was_promoted:
                    log_msg += f"\nPromote {id}: sum_costs = {sum_costs:.2f}"
                    ret_id = id
                    break
            logger.debug(log_msg)  # DEBUG
        return ret_id

    def _register_metrics_at_rung_level(self, trial_id, result, recorded):
        metric_value = result[self._metric]
        cost_value = result[self._cost_attr]
        assert trial_id not in recorded  # Sanity check
        recorded[trial_id] = (metric_value, cost_value, False)

File Path: syne_tune/optimizer/schedulers/hyperband_pasha.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
from syne_tune.optimizer.schedulers.hyperband_promotion import PromotionRungSystem
import itertools

class PASHARungSystem(PromotionRungSystem):
    """
    Implements PASHA algorithm. It is very similar to ASHA, but it progressively
    extends the maximum resources if the ranking in the top two current rungs changes.

    A report introducing and evaluating the approach is available at
    TODO: add link
    """

    def __init__(
        self,
        rung_levels,
        promote_quantiles,
        metric,
        mode,
        resource_attr,
        max_t,
        ranking_criterion,
        epsilon,
        epsilon_scaling,
    ):
        super().__init__(
            rung_levels, promote_quantiles, metric, mode, resource_attr, max_t
        )
        self.ranking_criterion = ranking_criterion
        # define the index of the current top rung, starting from 1 for the lowest rung
        #
        self.current_rung_idx = 2
        self.rung_levels = rung_levels

        # initialize current maximum resources
        self.current_max_t = rung_levels[self.current_rung_idx - 1]

        self.epsilon = epsilon
        self.epsilon_scaling = epsilon_scaling
        if ranking_criterion == 'soft_ranking_auto':
            self.per_epoch_results = {}
            self.epoch_to_trials = {}
            self.current_max_epoch = -1

    # overriding the method in HB promotion to accomodate the increasing max resources level
    def _effective_max_t(self):
        return self.current_max_t

    def _get_top_rungs_rankings(self, num_rungs=2):
        """
        Look at the current top two rungs and get the rankings of the configurations.
        The rungs can be empty, in which case we will return a list with 0 or 1 elements.
        Normally the list will include rankings for both rungs.

        The rankings are stored as a list of tuples (trial_id, rank, value).

        Lower values have lower ranks, starting from zero. For example:
        [('0', 0, 10.0), ('1', 3, 19.6), ('2', 2, 14.3), ('3', 1, 11.6)]

        :param num_rungs: int describing how many top rungs to return
        :return: rankings
            List of at most two lists with tuple(trial_id, rank, score)
        """
        rankings = []
        # be careful, self._rungs is ordered with the highest resources level in the beginning
        rungs = [self._rungs[-self.current_rung_idx + e] for e in range(num_rungs)]
        for rung in rungs:
            if rung.data != {}:
                trial_ids = rung.data.keys()
                values = []
                for trial_id in trial_ids:
                    values.append(rung.data[trial_id][0])
                # order specifies where the value should be placed in the sorted list
                values_order = np.array(values).argsort()
                # calling argsort on the order will give us the ranking
                values_ranking = values_order.argsort()
                ranking = list(zip(trial_ids, values_ranking, values))

                rankings.append(ranking)

        return rankings

    def _get_sorted_top_rungs(self, rankings):
        """
        Sort the configurations in the top rung and the previous rung.
        Filter out the configurations from the previous rung that
        are not in the top rung.

        :param rankings: list of at most two lists with tuple(trial_id, rank, score)
        return: sorted_top_rung, sorted_previous_rung
        """
        # filter only the relevant configurations from the earlier rung
        top_rung_keys = set([e[0] for e in rankings[0]])
        corresponding_previous_rung_trials = filter(
            lambda e: e[0] in top_rung_keys, rankings[1]
        )
        # if we try to maximize the objective, we need to reverse the ranking
        if self._mode == "max":
            reverse = True
        else:
            reverse = False

        sorted_top_rung = sorted(rankings[0], key=lambda e: e[1], reverse=reverse)
        sorted_previous_rung = sorted(
            corresponding_previous_rung_trials, key=lambda e: e[1], reverse=reverse
        )
        return sorted_top_rung, sorted_previous_rung

    def _evaluate_soft_ranking(self, sorted_top_rung, sorted_previous_rung) -> bool:
        """
        Soft ranking creates groups of similarly performing configurations
        and increases the resources only if a configuration goes outside of
        its group.

        :param sorted_top_rung: list of tuple(trial_id, rank, score)
        :param sorted_previous_rung: list of tuple(sorted_top_rung, rank, score)
        :return: keep_current_budget
        """
        keep_current_budget = True
        if len(sorted_previous_rung) < 2:
            epsilon = 0.0
        elif self.ranking_criterion == "soft_ranking_std":
            epsilon = (
                np.std([e[2] for e in sorted_previous_rung]) * self.epsilon_scaling
            )
        elif (
            self.ranking_criterion == "soft_ranking_median_dst"
            or self.ranking_criterion == "soft_ranking_mean_dst"
        ):
            scores = [e[2] for e in sorted_previous_rung]
            distances = [
                abs(e1 - e2)
                for idx1, e1 in enumerate(scores)
                for idx2, e2 in enumerate(scores)
                if idx1 != idx2
            ]
            if self.ranking_criterion == "soft_ranking_mean_dst":
                epsilon = np.mean(distances) * self.epsilon_scaling
            elif self.ranking_criterion == "soft_ranking_median_dst":
                epsilon = np.median(distances) * self.epsilon_scaling
            else:
                raise ValueError(
                    "Ranking criterion {} is not supported".format(
                        self.ranking_criterion
                    )
                )
        else:
            epsilon = self.epsilon

        # create groups of configurations with similar performance
        previous_rung_groups = []
        for idx, item in enumerate(sorted_previous_rung):
            current_rung_group = [item[0]]
            # add configurations that are after the current configuration
            for idx_after in range(idx + 1, len(sorted_previous_rung)):
                new_item = sorted_previous_rung[idx_after]

                if self._mode == "max":
                    if new_item[2] < item[2] - epsilon:
                        break
                else:
                    if new_item[2] > item[2] + epsilon:
                        break
                current_rung_group.append(new_item[0])
            # add configurations that are before the current configuration
            for idx_before in range(idx - 1, -1, -1):
                new_item = sorted_previous_rung[idx_before]
                if self._mode == "max":
                    if new_item[2] > item[2] + epsilon:
                        break
                else:
                    if new_item[2] < item[2] - epsilon:
                        break
                current_rung_group.append(new_item[0])
            previous_rung_groups.append(set(current_rung_group))

        # evaluate if a configuration has switched its group
        for idx, item in enumerate(sorted_top_rung):
            if item[0] not in previous_rung_groups[idx]:
                keep_current_budget = False
                break

        return keep_current_budget

    def _update_epsilon(self):
        """
        This function is used to automatically calculate the value of epsilon.
        It finds the configurations which swapped their rankings across rungs
        and estimates the value of epsilon as the 90th percentile of the difference
        between their performance in the previous rung.

        The original value of epsilon is kept if no suitable configurations were found.
        """

        seen_pairs = set()
        noisy_cfg_distances = []
        top_epoch = min(self.current_max_epoch, self._rungs[-self.current_rung_idx].level)
        bottom_epoch = min(self._rungs[-self.current_rung_idx+1].level, self.current_max_epoch)
        for epoch in range(top_epoch, bottom_epoch, -1):
            if len(self.epoch_to_trials[epoch]) > 1:
                for pair in itertools.combinations(self.epoch_to_trials[epoch], 2):
                    c1, c2 = pair[0], pair[1]
                    if (c1, c2) not in seen_pairs:
                        seen_pairs.add((c1, c2))
                        p1, p2 = self.per_epoch_results[c1][epoch], self.per_epoch_results[c2][epoch]
                        cond = p1 > p2

                        opposite_order = False
                        same_order_after_opposite = False
                        # now we need to check the earlier epochs to see if at any point they had a different order
                        for prev_epoch in range(epoch - 1, 0, -1):
                            pp1, pp2 = self.per_epoch_results[c1][prev_epoch], self.per_epoch_results[c2][prev_epoch]
                            p_cond = pp1 > pp2
                            if p_cond == (not cond):
                                opposite_order = True
                            if opposite_order and p_cond == cond:
                                same_order_after_opposite = True
                                break

                        if opposite_order and same_order_after_opposite:
                            noisy_cfg_distances.append(abs(p1 - p2))

        if len(noisy_cfg_distances) > 0:
            self.epsilon = np.percentile(noisy_cfg_distances, 90)
            if str(self.epsilon) == 'nan':
                raise ValueError('Epsilon became nan') 

    def _update_per_epoch_results(self, trial_id, result):
        if trial_id not in self.per_epoch_results:
            self.per_epoch_results[trial_id] = {}
        self.per_epoch_results[trial_id][result[self._resource_attr]] = result[self._metric]

        if result[self._resource_attr] not in self.epoch_to_trials:
            self.epoch_to_trials[result[self._resource_attr]] = set() 
        self.epoch_to_trials[result[self._resource_attr]].add(trial_id)

        if result[self._resource_attr] > self.current_max_epoch:
            self.current_max_epoch = result[self._resource_attr]

    def _decide_resource_increase(self, rankings) -> bool:
        """
        Decide if to increase the resources given the current rankings.
        Currently we look at the rankings and if elements in the first list
        have the same order also in the second list, we keep the current resource
        budget. If the rankings are different, we will increase the budget.

        The rankings can only be incorrect if we have rankings for both rungs.

        :param rankings: list of at most two lists with tuple(trial_id, rank, score)
        :return: not keep_current_budget
        """
        if len(rankings) == 2:
            sorted_top_rung, sorted_previous_rung = self._get_sorted_top_rungs(rankings)
        else:
            return False

        keep_current_budget = self._evaluate_soft_ranking(
            sorted_top_rung, sorted_previous_rung
        )

        return not keep_current_budget

    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:
        """
        Apart from calling the superclass method, we also check the rankings
        and decides if to increase the current maximum resources.
        """
        ret_dict = super().on_task_report(trial_id, result, skip_rungs)

        if self.ranking_criterion == "soft_ranking_auto":
            self._update_per_epoch_results(trial_id, result)
            self._update_epsilon()

        # check the rankings and decide if to increase the current maximum resources
        rankings = self._get_top_rungs_rankings(num_rungs=2)
        increase_resources = self._decide_resource_increase(rankings)

        # we have a maximum amount of resources that PASHA can use
        # the resources should not increase indefinitely
        if increase_resources:
            if self.current_rung_idx < len(self._rungs):
                self.current_rung_idx += 1
                # be careful, self.rung_levels is ordered with the highest resources level at the end
                # moreover, since we use rung levels for counting both from the beginning and from the end of the list
                # we need to remember that counting from the beginning it's zero indexed
                self.current_max_t = self.rung_levels[self.current_rung_idx - 1]
            else:
                self.current_max_t = self.max_t

        return ret_dict

File Path: syne_tune/optimizer/schedulers/hyperband_promotion.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional

from syne_tune.optimizer.schedulers.hyperband_stopping import (
    quantile_cutoff,
    RungSystem,
)


class PromotionRungSystem(RungSystem):
    """
    Implements both the promotion and stopping logic for an asynchronous
    variant of Hyperband, known as ASHA:

    https://arxiv.org/abs/1810.05934

    In ASHA, configs sit paused at milestones (rung levels) until they get
    promoted, which means that a free task picks up their evaluation until
    the next milestone.

    The rule to decide whether a paused trial is promoted (or remains
    paused) is the same as in :class:`StoppingRungSystem`, except that
    `continues` becomes `gets_promoted`. If several paused trials in a
    rung can be promoted, the one with the best metric value is chosen.

    Note: Say that an evaluation is resumed from level resume_from. If the
    train_fn does not implement pause & resume, it needs to start training from
    scratch, in which case metrics are reported for every epoch, also those <
    resume_from. At least for some modes of fitting the searcher model to data,
    this would lead to duplicate target values for the same extended config
    (x, r), which we want to avoid. The solution is to maintain resume_from in
    the data for the terminator (see `PromotionRungSystem._running`). Given
    this, we can report in `on_task_report` that the current metric data should
    not be used for the searcher model (`ignore_data = True`), namely as long
    as the evaluation has not yet gone beyond level resume_from.
    """

    def __init__(
        self, rung_levels, promote_quantiles, metric, mode, resource_attr, max_t
    ):
        super().__init__(rung_levels, promote_quantiles, metric, mode, resource_attr)
        # The data entry in `_rungs` is a dict mapping trial_id to
        # (metric_value, was_promoted)
        self.max_t = max_t
        # `_running` maps `trial_id `to `dict(milestone, resume_from)`.
        # The tasks runs trial `trial_id` until resource reaches milestone.
        # The `resume_from` field can be None. If not, the task is running a
        # trial which has been resumed from rung level `resume_from.` This info
        # is required for `on_task_report` to properly report `ignore_data`.
        self._running = dict()

    def _cutoff(self, recorded: dict, prom_quant: float):
        values = [x[0] for x in recorded.values()]
        return quantile_cutoff(values, prom_quant, self._mode)

    def _find_promotable_trial(
        self, recorded: dict, prom_quant: float, resource: int
    ) -> Optional[str]:
        """
        Check whether any not yet promoted entry in `recorded` is
        promotable, i.e. its value is better or equal to the cutoff
        based on `prom_quant`. If there are several such, the one with the
        best value is chosen.

        :param recorded: Dict to scan
        :param prom_quant: Quantile for promotion
        :param resource: Resource level of rung (i.e., amount of resource
            spent by trials at this rung)
        :return: trial_id if found, otherwise None
        """
        ret_id = None
        # Code is written for 'max' mode. For 'min', we just negate all
        # criterion values
        sign = 1 - 2 * (self._mode == "min")
        cutoff = self._cutoff(recorded, prom_quant)
        if cutoff is not None:
            # Best id among trials paused at this rung (i.e., not yet promoted)
            trial_id, val = max(
                (
                    (k, v[0])
                    for k, v in recorded.items()
                    if self._is_promotable_trial(k, v[0], not v[1], resource)
                ),
                key=lambda x: sign * x[1],
                default=(None, 0.0),
            )
            if trial_id is not None and sign * (val - cutoff) >= 0:
                ret_id = trial_id
        return ret_id

    def _is_promotable_trial(
        self, trial_id: str, metric_value: float, is_paused: bool, resource: int
    ) -> bool:
        """
        Checks whether trial in rung level is promotable in principle, used
        as filter in `_find_promotable_trial`. Can be used in subclasses to
        sharpen the condition for promotion.

        """
        return is_paused

    @staticmethod
    def _mark_as_promoted(recorded: dict, trial_id: str):
        curr_val = recorded[trial_id]
        assert not curr_val[-1]  # Sanity check
        recorded[trial_id] = curr_val[:-1] + (True,)

    def _effective_max_t(self):
        """
        The following method is used in on_task_schedule to control the maximum
        amount of resources allocated to a single configuration during the
        optimization. For ASHA it's just a constant value.
        """
        return self.max_t

    def on_task_schedule(self) -> dict:
        """
        Used to implement _promote_trial of scheduler. Searches through rungs
        to find a trial which can be promoted. If one is found, we return the
        trial_id and other info (current milestone, milestone to be promoted
        to). We also mark the trial as being promoted at the rung level it sits
        right now.
        """
        trial_id = None
        next_milestone = self.max_t
        milestone = None
        recorded = None
        for rung in self._rungs:
            _milestone = rung.level
            prom_quant = rung.prom_quant
            _recorded = rung.data
            if _milestone < self._effective_max_t():
                trial_id = self._find_promotable_trial(
                    _recorded, prom_quant, rung.level
                )
            if trial_id is not None:
                recorded = _recorded
                milestone = _milestone
                break
            next_milestone = _milestone
        ret_dict = dict()
        if trial_id is not None:
            self._mark_as_promoted(recorded, trial_id)
            ret_dict = {
                "trial_id": trial_id,
                "resume_from": milestone,
                "milestone": next_milestone,
            }
        return ret_dict

    def on_task_add(self, trial_id: str, skip_rungs: int, **kwargs):
        """
        Called when new task is started. Depending on kwargs['new_config'],
        this could start an evaluation (True) or promote an existing config
        to the next milestone (False). In the latter case, kwargs contains
        additional information about the promotion.
        """
        new_config = kwargs.get("new_config", True)
        if new_config:
            # New trial
            milestone = self.get_first_milestone(skip_rungs)
            resume_from = None
        else:
            # Existing trial is resumed
            # Note that self._rungs has already been updated in
            # on_task_schedule
            milestone = kwargs["milestone"]
            resume_from = kwargs["resume_from"]
            assert resume_from < milestone  # Sanity check
        self._running[trial_id] = {"milestone": milestone, "resume_from": resume_from}

    def _register_metrics_at_rung_level(
        self, trial_id: str, result: dict, recorded: dict
    ):
        metric_value = result[self._metric]
        assert trial_id not in recorded  # Sanity check
        recorded[trial_id] = (metric_value, False)

    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:
        """
        Decision on whether task may continue (task_continues = True), or
        should be paused (task_continues = False).
        milestone_reached is a flag whether resource coincides with a
        milestone.
        For this scheduler, we have that

            task_continues == not milestone_reached,

        since a trial is always paused at a milestone.

        `ignore_data` is True if a result is received from a resumed trial
        at a level <= `resume_from`. This happens if checkpointing is not
        implemented (or not used), because resumed trials are started from
        scratch then. These metric values should in general be ignored.

        :param trial_id:
        :param result: Reported metrics
        :param skip_rungs: This number of lowest rung levels are not
            considered milestones for this task
        :return: dict(task_continues, milestone_reached, next_milestone,
                      ignore_data)
        """
        resource = result[self._resource_attr]
        milestone_reached = False
        next_milestone = None
        milestone = self._running[trial_id]["milestone"]
        resume_from = self._running[trial_id]["resume_from"]
        ignore_data = (resume_from is not None) and (resource <= resume_from)
        if resource >= milestone:
            assert resource == milestone, (
                f"trial_id {trial_id}: resource = {resource} > {milestone} "
                + "milestone. Make sure to report time attributes covering "
                + "all milestones"
            )
            milestone_reached = True
            try:
                rung_pos = next(
                    i for i, v in enumerate(self._rungs) if v.level == milestone
                )
                # Register metric_value at rung level (as not promoted)
                recorded = self._rungs[rung_pos].data
                self._register_metrics_at_rung_level(trial_id, result, recorded)
                next_milestone = (
                    self._rungs[rung_pos - 1].level if rung_pos > 0 else self.max_t
                )
            except StopIteration:
                # milestone not a rung level. This can happen, in particular
                # if milestone == self.max_t
                pass
        return {
            "task_continues": not milestone_reached,
            "milestone_reached": milestone_reached,
            "next_milestone": next_milestone,
            "ignore_data": ignore_data,
        }

    def on_task_remove(self, trial_id: str):
        del self._running[trial_id]

File Path: syne_tune/optimizer/schedulers/hyperband_rush.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Optional

from syne_tune.optimizer.schedulers.hyperband_promotion import PromotionRungSystem
from syne_tune.optimizer.schedulers.hyperband_stopping import StoppingRungSystem

logger = logging.getLogger(__name__)


class RUSHDecider:
    """
    Implements the additional decision logic according to the RUSH algorithm.
    It is used as part of RUSHStoppingRungSystem and RUSHPromotionRungSystem.

    Reference: A resource-efficient method for repeated HPO and NAS.
    Giovanni Zappella, David Salinas, Cédric Archambeau. AutoML workshop @ ICML 2021.

    For a more detailed description, refer to
    :class:`RUSHScheduler`.
    """

    def __init__(self, num_threshold_candidates: int, mode: str):
        if num_threshold_candidates <= 0:
            logger.warning(
                "No threshold candidates provided. 'rush_stopping' will behave exactly like 'stopping'."
            )
        self._num_threshold_candidates = num_threshold_candidates
        self._mode = mode
        self._thresholds = (
            dict()
        )  # thresholds at different resource levels that must be met

    def task_continues(
        self, task_continues: bool, trial_id: str, metric_value: float, resource: int
    ) -> bool:
        if not task_continues:
            return False
        if self._is_in_points_to_evaluate(trial_id):
            self._thresholds[resource] = self._return_better(
                self._thresholds.get(resource), metric_value
            )
            return True
        return self._meets_threshold(metric_value, resource)

    def _is_in_points_to_evaluate(self, trial_id: str) -> bool:
        return int(trial_id) < self._num_threshold_candidates

    def _return_better(self, val1: Optional[float], val2: Optional[float]) -> float:
        if self._mode == "min":
            better_val = min(
                float("inf") if val1 is None else val1,
                float("inf") if val2 is None else val2,
            )
        else:
            better_val = max(
                float("-inf") if val1 is None else val1,
                float("-inf") if val2 is None else val2,
            )
        return better_val

    def _meets_threshold(self, metric_value: float, resource: int) -> bool:
        return (
            self._return_better(self._thresholds.get(resource), metric_value)
            == metric_value
        )


class RUSHStoppingRungSystem(StoppingRungSystem):
    def __init__(self, num_threshold_candidates: int, **kwargs):
        super().__init__(**kwargs)
        self._decider = RUSHDecider(num_threshold_candidates, self._mode)

    def _task_continues(
        self,
        metric_value: float,
        recorded: dict,
        prom_quant: float,
        trial_id: str,
        resource: int,
    ) -> bool:
        task_continues = super()._task_continues(
            metric_value, recorded, prom_quant, trial_id, resource
        )
        return self._decider.task_continues(
            task_continues, trial_id, metric_value, resource
        )


class RUSHPromotionRungSystem(PromotionRungSystem):
    def __init__(self, num_threshold_candidates: int, **kwargs):
        super().__init__(**kwargs)
        self._decider = RUSHDecider(num_threshold_candidates, self._mode)

    def _is_promotable_trial(
        self, trial_id: str, metric_value: float, is_paused: bool, resource: int
    ) -> bool:
        task_continues = super()._is_promotable_trial(
            trial_id, metric_value, is_paused, resource
        )
        return self._decider.task_continues(
            task_continues, trial_id, metric_value, resource
        )

File Path: syne_tune/optimizer/schedulers/hyperband_stopping.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from dataclasses import dataclass
from typing import List, Tuple

import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class RungEntry:
    level: int  # Rung level r_j
    prom_quant: float  # Promotion quantile q_j
    data: dict  # Data of all previous jobs reaching the level


def quantile_cutoff(values, prom_quant, mode):
    if len(values) < 2:
        # Cannot determine cutoff from one value
        return None
    q = prom_quant if mode == "min" else (1 - prom_quant)
    return np.quantile(values, q)


class RungSystem:
    """
    Terminology: trials emit results at certain resource levels (e.g.,
    epoch numbers). Some resource levels are rung levels, this is where
    scheduling decisions (stop, continue or pause, resume) are taken.

    For a running trial, the next rung level it will reach is called
    its milestone.

    """

    def __init__(self, rung_levels, promote_quantiles, metric, mode, resource_attr):
        assert len(rung_levels) == len(promote_quantiles)
        self._metric = metric
        self._mode = mode
        self._resource_attr = resource_attr
        # The data entry in `_rungs` is a dict with key trial_id. The
        # value type depends on the subclass, but it contains the
        # metric value
        self._rungs = [
            RungEntry(level=x, prom_quant=y, data=dict())
            for x, y in reversed(list(zip(rung_levels, promote_quantiles)))
        ]

    def on_task_schedule(self) -> dict:
        """
        Called when new task is to be scheduled.
        For a promotion-based rung system, check whether any trial can be
        promoted. If so, return dict with keys `trial_id`, `resume_from`
        (rung level where trial is paused), `milestone` (next rung level
        if will reach, or None).
        If no trial can be promoted, or if the rung system is not
        promotion-based, an empty dict is returned.

        :return: See above
        """
        raise NotImplementedError

    def on_task_add(self, trial_id: str, skip_rungs: int, **kwargs):
        """
        Called when new task is started.

        :param trial_id:
        :param skip_rungs: This number of lowest rung levels are not
            considered milestones for this task
        :param kwargs:
        """
        pass

    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:
        """
        Called when a trial reports metric results. Returns dict with
        `milestone_reached` (trial reaches its milestone), `task_continues`
        (trial should continue; otherwise it is stopped or paused),
        `next_milestone` (next milestone it will reach, or None).
        For certain subclasses, there may be additional fields.

        :param trial_id:
        :param result: Reported metrics
        :param skip_rungs: This number of lowest rung levels are not
            considered milestones for this task
        :return: See above
        """
        raise NotImplementedError

    def on_task_remove(self, trial_id: str):
        """
        Called when task is removed.

        :param trial_id:
        """
        pass

    def get_first_milestone(self, skip_rungs: int) -> int:
        """
        :param skip_rungs: This number of lowest rung levels are not
            considered milestones for this task
        :return: First milestone to be considered
        """
        return self._rungs[-(skip_rungs + 1)].level

    def _milestone_rungs(self, skip_rungs: int) -> List[RungEntry]:
        if skip_rungs > 0:
            return self._rungs[:(-skip_rungs)]
        else:
            return self._rungs

    def get_milestones(self, skip_rungs: int) -> List[int]:
        """
         :param skip_rungs: This number of lowest rung levels are not
            considered milestones for this task
        :return: All milestones to be considered
        """
        milestone_rungs = self._milestone_rungs(skip_rungs)
        return [x.level for x in milestone_rungs]

    def snapshot_rungs(self, skip_rungs: int) -> List[Tuple[int, dict]]:
        """
        A snapshot is a list of rung levels with entries `(level, data)`,
        ordered from top to bottom (highest rung first).

        :param skip_rungs: This number of lowest rung levels are not
            considered milestones for this task
        :return: Snapshot (see above)
        """
        milestone_rungs = self._milestone_rungs(skip_rungs)
        return [(x.level, x.data) for x in milestone_rungs]


class StoppingRungSystem(RungSystem):
    """
    The decision on whether a trial x continues or is stopped at a rung
    level r, is taken in `on_task_report`. To this end, the metric value
    f(x, r) is inserted into r.data. Then:

        continues(x, r)  <==>  f(x, r) <= np.quantile(r.data, r.prom_quant)

    in case `mode == 'min'`. See `_task_continues`.

    """

    def _cutoff(self, recorded, prom_quant):
        values = list(recorded.values())
        return quantile_cutoff(values, prom_quant, self._mode)

    def _task_continues(
        self,
        metric_value: float,
        recorded: dict,
        prom_quant: float,
        trial_id: str,
        resource: int,
    ) -> bool:
        """
        :param metric_value: f(x, r) for trial x at rung r
        :param recorded: Data for rung r (including r(x, r))
        :param prom_quant: Quantile threshold (for mode 'min')
        :param trial_id: ID of trial
        :param resource: Rung level
        :return: Continue trial? Stop otherwise
        """
        cutoff = self._cutoff(recorded, prom_quant)
        if cutoff is None:
            return True
        return metric_value <= cutoff if self._mode == "min" else metric_value >= cutoff

    def on_task_schedule(self) -> dict:
        return dict()

    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:
        resource = result[self._resource_attr]
        metric_value = result[self._metric]
        task_continues = True
        milestone_reached = False
        next_milestone = None
        milestone_rungs = self._milestone_rungs(skip_rungs)
        for rung in milestone_rungs:
            milestone = rung.level
            prom_quant = rung.prom_quant
            recorded = rung.data
            if not (resource < milestone or trial_id in recorded):
                # Note: It is important for model-based searchers that
                # milestones are reached exactly, not jumped over. In
                # particular, if a future milestone is reported via
                # register_pending, its reward value has to be passed
                # later on via update.
                if resource > milestone:
                    logger.warning(
                        f"resource = {resource} > {milestone} = milestone. "
                        "Make sure to report time attributes covering all milestones.\n"
                        f"Continueing, but milestone {milestone} has been skipped."
                    )
                else:
                    milestone_reached = True
                    # Enter new metric value before checking condition
                    recorded[trial_id] = metric_value
                    task_continues = self._task_continues(
                        metric_value=metric_value,
                        recorded=recorded,
                        prom_quant=prom_quant,
                        trial_id=trial_id,
                        resource=resource,
                    )
                break
            next_milestone = milestone
        return {
            "task_continues": task_continues,
            "milestone_reached": milestone_reached,
            "next_milestone": next_milestone,
        }

File Path: syne_tune/optimizer/schedulers/median_stopping_rule.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Example showing how to implement a new Scheduler.
"""
import logging
from collections import defaultdict
from typing import Optional, Dict, List

import numpy as np

from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import (
    TrialScheduler,
    SchedulerDecision,
    TrialSuggestion,
)


class MedianStoppingRule(TrialScheduler):
    def __init__(
        self,
        scheduler: TrialScheduler,
        resource_attr: str,
        running_average: bool = True,
        metric: Optional[str] = None,
        grace_time: Optional[int] = 1,
        grace_population: int = 5,
        rank_cutoff: float = 0.5,
    ):
        """
        Applies median stopping rule in top of an existing scheduler.
        * If result at time-step ranks less than the cutoff of other results observed at this time-step, the trial is
        interrupted and otherwise, the wrapped scheduler is called to make the stopping decision.
        * Suggest decisions are left to the wrapped scheduler.
        * The mode of the wrapped scheduler is used.
        Reference: Google Vizier: A Service for Black-Box Optimization. Golovin et al. 2017.
        :param scheduler: scheduler to be called for trial suggestion or when median-stopping-rule decision is to
        continue.
        :param resource_attr: key in the reported dictionary that accounts for the resource (e.g. epoch or
        wall-clocktime).
        :param running_average: if True, then uses the running average of observation instead of raw observations.
        :param metric: metric to be considered.
        :param grace_time: median stopping rule is only applied for results whose `time_attr` exceeds this amount.
        :param grace_population: median stopping rule when at least `grace_population` have been observed at a resource
        level.
        :param rank_cutoff: results whose quantiles are bellow this level are discarded (discard by default trials
         whose results are bellow the median).
        """
        super(MedianStoppingRule, self).__init__(config_space=scheduler.config_space)
        self.metric = scheduler.metric if metric is None else metric
        self.sorted_results = defaultdict(list)
        self.scheduler = scheduler
        self.resource_attr = resource_attr
        self.rank_cutoff = rank_cutoff
        self.grace_time = grace_time
        self.min_samples_required = grace_population
        self.running_average = running_average
        if running_average:
            self.trial_to_results = defaultdict(list)
        self.mode = scheduler.metric_mode()

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        return self.scheduler._suggest(trial_id=trial_id)

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        new_metric = result[self.metric]
        if self.mode == "max":
            new_metric *= -1
        time_step = result[self.resource_attr]

        if self.running_average:
            # gets the running average of current observations
            self.trial_to_results[trial.trial_id].append(new_metric)
            new_metric = np.mean(self.trial_to_results[trial.trial_id])

        # insert new metric in sorted results acquired at this resource
        index = np.searchsorted(self.sorted_results[time_step], new_metric)
        self.sorted_results[time_step] = np.insert(
            self.sorted_results[time_step], index, new_metric
        )
        normalized_rank = index / float(len(self.sorted_results[time_step]))

        if self.grace_condition(time_step=time_step):
            return self.scheduler.on_trial_result(trial=trial, result=result)
        elif normalized_rank <= self.rank_cutoff:
            return self.scheduler.on_trial_result(trial=trial, result=result)
        else:
            logging.info(
                f"see new results {new_metric} at time-step {time_step} for trial {trial.trial_id}"
                f" with rank {int(normalized_rank * 100)}%, "
                f"stopping it as it does not rank on the top {int(self.rank_cutoff * 100)}%"
            )
            return SchedulerDecision.STOP

    def grace_condition(self, time_step: float) -> bool:
        # lets the trial continue when the time is bellow the grace time and when not sufficiently many observations
        # are present for this time budget
        if (
            self.min_samples_required is not None
            and len(self.sorted_results[time_step]) < self.min_samples_required
        ):
            return True
        if self.grace_time is not None and time_step < self.grace_time:
            return True
        return False

    def metric_names(self) -> List[str]:
        return self.scheduler.metric_names()

    def metric_mode(self) -> str:
        return self.scheduler.metric_mode()

File Path: syne_tune/optimizer/schedulers/multiobjective/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

__all__ = ["MOASHA"]

from syne_tune.optimizer.schedulers.multiobjective.moasha import MOASHA

File Path: syne_tune/optimizer/schedulers/multiobjective/moasha.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Dict, Optional, Union, List

import numpy as np
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import (
    TrialScheduler,
    SchedulerDecision,
    TrialSuggestion,
)
from syne_tune.optimizer.schedulers.multiobjective.multiobjective_priority import (
    MOPriority,
    NonDominatedPriority,
)

logger = logging.getLogger(__name__)


class MOASHA(TrialScheduler):
    """Implements Multiojbective asynchronous successive halving with different multiobjective sort options.

    References:
    A multi-objective perspective on jointly tuning hardware and hyperparameters
    David Salinas, Valerio Perrone, Cedric Archambeau and Olivier Cruchant
    NAS workshop, ICLR2021.

    and

    Multi-objective multi-fidelity hyperparameter optimization with application to fairness
    Robin Schmucker, Michele Donini, Valerio Perrone, Cédric Archambeau


    Args:
        time_attr: A training result attr to use for comparing time.
            Note that you can pass in something non-temporal such as
            `training_iteration` as a measure of progress, the only requirement
            is that the attribute should increase monotonically.
        multiobjective_priority: The multiobjective priority that is used to sort multiobjectives candidates.
        We support several choices such as non-dominated sort or linear scalarization, default is non-dominated sort.
        metrics: The training result objectives to optimize. Stopping
            procedures will use this attribute.
        mode: One of {min, max} or a list of {min, max}. Determines whether objectives are minimized or maximized,
        in a case of a list the specification is done per objective. By default, all objectives are minimized.
        max_t: max time units per trial. Trials will be stopped after
            max_t time units (determined by time_attr) have passed.
        grace_period: Only stop trials at least this old in time.
            The units are the same as the attribute named by `time_attr`.
        reduction_factor: Used to set halving rate and amount. This
            is simply a unit-less scalar.
        brackets: Number of brackets. Each bracket has a different
            halving rate, specified by the reduction factor.
    """

    def __init__(
        self,
        config_space: Dict,
        metrics: List[str],
        time_attr: str = "training_iteration",
        multiobjective_priority: Optional[MOPriority] = None,
        mode: Optional[Union[str, List[str]]] = None,
        max_t: int = 100,
        grace_period: int = 1,
        reduction_factor: float = 3,
        brackets: int = 1,
    ):
        super(MOASHA, self).__init__(config_space=config_space)
        assert max_t > 0, "Max (time_attr) not valid!"
        assert max_t >= grace_period, "grace_period must be <= max_t!"
        assert grace_period > 0, "grace_period must be positive!"
        assert reduction_factor > 1, "Reduction Factor not valid!"
        assert brackets > 0, "brackets must be positive!"
        if mode:
            if isinstance(mode, List):
                assert len(mode) == len(metrics), "one mode should be given per metric"
                assert all(
                    m in ["min", "max"] for m in mode
                ), "all modes should be 'min' or 'max'."
            else:
                assert mode in ["min", "max"], "`mode` must be 'min' or 'max'."
        else:
            mode = "min"

        if multiobjective_priority is None:
            self._multiobjective_priority = NonDominatedPriority()
        else:
            self._multiobjective_priority = multiobjective_priority

        self._reduction_factor = reduction_factor
        self._max_t = max_t
        self._trial_info = {}  # Stores Trial -> Bracket

        # Tracks state for new trial add
        self._brackets = [
            _Bracket(
                grace_period, max_t, reduction_factor, s, self._multiobjective_priority
            )
            for s in range(brackets)
        ]
        self._num_stopped = 0
        self._metrics = metrics
        self._mode = mode
        if isinstance(self._mode, List):
            self._metric_op = {
                metric: 1 if mode == "min" else -1
                for metric, mode in zip(metrics, self._mode)
            }
        else:
            if self._mode == "min":
                self._metric_op = dict(zip(self._metrics, [1.0] * len(self._metrics)))
            elif self._mode == "max":
                self._metric_op = dict(zip(self._metrics, [-1.0] * len(self._metrics)))
        self._time_attr = time_attr

    def metric_names(self) -> List[str]:
        return self._metrics

    def metric_mode(self) -> str:
        return self._mode

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        """
        Implements `suggest`, except for basic postprocessing of
        config.
        """
        config = {
            k: v.sample() if hasattr(v, "sample") else v
            for k, v in self.config_space.items()
        }
        return TrialSuggestion.start_suggestion(config)

    def on_trial_add(self, trial: Trial):
        sizes = np.array([len(b._rungs) for b in self._brackets])
        probs = np.e ** (sizes - sizes.max())
        normalized = probs / probs.sum()
        idx = np.random.choice(len(self._brackets), p=normalized)
        print(f"adding trial {trial.trial_id}")
        self._trial_info[trial.trial_id] = self._brackets[idx]

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        self._check_metrics_are_present(result)
        if result[self._time_attr] >= self._max_t:
            action = SchedulerDecision.STOP
        else:
            bracket = self._trial_info[trial.trial_id]
            metrics = self._metric_dict(result)
            action = bracket.on_result(
                trial_id=trial.trial_id,
                cur_iter=result[self._time_attr],
                metrics=metrics,
            )
        if action == SchedulerDecision.STOP:
            self._num_stopped += 1
        return action

    def _metric_dict(self, reported_results: Dict) -> Dict:
        return {
            metric: reported_results[metric] * self._metric_op[metric]
            for metric in self._metrics
        }

    def _check_metrics_are_present(self, result: Dict):
        for key in [self._time_attr] + self._metrics:
            if key not in result:
                assert key in result, f"{key} not found in reported result {result}"

    def on_trial_complete(self, trial: Trial, result: Dict):
        self._check_metrics_are_present(result)
        bracket = self._trial_info[trial.trial_id]
        bracket.on_result(
            trial_id=trial.trial_id,
            cur_iter=result[self._time_attr],
            metrics=self._metric_dict(result),
        )
        del self._trial_info[trial.trial_id]

    def on_trial_remove(self, trial: Trial):
        del self._trial_info[trial.trial_id]


class _Bracket:
    """Bookkeeping system to track recorded values.

    Rungs are created in reversed order so that we can more easily find
    the correct rung corresponding to the current iteration of the result.
    """

    def __init__(
        self,
        min_t: int,
        max_t: int,
        reduction_factor: float,
        s: int,
        mo_priority: MOPriority = NonDominatedPriority(),
    ):
        self.rf = reduction_factor
        MAX_RUNGS = int(np.log(max_t / min_t) / np.log(self.rf) - s + 1)
        self._rungs = [
            (min_t * self.rf ** (k + s), {}) for k in reversed(range(MAX_RUNGS))
        ]
        self.priority = mo_priority

    def on_result(self, trial_id: int, cur_iter: int, metrics: Optional[Dict]) -> str:
        action = SchedulerDecision.CONTINUE
        for milestone, recorded in self._rungs:
            if cur_iter < milestone or trial_id in recorded:
                continue
            else:
                if not recorded:
                    # if no result was previously recorded, we saw the first result and we continue
                    action = SchedulerDecision.CONTINUE
                else:
                    # get the list of metrics seen for the rung, compute multiobjective priority and decide to continue
                    # if priority is in the top ones according to a rank induced by the `reduction_factor`.
                    metric_recorded = np.array(
                        [list(x.values()) for x in recorded.values()]
                        + [list(metrics.values())]
                    )
                    priorities = self.priority(metric_recorded)

                    # self._plot(milestone, metric_recorded, priorities)

                    # We sort priorities at every call, assuming the cost of sort would be negligible
                    # in case this becomes slow, we could just maintain a sorted list of priorities in cost
                    # of memory.
                    ranks = np.searchsorted(sorted(priorities), priorities) / len(
                        priorities
                    )
                    new_priority_rank = ranks[-1]
                    if new_priority_rank > 1 / self.rf:
                        action = SchedulerDecision.STOP
                recorded[trial_id] = metrics
                break
        return action

    def _plot(self, milestone, metric_recorded, priorities):
        """
        Plots the multiobjective candidates and the rank given by the multiobjective priority.
        """
        import numpy as np
        import matplotlib.pyplot as plt

        if len(metric_recorded) < 5:
            return

        fig, ax = plt.subplots()

        ranks = np.searchsorted(sorted(priorities), priorities)

        ax.scatter(metric_recorded[:, 0], metric_recorded[:, 1])

        font_size = 14
        plt.rcParams.update({"font.size": font_size})

        for i, indice in enumerate(ranks):
            ax.annotate(
                str(ranks[i]),
                metric_recorded[i],
                textcoords="offset points",  # how to position the text
                xytext=(-10, -10),  # distance from text to points (x,y)
            )

        # plt.legend()
        plt.tight_layout()
        plt.savefig(f"non-dominated-sorting-{milestone}.pdf")
        plt.show()

File Path: syne_tune/optimizer/schedulers/multiobjective/multiobjective_priority.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List

import numpy as np
from syne_tune.optimizer.schedulers.multiobjective.non_dominated_priority import (
    nondominated_sort,
)


class MOPriority:
    def __init__(self, metrics: Optional[List[str]] = None):
        """
        :param metrics: name of the objectives, optional if not passed anonymous names are created when seeing the
        first objectives to rank.
        """
        self.metrics = metrics

    def __call__(self, objectives: np.array) -> np.array:
        """
        :param objectives: that should be argsorted with shape (num_samples, num_objectives)
        :return: a vector with shape (num_samples,) that gives priority for the different elements (lower elements
        are picked first).
        """
        num_samples, num_objectives = objectives.shape
        if self.metrics is None:
            # set anonymous metric names
            self.metrics = [f"metric-{i}" for i in range(num_objectives)]
        assert num_objectives == len(self.metrics)
        return self.priority_unsafe(objectives=objectives)

    def priority_unsafe(self, objectives: np.array) -> np.array:
        raise NotImplementedError()


class LinearScalarizationPriority(MOPriority):
    def __init__(
        self, metrics: Optional[List[str]] = None, weights: Optional[np.array] = None
    ):
        """
        A simple multiobjective scalarization strategy that do a weighed sum to assign a priority to the objectives.
        :param metrics:
        :param weights:
        """
        super(LinearScalarizationPriority, self).__init__(metrics=metrics)
        if weights is None:
            # uniform weights by default
            self.weights = np.ones(1)
        else:
            if metrics is not None:
                assert len(weights) == len(metrics)
            self.weights = weights

        # makes multiplication convenient with batch of samples
        self.weights = np.expand_dims(self.weights, 0)

    def priority_unsafe(self, objectives: np.array) -> np.array:
        weighted_objectives = (objectives * self.weights).mean(axis=-1)
        return weighted_objectives


class FixedObjectivePriority(MOPriority):
    def __init__(self, metrics: Optional[List[str]] = None, dim: Optional[int] = None):
        """
        Optimizes a fixed objective, the first one by default.
        :param metrics:
        :param dim: dimension of the objective to optimize, first one by default.
        """
        super(FixedObjectivePriority, self).__init__(metrics=metrics)
        self.dim = dim if dim is not None else 0

    def priority_unsafe(self, objectives: np.array) -> np.array:
        return objectives[:, self.dim]


class NonDominatedPriority(MOPriority):
    def __init__(
        self,
        metrics: Optional[List[str]] = None,
        dim: Optional[int] = 0,
        max_num_samples: Optional[int] = None,
    ):
        """
        A non-dominated sort strategy that uses an epsilon-net strategy instead of crowding distance proposed in:

        A multi-objective perspective on jointly tuning hardware and hyperparameters
        David Salinas, Valerio Perrone, Cedric Archambeau and Olivier Cruchant
        NAS workshop, ICLR2021.

        :param metrics:
        :param dim: The objective to prefer when ranking items within the Pareto front and picking the first
        element. If `None`, the first element is chosen randomly.
        :param max_num_samples: The maximum number of samples that should be returned.
        When this is `None`, all items are sorted (less efficient), if you have a large number of samples but only want
        the top k indices, set this to k for efficiency.
        """
        super(NonDominatedPriority, self).__init__(metrics=metrics)
        self.dim = dim
        self.max_num_samples = max_num_samples

    def priority_unsafe(self, objectives: np.array) -> np.array:
        return np.array(
            nondominated_sort(
                X=objectives, dim=self.dim, max_items=self.max_num_samples
            )
        )

File Path: syne_tune/optimizer/schedulers/multiobjective/non_dominated_priority.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List, Union

import numpy as np


def pareto_efficient(X: np.ndarray) -> np.ndarray:
    """
    Evaluates for each allocation in the provided array whether it is Pareto efficient. The costs
    are assumed to be improved by lowering them (eg lower is better).

    Parameters
    ----------
    X: np.ndarray [N, D]
        The allocations to check where N is the number of allocations and D the number of costs per
        allocation.

    Returns
    -------
    np.ndarray [N]
        A boolean array, indicating for each allocation whether it is Pareto efficient.
    """
    # First, we assume that all allocations are Pareto efficient, i.e. not dominated
    mask = np.ones(X.shape[0], dtype=bool)
    # Then, we iterate over all allocations A and check which are dominated by then current
    # allocation A. If it is, we don't need to check it against another allocation.
    for i, allocation in enumerate(X):
        # Only consider allocation if it hasn't been dominated yet
        if mask[i]:
            # An allocation is dominated by A if all costs are equal or lower and at least one cost
            # is strictly lower. Using that definition, A cannot be dominated by itself.
            dominated = np.all(allocation <= X[mask], axis=1) * np.any(
                allocation < X[mask], axis=1
            )
            mask[mask] = ~dominated

    return mask


def compute_epsilon_net(X: np.ndarray, dim: Optional[int] = None) -> np.ndarray:
    """
    Outputs an order of the items in the provided array such that the items are spaced well. This
    means that after choosing a seed item, the next item is chosen to be the farthest from the seed
    item. The third item is then chosen to maximize the distance to the existing points and so on.

    This algorithm is taken from "Nearest-Neighbor Searching and Metric Space Dimensions"
    (Clarkson, 2005, p.17).

    Parameters
    ----------
    X: np.ndarray [N, D]
        The items to sparsify where N is the number of items and D their dimensionality.
    dim: Optional[int], default: None
        The index of the dimension which to use to choose the seed item. If `None`, an item is
        chosen at random, otherwise the item with the lowest value in the specified dimension is
        used.

    Returns
    -------
    np.ndarray [N]
        A list of item indices, defining a sparsified order of the items.
    """
    indices = set(range(X.shape[0]))

    # Choose the seed item according to dim
    if dim is None:
        initial_index = np.random.choice(X.shape[0])
    else:
        initial_index = np.argmin(X, axis=0)[dim]

    # Initialize the order
    order = [initial_index]
    indices.remove(initial_index)

    # Iterate until all models have been chosen
    while indices:
        # Get the distance to all items that have already been chosen
        ordered_indices = list(indices)
        diff = X[ordered_indices][:, None, :].repeat(len(order), axis=1) - X[order]
        min_distances = np.linalg.norm(diff, axis=-1).min(-1)

        # Then, choose the one with the maximum distance to all points
        choice = ordered_indices[min_distances.argmax()]
        order.append(choice)
        indices.remove(choice)

    # convert argsort indices to rank
    ranks = np.empty(len(order), dtype=int)
    for rank, i in enumerate(order):
        ranks[i] = rank
    return np.array(ranks)


def nondominated_sort(
    X: np.ndarray,
    dim: Optional[int] = None,
    max_items: Optional[int] = None,
    flatten: bool = True,
) -> Union[List[int], List[List[int]]]:
    """
    Performs a multi-objective sort by iteratively computing the Pareto front and sparsifying the
    items within the Pareto front. This is a non-dominated sort leveraging an epsilon-net.

    Parameters
    ----------
    X: np.ndarray [N, D]
        The multi-dimensional items to sort.
    dim: Optional[int], default: None
        The feature (metric) to prefer when ranking items within the Pareto front. If `None`, items
        are chosen randomly.
    max_items: Optional[int], default: None
        The maximum number of items that should be returned. When this is `None`, all items are
        sorted.
    flatten: bool, default: True
        Whether to flatten the resulting array.

    Returns
    -------
    Union[List[int], List[List[int]]]
        The indices of the sorted items, either globally or within each of the Pareto front
        depending on the value of `flatten`.
    """
    remaining = np.arange(X.shape[0])
    indices = []
    num_items = 0

    # Iterate until max_items are reached or there are no items left
    while remaining.size > 0 and (max_items is None or num_items < max_items):
        # Compute the Pareto front and sort the items within
        pareto_mask = pareto_efficient(X[remaining])
        pareto_front = remaining[pareto_mask]
        pareto_order = compute_epsilon_net(X[pareto_front], dim=dim)

        # Add order to the indices
        indices.append(pareto_front[pareto_order].tolist())
        num_items += len(pareto_front)

        # Remove items in the Pareto front from the remaining items
        remaining = remaining[~pareto_mask]

    # Restrict the number of items returned and optionally flatten
    if max_items is not None:
        limit = max_items - sum(len(x) for x in indices[:-1])
        indices[-1] = indices[-1][:limit]
        if not indices[-1]:
            indices = indices[:-1]

    if flatten:
        return [i for ix in indices for i in ix]
    return indices

File Path: syne_tune/optimizer/schedulers/neuralbands/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/neuralbands/networks.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

if torch.cuda.is_available():
    dev = "cuda:0"
else:
    dev = "cpu"
device = torch.device(dev)


class NetworkExploitation(nn.Module):
    def __init__(self, dim: int, hidden_size: int = 100):
        super(NetworkExploitation, self).__init__()
        self.fc1 = nn.Linear(dim, hidden_size)
        self.activate = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)

    def forward(self, x1, b):
        f1 = self.activate(self.fc1(x1))
        f2 = self.activate(self.fc2(b * f1))
        return self.fc3(f2)


class Exploitation:
    def __init__(self, dim: int, lr: float = 0.001, hidden: int = 100):

        """
        the budget-aware network of NeuralBand

        :param dim: number of dimensions of configuration vector
        :param lr: learning rate of Adam
        :param hidden: width of neural network
        """
        self.lr = lr
        self.func = NetworkExploitation(dim, hidden_size=hidden).to(device)

        # store all configuration vectors
        self.x1_list = []
        # store all budgets
        self.b_list = []
        # store all evaluated scores
        self.reward_list = []

        # number of parameters of neural network
        self.total_param = sum(
            p.numel() for p in self.func.parameters() if p.requires_grad
        )
        # size of stored data
        self.data_size = 0

        # sum of all budgets
        self.sum_b = 0.01
        # average over all budgets
        self.average_b = 0.01
        # the maximal budget occured so far
        self.max_b = 0.01

    def add_data(self, x: list, reward: float):
        x1 = torch.tensor(x[0]).float()
        b = torch.tensor(x[1]).float()
        self.x1_list.append(x1)
        self.b_list.append(b)
        self.reward_list.append(reward)
        self.data_size += 1
        self.sum_b += x[1]
        if self.max_b < x[1]:
            self.max_b = x[1]
        self.average_b = float(self.sum_b / self.data_size)

    def predict(self, x: list) -> torch.Tensor:
        x1 = torch.tensor(x[0]).float().to(device)
        b = torch.tensor(x[1]).float().to(device)
        res = self.func(x1, b)
        return res

    def train(self) -> float:
        optimizer = optim.Adam(self.func.parameters(), lr=self.lr)
        length = len(self.reward_list)
        index = np.arange(length)
        np.random.shuffle(index)
        cnt = 0
        tot_loss = 0
        while True:
            batch_loss = 0
            for idx in index:
                x1 = self.x1_list[idx].to(device)
                b = self.b_list[idx].to(device)
                r = self.reward_list[idx]
                optimizer.zero_grad()
                loss = (self.func(x1, b) - r) ** 2
                loss.backward()
                optimizer.step()
                batch_loss += loss.item()
                tot_loss += loss.item()
                cnt += 1
                if cnt >= 500:
                    return tot_loss / cnt
            if batch_loss / length <= 1e-4:
                return batch_loss / length

File Path: syne_tune/optimizer/schedulers/neuralbands/neuralband.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Dict, Optional
import numpy as np

from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import SchedulerDecision, TrialSuggestion
from syne_tune.config_space import cast_config_values
from syne_tune.backend.time_keeper import RealTimeKeeper
from syne_tune.optimizer.schedulers.neuralbands.neuralband_supplement import (
    NeuralbandSchedulerBase,
)


logger = logging.getLogger(__name__)


def is_continue_decision(trial_decision: str) -> bool:
    return trial_decision == SchedulerDecision.CONTINUE


class NeuralbandScheduler(NeuralbandSchedulerBase):
    def __init__(
        self,
        config_space: Dict,
        gamma: float = 0.01,
        nu: float = 0.01,
        step_size: int = 30,
        max_while_loop: int = 100,
        **kwargs,
    ):
        """
        NeuralBand is a neural-bandit based HPO algorithm for the multi-fidelity setting. It uses a budget-aware neural
        network together with a feedback perturbation to efficiently explore the input space across fidelities.
        NeuralBand uses a novel configuration selection criterion to actively choose the configuration in each trial
        and incrementally exploits the knowledge of every past trial.

        :param config_space:
        :param gamma: Control aggressiveness of configuration selection criterion
        :param nu: Control aggressiveness of perturbing feedback for exploration
        :param step_size: How many trials we train network once
        :param max_while_loop: Maximal number of times we can draw a configuration from configuration space
        :param kwargs:
        """
        super(NeuralbandScheduler, self).__init__(
            config_space, step_size, max_while_loop, **kwargs
        )
        self.gamma = gamma
        self.nu = nu

        if self.mode == "min":
            self.max_while_loop = max_while_loop
        else:
            self.max_while_loop = 2

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        self._initialize_searcher()
        # If no time keeper was provided at construction, we use a local
        # one which is started here
        if self.time_keeper is None:
            self.time_keeper = RealTimeKeeper()
            self.time_keeper.start_of_time()
        # For pause/resume schedulers: Can a paused trial be promoted?
        promote_trial_id, extra_kwargs = self._promote_trial()
        if promote_trial_id is not None:
            promote_trial_id = int(promote_trial_id)
            return TrialSuggestion.resume_suggestion(
                trial_id=promote_trial_id, config=extra_kwargs
            )
        # Ask searcher for config of new trial to start
        extra_kwargs["elapsed_time"] = self._elapsed_time()
        trial_id = str(trial_id)

        # active selection criterion
        initial_budget = self.net.max_b
        while_loop_count = 0
        l_t_score = []
        while 1:
            config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
            if config is not None:
                config_encoding = self.hp_ranges.to_ndarray(config)
                predict_score = self.net.predict(
                    (config_encoding, initial_budget)
                ).item()
                l_t_score.append((config, predict_score))
                if self.mode == "min":
                    if (
                        self.currnet_best_score - predict_score
                        > self.gamma
                        * self.currnet_best_score
                        * (1.0 - initial_budget / self.max_t)
                    ):
                        break
                    if while_loop_count > self.max_while_loop:
                        l_t_score = sorted(l_t_score, key=lambda x: x[1])
                        config = l_t_score[0][0]
                        break
                else:
                    if predict_score * 100.0 - self.currnet_best_score > self.gamma * (
                        100.0 - self.currnet_best_score
                    ) * (1.0 - initial_budget / self.max_t):
                        break
                    if while_loop_count > self.max_while_loop:
                        l_t_score = sorted(l_t_score, key=lambda x: x[1], reverse=True)
                        config = l_t_score[0][0]
                        break
                while_loop_count += 1
            else:
                self._searcher_initialized = False
                self._initialize_searcher_new()
                config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
                break

        if config is not None:
            config = cast_config_values(config, self.config_space)
            config = self._on_config_suggest(config, trial_id, **extra_kwargs)
            config = TrialSuggestion.start_suggestion(config)

        return config

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        self._check_result(result)
        trial_id = str(trial.trial_id)
        debug_log = self.searcher.debug_log
        trial_decision = SchedulerDecision.CONTINUE
        if len(result) == 0:
            # An empty dict should just be skipped
            if debug_log is not None:
                logger.info(
                    f"trial_id {trial_id}: Skipping empty dict received "
                    "from reporter"
                )
        else:
            # Time since start of experiment
            time_since_start = self._elapsed_time()
            do_update = False
            config = self._preprocess_config(trial.config)
            cost_and_promotion = (
                self._cost_attr is not None
                and self._cost_attr in result
                and self.does_pause_resume()
            )
            if cost_and_promotion:
                # Trial may have paused/resumed before, so need to add cost
                # offset from these
                cost_offset = self._cost_offset.get(trial_id, 0)
                result[self._total_cost_attr()] = result[self._cost_attr] + cost_offset
            if trial_id not in self._active_trials:
                # Trial not in self._active_trials anymore, so must have been
                # stopped
                trial_decision = SchedulerDecision.STOP
                logger.warning(
                    f"trial_id {trial_id}: Was STOPPED, but receives another "
                    f"report {result}\nThis report is ignored"
                )
            elif not self._active_trials[trial_id]["running"]:
                # Trial must have been paused before
                trial_decision = SchedulerDecision.PAUSE
                logger.warning(
                    f"trial_id {trial_id}: Was PAUSED, but receives another "
                    f"report {result}\nThis report is ignored"
                )
            else:
                # perturb the feedback and train network
                config = trial.config
                config_encoding = self.hp_ranges.to_ndarray(config)
                if "epoch" in result:
                    hp_budget = float(result["epoch"] / self.max_t)
                else:
                    hp_budget = float(result["hp_epoch"] / self.max_t)
                test_loss = result[self.metric]
                # update current best score
                if self.mode == "min":
                    if test_loss < self.currnet_best_score:
                        self.currnet_best_score = test_loss
                    perturbed_loss = test_loss + np.random.normal(
                        0, self.nu * self.currnet_best_score * (1 - hp_budget)
                    )
                else:
                    if test_loss > self.currnet_best_score:
                        self.currnet_best_score = test_loss
                    perturbed_loss = (
                        test_loss
                        + np.random.normal(
                            0, self.nu * (100.0 - test_loss) * (1 - hp_budget)
                        )
                    ) / 100.0
                self.net.add_data((config_encoding, hp_budget), perturbed_loss)

                # train network
                if self.net.data_size % self.train_step_size == 0:
                    predict_score = self.net.predict(
                        (config_encoding, hp_budget)
                    ).item()
                    self.net.train()

                task_info = self.terminator.on_task_report(trial_id, result)
                task_continues = task_info["task_continues"]
                milestone_reached = task_info["milestone_reached"]
                if cost_and_promotion:
                    if milestone_reached:
                        # Trial reached milestone and will pause there: Update
                        # cost offset
                        if self._cost_attr is not None:
                            self._cost_offset[trial_id] = result[
                                self._total_cost_attr()
                            ]
                    elif task_info.get("ignore_data", False):
                        if self._cost_offset[trial_id] > 0:
                            logger.info(
                                f"trial_id {trial_id}: Resumed trial seems to have been "
                                + "started from scratch (no checkpointing?), so we erase "
                                + "the cost offset."
                            )
                        self._cost_offset[trial_id] = 0

                do_update = self._update_searcher(trial_id, config, result, task_info)
                resource = int(result[self._resource_attr])
                self._active_trials[trial_id].update(
                    {
                        "time_stamp": time_since_start,
                        "reported_result": {
                            self.metric: result[self.metric],
                            self._resource_attr: resource,
                        },
                        "keep_case": milestone_reached,
                    }
                )
                if do_update:
                    largest_update_resource = self._active_trials[trial_id][
                        "largest_update_resource"
                    ]
                    if largest_update_resource is None:
                        largest_update_resource = resource - 1
                    assert largest_update_resource <= resource, (
                        f"Internal error (trial_id {trial_id}): "
                        + f"on_trial_result called with resource = {resource}, "
                        + f"but largest_update_resource = {largest_update_resource}"
                    )
                    if resource == largest_update_resource:
                        do_update = False  # Do not update again
                    else:
                        self._active_trials[trial_id][
                            "largest_update_resource"
                        ] = resource
                if not task_continues:
                    if (not self.does_pause_resume()) or resource >= self.max_t:
                        trial_decision = SchedulerDecision.STOP
                        act_str = "Terminating"
                    else:
                        trial_decision = SchedulerDecision.PAUSE
                        act_str = "Pausing"
                    self._cleanup_trial(trial_id)
                if debug_log is not None:
                    if not task_continues:
                        logger.info(
                            f"trial_id {trial_id}: {act_str} evaluation "
                            f"at {resource}"
                        )
                    elif milestone_reached:
                        msg = f"trial_id {trial_id}: Reaches {resource}, continues"
                        next_milestone = task_info.get("next_milestone")
                        if next_milestone is not None:
                            msg += f" to {next_milestone}"
                        logger.info(msg)
            self.searcher.on_trial_result(
                trial_id, config, result=result, update=do_update
            )
        # Extra info in debug mode
        log_msg = f"trial_id {trial_id} (metric = {result[self.metric]:.3f}"
        for k, is_float in ((self._resource_attr, False), ("elapsed_time", True)):
            if k in result:
                if is_float:
                    log_msg += f", {k} = {result[k]:.2f}"
                else:
                    log_msg += f", {k} = {result[k]}"
        log_msg += f"): decision = {trial_decision}"
        logger.debug(log_msg)
        return trial_decision

File Path: syne_tune/optimizer/schedulers/neuralbands/neuralband_supplement.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Dict, Optional
import numpy as np
import torch

from syne_tune.backend.time_keeper import RealTimeKeeper
from syne_tune.optimizer.scheduler import SchedulerDecision, TrialSuggestion
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.schedulers.searchers.searcher import BaseSearcher
from syne_tune.optimizer.schedulers.searchers.searcher_factory import searcher_factory
from syne_tune.config_space import cast_config_values
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.neuralbands.networks import Exploitation
from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler


logger = logging.getLogger(__name__)


def is_continue_decision(trial_decision: str) -> bool:
    return trial_decision == SchedulerDecision.CONTINUE


class NeuralbandSchedulerBase(HyperbandScheduler):
    def __init__(
        self, config_space: Dict, step_size: int, max_while_loop: int, **kwargs
    ):
        """
        Shared base scheduler for NeuralBand.

        :param config_space:
        :param step_size: How many trials we train the network once
        :param max_while_loop: Maximal number of times we can draw a configuration from configuration space
        :param kwargs:
        """

        super(NeuralbandSchedulerBase, self).__init__(config_space, **kwargs)
        self.kwargs = kwargs

        # to encode configuration
        self.hp_ranges = make_hyperparameter_ranges(config_space=self.config_space)
        self.input_dim = self.hp_ranges.ndarray_size

        # initialize neural network
        self.net = Exploitation(dim=self.input_dim)
        self.currnet_best_score = 1.0
        self.train_step_size = step_size
        self.max_while_loop = max_while_loop

    def _initialize_searcher_new(self):
        searcher = self.kwargs["searcher"]
        search_options = self.kwargs.get("search_options")
        if search_options is None:
            search_options = dict()
        else:
            search_options = search_options.copy()
        search_options.update(
            {
                "config_space": self.config_space.copy(),
                "metric": self.metric,
                "points_to_evaluate": self.kwargs.get("points_to_evaluate"),
                "scheduler_mode": self.kwargs["mode"],
                "mode": self.kwargs["mode"],
                "random_seed_generator": self.random_seed_generator,
            }
        )
        if self.max_t is not None:
            search_options["max_epochs"] = self.max_t
        # Subclasses may extend `search_options`
        search_options = self._extend_search_options(search_options)
        # Adjoin scheduler info to search_options, if not already done by
        # subclass (via `_extend_search_options`)
        if "scheduler" not in search_options:
            search_options["scheduler"] = "fifo"
        self.searcher: BaseSearcher = searcher_factory(searcher, **search_options)
        self._searcher_initialized = True

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        self._check_result(result)
        trial_id = str(trial.trial_id)
        debug_log = self.searcher.debug_log
        trial_decision = SchedulerDecision.CONTINUE
        if len(result) == 0:
            # An empty dict should just be skipped
            if debug_log is not None:
                logger.info(
                    f"trial_id {trial_id}: Skipping empty dict received "
                    "from reporter"
                )
        else:
            # Time since start of experiment
            time_since_start = self._elapsed_time()
            do_update = False
            config = self._preprocess_config(trial.config)
            cost_and_promotion = (
                self._cost_attr is not None
                and self._cost_attr in result
                and self.does_pause_resume()
            )
            if cost_and_promotion:
                # Trial may have paused/resumed before, so need to add cost
                # offset from these
                cost_offset = self._cost_offset.get(trial_id, 0)
                result[self._total_cost_attr()] = result[self._cost_attr] + cost_offset
            if trial_id not in self._active_trials:
                # Trial not in self._active_trials anymore, so must have been
                # stopped
                trial_decision = SchedulerDecision.STOP
                logger.warning(
                    f"trial_id {trial_id}: Was STOPPED, but receives another "
                    f"report {result}\nThis report is ignored"
                )
            elif not self._active_trials[trial_id]["running"]:
                # Trial must have been paused before
                trial_decision = SchedulerDecision.PAUSE
                logger.warning(
                    f"trial_id {trial_id}: Was PAUSED, but receives another "
                    f"report {result}\nThis report is ignored"
                )
            else:
                # update neural network
                config = trial.config
                config_encoding = self.hp_ranges.to_ndarray(config)
                if "epoch" in result:
                    hp_budget = float(result["epoch"] / self.max_t)
                else:
                    hp_budget = float(result["hp_epoch"] / self.max_t)
                test_loss = result[self.metric]

                # update current best score
                if self.mode == "min":
                    if test_loss < self.currnet_best_score:
                        self.currnet_best_score = test_loss
                else:
                    if test_loss > self.currnet_best_score:
                        self.currnet_best_score = test_loss

                self.net.add_data((config_encoding, hp_budget), test_loss)

                # train network
                if self.net.data_size % self.train_step_size == 0:
                    self.net.train()

                task_info = self.terminator.on_task_report(trial_id, result)
                task_continues = task_info["task_continues"]
                milestone_reached = task_info["milestone_reached"]
                if cost_and_promotion:
                    if milestone_reached:
                        # Trial reached milestone and will pause there: Update
                        # cost offset
                        if self._cost_attr is not None:
                            self._cost_offset[trial_id] = result[
                                self._total_cost_attr()
                            ]
                    elif task_info.get("ignore_data", False):
                        if self._cost_offset[trial_id] > 0:
                            logger.info(
                                f"trial_id {trial_id}: Resumed trial seems to have been "
                                + "started from scratch (no checkpointing?), so we erase "
                                + "the cost offset."
                            )
                        self._cost_offset[trial_id] = 0

                do_update = self._update_searcher(trial_id, config, result, task_info)
                resource = int(result[self._resource_attr])
                self._active_trials[trial_id].update(
                    {
                        "time_stamp": time_since_start,
                        "reported_result": {
                            self.metric: result[self.metric],
                            self._resource_attr: resource,
                        },
                        "keep_case": milestone_reached,
                    }
                )
                if do_update:
                    largest_update_resource = self._active_trials[trial_id][
                        "largest_update_resource"
                    ]
                    if largest_update_resource is None:
                        largest_update_resource = resource - 1
                    assert largest_update_resource <= resource, (
                        f"Internal error (trial_id {trial_id}): "
                        + f"on_trial_result called with resource = {resource}, "
                        + f"but largest_update_resource = {largest_update_resource}"
                    )
                    if resource == largest_update_resource:
                        do_update = False  # Do not update again
                    else:
                        self._active_trials[trial_id][
                            "largest_update_resource"
                        ] = resource
                if not task_continues:
                    if (not self.does_pause_resume()) or resource >= self.max_t:
                        trial_decision = SchedulerDecision.STOP
                        act_str = "Terminating"
                    else:
                        trial_decision = SchedulerDecision.PAUSE
                        act_str = "Pausing"
                    self._cleanup_trial(trial_id)
                if debug_log is not None:
                    if not task_continues:
                        logger.info(
                            f"trial_id {trial_id}: {act_str} evaluation "
                            f"at {resource}"
                        )
                    elif milestone_reached:
                        msg = f"trial_id {trial_id}: Reaches {resource}, continues"
                        next_milestone = task_info.get("next_milestone")
                        if next_milestone is not None:
                            msg += f" to {next_milestone}"
                        logger.info(msg)
            self.searcher.on_trial_result(
                trial_id, config, result=result, update=do_update
            )
        # Extra info in debug mode
        log_msg = f"trial_id {trial_id} (metric = {result[self.metric]:.3f}"
        for k, is_float in ((self._resource_attr, False), ("elapsed_time", True)):
            if k in result:
                if is_float:
                    log_msg += f", {k} = {result[k]:.2f}"
                else:
                    log_msg += f", {k} = {result[k]}"
        log_msg += f"): decision = {trial_decision}"
        logger.debug(log_msg)
        return trial_decision


class NeuralbandEGreedyScheduler(NeuralbandSchedulerBase):
    def __init__(
        self,
        config_space: Dict,
        epsilon: float = 0.1,
        step_size: int = 30,
        max_while_loop: int = 100,
        **kwargs,
    ):
        """
        We combine the epsilon-greedy strategy with NeuralBand, where, with probability epsilon,
        we select configurations either randomly or, with probability 1 - epsilon, greedily by
        maximizing the acquisition function in each trial.

        :param config_space:
        :param epsilon:
        :param step_size:
        :param max_while_loop:
        :param kwargs:
        """
        super(NeuralbandEGreedyScheduler, self).__init__(
            config_space, step_size, max_while_loop, **kwargs
        )
        self.epsilon = epsilon

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        self._initialize_searcher()
        # If no time keeper was provided at construction, we use a local
        # one which is started here
        if self.time_keeper is None:
            self.time_keeper = RealTimeKeeper()
            self.time_keeper.start_of_time()
        # For pause/resume schedulers: Can a paused trial be promoted?
        promote_trial_id, extra_kwargs = self._promote_trial()
        if promote_trial_id is not None:
            promote_trial_id = int(promote_trial_id)
            return TrialSuggestion.resume_suggestion(
                trial_id=promote_trial_id, config=extra_kwargs
            )
        # Ask searcher for config of new trial to start
        extra_kwargs["elapsed_time"] = self._elapsed_time()
        trial_id = str(trial_id)

        # epsilon greedy selection criterion
        initial_budget = self.net.average_b
        while_loop_count = 0
        l_t_score = []
        epsilon = np.random.binomial(1, self.epsilon)
        if epsilon:
            config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
        else:
            while 1:
                config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
                if config is not None:
                    config_encoding = self.hp_ranges.to_ndarray(config)
                    predict_score = self.net.predict(
                        (config_encoding, initial_budget)
                    ).item()
                    l_t_score.append((config, predict_score))
                    while_loop_count += 1
                    if self.mode == "min":
                        if while_loop_count > self.max_while_loop:
                            l_t_score = sorted(l_t_score, key=lambda x: x[1])
                            config = l_t_score[0][0]
                            break
                    else:
                        if while_loop_count > self.max_while_loop:
                            l_t_score = sorted(
                                l_t_score, key=lambda x: x[1], reverse=True
                            )
                            config = l_t_score[0][0]
                            break
                else:
                    self._searcher_initialized = False
                    self._initialize_searcher_new()
                    config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
                    break

        if config is not None:
            config = cast_config_values(config, self.config_space)
            config = self._on_config_suggest(config, trial_id, **extra_kwargs)
            config = TrialSuggestion.start_suggestion(config)
        return config


class NeuralbandTSScheduler(NeuralbandSchedulerBase):
    def __init__(
        self,
        config_space: Dict,
        lamdba: float = 0.1,
        nu: float = 0.01,
        step_size: int = 30,
        max_while_loop: int = 100,
        **kwargs,
    ):
        """
        We combine Thompson Sampling strategy with NeuralBand, where configurations are selected based on the
        criterion described by [1].

        Reference: [1] ZHANG, Weitong, et al. "Neural Thompson Sampling." International Conference on Learning
        Representations. 2020.

        :param config_space:
        :param lamdba: Regularization term of gradient vector;
        :param nu: Control aggressiveness of exploration.
        :param step_size:
        :param max_while_loop:
        :param kwargs:
        """
        super(NeuralbandTSScheduler, self).__init__(
            config_space, step_size, max_while_loop, **kwargs
        )
        self.lamdba = lamdba
        self.nu = nu
        # graident vector
        self.U = self.lamdba * torch.ones((self.net.total_param,))

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        self._initialize_searcher()
        # If no time keeper was provided at construction, we use a local
        # one which is started here
        if self.time_keeper is None:
            self.time_keeper = RealTimeKeeper()
            self.time_keeper.start_of_time()
        # For pause/resume schedulers: Can a paused trial be promoted?
        promote_trial_id, extra_kwargs = self._promote_trial()
        if promote_trial_id is not None:
            promote_trial_id = int(promote_trial_id)
            return TrialSuggestion.resume_suggestion(
                trial_id=promote_trial_id, config=extra_kwargs
            )
        # Ask searcher for config of new trial to start
        extra_kwargs["elapsed_time"] = self._elapsed_time()
        trial_id = str(trial_id)

        # TS selection criterion
        initial_budget = self.net.average_b
        while_loop_count = 0
        l_t_score = []
        while 1:
            config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
            if config is not None:
                config_encoding = self.hp_ranges.to_ndarray(config)
                predict_value = self.net.predict((config_encoding, initial_budget))
                self.net.func.zero_grad()
                predict_value.backward(retain_graph=True)
                g = torch.cat(
                    [p.grad.flatten().detach() for p in self.net.func.parameters()]
                )
                cb = (
                    torch.sqrt(torch.sum(self.lamdba * g * g / self.U)).item() * self.nu
                )
                mean_value = torch.tensor(predict_value.item())
                predict_score = torch.normal(mean=mean_value, std=cb).item()
                l_t_score.append((config, predict_score))
                if self.mode == "min":
                    if while_loop_count > self.max_while_loop:
                        l_t_score = sorted(l_t_score, key=lambda x: x[1])
                        config = l_t_score[0][0]
                        break
                else:
                    if while_loop_count > (self.max_while_loop / 20):
                        l_t_score = sorted(l_t_score, key=lambda x: x[1], reverse=True)
                        config = l_t_score[0][0]
                        break
                while_loop_count += 1
            else:
                self._searcher_initialized = False
                self._initialize_searcher_new()
                config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
                break

        if config is not None:
            config = cast_config_values(config, self.config_space)
            config = self._on_config_suggest(config, trial_id, **extra_kwargs)
            config = TrialSuggestion.start_suggestion(config)
        return config


class NeuralbandUCBScheduler(NeuralbandSchedulerBase):
    def __init__(
        self,
        config_space: Dict,
        lamdba: float = 0.01,
        nu: float = 0.01,
        step_size: int = 30,
        max_while_loop: int = 100,
        **kwargs,
    ):

        """
        We combine Upper Confidence Bound with NeuralBand, where configurations are selected based on the
        upper confidence bound criterion following [1].

        Reference: [1] Zhou, Dongruo, Lihong Li, and Quanquan Gu. "Neural contextual bandits with ucb-based
        exploration." International Conference on Machine Learning. PMLR, 2020.

        :param config_space:
        :param lamdba: Regularization term of gradient vector
        :param nu: Control aggressiveness of exploration
        :param step_size:
        :param max_while_loop:
        :param kwargs:
        """

        super(NeuralbandUCBScheduler, self).__init__(
            config_space, step_size, max_while_loop, **kwargs
        )
        self.lamdba = lamdba
        self.nu = nu
        # graident vector
        self.U = self.lamdba * torch.ones((self.net.total_param,))

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        self._initialize_searcher()
        # If no time keeper was provided at construction, we use a local
        # one which is started here
        if self.time_keeper is None:
            self.time_keeper = RealTimeKeeper()
            self.time_keeper.start_of_time()
        # For pause/resume schedulers: Can a paused trial be promoted?
        promote_trial_id, extra_kwargs = self._promote_trial()
        if promote_trial_id is not None:
            promote_trial_id = int(promote_trial_id)
            return TrialSuggestion.resume_suggestion(
                trial_id=promote_trial_id, config=extra_kwargs
            )
        # Ask searcher for config of new trial to start
        extra_kwargs["elapsed_time"] = self._elapsed_time()
        trial_id = str(trial_id)

        # UCB selection criterion
        initial_budget = self.net.average_b
        while_loop_count = 0
        l_t_score = []
        while 1:
            config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
            if config is not None:
                config_encoding = self.hp_ranges.to_ndarray(config)
                predict_value = self.net.predict((config_encoding, initial_budget))
                self.net.func.zero_grad()
                predict_value.backward(retain_graph=True)
                g = torch.cat(
                    [p.grad.flatten().detach() for p in self.net.func.parameters()]
                )
                cb = (
                    torch.sqrt(torch.sum(self.lamdba * g * g / self.U)).item() * self.nu
                )
                predict_score = predict_value.item() + cb
                l_t_score.append((config, predict_score))
                while_loop_count += 1
                if self.mode == "min":
                    if while_loop_count > self.max_while_loop:
                        l_t_score = sorted(l_t_score, key=lambda x: x[1])
                        config = l_t_score[0][0]
                        break
                else:
                    if while_loop_count > self.max_while_loop:
                        l_t_score = sorted(l_t_score, key=lambda x: x[1], reverse=True)
                        config = l_t_score[0][0]
                        break
            else:
                self._searcher_initialized = False
                self._initialize_searcher_new()
                config = self.searcher.get_config(**extra_kwargs, trial_id=trial_id)
                break
        if config is not None:
            config = cast_config_values(config, self.config_space)
            config = self._on_config_suggest(config, trial_id, **extra_kwargs)
            config = TrialSuggestion.start_suggestion(config)
        return config

File Path: syne_tune/optimizer/schedulers/pbt.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import copy
import logging
import math
import numpy as np

from dataclasses import dataclass
from collections import deque
from typing import Callable, Dict, List, Optional, Tuple

from syne_tune.config_space import Domain, Integer, Float, FiniteRange
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import SchedulerDecision, TrialSuggestion
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.config_space import cast_config_values
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
    Integer as DA_Integer,
    Boolean as DA_Boolean,
    filter_by_key,
    String as DA_String,
    Float as DA_Float,
)

logger = logging.getLogger(__name__)


@dataclass
class PBTTrialState:
    """Internal PBT state tracked per-trial."""

    trial: Trial
    last_score: float = None
    last_checkpoint: int = None
    last_perturbation_time: int = 0
    stopped: bool = False


_ARGUMENT_KEYS = {
    "resource_attr",
    "population_size",
    "perturbation_interval",
    "quantile_fraction",
    "resample_probability",
    "log_config",
}

_DEFAULT_OPTIONS = {
    "resource_attr": "time_total_s",
    "population_size": 4,
    "perturbation_interval": 60.0,
    "quantile_fraction": 0.25,
    "resample_probability": 0.25,
    "log_config": True,
}

_CONSTRAINTS = {
    "resource_attr": DA_String(),
    "population_size": DA_Integer(1, None),
    "perturbation_interval": DA_Float(0.01, None),
    "quantile_fraction": DA_Float(0.0, 0.5),
    "resample_probability": DA_Float(0.0, 1.0),
    "log_config": DA_Boolean(),
}


class PopulationBasedTraining(FIFOScheduler):
    """
    Implements the Population Based Training (PBT) algorithm. This is an adapted version of
    the Raytune implementation for Syne Tune:
     https://docs.ray.io/en/latest/tune/tutorials/tune-advanced-tutorial.html

    PBT was original presented in the following paper:
    https://deepmind.com/blog/population-based-training-neural-networks


    Population based training (PBT) maintains a population of neural network models spread across
    an asynchronous set of workers and dynamically adjust their hyperparameters during training.
    Every time a worker reaches a user-defined milestone, it returns the performance of the currently
    evaluated network. If the network is within the top percentile of the population,
    the worker resumes its training until the next milestone. If not, PBT selects a neural network
    from the top percentile uniformly at random. The worker now continues with the latest checkpoint
    of this new neural network but mutates the hyperparameters.

    The mutation happens as following. For each hyperparameter, we either resample
    its value uniformly at random, or otherwise increment (multiply by 1.2) or
    decrement (multiply by 0.8) the value (probability 0.5 each). For categorical
    hyperparameters, the value is always resampled uniformly.

    Note: While this is implemented as child of :class:`FIFOScheduler`, we
    require `searcher='random'` (default), since the current code only supports
    a random searcher.

    Parameters
    ----------
    config_space: dict
        Configuration space for trial evaluation function
    search_options : dict
        If searcher is str, these arguments are passed to searcher_factory.
    metric : str
        Name of metric to optimize, key in result's obtained via
        `on_trial_result`
    mode : str
        Mode to use for the metric given, can be 'min' or 'max', default to 'min'.
    points_to_evaluate: list[dict] or None
        See :class:`FIFOScheduler`
    random_seed : int
        Master random seed. Generators used in the scheduler or searcher are
        seeded using `RandomSeedGenerator`. If not given, the master random
        seed is drawn at random here.
    max_t : int (optional)
        See :class:`FIFOScheduler`. This is mandatory here. If not given, we
        try to infer it.
    max_resource_attr : str (optional)
        See :class:`FIFOScheduler`.
    population_size : int)
        Defines the size of the population.
    perturbation_interval : float
        Models will be considered for perturbation at this interval of `time_attr`.
        Note that perturbation incurs checkpoint overhead, so you shouldn't set
        this to be too frequent.
    quantile_fraction : float
        Parameters are transferred from the top `quantile_fraction` fraction of
        trials to the bottom `quantile_fraction` fraction. Needs to be between
        0 and 0.5. Setting it to 0 essentially implies doing no exploitation at
        all.
    resample_probability : float
        The probability of resampling from the original distribution when
        applying `hyperparam_mutations`. If not resampled, the value will be
        perturbed by a factor of 1.2 or 0.8 if continuous, or changed to an
        adjacent value if discrete.
    custom_explore_fn : callable
        You can also specify a custom exploration function. This function is
        invoked as `f(config)` after built-in perturbations from
        `hyperparam_mutations` are applied, and should return `config` updated
        as needed. You must specify at least one of `hyperparam_mutations` or
        `custom_explore_fn`.
    log_config (bool): Whether to log the ray config of each model to\
        `local_dir` at each exploit. Allows config schedule to be reconstructed.
    time_keeper : TimeKeeper
        See :class:`FIFOScheduler`
    """

    def __init__(
        self,
        config_space: Dict,
        custom_explore_fn: Optional[Callable[[Dict], Dict]] = None,
        **kwargs,
    ):
        # The current implementation only supports a random searcher
        searcher = kwargs.get("searcher")
        if searcher is not None:
            assert (
                isinstance(searcher, str) and searcher == "random"
            ), "PopulationBasedTraining only supports searcher='random' for now"
        kwargs = check_and_merge_defaults(
            kwargs, set(), _DEFAULT_OPTIONS, _CONSTRAINTS, dict_name="scheduler_options"
        )
        self._resource_attr = kwargs["resource_attr"]
        self._population_size = kwargs["population_size"]
        self._perturbation_interval = kwargs["perturbation_interval"]
        self._quantile_fraction = kwargs["quantile_fraction"]
        self._resample_probability = kwargs["resample_probability"]
        self._custom_explore_fn = custom_explore_fn
        self._log_config = kwargs["log_config"]
        # Superclass constructor
        super().__init__(config_space, **filter_by_key(kwargs, _ARGUMENT_KEYS))
        assert self.max_t is not None, (
            "Either max_t must be specified, or it has to be specified as "
            + "config_space['epochs'], config_space['max_t'], "
            + "config_space['max_epochs']"
        )

        self._metric_op = 1.0 if self.mode == "max" else -1.0
        self._trial_state = {}
        self._next_perturbation_sync = self._perturbation_interval
        self._trial_decisions_stack = deque()
        self._checkpointing_history = []
        self._num_checkpoints = 0
        self._num_perturbations = 0
        self._random_state = np.random.RandomState(self.random_seed_generator())

    def on_trial_add(self, trial: Trial):
        self._trial_state[trial.trial_id] = PBTTrialState(trial=trial)

    def _get_trial_id_to_continue(self, trial: Trial):
        """
        Determine which trial to continue. Following the original PBT formulation if the trial is not in the top %n
        percent, we sample a trial uniformly at random from the upper quantile.
        :param trial:
        :return: int that specifies which trial should be continued
        """
        lower_quantile, upper_quantile = self._quantiles()
        # If we are not in the upper quantile, we pause:
        if trial.trial_id in lower_quantile:
            logger.debug(f"Trial {trial.trial_id} is in lower quantile")
            # sample random trial from upper quantile
            trial_id_to_clone = self._random_state.choice(upper_quantile)
            assert trial.trial_id is not trial_id_to_clone
            return trial_id_to_clone
        else:
            return trial.trial_id

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        if self._resource_attr not in result:
            time_missing_msg = (
                f"Cannot find resource_attr {self._resource_attr} "
                f"in trial result {result}. Make sure that this "
                "attribute is returned in the "
                "results of your Trainable."
            )
            raise RuntimeError(time_missing_msg)
        if self.metric not in result:
            metric_missing_msg = (
                f"Cannot find metric {self.metric} in trial result {result}. "
                "Make sure that this attribute is returned "
                "in the "
                "results of your Trainable."
            )
            raise RuntimeError(metric_missing_msg)

        cost = result[self._resource_attr]
        state = self._trial_state[trial.trial_id]

        # Stop if we reached the maximum budget of this configuration
        if cost >= self.max_t:
            state.stopped = True
            return SchedulerDecision.STOP

        # Continue training if perturbation interval has not been reached yet.
        if cost - state.last_perturbation_time < self._perturbation_interval:
            return SchedulerDecision.CONTINUE

        self._save_trial_state(state, cost, result)

        state.last_perturbation_time = cost

        trial_id_to_continue = self._get_trial_id_to_continue(trial)

        # bookkeeping for debugging reasons
        self._checkpointing_history.append(
            (trial.trial_id, trial_id_to_continue, self._elapsed_time())
        )

        if trial_id_to_continue == trial.trial_id:
            # continue current trial
            return SchedulerDecision.CONTINUE
        else:
            state.stopped = True
            # exploit step
            trial_to_clone = self._trial_state[trial_id_to_continue].trial

            # explore step
            config = self._explore(trial_to_clone.config)
            self._trial_decisions_stack.append((trial_id_to_continue, config))
            return SchedulerDecision.PAUSE

    def _save_trial_state(self, state: PBTTrialState, time: int, result: Dict) -> Dict:
        """Saves necessary trial information when result is received.
        Args:
            state (PBTTrialState): The state object for the trial.
            time (int): The current timestep of the trial.
            result (dict): The trial's result dictionary.
        """

        # This trial has reached its perturbation interval.
        # Record new state in the state object.
        score = self._metric_op * result[self.metric]
        state.last_score = score
        state.last_train_time = time
        state.last_result = result

        return score

    def _quantiles(self) -> Tuple[List[Trial], List[Trial]]:
        """Returns trials in the lower and upper `quantile` of the population.

        If there is not enough data to compute this, returns empty lists.
        """
        trials = []
        for trial, state in self._trial_state.items():
            if not state.stopped and state.last_score is not None:
                trials.append(trial)

        trials.sort(key=lambda t: self._trial_state[t].last_score)

        if len(trials) <= 1:
            return [], []
        else:
            num_trials_in_quantile = int(
                math.ceil(len(trials) * self._quantile_fraction)
            )
            if num_trials_in_quantile > len(trials) / 2:
                num_trials_in_quantile = int(math.floor(len(trials) / 2))
            return (trials[:num_trials_in_quantile], trials[-num_trials_in_quantile:])

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        # If no time keeper was provided at construction, we use a local
        # one which is started here
        if len(self._trial_decisions_stack) == 0:
            # If our stack is empty, we simply start a new random configuration.
            return super()._suggest(trial_id)
        else:
            assert self.time_keeper is not None  # Sanity check
            trial_id_to_continue, config = self._trial_decisions_stack.pop()
            config["elapsed_time"] = self._elapsed_time()
            config = cast_config_values(
                config=config, config_space=self.searcher.config_space
            )
            config["trial_id"] = trial_id
            return TrialSuggestion.start_suggestion(
                config=config, checkpoint_trial_id=trial_id_to_continue
            )

    def _explore(self, config: Dict) -> Dict:
        """Return a config perturbed as specified.

        Args:
            config (dict): Original hyperparameter configuration from the cloned trial
        """

        new_config = copy.deepcopy(config)

        self._num_perturbations += 1

        if self._custom_explore_fn:
            new_config = self._custom_explore_fn(new_config)
            assert (
                new_config is not None
            ), "Custom explore fn failed to return new config"
            return new_config

        for key, hp_range in self.config_space.items():
            if isinstance(hp_range, Domain):
                # For `Categorical`, all values have the same distance from each
                # other, so we can always resample uniformly
                is_numerical = (
                    isinstance(hp_range, Float)
                    or isinstance(hp_range, Integer)
                    or isinstance(hp_range, FiniteRange)
                )
                if (
                    not is_numerical
                ) or self._random_state.rand() < self._resample_probability:
                    new_config[key] = hp_range.sample(
                        size=1, random_state=self._random_state
                    )
                else:
                    multiplier = 1.2 if self._random_state.rand() > 0.5 else 0.8
                    new_config[key] = hp_range.cast(
                        np.clip(
                            config[key] * multiplier, hp_range.lower, hp_range.upper
                        )
                    )

        # Only log mutated hyperparameters and not entire config.
        old_hparams = {k: v for k, v in config.items() if k in self.config_space}
        new_hparams = {k: v for k, v in new_config.items() if k in self.config_space}
        logger.debug(f"[explore] perturbed config from {old_hparams} -> {new_hparams}")

        return new_config

File Path: syne_tune/optimizer/schedulers/random_seeds.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np


class RandomSeedGenerator:
    def __init__(self, master_seed: int):
        self._random_state = np.random.RandomState(master_seed)

    def __call__(self) -> int:
        return self._random_state.randint(0, 2**32)

File Path: syne_tune/optimizer/schedulers/ray_scheduler.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, List
import logging

from syne_tune.optimizer.scheduler import TrialScheduler, TrialSuggestion
from syne_tune.backend.trial_status import Trial
import syne_tune.config_space as sp

__all__ = ["RayTuneScheduler"]

logger = logging.getLogger(__name__)


class RayTuneScheduler(TrialScheduler):
    from ray.tune.schedulers import FIFOScheduler as RT_FIFOScheduler
    from ray.tune.suggest import Searcher as RT_Searcher

    class RandomSearch(RT_Searcher):
        def __init__(
            self, config_space: Dict, points_to_evaluate: List[Dict], mode: str
        ):
            super().__init__(mode=mode)
            self.config_space = config_space
            self._points_to_evaluate = points_to_evaluate

        def _next_initial_config(self) -> Optional[Dict]:
            if self._points_to_evaluate:
                return self._points_to_evaluate.pop(0)
            else:
                return None  # No more initial configs

        def suggest(self, trial_id: str) -> Optional[Dict]:
            config = self._next_initial_config()
            if config is None:
                config = {
                    k: v.sample() if hasattr(v, "sample") else v
                    for k, v in self.config_space.items()
                }
            return config

        def on_trial_complete(
            self, trial_id: str, result: Optional[Dict] = None, error: bool = False
        ):
            pass

    def __init__(
        self,
        config_space: Dict,
        ray_scheduler=None,
        ray_searcher: Optional[RT_Searcher] = None,
        points_to_evaluate: Optional[List[Dict]] = None,
    ):
        """
        Allow to use Ray scheduler and searcher. Any searcher/scheduler should
        work, except such which need access to TrialRunner (e.g., PBT), this
        feature is not implemented yet.

        If `ray_searcher` is not given (defaults to random searcher), initial
        configurations to evaluate can be passed in `points_to_evaluate`. If
        `ray_searcher` is given, this argument is ignored (needs to be passed
        to `ray_searcher` at construction). Note: Use

        syne_tune.optimizer.schedulers.searchers.impute_points_to_evaluate

        in order to preprocess `points_to_evaluate` specified by the user or
        the benchmark.

        :param config_space: configuration of the sampled space, for instance
        ```python
        hyperparameters = {
            "steps": max_steps,
            "width": uniform(0, 20),
            "height": uniform(-100, 100),
            "activation": choice(["relu", "tanh"])
        }
        ```
        :param ray_scheduler: Ray scheduler, defaults to FIFO scheduler
        :param ray_searcher: Ray searcher, defaults to random search
        :param points_to_evaluate: See above
        """
        super().__init__(config_space)
        if ray_scheduler is None:
            ray_scheduler = self.RT_FIFOScheduler()
        self.scheduler = ray_scheduler

        if ray_searcher is not None:
            self.mode = ray_searcher.mode
        else:
            if hasattr(ray_scheduler, "_mode"):
                self.mode = ray_scheduler._mode
            else:
                self.mode = "min"

        if ray_searcher is None:
            ray_searcher = self.RandomSearch(
                config_space=self.convert_config_space(config_space),
                points_to_evaluate=points_to_evaluate,
                mode=self.mode,
            )

        elif points_to_evaluate is not None:
            logger.warning(
                "points_to_evaluate specified here will not be used. Pass this"
                " argument when creating ray_searcher"
            )
        self.searcher = ray_searcher
        # todo this one is not implemented yet, PBT would require it
        self.trial_runner_wrapper = None

        if self.searcher.metric is not None and self.scheduler.metric is not None:
            assert (
                self.scheduler.metric == self.searcher.metric
            ), "searcher and scheduler must have the same metric."

    def on_trial_add(self, trial: Trial):
        self.scheduler.on_trial_add(
            trial_runner=self.trial_runner_wrapper,
            trial=trial,
        )

    def on_trial_error(self, trial: Trial):
        self.scheduler.on_trial_error(
            trial_runner=self.trial_runner_wrapper,
            trial=trial,
        )

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        self._check_valid_result(result=result)
        self.searcher.on_trial_result(trial_id=str(trial.trial_id), result=result)
        return self.scheduler.on_trial_result(
            trial_runner=self.trial_runner_wrapper, trial=trial, result=result
        )

    def on_trial_complete(self, trial: Trial, result: Dict):
        self._check_valid_result(result=result)
        self.searcher.on_trial_complete(trial_id=str(trial.trial_id), result=result)
        self.scheduler.on_trial_complete(
            trial_runner=self.trial_runner_wrapper, trial=trial, result=result
        )

    def _check_valid_result(self, result: Dict):
        for m in self.metric_names():
            assert m in result, (
                f"metric {m} is not present in reported results {result},"
                f" the metrics present when calling `report(...)` in your training functions should"
                f" be identical to the ones passed as metrics/time_attr to the scheduler and searcher"
            )

    def on_trial_remove(self, trial: Trial):
        return self.scheduler.on_trial_remove(
            trial_runner=self.trial_runner_wrapper, trial=trial
        )

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        config = self.searcher.suggest(trial_id=str(trial_id))
        return TrialSuggestion.start_suggestion(config)

    def metric_names(self) -> List[str]:
        return [self.scheduler.metric]

    def metric_mode(self) -> str:
        return self.mode

    @staticmethod
    def convert_config_space(config_space):
        """
        Converts config_space from our type to the one of Ray Tune.

        Note: `randint(lower, upper)` in Ray Tune has exclusive `upper`, while
        this is inclusive for us. On the other hand, `lograndint(lower, upper)`
        has inclusive `upper` in Ray Tune as well.

        :param config_space:
        :return:
        """
        import ray.tune.sample as ray_sp

        ray_config_space = dict()
        for name, hp_range in config_space.items():
            assert not isinstance(
                hp_range, sp.FiniteRange
            ), f"'{name}' has type FiniteRange, not supported by Ray Tune"
            if isinstance(hp_range, sp.Domain):
                cls_mapping = {
                    sp.Integer: ray_sp.Integer,
                    sp.Float: ray_sp.Float,
                    sp.LogUniform: ray_sp.LogUniform,
                    sp.Categorical: ray_sp.Categorical,
                    sp.Normal: ray_sp.Normal,
                }
                sampler_mapping = {
                    sp.Integer._Uniform: ray_sp.Integer._Uniform,
                    sp.Integer._LogUniform: ray_sp.Integer._LogUniform,
                    sp.Float._Uniform: ray_sp.Float._Uniform,
                    sp.Float._LogUniform: ray_sp.Float._LogUniform,
                    sp.Categorical._Uniform: ray_sp.Categorical._Uniform,
                    sp.Float._Normal: ray_sp.Float._Normal,
                }

                ray_cls = cls_mapping[type(hp_range)]
                domain_kwargs = {
                    k: v for k, v in hp_range.__dict__.items() if k != "sampler"
                }

                # Note: `tune.randint` has exclusive upper while we have inclusive
                if isinstance(hp_range, sp.Integer):
                    domain_kwargs["upper"] = domain_kwargs["upper"] + 1

                ray_domain = ray_cls(**domain_kwargs)
                ray_sampler = sampler_mapping[type(hp_range.get_sampler())](
                    **hp_range.get_sampler().__dict__
                )
                ray_domain.set_sampler(ray_sampler)
                ray_config_space[name] = ray_domain
            else:
                ray_config_space[name] = hp_range

        return ray_config_space

File Path: syne_tune/optimizer/schedulers/searchers/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# TODO wildcard import should be avoided
import logging

from syne_tune.try_import import try_import_gpsearchers_message
from syne_tune.optimizer.schedulers.searchers.searcher import *  # noqa: F401
from syne_tune.optimizer.schedulers.searchers.searcher_factory import *  # noqa: F401

try:
    from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import *  # noqa: F401
    from syne_tune.optimizer.schedulers.searchers.gp_multifidelity_searcher import *  # noqa: F401
except ImportError:
    logging.info(try_import_gpsearchers_message())

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/common.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Union, Dict, Optional, Callable, List
from dataclasses import dataclass
import numpy as np


INTERNAL_METRIC_NAME = "active_metric"

INTERNAL_CONSTRAINT_NAME = "constraint_metric"

INTERNAL_COST_NAME = "cost_metric"


def dictionarize_objective(x):
    return {INTERNAL_METRIC_NAME: x}


Hyperparameter = Union[str, int, float]

Configuration = Dict[str, Hyperparameter]


# Type of `filter_observed_data`, which is (optionally) used to filter the
# observed data in `TuningJobState.trials_evaluations` when determining
# the best config (incumbent) or the exclusion list. One use case is
# warm-starting, where the observed data can come from a number of tasks, only
# one of which is active.
ConfigurationFilter = Callable[[Configuration], bool]


MetricValues = Union[float, Dict[str, float]]


@dataclass
class TrialEvaluations:
    """
    For each fixed k, `metrics[k]` is either a single value or a dict. The
    latter is used, for example, for multi-fidelity schedulers, where
    `metrics[k][str(r)]` is the value at resource level r.

    """

    trial_id: str
    metrics: Dict[str, MetricValues]

    def num_cases(self, metric_name: str = INTERNAL_METRIC_NAME) -> int:
        metric_vals = self.metrics.get(metric_name)
        if metric_vals is None:
            return 0
        elif isinstance(metric_vals, dict):
            return len(metric_vals)
        else:
            return 1

    def _map_value_for_matching(
        self, value: MetricValues
    ) -> (Optional[List[str]], np.ndarray):
        if isinstance(value, dict):
            keys = list(sorted(value.keys()))
            vals = np.array(value[k] for k in keys)
        else:
            keys = None
            vals = np.array([value])
        return keys, vals

    def __eq__(self, other) -> bool:
        if not isinstance(other, TrialEvaluations):
            return False
        if self.trial_id != other.trial_id:
            return False
        if set(self.metrics.keys()) != set(other.metrics.keys()):
            return False
        for name, value in self.metrics.items():
            keys, vals = self._map_value_for_matching(value)
            keys_other, vals_other = self._map_value_for_matching(other.metrics[name])
            if keys != keys_other or (not np.allclose(vals, vals_other)):
                return False
        return True


class PendingEvaluation:
    """
    Maintains information for pending candidates (i.e. candidates which have
    been queried for labeling, but target feedback has not yet been obtained.

    The minimum information is the candidate which has been queried.
    """

    def __init__(self, trial_id: str, resource: Optional[int] = None):
        self._trial_id = trial_id
        self._resource = resource

    @property
    def trial_id(self) -> str:
        return self._trial_id

    @property
    def resource(self) -> Optional[int]:
        return self._resource


class FantasizedPendingEvaluation(PendingEvaluation):
    """
    Here, latent target values are integrated out by Monte Carlo samples,
    also called "fantasies".

    """

    def __init__(
        self,
        trial_id: str,
        fantasies: Dict[str, np.ndarray],
        resource: Optional[int] = None,
    ):
        super().__init__(trial_id, resource)
        fantasy_sizes = [fantasy_values.size for fantasy_values in fantasies.values()]
        assert all(
            fantasy_size > 0 for fantasy_size in fantasy_sizes
        ), "fantasies must be non-empty"
        assert len(set(fantasy_sizes)) == 1, "fantasies must all have the same length"
        self._fantasies = fantasies.copy()

    @property
    def fantasies(self):
        return self._fantasies

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/config_ext.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Tuple
import copy

from syne_tune.config_space import randint
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)

RESOURCE_ATTR_PREFIX = "RESOURCE_ATTR_"


class ExtendedConfiguration:
    """
    This class facilitates handling extended configs, which consist of a normal
    config and a resource attribute.

    The config space hp_ranges is extended by an additional resource
    attribute. Note that this is not a hyperparameter we optimize over,
    but it is under the control of the scheduler.
    Its allowed range is [1, resource_attr_range[1]], which can be larger than
    [resource_attr_range[0], resource_attr_range[1]]. This is because extended
    configs with resource values outside of resource_attr_range may arise (for
    example, in the early stopping context, we may receive data from
    epoch < resource_attr_range[0]).

    """

    def __init__(
        self,
        hp_ranges: HyperparameterRanges,
        resource_attr_key: str,
        resource_attr_range: Tuple[int, int],
    ):
        assert resource_attr_range[0] >= 1
        assert resource_attr_range[1] >= resource_attr_range[0]
        self.hp_ranges = hp_ranges
        self.resource_attr_key = resource_attr_key
        self.resource_attr_range = resource_attr_range
        # Extended configuration space including resource attribute
        config_space_ext = copy.deepcopy(hp_ranges.config_space)
        self.resource_attr_name = RESOURCE_ATTR_PREFIX + resource_attr_key
        # Allowed range: [1, resource_attr_range[1]]
        assert self.resource_attr_name not in config_space_ext, (
            f"key = {self.resource_attr_name} is reserved, but appears in "
            + f"config_space = {list(config_space_ext.keys())}"
        )
        config_space_ext[self.resource_attr_name] = randint(
            lower=1, upper=resource_attr_range[1]
        )
        self.hp_ranges_ext = type(hp_ranges)(
            config_space_ext, name_last_pos=self.resource_attr_name
        )

    def get(self, config: Configuration, resource: int) -> Configuration:
        """
        Create extended config with resource added.

        :param config:
        :param resource:
        :return: Extended config
        """
        values = copy.copy(config)
        values[self.resource_attr_name] = resource
        return values

    def remove_resource(self, config_ext: Configuration) -> Configuration:
        """
        Strips away resource attribute and returns normal config. If
        `config_ext` is already normal, it is returned as is.

        :param config_ext: Extended config
        :return: config_ext without resource attribute
        """
        if self.resource_attr_name in config_ext:
            config = {
                k: v for k, v in config_ext.items() if k != self.resource_attr_name
            }
        else:
            config = config_ext
        return config

    def split(self, config_ext: Configuration) -> (Configuration, int):
        """
        Split extended config into normal config and resource value.

        :param config_ext: Extended config
        :return: (config, resource_value)
        """
        x_res = copy.copy(config_ext)
        resource_value = int(x_res[self.resource_attr_name])
        del x_res[self.resource_attr_name]
        return x_res, resource_value

    def get_resource(self, config_ext: Configuration) -> int:
        """
        :param config_ext: Extended config
        :return: Value of resource attribute
        """
        return int(config_ext[self.resource_attr_name])

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/hp_ranges.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Tuple, List, Iterable, Dict, Optional
import numpy as np
from numpy.random import RandomState

from syne_tune.config_space import (
    non_constant_hyperparameter_keys,
    is_log_space,
    config_to_match_string,
    is_reverse_log_space,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Hyperparameter,
    Configuration,
)

__all__ = ["HyperparameterRanges"]


def _filter_constant_hyperparameters(config_space: Dict) -> Dict:
    nonconst_keys = set(non_constant_hyperparameter_keys(config_space))
    return {k: v for k, v in config_space.items() if k in nonconst_keys}


class HyperparameterRanges:
    def __init__(
        self,
        config_space: Dict,
        name_last_pos: Optional[str] = None,
        value_for_last_pos=None,
        active_config_space: Optional[Dict] = None,
        prefix_keys: Optional[List[str]] = None,
    ):
        """
        If name_last_pos is given, the hyperparameter of that name is assigned
        the final position in the vector returned by `to_ndarray`. This can be
        used to single out the (time) resource for a GP model, where that
        component has to come last.

        If in this case (name_last_pos given), value_for_last_pos is also given,
        some methods are modified:
        - `random_config` samples a config as normal, but then overwrites the
          name_last_pos component by value_for_last_pos
        - `get_ndarray_bounds` works as normal, but returns bound (a, a) for
          name_last_pos component, where a is the internal value corresponding
          to value_for_last_pos
        The use case is HPO with a resource attribute. This attribute should be
        fixed when optimizing the acquisition function, but can take different
        values in the evaluation data (coming from all previous searches).

        If `active_config_space` is given, it contains a subset of non-constant
        hyperparameters in `config_space`, and the range of each entry is a
        subset of the range of the corresponding `config_space` entry. These
        active ranges affect the choice of new configs (by sampling). While the
        internal encoding is based on original ranges, search is restricted to
        active ranges (e.g., optimization of surrogate model).

        :param config_space: Configuration space. Constant hyperparameters are
            filtered out here
        :param name_last_pos: See above
        :param value_for_last_pos: See above
        :param active_config_space: See above
        :param prefix_keys: If given, these keys into `config_space` come first
            in the internal ordering, which determines the internal
            encoding
        """
        self.config_space = _filter_constant_hyperparameters(config_space)
        self.name_last_pos = name_last_pos
        self.value_for_last_pos = value_for_last_pos
        self._set_internal_keys(prefix_keys)
        self._set_active_config_space(active_config_space)

    def _set_internal_keys(self, prefix_keys: Optional[List[str]]):
        keys = sorted(self.config_space.keys())
        if prefix_keys is not None:
            pk_set = set(prefix_keys)
            assert pk_set.issubset(
                set(keys)
            ), f"prefix_keys = {prefix_keys} is not a subset of {keys}"
            keys = prefix_keys + [key for key in keys if key not in pk_set]
        if self.name_last_pos is not None:
            assert self.name_last_pos in keys, (
                f"name_last_pos = '{self.name_last_pos}' not among "
                + f"hyperparameter names [{keys}]"
            )
            pos = keys.index(self.name_last_pos)
            keys = keys[:pos] + keys[(pos + 1) :] + [self.name_last_pos]
        self._internal_keys = keys

    def _set_active_config_space(self, active_config_space: Dict):
        if active_config_space is None:
            self.active_config_space = dict()
            self._config_space_for_sampling = self.config_space
        else:
            self._assert_sub_config_space(active_config_space)
            self.active_config_space = active_config_space
            self._config_space_for_sampling = dict(
                self.config_space, **active_config_space
            )

    def _assert_sub_config_space(self, active_config_space: Dict):
        for k, v in active_config_space.items():
            assert (
                k in self.config_space
            ), f"active_config_space[{k}] not in config_space"
            same_value_type = v.value_type == self.config_space[k].value_type
            same_log_type = is_log_space(v) == is_log_space(
                self.config_space[k]
            ) and is_reverse_log_space(v) == is_reverse_log_space(self.config_space[k])
            same_domain_type = isinstance(v, type(self.config_space[k]))
            assert (
                k in self.config_space
                and same_value_type
                and same_log_type
                and same_domain_type
            ), f"active_config_space[{k}] has different type"

    @property
    def internal_keys(self) -> List[str]:
        return self._internal_keys

    @property
    def config_space_for_sampling(self) -> Dict:
        return self._config_space_for_sampling

    def to_ndarray(self, config: Configuration) -> np.ndarray:
        """
        Categorical values are one-hot encoded.

        :param config: Config to encode
        :return: Encoded HP vector
        """
        raise NotImplementedError()

    def to_ndarray_matrix(self, configs: Iterable[Configuration]) -> np.ndarray:
        return np.vstack([self.to_ndarray(config) for config in configs])

    @property
    def ndarray_size(self) -> int:
        """
        :return: Dimensionality of encoded HP vector returned by `to_ndarray`
        """
        raise NotImplementedError()

    def from_ndarray(self, enc_config: np.ndarray) -> Configuration:
        """
        Converts a config from internal ndarray representation (fed to the
        GP) to its external (dict) representation. This typically involves
        rounding.
        """
        raise NotImplementedError()

    @property
    def encoded_ranges(self) -> Dict[str, Tuple[int, int]]:
        """
        :return: Ranges of HPs in the encoded ndarray representation
        """
        raise NotImplementedError()

    def is_attribute_fixed(self):
        return (self.name_last_pos is not None) and (
            self.value_for_last_pos is not None
        )

    def _fix_attribute_value(self, name):
        return self.is_attribute_fixed() and name == self.name_last_pos

    def _transform_config(self, config: Configuration):
        if self.is_attribute_fixed():
            config[self.name_last_pos] = self.value_for_last_pos
        return config

    def _random_config(self, random_state: RandomState) -> Configuration:
        return {
            k: v.sample(random_state=random_state)
            for k, v in self._config_space_for_sampling.items()
        }

    def random_config(self, random_state: RandomState) -> Configuration:
        return self._transform_config(self._random_config(random_state))

    def _random_configs(
        self, random_state: RandomState, num_configs: int
    ) -> List[Configuration]:
        return [self._random_config(random_state) for _ in range(num_configs)]

    def random_configs(self, random_state, num_configs: int) -> List[Configuration]:
        return [
            self._transform_config(config)
            for config in self._random_configs(random_state, num_configs)
        ]

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        """
        Returns (lower, upper) bounds for each dimension in ndarray vector
        representation.
        """
        raise NotImplementedError()

    def __repr__(self) -> str:
        raise NotImplementedError()

    def __eq__(self, other: object) -> bool:
        raise NotImplementedError()

    def __len__(self) -> int:
        return len(self.config_space)

    def filter_for_last_pos_value(
        self, configs: List[Configuration]
    ) -> List[Configuration]:
        """
        If is_attribute_fixed, `configs` is filtered by removing
        entries whose name_last_pos attribute value is different from
        value_for_last_pos. Otherwise, it is returned unchanged.

        """
        if self.is_attribute_fixed():
            configs = [
                config
                for config in configs
                if config[self.name_last_pos] == self.value_for_last_pos
            ]
        return configs

    def config_to_tuple(
        self, config: Configuration, keys=None, skip_last: bool = False
    ) -> Tuple[Hyperparameter, ...]:
        """
        :param config: Configuration
        :param keys: Overrides `_internal_keys`
        :param skip_last: If True and `name_last_pos` is used, the
            corresponding attribute is skipped, so that config and
            tuple are non-extended
        :return: Tuple representation
        """
        if keys is None:
            keys = self.internal_keys
            if skip_last and self.name_last_pos is not None:
                keys = keys[:-1]  # Skip last pos
        return tuple(config[k] for k in keys)

    def tuple_to_config(
        self, config_tpl: Tuple[Hyperparameter, ...], keys=None, skip_last: bool = False
    ) -> Configuration:
        if keys is None:
            keys = self.internal_keys
            if skip_last and self.name_last_pos is not None:
                keys = keys[:-1]  # Skip last pos
        return dict(zip(keys, config_tpl))

    def config_to_match_string(
        self, config: Configuration, keys=None, skip_last: bool = False
    ) -> str:
        """
        Maps configuration to  match string, used to compare for approximate
        equality. Two configurations are considered to be different if their
        match strings are not the same.

        :param config: Configuration
        :param keys: Overrides `_internal_keys`
        :param skip_last: If True and `name_last_pos` is used, the
            corresponding attribute is skipped, so that config and
            tuple are non-extended
        :return: Match string
        """
        if keys is None:
            keys = self.internal_keys
            if skip_last and self.name_last_pos is not None:
                keys = keys[:-1]  # Skip last pos
        return config_to_match_string(config, self.config_space, keys)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/hp_ranges_factory.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, List
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_impl import (
    HyperparameterRangesImpl,
)

logger = logging.getLogger(__name__)


def make_hyperparameter_ranges(
    config_space: Dict,
    name_last_pos: Optional[str] = None,
    value_for_last_pos=None,
    active_config_space: Optional[Dict] = None,
    prefix_keys: Optional[List[str]] = None,
) -> HyperparameterRanges:
    hp_ranges = HyperparameterRangesImpl(
        config_space,
        name_last_pos=name_last_pos,
        value_for_last_pos=value_for_last_pos,
        active_config_space=active_config_space,
        prefix_keys=prefix_keys,
    )
    return hp_ranges

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/hp_ranges_impl.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Tuple, Dict, List, Any, Optional, Union
import numpy as np
from autograd import numpy as anp

from syne_tune.config_space import Domain, FiniteRange, Categorical, Ordinal
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Hyperparameter,
    Configuration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.scaling import (
    Scaling,
    LinearScaling,
    get_scaling,
)

__all__ = [
    "HyperparameterRangesImpl",
    "decode_extended_features",
]

# Epsilon margin to account for numerical errors
EPS = 1e-8


class HyperparameterRange:
    def __init__(self, name: str):
        self._name = name

    @property
    def name(self) -> str:
        return self._name

    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:
        raise NotImplementedError

    def from_ndarray(self, cand_ndarray: np.ndarray) -> Hyperparameter:
        raise NotImplementedError

    def ndarray_size(self) -> int:
        return 1

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        raise NotImplementedError


def scale_from_zero_one(
    value: float,
    lower_bound: float,
    upper_bound: float,
    scaling: Scaling,
    lower_internal: float,
    upper_internal: float,
):
    assert -EPS <= value <= 1.0 + EPS, value
    size = upper_internal - lower_internal
    hp = lower_bound
    if size > 0:
        internal_value = value * size + lower_internal
        hp = np.clip(scaling.from_internal(internal_value), lower_bound, upper_bound)
    return hp


class HyperparameterRangeContinuous(HyperparameterRange):
    def __init__(
        self,
        name: str,
        lower_bound: float,
        upper_bound: float,
        scaling: Scaling,
        active_lower_bound: float = None,
        active_upper_bound: float = None,
    ):
        """
        Real valued hyperparameter.
        If `active_lower_bound` and/or `active_upper_bound` are given, the
        feasible interval for values of new configs is reduced, but data can
        still contain configs with values in `[lower_bound, upper_bound]`, and
        internal encoding is done w.r.t. this original range.

        :param name: unique name of the hyperparameter.
        :param lower_bound: inclusive lower bound on all the values that
            parameter can take.
        :param upper_bound: inclusive upper bound on all the values that
            parameter can take.
        :param scaling: determines how the values of the parameter are enumerated internally.
            The parameter value is expressed as parameter = scaling(internal), where internal
            is internal representation of parameter, which is a real value, normally in range
            [0, 1]. To optimize the parameter, the internal is varied, and the parameter to be
            tested is calculated from such internal representation.
        :param active_lower_bound: See above
        :param active_upper_bound: See above
        """
        super().__init__(name)
        assert lower_bound <= upper_bound
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.scaling = scaling
        self.lower_internal = scaling.to_internal(lower_bound)
        self.upper_internal = scaling.to_internal(upper_bound)
        if active_lower_bound is None:
            active_lower_bound = lower_bound
        if active_upper_bound is None:
            active_upper_bound = upper_bound
        assert lower_bound <= active_upper_bound <= upper_bound
        assert lower_bound <= active_lower_bound <= upper_bound
        assert active_lower_bound <= active_upper_bound
        self._ndarray_bounds = [
            (
                self.to_ndarray(active_lower_bound)[0],
                self.to_ndarray(active_upper_bound)[0],
            )
        ]

    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:
        assert self.lower_bound - EPS <= hp <= self.upper_bound + EPS, (hp, self)
        # convert everything to internal scaling, and then normalize between zero and one
        lower, upper = self.lower_internal, self.upper_internal
        if upper == lower:
            result = 0.0  # if the bounds are fixed for a dimension
        else:
            hp_internal = self.scaling.to_internal(hp)
            result = np.clip((hp_internal - lower) / (upper - lower), 0.0, 1.0)
        return np.array([result])

    def from_ndarray(self, ndarray: np.ndarray) -> Hyperparameter:
        return scale_from_zero_one(
            ndarray.item(),
            self.lower_bound,
            self.upper_bound,
            self.scaling,
            self.lower_internal,
            self.upper_internal,
        )

    def __repr__(self) -> str:
        return "{}({}, {}, {}, {})".format(
            self.__class__.__name__,
            repr(self.name),
            repr(self.scaling),
            repr(self.lower_bound),
            repr(self.upper_bound),
        )

    def __eq__(self, other: object) -> bool:
        if isinstance(other, HyperparameterRangeContinuous):
            return (
                self.name == other.name
                and np.allclose([self.lower_bound], [other.lower_bound])
                and np.allclose([self.upper_bound], [other.upper_bound])
                and self.scaling == other.scaling
            )
        return False

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        return self._ndarray_bounds


class HyperparameterRangeInteger(HyperparameterRange):
    def __init__(
        self,
        name: str,
        lower_bound: int,
        upper_bound: int,
        scaling: Scaling,
        active_lower_bound: int = None,
        active_upper_bound: int = None,
    ):
        """
        Both bounds are INCLUDED in the valid values. Under the hood generates a continuous
        range from lower_bound - 0.5 to upper_bound + 0.5.
        See docs for continuous hyperparameter for more information.
        """
        super().__init__(name)
        assert lower_bound <= upper_bound
        self.lower_bound = int(lower_bound)
        self.upper_bound = int(upper_bound)
        self.active_lower_bound = (
            self.lower_bound if active_lower_bound is None else int(active_lower_bound)
        )
        self.active_upper_bound = (
            self.upper_bound if active_upper_bound is None else int(active_upper_bound)
        )
        self._continuous_range = HyperparameterRangeContinuous(
            name,
            self.lower_bound - 0.5 + EPS,
            self.upper_bound + 0.5 - EPS,
            scaling,
            self.active_lower_bound - 0.5 + EPS,
            self.active_upper_bound + 0.5 - EPS,
        )

    @property
    def scaling(self) -> Scaling:
        return self._continuous_range.scaling

    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:
        return self._continuous_range.to_ndarray(float(hp))

    def _round_to_int(self, value: float) -> int:
        return int(np.clip(round(value), self.lower_bound, self.upper_bound))

    def from_ndarray(self, ndarray: np.ndarray) -> Hyperparameter:
        continuous = self._continuous_range.from_ndarray(ndarray)
        return self._round_to_int(continuous)

    def __repr__(self) -> str:
        return "{}({}, {}, {}, {})".format(
            self.__class__.__name__,
            repr(self.name),
            repr(self.scaling),
            repr(self.lower_bound),
            repr(self.upper_bound),
        )

    def __eq__(self, other):
        if isinstance(other, HyperparameterRangeInteger):
            return (
                self.name == other.name
                and self.lower_bound == other.lower_bound
                and self.upper_bound == other.upper_bound
                and self.scaling == other.scaling
            )
        return False

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        return self._continuous_range.get_ndarray_bounds()


class HyperparameterRangeFiniteRange(HyperparameterRange):
    def __init__(
        self,
        name: str,
        lower_bound: float,
        upper_bound: float,
        size: int,
        scaling: Scaling,
        cast_int: bool = False,
    ):
        """
        See :class:`FiniteRange` in `config_space`. Internally, we use an int
        with linear scaling.
        Note: Different to `HyperparameterRangeContinuous`, we require that
        `lower_bound < upper_bound` and `size >=2`.

        """
        super().__init__(name)
        assert lower_bound <= upper_bound
        assert size >= 1
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.cast_int = cast_int
        self._scaling = scaling
        self._lower_internal = scaling.to_internal(lower_bound)
        self._upper_internal = scaling.to_internal(upper_bound)
        self._step_internal = (
            (self._upper_internal - self._lower_internal) / (size - 1)
            if size > 1
            else 0
        )
        self._range_int = HyperparameterRangeInteger(
            name=name + "_INTERNAL",
            lower_bound=0,
            upper_bound=size - 1,
            scaling=LinearScaling(),
        )

    @property
    def scaling(self) -> Scaling:
        return self._scaling

    def _map_from_int(self, x: int) -> Union[float, int]:
        y = x * self._step_internal + self._lower_internal
        y = np.clip(self._scaling.from_internal(y), self.lower_bound, self.upper_bound)
        if not self.cast_int:
            return float(y)
        else:
            return int(np.round(y))

    def _map_to_int(self, y: Union[float, int]) -> int:
        if self._step_internal == 0:
            return 0
        else:
            y_int = np.clip(
                self._scaling.to_internal(y), self._lower_internal, self._upper_internal
            )
            return int(round((y_int - self._lower_internal) / self._step_internal))

    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:
        return self._range_int.to_ndarray(self._map_to_int(hp))

    def from_ndarray(self, ndarray: np.ndarray) -> Hyperparameter:
        int_val = self._range_int.from_ndarray(ndarray)
        return self._map_from_int(int_val)

    def __repr__(self) -> str:
        return "{}({}, {}, {}, {}, {})".format(
            self.__class__.__name__,
            repr(self.name),
            repr(self.scaling),
            repr(self.lower_bound),
            repr(self.upper_bound),
            repr(self.cast_int),
        )

    def __eq__(self, other):
        if isinstance(other, HyperparameterRangeFiniteRange):
            return (
                self.name == other.name
                and np.allclose([self.lower_bound], [other.lower_bound])
                and np.allclose([self.upper_bound], [other.upper_bound])
                and self._scaling == other._scaling
                and self.cast_int == other.cast_int
                and self._range_int.upper_bound == other._range_int.upper_bound
            )
        return False

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        return self._range_int.get_ndarray_bounds()


class HyperparameterRangeCategorical(HyperparameterRange):
    def __init__(self, name: str, choices: Tuple[Any, ...]):
        super().__init__(name)
        self._assert_choices(choices)
        self.choices = list(choices)
        self.num_choices = len(self.choices)
        assert self.num_choices > 0

    @staticmethod
    def _assert_value_type(value):
        assert (
            isinstance(value, str) or isinstance(value, int) or isinstance(value, float)
        ), f"value = {value} has type {type(value)}, must be str, int, or float"

    @staticmethod
    def _assert_choices(choices: Tuple[Any, ...]):
        assert len(choices) > 0
        HyperparameterRangeCategorical._assert_value_type(choices[0])
        value_type = type(choices[0])
        assert any(
            type(x) == value_type for x in choices
        ), f"All entries in choices = {choices} must have the same type {value_type}"

    def __repr__(self) -> str:
        return "{}({}, {})".format(
            self.__class__.__name__, repr(self.name), repr(self.choices)
        )

    def __eq__(self, other) -> bool:
        if isinstance(other, HyperparameterRangeCategorical):
            return self.name == other.name and self.choices == other.choices
        return False


class HyperparameterRangeCategoricalNonBinary(HyperparameterRangeCategorical):
    def __init__(
        self,
        name: str,
        choices: Tuple[Any, ...],
        active_choices: Tuple[Any, ...] = None,
    ):
        """
        Can take on discrete set of values. We use one-hot encoding internally.
        If the value range has size 2, it is more efficient to use
        :class:`HyperparameterRangeCategoricalBinary`.

        :param name: name of dimension.
        :param choices: possible values of the hyperparameter
        :param active_choices: If given, must be nonempty subset of `choices`.
        """
        super().__init__(name, choices)
        if active_choices is None:
            if self.num_choices > 1:
                self._ndarray_bounds = [(0.0, 1.0)] * self.num_choices
            else:
                self._ndarray_bounds = [(1.0, 1.0)]
        else:
            self._assert_choices(active_choices)
            _active_choices = set(active_choices)
            num_active_choices = len(active_choices)
            self._ndarray_bounds = [(0.0, 0.0)] * self.num_choices
            num = 0
            val_nonzero = (0.0, 1.0) if num_active_choices > 1 else (1.0, 1.0)
            for pos, val in enumerate(self.choices):
                if val in _active_choices:
                    self._ndarray_bounds[pos] = val_nonzero
                    num += 1
            assert num == num_active_choices, (
                f"active_choices = {active_choices} must be a subset of "
                + f"choices = {choices}"
            )

    def ndarray_size(self) -> int:
        return self.num_choices

    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:
        self._assert_value_type(hp)
        assert hp in self.choices, "{} not in {}".format(hp, self)
        idx = self.choices.index(hp)
        result = np.zeros(shape=(self.num_choices,))
        result[idx] = 1.0
        return result

    def from_ndarray(self, cand_ndarray: np.ndarray) -> Hyperparameter:
        assert len(cand_ndarray) == self.num_choices, (cand_ndarray, self)
        return self.choices[int(np.argmax(cand_ndarray))]

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        return self._ndarray_bounds


class HyperparameterRangeCategoricalBinary(HyperparameterRangeCategorical):
    def __init__(
        self,
        name: str,
        choices: Tuple[Any, ...],
        active_choices: Tuple[Any, ...] = None,
    ):
        """
        Here, the value range must be of size 2. The internal encoding is an
        single int, so 1 instead of 2 dimensions.

        :param name: name of dimension.
        :param choices: possible values of the hyperparameter (size 2)
        :param active_choices: If given, must be nonempty subset of `choices`.
        """
        assert len(choices) == 2, (
            f"len(choices) = {len(choices)}, must be 2. Use "
            + "HyperparameterRangeCategoricalNonBinary instead"
        )
        super().__init__(name, choices)
        active_value = None
        if active_choices is not None:
            self._assert_choices(active_choices)
            _active_choices = set(active_choices)
            num = 0
            for pos, val in enumerate(self.choices):
                if val in _active_choices:
                    active_value = pos
                    num += 1
            assert num == len(_active_choices), (
                f"active_choices = {active_choices} must be a subset of "
                + f"choices = {choices}"
            )
            if num == 2:
                active_value = None
        # Internal encoding
        self._range_int = HyperparameterRangeInteger(
            name=name + "_INTERNAL",
            lower_bound=0,
            upper_bound=1,
            scaling=LinearScaling(),
            active_lower_bound=active_value,
            active_upper_bound=active_value,
        )

    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:
        self._assert_value_type(hp)
        assert hp in self.choices, "{} not in {}".format(hp, self)
        idx = self.choices.index(hp)
        return self._range_int.to_ndarray(idx)

    def from_ndarray(self, cand_ndarray: np.ndarray) -> Hyperparameter:
        assert len(cand_ndarray) == 1
        return self.choices[self._range_int.from_ndarray(cand_ndarray)]

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        return self._range_int.get_ndarray_bounds()


class HyperparameterRangeOrdinal(HyperparameterRangeCategorical):
    def __init__(self, name: str, choices: Tuple[Any, ...]):
        super().__init__(name, choices)
        self._range_int = HyperparameterRangeInteger(
            name=name + "_INTERNAL",
            lower_bound=0,
            upper_bound=self.num_choices - 1,
            scaling=LinearScaling(),
        )

    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:
        self._assert_value_type(hp)
        assert hp in self.choices, "{} not in {}".format(hp, self)
        idx = self.choices.index(hp)
        return self._range_int.to_ndarray(idx)

    def from_ndarray(self, cand_ndarray: np.ndarray) -> Hyperparameter:
        assert len(cand_ndarray) == 1
        return self.choices[self._range_int.from_ndarray(cand_ndarray)]

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        return self._range_int.get_ndarray_bounds()

    def __eq__(self, other) -> bool:
        if isinstance(other, HyperparameterRangeOrdinal):
            return self.name == other.name and self.choices == other.choices
        return False


class HyperparameterRangesImpl(HyperparameterRanges):
    """
    Basic implementation of :class:`HyperparameterRanges`.
    """

    def __init__(
        self,
        config_space: Dict,
        name_last_pos: str = None,
        value_for_last_pos=None,
        active_config_space: Dict = None,
        prefix_keys: Optional[List[str]] = None,
    ):
        super().__init__(
            config_space,
            name_last_pos,
            value_for_last_pos,
            active_config_space,
            prefix_keys,
        )
        hp_ranges = []
        for name in self.internal_keys:
            hp_range = self.config_space[name]
            assert isinstance(hp_range, Domain)
            tp = hp_range.value_type
            if isinstance(hp_range, Categorical):
                kwargs = dict()
                is_in_active = name in self.active_config_space
                if isinstance(hp_range, Ordinal):
                    assert (
                        not is_in_active
                    ), f"Parameter '{name}' of type Ordinal cannot be used in active_config_space"
                    _cls = HyperparameterRangeOrdinal
                else:
                    if is_in_active:
                        kwargs["active_choices"] = tuple(
                            self.active_config_space[name].categories
                        )
                    if len(hp_range.categories) == 2:
                        _cls = HyperparameterRangeCategoricalBinary
                    else:
                        _cls = HyperparameterRangeCategoricalNonBinary
                hp_ranges.append(
                    _cls(
                        name,
                        choices=tuple(hp_range.categories),
                        **kwargs,
                    )
                )
            else:
                scaling = get_scaling(hp_range)
                kwargs = {
                    "name": name,
                    "lower_bound": hp_range.lower,
                    "upper_bound": hp_range.upper,
                    "scaling": scaling,
                }
                if isinstance(hp_range, FiniteRange):
                    assert (
                        name not in self.active_config_space
                    ), f"Parameter '{name}' of type FiniteRange cannot be used in active_config_space"
                    hp_ranges.append(
                        HyperparameterRangeFiniteRange(
                            **kwargs, size=len(hp_range), cast_int=hp_range.cast_int
                        )
                    )
                else:
                    if name in self.active_config_space:
                        active_hp_range = self.active_config_space[name]
                        kwargs.update(
                            {
                                "active_lower_bound": active_hp_range.lower,
                                "active_upper_bound": active_hp_range.upper,
                            }
                        )
                    if tp == float:
                        hp_ranges.append(HyperparameterRangeContinuous(**kwargs))
                    else:
                        hp_ranges.append(HyperparameterRangeInteger(**kwargs))
        self._hp_ranges = hp_ranges
        csum = [0] + list(np.cumsum([d.ndarray_size() for d in hp_ranges]))
        self._ndarray_size = csum[-1]
        self._encoded_ranges = dict(
            zip((d.name for d in hp_ranges), zip(csum[:-1], csum[1:]))
        )

    @property
    def ndarray_size(self) -> int:
        return self._ndarray_size

    def to_ndarray(self, config: Configuration) -> np.ndarray:
        config_tpl = self.config_to_tuple(config)
        pieces = [
            hp_range.to_ndarray(hp) for hp_range, hp in zip(self._hp_ranges, config_tpl)
        ]
        return np.hstack(pieces)

    def from_ndarray(self, enc_config: np.ndarray) -> Configuration:
        """
        Converts a config from internal ndarray representation (fed to the GP)
        into an external config.

        For numerical HPs it assumes values scaled between 0.0 and 1.0, for
        categorical HPs it assumes one scalar per category, which will convert
        to the category with the highest value.
        """
        enc_config = enc_config.reshape((-1, 1))
        assert enc_config.size == self._ndarray_size, (
            enc_config.size,
            self._ndarray_size,
        )
        hps = []
        start = 0
        for hp_range in self._hp_ranges:
            end = start + hp_range.ndarray_size()
            enc_attr = enc_config[start:end]
            hps.append(hp_range.from_ndarray(enc_attr))
            start = end
        return self.tuple_to_config(tuple(hps))

    @property
    def encoded_ranges(self) -> Dict[str, Tuple[int, int]]:
        return self._encoded_ranges

    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:
        bounds = [
            x for hp_range in self._hp_ranges for x in hp_range.get_ndarray_bounds()
        ]
        if self.is_attribute_fixed():
            hp_range = self._hp_ranges[-1]
            assert hp_range.name == self.name_last_pos
            enc_fixed = hp_range.to_ndarray(self.value_for_last_pos).reshape((-1,))
            offset = self.ndarray_size - enc_fixed.size
            for i, val in enumerate(enc_fixed):
                bounds[i + offset] = (val, val)
        return bounds

    def __repr__(self) -> str:
        return "{}{}".format(self.__class__.__name__, repr(self._hp_ranges))

    def __eq__(self, other: object) -> bool:
        if isinstance(other, HyperparameterRangesImpl):
            return self._hp_ranges == other._hp_ranges
        return False


def decode_extended_features(
    features_ext: np.ndarray,
    resource_attr_range: Tuple[int, int],
) -> (np.ndarray, np.ndarray):
    """
    Given matrix of features from extended configs, corresponding to
    `ExtendedConfiguration`, split into feature matrix from normal
    configs and resource values.

    :param features_ext: Matrix of features from extended configs
    :param resource_attr_range: (r_min, r_max)
    :return: (features, resources)
    """
    r_min, r_max = resource_attr_range
    features = features_ext[:, :-1]
    resources_encoded = features_ext[:, -1].reshape((-1,))
    lower = r_min - 0.5 + EPS
    width = r_max - r_min + 1 - 2 * EPS
    resources = anp.clip(
        anp.round(resources_encoded * width + lower), r_min, r_max
    ).astype("int64")
    return features, resources

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/scaling.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np

from syne_tune.config_space import Domain, is_log_space, is_reverse_log_space


class Scaling:
    def to_internal(self, value: float) -> float:
        raise NotImplementedError

    def from_internal(self, value: float) -> float:
        raise NotImplementedError

    def __repr__(self):
        return "{}()".format(self.__class__.__name__)

    def __eq__(self, other):
        # For usage in tests. Make sure to edit if parameters are added.
        return self.__class__ == other.__class__


class LinearScaling(Scaling):
    def to_internal(self, value: float) -> float:
        return value

    def from_internal(self, value: float) -> float:
        return value


class LogScaling(Scaling):
    def to_internal(self, value: float) -> float:
        assert value > 0, "Value must be strictly positive to be log-scaled."
        return np.log(value)

    def from_internal(self, value: float) -> float:
        return np.exp(value)


class ReverseLogScaling(Scaling):
    def to_internal(self, value: float) -> float:
        assert (
            0 <= value < 1
        ), "Value must be between 0 (inclusive) and 1 (exclusive) to be reverse-log-scaled."
        return -np.log(1.0 - value)

    def from_internal(self, value: float) -> float:
        return 1.0 - np.exp(-value)


def get_scaling(hp_range: Domain) -> Scaling:
    if is_log_space(hp_range):
        return LogScaling()
    elif is_reverse_log_space(hp_range):
        return ReverseLogScaling()
    else:
        return LinearScaling()

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/datatypes/tuning_job_state.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Dict, Optional

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
    TrialEvaluations,
    PendingEvaluation,
    MetricValues,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)


class TuningJobState:
    """
    Collects all data determining the state of a tuning experiment. Trials
    are indexed by `trial_id`. The configurations associated with trials are
    listed in `config_for_trial`.
    `trials_evaluations` contains observations, `failed_trials` lists
    trials for which evaluations have failed, `pending_evaluations` lists
    trials for which observations are pending.

    `trials_evaluations` may store values for different metrics in each
    record, and each such value may be a dict (see:class:`TrialEvaluations`).
    For example, for multi-fidelity schedulers,
    `trials_evaluations[i].metrics[k][str(r)]` is the value for metric k
    and trial `trials_evaluations[i].trial_id` observed at resource level
    r.
    """

    def __init__(
        self,
        hp_ranges: HyperparameterRanges,
        config_for_trial: Dict[str, Configuration],
        trials_evaluations: List[TrialEvaluations],
        failed_trials: List[str] = None,
        pending_evaluations: List[PendingEvaluation] = None,
    ):
        if failed_trials is None:
            failed_trials = []
        if pending_evaluations is None:
            pending_evaluations = []
        self._check_trial_ids(
            config_for_trial, trials_evaluations, failed_trials, pending_evaluations
        )
        self.hp_ranges = hp_ranges
        self.config_for_trial = config_for_trial
        self.trials_evaluations = trials_evaluations
        self.failed_trials = failed_trials
        self.pending_evaluations = pending_evaluations

    @staticmethod
    def _check_all_string(trial_ids: List[str], name: str):
        assert all(
            isinstance(x, str) for x in trial_ids
        ), f"trial_ids in {name} contain non-string values:\n{trial_ids}"

    @staticmethod
    def _check_trial_ids(
        config_for_trial, trials_evaluations, failed_trials, pending_evaluations
    ):
        observed_trials = [x.trial_id for x in trials_evaluations]
        pending_trials = [x.trial_id for x in pending_evaluations]
        TuningJobState._check_all_string(observed_trials, "trials_evaluations")
        TuningJobState._check_all_string(failed_trials, "failed_trials")
        TuningJobState._check_all_string(pending_trials, "pending_evaluations")
        trial_ids = set(observed_trials + failed_trials + pending_trials)
        for trial_id in trial_ids:
            assert (
                trial_id in config_for_trial
            ), f"trial_id {trial_id} not contained in configs_for_trials"

    @staticmethod
    def empty_state(hp_ranges: HyperparameterRanges) -> "TuningJobState":
        return TuningJobState(
            hp_ranges=hp_ranges,
            config_for_trial=dict(),
            trials_evaluations=[],
            failed_trials=[],
            pending_evaluations=[],
        )

    def _find_labeled(self, trial_id: str) -> int:
        try:
            return next(
                i
                for i, x in enumerate(self.trials_evaluations)
                if x.trial_id == trial_id
            )
        except StopIteration:
            return -1

    def _find_pending(self, trial_id: str, resource: Optional[int] = None) -> int:
        try:
            return next(
                i
                for i, x in enumerate(self.pending_evaluations)
                if x.trial_id == trial_id and x.resource == resource
            )
        except StopIteration:
            return -1

    def _register_config_for_trial(
        self, trial_id: str, config: Optional[Configuration] = None
    ):
        if config is None:
            assert trial_id in self.config_for_trial, (
                f"trial_id = {trial_id} not yet registered in "
                + "config_for_trial, so config must be given"
            )
        elif trial_id not in self.config_for_trial:
            self.config_for_trial[trial_id] = config.copy()

    def metrics_for_trial(
        self, trial_id: str, config: Optional[Configuration] = None
    ) -> MetricValues:
        """
        Helper for inserting new entry into `trials_evaluations`. If `trial_id`
        is already contained there, the corresponding `eval.metrics` is
        returned. Otherwise, a new entry `new_eval` is appended to
        `trials_evaluations` and its `new_eval.metrics` is returned
        (empty dict). In the latter case, `config` needs to be passed,
        because it may not yet feature in `config_for_trial`.

        """
        # NOTE: If `trial_id` exists in `config_for_trial` and `config` is
        # given, we do not check that `config` is correct. In fact, we ignore
        # `config` in this case.
        self._register_config_for_trial(trial_id, config)
        pos = self._find_labeled(trial_id)
        if pos != -1:
            metrics = self.trials_evaluations[pos].metrics
        else:
            # New entry
            metrics = dict()
            new_eval = TrialEvaluations(trial_id=trial_id, metrics=metrics)
            self.trials_evaluations.append(new_eval)
        return metrics

    def num_observed_cases(self, metric_name: str = INTERNAL_METRIC_NAME) -> int:
        return sum(ev.num_cases(metric_name) for ev in self.trials_evaluations)

    def observed_data_for_metric(
        self, metric_name: str = INTERNAL_METRIC_NAME, resource_attr_name: str = None
    ) -> (List[Configuration], List[float]):
        """
        Extracts datapoints from `trials_evaluations` for particular
        metric `metric_name`, in the form of a list of configs and a list of
        metric values.
        If `metric_name` is a dict-valued metric, the dict keys must be
        resource values, and the returned configs are extended. Here, the
        name of the resource attribute can be passed in `resource_attr_name`
        (if not given, it can be obtained from `hp_ranges` if this is extended).

        Note: Implements the default behaviour, namely to return extended
        configs for dict-valued metrics, which also require `hp_ranges` to be
        extended. This is not correct for some specific multi-fidelity
        surrogate models, which should access the data directly.

        :param metric_name:
        :param resource_attr_name:
        :return: configs, metric_values
        """
        if resource_attr_name is None:
            resource_attr_name = self.hp_ranges.name_last_pos
        configs = []
        metric_values = []
        for ev in self.trials_evaluations:
            config = self.config_for_trial[ev.trial_id]
            metric_entry = ev.metrics.get(metric_name)
            if metric_entry is not None:
                if isinstance(metric_entry, dict):
                    assert resource_attr_name is not None, (
                        "Need resource_attr_name for dict-valued metric " + metric_name
                    )
                    for resource, metric_val in metric_entry.items():
                        config_ext = dict(config, **{resource_attr_name: int(resource)})
                        configs.append(config_ext)
                        metric_values.append(metric_val)
                else:
                    configs.append(config)
                    metric_values.append(metric_entry)
        return configs, metric_values

    def is_pending(self, trial_id: str, resource: Optional[int] = None) -> bool:
        return self._find_pending(trial_id, resource) != -1

    def is_labeled(
        self,
        trial_id: str,
        metric_name: str = INTERNAL_METRIC_NAME,
        resource: Optional[int] = None,
    ) -> bool:
        """
        Checks whether `trial_id` has observed data under `metric_name`. If
        `resource` is given, the observation must be at that resource level.

        """
        pos = self._find_labeled(trial_id)
        result = False
        if pos != -1:
            metric_entry = self.trials_evaluations[pos].metrics.get(metric_name)
            if metric_entry is not None:
                if resource is None:
                    result = True
                elif isinstance(metric_entry, dict):
                    result = str(resource) in metric_entry
        return result

    def append_pending(
        self,
        trial_id: str,
        config: Optional[Configuration] = None,
        resource: Optional[int] = None,
    ):
        """
        Appends new pending evaluation. If the trial has not been registered
        here, `config` must be given. Otherwise, it is ignored.

        """
        self._register_config_for_trial(trial_id, config)
        assert not self.is_pending(trial_id, resource)
        self.pending_evaluations.append(
            PendingEvaluation(trial_id=trial_id, resource=resource)
        )

    def remove_pending(self, trial_id: str, resource: Optional[int] = None) -> bool:
        pos = self._find_pending(trial_id, resource)
        if pos != -1:
            self.pending_evaluations.pop(pos)
            return True
        else:
            return False

    def pending_configurations(
        self, resource_attr_name: str = None
    ) -> List[Configuration]:
        """
        Returns list of configurations corresponding to pending evaluations.
        If the latter have resource values, the configs are extended.

        """
        if resource_attr_name is None:
            resource_attr_name = self.hp_ranges.name_last_pos
        configs = []
        for pend_eval in self.pending_evaluations:
            config = self.config_for_trial[pend_eval.trial_id]
            resource = pend_eval.resource
            if resource is not None:
                assert (
                    resource_attr_name is not None
                ), f"Need resource_attr_name, or hp_ranges to be extended"
                config = dict(config, **{resource_attr_name: int(resource)})
            configs.append(config)
        return configs

    def _map_configs_for_matching(
        self, config_for_trial: Dict[str, Configuration]
    ) -> Dict[str, str]:
        return {
            trial_id: self.hp_ranges.config_to_match_string(config)
            for trial_id, config in config_for_trial.items()
        }

    def __eq__(self, other) -> bool:
        if not isinstance(other, TuningJobState):
            return False
        if (
            self.failed_trials != other.failed_trials
            or self.pending_evaluations != other.pending_evaluations
        ):
            return False
        if self.hp_ranges != other.hp_ranges:
            return False
        if self.trials_evaluations != other.trials_evaluations:
            return False
        return self._map_configs_for_matching(
            self.config_for_trial
        ) == self._map_configs_for_matching(other.config_for_trial)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
class SliceException(Exception):
    pass

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/constants.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# This file contains various constants required for the definition of the model
# or to set up the optimization

import autograd.numpy as anp
from dataclasses import dataclass

DEFAULT_ENCODING = "logarithm"  # the other choices is positive

NUMERICAL_JITTER = 1e-9

INITIAL_NOISE_VARIANCE = 1e-3
INITIAL_MEAN_VALUE = 0.0
INITIAL_COVARIANCE_SCALE = 1.0
INITIAL_INVERSE_BANDWIDTHS = 1.0
INITIAL_WARPING = 1.0

INVERSE_BANDWIDTHS_LOWER_BOUND = 1e-4
INVERSE_BANDWIDTHS_UPPER_BOUND = 100

COVARIANCE_SCALE_LOWER_BOUND = 1e-3
COVARIANCE_SCALE_UPPER_BOUND = 1e3

NOISE_VARIANCE_LOWER_BOUND = 1e-9
NOISE_VARIANCE_UPPER_BOUND = 1e6

WARPING_LOWER_BOUND = 0.25
WARPING_UPPER_BOUND = 4.0

MIN_POSTERIOR_VARIANCE = 1e-12

MIN_CHOLESKY_DIAGONAL_VALUE = 1e-10

DATA_TYPE = anp.float64


@dataclass
class OptimizationConfig:
    lbfgs_tol: float
    lbfgs_maxiter: int
    verbose: bool
    n_starts: int


@dataclass
class MCMCConfig:
    """
    `n_samples` is the total number of samples drawn. The first `n_burnin` of
    these are dropped (burn-in), and every `n_thinning` of the rest is
    returned. This means we return
    `(n_samples - n_burnin) // n_thinning` samples.
    """

    n_samples: int
    n_burnin: int
    n_thinning: int


DEFAULT_OPTIMIZATION_CONFIG = OptimizationConfig(
    lbfgs_tol=1e-6, lbfgs_maxiter=500, verbose=False, n_starts=5
)

DEFAULT_MCMC_CONFIG = MCMCConfig(n_samples=300, n_burnin=250, n_thinning=5)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/custom_op.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp
import autograd.scipy.linalg as aspl
from autograd.extend import primitive, defvjp
import numpy as np
import scipy.linalg as spl
import logging
import math

logger = logging.getLogger(__name__)

__all__ = ["AddJitterOp", "flatten_and_concat", "cholesky_factorization"]


INITIAL_JITTER_FACTOR = 1e-9
JITTER_GROWTH = 10.0
JITTER_UPPERBOUND_FACTOR = 1e3


def flatten_and_concat(x: anp.ndarray, sigsq_init: anp.ndarray):
    return anp.append(anp.reshape(x, (-1,)), sigsq_init)


@primitive
def AddJitterOp(
    inputs: np.ndarray,
    initial_jitter_factor=INITIAL_JITTER_FACTOR,
    jitter_growth=JITTER_GROWTH,
    debug_log="false",
):
    """
    Finds smaller jitter to add to diagonal of square matrix to render the
    matrix positive definite (in that linalg.potrf works).

    Given input x (positive semi-definite matrix) and sigsq_init (nonneg
    scalar), find sigsq_final (nonneg scalar), so that:
        sigsq_final = sigsq_init + jitter, jitter >= 0,
        x + sigsq_final * Id positive definite (so that potrf call works)
    We return the matrix x + sigsq_final * Id, for which potrf has not failed.

    For the gradient, the dependence of jitter on the inputs is ignored.

    The values tried for sigsq_final are:
        sigsq_init, sigsq_init + initial_jitter * (jitter_growth ** k),
        k = 0, 1, 2, ...,
        initial_jitter = initial_jitter_factor * max(mean(diag(x)), 1)

    Note: The scaling of initial_jitter with mean(diag(x)) is taken from GPy.
    The rationale is that the largest eigenvalue of x is >= mean(diag(x)), and
    likely of this magnitude.

    There is no guarantee that the Cholesky factor returned is well-conditioned
    enough for subsequent computations to be reliable. A better solution
    would be to estimate the condition number of the Cholesky factor, and to add
    jitter until this is bounded below a threshold we tolerate. See

        Higham, N.
        A Survey of Condition Number Estimation for Triangular Matrices
        MIMS EPrint: 2007.10

    Algorithm 4.1 could work for us.
    """
    assert initial_jitter_factor > 0.0 and jitter_growth > 1.0
    n_square = inputs.shape[0] - 1
    n = int(math.sqrt(n_square))
    assert (
        n_square % n == 0 and n_square // n == n
    ), "x must be square matrix, shape (n, n)"
    x, sigsq_init = np.reshape(inputs[:-1], (n, -1)), inputs[-1]

    def _get_constant_identity(x, constant):
        n, _ = x.shape
        return np.diag(np.ones((n,)) * constant)

    def _get_jitter_upperbound(x):
        # To define a safeguard in the while-loop of the forward,
        # we define an upperbound on the jitter we can reasonably add
        # the bound is quite generous, and is dependent on the scale of the input x
        # (the scale is captured via the trace of x)
        # the primary goal is avoid any infinite while-loop.
        return JITTER_UPPERBOUND_FACTOR * max(1.0, np.mean(np.diag(x)))

    jitter = 0.0
    jitter_upperbound = _get_jitter_upperbound(x)
    must_increase_jitter = True
    x_plus_constant = None

    while must_increase_jitter and jitter <= jitter_upperbound:
        try:
            x_plus_constant = x + _get_constant_identity(x, sigsq_init + jitter)
            # Note: Do not use np.linalg.cholesky here, this can cause
            # locking issues
            L = spl.cholesky(x_plus_constant, lower=True)
            must_increase_jitter = False
        except spl.LinAlgError:
            if debug_log == "true":
                logger.info("sigsq = {} does not work".format(sigsq_init + jitter))
            if jitter == 0.0:
                jitter = initial_jitter_factor * max(1.0, np.mean(np.diag(x)))
            else:
                jitter = jitter * jitter_growth

    assert (
        not must_increase_jitter
    ), "The jitter ({}) has reached its upperbound ({}) while the Cholesky of the input matrix still cannot be computed.".format(
        jitter, jitter_upperbound
    )

    if debug_log == "true":
        logger.info("sigsq_final = {}".format(sigsq_init + jitter))

    return x_plus_constant


def AddJitterOp_vjp(
    ans: np.ndarray,
    inputs: np.ndarray,
    initial_jitter_factor=INITIAL_JITTER_FACTOR,
    jitter_growth=JITTER_GROWTH,
    debug_log="false",
):
    return lambda g: anp.append(anp.reshape(g, (-1,)), anp.sum(anp.diag(g)))


defvjp(AddJitterOp, AddJitterOp_vjp)


@primitive
def cholesky_factorization(a):
    """
    Replacement for autograd.numpy.linalg.cholesky. Our backward (vjp) is
    faster and simpler, while somewhat less general (only works if
    a.ndim == 2).

    See https://arxiv.org/abs/1710.08717 for derivation of backward (vjp)
    expression.

    :param a: Symmmetric positive definite matrix A
    :return: Lower-triangular Cholesky factor L of A
    """
    # Note: Do not use np.linalg.cholesky here, this can cause locking issues
    return spl.cholesky(a, lower=True)


def copyltu(x):
    return anp.tril(x) + anp.transpose(anp.tril(x, -1))


def cholesky_factorization_backward(l, lbar):
    abar = copyltu(anp.matmul(anp.transpose(l), lbar))
    abar = anp.transpose(aspl.solve_triangular(l, abar, lower=True, trans="T"))
    abar = aspl.solve_triangular(l, abar, lower=True, trans="T")
    return 0.5 * abar


def cholesky_factorization_vjp(l, a):
    return lambda lbar: cholesky_factorization_backward(l, lbar)


defvjp(cholesky_factorization, cholesky_factorization_vjp)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/distribution.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp
from autograd.builtins import isinstance
import numbers
from scipy.special import gammaln

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    MIN_POSTERIOR_VARIANCE,
)

__all__ = ["Distribution", "Gamma", "Uniform", "Normal", "LogNormal", "Horseshoe"]


class Distribution:
    def negative_log_density(self, x):
        """
        Negative log density. lower and upper limits are ignored.
        If x is not a scalar, the distribution is i.i.d. over all
        entries.
        """
        raise NotImplementedError


class Gamma(Distribution):
    """
    Gamma(mean, alpha):

        p(x) = C(alpha, beta) x^{alpha - 1} exp( -beta x), beta = alpha / mean,
        C(alpha, beta) = beta^alpha / Gamma(alpha)
    """

    def __init__(self, mean, alpha):
        self._assert_positive_number(mean, "mean")
        self._assert_positive_number(alpha, "alpha")
        self.mean = anp.maximum(mean, MIN_POSTERIOR_VARIANCE)
        self.alpha = anp.maximum(alpha, MIN_POSTERIOR_VARIANCE)
        self.beta = self.alpha / self.mean
        self.log_const = gammaln(self.alpha) - self.alpha * anp.log(self.beta)
        self.__call__ = self.negative_log_density

    @staticmethod
    def _assert_positive_number(x, name):
        assert (
            isinstance(x, numbers.Real) and x > 0.0
        ), "{} = {}, must be positive number".format(name, x)

    def negative_log_density(self, x):
        x_safe = anp.maximum(x, MIN_POSTERIOR_VARIANCE)
        return anp.sum(
            (1.0 - self.alpha) * anp.log(x_safe) + self.beta * x_safe + self.log_const
        )

    def __call__(self, x):
        return self.negative_log_density(x)


class Uniform(Distribution):
    def __init__(self, lower: float, upper: float):
        self.log_const = anp.log(upper - lower)
        self.__call__ = self.negative_log_density

    def negative_log_density(self, x):
        return x.size * self.log_const

    def __call__(self, x):
        return self.negative_log_density(x)


class Normal(Distribution):
    def __init__(self, mean: float, sigma: float):
        self.mean = mean
        self.sigma = sigma
        self.__call__ = self.negative_log_density

    def negative_log_density(self, x):
        return anp.sum(anp.square(x - self.mean)) * (0.5 / anp.square(self.sigma))

    def __call__(self, x):
        return self.negative_log_density(x)


class LogNormal(Distribution):
    def __init__(self, mean: float, sigma: float):
        self.mean = mean
        self.sigma = sigma
        self.__call__ = self.negative_log_density

    def negative_log_density(self, x):
        x_safe = anp.maximum(x, MIN_POSTERIOR_VARIANCE)
        return anp.sum(
            anp.log(x_safe * self.sigma)
            + anp.square(anp.log(x_safe) - self.mean) * (0.5 / anp.square(self.sigma))
        )

    def __call__(self, x):
        return self.negative_log_density(x)


class Horseshoe(Distribution):
    def __init__(self, s: float):
        assert s > 0.0
        self.s = max(s, MIN_POSTERIOR_VARIANCE)
        self.__call__ = self.negative_log_density

    def negative_log_density(self, x):
        arg = anp.maximum(3.0 * anp.square(self.s / x), MIN_POSTERIOR_VARIANCE)
        return -anp.sum(anp.log(anp.log1p(arg)))

    def __call__(self, x):
        return self.negative_log_density(x)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/gluon.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""Gluon APIs for autograd"""
import threading
import warnings
import re
from collections import OrderedDict
import autograd.numpy as anp
from autograd.builtins import isinstance

__all__ = ["Block", "Parameter", "ParameterDict"]


def _indent(s_, numSpaces):
    """Indent string"""
    s = s_.split("\n")
    if len(s) == 1:
        return s_
    first = s.pop(0)
    s = [first] + [(numSpaces * " ") + line for line in s]
    s = "\n".join(s)
    return s


def shape_is_known(shape):
    """Check whether a shape is completely known with or without np semantics.
    Please see the doc of is_np_shape for more details.
    """
    if shape is None:
        return False
    unknown_dim_size = -1
    if len(shape) == 0:
        return unknown_dim_size == -1
    for dim_size in shape:
        if dim_size == unknown_dim_size:
            return False
        assert (
            dim_size > unknown_dim_size
        ), "shape dimension size cannot be less than {}, while " "received {}".format(
            unknown_dim_size, dim_size
        )
    return True


class Parameter:
    """A Container holding parameters (weights) of Blocks.
    :py:class:`Parameter` holds a copy of the parameter on each :py:class:`Context` after
    it is initialized with ``Parameter.initialize(...)``. If :py:attr:`grad_req` is
    not ``'null'``, it will also hold a gradient array on each :py:class:`Context`::
        x = np.zeros((16, 100))
        w = Parameter('fc_weight', shape=(16, 100), init=np.random.uniform)
        w.initialize()
        b.initialize()
        z = x + w.data
    Parameters
    ----------
    name : str
        Name of this parameter.
    grad_req : {'write', 'add', 'null'}, default 'write'
        Specifies how to update gradient to grad arrays.
        - ``'write'`` means everytime gradient is written to grad :py:class:`NDArray`.
        - ``'add'`` means everytime gradient is added to the grad :py:class:`NDArray`. You need
          to manually call ``zero_grad()`` to clear the gradient buffer before each
          iteration when using this option.
        - 'null' means gradient is not requested for this parameter. gradient arrays
          will not be allocated.
    shape : int or tuple of int, default None
        Shape of this parameter. By default shape is not specified. Parameter with
        unknown shape can be used for :py:class:`Symbol` API, but ``init`` will throw an error
        when using :py:class:`NDArray` API.
    dtype : numpy.dtype or str, default 'float64'
        Data type of this parameter. For example, ``numpy.float64`` or ``'float64'``.
    lr_mult : float, default 1.0
        Learning rate multiplier. Learning rate will be multiplied by lr_mult
        when updating this parameter with optimizer.
    wd_mult : float, default 1.0
        Weight decay multiplier (L2 regularizer coefficient). Works similar to lr_mult.
    init : Initializer, default None
        Initializer of this parameter. Will use the global initializer by default.
    stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.
        The storage type of the parameter.
    grad_stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.
        The storage type of the parameter's gradient.
    Attributes
    ----------
    grad_req : {'write', 'add', 'null'}
        This can be set before or after initialization. Setting ``grad_req`` to ``'null'``
        with ``x.grad_req = 'null'`` saves memory and computation when you don't
        need gradient w.r.t x.
    lr_mult : float
        Local learning rate multiplier for this Parameter. The actual learning rate
        is calculated with ``learning_rate * lr_mult``. You can set it with
        ``param.lr_mult = 2.0``
    wd_mult : float
        Local weight decay multiplier for this Parameter.
    """

    def __init__(
        self,
        name,
        grad_req="write",
        shape=None,
        dtype=anp.float64,
        lr_mult=1.0,
        wd_mult=1.0,
        init=None,
        allow_deferred_init=False,
        differentiable=True,
        stype="default",
        grad_stype="default",
    ):
        self._var = None
        self._data = None
        self._grad = None
        self._ctx_list = None
        self._ctx_map = None
        self._trainer = None
        self._deferred_init = ()
        self._differentiable = differentiable
        if allow_deferred_init:
            raise NotImplementedError(
                "allow_deferred_init is not a valid option in autograd"
            )
        self._allow_deferred_init = allow_deferred_init
        self._grad_req = None
        if isinstance(shape, int):
            shape = (shape,)
        self._shape = shape
        self.name = name
        self._dtype = dtype
        self.lr_mult = lr_mult
        self.wd_mult = wd_mult
        self.grad_req = grad_req
        self.init = init
        # sparse related storage type information
        valid_stypes = ["default"]
        assert grad_stype in valid_stypes, (
            "grad_stype for Parameter '%s' must be "
            "one of 'default', 'row_sparse', or 'csr', but got '%s'"
            % (name, grad_stype)
        )
        assert stype in valid_stypes, (
            "stype for Parameter '%s' must be "
            "one of 'default', 'row_sparse', or 'csr', but got '%s'" % (name, stype)
        )
        self._grad_stype = grad_stype
        self._stype = stype

    def __repr__(self):
        s = "Parameter {name} (shape={shape}, dtype={dtype})"
        return s.format(name=self.name, shape=self.shape, dtype=self.dtype)

    @property
    def grad_req(self):
        return self._grad_req

    @grad_req.setter
    def grad_req(self, req):
        assert req in ["write", "add", "null"], (
            "grad_req must be one of 'write', 'add', or 'null', but got '%s'" % req
        )
        if not self._differentiable:
            req = "null"
        if self._grad_req == req:
            return
        self._grad_req = req
        if req == "null" and self._grad is not None:
            self._grad = None
            self._data = [i.detach() for i in self._data]
        elif self._data is not None:
            self._init_grad()

    @property
    def dtype(self):
        """The type of the parameter.
        Setting the dtype value is equivalent to casting the value of the parameter
        """
        return self._dtype

    @dtype.setter
    def dtype(self, dtype):
        self.cast(dtype)

    @property
    def shape(self):
        """The shape of the parameter.
        By default, an unknown dimension size is 0. However, when the NumPy semantic
        is turned on, unknown dimension size is -1.
        """
        if self._shape is None:
            return None
        else:
            # Parameters shouldn't be zero-size. If one of its dimension is 0,
            # it means the parameter isn't initialized. In the NumPy semantics,
            # the unknown dimension should be marked with -1.
            return tuple(i if i != 0 else -1 for i in self._shape)

    @shape.setter
    def shape(self, new_shape):
        if self._shape is None:
            self._shape = new_shape
            return

        assert len(self._shape) == len(new_shape) and all(
            j in (-1, 0, i) for i, j in zip(new_shape, self._shape)
        ), "Expected shape %s is incompatible with given shape %s." % (
            str(new_shape),
            str(self._shape),
        )  # -1 means unknown dim size in np_shape mode

        self._shape = new_shape

    def _check_and_get(self, arr_list, ctx):
        if arr_list is not None:
            if ctx is list:
                return arr_list
            if ctx is None:
                if len(arr_list) == 1:
                    return arr_list[0]
                # else:
                #    ctx = context.current_context()
            ctx_list = self._ctx_map[ctx.device_typeid & 1]
            if ctx.device_id < len(ctx_list):
                idx = ctx_list[ctx.device_id]
                if idx is not None:
                    return arr_list[idx]
            raise RuntimeError(
                "Parameter '%s' was not initialized on context %s. "
                "It was only initialized on %s."
                % (self.name, str(ctx), str(self._ctx_list))
            )
        if self._deferred_init:
            raise NotImplementedError("Cannot enable deferred init")
        raise RuntimeError(
            "Parameter '%s' has not been initialized. Note that "
            "you should initialize parameters and create Trainer "
            "with Block.collect_params() instead of Block.params "
            "because the later does not include Parameters of "
            "nested child Blocks" % (self.name)
        )

    def _init_impl(self, data, ctx_list=None):
        """Sets data and grad."""
        self._data = [data]
        self._init_grad()

    def _init_grad(self):
        """Initialize grad buffers."""
        if self.grad_req == "null":
            self._grad = None
            return

        if self._grad_stype != "default":
            raise ValueError(
                "numpy.zeros does not support stype = {}".format(self._grad_stype)
            )
        self._grad = [anp.zeros(shape=i.shape, dtype=i.dtype) for i in self._data]

        # autograd.mark_variables(self._check_and_get(self._data, list),
        #                         self._grad, self.grad_req)

    def initialize(self, init=None, ctx=None, default_init=None, force_reinit=False):
        """Initializes parameter and gradient arrays. Only used for :py:class:`NDArray` API.
        Parameters
        ----------
        init : Initializer
            The initializer to use. Overrides :py:meth:`Parameter.init` and default_init.
        ctx : Context or list of Context, defaults to :py:meth:`context.current_context()`.
            Initialize Parameter on given context. If ctx is a list of Context, a
            copy will be made for each context.
            .. note::
                Copies are independent arrays. User is responsible for keeping
                their values consistent when updating.
                Normally :py:class:`gluon.Trainer` does this for you.
        default_init : Initializer
            Default initializer is used when both :py:func:`init`
            and :py:meth:`Parameter.init` are ``None``.
        force_reinit : bool, default False
            Whether to force re-initialization if parameter is already initialized.
        Examples
        --------
        >>> weight = mx.gluon.Parameter('weight', shape=(2, 2))
        >>> weight.initialize(ctx=mx.cpu(0))
        >>> weight.data()
        [[-0.01068833  0.01729892]
         [ 0.02042518 -0.01618656]]
        <NDArray 2x2 @cpu(0)>
        >>> weight.grad()
        [[ 0.  0.]
         [ 0.  0.]]
        <NDArray 2x2 @cpu(0)>
        >>> weight.initialize(ctx=[mx.gpu(0), mx.gpu(1)])
        >>> weight.data(mx.gpu(0))
        [[-0.00873779 -0.02834515]
         [ 0.05484822 -0.06206018]]
        <NDArray 2x2 @gpu(0)>
        >>> weight.data(mx.gpu(1))
        [[-0.00873779 -0.02834515]
         [ 0.05484822 -0.06206018]]
        <NDArray 2x2 @gpu(1)>
        """
        if default_init is None:
            default_init = anp.random.uniform
        if self._data is not None and not force_reinit:
            warnings.warn(
                "Parameter '%s' is already initialized, ignoring. "
                "Set force_reinit=True to re-initialize." % self.name,
                stacklevel=2,
            )
            return
        self._data = self._grad = None

        # init -> self.init -> default_init
        if init is None:
            init = default_init if self.init is None else self.init
        if not shape_is_known(self.shape):
            if self._allow_deferred_init:
                raise NotImplementedError("deferred_init not implemented for autograd")
                return
            raise ValueError(
                "Cannot initialize Parameter '%s' because it has "
                "invalid shape: %s." % (self.name, str(self.shape))
            )

        try:
            data = init(shape=self.shape)
        except TypeError:
            data = init(size=self.shape)
        self._init_impl(data, ctx_list=ctx)

    def reset_ctx(self, ctx):
        """Re-assign Parameter to other contexts.
        Parameters
        ----------
        ctx : Context or list of Context, default ``context.current_context()``.
            Assign Parameter to given context. If ctx is a list of Context, a
            copy will be made for each context.
        """
        return

    def set_data(self, data):
        """Sets this parameter's value on all contexts."""
        self.shape = data.shape

        if self._data is None:
            assert self._deferred_init, (
                "Parameter '%s' has not been initialized" % self.name
            )
            self._deferred_init = self._deferred_init[:3] + (data,)
            return

        # self._check_and_get(self._data, list)
        # added, raise no initialization error
        #       for arr in self._check_and_get(self._data, list):
        #           arr[:] = data
        for i in range(len(self._data)):
            self._data[i] = anp.array(data, copy=True)

    def data(self, ctx=None):
        """Returns a copy of this parameter on one context. Must have been
        initialized on this context before. For sparse parameters, use
        :py:meth:`Parameter.row_sparse_data` instead.
        Parameters
        ----------
        ctx : Context
            Desired context.
        Returns
        -------
        NDArray on ctx
        """
        if self._stype != "default":
            raise RuntimeError(
                "Cannot return a copy of Parameter '%s' on ctx %s via data() "
                "because its storage type is %s. Please use row_sparse_data() "
                "instead." % (self.name, str(ctx), self._stype)
            )
        return self._check_and_get(self._data, ctx)

    def list_data(self):
        """Returns copies of this parameter on all contexts, in the same order
        as creation. For sparse parameters, use :py:meth:`Parameter.list_row_sparse_data`
        instead.
        Returns
        -------
        list of NDArrays
        """
        if self._stype != "default":
            raise RuntimeError(
                "Cannot return copies of Parameter '%s' on all contexts via "
                "list_data() because its storage type is %s. Please use "
                "row_sparse_data() instead." % (self.name, self._stype)
            )
        return self._check_and_get(self._data, list)

    def grad(self, ctx=None):
        """Returns a gradient buffer for this parameter on one context.
        Parameters
        ----------
        ctx : Context
            Desired context.
        """
        if self._data is not None and self._grad is None:
            raise RuntimeError(
                "Cannot get gradient array for Parameter '%s' "
                "because grad_req='null'" % (self.name)
            )
        return self._check_and_get(self._grad, ctx)

    def list_grad(self):
        """Returns gradient buffers on all contexts, in the same order
        as :py:meth:`values`."""
        if self._data is not None and self._grad is None:
            raise RuntimeError(
                "Cannot get gradient array for Parameter '%s' "
                "because grad_req='null'" % (self.name)
            )
        return self._check_and_get(self._grad, list)

    def list_ctx(self):
        """Returns a list of contexts this parameter is initialized on."""
        if self._data is None:
            if self._deferred_init:
                return self._deferred_init[1]
            raise RuntimeError("Parameter '%s' has not been initialized" % self.name)
        return self._ctx_list

    def zero_grad(self):
        """Sets gradient buffer on all contexts to 0. No action is taken if
        parameter is uninitialized or doesn't require gradient."""
        if self._grad is None:
            return
        for i in self._grad:
            i[:] = 0

    def cast(self, dtype):
        """Cast data and gradient of this Parameter to a new data type.
        Parameters
        ----------
        dtype : str or numpy.dtype
            The new data type.
        """
        self._dtype = dtype
        if self._data is None:
            return

        self._data = [i.astype(dtype) for i in self._data]
        if self._grad is None:
            return
        self._grad = [i.astype(dtype) for i in self._grad]


class ParameterDict:
    """A dictionary managing a set of parameters.
    Parameters
    ----------
    prefix : str, default ``''``
        The prefix to be prepended to all Parameters' names created by this dict.
    shared : ParameterDict or None
        If not ``None``, when this dict's :py:meth:`get` method creates a new parameter, will
        first try to retrieve it from "shared" dict. Usually used for sharing
        parameters with another Block.
    """

    def __init__(self, prefix="", shared=None):
        self._prefix = prefix
        self._params = OrderedDict()
        self._shared = shared

    def __repr__(self):
        s = "{name}(\n{content}\n)"
        name = self._prefix + " " if self._prefix else ""
        return s.format(
            name=name,
            content="\n".join([_indent("  {0}".format(v), 2) for v in self.values()]),
        )

    def __getitem__(self, key):
        return self._params[key]

    def __iter__(self):
        return iter(self._params)

    def items(self):
        return self._params.items()

    def keys(self):
        return self._params.keys()

    def values(self):
        return self._params.values()

    @property
    def prefix(self):
        """Prefix of this dict. It will be prepended to :py:class:`Parameter`s' name created
        with :py:func:`get`."""
        return self._prefix

    def _get_impl(self, name):
        if name in self._params:
            return self._params[name]
        if self._shared is not None and name in self._shared._params:
            self._params[name] = self._shared._params[name]
            return self._shared._params[name]
        return None

    def get(self, name, **kwargs):
        """Retrieves a :py:class:`Parameter` with name ``self.prefix+name``. If not found,
        :py:func:`get` will first try to retrieve it from "shared" dict. If still not
        found, :py:func:`get` will create a new :py:class:`Parameter` with key-word arguments and
        insert it to self.
        Parameters
        ----------
        name : str
            Name of the desired Parameter. It will be prepended with this dictionary's
            prefix.
        **kwargs : dict
            The rest of key-word arguments for the created :py:class:`Parameter`.
        Returns
        -------
        Parameter
            The created or retrieved :py:class:`Parameter`.
        """
        name = self.prefix + name
        param = self._get_impl(name)
        if param is None:  # pylint: disable=too-many-nested-blocks
            param = Parameter(name, **kwargs)
            self._params[name] = param
        else:
            for k, v in kwargs.items():
                if hasattr(param, k) and getattr(param, k) is not None:
                    existing = getattr(param, k)
                    if k == "shape" and len(v) == len(existing):
                        inferred_shape = []
                        matched = True
                        for dim1, dim2 in zip(v, existing):
                            if dim1 != dim2 and dim1 > 0 and dim2 > 0:
                                matched = False
                                break
                            elif dim1 == dim2:
                                inferred_shape.append(dim1)
                            elif dim1 in (
                                0,
                                -1,
                            ):  # -1 means unknown dim size in np_shape mode
                                inferred_shape.append(dim2)
                            else:
                                inferred_shape.append(dim1)

                        if matched:
                            param._shape = tuple(inferred_shape)
                            continue
                    elif k == "dtype" and anp.dtype(v) == anp.dtype(existing):
                        continue

                    assert v is None or v == existing, (
                        "Cannot retrieve Parameter '%s' because desired attribute "
                        "does not match with stored for attribute '%s': "
                        "desired '%s' vs stored '%s'."
                        % (name, k, str(v), str(getattr(param, k)))
                    )
                else:
                    setattr(param, k, v)
        return param

    def update(self, other):
        """Copies all Parameters in ``other`` to self."""
        for k, v in other.items():
            if k in self._params:
                assert self._params[k] is v, (
                    "Cannot update self with other because they have different "
                    "Parameters with the same name '%s'" % k
                )

        for k, v in other.items():
            self._params[k] = v

    def initialize(self, init=None, ctx=None, verbose=False, force_reinit=False):
        """Initializes all Parameters managed by this dictionary to be used for :py:class:`NDArray`
        API. It has no effect when using :py:class:`Symbol` API.
        Parameters
        ----------
        init : Initializer
            Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.
            Otherwise, :py:meth:`Parameter.init` takes precedence.
        ctx : Context or list of Context
            Keeps a copy of Parameters on one or many context(s).
        verbose : bool, default False
            Whether to verbosely print out details on initialization.
        force_reinit : bool, default False
            Whether to force re-initialization if parameter is already initialized.
        """
        if init is None:
            init = anp.random.uniform
        if verbose:
            init.set_verbosity(verbose=verbose)
        for _, v in self.items():
            v.initialize(
                init=None, ctx=ctx, default_init=init, force_reinit=force_reinit
            )

    def reset_ctx(self, ctx):
        """Re-assign all Parameters to other contexts.
        Parameters
        ----------
        ctx : Context or list of Context, default :py:meth:`context.current_context()`.
            Assign Parameter to given context. If ctx is a list of Context, a
            copy will be made for each context.
        """
        for i in self.values():
            i.reset_ctx(ctx)

    def list_ctx(self):
        """Returns a list of all the contexts on which the underlying Parameters
        are initialized."""
        s = set()
        for i in self.values():
            s.update(i.list_ctx())
        return list(s)

    def setattr(self, name, value):
        """Set an attribute to a new value for all Parameters.
        For example, set grad_req to null if you don't need gradient w.r.t a
        model's Parameters::
            model.collect_params().setattr('grad_req', 'null')
        or change the learning rate multiplier::
            model.collect_params().setattr('lr_mult', 0.5)
        Parameters
        ----------
        name : str
            Name of the attribute.
        value : valid type for attribute name
            The new value for the attribute.
        """
        for i in self.values():
            setattr(i, name, value)


class NameManager:
    """NameManager to do automatic naming.
    Developers can also inherit from this class to change naming behavior.
    """

    _current = threading.local()

    def __init__(self):
        self._counter = {}
        self._old_manager = None

    def get(self, name, hint):
        """Get the canonical name for a symbol.
        This is the default implementation.
        If the user specifies a name,
        the user-specified name will be used.
        When user does not specify a name, we automatically generate a
        name based on the hint string.
        Parameters
        ----------
        name : str or None
            The name specified by the user.
        hint : str
            A hint string, which can be used to generate name.
        Returns
        -------
        full_name : str
            A canonical name for the symbol.
        """
        if name:
            return name
        if hint not in self._counter:
            self._counter[hint] = 0
        name = "%s%d" % (hint, self._counter[hint])
        self._counter[hint] += 1
        return name

    def __enter__(self):
        if not hasattr(NameManager._current, "value"):
            NameManager._current.value = NameManager()
        self._old_manager = NameManager._current.value
        NameManager._current.value = self
        return self

    def __exit__(self, ptype, value, trace):
        assert self._old_manager
        NameManager._current.value = self._old_manager


class Prefix(NameManager):
    """A name manager that attaches a prefix to all names.
    Examples
    --------
    >>> import mxnet as mx
    >>> data = mx.symbol.Variable('data')
    >>> with mx.name.Prefix('mynet_'):
            net = mx.symbol.FullyConnected(data, num_hidden=10, name='fc1')
    >>> net.list_arguments()
    ['data', 'mynet_fc1_weight', 'mynet_fc1_bias']
    """

    def __init__(self, prefix):
        super(Prefix, self).__init__()
        self._prefix = prefix

    def get(self, name, hint):
        name = super(Prefix, self).get(name, hint)
        return self._prefix + name


# initialize the default name manager
NameManager._current.value = NameManager()


class _BlockScope:
    """Scope for collecting child `Block` s."""

    _current = threading.local()

    def __init__(self, block):
        self._block = block
        self._counter = {}
        self._old_scope = None
        self._name_scope = None

    @staticmethod
    def create(prefix, params, hint):
        """Creates prefix and params for new `Block`."""
        current = getattr(_BlockScope._current, "value", None)
        if current is None:
            if prefix is None:
                if not hasattr(NameManager._current, "value"):
                    NameManager._current.value = NameManager()
                prefix = NameManager._current.value.get(None, hint) + "_"
            if params is None:
                params = ParameterDict(prefix)
            else:
                params = ParameterDict(params.prefix, params)
            return prefix, params

        if prefix is None:
            count = current._counter.get(hint, 0)
            prefix = "%s%d_" % (hint, count)
            current._counter[hint] = count + 1
        if params is None:
            parent = current._block.params
            params = ParameterDict(parent.prefix + prefix, parent._shared)
        else:
            params = ParameterDict(params.prefix, params)
        return current._block.prefix + prefix, params

    def __enter__(self):
        if self._block._empty_prefix:
            return self
        self._old_scope = getattr(_BlockScope._current, "value", None)
        _BlockScope._current.value = self
        self._name_scope = Prefix(self._block.prefix)
        self._name_scope.__enter__()
        return self

    def __exit__(self, ptype, value, trace):
        if self._block._empty_prefix:
            return
        self._name_scope.__exit__(ptype, value, trace)
        self._name_scope = None
        _BlockScope._current.value = self._old_scope


class Block:
    """Base class for all neural network layers and models. Your models should
    subclass this class.
    :py:class:`Block` can be nested recursively in a tree structure. You can create and
    assign child :py:class:`Block` as regular attributes::
        from mxnet.gluon import Block, nn
        from mxnet import ndarray as F
        class Model(Block):
            def __init__(self, **kwargs):
                super(Model, self).__init__(**kwargs)
                # use name_scope to give child Blocks appropriate names.
                with self.name_scope():
                    self.dense0 = nn.Dense(20)
                    self.dense1 = nn.Dense(20)
            def forward(self, x):
                x = F.relu(self.dense0(x))
                return F.relu(self.dense1(x))
        model = Model()
        model.initialize(ctx=mx.cpu(0))
        model(F.zeros((10, 10), ctx=mx.cpu(0)))
    Child :py:class:`Block` assigned this way will be registered and :py:meth:`collect_params`
    will collect their Parameters recursively. You can also manually register
    child blocks with :py:meth:`register_child`.
    Parameters
    ----------
    prefix : str
        Prefix acts like a name space. All children blocks created in parent block's
        :py:meth:`name_scope` will have parent block's prefix in their name.
        Please refer to
        `naming tutorial </api/python/docs/tutorials/packages/gluon/blocks/naming.html>`_
        for more info on prefix and naming.
    params : ParameterDict or None
        :py:class:`ParameterDict` for sharing weights with the new :py:class:`Block`. For example,
        if you want ``dense1`` to share ``dense0``'s weights, you can do::
            dense0 = nn.Dense(20)
            dense1 = nn.Dense(20, params=dense0.collect_params())
    """

    def __init__(self, prefix=None, params=None):
        self._empty_prefix = prefix == ""
        self._prefix, self._params = _BlockScope.create(prefix, params, self._alias())
        self._name = self._prefix[:-1] if self._prefix.endswith("_") else self._prefix
        self._scope = _BlockScope(self)
        self._children = OrderedDict()
        self._reg_params = {}
        self._forward_hooks = OrderedDict()
        self._forward_pre_hooks = OrderedDict()

    def __repr__(self):
        s = "{name}(\n{modstr}\n)"
        modstr = "\n".join(
            [
                "  ({key}): {block}".format(key=key, block=_indent(block.__repr__(), 2))
                for key, block in self.__dict__.items()
                if isinstance(block, Block)
            ]
        )
        return s.format(name=self.__class__.__name__, modstr=modstr)

    def __setattr__(self, name, value):
        """Registers parameters."""

        if hasattr(self, name):
            existing = getattr(self, name)
            if isinstance(existing, (Parameter, Block)) and not isinstance(
                value, type(existing)
            ):
                raise TypeError(
                    "Changing attribute type for {name} from {type1} to {type2}"
                    "is not allowed.".format(
                        name=name, type1=type(existing), type2=type(value)
                    )
                )

        if isinstance(value, Block):
            self.register_child(value, name)
        elif isinstance(value, Parameter):
            assert name not in self._reg_params, (
                "Overriding Parameter attribute %s is not allowed. "
                "If you want to share parameters between blocks, please set "
                "'params' at Block construction instead."
            )
            self._reg_params[name] = value

        super(Block, self).__setattr__(name, value)

    def _check_container_with_block(self):
        children = set(self._children.values())

        def _find_unregistered_block_in_container(data):
            # Find whether a nested container structure contains Blocks
            if isinstance(data, (list, tuple)):
                for ele in data:
                    if _find_unregistered_block_in_container(ele):
                        return True
                return False
            elif isinstance(data, dict):
                for _, v in data.items():
                    if _find_unregistered_block_in_container(v):
                        return True
                return False
            elif isinstance(data, Block):
                return not data in children
            else:
                return False

        for k, v in self.__dict__.items():
            if isinstance(v, (list, tuple, dict)) and not (
                k.startswith("__") or k == "_children"
            ):
                if _find_unregistered_block_in_container(v):
                    warnings.warn(
                        '"{name}" is an unregistered container with Blocks. '
                        "Note that Blocks inside the list, tuple or dict will not be "
                        "registered automatically. Make sure to register them using "
                        "register_child() or switching to "
                        "nn.Sequential/nn.HybridSequential instead. ".format(
                            name=self.__class__.__name__ + "." + k
                        ),
                        stacklevel=3,
                    )

    def _alias(self):
        return self.__class__.__name__.lower()

    @property
    def prefix(self):
        """Prefix of this :py:class:`Block`."""
        return self._prefix

    @property
    def name(self):
        """Name of this :py:class:`Block`, without '_' in the end."""
        return self._name

    def name_scope(self):
        """Returns a name space object managing a child :py:class:`Block` and parameter
        names. Should be used within a ``with`` statement::
            with self.name_scope():
                self.dense = nn.Dense(20)
        Please refer to
        `the naming tutorial </api/python/docs/tutorials/packages/gluon/blocks/naming.html>`_
        for more info on prefix and naming.
        """
        return self._scope

    @property
    def params(self):
        """Returns this :py:class:`Block`'s parameter dictionary (does not include its
        children's parameters)."""
        return self._params

    def collect_params(self, select=None):
        """Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its
        children's Parameters(default), also can returns the select :py:class:`ParameterDict`
        which match some given regular expressions.
        For example, collect the specified parameters in ['conv1_weight', 'conv1_bias', 'fc_weight',
        'fc_bias']::
            model.collect_params('conv1_weight|conv1_bias|fc_weight|fc_bias')
        or collect all parameters whose names end with 'weight' or 'bias', this can be done
        using regular expressions::
            model.collect_params('.*weight|.*bias')
        Parameters
        ----------
        select : str
            regular expressions
        Returns
        -------
        The selected :py:class:`ParameterDict`
        """
        # We need to check here because blocks inside containers are not supported.
        self._check_container_with_block()
        ret = ParameterDict(self._params.prefix)
        if not select:
            ret.update(self.params)
        else:
            pattern = re.compile(select)
            ret.update(
                {
                    name: value
                    for name, value in self.params.items()
                    if pattern.match(name)
                }
            )
        for cld in self._children.values():
            ret.update(cld.collect_params(select=select))
        return ret

    def _collect_params_with_prefix(self, prefix=""):
        if prefix:
            prefix += "."
        ret = {prefix + key: val for key, val in self._reg_params.items()}
        for name, child in self._children.items():
            ret.update(child._collect_params_with_prefix(prefix + name))
        return ret

    def register_child(self, block, name=None):
        """Registers block as a child of self. :py:class:`Block` s assigned to self as
        attributes will be registered automatically."""
        if name is None:
            name = str(len(self._children))
        self._children[name] = block

    # def register_forward_pre_hook(self, hook):
    #     r"""Registers a forward pre-hook on the block.
    #     The hook function is called immediately before :func:`forward`.
    #     It should not modify the input or output.
    #     Parameters
    #     ----------
    #     hook : callable
    #         The forward hook function of form `hook(block, input) -> None`.
    #     Returns
    #     -------
    #     :class:`mxnet.gluon.utils.HookHandle`
    #     """
    #     handle = HookHandle()
    #     handle.attach(self._forward_pre_hooks, hook)
    #     return handle

    # def register_forward_hook(self, hook):
    #     r"""Registers a forward hook on the block.
    #     The hook function is called immediately after :func:`forward`.
    #     It should not modify the input or output.
    #     Parameters
    #     ----------
    #     hook : callable
    #         The forward hook function of form `hook(block, input, output) -> None`.
    #     Returns
    #     -------
    #     :class:`mxnet.gluon.utils.HookHandle`
    #     """
    #     handle = HookHandle()
    #     handle.attach(self._forward_hooks, hook)
    #     return handle

    def apply(self, fn):
        r"""Applies ``fn`` recursively to every child block as well as self.
        Parameters
        ----------
        fn : callable
            Function to be applied to each submodule, of form `fn(block)`.
        Returns
        -------
        this block
        """
        for cld in self._children.values():
            cld.apply(fn)
        fn(self)
        return self

    def initialize(self, init=None, ctx=None, verbose=False, force_reinit=False):
        """Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.
        Equivalent to ``block.collect_params().initialize(...)``
        Parameters
        ----------
        init : Initializer
            Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.
            Otherwise, :py:meth:`Parameter.init` takes precedence.
        ctx : Context or list of Context
            Keeps a copy of Parameters on one or many context(s).
        verbose : bool, default False
            Whether to verbosely print out details on initialization.
        force_reinit : bool, default False
            Whether to force re-initialization if parameter is already initialized.
        """
        if init is None:
            init = anp.random.uniform
        self.collect_params().initialize(init, ctx, verbose, force_reinit)

    def hybridize(self, active=True, **kwargs):
        """Please refer description of HybridBlock hybridize()."""
        for cld in self._children.values():
            cld.hybridize(active, **kwargs)

    def cast(self, dtype):
        """Cast this Block to use another data type.
        Parameters
        ----------
        dtype : str or numpy.dtype
            The new data type.
        """
        for child in self._children.values():
            child.cast(dtype)
        for _, param in self.params.items():
            param.cast(dtype)

    def __call__(self, *args):
        """Calls forward. Only accepts positional arguments."""
        # for hook in self._forward_pre_hooks.values():
        #     hook(self, args)

        out = self.forward(*args)

        # for hook in self._forward_hooks.values():
        #     hook(self, args, out)
        # if _mx_npx.is_np_array():
        #     _check_all_np_ndarrays(out)
        return out

    def forward(self, *args):
        """Overrides to implement forward computation using :py:class:`NDArray`. Only
        accepts positional arguments.
        Parameters
        ----------
        *args : list of NDArray
            Input tensors.
        """
        raise NotImplementedError
        # pylint: disable= invalid-name

    def hybrid_forward(self, *args):
        return self(*args)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/gluon_blocks_helpers.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numbers
import autograd.numpy as anp
import numpy as np
from autograd.builtins import isinstance
from autograd.tracer import getval

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon import (
    Parameter,
    Block,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DATA_TYPE,
)

__all__ = [
    "ConstantPositiveVector",
    "PositiveScalarEncoding",
    "IdentityScalarEncoding",
    "LogarithmScalarEncoding",
    "unwrap_parameter",
    "encode_unwrap_parameter",
    "param_to_pretty_string",
    "register_parameter",
    "create_encoding",
]


def unwrap_parameter(param_internal, some_arg=None):
    assert isinstance(param_internal, Parameter)
    val = param_internal.data()
    return val


def encode_unwrap_parameter(param_internal, encoding, some_arg=None):
    return encoding.get(unwrap_parameter(param_internal, some_arg))


def param_to_pretty_string(gluon_param, encoding):
    """
    Take a gluon parameter and transform it to a string amenable to plotting
    If need be, the gluon parameter is appropriately encoded (e.g., log-exp transform).

    :param gluon_param: gluon parameter
    :param encoding: object in charge of encoding/decoding the gluon_param
    """
    assert isinstance(gluon_param, Parameter)
    assert encoding is not None, "encoding of param {} should not be None".format(
        gluon_param.name
    )
    param_as_numpy = encoding.get(getval(gluon_param.data()))

    return "{}: {}".format(
        gluon_param.name, ";".join("{:.6f}".format(value) for value in param_as_numpy)
    )


PARAMETER_POSTFIX = "_internal"


def get_name_internal(name):
    return name + PARAMETER_POSTFIX


def register_parameter(params, name, encoding, shape=(1,), dtype=DATA_TYPE):
    return params.get(
        get_name_internal(name),
        shape=shape,
        init=init_Constant(encoding.init_val_int),
        dtype=dtype,
    )


class ScalarEncodingBase:
    """
    ScalarEncodingBase
    ==================

    Base class for encoding and box constraints for Gluon parameter,
    represented as gluon.Parameter. The parameter is with shape (dimension,)
    where dimension is 1 by default.

    An encoding is given as

        param = enc(param_internal), param_internal = dec(param)

    The Gluon parameter represents param_internal, while param is what is
    visible to the outside.

    Here, enc and dec are inverses of each other. enc is used in 'get', dec
    is used in 'set'. Use 'IdentityScalarEncoding' for no encoding (identity).
    NOTE: enc (and dec) must be strictly increasing.

    Box constraints are given by
    constr_lower_int < constr_upper_int.

    Here, None means no constraint. The constraints apply to param_internal. If both
    are None, param_internal is unconstrained (default).

    NOTE: Box constraints are just maintained here, they have to be enforced
    by an optimizer!

    If regularizer is given, it specifies a regularization term for the
    (encoded) parameter which can be added to a criterion function. It is
    evaluated as regularizer(param).

    Typical use cases:
    - Unconstrained optimizer, positive scalar > lower:
      Use PositiveScalarEncoding(lower), box constraints = [None, None]
    - Optimizer supports box constaints [constr_lower, constr_upper]:
      Use IdentityScalarEncoding(constr_lower, constr_upper)
    """

    def __init__(
        self,
        init_val,
        constr_lower=None,
        constr_upper=None,
        regularizer=None,
        dimension=1,
    ):

        if constr_lower is not None and constr_upper is not None:
            assert constr_lower < constr_upper
        init_val = self._check_or_set_init_val(init_val, constr_lower, constr_upper)
        init_val_int = self.decode(init_val, "init_val")

        if constr_lower is not None:
            assert init_val >= constr_lower
            constr_lower_int = self.decode(constr_lower, "constr_lower")
        else:
            constr_lower_int = None

        if constr_upper is not None:
            assert init_val <= constr_upper
            constr_upper_int = self.decode(constr_upper, "constr_upper")
        else:
            constr_upper_int = None

        self.constraints = (constr_lower, constr_upper)
        self.constraints_internal = (constr_lower_int, constr_upper_int)
        self.init_val_int = init_val_int
        self.regularizer = regularizer
        self.dimension = dimension

    def get(self, param_internal):
        raise NotImplementedError("get must be implemented")

    def set(self, param_internal, param_val):
        assert isinstance(param_internal, Parameter)
        assert param_internal.shape == (self.dimension,)

        if isinstance(param_val, (list, np.ndarray)):
            assert len(param_val) == self.dimension
            assert np.array(param_val).ndim == 1
            val_int_list = [self.decode(val, "param_val") for val in param_val]
        else:
            assert np.isscalar(param_val) is True
            val_int_list = [self.decode(param_val, "param_val")] * self.dimension

        param_internal.set_data(anp.array(val_int_list))

    def decode(self, val, name):
        raise NotImplementedError("decode to be implemented in subclass")

    def box_constraints_internal(self, param_internal):
        assert isinstance(param_internal, Parameter)
        assert param_internal.shape == (self.dimension,)
        return {param_internal.name: self.constraints_internal}

    def box_constraints(self):
        return self.constraints

    @staticmethod
    def _check_or_set_init_val(init_val, constr_lower, constr_upper):
        # Check constraints and init value
        if init_val is not None:
            if constr_upper is not None:
                assert init_val <= constr_upper
            if constr_lower is not None:
                assert constr_lower <= init_val
        else:
            # Just pick some value which is feasible
            if constr_lower is not None:
                if constr_upper is not None:
                    init_val = 0.5 * (constr_upper + constr_lower)
                else:
                    init_val = 1.1 * constr_lower
            else:
                if constr_upper is not None:
                    init_val = 0.9 * constr_upper
                else:
                    init_val = 1.0
        return init_val


class IdentityScalarEncoding(ScalarEncodingBase):
    """
    IdentityScalarEncoding
    ======================

    Identity encoding for scalar and vector:

        param = param_internal

    This does not ensure that param is positive! Use this only if positivity
    is otherwise guaranteed.
    """

    def __init__(
        self,
        constr_lower=None,
        constr_upper=None,
        init_val=None,
        regularizer=None,
        dimension=1,
    ):
        super(IdentityScalarEncoding, self).__init__(
            init_val,
            constr_lower=constr_lower,
            constr_upper=constr_upper,
            regularizer=regularizer,
            dimension=dimension,
        )

    def get(self, param_internal):
        return param_internal

    def decode(self, val, name):
        return val


class LogarithmScalarEncoding(ScalarEncodingBase):
    """
    LogarithmScalarEncoding
    =======================

    Logarithmic encoding for scalar and vector:

        param = exp(param_internal)
        param_internal = param
    """

    def __init__(
        self,
        constr_lower=None,
        constr_upper=None,
        init_val=None,
        regularizer=None,
        dimension=1,
    ):
        super(LogarithmScalarEncoding, self).__init__(
            init_val,
            constr_lower=constr_lower,
            constr_upper=constr_upper,
            regularizer=regularizer,
            dimension=dimension,
        )

    def get(self, param_internal):
        return anp.exp(param_internal)

    def decode(self, val, name):
        assert val > 0.0, "{} = {} must be positive".format(name, val)
        return anp.log(anp.maximum(val, 1e-15))


class PositiveScalarEncoding(ScalarEncodingBase):
    """
    PositiveScalarEncoding
    ======================

    Provides encoding for positive scalar and vector: param > lower.
    Here, param is represented as gluon.Parameter. The param
    is with shape (dimension,) where dimension is 1 by default.

    The encoding is given as:

        param = softrelu(param_internal) + lower,
        softrelu(x) = log(1 + exp(x))

    If constr_upper is used, the constraint

        param_internal < dec(constr_upper)

    can be enforced by an optimizer. Since dec is increasing, this translates
    to param < constr_upper.
    NOTE: While lower is enforced by the encoding, the upper bound is not, has
    to be enforced by an optimizer.
    """

    def __init__(
        self, lower, constr_upper=None, init_val=None, regularizer=None, dimension=1
    ):
        assert isinstance(lower, numbers.Real) and lower >= 0.0
        # lower should be a real number
        self.lower = lower
        super(PositiveScalarEncoding, self).__init__(
            init_val,
            constr_lower=None,
            constr_upper=constr_upper,
            regularizer=regularizer,
            dimension=dimension,
        )

    def get(self, param_internal):
        return anp.log1p(anp.exp(param_internal)) + self.lower

    def decode(self, val, name):
        assert val > self.lower, "{} = {} must be > self.lower = {}".format(
            name, val, self.lower
        )
        # Inverse of encoding: Careful with numerics:
        # val_int = log(exp(arg) - 1) = arg + log(1 - exp(-arg))
        #         = arg + log1p(-exp(-arg))
        arg = val - self.lower
        return arg + anp.log1p(-anp.exp(-arg))


class init_Constant:
    def __init__(self, val):
        self.val = val

    def __call__(self, shape=(1,)):
        return anp.ones(shape) * self.val


def create_encoding(
    encoding_name, init_val, constr_lower, constr_upper, dimension, prior
):
    assert encoding_name in [
        "logarithm",
        "positive",
    ], "encoding name can only be 'logarithm' or 'positive'"

    if encoding_name == "logarithm":
        return LogarithmScalarEncoding(
            init_val=init_val,
            constr_lower=constr_lower,
            constr_upper=constr_upper,
            dimension=dimension,
            regularizer=prior,
        )
    else:
        return PositiveScalarEncoding(
            lower=constr_lower,
            init_val=init_val,
            constr_upper=constr_upper,
            dimension=dimension,
            regularizer=prior,
        )


class ConstantPositiveVector(Block):
    """
    ConstantPositiveVector

    # Just constant positive vectors...
    ======================

    Represents constant vector, with positive entry value represented as Gluon
    parameter, to be used in the context of wrapper classes in
    gluon_blocks.py. Shape, dtype, and context are determined from the
    features argument:

    - If features.shape = (n, d):
       shape = (d, 1) if size_cols = True (number cols of features)
       shape = (n, 1) if size_cols = False (number rows of features)
    - dtype = features.dtype, ctx = features.ctx

    Encoding and internal Gluon parameter:
    The positive scalar parameter is encoded via encoding (see
    ScalarEncodingBase). The internal Gluon parameter (before encoding) has the
    name param_name + '_internal'.
    """

    def __init__(self, param_name, encoding, size_cols, **kwargs):
        super(ConstantPositiveVector, self).__init__(**kwargs)
        assert isinstance(encoding, ScalarEncodingBase)
        self.param_name = param_name
        self.encoding = encoding
        self.size_cols = size_cols
        with self.name_scope():
            self.param_internal = register_parameter(self.params, param_name, encoding)

    def forward(self, features, param_internal):
        """Returns constant positive vector

        If features.shape = (n, d), the shape of the vector returned is
        (d, 1) if size_cols = True, (n, 1) otherwise.

        :param features: Matrix for shape, dtype, ctx
        :param param_internal: Unwrapped parameter
        :return: Constant positive vector
        """
        # Shape, dtype, ctx is determined by extracting column or row from
        # features, then use ones_like
        axis = 0 if self.size_cols else 1
        ones_vec = anp.ones((features.size // getval(features.shape)[axis], 1))
        param = anp.reshape(self.encoding.get(param_internal), (1, 1))
        return anp.multiply(ones_vec, param)
        # returned (para_internal, ..., para_interval)

    def set(self, val):
        self.encoding.set(self.param_internal, val)

    def get(self):
        param_internal = unwrap_parameter(self.param_internal, None)
        return self.encoding.get(param_internal)

    def get_box_constraints_internal(self):
        return self.encoding.box_constraints_internal(self.param_internal)

    def log_parameters(self):
        return "{} = {}".format(self.param_name, self.get())

    def get_parameters(self):
        return {self.param_name: self.get()}

    def switch_updating(self, flag):
        """Is the underlying parameter updated during learning?

        By default, the parameter takes part in learning (its grad_req
        attribute is 'write'). For flag == False, the attribute is
        flipped to 'null', and the parameter remains constant during
        learning.

        :param flag: Update parameter during learning?
        """
        grad_req = "write" if flag else "null"
        self.param_internal.grad_req = grad_req

    def has_regularizer(self):
        return self.encoding.regularizer is not None

    def eval_regularizer(self, features):
        if self.has_regularizer():
            param_internal = unwrap_parameter(self.param_internal, features)
            param = self.encoding.get(param_internal)
            return self.encoding.regularizer(param)
        else:
            return 0.0

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/gp_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import autograd.numpy as anp
from autograd.builtins import isinstance
from typing import List, Optional
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DATA_TYPE,
    OptimizationConfig,
    DEFAULT_OPTIMIZATION_CONFIG,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    PosteriorState,
    PosteriorStateWithSampleJoint,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    MarginalLikelihood,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.optimization_utils import (
    apply_lbfgs_with_multiple_starts,
    create_lbfgs_arguments,
)

logger = logging.getLogger(__name__)


class GaussianProcessModel:
    """
    Base class for Gaussian-linear models which support parameter fitting and
    prediction.
    """

    def __init__(self, random_seed=None):
        if random_seed is None:
            random_seed = 31415927
        self._random_state = np.random.RandomState(random_seed)

    @property
    def random_state(self) -> np.random.RandomState:
        return self._random_state

    @property
    def states(self) -> Optional[List[PosteriorState]]:
        """
        :return: Current posterior states (one per MCMC sample; just a single
            state if model parameters are optimized)
        """
        raise NotImplementedError

    def fit(self, data: dict, profiler: Optional[SimpleProfiler] = None):
        """
        Adjust model parameters based on training data `data`. Can be done via
        optimization or MCMC sampling. The posterior states are computed at the
        end as well.

        :param data: Training data
        :param profiler: Used for profiling
        """
        raise NotImplementedError

    def recompute_states(self, data: dict):
        """
        Recomputes posterior states for current model parameters.

        :param data: Training data
        """
        raise NotImplementedError

    @staticmethod
    def _check_and_format_input(u):
        """
        Check and massage the input to conform with the numerical type and context

        :param u: some np.ndarray
        """
        assert isinstance(u, anp.ndarray)

        if u.ndim == 1:
            u = anp.reshape(u, (-1, 1))
        if u.dtype != DATA_TYPE:
            return anp.array(u, dtype=DATA_TYPE)
        else:
            return u

    @staticmethod
    def _check_features_targets(features, targets):
        features = GaussianProcessModel._check_and_format_input(features)
        targets = GaussianProcessModel._check_and_format_input(targets)
        assert features.shape[0] == targets.shape[0], (
            f"features and targets should have the same number of points "
            + f"(received {features.shape[0]} and {targets.shape[0]})"
        )
        return features, targets

    def predict(self, features_test: np.ndarray):
        """
        Compute the posterior mean(s) and variance(s) for the points in features_test.
        If the posterior state is based on m target vectors, a (n, m) matrix is returned for posterior means.

        :param features_test: Data matrix X_test of size (n, d) (type np.ndarray) for which n predictions are made
        :return: posterior_means, posterior_variances
        """
        features_test = self._assert_check_xtest(features_test)

        predictions = []
        for state in self.states:
            post_means, post_vars = state.predict(features_test)
            # Just to make sure the return shapes are the same as before:
            if post_means.shape[1] == 1:
                post_means = anp.reshape(post_means, (-1,))
            predictions.append((post_means, post_vars))
        return predictions

    def _assert_check_xtest(self, features_test: np.ndarray):
        assert self.states is not None, "Posterior state does not exist (run 'fit')"
        features_test = self._check_and_format_input(features_test)
        return features_test

    def multiple_targets(self):
        """
        :return: Posterior state based on multiple (fantasized) target
        """
        assert self.states is not None, "Posterior state does not exist (run 'fit')"
        return self.states[0].num_fantasies > 1

    def sample_marginals(self, features_test: np.ndarray, num_samples: int = 1):
        """
        Draws marginal samples from predictive distribution at n test points.
        Notice we concat the samples for each state. Let n_states = len(self._states)

        If the posterior state is based on m > 1 target vectors, a
        (n, m, num_samples * n_states) tensor is returned, for m == 1 we return a
        (n, num_samples * n_states) matrix.

        :param features_test: Test input points, shape (n, d)
        :param num_samples: Number of samples
        :return: Samples with shape (n, num_samples * n_states) or
            (n, m, num_samples * n_states) if m > 1
        """
        features_test = self._assert_check_xtest(features_test)
        samples_list = [
            state.sample_marginals(
                features_test, num_samples, random_state=self._random_state
            )
            for state in self.states
        ]
        return _concatenate_samples(samples_list)

    def sample_joint(self, features_test: np.ndarray, num_samples: int = 1):
        """
        Draws joint samples from predictive distribution at n test points.
        This scales cubically with n.
        the posterior state must be based on a single target vector
        (m > 1 is not supported).

        :param features_test: Test input points, shape (n, d)
        :param num_samples: Number of samples
        :return: Samples, shape (n, num_samples)
        """
        features_test = self._assert_check_xtest(features_test)
        assert isinstance(
            self.states[0], PosteriorStateWithSampleJoint
        ), "Implemented only for joint Gaussian process models"
        samples_list = [
            state.sample_joint(
                features_test, num_samples, random_state=self._random_state
            )
            for state in self.states
        ]
        return _concatenate_samples(samples_list)


def _concatenate_samples(samples_list: List[anp.ndarray]) -> anp.ndarray:
    return anp.concatenate(samples_list, axis=-1)


class GaussianProcessOptimizeModel(GaussianProcessModel):
    """
    Base class for models where parameters are fit by maximizing the marginal
    likelihood.
    """

    def __init__(
        self,
        optimization_config: OptimizationConfig = None,
        random_seed=None,
        fit_reset_params: bool = True,
    ):
        super().__init__(random_seed)
        if optimization_config is None:
            optimization_config = DEFAULT_OPTIMIZATION_CONFIG
        self._states = None
        self.fit_reset_params = fit_reset_params
        self.optimization_config = optimization_config

    @property
    def states(self) -> Optional[List[PosteriorState]]:
        return self._states

    @property
    def likelihood(self) -> MarginalLikelihood:
        raise NotImplementedError

    def fit(self, data: dict, profiler: SimpleProfiler = None):
        """
        Fit the model parameters by optimizing the marginal likelihood,
        and set posterior states.

        We catch exceptions during the optimization restarts. If any restarts
        fail, log messages are written. If all restarts fail, the current
        parameters are not changed.

        :param data: Input data
        :param profiler: Profiler, optional
        """
        self.likelihood.on_fit_start(data, profiler)
        if self.fit_reset_params:
            self.reset_params()
        n_starts = self.optimization_config.n_starts
        ret_infos = apply_lbfgs_with_multiple_starts(
            *create_lbfgs_arguments(
                criterion=self.likelihood,
                crit_args=[data],
                verbose=self.optimization_config.verbose,
            ),
            bounds=self.likelihood.box_constraints_internal(),
            random_state=self._random_state,
            n_starts=n_starts,
            tol=self.optimization_config.lbfgs_tol,
            maxiter=self.optimization_config.lbfgs_maxiter,
        )

        # Logging in response to failures of optimization runs
        n_succeeded = sum(x is None for x in ret_infos)
        if n_succeeded < n_starts:
            log_msg = "[GaussianProcessOptimizeModel.fit]\n"
            log_msg += (
                "{} of the {} restarts failed with the following exceptions:\n".format(
                    n_starts - n_succeeded, n_starts
                )
            )
            copy_params = {
                param.name: param.data()
                for param in self.likelihood.collect_params().values()
            }
            for i, ret_info in enumerate(ret_infos):
                if ret_info is not None:
                    log_msg += "- Restart {}: Exception {}\n".format(
                        i, ret_info["type"]
                    )
                    log_msg += "  Message: {}\n".format(ret_info["msg"])
                    log_msg += "  Args: {}\n".format(ret_info["args"])
                    # Set parameters in order to print them. These are the
                    # parameters for which the evaluation failed
                    self._set_likelihood_params(ret_info["params"])
                    log_msg += "  Params: " + str(self.get_params())
                    logger.info(log_msg)
            # Restore parameters
            self._set_likelihood_params(copy_params)
            if n_succeeded == 0:
                logger.info(
                    "All restarts failed: Skipping hyperparameter fitting for now"
                )
        # Recompute posterior state for new hyperparameters
        self._recompute_states(data)

    def _set_likelihood_params(self, params: dict):
        for param in self.likelihood.collect_params().values():
            vec = params.get(param.name)
            if vec is not None:
                param.set_data(vec)

    def recompute_states(self, data: dict):
        self._recompute_states(data)

    def _recompute_states(self, data: dict):
        self.likelihood.data_precomputations(data)
        self._states = [self.likelihood.get_posterior_state(data)]

    def get_params(self):
        return self.likelihood.get_params()

    def set_params(self, param_dict):
        self.likelihood.set_params(param_dict)

    def reset_params(self):
        """
        Reset hyperparameters to their initial values (or resample them).
        """
        self.likelihood.reset_params(self._random_state)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/gp_regression.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    OptimizationConfig,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_model import (
    GaussianProcessOptimizeModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    MarginalLikelihood,
    GaussianProcessMarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
    MeanFunction,
)

logger = logging.getLogger(__name__)


class GaussianProcessRegression(GaussianProcessOptimizeModel):
    """
    Gaussian Process Regression

    Takes as input a mean function (which depends on X only) and a kernel
    function.

    :param kernel: Kernel function (for instance, a Matern52---note we cannot
        provide Matern52() as default argument since we need to provide with
        the dimension of points in X)
    :param mean: Mean function (which depends on X only)
    :param initial_noise_variance: Initial value for noise variance parameter
    :param optimization_config: Configuration that specifies the behavior of
        the optimization of the marginal likelihood.
    :param random_seed: Random seed to be used (optional)
    :param fit_reset_params: Reset parameters to initial values before running
        'fit'? If False, 'fit' starts from the current values
    """

    def __init__(
        self,
        kernel: KernelFunction,
        mean: MeanFunction = None,
        initial_noise_variance: float = None,
        optimization_config: OptimizationConfig = None,
        random_seed=None,
        fit_reset_params: bool = True,
    ):
        super().__init__(
            optimization_config=optimization_config,
            random_seed=random_seed,
            fit_reset_params=fit_reset_params,
        )
        if mean is None:
            mean = ScalarMeanFunction()
        self._likelihood = GaussianProcessMarginalLikelihood(
            kernel=kernel, mean=mean, initial_noise_variance=initial_noise_variance
        )
        self.reset_params()

    @property
    def likelihood(self) -> MarginalLikelihood:
        return self._likelihood

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/gpr_mcmc.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Callable, Optional, List
import autograd.numpy as anp
from autograd.builtins import isinstance
from numpy.random import RandomState

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_MCMC_CONFIG,
    MCMCConfig,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_model import (
    GaussianProcessModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.optimization_utils import (
    add_regularizer_to_criterion,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    GaussianProcessMarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    GaussProcPosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.slice import (
    SliceSampler,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler


class GPRegressionMCMC(GaussianProcessModel):
    def __init__(
        self,
        build_kernel: Callable[[], KernelFunction],
        mcmc_config: MCMCConfig = DEFAULT_MCMC_CONFIG,
        random_seed=None,
    ):
        super().__init__(random_seed)
        self.mcmc_config = mcmc_config
        self.likelihood = _create_likelihood(
            build_kernel, random_state=self._random_state
        )
        self._states = None
        self.samples = None
        self.build_kernel = build_kernel

    @property
    def states(self) -> Optional[List[GaussProcPosteriorState]]:
        return self._states

    @property
    def number_samples(self) -> int:
        return self.mcmc_config.n_samples

    def fit(self, data: dict, profiler: Optional[SimpleProfiler] = None):
        features, targets = self._check_features_targets(
            features=data["features"], targets=data["targets"]
        )
        assert (
            targets.shape[1] == 1
        ), "targets cannot be a matrix if parameters are to be fit"

        mean_function = self.likelihood.mean
        if isinstance(mean_function, ScalarMeanFunction):
            mean_function.set_mean_value(anp.mean(targets))

        def _log_posterior_density(hp_values: anp.ndarray) -> float:
            # We check box constraints before converting hp_values to
            # internal
            if not self._is_feasible(hp_values):
                return -float("inf")
            # Decode and write into Gluon parameters
            _set_gp_hps(hp_values, self.likelihood)
            neg_log = add_regularizer_to_criterion(
                criterion=self.likelihood, crit_args=[data]
            )
            return -neg_log

        slice_sampler = SliceSampler(
            log_density=_log_posterior_density,
            scale=1.0,
            random_state=self._random_state,
        )
        init_hp_values = _get_gp_hps(self.likelihood)

        self.samples = slice_sampler.sample(
            init_hp_values,
            self.mcmc_config.n_samples,
            self.mcmc_config.n_burnin,
            self.mcmc_config.n_thinning,
        )
        self._states = self._create_posterior_states(self.samples, features, targets)

    def recompute_states(self, data: dict):
        """
        Supports fantasizing, in that targets can be a matrix. Then,
        ycols = targets.shape[1] must be a multiple of self.number_samples.

        """
        features, targets = self._check_features_targets(
            features=data["features"], targets=data["targets"]
        )
        ycols = targets.shape[1]
        if ycols > 1:
            assert ycols % self.number_samples == 0, (
                f"targets.shape[1] = {ycols}, must be multiple of number_samples"
                + f" = {self.number_samples}"
            )
        else:
            assert ycols == 1, "targets must not be empty"
        assert len(self.samples) > 0
        self._states = self._create_posterior_states(self.samples, features, targets)

    def _is_feasible(self, hp_values: anp.ndarray) -> bool:
        pos = 0
        for _, encoding in self.likelihood.param_encoding_pairs():
            lower, upper = encoding.box_constraints()
            dim = encoding.dimension
            if lower is not None or upper is not None:
                values = hp_values[pos : (pos + dim)]
                if (lower is not None) and any(values < lower):
                    return False
                if (upper is not None) and any(values > upper):
                    return False
            pos += dim
        return True

    def _create_posterior_states(self, samples, features, targets):
        ycols = targets.shape[1]
        if ycols == 1:
            num_fantasy_samples = 0
        else:
            num_fantasy_samples = ycols // self.number_samples
            ycols = num_fantasy_samples
        states = []
        offset = 0
        for sample in samples:
            likelihood = _create_likelihood(
                self.build_kernel, random_state=self._random_state
            )
            _set_gp_hps(sample, likelihood)
            targets_part = targets[:, offset : (offset + ycols)]
            state = GaussProcPosteriorState(
                features=features,
                targets=targets_part,
                mean=likelihood.mean,
                kernel=likelihood.kernel,
                noise_variance=likelihood.get_noise_variance(as_ndarray=True),
            )
            states.append(state)
            offset += num_fantasy_samples
        return states


def _get_gp_hps(likelihood: GaussianProcessMarginalLikelihood) -> anp.ndarray:
    """Get GP hyper-parameters as numpy array for a given likelihood object."""
    hp_values = []
    for param_int, encoding in likelihood.param_encoding_pairs():
        hp_values.append(encode_unwrap_parameter(param_int, encoding))
    return anp.concatenate(hp_values)


def _set_gp_hps(
    params_numpy: anp.ndarray, likelihood: GaussianProcessMarginalLikelihood
):
    """Set GP hyper-parameters from numpy array for a given likelihood object."""
    pos = 0
    for param, encoding in likelihood.param_encoding_pairs():
        dim = encoding.dimension
        values = params_numpy[pos : (pos + dim)]
        if dim == 1:
            internal_values = encoding.decode(values, param.name)
        else:
            internal_values = anp.array(
                [encoding.decode(v, param.name) for v in values]
            )
        param.set_data(internal_values)
        pos += dim


def _create_likelihood(
    build_kernel, random_state: RandomState
) -> GaussianProcessMarginalLikelihood:
    """
    Create a MarginalLikelihood object with default initial GP hyperparameters.
    """
    likelihood = GaussianProcessMarginalLikelihood(
        kernel=build_kernel(), mean=ScalarMeanFunction(), initial_noise_variance=None
    )
    # Note: The `init` parameter is a default sampler which is used only
    # for parameters which do not have initializers specified. Right now,
    # all our parameters have such initializers (constant in general),
    # so this is just to be safe (if `init` is not specified here, it
    # defaults to `np.random.uniform`, whose seed we do not control).
    likelihood.initialize(init=random_state.uniform, force_reinit=True)

    return likelihood

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/independent/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/independent/gpind_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Callable, List, Tuple

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    OptimizationConfig,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_model import (
    GaussianProcessOptimizeModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    MarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.independent.likelihood import (
    IndependentGPPerResourceMarginalLikelihood,
)

logger = logging.getLogger(__name__)


class IndependentGPPerResourceModel(GaussianProcessOptimizeModel):
    """
    GP multi-fidelity model over f(x, r), where for each r, f(x, r) is
    represented by an independent GP. The different processes share the same
    kernel, but have their own mean functions mu_r and covariance scales c_r.

    The likelihood object is not created at construction, but only with
    `create_likelihood`. This is because we need to know the rung levels of
    the Hyperband scheduler.

    :param kernel: Kernel function without covariance scale, shared by models
        for all resources r
    :param mean_factory: Factory function for mean functions mu_r(x)
    :param resource_attr_range: (r_min, r_max)
    :param separate_noise_variances: Separate noise variance for each r?
        Otherwise, noise variance is shared
    :param initial_noise_variance: Initial value for noise variance parameter
    :param initial_covariance_scale: Initial value for covariance scale
        parameters c_r
    :param optimization_config: Configuration that specifies the behavior of
        the optimization of the marginal likelihood.
    :param random_seed: Random seed to be used (optional)
    :param fit_reset_params: Reset parameters to initial values before running
        'fit'? If False, 'fit' starts from the current values
    """

    def __init__(
        self,
        kernel: KernelFunction,
        mean_factory: Callable[[int], MeanFunction],
        resource_attr_range: Tuple[int, int],
        separate_noise_variances: bool = False,
        initial_noise_variance: float = None,
        initial_covariance_scale: float = None,
        optimization_config: OptimizationConfig = None,
        random_seed=None,
        fit_reset_params: bool = True,
    ):
        super().__init__(
            optimization_config=optimization_config,
            random_seed=random_seed,
            fit_reset_params=fit_reset_params,
        )
        self._kernel = kernel
        self._mean_factory = mean_factory
        self._resource_attr_range = resource_attr_range
        self._likelihood_kwargs = {
            "separate_noise_variances": separate_noise_variances,
            "initial_noise_variance": initial_noise_variance,
            "initial_covariance_scale": initial_covariance_scale,
        }
        self._likelihood = None  # Delayed creation

    def create_likelihood(self, rung_levels: List[int]):
        """
        Delayed creation of likelihood, needs to know rung levels of Hyperband
        scheduler.

        Note: last entry of `rung_levels` must be `max_t`, even if this is not
        a rung level in Hyperband.

        :param rung_levels: Rung levels
        """
        mean = {resource: self._mean_factory(resource) for resource in rung_levels}
        self._likelihood = IndependentGPPerResourceMarginalLikelihood(
            kernel=self._kernel,
            mean=mean,
            resource_attr_range=self._resource_attr_range,
            **self._likelihood_kwargs,
        )
        self.reset_params()

    @property
    def likelihood(self) -> MarginalLikelihood:
        assert (
            self._likelihood is not None
        ), "Call create_likelihood (passing rung levels) in order to complete creation"
        return self._likelihood

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/independent/likelihood.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List, Dict, Tuple
import numpy as np
import autograd.numpy as anp

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    MarginalLikelihood,
    GaussianProcessMarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.independent.posterior_state import (
    IndependentGPPerResourcePosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    INITIAL_NOISE_VARIANCE,
    NOISE_VARIANCE_LOWER_BOUND,
    NOISE_VARIANCE_UPPER_BOUND,
    DEFAULT_ENCODING,
    INITIAL_COVARIANCE_SCALE,
    COVARIANCE_SCALE_LOWER_BOUND,
    COVARIANCE_SCALE_UPPER_BOUND,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    Gamma,
    LogNormal,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    register_parameter,
    create_encoding,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    PosteriorState,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import (
    SimpleProfiler,
)


class IndependentGPPerResourceMarginalLikelihood(MarginalLikelihood):
    """
    Marginal likelihood for GP multi-fidelity model over f(x, r), where for
    each r, f(x, r) is represented by an independent GP. The different
    processes share the same kernel, but have their own mean functions mu_r
    and covariance scales c_r.
    If `separate_noise_variances` is True, each process has its own noise
    variance, otherwise all processes share the same noise variance.
    """

    def __init__(
        self,
        kernel: KernelFunction,
        mean: Dict[int, MeanFunction],
        resource_attr_range: Tuple[int, int],
        separate_noise_variances: bool = False,
        initial_noise_variance=None,
        initial_covariance_scale=None,
        encoding_type=None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        if initial_noise_variance is None:
            initial_noise_variance = INITIAL_NOISE_VARIANCE
        if initial_covariance_scale is None:
            initial_covariance_scale = INITIAL_COVARIANCE_SCALE
        if encoding_type is None:
            encoding_type = DEFAULT_ENCODING
        self.encoding_noise = create_encoding(
            encoding_name=encoding_type,
            init_val=initial_noise_variance,
            constr_lower=NOISE_VARIANCE_LOWER_BOUND,
            constr_upper=NOISE_VARIANCE_UPPER_BOUND,
            dimension=1,
            prior=Gamma(mean=0.1, alpha=0.1),
        )
        self.encoding_covscale = create_encoding(
            encoding_name=encoding_type,
            init_val=initial_covariance_scale,
            constr_lower=COVARIANCE_SCALE_LOWER_BOUND,
            constr_upper=COVARIANCE_SCALE_UPPER_BOUND,
            dimension=1,
            prior=LogNormal(0.0, 1.0),
        )
        self.mean = mean.copy()
        for v in self.mean.values():
            self.register_child(v)
        self.kernel = kernel
        self.resource_attr_range = resource_attr_range
        self._separate_noise_variances = separate_noise_variances
        with self.name_scope():
            self.covariance_scale_internal = {
                resource: register_parameter(
                    self.params, f"covariance_scale_{resource}", self.encoding_covscale
                )
                for resource in mean.keys()
            }
            if separate_noise_variances:
                self.noise_variance_internal = {
                    resource: register_parameter(
                        self.params, f"noise_variance_{resource}", self.encoding_noise
                    )
                    for resource in mean.keys()
                }
            else:
                self.noise_variance_internal = register_parameter(
                    self.params, "noise_variance", self.encoding_noise
                )

    def _noise_variance(self):
        if self._separate_noise_variances:
            return {
                resource: encode_unwrap_parameter(internal, self.encoding_noise)
                for resource, internal in self.noise_variance_internal.items()
            }
        else:
            return encode_unwrap_parameter(
                self.noise_variance_internal, self.encoding_noise
            )

    def _covariance_scale(self):
        return {
            resource: encode_unwrap_parameter(internal, self.encoding_covscale)
            for resource, internal in self.covariance_scale_internal.items()
        }

    def get_posterior_state(self, data: dict) -> PosteriorState:
        GaussianProcessMarginalLikelihood._assert_data_entries(data)
        return IndependentGPPerResourcePosteriorState(
            features=data["features"],
            targets=data["targets"],
            kernel=self.kernel,
            mean=self.mean,
            covariance_scale=self._covariance_scale(),
            noise_variance=self._noise_variance(),
            resource_attr_range=self.resource_attr_range,
        )

    def forward(self, data: dict):
        return self.get_posterior_state(data).neg_log_likelihood()

    def param_encoding_pairs(self) -> List[tuple]:
        result = self.kernel.param_encoding_pairs()
        for mean in self.mean.values():
            result.extend(mean.param_encoding_pairs())
        for internal in self.covariance_scale_internal.values():
            result.append((internal, self.encoding_covscale))
        _noise_variances = (
            list(self.noise_variance_internal.values())
            if self._separate_noise_variances
            else [self.noise_variance_internal]
        )
        for internal in _noise_variances:
            result.append((internal, self.encoding_noise))
        return result

    def get_noise_variance(self, as_ndarray=False):
        noise_variance = self._noise_variance()
        if as_ndarray:
            return noise_variance
        elif self._separate_noise_variances:
            return {k: anp.reshape(v, (1,))[0] for k, v in noise_variance.items()}
        else:
            return anp.reshape(noise_variance, (1,))[0]

    def _set_noise_variance(self, resource: int, val: float):
        if self._separate_noise_variances:
            internal = self.noise_variance_internal.get(resource)
            assert internal is not None, f"resource = {resource} not supported"
        else:
            internal = self.noise_variance_internal
        self.encoding_noise.set(internal, val)

    def get_covariance_scale(self, resource: int, as_ndarray=False):
        cov_scale = self._covariance_scale()[resource]
        return cov_scale if as_ndarray else anp.reshape(cov_scale, (1,))[0]

    def set_covariance_scale(self, resource: int, val: float):
        internal = self.covariance_scale_internal.get(resource)
        assert internal is not None, f"resource = {resource} not supported"
        self.encoding_covscale.set(internal, val)

    def _components(self):
        return [("kernel_", self.kernel)] + [
            (f"mean{resource}_", mean) for resource, mean in self.mean.items()
        ]

    def get_params(self) -> Dict[str, np.ndarray]:
        result = dict()
        for pref, func in self._components():
            result.update({(pref + k): v for k, v in func.get_params().items()})
        for resource, val in self._covariance_scale().items():
            result[f"covariance_scale{resource}"] = val
        if self._separate_noise_variances:
            for resource, val in self._noise_variance().items():
                result[f"noise_variance{resource}"] = val
        else:
            result["noise_variance"] = self._noise_variance()
        return result

    def set_params(self, param_dict: Dict[str, np.ndarray]):
        for pref, func in self._components():
            len_pref = len(pref)
            stripped_dict = {
                k[len_pref:]: v for k, v in param_dict.items() if k.startswith(pref)
            }
            func.set_params(stripped_dict)
        for resource in self.covariance_scale_internal.keys():
            self.set_covariance_scale(
                resource, param_dict[f"covariance_scale{resource}"]
            )
        if self._separate_noise_variances:
            for resource in self.noise_variance_internal.keys():
                self._set_noise_variance(
                    resource, param_dict[f"noise_variance{resource}"]
                )
        else:
            self._set_noise_variance(1, param_dict["noise_variance"])

    def on_fit_start(self, data: dict, profiler: Optional[SimpleProfiler] = None):
        GaussianProcessMarginalLikelihood._assert_data_entries(data)
        targets = data["targets"]
        assert (
            targets.shape[1] == 1
        ), "targets cannot be a matrix if parameters are to be fit"

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/independent/posterior_state.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Tuple, Optional, Callable, Union
import numpy as np
import autograd.numpy as anp
from numpy.random import RandomState

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    PosteriorStateWithSampleJoint,
    GaussProcPosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_impl import (
    decode_extended_features,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)


NoiseVariance = Union[np.ndarray, Dict[int, np.ndarray]]


class IndependentGPPerResourcePosteriorState(PosteriorStateWithSampleJoint):
    """
    Posterior state for model over f(x, r), where for a fixed set of resource
    levels r, each f(x, r) is represented by an independent Gaussian process.
    These processes share a common covariance function k(x, x), but can have
    their own mean functions mu_r and covariance scales c_r. They can also
    have their own noise variances, or the noise variance is shared.

    Attention: Predictions can only be done at (x, r) where r has at least
    one training datapoint. This is because a posterior state cannot
    represent the prior.
    """

    def __init__(
        self,
        features: np.ndarray,
        targets: np.ndarray,
        kernel: KernelFunction,
        mean: Dict[int, MeanFunction],
        covariance_scale: Dict[int, np.ndarray],
        noise_variance: NoiseVariance,
        resource_attr_range: Tuple[int, int],
        debug_log: bool = False,
    ):
        """
        `mean` and `covariance_scale` map supported resource levels r to
        mean function mu_r and covariance scale c_r.

        :param features: Input points X, extended features, shape (n, d)
        :param targets: Targets Y, shape (n, m)
        :param kernel: Kernel function k(X, X')
        :param mean: See above
        :param covariance_scale: See above
        :param noise_variance: See above
        :param resource_attr_range: (r_min, r_max)
        """
        assert isinstance(kernel, KernelFunction), "kernel must be KernelFunction"
        self.rung_levels = sorted(mean.keys())
        assert self.rung_levels == sorted(
            covariance_scale.keys()
        ), "mean, covariance_scale must have the same keys"
        if isinstance(noise_variance, dict):
            assert self.rung_levels == sorted(
                noise_variance.keys()
            ), "mean, noise_variance must have the same keys"
        else:
            assert isinstance(
                noise_variance, np.ndarray
            ), "noise_variance must be Dict[int, np.ndarray] or np.ndarray"
            _noise_variance = noise_variance
            noise_variance = {
                resource: _noise_variance for resource in self.rung_levels
            }
        self._compute_states(
            features,
            targets,
            kernel,
            mean,
            covariance_scale,
            noise_variance,
            resource_attr_range,
            debug_log,
        )
        self._mean = mean  # See `sample_joint`
        self._num_data = features.shape[0]
        self._num_features = features.shape[1]
        self._num_fantasies = targets.shape[1]
        self._resource_attr_range = resource_attr_range

    def _compute_states(
        self,
        features: np.ndarray,
        targets: np.ndarray,
        kernel: KernelFunction,
        mean: Dict[int, MeanFunction],
        covariance_scale: Dict[int, np.ndarray],
        noise_variance: Dict[int, np.ndarray],
        resource_attr_range: Tuple[int, int],
        debug_log: bool = False,
    ):
        features, resources = decode_extended_features(features, resource_attr_range)
        self._states = dict()
        for resource, mean_function in mean.items():
            cov_scale = covariance_scale[resource]
            rows = np.flatnonzero(resources == resource)
            if rows.size > 0:
                r_features = features[rows]
                r_targets = targets[rows]
                self._states[resource] = GaussProcPosteriorState(
                    features=r_features,
                    targets=r_targets,
                    mean=mean_function,
                    kernel=(kernel, cov_scale),
                    noise_variance=noise_variance[resource],
                    debug_log=debug_log,
                )

    def state(self, resource: int) -> GaussProcPosteriorState:
        return self._states[resource]

    @property
    def num_data(self):
        return self._num_data

    @property
    def num_features(self):
        return self._num_features

    @property
    def num_fantasies(self):
        return self._num_fantasies

    def neg_log_likelihood(self) -> anp.ndarray:
        return anp.sum([state.neg_log_likelihood() for state in self._states.values()])

    # Different to `sample_marginals`, `sample_joint`, this method supports
    # `autograd` differentiation
    def predict(self, test_features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        test_features, resources = decode_extended_features(
            test_features, self._resource_attr_range
        )
        if len(set(resources)) == 1:
            return self._states[resources[0]].predict(test_features)
        else:
            num_rows = resources.size
            # Group resources together by sorting them
            ind = np.argsort(resources)
            test_features = test_features[ind]
            resources = resources[ind]
            # Find positions where resource value changes
            change_pos = (
                [0]
                + list(np.flatnonzero(resources[:-1] != resources[1:]) + 1)
                + [num_rows]
            )
            p_means, p_vars = zip(
                *[
                    self._states[resources[start]].predict(test_features[start:end])
                    for start, end in zip(change_pos[:-1], change_pos[1:])
                ]
            )
            reverse_ind = np.empty_like(ind)
            reverse_ind[ind] = np.arange(num_rows)
            posterior_means = anp.concatenate(p_means, axis=0)[reverse_ind]
            posterior_variances = anp.concatenate(p_vars, axis=0)[reverse_ind]
            return posterior_means, posterior_variances

    def _split_features(self, features: np.ndarray):
        features, resources = decode_extended_features(
            features, self._resource_attr_range
        )
        result = dict()
        for resource in set(resources):
            rows = np.flatnonzero(resources == resource)
            result[resource] = (features[rows], rows)
        return result

    def _sample_internal(
        self,
        test_features: np.ndarray,
        sample_func: Callable[[int, np.ndarray], np.ndarray],
        num_samples: int,
    ) -> np.ndarray:
        features_per_resource = self._split_features(test_features)
        num_test = test_features.shape[0]
        nf = self.num_fantasies
        shp = (num_test, num_samples) if nf == 1 else (num_test, nf, num_samples)
        samples = np.zeros(shp)
        bc_shp = (1,) * (len(shp) - 1)
        for resource, (features, rows) in features_per_resource.items():
            if resource in self._states:
                sample_part = sample_func(resource, features)
            else:
                assert resource in self._mean, (
                    f"resource = {resource} not supported (keys = "
                    + str(list(self._mean.keys()))
                    + ")"
                )
                vec = self._mean[resource](features)
                sample_part = np.reshape(vec, (vec.size,) + bc_shp)
            samples[rows] = sample_part
        return samples

    def sample_marginals(
        self,
        test_features: np.ndarray,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> np.ndarray:
        """
        Different to `predict`, entries in `test_features`
        may have resources not covered by data in posterior state. For such
        entries, we return the prior mean. We do not sample from the prior.
        If `sample_marginals` is used to draw fantasy values, this corresponds to
        the Kriging believer heuristic.
        """

        def sample_func(resource: int, features: np.ndarray):
            return self._states[resource].sample_marginals(
                features, num_samples, random_state
            )

        return self._sample_internal(
            test_features=test_features,
            sample_func=sample_func,
            num_samples=num_samples,
        )

    def sample_joint(
        self,
        test_features: np.ndarray,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> np.ndarray:
        """
        Different to `predict`, entries in `test_features`
        may have resources not covered by data in posterior state. For such
        entries, we return the prior mean. We do not sample from the prior.
        If `sample_joint` is used to draw fantasy values, this corresponds to
        the Kriging believer heuristic.
        """

        def sample_func(resource: int, features: np.ndarray):
            return self._states[resource].sample_joint(
                features, num_samples, random_state
            )

        return self._sample_internal(
            test_features=test_features,
            sample_func=sample_func,
            num_samples=num_samples,
        )

    def backward_gradient(
        self,
        input: np.ndarray,
        head_gradients: Dict[str, np.ndarray],
        mean_data: float,
        std_data: float,
    ) -> np.ndarray:
        inner_input, resource = decode_extended_features(
            input.reshape((1, -1)), self._resource_attr_range
        )
        assert resource.size == 1
        resource = resource.item()
        inner_grad = (
            self._states[resource]
            .backward_gradient(inner_input, head_gradients, mean_data, std_data)
            .reshape((-1,))
        )
        return np.reshape(np.concatenate((inner_grad, np.zeros((1,)))), input.shape)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# TODO wildcard import should be avoided
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import *  # noqa: F401
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.exponential_decay import *  # noqa: F401
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.fabolas import *  # noqa: F401
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.product_kernel import *  # noqa: F401
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.freeze_thaw import *  # noqa: F401
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.cross_validation import *  # noqa: F401
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.range_kernel import *  # noqa: F401

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/base.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp
from autograd.tracer import getval

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    INITIAL_COVARIANCE_SCALE,
    INITIAL_INVERSE_BANDWIDTHS,
    DEFAULT_ENCODING,
    INVERSE_BANDWIDTHS_LOWER_BOUND,
    INVERSE_BANDWIDTHS_UPPER_BOUND,
    COVARIANCE_SCALE_LOWER_BOUND,
    COVARIANCE_SCALE_UPPER_BOUND,
    NUMERICAL_JITTER,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    Uniform,
    LogNormal,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon import Block
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    register_parameter,
    create_encoding,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)

__all__ = ["KernelFunction", "Matern52"]


class KernelFunction(MeanFunction):
    """
    Base class of kernel (or covariance) functions
    """

    def __init__(self, dimension: int, **kwargs):
        """
        :param dimension: Dimensionality of input points after encoding into
            ndarray
        """
        super().__init__(**kwargs)
        self._dimension = dimension

    @property
    def dimension(self):
        """
        :return: Dimension d of input points
        """
        return self._dimension

    def diagonal(self, X):
        """
        :param X: Input data, shape (n, d)
        :return: Diagonal of K(X, X), shape (n,)
        """
        raise NotImplementedError

    def diagonal_depends_on_X(self):
        """
        For stationary kernels, diagonal does not depend on X

        :return: Does diagonal(X) depend on X?
        """
        raise NotImplementedError

    def _check_input_shape(self, X):
        return anp.reshape(X, (getval(X.shape[0]), self._dimension))


class SquaredDistance(Block):
    """
    Block that is responsible for the computation of matrices of squared
    distances. The distances can possibly be weighted (e.g., ARD
    parametrization). For instance:
        X1 with size (n1,d)
        X2 with size (n2,d)
        inverse_bandwidths with size (1,d)
        results in a matrix of size (n1,n2) with i,j entry equal to
            sum_{k=1}^d (X1[i,k] - X2[j,k])^2 * inverse_bandwidths[k]^2

    if ARD == False, inverse_bandwidths is equal to a scalar broadcast to the
    d components (with d=dimension, i.e., the number of features in X)
    otherwise, inverse_bandwidths is (1,d)
    """

    def __init__(self, dimension, ARD=False, encoding_type=DEFAULT_ENCODING, **kwargs):
        super().__init__(**kwargs)
        self.ARD = ARD
        inverse_bandwidths_dimension = 1 if not ARD else dimension
        self.encoding = create_encoding(
            encoding_type,
            INITIAL_INVERSE_BANDWIDTHS,
            INVERSE_BANDWIDTHS_LOWER_BOUND,
            INVERSE_BANDWIDTHS_UPPER_BOUND,
            inverse_bandwidths_dimension,
            Uniform(INVERSE_BANDWIDTHS_LOWER_BOUND, INVERSE_BANDWIDTHS_UPPER_BOUND),
        )

        with self.name_scope():
            self.inverse_bandwidths_internal = register_parameter(
                self.params,
                "inverse_bandwidths",
                self.encoding,
                shape=(inverse_bandwidths_dimension,),
            )

    def _inverse_bandwidths(self):
        return encode_unwrap_parameter(self.inverse_bandwidths_internal, self.encoding)

    def forward(self, X1, X2):
        """
        Actual computation of the matrix of squared distances (see details above)

        :param X1: input data of size (n1,d)
        :param X2: input data of size (n2,d)
        """
        # In case inverse_bandwidths if of size (1, dimension), dimension>1,
        # ARD is handled by broadcasting
        inverse_bandwidths = anp.reshape(self._inverse_bandwidths(), (1, -1))

        if X2 is X1:
            X1_scaled = anp.multiply(X1, inverse_bandwidths)
            D = -2.0 * anp.dot(X1_scaled, anp.transpose(X1_scaled))
            X1_squared_norm = anp.sum(anp.square(X1_scaled), axis=1)
            D = D + anp.reshape(X1_squared_norm, (1, -1))
            D = D + anp.reshape(X1_squared_norm, (-1, 1))
        else:
            X1_scaled = anp.multiply(X1, inverse_bandwidths)
            X2_scaled = anp.multiply(X2, inverse_bandwidths)
            X1_squared_norm = anp.sum(anp.square(X1_scaled), axis=1)
            X2_squared_norm = anp.sum(anp.square(X2_scaled), axis=1)
            D = -2.0 * anp.matmul(X1_scaled, anp.transpose(X2_scaled))
            D = D + anp.reshape(X1_squared_norm, (-1, 1))
            D = D + anp.reshape(X2_squared_norm, (1, -1))

        return anp.abs(D)

    def get_params(self):
        """
        Parameter keys are inv_bw<k> if dimension > 1, and inv_bw if
        dimension == 1.
        """
        inverse_bandwidths = anp.reshape(self._inverse_bandwidths(), (-1,))
        if inverse_bandwidths.size == 1:
            return {"inv_bw": inverse_bandwidths[0]}
        else:
            return {
                "inv_bw{}".format(k): inverse_bandwidths[k]
                for k in range(inverse_bandwidths.size)
            }

    def set_params(self, param_dict):
        dimension = self.encoding.dimension
        if dimension == 1:
            inverse_bandwidths = [param_dict["inv_bw"]]
        else:
            keys = ["inv_bw{}".format(k) for k in range(dimension)]
            for k in keys:
                assert k in param_dict, "'{}' not in param_dict = {}".format(
                    k, param_dict
                )
            inverse_bandwidths = [param_dict[k] for k in keys]
        self.encoding.set(self.inverse_bandwidths_internal, inverse_bandwidths)


class Matern52(KernelFunction):
    """
    Block that is responsible for the computation of Matern52 kernel
    matrices. For instance:
        X1 with size (n1,d)
        X2 with size (n2,d)
    results in a matrix of size (n1,n2) with i,j entry equal to the
    Matern52 kernel at (X1[i,:], X2[j,:]).

    If ARD == False, inverse_bandwidths is equal to a scalar broadcast to the
    d components (with d=dimension, i.e., the number of features in X)
    otherwise (ARD == True), inverse_bandwidths is (1,d)
    """

    def __init__(
        self,
        dimension,
        ARD=False,
        encoding_type=DEFAULT_ENCODING,
        has_covariance_scale=True,
        **kwargs
    ):
        super(Matern52, self).__init__(dimension, **kwargs)
        self.ARD = ARD
        self.has_covariance_scale = has_covariance_scale
        self.squared_distance = SquaredDistance(
            dimension=dimension, ARD=ARD, encoding_type=encoding_type
        )
        if has_covariance_scale:
            self.encoding = create_encoding(
                encoding_name=encoding_type,
                init_val=INITIAL_COVARIANCE_SCALE,
                constr_lower=COVARIANCE_SCALE_LOWER_BOUND,
                constr_upper=COVARIANCE_SCALE_UPPER_BOUND,
                dimension=1,
                prior=LogNormal(0.0, 1.0),
            )
            with self.name_scope():
                self.covariance_scale_internal = register_parameter(
                    self.params, "covariance_scale", self.encoding
                )

    def _covariance_scale(self):
        if self.has_covariance_scale:
            return encode_unwrap_parameter(
                self.covariance_scale_internal, self.encoding
            )
        else:
            return 1.0

    def forward(self, X1, X2):
        """
        Actual computation of the Matern52 kernel matrix (see details above)

        :param X1: input data of size (n1, d)
        :param X2: input data of size (n2, d)
        """
        covariance_scale = self._covariance_scale()
        X1 = self._check_input_shape(X1)
        if X2 is not X1:
            X2 = self._check_input_shape(X2)
        D = self.squared_distance(X1, X2)
        # Using the plain np.sqrt is numerically unstable for D ~ 0
        # (non-differentiability)
        # that's why we add NUMERICAL_JITTER
        B = anp.sqrt(5.0 * D + NUMERICAL_JITTER)
        K = anp.multiply((1.0 + B + 5.0 / 3.0 * D) * anp.exp(-B), covariance_scale)

        return K

    def diagonal(self, X):
        X = self._check_input_shape(X)
        covariance_scale = self._covariance_scale()
        covariance_scale_times_ones = anp.multiply(
            anp.ones((getval(X.shape[0]), 1)), covariance_scale
        )

        return anp.reshape(covariance_scale_times_ones, (-1,))

    def diagonal_depends_on_X(self):
        return False

    def param_encoding_pairs(self):
        result = [
            (
                self.squared_distance.inverse_bandwidths_internal,
                self.squared_distance.encoding,
            )
        ]
        if self.has_covariance_scale:
            result.insert(0, (self.covariance_scale_internal, self.encoding))
        return result

    def get_covariance_scale(self):
        if self.has_covariance_scale:
            return self._covariance_scale()[0]
        else:
            return 1.0

    def set_covariance_scale(self, covariance_scale):
        assert self.has_covariance_scale, "covariance_scale is fixed to 1"
        self.encoding.set(self.covariance_scale_internal, covariance_scale)

    def get_params(self):
        result = self.squared_distance.get_params()
        if self.has_covariance_scale:
            result["covariance_scale"] = self.get_covariance_scale()
        return result

    def set_params(self, param_dict):
        self.squared_distance.set_params(param_dict)
        if self.has_covariance_scale:
            self.set_covariance_scale(param_dict["covariance_scale"])

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/cross_validation.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp
from autograd.builtins import isinstance

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)

__all__ = ["CrossValidationKernelFunction", "CrossValidationMeanFunction"]


def decode_resource_values(res_encoded, num_folds):
    """
    We assume the resource attribute r is encoded as `randint(1, num_folds)`.
    Internally, r is taken as value in the real interval
    `[0.5, num_folds + 0.5]`, which is linearly transformed to `[0, 1]` for
    encoding.

    :param res_encoded: Encoded values r
    :param num_folds: Maximum number of folds
    :return: Original values r (not rounded to int)
    """
    return res_encoded * num_folds + 0.5


class CrossValidationKernelFunction(KernelFunction):
    """
    Kernel function suitable for f(x, r) being the average of r validation
    metrics evaluated on different (train, validation) splits.

    More specifically, there are 'num_folds` such splits, and f(x, r) is
    the average over the first r of them.

    We model the score on fold k as

        e_k(x) = f(x) + g_k(x),

    where f(x) and the g_k(x) are a priori independent Gaussian processes with
    kernels `kernel_main` and `kernel_residual` (all g_k share the same kernel).
    Moreover, the g_k are zero-mean, while f(x) may have a mean function. Then:

        f(x, r) = (1/r) sum_{k <= r} e_k(x)

        k((x, r), (x', r')) = k_main(x, x') + k_residual(x, x') / max(r, r')

    Note that `kernel_main`, `kernel_residual` are over inputs x (dimension d),
    while the kernel represented here is over inputs (x, r) of dimension d + 1,
    where the resource attribute r (number of folds) is last.

    Inputs are encoded. We assume a linear encoding for r with bounds 1 and
    `num_folds`.
    TODO: Right now, all HPs are encoded, and the resource attribute counts as
    HP, even if it is not optimized over. This creates a dependence to how
    inputs are encoded.
    """

    def __init__(
        self,
        kernel_main: KernelFunction,
        kernel_residual: KernelFunction,
        mean_main: MeanFunction,
        num_folds: int,
        **kwargs,
    ):
        """
        :param kernel_main: Kernel for main effect f(x)
        :param kernel_residual: Kernel for residuals g_k(x)
        :param mean_main: Mean function for main effect f(x)
        :param num_folds: Maximum number of folds: 1 <= r <= `num_folds`
        """
        super().__init__(dimension=kernel_main.dimension + 1, **kwargs)
        assert kernel_main.dimension == kernel_residual.dimension, (
            f"kernel_main.dimension = {kernel_main.dimension} != "
            + f"{kernel_residual.dimension} = kernel_residual.dimension"
        )
        assert (
            round(num_folds) == num_folds and num_folds >= 2
        ), f"num_folds = {num_folds} must be int >= 2"
        self.kernel_main = kernel_main
        self.kernel_residual = kernel_residual
        self.mean_main = mean_main
        self.num_folds = num_folds

    def _compute_terms(self, X):
        dim = self.kernel_main.dimension
        cfg = X[:, :dim]
        res_encoded = X[:, dim:]
        res_decoded = decode_resource_values(res_encoded, self.num_folds)
        return cfg, res_decoded

    def forward(self, X1, X2, **kwargs):
        cfg1, res1 = self._compute_terms(X1)
        if X2 is not X1:
            cfg2, res2 = self._compute_terms(X2)
        else:
            cfg2, res2 = cfg1, res1
        res1 = anp.reshape(res1, (-1, 1))
        res2 = anp.reshape(res2, (1, -1))

        kmat_main = self.kernel_main(cfg1, cfg2)
        kmat_residual = self.kernel_residual(cfg1, cfg2)
        max_resources = anp.maximum(res1, res2)

        return (kmat_residual / max_resources) + kmat_main

    def diagonal(self, X):
        cfg, res = self._compute_terms(X)
        res = anp.reshape(res, (-1,))
        kdiag_main = self.kernel_main.diagonal(cfg)
        kdiag_residual = self.kernel_residual.diagonal(cfg)

        return (kdiag_residual / res) + kdiag_main

    def diagonal_depends_on_X(self):
        return True

    def param_encoding_pairs(self):
        enc_list = []
        enc_list.extend(self.kernel_main.param_encoding_pairs())
        enc_list.extend(self.kernel_residual.param_encoding_pairs())
        enc_list.extend(self.mean_main.param_encoding_pairs())
        return enc_list

    def mean_function(self, X):
        cfg, _ = self._compute_terms(X)
        return self.mean_main(cfg)

    def get_params(self):
        result = dict()
        for pref, func in [
            ("kernelm_", self.kernel_main),
            ("meanm_", self.mean_main),
            ("kernelr_", self.kernel_residual),
        ]:
            result.update({(pref + k): v for k, v in func.get_params().items()})
        return result

    def set_params(self, param_dict):
        for pref, func in [
            ("kernelm_", self.kernel_main),
            ("meanm_", self.mean_main),
            ("kernelr_", self.kernel_residual),
        ]:
            len_pref = len(pref)
            stripped_dict = {
                k[len_pref:]: v for k, v in param_dict.items() if k.startswith(pref)
            }
            func.set_params(stripped_dict)


class CrossValidationMeanFunction(MeanFunction):
    def __init__(self, kernel: CrossValidationKernelFunction, **kwargs):
        super().__init__(**kwargs)
        assert isinstance(kernel, CrossValidationKernelFunction)
        self.kernel = kernel

    def forward(self, X):
        return self.kernel.mean_function(X)

    def param_encoding_pairs(self):
        return []

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/exponential_decay.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp
from autograd.builtins import isinstance

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_ENCODING,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    unwrap_parameter,
    IdentityScalarEncoding,
    register_parameter,
    get_name_internal,
    create_encoding,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)

__all__ = [
    "ExponentialDecayResourcesKernelFunction",
    "ExponentialDecayResourcesMeanFunction",
]


class ExponentialDecayResourcesKernelFunction(KernelFunction):
    """
    Variant of the kernel function for modeling exponentially decaying
    learning curves, proposed in:

        Swersky, K., Snoek, J., & Adams, R. P. (2014).
        Freeze-Thaw Bayesian Optimization.
        ArXiv:1406.3896 [Cs, Stat).
        Retrieved from http://arxiv.org/abs/1406.3896

    The argument in that paper actually justifies using a non-zero mean
    function (see ExponentialDecayResourcesMeanFunction) and centralizing
    the kernel proposed there. This is done here. Details in:

        Tiao, Klein, Archambeau, Seeger (2020)
        Model-based Asynchronous Hyperparameter Optimization
        https://arxiv.org/abs/2003.10865

    We implement a new family of kernel functions, for which the additive
    Freeze-Thaw kernel is one instance (delta = 0).
    The kernel has parameters alpha, mean_lam, gamma > 0, and delta in [0, 1].
    Note that beta = alpha / mean_lam is used in the Freeze-Thaw paper (the
    Gamma distribution over lambda is parameterized differently).
    The additive Freeze-Thaw kernel is obtained for delta = 0 (use
    delta_fixed_value = 0).

    In fact, this class is configured with a kernel and a mean function over
    inputs x (dimension d) and represents a kernel (and mean function) over
    inputs (x, r) (dimension d + 1), where the resource attribute r >= 0 is
    last.
    """

    def __init__(
        self,
        kernel_x: KernelFunction,
        mean_x: MeanFunction,
        encoding_type=DEFAULT_ENCODING,
        alpha_init=1.0,
        mean_lam_init=0.5,
        gamma_init=0.5,
        delta_fixed_value=None,
        delta_init=0.5,
        max_metric_value=1.0,
        **kwargs
    ):
        """
        :param kernel_x: Kernel k_x(x, x') over configs
        :param mean_x: Mean function mu_x(x) over configs
        :param encoding_type: Encoding used for alpha, mean_lam, gamma (positive
            values)
        :param alpha_init: Initial value alpha
        :param mean_lam_init: Initial value mean_lam
        :param gamma_init: Initial value gamma
        :param delta_fixed_value: If not None, delta is fixed to this value, and
            does not become a free parameter
        :param delta_init: Initial value delta (if delta_fixed_value is None)
        :param max_metric_value: Maximum value which metric can attend. This is
            used as upper bound on gamma
        """
        super().__init__(dimension=kernel_x.dimension + 1, **kwargs)
        self.kernel_x = kernel_x
        self.mean_x = mean_x
        alpha_lower, alpha_upper = 1e-6, 250.0
        alpha_init = self._wrap_initvals(alpha_init, alpha_lower, alpha_upper)
        self.encoding_alpha = create_encoding(
            encoding_type, alpha_init, alpha_lower, alpha_upper, 1, None
        )
        mean_lam_lower, mean_lam_upper = 1e-4, 50.0
        mean_lam_init = self._wrap_initvals(
            mean_lam_init, mean_lam_lower, mean_lam_upper
        )
        self.encoding_mean_lam = create_encoding(
            encoding_type, mean_lam_init, mean_lam_lower, mean_lam_upper, 1, None
        )
        # If f(x, 0) is the metric value at r -> 0, f(x) at r -> infty,
        # then f(x, 0) = gamma (for delta = 1), or f(x, 0) = gamma + f(x) for
        # delta = 0. gamma should not be larger than the maximum metric
        # value.
        gamma_lower = max_metric_value * 0.0001
        gamma_upper = max_metric_value
        gamma_init = self._wrap_initvals(gamma_init, gamma_lower, gamma_upper)
        self.encoding_gamma = create_encoding(
            encoding_type, gamma_init, gamma_lower, gamma_upper, 1, None
        )
        if delta_fixed_value is None:
            delta_init = self._wrap_initvals(delta_init, 0.0, 1.0)
            self.encoding_delta = IdentityScalarEncoding(
                constr_lower=0.0, constr_upper=1.0, init_val=delta_init
            )
        else:
            assert (
                0.0 <= delta_fixed_value <= 1.0
            ), "delta_fixed_value = {}, must lie in [0, 1]".format(delta_fixed_value)
            self.encoding_delta = None
            self.delta_fixed_value = delta_fixed_value

        with self.name_scope():
            self.alpha_internal = register_parameter(
                self.params, "alpha", self.encoding_alpha
            )
            self.mean_lam_internal = register_parameter(
                self.params, "mean_lam", self.encoding_mean_lam
            )
            self.gamma_internal = register_parameter(
                self.params, "gamma", self.encoding_gamma
            )
            if delta_fixed_value is None:
                self.delta_internal = register_parameter(
                    self.params, "delta", self.encoding_delta
                )

    @staticmethod
    def _wrap_initvals(init, lower, upper):
        return max(min(init, upper * 0.999), lower * 1.001)

    @staticmethod
    def _compute_kappa(x, alpha, mean_lam):
        beta = alpha / mean_lam
        return anp.power(anp.divide(beta, anp.add(x, beta)), alpha)

    def _compute_terms(self, X, alpha, mean_lam, gamma, delta, ret_mean=False):
        dim = self.kernel_x.dimension
        cfg = X[:, :dim]
        res = X[:, dim:]
        kappa = self._compute_kappa(res, alpha, mean_lam)
        kr_pref = anp.reshape(gamma, (1, 1))

        if ret_mean or (self.encoding_delta is not None) or delta > 0.0:
            mean = self.mean_x(cfg)
        else:
            mean = None
        if self.encoding_delta is not None:
            kr_pref = anp.subtract(kr_pref, anp.multiply(delta, mean))
        elif delta > 0.0:
            kr_pref = anp.subtract(kr_pref, mean * delta)

        return cfg, res, kappa, kr_pref, mean

    @staticmethod
    def _unwrap(X, kwargs, key, enc, var_internal):
        return enc.get(
            kwargs.get(get_name_internal(key), unwrap_parameter(var_internal, X))
        )

    def _get_params(self, X, **kwargs):
        alpha = self._unwrap(
            X, kwargs, "alpha", self.encoding_alpha, self.alpha_internal
        )
        mean_lam = self._unwrap(
            X, kwargs, "mean_lam", self.encoding_mean_lam, self.mean_lam_internal
        )
        gamma = self._unwrap(
            X, kwargs, "gamma", self.encoding_gamma, self.gamma_internal
        )
        if self.encoding_delta is not None:
            delta = anp.reshape(
                self._unwrap(
                    X, kwargs, "delta", self.encoding_delta, self.delta_internal
                ),
                (1, 1),
            )
        else:
            delta = self.delta_fixed_value

        return (alpha, mean_lam, gamma, delta)

    def forward(self, X1, X2, **kwargs):
        alpha, mean_lam, gamma, delta = self._get_params(X1, **kwargs)
        cfg1, res1, kappa1, kr_pref1, _ = self._compute_terms(
            X1, alpha, mean_lam, gamma, delta
        )
        if X2 is not X1:
            cfg2, res2, kappa2, kr_pref2, _ = self._compute_terms(
                X2, alpha, mean_lam, gamma, delta
            )
        else:
            cfg2, res2, kappa2, kr_pref2 = cfg1, res1, kappa1, kr_pref1
        res2 = anp.reshape(res2, (1, -1))
        kappa2 = anp.reshape(kappa2, (1, -1))
        kr_pref2 = anp.reshape(kr_pref2, (1, -1))
        kappa12 = self._compute_kappa(anp.add(res1, res2), alpha, mean_lam)
        kmat_res = anp.subtract(kappa12, anp.multiply(kappa1, kappa2))
        kmat_res = anp.multiply(kr_pref1, anp.multiply(kr_pref2, kmat_res))

        kmat_x = self.kernel_x(cfg1, cfg2)
        if self.encoding_delta is None:
            if delta > 0.0:
                tmpmat = anp.add(kappa1, anp.subtract(kappa2, kappa12 * delta))
                tmpmat = tmpmat * (-delta) + 1.0
            else:
                tmpmat = 1.0
        else:
            tmpmat = anp.add(kappa1, anp.subtract(kappa2, anp.multiply(kappa12, delta)))
            tmpmat = anp.multiply(tmpmat, -delta) + 1.0

        return kmat_x * tmpmat + kmat_res

    def diagonal(self, X):
        alpha, mean_lam, gamma, delta = self._get_params(X)
        cfg, res, kappa, kr_pref, _ = self._compute_terms(
            X, alpha, mean_lam, gamma, delta
        )
        kappa2 = self._compute_kappa(res * 2, alpha, mean_lam)
        kdiag_res = anp.subtract(kappa2, anp.square(kappa))
        kdiag_res = anp.reshape(anp.multiply(kdiag_res, anp.square(kr_pref)), (-1,))
        kdiag_x = self.kernel_x.diagonal(cfg)
        if self.encoding_delta is None:
            if delta > 0.0:
                tmpvec = anp.subtract(kappa * 2, kappa2 * delta)
                tmpvec = anp.reshape(tmpvec * (-delta) + 1.0, (-1,))
            else:
                tmpvec = 1.0
        else:
            tmpvec = anp.subtract(kappa * 2, anp.multiply(kappa2, delta))
            tmpvec = anp.reshape(anp.multiply(tmpvec, -delta) + 1.0, (-1,))

        return kdiag_x * tmpvec + kdiag_res

    def diagonal_depends_on_X(self):
        return True

    def param_encoding_pairs(self):
        enc_list = [
            (self.alpha_internal, self.encoding_alpha),
            (self.mean_lam_internal, self.encoding_mean_lam),
            (self.gamma_internal, self.encoding_gamma),
        ]
        if self.encoding_delta is not None:
            enc_list.append((self.delta_internal, self.encoding_delta))
        enc_list.extend(self.kernel_x.param_encoding_pairs())
        enc_list.extend(self.mean_x.param_encoding_pairs())

        return enc_list

    def mean_function(self, X):
        alpha, mean_lam, gamma, delta = self._get_params(X)
        cfg, res, kappa, kr_pref, mean = self._compute_terms(
            X, alpha, mean_lam, gamma, delta, ret_mean=True
        )

        return anp.add(mean, anp.multiply(kappa, kr_pref))

    def get_params(self):
        """
        Parameter keys are alpha, mean_lam, gamma, delta (only if not fixed
        to delta_fixed_value), as well as those of self.kernel_x (prefix
        'kernelx_') and of self.mean_x (prefix 'meanx_').
        """
        values = list(self._get_params(None))
        keys = ["alpha", "mean_lam", "gamma", "delta"]
        if self.encoding_delta is None:
            values.pop()
            keys.pop()
        result = {k: anp.reshape(v, (1,))[0] for k, v in zip(keys, values)}
        for pref, func in [("kernelx_", self.kernel_x), ("meanx_", self.mean_x)]:
            result.update({(pref + k): v for k, v in func.get_params().items()})

        return result

    def set_params(self, param_dict):
        for pref, func in [("kernelx_", self.kernel_x), ("meanx_", self.mean_x)]:
            len_pref = len(pref)
            stripped_dict = {
                k[len_pref:]: v for k, v in param_dict.items() if k.startswith(pref)
            }
            func.set_params(stripped_dict)
        self.encoding_alpha.set(self.alpha_internal, param_dict["alpha"])
        self.encoding_mean_lam.set(self.mean_lam_internal, param_dict["mean_lam"])
        self.encoding_gamma.set(self.gamma_internal, param_dict["gamma"])
        if self.encoding_delta is not None:
            self.encoding_delta.set(self.delta_internal, param_dict["delta"])


class ExponentialDecayResourcesMeanFunction(MeanFunction):
    def __init__(self, kernel: ExponentialDecayResourcesKernelFunction, **kwargs):
        super(ExponentialDecayResourcesMeanFunction, self).__init__(**kwargs)
        assert isinstance(kernel, ExponentialDecayResourcesKernelFunction)
        self.kernel = kernel

    def forward(self, X):
        return self.kernel.mean_function(X)

    def param_encoding_pairs(self):
        return []

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/fabolas.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    COVARIANCE_SCALE_LOWER_BOUND,
    COVARIANCE_SCALE_UPPER_BOUND,
    DEFAULT_ENCODING,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    IdentityScalarEncoding,
    register_parameter,
    create_encoding,
)

__all__ = ["FabolasKernelFunction"]


class FabolasKernelFunction(KernelFunction):
    """
    The kernel function proposed in:

        Klein, A., Falkner, S., Bartels, S., Hennig, P., & Hutter, np. (2016).
        Fast Bayesian Optimization of Machine Learning Hyperparameters
        on Large Datasets, in AISTATS 2017.
        ArXiv:1605.07079 [Cs, Stat]. Retrieved from http://arxiv.org/abs/1605.07079

    Please note this is only one of the components of the factorized kernel
    proposed in the paper. This is the finite-rank ("degenerate") kernel for
    modelling data subset fraction sizes. Defined as:

        k(x, y) = (U phi(x))^T (U phi(y)),  x, y in [0, 1],
        phi(x) = [1, (1 - x)^2]^T,  U = [[u1, u3], [0, u2]] upper triangular,
        u1, u2 > 0.
    """

    def __init__(
        self,
        dimension=1,
        encoding_type=DEFAULT_ENCODING,
        u1_init=1.0,
        u3_init=0.0,
        **kwargs
    ):
        super(FabolasKernelFunction, self).__init__(dimension=dimension, **kwargs)
        self.encoding_u12 = create_encoding(
            encoding_type,
            u1_init,
            COVARIANCE_SCALE_LOWER_BOUND,
            COVARIANCE_SCALE_UPPER_BOUND,
            1,
            None,
        )
        # This is not really needed, but param_encoding_pairs needs an encoding
        # for each parameter
        self.encoding_u3 = IdentityScalarEncoding(init_val=u3_init)
        with self.name_scope():
            self.u1_internal = register_parameter(self.params, "u1", self.encoding_u12)
            self.u2_internal = register_parameter(self.params, "u2", self.encoding_u12)
            self.u3_internal = register_parameter(self.params, "u3", self.encoding_u3)

    @staticmethod
    def _compute_factor(x, u1, u2, u3):
        tvec = (1.0 - x) ** 2
        return anp.concatenate(
            [anp.add(anp.multiply(tvec, u3), u1), anp.multiply(tvec, u2)], axis=1
        )

    def forward(self, X1, X2):
        u1_internal = self.u1_internal.data()
        u2_internal = self.u1_internal.data()
        u3_internal = self.u1_internal.data()
        X1 = self._check_input_shape(X1)
        u1 = self.encoding_u12.get(u1_internal)
        u2 = self.encoding_u12.get(u2_internal)
        u3 = self.encoding_u3.get(u3_internal)
        mat1 = self._compute_factor(X1, u1, u2, u3)
        if X2 is X1:
            return anp.dot(mat1, anp.transpose(mat1))
        else:
            X2 = self._check_input_shape(X2)
            mat2 = self._compute_factor(X2, u1, u2, u3)
            return anp.dot(mat1, anp.transpose(mat2))

    def _get_pars(self, X):
        u1 = encode_unwrap_parameter(self.u1_internal, self.encoding_u12, X)
        u2 = encode_unwrap_parameter(self.u2_internal, self.encoding_u12, X)
        u3 = encode_unwrap_parameter(self.u3_internal, self.encoding_u3, X)
        return (u1, u2, u3)

    def diagonal(self, X):
        X = self._check_input_shape(X)
        u1, u2, u3 = self._get_pars(X)
        mat = self._compute_factor(X, u1, u2, u3)
        return anp.sum(mat**2, axis=1)

    def diagonal_depends_on_X(self):
        return True

    def param_encoding_pairs(self):
        return [
            (self.u1_internal, self.encoding_u12),
            (self.u2_internal, self.encoding_u12),
            (self.u3_internal, self.encoding_u3),
        ]

    def get_params(self):
        values = list(self._get_pars(None))
        keys = ["u1", "u2", "u3"]
        return {k: anp.reshape(v, (1,))[0] for k, v in zip(keys, values)}

    def set_params(self, param_dict):
        self.encoding_u12.set(self.u1_internal, param_dict["u1"])
        self.encoding_u12.set(self.u2_internal, param_dict["u2"])
        self.encoding_u3.set(self.u3_internal, param_dict["u3"])

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/freeze_thaw.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp
from autograd.builtins import isinstance
from autograd.tracer import getval

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.exponential_decay import (
    ExponentialDecayResourcesKernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_ENCODING,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    register_parameter,
    create_encoding,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)

__all__ = ["FreezeThawKernelFunction", "FreezeThawMeanFunction"]


class FreezeThawKernelFunction(KernelFunction):
    """
    Variant of the kernel function for modeling exponentially decaying
    learning curves, proposed in:

        Swersky, K., Snoek, J., & Adams, R. P. (2014).
        Freeze-Thaw Bayesian Optimization.
        ArXiv:1406.3896 [Cs, Stat).
        Retrieved from http://arxiv.org/abs/1406.3896

    The argument in that paper actually justifies using a non-zero mean
    function (see :class:`ExponentialDecayResourcesMeanFunction`) and
    centralizing the kernel proposed there. This is done here.

    As in the Freeze-Thaw paper, learning curves for different configs are
    conditionally independent.

    This class is configured with a kernel and a mean function over
    inputs x (dimension d) and represents a kernel (and mean function) over
    inputs (x, r) (dimension d + 1), where the resource attribute r >= 0 is
    last.

    Note: This kernel is mostly for debugging! Its conditional independence
    assumptions allow for faster inference, as implemented in
    :class:`GaussProcExpDecayPosteriorState`.
    """

    def __init__(
        self,
        kernel_x: KernelFunction,
        mean_x: MeanFunction,
        encoding_type=DEFAULT_ENCODING,
        alpha_init=1.0,
        mean_lam_init=0.5,
        gamma_init=0.5,
        max_metric_value=1.0,
        **kwargs
    ):
        """
        :param kernel_x: Kernel k_x(x, x') over configs
        :param mean_x: Mean function mu_x(x) over configs
        :param encoding_type: Encoding used for alpha, mean_lam, gamma (positive
            values)
        :param alpha_init: Initial value alpha
        :param mean_lam_init: Initial value mean_lam
        :param gamma_init: Initial value gamma
        :param max_metric_value: Maximum value which metric can attend. This is
            used as upper bound on gamma
        """
        super().__init__(dimension=kernel_x.dimension + 1, **kwargs)
        self.kernel_x = kernel_x
        self.mean_x = mean_x
        alpha_lower, alpha_upper = 1e-6, 250.0
        alpha_init = ExponentialDecayResourcesKernelFunction._wrap_initvals(
            alpha_init, alpha_lower, alpha_upper
        )
        self.encoding_alpha = create_encoding(
            encoding_type, alpha_init, alpha_lower, alpha_upper, 1, None
        )
        mean_lam_lower, mean_lam_upper = 1e-4, 50.0
        mean_lam_init = ExponentialDecayResourcesKernelFunction._wrap_initvals(
            mean_lam_init, mean_lam_lower, mean_lam_upper
        )
        self.encoding_mean_lam = create_encoding(
            encoding_type, mean_lam_init, mean_lam_lower, mean_lam_upper, 1, None
        )
        gamma_lower = max_metric_value * 0.0001
        gamma_upper = max_metric_value
        gamma_init = ExponentialDecayResourcesKernelFunction._wrap_initvals(
            gamma_init, gamma_lower, gamma_upper
        )
        self.encoding_gamma = create_encoding(
            encoding_type, gamma_init, gamma_lower, gamma_upper, 1, None
        )

        with self.name_scope():
            self.alpha_internal = register_parameter(
                self.params, "alpha", self.encoding_alpha
            )
            self.mean_lam_internal = register_parameter(
                self.params, "mean_lam", self.encoding_mean_lam
            )
            self.gamma_internal = register_parameter(
                self.params, "gamma", self.encoding_gamma
            )

    def _compute_terms(self, X, alpha, mean_lam, ret_mean=False):
        dim = self.kernel_x.dimension
        cfg = X[:, :dim]
        res = X[:, dim:]
        kappa = ExponentialDecayResourcesKernelFunction._compute_kappa(
            res, alpha, mean_lam
        )
        if ret_mean:
            mean = self.mean_x(cfg)
        else:
            mean = None

        return cfg, res, kappa, mean

    def _get_params(self, X, **kwargs):
        alpha = ExponentialDecayResourcesKernelFunction._unwrap(
            X, kwargs, "alpha", self.encoding_alpha, self.alpha_internal
        )
        mean_lam = ExponentialDecayResourcesKernelFunction._unwrap(
            X, kwargs, "mean_lam", self.encoding_mean_lam, self.mean_lam_internal
        )
        gamma = ExponentialDecayResourcesKernelFunction._unwrap(
            X, kwargs, "gamma", self.encoding_gamma, self.gamma_internal
        )

        return (alpha, mean_lam, gamma)

    @staticmethod
    def _to_tuples(cfg):
        return [
            tuple(anp.ravel(x)) for x in anp.split(cfg, getval(cfg.shape[0]), axis=0)
        ]

    def forward(self, X1, X2, **kwargs):
        alpha, mean_lam, gamma = self._get_params(X1, **kwargs)
        gamma = anp.reshape(gamma, (1, 1))
        cfg1, res1, kappa1, _ = self._compute_terms(X1, alpha, mean_lam)
        cfg1_tpls = self._to_tuples(cfg1)
        if X2 is not X1:
            cfg2, res2, kappa2, _ = self._compute_terms(X2, alpha, mean_lam)
            cfg2_tpls = self._to_tuples(cfg2)
            cfg_set = set(cfg1_tpls + cfg2_tpls)
        else:
            cfg2, res2, kappa2, cfg2_tpls = cfg1, res1, kappa1, cfg1_tpls
            cfg_set = set(cfg1_tpls)
        cfg_map = dict(zip(cfg_set, range(len(cfg_set))))
        cfg1_ind = anp.reshape(anp.array([cfg_map[x] for x in cfg1_tpls]), (-1, 1))
        if X2 is not X1:
            cfg2_ind = anp.reshape(anp.array([cfg_map[x] for x in cfg2_tpls]), (1, -1))
        else:
            cfg2_ind = anp.reshape(cfg1_ind, (1, -1))

        res2 = anp.reshape(res2, (1, -1))
        kappa2 = anp.reshape(kappa2, (1, -1))
        kappa12 = ExponentialDecayResourcesKernelFunction._compute_kappa(
            anp.add(res1, res2), alpha, mean_lam
        )
        kmat_res = anp.subtract(kappa12, anp.multiply(kappa1, kappa2))
        kmat_res = kmat_res * anp.square(gamma)
        kmat_res = kmat_res * (cfg1_ind == cfg2_ind)

        kmat_x = self.kernel_x(cfg1, cfg2)
        return kmat_x + kmat_res

    def diagonal(self, X):
        alpha, mean_lam, gamma = self._get_params(X)
        gamma = anp.reshape(gamma, (1, 1))
        cfg, res, kappa, _ = self._compute_terms(X, alpha, mean_lam)
        kappa2 = ExponentialDecayResourcesKernelFunction._compute_kappa(
            res * 2, alpha, mean_lam
        )
        kdiag_res = anp.subtract(kappa2, anp.square(kappa))
        kdiag_res = anp.reshape(kdiag_res * anp.square(gamma), (-1,))

        kdiag_x = self.kernel_x.diagonal(cfg)
        return kdiag_x + kdiag_res

    def diagonal_depends_on_X(self):
        return True

    def param_encoding_pairs(self):
        enc_list = [
            (self.alpha_internal, self.encoding_alpha),
            (self.mean_lam_internal, self.encoding_mean_lam),
            (self.gamma_internal, self.encoding_gamma),
        ]
        enc_list.extend(self.kernel_x.param_encoding_pairs())
        enc_list.extend(self.mean_x.param_encoding_pairs())
        return enc_list

    def mean_function(self, X):
        alpha, mean_lam, gamma = self._get_params(X)
        gamma = anp.reshape(gamma, (1, 1))
        cfg, res, kappa, mean = self._compute_terms(X, alpha, mean_lam, ret_mean=True)
        return anp.add(mean, anp.multiply(kappa, gamma))

    def get_params(self):
        """
        Parameter keys are alpha, mean_lam, gamma, delta (only if not fixed
        to delta_fixed_value), as well as those of self.kernel_x (prefix
        'kernelx_') and of self.mean_x (prefix 'meanx_').
        """
        values = list(self._get_params(None))
        keys = ["alpha", "mean_lam", "gamma"]
        result = {k: anp.reshape(v, (1,))[0] for k, v in zip(keys, values)}
        for pref, func in [("kernelx_", self.kernel_x), ("meanx_", self.mean_x)]:
            result.update({(pref + k): v for k, v in func.get_params().items()})
        return result

    def set_params(self, param_dict):
        for pref, func in [("kernelx_", self.kernel_x), ("meanx_", self.mean_x)]:
            len_pref = len(pref)
            stripped_dict = {
                k[len_pref:]: v for k, v in param_dict.items() if k.startswith(pref)
            }
            func.set_params(stripped_dict)
        self.encoding_alpha.set(self.alpha_internal, param_dict["alpha"])
        self.encoding_mean_lam.set(self.mean_lam_internal, param_dict["mean_lam"])
        self.encoding_gamma.set(self.gamma_internal, param_dict["gamma"])


class FreezeThawMeanFunction(MeanFunction):
    def __init__(self, kernel: FreezeThawKernelFunction, **kwargs):
        super().__init__(**kwargs)
        assert isinstance(kernel, FreezeThawKernelFunction)
        self.kernel = kernel

    def forward(self, X):
        return self.kernel.mean_function(X)

    def param_encoding_pairs(self):
        return []

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/product_kernel.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    KernelFunction,
)

__all__ = ["ProductKernelFunction"]


class ProductKernelFunction(KernelFunction):
    """
    Given two kernel functions K1, K2, this class represents the product kernel
    function given by

        ((x1, x2), (y1, y2)) -> K(x1, y1) * K(x2, y2)

    We assume that parameters of K1 and K2 are disjoint.
    """

    def __init__(
        self,
        kernel1: KernelFunction,
        kernel2: KernelFunction,
        name_prefixes=None,
        **kwargs
    ):
        """
        :param kernel1: Kernel function K1
        :param kernel2: Kernel function K2
        :param name_prefixes: Name prefixes for K1, K2 used in get_params
        """
        super(ProductKernelFunction, self).__init__(
            kernel1.dimension + kernel2.dimension, **kwargs
        )
        self.kernel1 = kernel1
        self.kernel2 = kernel2
        if name_prefixes is None:
            self.name_prefixes = ["kernel1", "kernel2"]
        else:
            assert len(name_prefixes) == 2
            self.name_prefixes = name_prefixes

    def forward(self, X1, X2):
        d1 = self.kernel1.dimension
        X1_1 = X1[:, :d1]
        X1_2 = X1[:, d1:]
        X2_1 = X2[:, :d1]
        X2_2 = X2[:, d1:]
        kmat1 = self.kernel1(X1_1, X2_1)
        kmat2 = self.kernel2(X1_2, X2_2)
        return kmat1 * kmat2

    def diagonal(self, X):
        d1 = self.kernel1.dimension
        X1 = X[:, :d1]
        X2 = X[:, d1:]
        diag1 = self.kernel1.diagonal(X1)
        diag2 = self.kernel2.diagonal(X2)
        return diag1 * diag2

    def diagonal_depends_on_X(self):
        return (
            self.kernel1.diagonal_depends_on_X() or self.kernel2.diagonal_depends_on_X()
        )

    def param_encoding_pairs(self):
        """
        Note: We assume that K1 and K2 have disjoint parameters, otherwise
        there will be a redundancy here.
        """
        return self.kernel1.param_encoding_pairs() + self.kernel2.param_encoding_pairs()

    def get_params(self):
        result = dict()
        prefs = [k + "_" for k in self.name_prefixes]
        for pref, kernel in zip(prefs, [self.kernel1, self.kernel2]):
            result.update({(pref + k): v for k, v in kernel.get_params().items()})

        return result

    def set_params(self, param_dict):
        prefs = [k + "_" for k in self.name_prefixes]
        for pref, kernel in zip(prefs, [self.kernel1, self.kernel2]):
            len_pref = len(pref)
            stripped_dict = {
                k[len_pref:]: v for k, v in param_dict.items() if k.startswith(pref)
            }
            kernel.set_params(stripped_dict)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/kernel/range_kernel.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    KernelFunction,
)

__all__ = ["RangeKernelFunction"]


class RangeKernelFunction(KernelFunction):
    """
    Given kernel function K and range R, this class represents

        (x, y) -> K(x[R], y[R])

    """

    def __init__(self, dimension: int, kernel: KernelFunction, start: int, **kwargs):
        """
        :param dimension: Input dimension
        :param kernel: Kernel function K
        :param start: Range is `range(start, start + kernel.dimension)`

        """
        super().__init__(dimension, **kwargs)
        assert start >= 0 and start + kernel.dimension <= dimension, (
            start,
            dimension,
            kernel.dimension,
        )
        self.kernel = kernel
        self.start = start

    def forward(self, X1, X2):
        a = self.start
        b = a + self.kernel.dimension
        X1_part = X1[:, a:b]
        if X2 is X1:
            X2_part = X1_part
        else:
            X2_part = X2[:, a:b]
        return self.kernel(X1_part, X2_part)

    def diagonal(self, X):
        a = self.start
        b = a + self.kernel.dimension
        return self.kernel.diagonal(X[:, a:b])

    def diagonal_depends_on_X(self):
        return self.kernel.diagonal_depends_on_X()

    def param_encoding_pairs(self):
        """
        Note: We assume that K1 and K2 have disjoint parameters, otherwise
        there will be a redundancy here.
        """
        return self.kernel.param_encoding_pairs()

    def get_params(self):
        return self.kernel.get_params()

    def set_params(self, param_dict):
        self.kernel.set_params(param_dict)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/learncurve/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/learncurve/freeze_thaw.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Dict

import numpy as np
import autograd.numpy as anp
from autograd.scipy.linalg import solve_triangular
from numpy.random import RandomState

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.exponential_decay import (
    ExponentialDecayResourcesKernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.custom_op import (
    cholesky_factorization,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.issm import (
    _flatvec,
    _colvec,
    _rowvec,
    _squared_norm,
    _inner_product,
    predict_posterior_marginals,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_impl import (
    EPS,
)


class ZeroKernel(KernelFunction):
    """
    Constant zero kernel. This works only in the context used here, we do
    return matrices or vectors, but zero scalars.

    """

    def __init__(self, dimension: int, **kwargs):
        super().__init__(dimension, **kwargs)

    def forward(self, X1, X2, **kwargs):
        return 0.0

    def diagonal(self, X):
        return 0.0

    def diagonal_depends_on_X(self):
        return False

    def param_encoding_pairs(self):
        return []

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass


class ZeroMean(MeanFunction):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def forward(self, X):
        return 0.0

    def param_encoding_pairs(self):
        return []

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass


class ExponentialDecayBaseKernelFunction(KernelFunction):
    """
    Implements exponential decay kernel k_r(r, r') from the Freeze-Thaw
    paper, corresponding to :class:`ExponentialDecayResourcesKernelFunction`
    with delta=0 and no x attributes.

    Note: Inputs r lie in [r_min, r_max]. Optionally, they are normalized to
    [0, 1].

    """

    def __init__(
        self, r_max: int, r_min: int = 1, normalize_inputs: bool = False, **kwargs
    ):
        super().__init__(dimension=1, **kwargs)
        self.kernel = ExponentialDecayResourcesKernelFunction(
            kernel_x=ZeroKernel(0), mean_x=ZeroMean(), delta_fixed_value=0.0
        )
        assert r_max > r_min
        self.r_min = r_min
        self.r_max = r_max
        self.lower = r_min - 0.5 + EPS
        self.width = r_max - r_min + 1 - 2 * EPS
        self.normalize_inputs = normalize_inputs

    def _normalize(self, X):
        return (X - self.lower) / self.width

    def forward(self, X1, X2):
        same_12 = X2 is X1
        if self.normalize_inputs:
            X1 = self._normalize(X1)
            if same_12:
                X2 = X1
            else:
                X2 = self._normalize(X2)
        return self.kernel(X1, X2)

    def diagonal(self, X):
        if self.normalize_inputs:
            X = self._normalize(X)
        return self.kernel.diagonal(X)

    def diagonal_depends_on_X(self):
        return self.kernel.diagonal_depends_on_X()

    def param_encoding_pairs(self):
        return self.kernel.param_encoding_pairs()

    def get_params(self):
        return self.kernel.get_params()

    def set_params(self, param_dict):
        self.kernel.set_params(param_dict)

    def mean_function(self, X):
        if self.normalize_inputs:
            X = self._normalize(X)
        return self.kernel.mean_function(X)


def logdet_cholfact_cov_resource(likelihood: Dict) -> float:
    """
    Computes the additional log(det(Lbar)) term. This is
    sum_i log(det(Lbar_i)), where Lbar_i is upper left submatrix of
    `likelihood['lfact_all']`, with size `likelihood['ydims'][i]`.

    :param likelihood: Result of `resource_kernel_likelihood_computations`
    :return: log(det(Lbar))
    """
    lfact_all = likelihood["lfact_all"]
    ydims = likelihood["ydims"]
    dim = max(ydims)
    log_diag = anp.log(anp.diag(lfact_all))
    # Weights:
    #   w_j = sum_i I[ydims[i] > j], j = 0, 1, ...
    weights = anp.sum(_colvec(anp.array(ydims)) > _rowvec(anp.arange(dim)), axis=0)
    return _inner_product(log_diag[:dim], weights)


def resource_kernel_likelihood_precomputations(targets: List[np.ndarray]) -> Dict:
    """
    Precomputations required by `resource_kernel_likelihood_computations`.

    Importantly, `prepare_data` orders datapoints by nonincreasing number of
    targets `ydims[i]`. For `0 <= j < ydim_max`, `ydim_max = ydims[0] =
    max(ydims)`, `num_configs[j]` is the number of datapoints i for which
    `ydims[i] > j`.
    `yflat` is a flat matrix (rows corresponding to fantasy samples; column
    vector if no fantasizing) consisting of `ydim_max` parts, where part j is of
    size `num_configs[j]` and contains `y[j]` for targets of those i counted in
    `num_configs[j]`.

    :param targets: Targets from data representation returned by
        `prepare_data`
    :return: See above
    """
    ydims = [y.shape[0] for y in targets]
    ydim_max = ydims[0]
    num_configs = list(
        np.sum(
            np.array(ydims).reshape((-1, 1)) > np.arange(ydim_max).reshape((1, -1)),
            axis=0,
        ).reshape((-1,))
    )
    assert num_configs[0] == len(targets), (num_configs, len(targets))
    assert num_configs[-1] > 0, num_configs
    total_size = sum(num_configs)
    assert total_size == sum(ydims)
    # Attention: When comparing this to `issm.issm_likelihood_precomputations`,
    # `targets` maps to `yflat` in the same ordering (the index is still r),
    # whereas there the index j of `deltay` runs in the opposite direction of
    # the index r of `targets`
    yflat_rows = []
    for pos, num in enumerate(num_configs):
        yflat_rows.extend([y[pos].reshape((1, -1)) for y in targets[:num]])
    yflat = np.vstack(yflat_rows)
    assert yflat.shape[0] == total_size
    return {"ydims": ydims, "num_configs": num_configs, "yflat": yflat}


# TODO: This code is complex. If it does not run faster than
# `resource_kernel_likelihood_slow_computations`, remove it.
def resource_kernel_likelihood_computations(
    precomputed: Dict,
    res_kernel: ExponentialDecayBaseKernelFunction,
    noise_variance,
    skip_c_d: bool = False,
) -> Dict:
    """
    Given `precomputed` from `resource_kernel_likelihood_precomputations` and
    resource kernel function `res_kernel`, compute quantities required for
    inference and marginal likelihood computation, pertaining to the likelihood
    of a additive model, as in the Freeze-Thaw paper.

    Note that `res_kernel` takes raw (unnormalized) r as inputs. The code here
    works for any resource kernel and mean function, not just for
    :class:`ExponentialDecayBaseKernelFunction`.

    Results returned are:
    - c: n vector [c_i]
    - d: n vector [d_i], positive
    - vtv: n vector [|v_i|^2]
    - wtv: (n, F) matrix[(W_i)^T v_i], F number of fantasy samples
    - wtw: n vector [|w_i|^2] (only if no fantasizing)
    - lfact_all: Cholesky factor for kernel matrix
    - ydims: Target vector sizes (copy from `precomputed`)

    :param precomputed: Output of `resource_kernel_likelihood_precomputations`
    :param res_kernel: Kernel k(r, r') over resources
    :param noise_variance: Noise variance sigma^2
    :param skip_c_d: If True, c and d are not computed
    :return: Quantities required for inference and learning criterion

    """
    num_configs = precomputed["num_configs"]
    num_all_configs = num_configs[0]
    r_min, r_max = res_kernel.r_min, res_kernel.r_max
    num_res = r_max + 1 - r_min
    assert num_all_configs > 0, "targets must not be empty"
    assert num_res > 0, f"r_min = {r_min} must be <= r_max = {r_max}"
    num_fantasy_samples = precomputed["yflat"].shape[1]
    compute_wtw = num_fantasy_samples == 1

    # Compute Cholesky factor for largest target vector size, or for full size
    ydims = precomputed["ydims"]
    rvals = _colvec(anp.arange(r_min, r_min + num_res))
    means_all = _flatvec(res_kernel.mean_function(rvals))
    amat = res_kernel(rvals, rvals) / noise_variance + anp.diag(anp.ones(num_res))
    # TODO: Do we need AddJitterOp here?
    lfact_all = cholesky_factorization(amat)  # L (Cholesky factor)

    # Loop over ydim
    yflat = precomputed["yflat"]
    off = num_all_configs
    ilscal = 1.0 / lfact_all[0, 0]
    vvec = anp.array([ilscal]).reshape((1, 1))
    # `yflat` is a (*, F) matrix, where F == `num_fantasy_samples`. These
    # matrices are flattened out as rows of `wmat`, and reshaped back before
    # writing into `wtv_lst`
    wmat = _rowvec(yflat[:off, :] - means_all[0]) * ilscal
    # Note: We need the detour via `wtv_lst`, etc, because `autograd` does not
    # support overwriting the content of an `ndarray`. Their role is to collect
    # parts of the final vectors, in reverse ordering
    wtv_lst = []
    wtw_lst = []
    num_prev = off
    for ydim, num in enumerate(num_configs[1:], start=1):
        if num < num_prev:
            # These parts are done:
            pos = num * num_fantasy_samples
            wdone = wmat[:, pos:]
            wtv_part = anp.reshape(anp.matmul(vvec, wdone), (-1, num_fantasy_samples))
            wtv_lst.append(wtv_part)
            if compute_wtw:
                wtw_lst.append(_flatvec(anp.sum(anp.square(wdone), axis=0)))
            wmat = wmat[:, :pos]
            num_prev = num
        # Update W matrix
        rhs = _rowvec(yflat[off : (off + num), :] - means_all[ydim])
        off += num
        lvec = _rowvec(lfact_all[ydim, :ydim])
        ilscal = 1.0 / lfact_all[ydim, ydim]
        w_new = (rhs - anp.matmul(lvec, wmat)) * ilscal
        wmat = anp.concatenate((wmat, w_new), axis=0)
        # Update v vector (row vector)
        v_new = anp.array([(1.0 - _inner_product(lvec, vvec)) * ilscal]).reshape((1, 1))
        vvec = anp.concatenate((vvec, v_new), axis=1)
    wtv_part = anp.reshape(anp.matmul(vvec, wmat), (-1, num_fantasy_samples))
    wtv_lst.append(wtv_part)
    wtv_all = anp.concatenate(tuple(reversed(wtv_lst)), axis=0)
    if compute_wtw:
        wtw_lst.append(_flatvec(anp.sum(anp.square(wmat), axis=0)))
        wtw_all = anp.concatenate(tuple(reversed(wtw_lst)), axis=0)
    vtv_for_ydim = anp.cumsum(anp.square(vvec))
    vtv_all = anp.array([vtv_for_ydim[ydim - 1] for ydim in ydims])
    # Compile results
    result = {
        "num_data": sum(ydims),
        "vtv": vtv_all,
        "wtv": wtv_all,
        "lfact_all": lfact_all,
        "means_all": means_all,
        "ydims": ydims,
    }
    if compute_wtw:
        result["wtw"] = wtw_all
    if not skip_c_d:
        result["c"] = anp.zeros(num_all_configs)
        result["d"] = anp.zeros(num_all_configs)
    return result


# TODO: It is not clear whether this code is slower, and it is certainly
# simpler.
def resource_kernel_likelihood_slow_computations(
    targets: List[np.ndarray],
    res_kernel: ExponentialDecayBaseKernelFunction,
    noise_variance,
    skip_c_d: bool = False,
) -> Dict:
    """
    Naive implementation of `resource_kernel_likelihood_computations`, which
    does not require precomputations, but is somewhat slower. Here, results are
    computed one datapoint at a time, instead of en bulk.

    This code is used in unit testing only.
    """
    num_configs = len(targets)
    r_min, r_max = res_kernel.r_min, res_kernel.r_max
    num_res = r_max + 1 - r_min
    assert num_configs > 0, "targets must not be empty"
    compute_wtw = targets[0].shape[1] == 1
    # Compute Cholesky factor for largest target vector size
    ydims = [y.shape[0] for y in targets]
    rvals = _colvec(anp.arange(r_min, r_min + num_res))
    means_all = _flatvec(res_kernel.mean_function(rvals))
    amat = res_kernel(rvals, rvals) / noise_variance + anp.diag(anp.ones(num_res))
    # TODO: Do we need AddJitterOp here?
    lfact_all = cholesky_factorization(amat)  # L (Cholesky factor)
    # Outer loop over configurations
    vtv_lst = []
    wtv_lst = []
    wtw_lst = []
    num_data = 0
    for i, (ymat, ydim) in enumerate(zip(targets, ydims)):
        assert 0 < ydim <= num_res, f"len(y[{i}]) = {ydim}, num_res = {num_res}"
        num_data += ydim
        lfact = lfact_all[:ydim, :ydim]
        rhs = anp.ones((ydim, 1))
        vvec = _flatvec(solve_triangular(lfact, rhs, lower=True))
        means = means_all[:ydim]
        rhs = ymat - _colvec(means)
        wmat = solve_triangular(lfact, rhs, lower=True)
        vtv_lst.append(_squared_norm(vvec))
        wtv_lst.append(anp.matmul(_rowvec(vvec), wmat))
        if compute_wtw:
            wtw_lst.append(_squared_norm(wmat))
    # Compile results
    result = {
        "num_data": num_data,
        "vtv": anp.array(vtv_lst),
        "wtv": anp.vstack(wtv_lst),
        "lfact_all": lfact_all,
        "means_all": means_all,
        "ydims": ydims,
    }
    if compute_wtw:
        result["wtw"] = anp.array(wtw_lst)
    if not skip_c_d:
        result["c"] = anp.zeros(num_configs)
        result["d"] = anp.zeros(num_configs)
    return result


def predict_posterior_marginals_extended(
    poster_state: Dict,
    mean,
    kernel,
    test_features,
    resources: List[int],
    res_kernel: ExponentialDecayBaseKernelFunction,
):
    """
    These are posterior marginals on f_r = h + g_r variables, where
    (x, r) are zipped from `test_features`, `resources`.
    `posterior_means` is a (n, F) matrix, where F is the number of fantasy
    samples, or F == 1 without fantasizing.

    :param poster_state: Posterior state
    :param mean: Mean function
    :param kernel: Kernel function
    :param test_features: Feature matrix for test points (not extended)
    :param resources: Resource values corresponding to rows of
        `test_features`
    :param res_kernel: Kernel k(r, r') over resources
    :return: posterior_means, posterior_variances

    """
    num_test = test_features.shape[0]
    assert len(resources) == num_test, (
        f"test_features.shape[0] = {num_test} != {len(resources)} " + "= len(resources)"
    )
    # Predictive marginals over h
    h_means, h_variances = predict_posterior_marginals(
        poster_state, mean, kernel, test_features
    )
    # Convert into predictive marginals over f_r
    rvals = _colvec(anp.array(resources))
    g_means = _colvec(res_kernel.mean_function(rvals))
    g_variances = _flatvec(res_kernel.diagonal(rvals))
    posterior_means = h_means + g_means
    posterior_variances = h_variances + g_variances
    return posterior_means, posterior_variances


def sample_posterior_joint(
    poster_state: Dict,
    mean,
    kernel,
    feature,
    targets: np.ndarray,
    res_kernel: ExponentialDecayBaseKernelFunction,
    noise_variance,
    lfact_all,
    means_all,
    random_state: RandomState,
    num_samples: int = 1,
) -> Dict:
    """
    Given `poster_state` for some data plus one additional configuration
    with data (`feature`, `targets`), draw joint samples of unobserved
    targets for this configuration. `targets` may be empty, but must not
    be complete (there must be some unobserved targets). The additional
    configuration must not be in the dataset used to compute `poster_state`.

    If `targets` correspond to resource values range(r_min, r_obs), we
    sample latent target values y_r corresponding to range(r_obs, r_max+1),
    returning a dict with [y_r] under `y` (matrix with `num_samples`
    columns).

    :param poster_state: Posterior state for data
    :param mean: Mean function
    :param kernel: Kernel function
    :param feature: Features for additional config
    :param targets: Target values for additional config
    :param res_kernel: Kernel k(r, r') over resources
    :param noise_variance: Noise variance sigma^2
    :param lfact_all: Cholesky factor of complete resource kernel matrix
    :param means_all: See `lfact_all`
    :param random_state: numpy.random.RandomState
    :param num_samples: Number of joint samples to draw (default: 1)
    :return: See above
    """
    r_min, r_max = res_kernel.r_min, res_kernel.r_max
    num_res = r_max + 1 - r_min
    targets = _colvec(targets, _np=np)
    ydim = targets.size
    assert ydim < num_res, f"len(targets) = {ydim} must be < {num_res}"
    assert lfact_all.shape == (
        num_res,
        num_res,
    ), f"lfact_all.shape = {lfact_all.shape}, must be {(num_res, num_res)}"
    assert (
        means_all.size == num_res
    ), f"means_all.size = {means_all.size}, must be {num_res}"

    # Posterior mean and variance of h for additional config
    post_mean, post_variance = predict_posterior_marginals(
        poster_state, mean, kernel, _rowvec(feature)
    )
    post_mean = post_mean[0]
    post_variance = post_variance[0]
    # Draw samples from joint distribution
    epsmat = random_state.normal(size=(num_res, num_samples))
    joint_samples = anp.matmul(lfact_all, epsmat) * anp.sqrt(
        noise_variance
    ) + anp.reshape(means_all, (-1, 1))
    hvec = (
        random_state.normal(size=(1, num_samples)) * anp.sqrt(post_variance) + post_mean
    )
    joint_samples = joint_samples + hvec
    if ydim > 0:
        # There are observed targets, so have to transform the joint sample
        # into a conditional one
        targets_samp = joint_samples[:ydim, :]
        lfact = lfact_all[:ydim, :ydim]  # L_Q
        rhs = anp.ones((ydim, 1))
        vvec = solve_triangular(lfact, rhs, lower=True)  # v
        vtv = _squared_norm(vvec)  # alpha
        # w_hat - w
        w_delta = solve_triangular(lfact, targets - targets_samp, lower=True)
        kappa = post_variance / noise_variance
        fact = kappa / (vtv * kappa + 1.0)
        # rho_hat - rho
        rho_delta = anp.matmul(_rowvec(vvec), w_delta) * fact
        tmpmat = w_delta - vvec * rho_delta
        lfact_pq = lfact_all[ydim:, :ydim]  # L_{P, Q}
        ysamples = joint_samples[ydim:, :] + anp.matmul(lfact_pq, tmpmat) + rho_delta
    else:
        # Nothing to condition on
        ysamples = joint_samples
    return {"y": ysamples}

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/learncurve/gpiss_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.likelihood import (
    GaussAdditiveMarginalLikelihood,
    LCModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    OptimizationConfig,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_model import (
    GaussianProcessOptimizeModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    MarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
    MeanFunction,
)

logger = logging.getLogger(__name__)


class GaussianProcessLearningCurveModel(GaussianProcessOptimizeModel):
    """
    Represents joint Gaussian model of learning curves over a number of
    configurations. The model has an additive form:

        f(x, r) = g(r | x) + h(x),

    where h(x) is a Gaussian process model for function values at r_max, and
    the g(r | x) are independent Gaussian models. Right now, g(r | x) can be:

    - Innovation state space model (ISSM) of a particular power-law decay
        form. For this one, g(r_max | x) = 0 for all x. Used if
        `res_model` is of type :class:`ISSModelParameters`
    - Gaussian process model with exponential decay covariance function. This
        is essentially the model from the Freeze Thaw paper, see also
        :class:`ExponentialDecayResourcesKernelFunction`. Used if
        `res_model` is of type :class:`ExponentialDecayBaseKernelFunction`

    Importantly, inference scales cubically only in the number of
    configurations, not in the number of observations.

    Details about ISSMs in general are found in

        Hyndman, R. and Koehler, A. and Ord, J. and Snyder, R.
        Forecasting with Exponential Smoothing: The State Space Approach
        Springer, 2008

    :param kernel: Kernel function k(X, X')
    :param res_model: Model for g(r | x)
    :param mean: Mean function mu(X)
    :param initial_noise_variance: A scalar to initialize the value of the
        residual noise variance
    :param optimization_config: Configuration that specifies the behavior of
        the optimization of the marginal likelihood.
    :param random_seed: Random seed to be used (optional)
    :param fit_reset_params: Reset parameters to initial values before running
        'fit'? If False, 'fit' starts from the current values
    """

    def __init__(
        self,
        kernel: KernelFunction,
        res_model: LCModel,
        mean: MeanFunction = None,
        initial_noise_variance: float = None,
        optimization_config: OptimizationConfig = None,
        random_seed=None,
        fit_reset_params: bool = True,
    ):
        super().__init__(
            optimization_config=optimization_config,
            random_seed=random_seed,
            fit_reset_params=fit_reset_params,
        )
        if mean is None:
            mean = ScalarMeanFunction()
        self._likelihood = GaussAdditiveMarginalLikelihood(
            kernel=kernel,
            res_model=res_model,
            mean=mean,
            initial_noise_variance=initial_noise_variance,
        )
        self.reset_params()

    @property
    def likelihood(self) -> MarginalLikelihood:
        return self._likelihood

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/learncurve/issm.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Dict, Optional, Tuple

import numpy as np
import scipy.linalg as spl
import autograd.numpy as anp
from autograd.scipy.special import logsumexp
from autograd.scipy.linalg import solve_triangular
from autograd.tracer import getval
from numpy.random import RandomState
from operator import itemgetter
from collections import Counter


from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    NUMERICAL_JITTER,
    MIN_POSTERIOR_VARIANCE,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.custom_op import (
    cholesky_factorization,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    FantasizedPendingEvaluation,
    Configuration,
    TrialEvaluations,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.config_ext import (
    ExtendedConfiguration,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler


def _prepare_data_internal(
    state: TuningJobState,
    data_lst: List[Tuple[Configuration, List, str]],
    config_space_ext: ExtendedConfiguration,
    active_metric: str,
    do_fantasizing: bool,
    mean: float,
    std: float,
) -> (List[Configuration], List[np.ndarray], List[str]):
    r_min, r_max = config_space_ext.resource_attr_range
    configs = [x[0] for x in data_lst]
    trial_ids = [x[2] for x in data_lst]
    targets = []

    fantasized = dict()
    num_fantasy_samples = None
    if do_fantasizing:
        for ev in state.pending_evaluations:
            assert isinstance(ev, FantasizedPendingEvaluation)
            trial_id = ev.trial_id
            entry = (ev.resource, ev.fantasies[active_metric])
            sz = entry[1].size
            if num_fantasy_samples is None:
                num_fantasy_samples = sz
            else:
                assert sz == num_fantasy_samples, (
                    "Number of fantasy samples must be the same for all "
                    + f"pending evaluations ({sz}, {num_fantasy_samples})"
                )
            if trial_id in fantasized:
                fantasized[trial_id].append(entry)
            else:
                fantasized[trial_id] = [entry]

    trial_ids_done = set()
    for config, observed, trial_id in data_lst:
        # Observations must be from r_min without any missing
        obs_res = [x[0] for x in observed]
        num_obs = len(observed)
        if num_obs > 0:
            test = list(range(r_min, r_min + num_obs))
            assert obs_res == test, (
                f"trial_id {trial_id} has observations at {obs_res}, but "
                + f"we need them at {test}"
            )
        # Note: Only observed targets are normalized, not fantasized ones
        this_targets = (
            np.array([x[1] for x in observed]).reshape((-1, 1)) - mean
        ) / std
        if do_fantasizing:
            if num_fantasy_samples > 1:
                this_targets = this_targets * np.ones((1, num_fantasy_samples))
            if trial_id in fantasized:
                this_fantasized = sorted(fantasized[trial_id], key=itemgetter(0))
                fanta_res = [x[0] for x in this_fantasized]
                start = r_min + num_obs
                test = list(range(start, start + len(this_fantasized)))
                assert fanta_res == test, (
                    f"trial_id {trial_id} has pending evaluations at {fanta_res}"
                    + f", but we need them at {test}"
                )
                this_targets = np.vstack(
                    [this_targets] + [x[1].reshape((1, -1)) for x in this_fantasized]
                )
                trial_ids_done.add(trial_id)
        targets.append(this_targets)

    if do_fantasizing:
        # There may be trials with pending evals, but no observes ones
        for trial_id, this_fantasized in fantasized.items():
            if trial_id not in trial_ids_done:
                configs.append(state.config_for_trial[trial_id])
                trial_ids.append(trial_id)
                this_fantasized = sorted(this_fantasized, key=itemgetter(0))
                fanta_res = [x[0] for x in this_fantasized]
                test = list(range(r_min, r_min + len(this_fantasized)))
                assert fanta_res == test, (
                    f"trial_id {trial_id} has pending evaluations at {fanta_res}"
                    + f", but we need them at {test}"
                )
                this_targets = np.vstack(
                    [x[1].reshape((1, -1)) for x in this_fantasized]
                )
                targets.append(this_targets)

    return configs, targets, trial_ids


def _create_tuple(ev: TrialEvaluations, active_metric: str, config_for_trial: Dict):
    metric_vals = ev.metrics[active_metric]
    assert isinstance(metric_vals, dict)
    observed = list(
        sorted(((int(k), v) for k, v in metric_vals.items()), key=itemgetter(0))
    )
    trial_id = ev.trial_id
    config = config_for_trial[trial_id]
    return config, observed, trial_id


def prepare_data(
    state: TuningJobState,
    config_space_ext: ExtendedConfiguration,
    active_metric: str,
    normalize_targets: bool = False,
    do_fantasizing: bool = False,
) -> Dict:
    """
    Prepares data in `state` for further processing. The entries
    `configs`, `targets` of the result dict are lists of one entry per trial,
    they are sorted in decreasing order of number of target values. `features`
    is the feature matrix corresponding to `configs`. If `normalize_targets`
    is True, the target values are normalized to mean 0, variance 1 (over all
    values), and `mean_targets`, `std_targets` is returned.

    If `do_fantasizing` is True, `state.pending_evaluations` is also taken into
    account. Entries there have to be of type `FantasizedPendingEvaluation`.
    Also, in terms of their resource levels, they need to be adjacent to
    observed entries, so there are no gaps. In this case, the entries of the
    `targets` list are matrices, each column corr´esponding to a fantasy sample.

    Note: If `normalize_targets`, mean and stddev are computed over observed
    values only. Also, fantasy values in `state.pending_evaluations` are not
    normalized, because they are assumed to be sampled from the posterior with
    normalized targets as well.

    :param state: `TuningJobState` with data
    :param config_space_ext: Extended config space
    :param active_metric:
    :param normalize_targets: See above
    :param do_fantasizing: See above
    :return: See above
    """
    r_min, r_max = config_space_ext.resource_attr_range
    hp_ranges = config_space_ext.hp_ranges
    data_lst = []
    targets = []
    for ev in state.trials_evaluations:
        tpl = _create_tuple(ev, active_metric, state.config_for_trial)
        data_lst.append(tpl)
        observed = tpl[1]
        targets += [x[1] for x in observed]
    mean = 0.0
    std = 1.0
    if normalize_targets:
        std = max(np.std(targets), 1e-9)
        mean = np.mean(targets)

    configs, targets, trial_ids = _prepare_data_internal(
        state=state,
        data_lst=data_lst,
        config_space_ext=config_space_ext,
        active_metric=active_metric,
        do_fantasizing=do_fantasizing,
        mean=mean,
        std=std,
    )
    # Sort in decreasing order w.r.t. number of targets
    configs, targets, trial_ids = zip(
        *sorted(zip(configs, targets, trial_ids), key=lambda x: -x[1].shape[0])
    )
    features = hp_ranges.to_ndarray_matrix(configs)
    result = {
        "configs": list(configs),
        "features": features,
        "targets": list(targets),
        "trial_ids": list(trial_ids),
        "r_min": r_min,
        "r_max": r_max,
        "do_fantasizing": do_fantasizing,
    }
    if normalize_targets:
        result["mean_targets"] = mean
        result["std_targets"] = std
    return result


def prepare_data_with_pending(
    state: TuningJobState,
    config_space_ext: ExtendedConfiguration,
    active_metric: str,
    normalize_targets: bool = False,
) -> (Dict, Dict):
    """
    Similar to `prepare_data` with `do_fantasizing=False`, but two dicts are
    returned, the first for trials without pending evaluations, the second
    for trials with pending evaluations. The latter dict also contains trials
    which have pending, but no observed evaluations.
    The second dict has the additional entry `num_pending`, which lists the
    number of pending evals for each trial. These evals must be contiguous and
    adjacent with observed evals, so that the union of observed and pending
    evals are contiguous (when it comes to resource levels).

    :param state: See `prepare_data`
    :param config_space_ext: See `prepare_data`
    :param active_metric: See `prepare_data`
    :param normalize_targets: See `prepare_data`
    :return: See above

    """
    r_min, r_max = config_space_ext.resource_attr_range
    hp_ranges = config_space_ext.hp_ranges
    data1_lst = []  # trials without pending evals
    data2_lst = []  # trials with pending evals
    num_pending = []
    num_pending_for_trial = Counter(ev.trial_id for ev in state.pending_evaluations)
    targets = []
    done_trial_ids = set()
    for ev in state.trials_evaluations:
        tpl = _create_tuple(ev, active_metric, state.config_for_trial)
        _, observed, trial_id = tpl
        if trial_id not in num_pending_for_trial:
            data1_lst.append(tpl)
        else:
            data2_lst.append(tpl)
            num_pending.append(num_pending_for_trial[trial_id])
        done_trial_ids.add(trial_id)
        targets += [x[1] for x in observed]
    mean = 0.0
    std = 1.0
    if normalize_targets:
        std = max(np.std(targets), 1e-9)
        mean = np.mean(targets)
    # There may be trials with pending evaluations, but no observed ones
    for ev in state.pending_evaluations:
        trial_id = ev.trial_id
        if trial_id not in done_trial_ids:
            config = state.config_for_trial[trial_id]
            data2_lst.append((config, [], trial_id))
            num_pending.append(num_pending_for_trial[trial_id])

    results = ()
    with_pending = False
    for data_lst in (data1_lst, data2_lst):
        configs, targets, trial_ids = _prepare_data_internal(
            state=state,
            data_lst=data_lst,
            config_space_ext=config_space_ext,
            active_metric=active_metric,
            do_fantasizing=False,
            mean=mean,
            std=std,
        )
        if configs:
            # Sort in decreasing order w.r.t. number of targets
            if not with_pending:
                configs, targets, trial_ids = zip(
                    *sorted(
                        zip(configs, targets, trial_ids), key=lambda x: -x[1].shape[0]
                    )
                )
            else:
                configs, targets, num_pending, trial_ids = zip(
                    *sorted(
                        zip(configs, targets, num_pending, trial_ids),
                        key=lambda x: -x[1].shape[0],
                    )
                )
            features = hp_ranges.to_ndarray_matrix(configs)
        else:
            # It is possible that `data1_lst` is empty
            features = None
        result = {
            "configs": list(configs),
            "features": features,
            "targets": list(targets),
            "trial_ids": list(trial_ids),
            "r_min": r_min,
            "r_max": r_max,
            "do_fantasizing": False,
        }
        if with_pending:
            result["num_pending"] = num_pending
        if normalize_targets:
            result["mean_targets"] = mean
            result["std_targets"] = std
        results = results + (result,)
        with_pending = True
    return results


def issm_likelihood_precomputations(targets: List[np.ndarray], r_min: int) -> Dict:
    """
    Precomputations required by `issm_likelihood_computations`.

    Importantly, `prepare_data` orders datapoints by nonincreasing number of
    targets `ydims[i]`. For `0 <= j < ydim_max`, `ydim_max = ydims[0] =
    max(ydims)`, `num_configs[j]` is the number of datapoints i for which
    `ydims[i] > j`.
    `deltay` is a flat matrix (rows corresponding to fantasy samples; column
    vector if no fantasizing) consisting of `ydim_max` parts, where part j is
    of size `num_configs[j]` and contains `y[j] - y[j-1]` for targets of
    those i counted in `num_configs[j]`, the term needed in the recurrence to
    compute `w[j]`.
    'logr` is a flat vector consisting of `ydim_max - 1` parts, where part j
    (starting from 1) is of size `num_configs[j]` and contains the logarithmic
    term for computing `a[j-1]` and `e[j]`.

    :param targets: Targets from data representation returned by
        `prepare_data`
    :param r_min: Value of r_min, as returned by `prepare_data`
    :return: See above
    """
    ydims = [y.shape[0] for y in targets]
    ydim_max = ydims[0]
    num_configs = list(
        np.sum(
            np.array(ydims).reshape((-1, 1)) > np.arange(ydim_max).reshape((1, -1)),
            axis=0,
        ).reshape((-1,))
    )
    assert num_configs[0] == len(targets), (num_configs, len(targets))
    assert num_configs[-1] > 0, num_configs
    total_size = sum(num_configs)
    assert total_size == sum(ydims)
    yprev = np.vstack([y[-1].reshape((1, -1)) for y in targets])
    deltay_parts = [yprev]
    log_r = []
    for pos, num in enumerate(num_configs[1:], start=1):
        ycurr = np.vstack([y[-(pos + 1)].reshape((1, -1)) for y in targets[:num]])
        deltay_parts.append(ycurr - yprev[:num, :])
        yprev = ycurr
        logr_curr = [np.log(ydim + r_min - pos) for ydim in ydims[:num]]
        log_r.extend(logr_curr)
    deltay = np.vstack(deltay_parts)
    assert deltay.shape[0] == total_size
    assert len(log_r) == total_size - num_configs[0]
    return {
        "ydims": ydims,
        "num_configs": num_configs,
        "deltay": deltay,
        "logr": np.array(log_r),
    }


def _squared_norm(a, _np=anp):
    return _np.sum(_np.square(a))


def _inner_product(a, b, _np=anp):
    return _np.sum(_np.multiply(a, b))


def _colvec(a, _np=anp):
    return _np.reshape(a, (-1, 1))


def _rowvec(a, _np=anp):
    return _np.reshape(a, (1, -1))


def _flatvec(a, _np=anp):
    return _np.reshape(a, (-1,))


def issm_likelihood_computations(
    precomputed: Dict,
    issm_params: Dict,
    r_min: int,
    r_max: int,
    skip_c_d: bool = False,
    profiler: Optional[SimpleProfiler] = None,
) -> Dict:
    """
    Given `precomputed` from `issm_likelihood_precomputations` and ISSM
    parameters `issm_params`, compute quantities required for inference and
    marginal likelihood computation, pertaining to the ISSM likelihood.

    The index for r is range(r_min, r_max + 1). Observations must be contiguous
    from r_min. The ISSM parameters are:
    - alpha: n-vector, negative
    - beta: n-vector
    - gamma: scalar, positive

    Results returned are:
    - c: n vector [c_i], negative
    - d: n vector [d_i], positive
    - vtv: n vector [|v_i|^2]
    - wtv: (n, F) matrix [(W_i)^T v_i], F number of fantasy samples
    - wtw: n-vector [|w_i|^2] (only if no fantasizing)

    :param precomputed: Output of `issm_likelihood_precomputations`
    :param issm_params: Parameters of ISSM likelihood
    :param r_min: Smallest resource value
    :param r_max: Largest resource value
    :param skip_c_d: If True, c and d are not computed
    :return: Quantities required for inference and learning criterion

    """
    num_all_configs = precomputed["num_configs"][0]
    num_res = r_max + 1 - r_min
    assert num_all_configs > 0, "targets must not be empty"
    assert num_res > 0, f"r_min = {r_min} must be <= r_max = {r_max}"
    num_fantasy_samples = precomputed["deltay"].shape[1]
    compute_wtw = num_fantasy_samples == 1
    alphas = _flatvec(issm_params["alpha"])
    betas = _flatvec(issm_params["beta"])
    gamma = issm_params["gamma"]
    n = getval(alphas.size)
    assert n == num_all_configs, f"alpha.size = {n} != {num_all_configs}"
    n = getval(betas.size)
    assert n == num_all_configs, f"beta.size = {n} != {num_all_configs}"

    if not skip_c_d:
        # We could probably refactor this to fit into the loop below, but it
        # seems subdominant
        if profiler is not None:
            profiler.start("issm_part1")
        c_lst = []
        d_lst = []
        for i, ydim in enumerate(precomputed["ydims"]):
            alpha = alphas[i]
            alpha_m1 = alpha - 1.0
            beta = betas[i]
            r_obs = r_min + ydim  # Observed in range(r_min, r_obs)
            assert 0 < ydim <= num_res, f"len(y[{i}]) = {ydim}, num_res = {num_res}"
            # c_i, d_i
            if ydim < num_res:
                lrvec = (
                    anp.array([np.log(r) for r in range(r_obs, r_max + 1)]) * alpha_m1
                    + beta
                )
                c_scal = alpha * anp.exp(logsumexp(lrvec))
                d_scal = anp.square(gamma * alpha) * anp.exp(logsumexp(lrvec * 2.0))
                c_lst.append(c_scal)
                d_lst.append(d_scal)
            else:
                c_lst.append(0.0)
                d_lst.append(0.0)
        if profiler is not None:
            profiler.stop("issm_part1")

    # Loop over ydim
    if profiler is not None:
        profiler.start("issm_part2")
    deltay = precomputed["deltay"]
    logr = precomputed["logr"]
    off_dely = num_all_configs
    vvec = anp.ones(off_dely)
    wmat = deltay[:off_dely, :]  # [y_0]
    vtv = anp.ones(off_dely)
    wtv = wmat.copy()
    if compute_wtw:
        wtw = _flatvec(anp.square(wmat))
    # Note: We need the detour via `vtv_lst`, etc, because `autograd` does not
    # support overwriting the content of an `ndarray`. Their role is to collect
    # parts of the final vectors, in reverse ordering
    vtv_lst = []
    wtv_lst = []
    wtw_lst = []
    alpham1s = alphas - 1
    num_prev = off_dely
    for num in precomputed["num_configs"][1:]:
        if num < num_prev:
            # Size of working vectors is shrinking
            assert vtv.size == num_prev
            # These parts are done: Collect them in the lists
            # All vectors are resized to `num`, dropping the tails
            vtv_lst.append(vtv[num:])
            wtv_lst.append(wtv[num:, :])
            vtv = vtv[:num]
            wtv = wtv[:num, :]
            if compute_wtw:
                wtw_lst.append(wtw[num:])
                wtw = wtw[:num]
            alphas = alphas[:num]
            alpham1s = alpham1s[:num]
            betas = betas[:num]
            vvec = vvec[:num]
            wmat = wmat[:num, :]
            num_prev = num
        # [a_{j-1}]
        off_logr = off_dely - num_all_configs
        logr_curr = logr[off_logr : (off_logr + num)]
        avec = alphas * anp.exp(logr_curr * alpham1s + betas)
        evec = avec * gamma + 1  # [e_j]
        vvec = vvec * evec  # [v_j]
        deltay_curr = deltay[off_dely : (off_dely + num), :]
        off_dely += num
        wmat = _colvec(evec) * wmat + deltay_curr + _colvec(avec)  # [w_j]
        vtv = vtv + anp.square(vvec)
        if compute_wtw:
            wtw = wtw + _flatvec(anp.square(wmat))
        wtv = wtv + _colvec(vvec) * wmat
    vtv_lst.append(vtv)
    wtv_lst.append(wtv)
    vtv_all = anp.concatenate(tuple(reversed(vtv_lst)), axis=0)
    wtv_all = anp.concatenate(tuple(reversed(wtv_lst)), axis=0)
    if compute_wtw:
        wtw_lst.append(wtw)
        wtw_all = anp.concatenate(tuple(reversed(wtw_lst)), axis=0)
    if profiler is not None:
        profiler.stop("issm_part2")

    # Compile results
    result = {"num_data": sum(precomputed["ydims"]), "vtv": vtv_all, "wtv": wtv_all}
    if compute_wtw:
        result["wtw"] = wtw_all
    if not skip_c_d:
        result["c"] = anp.array(c_lst)
        result["d"] = anp.array(d_lst)
    return result


def posterior_computations(
    features, mean, kernel, issm_likelihood: Dict, noise_variance
) -> Dict:
    """
    Computes posterior state (required for predictions) and negative log
    marginal likelihood (returned in `criterion`), The latter is computed only
    when there is no fantasizing (i.e., if `issm_likelihood` contains `wtw`).

    :param features: Input matrix X
    :param mean: Mean function
    :param kernel: Kernel function
    :param issm_likelihood: Outcome of `issm_likelihood_computations`
    :param noise_variance: Variance of ISSM innovations
    :return: Internal posterior state

    """
    num_data = issm_likelihood["num_data"]
    kernel_mat = kernel(features, features)  # K
    dvec = issm_likelihood["d"]
    s2vec = issm_likelihood["vtv"] / noise_variance  # S^2
    svec = _colvec(anp.sqrt(s2vec + NUMERICAL_JITTER))
    # A = I + S K_hat S = (I + S^2 D) + S K S
    dgvec = s2vec * dvec + 1.0
    amat = anp.multiply(svec, anp.multiply(kernel_mat, _rowvec(svec))) + anp.diag(dgvec)
    # TODO: Do we need AddJitterOp here?
    lfact = cholesky_factorization(amat)  # L (Cholesky factor)
    # r vectors
    muhat = _flatvec(mean(features)) - issm_likelihood["c"]
    s2muhat = s2vec * muhat  # S^2 mu_hat
    r2mat = issm_likelihood["wtv"] / noise_variance - _colvec(s2muhat)
    r3mat = anp.matmul(kernel_mat, r2mat) + _colvec(dvec) * r2mat
    r4mat = solve_triangular(lfact, r3mat * svec, lower=True)
    # Prediction matrix P
    pmat = r2mat - svec * solve_triangular(lfact, r4mat, lower=True, trans="T")
    result = {
        "features": features,
        "chol_fact": lfact,
        "svec": _flatvec(svec),
        "pmat": pmat,
        "likelihood": issm_likelihood,
    }
    if "wtw" in issm_likelihood:
        # Negative log marginal likelihood
        # Part sigma^{-2} |r_1|^2 - (r_2)^T r_3 + |r_4|^2
        r2vec = _flatvec(r2mat)
        r3vec = _flatvec(r3mat)
        r4vec = _flatvec(r4mat)
        # We use: r_1 = w - V mu_hat, sigma^{-2} V^T V = S^2, and
        # r_2 = sigma^{-2} V^T w - S^2 mu_hat, so that:
        # sigma^{-2} |r_1|^2
        # = sigma^{-2} |w|^2 + |S mu_hat|^2 - 2 sigma^{-2} w^T V mu_hat
        # = sigma^{-2} |w|^2 - |S mu_hat|^2 - 2 * (r_2)^T mu_hat
        # = sigma^{-2} |w|^2 - mu_hat^T (S^2 mu_hat + 2 r_2)
        part2 = 0.5 * (
            anp.sum(issm_likelihood["wtw"]) / noise_variance
            - _inner_product(muhat, s2muhat + 2.0 * r2vec)
            - _inner_product(r2vec, r3vec)
            + _squared_norm(r4vec)
        )
        part1 = anp.sum(anp.log(anp.abs(anp.diag(lfact)))) + 0.5 * num_data * anp.log(
            2 * anp.pi * noise_variance
        )
        result["criterion"] = part1 + part2
        result["r2vec"] = r2vec
        result["r4vec"] = r4vec
    return result


def predict_posterior_marginals(poster_state: Dict, mean, kernel, test_features):
    """
    These are posterior marginals on the h variable, whereas the full model is
    for f_r = h + g_r (additive).
    `posterior_means` is a (n, F) matrix, where F is the number of fantasy
    samples, or F == 1 without fantasizing.

    :param poster_state: Posterior state
    :param mean: Mean function
    :param kernel: Kernel function
    :param test_features: Feature matrix for test points (not extended)
    :return: posterior_means, posterior_variances
    """
    k_tr_te = kernel(poster_state["features"], test_features)
    posterior_means = anp.matmul(k_tr_te.T, poster_state["pmat"]) + _colvec(
        mean(test_features)
    )
    qmat = solve_triangular(
        poster_state["chol_fact"],
        anp.multiply(_colvec(poster_state["svec"]), k_tr_te),
        lower=True,
    )
    posterior_variances = kernel.diagonal(test_features) - anp.sum(
        anp.square(qmat), axis=0
    )
    return posterior_means, _flatvec(
        anp.maximum(posterior_variances, MIN_POSTERIOR_VARIANCE)
    )


def sample_posterior_marginals(
    poster_state: Dict,
    mean,
    kernel,
    test_features,
    random_state: RandomState,
    num_samples: int = 1,
):
    """
    We sample from posterior marginals on the h variance, see also
    `predict_posterior_marginals`.
    """
    post_means, post_vars = predict_posterior_marginals(
        poster_state, mean, kernel, test_features
    )
    assert getval(post_means.shape[1]) == 1, (
        "sample_posterior_marginals cannot be used for posterior state "
        + "based on fantasizing"
    )
    n01_mat = random_state.normal(size=(getval(post_means.shape[0]), num_samples))
    post_stds = _colvec(anp.sqrt(post_vars))
    return anp.multiply(post_stds, n01_mat) + _colvec(post_means)


def predict_posterior_marginals_extended(
    poster_state: Dict,
    mean,
    kernel,
    test_features,
    resources: List[int],
    issm_params: Dict,
    r_min: int,
    r_max: int,
):
    """
    These are posterior marginals on f_r = h + g_r variables, where
    (x, r) are zipped from `test_features`, `resources`. `issm_params`
    are likelihood parameters for the test configs.
    `posterior_means` is a (n, F) matrix, where F is the number of fantasy
    samples, or F == 1 without fantasizing.

    :param poster_state: Posterior state
    :param mean: Mean function
    :param kernel: Kernel function
    :param test_features: Feature matrix for test points (not extended)
    :param resources: Resource values corresponding to rows of
        `test_features`
    :param issm_params: See above
    :param r_min:
    :param r_max:
    :return: posterior_means, posterior_variances

    """
    num_test = test_features.shape[0]
    assert len(resources) == num_test, (
        f"test_features.shape[0] = {num_test} != {len(resources)} " + "= len(resources)"
    )
    alphas = anp.reshape(issm_params["alpha"], (-1,))
    betas = anp.reshape(issm_params["beta"], (-1,))
    gamma = issm_params["gamma"]
    n = getval(alphas.size)
    assert n == num_test, (
        f"Entries in issm_params must have size {num_test}, but " + f"have size {n}"
    )
    # Predictive marginals over h
    h_means, h_variances = predict_posterior_marginals(
        poster_state, mean, kernel, test_features
    )
    if all(r == r_max for r in resources):
        # Frequent special case
        posterior_means = h_means
        posterior_variances = h_variances
    else:
        # Convert into predictive marginals over f_r
        posterior_means = []
        posterior_variances = []
        for h_mean, h_variance, resource, alpha, beta in zip(
            h_means, h_variances, resources, alphas, betas
        ):
            sz = r_max - resource
            h_mean = _rowvec(h_mean)
            if sz == 0:
                posterior_means.append(h_mean)
                posterior_variances.append(h_variance)
            else:
                lrvec = (
                    anp.array([np.log(r_max - t) for t in range(sz)]) * (alpha - 1.0)
                    + beta
                )
                avec = alpha * anp.exp(lrvec)
                a2vec = anp.square(alpha * gamma) * anp.exp(lrvec * 2.0)
                c = anp.sum(avec)
                d = anp.sum(a2vec)
                posterior_means.append(h_mean - c)
                posterior_variances.append(h_variance + d)
        posterior_means = anp.vstack(posterior_means)
        posterior_variances = anp.array(posterior_variances)
    return posterior_means, posterior_variances


def sample_posterior_joint(
    poster_state: Dict,
    mean,
    kernel,
    feature,
    targets: np.ndarray,
    issm_params: Dict,
    r_min: int,
    r_max: int,
    random_state: RandomState,
    num_samples: int = 1,
) -> Dict:
    """
    Given `poster_state` for some data plus one additional configuration
    with data (`feature`, `targets`, `issm_params`), draw joint samples
    of the latent variables not fixed by the data, and of the latent
    target values. `targets` may be empty, but must not reach all the
    way to `r_max`. The additional configuration must not be in the
    dataset used to compute `poster_state`.

    If `targets` correspond to resource values range(r_min, r_obs), we
    sample latent target values y_r corresponding to range(r_obs, r_max+1)
    and latent function values f_r corresponding to range(r_obs-1, r_max+1),
    unless r_obs = r_min (i.e. `targets` empty), in which case both [y_r]
    and [f_r] ranges in range(r_min, r_max+1). We return a dict with
    [f_r] under `f`, [y_r] under `y`. These are matrices with `num_samples`
    columns.

    :param poster_state: Posterior state for data
    :param mean: Mean function
    :param kernel: Kernel function
    :param feature: Features for additional config
    :param targets: Target values for additional config
    :param issm_params: Likelihood parameters for additional config
    :param r_min: Smallest resource value
    :param r_max: Largest resource value
    :param random_state: numpy.random.RandomState
    :param num_samples: Number of joint samples to draw (default: 1)
    :return: See above
    """
    num_res = r_max + 1 - r_min
    targets = _colvec(targets, _np=np)
    ydim = targets.size
    t_obs = num_res - ydim
    assert t_obs > 0, f"targets.size = {ydim} must be < {num_res}"
    assert getval(poster_state["pmat"].shape[1]) == 1, (
        "sample_posterior_joint cannot be used for posterior state "
        + "based on fantasizing"
    )
    # ISSM parameters
    alpha = issm_params["alpha"][0]
    alpha_m1 = alpha - 1.0
    beta = issm_params["beta"][0]
    gamma = issm_params["gamma"]
    # Posterior mean and variance of h for additional config
    post_mean, post_variance = predict_posterior_marginals(
        poster_state, mean, kernel, _rowvec(feature, _np=np)
    )
    post_mean = post_mean[0].item()
    post_variance = post_variance[0].item()
    # Compute [a_t], [gamma^2 a_t^2]
    lrvec = np.array([np.log(r_max - t) for t in range(num_res - 1)]) * alpha_m1 + beta
    avec = alpha * np.exp(lrvec)
    a2vec = np.square(alpha * gamma) * np.exp(lrvec * 2.0)
    # Draw the [eps_t] for all samples
    epsmat = random_state.normal(size=(num_res, num_samples))
    # Compute samples [f_t], [y_t], not conditioned on targets
    hvec = (
        random_state.normal(size=(1, num_samples)) * np.sqrt(post_variance) + post_mean
    )
    f_rows = []
    y_rows = []
    fcurr = hvec
    for t in range(num_res - 1):
        eps_row = _rowvec(epsmat[t], _np=np)
        f_rows.append(fcurr)
        y_rows.append(fcurr + eps_row)
        fcurr = fcurr - avec[t] * (eps_row * gamma + 1.0)
    eps_row = _rowvec(epsmat[-1], _np=np)
    f_rows.append(fcurr)
    y_rows.append(fcurr + eps_row)
    if ydim > 0:
        # Condition on targets
        # Prior samples (reverse order t -> r)
        fsamples = np.concatenate(tuple(reversed(f_rows[: (t_obs + 1)])), axis=0)
        # Compute c1 and d1 vectors (same for all samples)
        zeroscal = np.zeros((1,))
        c1vec = np.flip(np.concatenate((zeroscal, np.cumsum(avec[:t_obs])), axis=None))
        d1vec = np.flip(np.concatenate((zeroscal, np.cumsum(a2vec[:t_obs])), axis=None))
        # Assemble targets for conditional means
        ymat = np.concatenate(tuple(reversed(y_rows[t_obs:])), axis=0)
        ycols = np.split(ymat, num_samples, axis=1)
        assert ycols[0].size == ydim  # Sanity check
        # v^T v, w^T v for sampled targets
        onevec = np.ones((num_samples,))
        _issm_params = {"alpha": alpha * onevec, "beta": beta * onevec, "gamma": gamma}
        issm_likelihood = issm_likelihood_slow_computations(
            targets=[_colvec(v, _np=np) for v in ycols],
            issm_params=_issm_params,
            r_min=r_min,
            r_max=r_max,
            skip_c_d=True,
        )
        vtv = issm_likelihood["vtv"]
        wtv = issm_likelihood["wtv"]
        # v^T v, w^T v for observed (last entry)
        issm_likelihood = issm_likelihood_slow_computations(
            targets=[targets], issm_params=issm_params, r_min=r_min, r_max=r_max
        )
        vtv = _rowvec(np.concatenate((vtv, issm_likelihood["vtv"]), axis=None), _np=np)
        wtv = _rowvec(np.concatenate((wtv, issm_likelihood["wtv"]), axis=None), _np=np)
        cscal = issm_likelihood["c"][0]
        dscal = issm_likelihood["d"][0]
        c1vec = _colvec(c1vec, _np=np)
        d1vec = _colvec(d1vec, _np=np)
        c2vec = cscal - c1vec
        d2vec = dscal - d1vec
        # Compute num_samples + 1 conditional mean vectors in one go
        denom = vtv * (post_variance + dscal) + 1.0
        cond_means = (
            (post_mean - c1vec) * (d2vec * vtv + 1.0)
            + (d1vec + post_variance) * (c2vec * vtv + wtv)
        ) / denom
        fsamples = (
            fsamples
            - cond_means[:, :num_samples]
            + _colvec(cond_means[:, num_samples], _np=np)
        )
        # Samples [y_r] from [f_r]
        frmat = fsamples[1:]
        frm1mat = fsamples[:-1]
        arvec = _colvec(
            np.minimum(np.flip(avec[:t_obs]), -MIN_POSTERIOR_VARIANCE), _np=np
        )
        ysamples = ((frmat - frm1mat) / arvec - 1.0) * (1.0 / gamma) + frmat
    else:
        # Nothing to condition on
        fsamples = np.concatenate(tuple(reversed(f_rows)), axis=0)
        ysamples = np.concatenate(tuple(reversed(y_rows)), axis=0)
    return {"f": fsamples, "y": ysamples}


def issm_likelihood_slow_computations(
    targets: List[np.ndarray],
    issm_params: Dict,
    r_min: int,
    r_max: int,
    skip_c_d: bool = False,
    profiler: Optional[SimpleProfiler] = None,
) -> Dict:
    """
    Naive implementation of `issm_likelihood_computations`, which does not
    require precomputations, but is much slower. Here, results are computed
    one datapoint at a time, instead of en bulk.

    This code is used in unit testing, and called from `sample_posterior_joint`.
    """
    num_configs = len(targets)
    num_res = r_max + 1 - r_min
    assert num_configs > 0, "targets must not be empty"
    assert num_res > 0, f"r_min = {r_min} must be <= r_max = {r_max}"
    compute_wtw = targets[0].shape[1] == 1
    alphas = _flatvec(issm_params["alpha"])
    betas = _flatvec(issm_params["beta"])
    gamma = issm_params["gamma"]
    n = getval(alphas.shape[0])
    assert n == num_configs, f"alpha.size = {n} != {num_configs}"
    n = getval(betas.shape[0])
    assert n == num_configs, f"beta.size = {n} != {num_configs}"
    # Outer loop over configurations
    c_lst = []
    d_lst = []
    vtv_lst = []
    wtv_lst = []
    wtw_lst = []
    num_data = 0
    for i, ymat in enumerate(targets):
        alpha = alphas[i]
        alpha_m1 = alpha - 1.0
        beta = betas[i]
        ydim = ymat.shape[0]
        if profiler is not None:
            profiler.start("issm_part1")
        num_data += ydim
        r_obs = r_min + ydim  # Observed in range(r_min, r_obs)
        assert 0 < ydim <= num_res, f"len(y[{i}]) = {ydim}, num_res = {num_res}"
        if not skip_c_d:
            # c_i, d_i
            if ydim < num_res:
                lrvec = (
                    anp.array([np.log(r) for r in range(r_obs, r_max + 1)]) * alpha_m1
                    + beta
                )
                c_scal = alpha * anp.exp(logsumexp(lrvec))
                d_scal = anp.square(gamma * alpha) * anp.exp(logsumexp(lrvec * 2.0))
                c_lst.append(c_scal)
                d_lst.append(d_scal)
            else:
                c_lst.append(0.0)
                d_lst.append(0.0)
        # Inner loop for v_i, w_i
        if profiler is not None:
            profiler.stop("issm_part1")
            profiler.start("issm_part2")
        yprev = ymat[-1].reshape((1, -1))  # y_{j-1} (vector)
        vprev = 1.0  # v_{j-1} (scalar)
        wprev = yprev  # w_{j-1} (row vector)
        vtv = vprev * vprev  # scalar
        wtv = wprev * vprev  # row vector
        if compute_wtw:
            wtw = wprev * wprev  # shape (1, 1)
        for j in range(1, ydim):
            ycurr = ymat[ydim - j - 1].reshape((1, -1))  # y_j (row vector)
            # a_{j-1}
            ascal = alpha * anp.exp(np.log(r_obs - j) * alpha_m1 + beta)
            escal = gamma * ascal + 1.0
            vcurr = escal * vprev  # v_j
            wcurr = escal * wprev + ycurr - yprev + ascal  # w_j
            vtv = vtv + vcurr * vcurr
            wtv = wtv + wcurr * vcurr
            if compute_wtw:
                wtw = wtw + wcurr * wcurr
            yprev = ycurr
            vprev = vcurr
            wprev = wcurr
        vtv_lst.append(vtv)
        wtv_lst.append(wtv)
        if compute_wtw:
            assert wtw.shape == (1, 1)
            wtw_lst.append(wtw.item())
        if profiler is not None:
            profiler.stop("issm_part2")
    # Compile results
    result = {
        "num_data": num_data,
        "vtv": anp.array(vtv_lst),
        "wtv": anp.vstack(wtv_lst),
    }
    if compute_wtw:
        result["wtw"] = anp.array(wtw_lst)
    if not skip_c_d:
        result["c"] = anp.array(c_lst)
        result["d"] = anp.array(d_lst)
    return result


def _update_posterior_internal(
    poster_state: Dict, kernel, feature, d_new, s_new, r2_new
) -> Dict:
    assert "r2vec" in poster_state and "r4vec" in poster_state
    features = poster_state["features"]
    r2vec = poster_state["r2vec"]
    r4vec = poster_state["r4vec"]
    svec = poster_state["svec"]
    chol_fact = poster_state["chol_fact"]
    # New row of L: [evec * s_new, l_new]
    feature = _rowvec(feature, _np=np)
    kvec = _flatvec(kernel(features, feature), _np=np)
    evec = _flatvec(
        spl.solve_triangular(chol_fact, _colvec(kvec * svec), lower=True), _np=np
    )
    kscal = _flatvec(kernel.diagonal(feature), _np=np)[0]
    khat_min_esq = kscal + d_new - _squared_norm(evec, _np=np)
    l_new = np.sqrt(khat_min_esq * np.square(s_new) + 1.0)
    # New entry of r_4
    pref = s_new / l_new
    r4_new = pref * (
        _inner_product(kvec, r2vec, _np=np)
        + khat_min_esq * r2_new
        - _inner_product(evec, r4vec, _np=np)
    )
    # Update of p
    p_new = r2_new - pref * r4_new
    # L^{-T} e
    ltinv_evec = _flatvec(
        spl.solve_triangular(chol_fact, _colvec(evec, _np=np), lower=True, trans="T"),
        _np=np,
    )
    return {
        "evec": evec,
        "ltinv_evec": ltinv_evec,
        "l_new": l_new,
        "r4_new": r4_new,
        "p_new": p_new,
    }


def update_posterior_state(
    poster_state: Dict, kernel, feature, d_new, s_new, r2_new
) -> Dict:
    """
    Incremental update of posterior state, given data for one additional
    configuration. The new datapoint gives rise to a new row/column of the
    Cholesky factor. r2vec and svec are extended by `r2_new`, `s_new`
    respectively. r4vec and pvec are extended and all entries change. The new
    datapoint is represented by `feature`, `d_new`, `s_new`, `r2_new`.

    Note: The field `criterion` is not updated, but set to np.nan.

    :param poster_state: Posterior state for data
    :param kernel: Kernel function
    :param feature: Features for additional config
    :param d_new: See above
    :param s_new: See above
    :param r2_new: See above
    :return: Updated posterior state
    """
    features = poster_state["features"]
    assert "r2vec" in poster_state and "r4vec" in poster_state
    r2vec = poster_state["r2vec"]
    r4vec = poster_state["r4vec"]
    svec = poster_state["svec"]
    pvec = poster_state["pmat"]
    assert pvec.shape[1] == 1, "Cannot update fantasizing posterior_state"
    pvec = _flatvec(pvec, _np=np)
    chol_fact = poster_state["chol_fact"]
    feature = _rowvec(feature, _np=np)
    # Update computations:
    result = _update_posterior_internal(
        poster_state, kernel, feature, d_new, s_new, r2_new
    )
    # Put together new state variables
    # Note: Criterion not updated, but invalidated
    new_poster_state = {
        "criterion": np.nan,
        "features": np.concatenate((features, feature), axis=0),
        "svec": np.concatenate((svec, np.array([s_new]))),
        "r2vec": np.concatenate((r2vec, np.array([r2_new]))),
    }
    evec = result["evec"]
    ltinv_evec = result["ltinv_evec"]
    l_new = result["l_new"]
    r4_new = result["r4_new"]
    p_new = result["p_new"]
    new_poster_state["r4vec"] = np.concatenate(
        [r4vec + evec * r2_new, np.array([r4_new])]
    )
    new_poster_state["pmat"] = _colvec(
        np.concatenate([pvec - (ltinv_evec * svec) * p_new, np.array([p_new])]), _np=np
    )
    lvec = _rowvec(evec, _np=np) * s_new
    zerovec = _colvec(np.zeros_like(lvec), _np=np)
    lscal = np.array([l_new]).reshape((1, 1))
    new_poster_state["chol_fact"] = np.concatenate(
        (
            np.concatenate((chol_fact, lvec), axis=0),
            np.concatenate((zerovec, lscal), axis=0),
        ),
        axis=1,
    )
    return new_poster_state


def update_posterior_pvec(
    poster_state: Dict, kernel, feature, d_new, s_new, r2_new
) -> np.ndarray:
    """
    Part of `update_posterior_state`, just returns the new p vector.

    :param poster_state: See `update_posterior_state`
    :param kernel:  See `update_posterior_state`
    :param feature:  See `update_posterior_state`
    :param d_new:  See `update_posterior_state`
    :param s_new:  See `update_posterior_state`
    :param r2_new:  See `update_posterior_state`
    :return: New p vector, as flat vector

    """
    # Update computations:
    result = _update_posterior_internal(
        poster_state, kernel, feature, d_new, s_new, r2_new
    )
    svec = poster_state["svec"]
    pvec = poster_state["pmat"]
    assert pvec.shape[1] == 1, "Cannot update fantasizing posterior_state"
    pvec = _flatvec(pvec, _np=np)
    ltinv_evec = result["ltinv_evec"]
    p_new = result["p_new"]
    return np.concatenate((pvec - (ltinv_evec * svec) * p_new, np.array([p_new])))

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/learncurve/likelihood.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Union, Optional, List
import autograd.numpy as anp

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    MarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    PosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.model_params import (
    ISSModelParameters,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.posterior_state import (
    GaussProcISSMPosteriorState,
    GaussProcExpDecayPosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.freeze_thaw import (
    ExponentialDecayBaseKernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    INITIAL_NOISE_VARIANCE,
    NOISE_VARIANCE_LOWER_BOUND,
    NOISE_VARIANCE_UPPER_BOUND,
    DEFAULT_ENCODING,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    Gamma,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    register_parameter,
    create_encoding,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
    MeanFunction,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler


LCModel = Union[ISSModelParameters, ExponentialDecayBaseKernelFunction]


class GaussAdditiveMarginalLikelihood(MarginalLikelihood):
    """
    Marginal likelihood of joint learning curve model, where each curve is
    modelled as sum of a Gaussian process over x (for the value at r_max)
    and a Gaussian model over r.

    The latter `res_model` is either an ISSM or another Gaussian process with
    exponential decay covariance function.

    :param kernel: Kernel function k(x, x')
    :param res_model: Gaussian model over r
    :param mean: Mean function mu(x)
    :param initial_noise_variance: A scalar to initialize the value of the
        residual noise variance
    """

    def __init__(
        self,
        kernel: KernelFunction,
        res_model: LCModel,
        mean: MeanFunction = None,
        initial_noise_variance=None,
        encoding_type=None,
        **kwargs
    ):
        super(GaussAdditiveMarginalLikelihood, self).__init__(**kwargs)
        assert isinstance(
            res_model, (ISSModelParameters, ExponentialDecayBaseKernelFunction)
        ), "res_model must be ISSModelParameters or ExponentialDecayBaseKernelFunction"
        if mean is None:
            mean = ScalarMeanFunction()
        if initial_noise_variance is None:
            initial_noise_variance = INITIAL_NOISE_VARIANCE
        if encoding_type is None:
            encoding_type = DEFAULT_ENCODING
        self.encoding = create_encoding(
            encoding_name=encoding_type,
            init_val=initial_noise_variance,
            constr_lower=NOISE_VARIANCE_LOWER_BOUND,
            constr_upper=NOISE_VARIANCE_UPPER_BOUND,
            dimension=1,
            prior=Gamma(mean=0.1, alpha=0.1),
        )
        self.mean = mean
        self.kernel = kernel
        self.res_model = res_model
        if isinstance(res_model, ISSModelParameters):
            tag = "issm_"
            self._type = GaussProcISSMPosteriorState
            self._posterstate_kwargs = {
                "mean": self.mean,
                "kernel": self.kernel,
                "iss_model": self.res_model,
            }
        else:
            tag = "expdecay_"
            self._type = GaussProcExpDecayPosteriorState
            self._posterstate_kwargs = {
                "mean": self.mean,
                "kernel": self.kernel,
                "res_kernel": self.res_model,
            }
        self._components = [
            ("kernel_", self.kernel),
            ("mean_", self.mean),
            (tag, self.res_model),
        ]
        self._profiler = None
        with self.name_scope():
            self.noise_variance_internal = register_parameter(
                self.params, "noise_variance", self.encoding
            )

    def set_profiler(self, profiler: Optional[SimpleProfiler]):
        self._profiler = profiler

    def get_posterior_state(self, data: dict) -> PosteriorState:
        return self._type(
            data,
            **self._posterstate_kwargs,
            noise_variance=self.get_noise_variance(),
            profiler=self._profiler
        )

    def forward(self, data: dict):
        assert not data["do_fantasizing"], (
            "data must not be for fantasizing. Call prepare_data with "
            + "do_fantasizing=False"
        )
        return super().forward(data)

    def param_encoding_pairs(self) -> List[tuple]:
        own_param_encoding_pairs = [(self.noise_variance_internal, self.encoding)]
        return (
            own_param_encoding_pairs
            + self.mean.param_encoding_pairs()
            + self.kernel.param_encoding_pairs()
            + self.res_model.param_encoding_pairs()
        )

    def get_noise_variance(self, as_ndarray=False):
        noise_variance = encode_unwrap_parameter(
            self.noise_variance_internal, self.encoding
        )
        return noise_variance if as_ndarray else anp.reshape(noise_variance, (1,))[0]

    def _set_noise_variance(self, val):
        self.encoding.set(self.noise_variance_internal, val)

    def get_params(self):
        result = {"noise_variance": self.get_noise_variance()}
        for pref, func in self._components:
            result.update({(pref + k): v for k, v in func.get_params().items()})
        return result

    def set_params(self, param_dict):
        for pref, func in self._components:
            len_pref = len(pref)
            stripped_dict = {
                k[len_pref:]: v for k, v in param_dict.items() if k.startswith(pref)
            }
            func.set_params(stripped_dict)
        self._set_noise_variance(param_dict["noise_variance"])

    def data_precomputations(self, data: dict, overwrite: bool = False):
        if overwrite or not self._type.has_precomputations(data):
            self._type.data_precomputations(data)

    def on_fit_start(self, data: dict, profiler: Optional[SimpleProfiler] = None):
        assert not data["do_fantasizing"], (
            "data must not be for fantasizing. Call prepare_data with "
            + "do_fantasizing=False"
        )
        self.data_precomputations(data)
        if profiler is not None:
            self.set_profiler(profiler)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/learncurve/model_params.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import autograd.numpy as anp
from autograd.tracer import getval

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    COVARIANCE_SCALE_LOWER_BOUND,
    DEFAULT_ENCODING,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    Gamma,
    Normal,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    IdentityScalarEncoding,
    register_parameter,
    create_encoding,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)


class ISSModelParameters(MeanFunction):
    """
    Maintains parameters of an ISSM of a particular power low decay form.

    For each configuration, we have alpha < 0 and beta. These may depend
    on the input feature x (encoded configuration):
        (alpha, beta) = F(x; params),
    where params are the internal parameters to be learned.

    There is also gamma > 0, which can be fixed to 1.

    """

    def __init__(self, gamma_is_one: bool = False, **kwargs):
        super().__init__(**kwargs)
        self.gamma_is_one = gamma_is_one
        if not gamma_is_one:
            self.gamma_encoding = create_encoding(
                DEFAULT_ENCODING,
                1.0,
                COVARIANCE_SCALE_LOWER_BOUND,
                10.0,
                1,
                Gamma(mean=1.0, alpha=0.1),
            )
            with self.name_scope():
                self.gamma_internal = register_parameter(
                    self.params, "gamma", self.gamma_encoding
                )

    def param_encoding_pairs(self):
        if self.gamma_is_one:
            return []
        else:
            return [(self.gamma_internal, self.gamma_encoding)]

    def get_gamma(self):
        if self.gamma_is_one:
            return 1.0
        else:
            gamma = encode_unwrap_parameter(self.gamma_internal, self.gamma_encoding)
            return anp.reshape(gamma, (1,))[0]

    def get_params(self):
        if self.gamma_is_one:
            return dict()
        else:
            return {"gamma": self.get_gamma()}

    def set_gamma(self, val):
        assert not self.gamma_is_one, "Cannot set gamma (fixed to 1)"
        self.gamma_encoding.set(self.gamma_internal, val)

    def set_params(self, param_dict):
        if not self.gamma_is_one:
            self.set_gamma(param_dict["gamma"])

    def get_issm_params(self, features) -> Dict:
        """
        Given feature matrix X, returns ISSM parameters which configure the
        likelihood: alpha, beta vectors (size n), gamma scalar.

        :param features: Feature matrix X, (n, d)
        :return: Dict with alpha, beta, gamma

        """
        raise NotImplementedError()


class IndependentISSModelParameters(ISSModelParameters):
    """
    Most basic implementation, where alpha, beta are scalars, independent of
    the configuration.

    """

    def __init__(self, gamma_is_one: bool = False, **kwargs):
        super().__init__(gamma_is_one, **kwargs)
        self.negalpha_encoding = create_encoding(
            DEFAULT_ENCODING, 0.5, 1e-3, 5.0, 1, Gamma(mean=0.5, alpha=0.1)
        )
        self.beta_encoding = IdentityScalarEncoding(
            constr_lower=-5.0,
            constr_upper=5.0,
            init_val=0.0,
            regularizer=Normal(0.0, 1.0),
        )
        with self.name_scope():
            self.negalpha_internal = register_parameter(
                self.params, "negalpha", self.negalpha_encoding
            )
            self.beta_internal = register_parameter(
                self.params, "beta", self.beta_encoding
            )

    def param_encoding_pairs(self):
        return super().param_encoding_pairs() + [
            (self.negalpha_internal, self.negalpha_encoding),
            (self.beta_internal, self.beta_encoding),
        ]

    def get_alpha(self):
        negalpha = encode_unwrap_parameter(
            self.negalpha_internal, self.negalpha_encoding
        )
        return -anp.reshape(negalpha, (1,))[0]

    def get_beta(self):
        beta = encode_unwrap_parameter(self.beta_internal, self.beta_encoding)
        return anp.reshape(beta, (1,))[0]

    def get_params(self):
        _params = super().get_params()
        return dict(_params, alpha=self.get_alpha(), beta=self.get_beta())

    def set_alpha(self, val):
        self.negalpha_encoding.set(self.negalpha_internal, -val)

    def set_beta(self, val):
        self.beta_encoding.set(self.beta_internal, val)

    def set_params(self, param_dict):
        super().set_params(param_dict)
        self.set_alpha(param_dict["alpha"])
        self.set_beta(param_dict["beta"])

    def get_issm_params(self, features) -> Dict:
        n = getval(features.shape[0])
        one_vec = anp.ones((n,))
        return {
            "alpha": one_vec * self.get_alpha(),
            "beta": one_vec * self.get_beta(),
            "gamma": self.get_gamma(),
        }

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/learncurve/posterior_state.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import autograd.numpy as anp
from autograd import grad
from typing import Tuple, Dict, List, Optional
from numpy.random import RandomState
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    PosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.issm import (
    issm_likelihood_slow_computations,
    posterior_computations,
    sample_posterior_marginals,
    _inner_product,
    issm_likelihood_computations,
    issm_likelihood_precomputations,
    _rowvec,
    update_posterior_state,
    update_posterior_pvec,
    _flatvec,
    _colvec,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_impl import (
    decode_extended_features,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.issm import (
    sample_posterior_joint as sample_posterior_joint_issm,
    predict_posterior_marginals_extended as predict_posterior_marginals_issm,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.freeze_thaw import (
    sample_posterior_joint as sample_posterior_joint_expdecay,
    predict_posterior_marginals_extended as predict_posterior_marginals_expdecay,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.model_params import (
    ISSModelParameters,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.freeze_thaw import (
    resource_kernel_likelihood_slow_computations,
    ExponentialDecayBaseKernelFunction,
    logdet_cholfact_cov_resource,
    resource_kernel_likelihood_computations,
    resource_kernel_likelihood_precomputations,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)

logger = logging.getLogger(__name__)

__all__ = [
    "GaussProcAdditivePosteriorState",
    "IncrementalUpdateGPAdditivePosteriorState",
    "GaussProcISSMPosteriorState",
    "GaussProcExpDecayPosteriorState",
]


class GaussProcAdditivePosteriorState(PosteriorState):
    """
    Represent posterior state for joint Gaussian model of learning curves over
    a number of configurations. The (additive) model is the sum of a Gaussian
    process model for function values at r_max and independent Gaussian models
    over r only.

    Importantly, inference scales cubically only in the number of
    configurations, not in the number of observations.

    """

    def __init__(
        self,
        data: Optional[dict],
        mean: MeanFunction,
        kernel: KernelFunction,
        noise_variance,
        **kwargs
    ):
        """
        `data` contains input points and targets, as obtained from
        `issm.prepare_data`. `iss_model` maintains the ISSM parameters.

        :param data: Input points and targets
        :param mean: Mean function m(X)
        :param kernel: Kernel function k(X, X')
        :param noise_variance: Noise variance
        """
        self.mean = mean
        self.kernel = kernel
        self.noise_variance = noise_variance
        self.poster_state = None
        if data is not None:
            self.r_min = data["r_min"]
            self.r_max = data["r_max"]
            # Compute posterior state
            self._compute_posterior_state(data, noise_variance, **kwargs)
        else:
            # Copy constructor, used by `IncrementalUpdateGPISSMPosteriorState`
            # subclass
            self.poster_state = kwargs["poster_state"]
            self.r_min = kwargs["r_min"]
            self.r_max = kwargs["r_max"]

    @property
    def num_data(self):
        return self.poster_state["features"].shape[0]

    @property
    def num_features(self):
        return self.poster_state["features"].shape[1]

    @property
    def num_fantasies(self):
        return self.poster_state["pmat"].shape[1]

    def _compute_posterior_state(self, data: dict, noise_variance, **kwargs):
        raise NotImplementedError()

    def neg_log_likelihood(self) -> anp.ndarray:
        assert (
            "criterion" in self.poster_state
        ), "neg_log_likelihood not defined for fantasizing posterior state"
        return self.poster_state["criterion"]

    def predict(self, test_features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        We compute marginals over f(x, r), where `test_features` are extended
        features.
        Note: The test configs must not overlap with any in the training set.
        Otherwise, at least if `r != r_max`, the predictive distributions
        computed here may be wrong.

        :param test_features: Extended features for test configs
        :return: posterior_means, posterior_variances
        """
        raise NotImplementedError()

    def sample_marginals(
        self,
        test_features: np.ndarray,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> np.ndarray:
        """
        See comments of `predict`.

        :param test_features: Input points for test configs
        :param num_samples: Number of samples
        :param random_state: PRNG
        :return: Marginal samples, (num_test, num_samples)
        """
        if random_state is None:
            random_state = np.random
        return sample_posterior_marginals(
            self.poster_state,
            self.mean,
            self.kernel,
            test_features,
            random_state=random_state,
            num_samples=num_samples,
        )

    def backward_gradient(
        self,
        input: np.ndarray,
        head_gradients: Dict[str, np.ndarray],
        mean_data: float,
        std_data: float,
    ) -> np.ndarray:
        """
        Implements SurrogateModel.backward_gradient, see comments there.
        This is for a single posterior state. If the SurrogateModel uses
        MCMC, have to call this for every sample.

        :param input: Single input point x, shape (d,)
        :param head_gradients: See SurrogateModel.backward_gradient
        :param mean_data: Mean used to normalize targets
        :param std_data: Stddev used to normalize targets
        :return:
        """
        test_feature = np.reshape(input, (1, -1))

        def diff_test_feature(test_feature_array):
            norm_mean, norm_variance = self.predict(test_feature_array)
            # De-normalize, and variance -> stddev
            pred_mean = norm_mean * std_data + mean_data
            pred_std = anp.sqrt(norm_variance) * std_data
            head_gradients_mean = anp.reshape(head_gradients["mean"], pred_mean.shape)
            head_gradients_std = anp.reshape(head_gradients["std"], pred_std.shape)
            # Added to mimic mxnet.autograd.backward
            pred_mean_sum = _inner_product(pred_mean, head_gradients_mean)
            pred_std_sum = _inner_product(pred_std, head_gradients_std)
            return pred_mean_sum + pred_std_sum

        test_feature_gradient = grad(diff_test_feature)
        return np.reshape(test_feature_gradient(test_feature), input.shape)

    def _sample_curves_internal(
        self,
        data: dict,
        poster_state: dict,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> List[dict]:
        raise NotImplementedError()

    def sample_curves(
        self,
        data: dict,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> List[dict]:
        """
        Given data from one or more configurations (as returned by
        `issm.prepare_data`), for each config, sample a curve from the
        joint posterior (predictive) distribution over latent targets.
        The curve for each config in `data` may be partly observed, but
        must not be fully observed. Samples for the different configs are
        independent. None of the configs in `data` must appear in the dataset
        used to compute the posterior state.

        The result is a list of dict, one for each config. If for a config,
        targets in `data` are given for resource values range(r_min, r_obs),
        the dict entry `y` is a joint sample [y_r], r in range(r_obs, r_max+1).
        For some subclasses (e.g., ISSM), there is also an entry `f` with a
        joint sample [f_r], r in range(r_obs-1, r_max+1), the latent function
        values before noise. These entries are matrices with `num_samples`
        columns, which are independent (the joint dependence is along the rows).

        :param data: Data for configs to predict at
        :param num_samples: Number of samples to draw from each curve
        :param random_state: PRNG state to be used for sampling
        :return: See above
        """
        return self._sample_curves_internal(
            data=data,
            poster_state=self.poster_state,
            num_samples=num_samples,
            random_state=random_state,
        )

    @staticmethod
    def has_precomputations(data: dict) -> bool:
        raise NotImplementedError()


class IncrementalUpdateGPAdditivePosteriorState(GaussProcAdditivePosteriorState):
    """
    Extension of :class:`GaussProcAdditivePosteriorState` which allows for
    incremental updating (single config added to the dataset).
    This is required for simulation-based scoring, and for support of
    fantasizing.

    """

    def __init__(
        self,
        data: Optional[dict],
        mean: MeanFunction,
        kernel: KernelFunction,
        noise_variance,
        **kwargs
    ):
        super().__init__(data, mean, kernel, noise_variance, **kwargs)

    def _prepare_update(
        self, feature: np.ndarray, targets: np.ndarray
    ) -> Tuple[float, float, float]:
        """
        Helper method for `update`. Returns new entries for d, s, r2 vectors.

        :param feature: See `update`
        :param targets: See `update`
        :return: (d_new, s_new, r2_new)
        """
        raise NotImplementedError()

    def _update_internal(self, feature: np.ndarray, targets: np.ndarray) -> dict:
        """
        Update posterior state, given a single new datapoint. `feature`,
        `targets` are like one entry of `data`. The method returns a new
        object with the updated state.

        :param feature: See above
        :param targets: See above
        :return: Arguments to create new posterior state
        """
        # Update posterior state
        feature = _rowvec(feature, _np=np)
        d_new, s_new, r2_new = self._prepare_update(feature, targets)
        new_poster_state = update_posterior_state(
            self.poster_state, self.kernel, feature, d_new, s_new, r2_new
        )
        # Return args to create new object by way of "copy constructor"
        return dict(
            data=None,
            mean=self.mean,
            kernel=self.kernel,
            noise_variance=self.noise_variance,
            poster_state=new_poster_state,
            r_min=self.r_min,
            r_max=self.r_max,
        )

    def update(
        self, feature: np.ndarray, targets: np.ndarray
    ) -> "IncrementalUpdateGPAdditivePosteriorState":
        raise NotImplementedError()

    def update_pvec(self, feature: np.ndarray, targets: np.ndarray) -> np.ndarray:
        """
        Part of `update`: Only update prediction vector p. This cannot be used
        to update p for several new datapoints.

        :param feature:
        :param targets:
        :return: New p vector
        """
        feature = _rowvec(feature, _np=np)
        d_new, s_new, r2_new = self._prepare_update(feature, targets)
        return update_posterior_pvec(
            self.poster_state, self.kernel, feature, d_new, s_new, r2_new
        )

    def _sample_posterior_joint_for_config(
        self,
        poster_state: dict,
        config: Configuration,
        feature: np.ndarray,
        targets: np.ndarray,
        random_state: RandomState,
    ) -> np.ndarray:
        data = {"features": [feature], "targets": [targets], "configs": [config]}
        results = self._sample_curves_internal(
            data, poster_state, num_samples=1, random_state=random_state
        )
        return results[0]["y"]

    def sample_and_update_for_pending(
        self,
        data_pending: dict,
        sample_all_nonobserved: bool = False,
        random_state: Optional[RandomState] = None,
    ) -> (List[np.ndarray], "IncrementalUpdateGPAdditivePosteriorState"):
        """
        This function is needed for sampling fantasy targets, and also to
        support simulation-based scoring.

        `issm.prepare_data_with_pending` creates two data dicts `data_nopending`,
        `data_pending`, the first for configs with observed data, but no
        pending evals, the second for configs with pending evals.
        You create the state with `data_nopending`, then call this method with
        `data_pending`.

        This method is iterating over configs (or trials) in `data_pending`.
        For each config, it draws a joint sample from some non-observed
        targets, then updates the state conditioned on observed and sampled
        targets (by calling `update`). If `sample_all_nonobserved` is False,
        the number of targets sampled is the entry in
        `data_pending['num_pending']`. Otherwise, targets are sampled for all
        non-observed positions.

        The method returns the list of sampled target vectors, and the state
        at the end (like `update` does as well).

        :param data_pending: See above
        :param sample_all_nonobserved: See above
        :param random_state: PRNG
        :return: pending_targets, final_state
        """
        if random_state is None:
            random_state = np.random
        curr_poster_state = self.poster_state
        targets_lst = []
        final_state = self
        for config, feature, targets, num_pending in zip(
            data_pending["configs"],
            data_pending["features"],
            data_pending["targets"],
            data_pending["num_pending"],
        ):
            # Draw joint sample
            fantasies = _flatvec(
                self._sample_posterior_joint_for_config(
                    curr_poster_state, config, feature, targets, random_state
                ),
                _np=np,
            )
            if not sample_all_nonobserved:
                fantasies = fantasies[:num_pending]
            fantasies = _colvec(fantasies, _np=np)
            targets_lst.append(fantasies)
            # Update state
            full_targets = np.vstack((targets, fantasies))
            final_state = self.update(feature, full_targets)
            curr_poster_state = final_state.poster_state
        return targets_lst, final_state


class GaussProcISSMPosteriorState(IncrementalUpdateGPAdditivePosteriorState):
    """
    Represent posterior state for joint Gaussian model of learning curves over
    a number of configurations. The model is the sum of a Gaussian process
    model for function values at r_max and independent Gaussian linear
    innovation state space models (ISSMs) of a particular power law decay
    form.

    """

    def __init__(
        self,
        data: Optional[dict],
        mean: MeanFunction,
        kernel: KernelFunction,
        iss_model: ISSModelParameters,
        noise_variance,
        **kwargs
    ):
        """
        `data` contains input points and targets, as obtained from
        `issm.prepare_data`. `iss_model` maintains the ISSM parameters.

        :param data: Input points and targets
        :param mean: Mean function m(X)
        :param kernel: Kernel function k(X, X')
        :param iss_model: ISS model
        :param noise_variance: Innovation and noise variance
        """
        self.iss_model = iss_model
        super().__init__(data, mean, kernel, noise_variance=noise_variance, **kwargs)

    @staticmethod
    def has_precomputations(data: dict) -> bool:
        return all(k in data for k in ("ydims", "num_configs", "deltay", "logr"))

    def _compute_posterior_state(self, data: dict, noise_variance, **kwargs):
        profiler = kwargs.get("profiler")
        # Compute posterior state
        issm_params = self.iss_model.get_issm_params(data["features"])
        if self.has_precomputations(data):
            issm_likelihood = issm_likelihood_computations(
                precomputed=data,
                issm_params=issm_params,
                r_min=self.r_min,
                r_max=self.r_max,
                profiler=profiler,
            )
        else:
            issm_likelihood = issm_likelihood_slow_computations(
                targets=data["targets"],
                issm_params=issm_params,
                r_min=self.r_min,
                r_max=self.r_max,
                profiler=profiler,
            )
        if profiler is not None:
            profiler.start("poster_comp")
        self.poster_state = posterior_computations(
            features=data["features"],
            mean=self.mean,
            kernel=self.kernel,
            issm_likelihood=issm_likelihood,
            noise_variance=noise_variance,
        )
        if profiler is not None:
            profiler.stop("poster_comp")

    def predict(self, test_features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        resource_attr_range = (self.r_min, self.r_max)
        features, resources = decode_extended_features(
            test_features, resource_attr_range=resource_attr_range
        )
        issm_params = self.iss_model.get_issm_params(features)
        return predict_posterior_marginals_issm(
            poster_state=self.poster_state,
            mean=self.mean,
            kernel=self.kernel,
            test_features=features,
            resources=list(resources),
            issm_params=issm_params,
            r_min=self.r_min,
            r_max=self.r_max,
        )

    @staticmethod
    def data_precomputations(data: dict):
        logger.info("Enhancing data dictionary by precomputed variables")
        data.update(
            issm_likelihood_precomputations(
                targets=data["targets"], r_min=data["r_min"]
            )
        )

    def _sample_curves_internal(
        self,
        data: dict,
        poster_state: dict,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> List[dict]:
        if random_state is None:
            random_state = np.random
        results = []
        for feature, targets, config in zip(
            data["features"], data["targets"], data["configs"]
        ):
            issm_params = self.iss_model.get_issm_params(feature.reshape((1, -1)))
            results.append(
                sample_posterior_joint_issm(
                    poster_state=poster_state,
                    mean=self.mean,
                    kernel=self.kernel,
                    feature=feature,
                    targets=targets,
                    issm_params=issm_params,
                    r_min=self.r_min,
                    r_max=self.r_max,
                    random_state=random_state,
                    num_samples=num_samples,
                )
            )
        return results

    def _prepare_update(
        self, feature: np.ndarray, targets: np.ndarray
    ) -> Tuple[float, float, float]:
        issm_params = self.iss_model.get_issm_params(feature.reshape((1, -1)))
        issm_likelihood = issm_likelihood_slow_computations(
            targets=[_colvec(targets, _np=np)],
            issm_params=issm_params,
            r_min=self.r_min,
            r_max=self.r_max,
        )
        d_new = issm_likelihood["d"].item()
        vtv = issm_likelihood["vtv"].item()
        wtv = issm_likelihood["wtv"].item()
        s_sq = vtv / self.noise_variance
        s_new = np.sqrt(s_sq)
        muhat = _flatvec(self.mean(feature)).item() - issm_likelihood["c"].item()
        r2_new = wtv / self.noise_variance - s_sq * muhat
        return d_new, s_new, r2_new

    def update(
        self, feature: np.ndarray, targets: np.ndarray
    ) -> "IncrementalUpdateGPAdditivePosteriorState":
        create_kwargs = self._update_internal(feature, targets)
        return GaussProcISSMPosteriorState(**create_kwargs, iss_model=self.iss_model)


class GaussProcExpDecayPosteriorState(IncrementalUpdateGPAdditivePosteriorState):
    """
    Represent posterior state for joint Gaussian model of learning curves over
    a number of configurations. The model is the sum of a Gaussian process
    model for function values at r_max and independent Gaussian processes over
    r, using an exponential decay covariance function. The latter is shared
    between all configs.

    This is essentially the model from the Freeze Thaw paper (see also
    :class:`ExponentialDecayResourcesKernelFunction`).

    """

    def __init__(
        self,
        data: Optional[dict],
        mean: MeanFunction,
        kernel: KernelFunction,
        res_kernel: ExponentialDecayBaseKernelFunction,
        noise_variance,
        **kwargs
    ):
        """
        `data` contains input points and targets, as obtained from
        `issm.prepare_data`.

        :param data: Input points and targets
        :param mean: Mean function m(X)
        :param kernel: Kernel function k(X, X')
        :param res_kernel: Kernel function k_r(r, r'), of exponential decay
            type
        :param noise_variance: Innovation and noise variance
        """
        self.res_kernel = res_kernel
        super().__init__(data, mean, kernel, noise_variance=noise_variance, **kwargs)
        assert self.r_min == res_kernel.r_min and self.r_max == res_kernel.r_max, (
            (self.r_min, self.r_max),
            (res_kernel.r_min, res_kernel.r_max),
        )

    @staticmethod
    def has_precomputations(data: dict) -> bool:
        return all(k in data for k in ("ydims", "num_configs", "yflat"))

    def _compute_posterior_state(self, data: dict, noise_variance, **kwargs):
        profiler = kwargs.get("profiler")
        # Compute posterior state
        if profiler is not None:
            profiler.start("likelihood")
        if self.has_precomputations(data):
            issm_likelihood = resource_kernel_likelihood_computations(
                precomputed=data,
                res_kernel=self.res_kernel,
                noise_variance=noise_variance,
            )
        else:
            issm_likelihood = resource_kernel_likelihood_slow_computations(
                targets=data["targets"],
                res_kernel=self.res_kernel,
                noise_variance=noise_variance,
            )
        if profiler is not None:
            profiler.stop("likelihood")
            profiler.start("poster_comp")
        self.poster_state = posterior_computations(
            features=data["features"],
            mean=self.mean,
            kernel=self.kernel,
            issm_likelihood=issm_likelihood,
            noise_variance=noise_variance,
        )
        if profiler is not None:
            profiler.stop("poster_comp")
        # Add missing term to criterion value
        if "criterion" in self.poster_state:
            part3 = logdet_cholfact_cov_resource(issm_likelihood)
            self.poster_state["criterion"] += part3
        # Extra terms required in `sample_curves`
        self.poster_state["lfact_all"] = issm_likelihood["lfact_all"]
        self.poster_state["means_all"] = issm_likelihood["means_all"]
        self.poster_state["noise_variance"] = noise_variance

    def predict(self, test_features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        resource_attr_range = (self.r_min, self.r_max)
        features, resources = decode_extended_features(
            test_features, resource_attr_range=resource_attr_range
        )
        return predict_posterior_marginals_expdecay(
            poster_state=self.poster_state,
            mean=self.mean,
            kernel=self.kernel,
            test_features=features,
            resources=list(resources),
            res_kernel=self.res_kernel,
        )

    @staticmethod
    def data_precomputations(data: dict):
        data.update(resource_kernel_likelihood_precomputations(targets=data["targets"]))

    def _sample_curves_internal(
        self,
        data: dict,
        poster_state: dict,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> List[dict]:
        assert "lfact_all" in poster_state
        if random_state is None:
            random_state = np.random
        lfact_all = poster_state["lfact_all"]
        means_all = poster_state["means_all"]
        noise_variance = poster_state["noise_variance"]
        results = []
        for feature, targets in zip(data["features"], data["targets"]):
            results.append(
                sample_posterior_joint_expdecay(
                    poster_state=poster_state,
                    mean=self.mean,
                    kernel=self.kernel,
                    feature=feature,
                    targets=targets,
                    res_kernel=self.res_kernel,
                    noise_variance=noise_variance,
                    lfact_all=lfact_all,
                    means_all=means_all,
                    random_state=random_state,
                    num_samples=num_samples,
                )
            )
        return results

    def _prepare_update(
        self, feature: np.ndarray, targets: np.ndarray
    ) -> Tuple[float, float, float]:
        issm_likelihood = resource_kernel_likelihood_slow_computations(
            targets=[_colvec(targets, _np=np)],
            res_kernel=self.res_kernel,
            noise_variance=self.noise_variance,
        )
        vtv = issm_likelihood["vtv"].item()
        wtv = issm_likelihood["wtv"].item()
        s_sq = vtv / self.noise_variance
        s_new = np.sqrt(s_sq)
        muhat = _flatvec(self.mean(feature)).item()
        r2_new = wtv / self.noise_variance - s_sq * muhat
        return 0.0, s_new, r2_new

    def update(
        self, feature: np.ndarray, targets: np.ndarray
    ) -> "IncrementalUpdateGPAdditivePosteriorState":
        create_kwargs = self._update_internal(feature, targets)
        # Extra terms required in `sample_curves`
        new_poster_state = create_kwargs["poster_state"]
        for k in ("lfact_all", "means_all", "noise_variance"):
            new_poster_state[k] = self.poster_state[k]
        return GaussProcExpDecayPosteriorState(
            **create_kwargs, res_kernel=self.res_kernel
        )

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/likelihood.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List, Dict, Tuple
import numpy as np
import autograd.numpy as anp
from numpy.random import RandomState

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    INITIAL_NOISE_VARIANCE,
    NOISE_VARIANCE_LOWER_BOUND,
    NOISE_VARIANCE_UPPER_BOUND,
    DEFAULT_ENCODING,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    Gamma,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon import Block
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    register_parameter,
    create_encoding,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
    MeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    PosteriorState,
    GaussProcPosteriorState,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import (
    SimpleProfiler,
)


class MarginalLikelihood(Block):
    """
    Interface for marginal likelihood of Gaussian-linear model.
    """

    def get_posterior_state(self, data: dict) -> PosteriorState:
        raise NotImplementedError

    def forward(self, data: dict):
        return self.get_posterior_state(data).neg_log_likelihood()

    def param_encoding_pairs(self) -> List[tuple]:
        """
        Return a list of tuples with the Gluon parameters of the likelihood
        and their respective encodings
        """
        raise NotImplementedError

    def box_constraints_internal(self) -> Dict[str, Tuple[float, float]]:
        """
        :return: Box constraints for all the underlying parameters
        """
        all_box_constraints = dict()
        for param, encoding in self.param_encoding_pairs():
            assert (
                encoding is not None
            ), "encoding of param {} should not be None".format(param.name)
            all_box_constraints.update(encoding.box_constraints_internal(param))
        return all_box_constraints

    def get_noise_variance(self, as_ndarray=False):
        raise NotImplementedError

    def get_params(self) -> Dict[str, np.ndarray]:
        raise NotImplementedError

    def set_params(self, param_dict: Dict[str, np.ndarray]):
        raise NotImplementedError

    def reset_params(self, random_state: RandomState):
        """
        Reset hyperparameters to their initial values (or resample them).
        """
        # Note: The `init` parameter is a default sampler which is used only
        # for parameters which do not have initializers specified. Right now,
        # all our parameters have such initializers (constant in general),
        # so this is just to be safe (if `init` is not specified here, it
        # defaults to `np.random.uniform`, whose seed we do not control).
        self.initialize(init=random_state.uniform, force_reinit=True)

    def data_precomputations(self, data: dict, overwrite: bool = False):
        """
        Some models require precomputations based on `data`. Precomputed
        variables are appended to `data`. This is done only if not already
        included in `data`, unless `overwrite` is True.

        :param data:
        :param overwrite:
        """
        pass

    def on_fit_start(self, data: dict, profiler: Optional[SimpleProfiler] = None):
        """
        Called at the beginning of `fit`.

        :param data: Argument passed to `fit`
        :param profiler: Argument passed to `fit`

        """
        raise NotImplementedError


class GaussianProcessMarginalLikelihood(MarginalLikelihood):
    """
    Marginal likelihood of Gaussian process with Gaussian likelihood

    :param kernel: Kernel function (for instance, a Matern52---note we cannot
        provide Matern52() as default argument since we need to provide with
        the dimension of points in X)
    :param mean: Mean function which depends on the input X only (by default,
        a scalar fitted while optimizing the likelihood)
    :param initial_noise_variance: A scalar to initialize the value of the
        residual noise variance
    """

    def __init__(
        self,
        kernel: KernelFunction,
        mean: MeanFunction = None,
        initial_noise_variance=None,
        encoding_type=None,
        **kwargs,
    ):
        super(GaussianProcessMarginalLikelihood, self).__init__(**kwargs)
        if mean is None:
            mean = ScalarMeanFunction()
        if initial_noise_variance is None:
            initial_noise_variance = INITIAL_NOISE_VARIANCE
        if encoding_type is None:
            encoding_type = DEFAULT_ENCODING
        self.encoding_noise = create_encoding(
            encoding_name=encoding_type,
            init_val=initial_noise_variance,
            constr_lower=NOISE_VARIANCE_LOWER_BOUND,
            constr_upper=NOISE_VARIANCE_UPPER_BOUND,
            dimension=1,
            prior=Gamma(mean=0.1, alpha=0.1),
        )
        self.mean = mean
        self.kernel = kernel
        with self.name_scope():
            self.noise_variance_internal = register_parameter(
                self.params, "noise_variance", self.encoding_noise
            )

    def _noise_variance(self):
        return encode_unwrap_parameter(
            self.noise_variance_internal, self.encoding_noise
        )

    @staticmethod
    def _assert_data_entries(data: dict):
        features = data.get("features")
        targets = data.get("targets")
        assert (
            features is not None and targets is not None
        ), "data must contain 'features' and 'targets'"
        assert features.ndim == 2, f"features.shape = {features.shape}, must be matrix"
        if targets.ndim == 1:
            targets = targets.reshape((-1, 1))
            data["targets"] = targets
        assert features.shape[0] == targets.shape[0], (
            f"features and targets should have the same number of points "
            + f"(received {features.shape[0]} and {targets.shape[0]})"
        )

    def get_posterior_state(self, data: dict) -> PosteriorState:
        self._assert_data_entries(data)
        return GaussProcPosteriorState(
            features=data["features"],
            targets=data["targets"],
            mean=self.mean,
            kernel=self.kernel,
            noise_variance=self._noise_variance(),
        )

    def forward(self, data: dict):
        """
        Actual computation of the marginal likelihood
        See http://www.gaussianprocess.org/gpml/chapters/RW.pdf, equation (2.30)

        :param features: input data matrix X of size (n, d)
        :param targets: targets corresponding to X, of size (n, 1)
        """
        return self.get_posterior_state(data).neg_log_likelihood()

    def param_encoding_pairs(self) -> List[tuple]:
        own_param_encoding_pairs = [(self.noise_variance_internal, self.encoding_noise)]
        return (
            own_param_encoding_pairs
            + self.mean.param_encoding_pairs()
            + self.kernel.param_encoding_pairs()
        )

    def get_noise_variance(self, as_ndarray=False):
        noise_variance = self._noise_variance()
        return noise_variance if as_ndarray else anp.reshape(noise_variance, (1,))[0]

    def _set_noise_variance(self, val: float):
        self.encoding_noise.set(self.noise_variance_internal, val)

    def get_params(self) -> Dict[str, np.ndarray]:
        result = {"noise_variance": self.get_noise_variance()}
        for pref, func in [("kernel_", self.kernel), ("mean_", self.mean)]:
            result.update({(pref + k): v for k, v in func.get_params().items()})
        return result

    def set_params(self, param_dict: Dict[str, np.ndarray]):
        for pref, func in [("kernel_", self.kernel), ("mean_", self.mean)]:
            len_pref = len(pref)
            stripped_dict = {
                k[len_pref:]: v for k, v in param_dict.items() if k.startswith(pref)
            }
            func.set_params(stripped_dict)
        self._set_noise_variance(param_dict["noise_variance"])

    def on_fit_start(self, data: dict, profiler: Optional[SimpleProfiler] = None):
        self._assert_data_entries(data)
        targets = data["targets"]
        assert (
            targets.shape[1] == 1
        ), "targets cannot be a matrix if parameters are to be fit"
        if isinstance(self.mean, ScalarMeanFunction):
            self.mean.set_mean_value(anp.mean(targets))

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/mean.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp
from autograd.tracer import getval

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    INITIAL_MEAN_VALUE,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    Normal,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon import Block
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    IdentityScalarEncoding,
    encode_unwrap_parameter,
    register_parameter,
)

__all__ = ["MeanFunction", "ScalarMeanFunction", "ZeroMeanFunction"]


class MeanFunction(Block):
    """
    Mean function, parameterizing a surrogate model together with a kernel function.

    Note: KernelFunction also inherits from this interface.
    """

    def __init__(self, **kwargs):
        Block.__init__(self, **kwargs)

    def param_encoding_pairs(self):
        """
        Returns list of tuples
            (param_internal, encoding)
        over all Gluon parameters maintained here.

        :return: List [(param_internal, encoding)]
        """
        raise NotImplementedError

    def get_params(self):
        """
        :return: Dictionary with hyperparameter values
        """
        raise NotImplementedError

    def set_params(self, param_dict):
        """
        :param param_dict: Dictionary with new hyperparameter values
        :return:
        """
        raise NotImplementedError


class ScalarMeanFunction(MeanFunction):
    """
    Mean function defined as a scalar (fitted while optimizing the marginal
    likelihood).

    :param initial_mean_value: A scalar to initialize the value of the mean
    """

    def __init__(self, initial_mean_value=INITIAL_MEAN_VALUE, **kwargs):
        super().__init__(**kwargs)

        # Even though we do not apply specific transformation to the mean value
        # we use an encoding to handle in a consistent way the box constraints
        # of Gluon parameters (like bandwidths or residual noise variance)

        self.encoding = IdentityScalarEncoding(
            init_val=initial_mean_value, regularizer=Normal(0.0, 1.0)
        )

        with self.name_scope():
            self.mean_value_internal = register_parameter(
                self.params, "mean_value", self.encoding
            )

    def forward(self, X):
        """
        Actual computation of the scalar mean function
        We compute mean_value * vector_of_ones, whose dimensions are given by
        the the first column of X

        :param X: input data of size (n,d) for which we want to compute the
            mean (here, only useful to extract the right dimension)
        """
        mean_value = encode_unwrap_parameter(self.mean_value_internal, self.encoding)
        return anp.multiply(anp.ones((getval(X.shape[0]), 1)), mean_value)

    def param_encoding_pairs(self):
        return [(self.mean_value_internal, self.encoding)]

    def get_mean_value(self):
        return encode_unwrap_parameter(self.mean_value_internal, self.encoding)[0]

    def set_mean_value(self, mean_value):
        self.encoding.set(self.mean_value_internal, mean_value)

    def get_params(self):
        return {"mean_value": self.get_mean_value()}

    def set_params(self, param_dict):
        self.set_mean_value(param_dict["mean_value"])


class ZeroMeanFunction(MeanFunction):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def forward(self, X):
        return anp.zeros((getval(X.shape[0]), 1))

    def param_encoding_pairs(self):
        return []

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/optimization_utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
from scipy import optimize
from autograd import value_and_grad
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon import (
    Parameter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    param_to_pretty_string,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    MarginalLikelihood,
)

logger = logging.getLogger(__name__)

__all__ = [
    "apply_lbfgs",
    "apply_lbfgs_with_multiple_starts",
    "add_regularizer_to_criterion",
    "create_lbfgs_arguments",
]


default_LBFGS_tol, default_LBFGS_maxiter = 1e-5, 500
N_STARTS = 5
STARTING_POINT_RANDOMIZATION_STD = 1.0


class ParamVecDictConverter:
    def __init__(self, param_dict: dict):
        self.param_dict = param_dict
        self.names = sorted(
            [name for name, value in param_dict.items() if value is not None]
        )
        self.shapes = []
        self.name_to_index = dict()
        pos = 0
        for name in self.names:
            shape = param_dict[name].data().shape
            self.shapes.append(shape)
            size = sum(shape)
            self.name_to_index[name] = np.arange(pos, pos + size)
            pos += size

    def from_vec(self, param_vec: np.ndarray):
        pos = 0
        for name, shape in zip(self.names, self.shapes):
            size = sum(shape)
            self.param_dict[name].set_data(
                np.reshape(param_vec[pos : (pos + size)], shape)
            )
            pos += size

    def to_vec(self):
        param_arrays = [self.param_dict[name].data() for name in self.names]
        return np.concatenate([np.reshape(x, (-1,)) for x in param_arrays])


def _make_scipy_objective(autograd_func):
    """
    Maps autograd expression into objective (criterion and gradient) for SciPy
    optimizer. The input to autograd_func is a flat param_vec.

    :param autograd_func: Autograd expression
    :return: SciPy optimizer objective
    """
    return value_and_grad(lambda x: autograd_func(x))


def _apply_lbfgs_internal(
    exec_func,
    param_converter: ParamVecDictConverter,
    param_numpy_array,
    param_bounds,
    **kwargs
):
    # Run L-BFGS-B
    LBFGS_tol = kwargs.get("tol", default_LBFGS_tol)
    LBFGS_maxiter = kwargs.get("maxiter", default_LBFGS_maxiter)
    LBFGS_callback = kwargs.get("callback", None)
    ret_info = None

    try:
        output = optimize.minimize(
            exec_func,
            param_numpy_array,
            jac=True,
            method="L-BFGS-B",
            bounds=param_bounds,
            tol=LBFGS_tol,
            options={"maxiter": LBFGS_maxiter},
            callback=LBFGS_callback,
        )
        # NOTE: Be aware that the stopping condition based on tol can terminate
        # with a gradient size which is not small.
        # To control L-BFGS convergence conditions for real, have to instead use
        # something like this:
        #                           tol=None,
        #                           options={
        #                               "maxiter": LBFGS_maxiter,
        #                               "ftol": 1e-6,
        #                               "gtol": 1e-1},
        #                           callback=LBFGS_callback)

        # Write result evaluation point back to param_dict
        param_converter.from_vec(output.x)

    except Exception as inst:
        ret_info = {"type": type(inst), "args": inst.args, "msg": str(inst)}

    return ret_info


class ExecutorDecorator:
    """
    This class is a lightweight decorator around the executor passed to L-BFGS
    It adds the functionality of keeping track of the best objective function
    """

    def __init__(self, exec_func):
        self.best_objective = np.inf
        self._exec_func = exec_func

    def exec_func(self, param_vec):
        objective, gradient = self._exec_func(param_vec)
        self.best_objective = min(self.best_objective, objective)
        return objective, gradient


def _deep_copy_param_dict(input_param_dict):
    """
    Make a deep copy of the input param_dict
    :param input_param_dict:
    :return: deep copy of input_param_dict
    """
    output_param_dict = {}
    for name, param in input_param_dict.items():
        param_copy = Parameter(name=param.name, shape=param.shape, init=param.init)
        param_copy.initialize()
        param_copy.set_data(param.data())
        output_param_dict[name] = param_copy
    return output_param_dict


def _inplace_param_dict_randomization(
    param_dict,
    mean_param_dict,
    bounds,
    random_state,
    std=STARTING_POINT_RANDOMIZATION_STD,
):
    """
    In order to initialize L-BFGS from multiple starting points, this function makes it possible to
    randomize, inplace, an param_dict (as used by executors to communicate parameters to L-BFGS).
    The randomization is centered around mean_param_dict, with standard deviation std.

    :param param_dict: dict param_name to np.ndarray (as used in executors). This argument is modified inplace
    :param mean_param_dict: param_dict around which the random perturbations occur (dict param_name to np.ndarray, as used in executors))
    :param bounds: dict param_name to (lower, upper) bounds, as used in L-BFGS
    :param std: standard deviation according to which the (Gaussian) random perturbations happen
    """
    # We check that param_dict and mean_param_dict are compatible
    assert param_dict.keys() == mean_param_dict.keys()
    for name, param in param_dict.items():
        assert param.shape == mean_param_dict[name].shape
        assert param.dtype == mean_param_dict[name].dtype

    # We apply a sort to make the for loop deterministic (especially with the internal calls to np.random)
    for name, param in sorted(param_dict.items()):

        lower, upper = bounds[name]
        lower = lower if lower is not None else -np.inf
        upper = upper if upper is not None else np.inf

        param_value_new = mean_param_dict[name].data() + random_state.normal(
            0.0, std, size=param.shape
        )
        # We project back param_dict[name] within its specified lower and upper bounds
        # (in case of we would have perturbed beyond those bounds)
        param_dict[name].set_data(np.maximum(lower, np.minimum(upper, param_value_new)))


def apply_lbfgs(exec_func, param_dict, bounds, **kwargs):
    """Run SciPy L-BFGS-B on criterion given by autograd code

    Run SciPy L-BFGS-B in order to minimize criterion given by autograd code.
    Criterion and gradient are computed by:

        crit_val, gradient = exec_func(param_vec)

    Given an autograd expression, use make_scipy_objective to obtain exec_func.
    param_vec must correspond to the parameter dictionary param_dict via
    ParamVecDictConverter. The initial param_vec is taken from param_dict,
    and final values are written back to param_dict (conversions are done
    by ParamVecDictConverter).

    L-BFGS-B allows box constraints [a, b] for any coordinate. Here, None
    stands for -infinity (a) or +infinity (b). The default is (None, None),
    so no constraints. In bounds, box constraints can be specified per
    argument (the constraint applies to all coordinates of the argument).
    Pass {} for no constraints.

    :param exec_func: Function to compute criterion and gradient
    :param param_dict: See above
    :param bounds: See above
    :return: None, or dict with info about exception caught
    """
    param_converter = ParamVecDictConverter(param_dict)
    # Initial evaluation point
    param_numpy_array = param_converter.to_vec()

    # Define bounds for L-BFGS, None by default
    param_bounds = np.array([(None, None)] * len(param_numpy_array))
    name_to_index = param_converter.name_to_index
    param_names = set(param_converter.names)
    for name, bound in bounds.items():
        if name in param_names:
            param_bounds[name_to_index[name]] = bound

    ret_info = _apply_lbfgs_internal(
        exec_func, param_converter, param_numpy_array, param_bounds, **kwargs
    )
    if ret_info is not None:
        # Caught exception: Return parameters for which evaluation failed
        ret_info["params"] = {k: v.data() for k, v in param_dict.items()}
        # Restore initial evaluation point
        param_converter.from_vec(param_numpy_array)
    return ret_info


def apply_lbfgs_with_multiple_starts(
    exec_func, param_dict, bounds, random_state, n_starts=N_STARTS, **kwargs
):
    """
    When dealing with non-convex problems (e.g., optimization the marginal
    likelihood), we typically need to start from various starting points. This
    function applies this logic around apply_lbfgs, randomizing the starting
    points around the initial values provided in param_dict (see below
    "copy_of_initial_param_dict").

    The first optimization happens exactly at param_dict, so that the case
    n_starts=1 exactly coincides with the previously used apply_lbfgs.
    Importantly, the communication with the L-BFGS solver happens via param_dict,
    hence all the operations with respect to param_dict are inplace.

    We catch exceptions and return ret_infos about these. If none of the
    restarts worked, param_dict is not modified.

    :param exec_func: see above
    :param param_dict: see above
    :param bounds: see above
    :param random_state: RandomState for sampling
    :param n_starts: Number of times we start an optimization with L-BFGS
        (must be >= 1)
    :return: List ret_infos of length n_starts. Entry is None if optimization
        worked, or otherwise has dict with info about exception caught
    """
    assert n_starts >= 1

    copy_of_initial_param_dict = _deep_copy_param_dict(param_dict)
    best_objective_over_restarts = None
    best_param_dict_over_restarts = copy_of_initial_param_dict

    # Loop over restarts
    ret_infos = []
    for iter in range(n_starts):
        if iter > 0:
            _inplace_param_dict_randomization(
                param_dict, copy_of_initial_param_dict, bounds, random_state
            )

        decorator = ExecutorDecorator(exec_func)
        ret_info = apply_lbfgs(decorator.exec_func, param_dict, bounds, **kwargs)

        ret_infos.append(ret_info)
        if ret_info is None and (
            best_objective_over_restarts is None
            or decorator.best_objective < best_objective_over_restarts
        ):
            best_objective_over_restarts = decorator.best_objective
            best_param_dict_over_restarts = _deep_copy_param_dict(param_dict)

    # We copy back the values of the best parameters into param_dict (again,
    # inplace, as required by the executor)
    for name in param_dict.keys():
        param_dict[name].set_data(best_param_dict_over_restarts[name].data())
    return ret_infos


def add_regularizer_to_criterion(criterion: MarginalLikelihood, crit_args: list):
    objective_nd = criterion(*crit_args)
    # Add neg log hyperpriors, whenever some are defined
    for param_int, encoding in criterion.param_encoding_pairs():
        if encoding.regularizer is not None:
            param = encode_unwrap_parameter(param_int, encoding)
            objective_nd = objective_nd + encoding.regularizer(param)
    return objective_nd


def create_lbfgs_arguments(
    criterion: MarginalLikelihood, crit_args: list, verbose: bool = False
):
    """
    Creates SciPy optimizer objective and param_dict for criterion
    function.

    :param criterion: Learning criterion (nullary)
    :param crit_args: Arguments for criterion.forward
    :return: scipy_objective, param_dict
    """
    param_dict = {param.name: param for param in criterion.collect_params().values()}
    # Wraps param_dict, conversion to/from flat vector:
    param_converter = ParamVecDictConverter(param_dict)

    def executor(param_vec):
        param_converter.from_vec(param_vec)  # Assign param_dict
        objective = add_regularizer_to_criterion(criterion, crit_args)
        if verbose:
            msg_lst = ["[criterion = {}]".format(objective)]
            for param, encoding in criterion.param_encoding_pairs():
                msg_lst.append(param_to_pretty_string(param, encoding))
            logger.info("\n".join(msg_lst))
        return objective

    return _make_scipy_objective(executor), param_dict

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/posterior_state.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import autograd.numpy as anp
from autograd import grad
from autograd.tracer import getval
from typing import Tuple, Optional, Dict, Callable
from numpy.random import RandomState

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_utils import (
    cholesky_computations,
    predict_posterior_marginals,
    sample_posterior_marginals,
    sample_posterior_joint,
    cholesky_update,
    negative_log_marginal_likelihood,
    sample_and_cholesky_update,
    KernelFunctionWithCovarianceScale,
)


class PosteriorState:
    """
    Interface for posterior state of Gaussian-linear model.
    """

    @property
    def num_data(self):
        raise NotImplementedError

    @property
    def num_features(self):
        raise NotImplementedError

    @property
    def num_fantasies(self):
        raise NotImplementedError

    def neg_log_likelihood(self) -> anp.ndarray:
        """
        :return: Negative log marginal likelihood
        """
        raise NotImplementedError

    def predict(self, test_features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Computes marginal statistics (means, variances) for a number of test
        features.

        :param test_features: Features for test configs
        :return: posterior_means, posterior_variances
        """
        raise NotImplementedError

    def sample_marginals(
        self,
        test_features: np.ndarray,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> np.ndarray:
        """
        See comments of `predict`.

        :param test_features: Input points for test configs
        :param num_samples: Number of samples
        :param random_state: PRNG
        :return: Marginal samples, (num_test, num_samples)
        """
        raise NotImplementedError

    def backward_gradient(
        self,
        input: np.ndarray,
        head_gradients: Dict[str, np.ndarray],
        mean_data: float,
        std_data: float,
    ) -> np.ndarray:
        """
        Implements SurrogateModel.backward_gradient, see comments there.
        This is for a single posterior state. If the SurrogateModel uses
        MCMC, have to call this for every sample.

        :param input: Single input point x, shape (d,)
        :param head_gradients: See SurrogateModel.backward_gradient
        :param mean_data: Mean used to normalize targets
        :param std_data: Stddev used to normalize targets
        :return:
        """
        raise NotImplementedError


class PosteriorStateWithSampleJoint(PosteriorState):
    def sample_joint(
        self,
        test_features: np.ndarray,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> np.ndarray:
        """
        See comments of `predict`.

        :param test_features: Input points for test configs
        :param num_samples: Number of samples
        :param random_state: PRNG
        :return: Joint samples, (num_test, num_samples)
        """
        raise NotImplementedError


class GaussProcPosteriorState(PosteriorStateWithSampleJoint):
    """
    Represent posterior state for Gaussian process regression model.
    Note that members are immutable. If the posterior state is to be
    updated, a new object is created and returned.
    """

    def __init__(
        self,
        features: np.ndarray,
        targets: Optional[np.ndarray],
        mean: MeanFunction,
        kernel: KernelFunctionWithCovarianceScale,
        noise_variance: np.ndarray,
        debug_log: bool = False,
        **kwargs
    ):
        """
        If targets has m > 1 columns, they correspond to fantasy samples.

        If targets is None, this is an internal (copy) constructor, where
        kwargs contains chol_fact, pred_mat.

        `kernel` can be a tuple `(_kernel, covariance_scale)`, where
        `_kernel` is a `KernelFunction`, `covariance_scale` a scalar
        parameter. In this case, the kernel function is their product.

        :param features: Input points X, shape (n, d)
        :param targets: Targets Y, shape (n, m)
        :param mean: Mean function m(X)
        :param kernel: Kernel function k(X, X'), or tuple (see above)
        :param noise_variance: Noise variance sigsq, shape (1,)
        """
        self.mean = mean
        self.kernel = self._check_and_assign_kernel(kernel)
        self.noise_variance = anp.array(noise_variance, copy=True)
        if targets is not None:
            targets_shape = getval(targets.shape)
            targets = anp.reshape(targets, (targets_shape[0], -1))
            self.chol_fact, self.pred_mat = cholesky_computations(
                features=features,
                targets=targets,
                mean=mean,
                kernel=kernel,
                noise_variance=noise_variance,
                debug_log=debug_log,
            )
            self.features = anp.array(features, copy=True)
        else:
            # Internal (copy) constructor
            self.features = features
            self.chol_fact = kwargs["chol_fact"]
            self.pred_mat = kwargs["pred_mat"]

    @staticmethod
    def _check_and_assign_kernel(kernel: KernelFunctionWithCovarianceScale):
        if isinstance(kernel, tuple):
            assert len(kernel) == 2
            kernel, covariance_scale = kernel
            assert isinstance(kernel, KernelFunction)
            return kernel, anp.array(covariance_scale, copy=True)
        else:
            assert isinstance(kernel, KernelFunction)
            return kernel

    @property
    def num_data(self):
        return self.features.shape[0]

    @property
    def num_features(self):
        return self.features.shape[1]

    @property
    def num_fantasies(self):
        return self.pred_mat.shape[1]

    def _state_kwargs(self) -> dict:
        return {
            "features": self.features,
            "mean": self.mean,
            "kernel": self.kernel,
            "chol_fact": self.chol_fact,
            "pred_mat": self.pred_mat,
        }

    def neg_log_likelihood(self) -> anp.ndarray:
        """
        Works only if fantasy samples are not used (single targets vector).
        """
        critval = negative_log_marginal_likelihood(self.chol_fact, self.pred_mat)
        return critval

    def predict(self, test_features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        return predict_posterior_marginals(
            **self._state_kwargs(), test_features=test_features
        )

    def sample_marginals(
        self,
        test_features: np.ndarray,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> np.ndarray:
        if random_state is None:
            random_state = np.random
        return sample_posterior_marginals(
            **self._state_kwargs(),
            test_features=test_features,
            random_state=random_state,
            num_samples=num_samples
        )

    def backward_gradient(
        self,
        input: np.ndarray,
        head_gradients: Dict[str, np.ndarray],
        mean_data: float,
        std_data: float,
    ) -> np.ndarray:
        """
        Implements SurrogateModel.backward_gradient, see comments there.
        This is for a single posterior state. If the SurrogateModel uses
        MCMC, have to call this for every sample.

        The posterior represented here is based on normalized data, while
        the acquisition function is based on the de-normalized predictive
        distribution, which is why we need 'mean_data', 'std_data' here.

        :param input: Single input point x, shape (d,)
        :param head_gradients: See SurrogateModel.backward_gradient
        :param mean_data: Mean used to normalize targets
        :param std_data: Stddev used to normalize targets
        :return:
        """

        def predict_func(test_feature_array):
            return self.predict(test_feature_array)

        return backward_gradient_given_predict(
            predict_func=predict_func,
            input=input,
            head_gradients=head_gradients,
            mean_data=mean_data,
            std_data=std_data,
        )

    def sample_joint(
        self,
        test_features: np.ndarray,
        num_samples: int = 1,
        random_state: Optional[RandomState] = None,
    ) -> np.ndarray:
        if random_state is None:
            random_state = np.random
        return sample_posterior_joint(
            **self._state_kwargs(),
            test_features=test_features,
            random_state=random_state,
            num_samples=num_samples
        )


def backward_gradient_given_predict(
    predict_func: Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray]],
    input: np.ndarray,
    head_gradients: Dict[str, np.ndarray],
    mean_data: float,
    std_data: float,
) -> np.ndarray:
    """
    Implements SurrogateModel.backward_gradient, see comments there.
    This is for a single posterior state. If the SurrogateModel uses
    MCMC, have to call this for every sample.

    The posterior represented here is based on normalized data, while
    the acquisition function is based on the de-normalized predictive
    distribution, which is why we need 'mean_data', 'std_data' here.

    :param predict_func: Function mapping input x to mean, variance
    :param input: Single input point x, shape (d,)
    :param head_gradients: See SurrogateModel.backward_gradient
    :param mean_data: Mean used to normalize targets
    :param std_data: Stddev used to normalize targets
    :return:
    """
    test_feature = np.reshape(input, (1, -1))
    assert "mean" in head_gradients, "Need head_gradients['mean'] for backward_gradient"
    has_std = "std" in head_gradients

    def diff_test_feature(test_feature_array):
        norm_mean, norm_variance = predict_func(test_feature_array)
        # De-normalize, and variance -> stddev
        pred_mean = norm_mean * std_data + mean_data
        head_gradients_mean = anp.reshape(head_gradients["mean"], pred_mean.shape)
        # Added to mimic mxnet.autograd.backward
        pred_mean_sum = anp.sum(anp.multiply(pred_mean, head_gradients_mean))
        if has_std:
            pred_std = anp.sqrt(norm_variance) * std_data
            head_gradients_std = anp.reshape(head_gradients["std"], pred_std.shape)
            pred_std_sum = anp.sum(anp.multiply(pred_std, head_gradients_std))
        else:
            pred_std_sum = 0.0
        return pred_mean_sum + pred_std_sum

    test_feature_gradient = grad(diff_test_feature)
    return np.reshape(test_feature_gradient(test_feature), input.shape)


class IncrementalUpdateGPPosteriorState(GaussProcPosteriorState):
    """
    Extension of GaussProcPosteriorState which allows for incremental
    updating, given that a single data case is appended to the training
    set.

    In order to not mutate members,
    "the update method returns a new object."
    """

    def __init__(
        self,
        features: np.ndarray,
        targets: Optional[np.ndarray],
        mean: MeanFunction,
        kernel: KernelFunctionWithCovarianceScale,
        noise_variance: np.ndarray,
        **kwargs
    ):
        super(IncrementalUpdateGPPosteriorState, self).__init__(
            features, targets, mean, kernel, noise_variance, **kwargs
        )

    def update(
        self, feature: np.ndarray, target: np.ndarray
    ) -> "IncrementalUpdateGPPosteriorState":
        """
        :param feature: Additional input xstar, shape (1, d)
        :param target: Additional target ystar, shape (1, m)
        :return: Posterior state for increased data set
        """
        feature = anp.reshape(feature, (1, -1))
        target = anp.reshape(target, (1, -1))
        assert (
            feature.shape[1] == self.features.shape[1]
        ), "feature.shape[1] = {} != {} = self.features.shape[1]".format(
            feature.shape[1], self.features.shape[1]
        )
        assert (
            target.shape[1] == self.pred_mat.shape[1]
        ), "target.shape[1] = {} != {} = self.pred_mat.shape[1]".format(
            target.shape[1], self.pred_mat.shape[1]
        )
        chol_fact_new, pred_mat_new = cholesky_update(
            **self._state_kwargs(),
            noise_variance=self.noise_variance,
            feature=feature,
            target=target
        )
        features_new = anp.concatenate([self.features, feature], axis=0)
        state_new = IncrementalUpdateGPPosteriorState(
            features=features_new,
            targets=None,
            mean=self.mean,
            kernel=self.kernel,
            noise_variance=self.noise_variance,
            chol_fact=chol_fact_new,
            pred_mat=pred_mat_new,
        )
        return state_new

    def sample_and_update(
        self,
        feature: np.ndarray,
        mean_impute_mask=None,
        random_state: Optional[RandomState] = None,
    ) -> (np.ndarray, "IncrementalUpdateGPPosteriorState"):
        """
        Draw target(s), shape (1, m), from current posterior state, then update
        state based on these. The main computation of lvec is shared among the
        two.
        If mean_impute_mask is given, it is a boolean vector of size m (number
        columns of pred_mat). Columns j of target, where mean_impute_ mask[j]
        is true, are set to the predictive mean (instead of being sampled).

        :param feature: Additional input xstar, shape (1, d)
        :param mean_impute_mask: See above
        :param random_state: PRN generator
        :return: target, poster_state_new
        """
        feature = anp.reshape(feature, (1, -1))
        assert (
            feature.shape[1] == self.features.shape[1]
        ), "feature.shape[1] = {} != {} = self.features.shape[1]".format(
            feature.shape[1], self.features.shape[1]
        )
        if random_state is None:
            random_state = np.random
        chol_fact_new, pred_mat_new, features_new, target = sample_and_cholesky_update(
            **self._state_kwargs(),
            noise_variance=self.noise_variance,
            feature=feature,
            random_state=random_state,
            mean_impute_mask=mean_impute_mask
        )
        state_new = IncrementalUpdateGPPosteriorState(
            features=features_new,
            targets=None,
            mean=self.mean,
            kernel=self.kernel,
            noise_variance=self.noise_variance,
            chol_fact=chol_fact_new,
            pred_mat=pred_mat_new,
        )
        return target, state_new

    def expand_fantasies(
        self, num_fantasies: int
    ) -> "IncrementalUpdateGPPosteriorState":
        """
        If this posterior has been created with a single targets vector,
        shape (n, 1), use this to duplicate this vector m = num_fantasies
        times. Call this method before fantasy targets are appended by
        update.

        :param num_fantasies: Number m of fantasy samples
        :return: New state with targets duplicated m times
        """
        assert num_fantasies > 1
        assert (
            self.pred_mat.shape[1] == 1
        ), "Method requires posterior without fantasy samples"
        pred_mat_new = anp.concatenate(([self.pred_mat] * num_fantasies), axis=1)
        return IncrementalUpdateGPPosteriorState(
            features=self.features,
            targets=None,
            mean=self.mean,
            kernel=self.kernel,
            noise_variance=self.noise_variance,
            chol_fact=self.chol_fact,
            pred_mat=pred_mat_new,
        )

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/posterior_utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Tuple, Union
import autograd.numpy as anp
import autograd.scipy.linalg as aspl
import numpy as np
from autograd.builtins import isinstance
from autograd.tracer import getval
from numpy.random import RandomState

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    NOISE_VARIANCE_LOWER_BOUND,
    MIN_POSTERIOR_VARIANCE,
    MIN_CHOLESKY_DIAGONAL_VALUE,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.custom_op import (
    AddJitterOp,
    cholesky_factorization,
    flatten_and_concat,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)


KernelFunctionWithCovarianceScale = Union[
    KernelFunction, Tuple[KernelFunction, np.ndarray]
]


def _extract_kernel_and_scale(kernel: KernelFunctionWithCovarianceScale):
    if isinstance(kernel, tuple):
        return kernel[0], anp.reshape(kernel[1], (1, 1))
    else:
        return kernel, 1.0


def cholesky_computations(
    features,
    targets,
    mean: MeanFunction,
    kernel: KernelFunctionWithCovarianceScale,
    noise_variance,
    debug_log: bool = False,
):
    """
    Given input matrix X (features), target matrix Y (targets), mean and kernel
    function, compute posterior state {L, P}, where L is the Cholesky factor
    of
        k(X, X) + sigsq_final * I
    and
        L P = Y - mean(X)
    Here, sigsq_final >= noise_variance is minimal such that the Cholesky
    factorization does not fail.

    :param features: Input matrix X (n, d)
    :param targets: Target matrix Y (n, m)
    :param mean: Mean function
    :param kernel: Kernel function, or tuple
    :param noise_variance: Noise variance (may be increased)
    :param debug_log: Debug output during add_jitter CustomOp?
    :return: L, P
    """
    _kernel, covariance_scale = _extract_kernel_and_scale(kernel)
    kernel_mat = _kernel(features, features) * covariance_scale
    # Add jitter to noise_variance (if needed) in order to guarantee that
    # Cholesky factorization works
    sys_mat = AddJitterOp(
        flatten_and_concat(kernel_mat, noise_variance),
        initial_jitter_factor=NOISE_VARIANCE_LOWER_BOUND,
        debug_log="true" if debug_log else "false",
    )
    chol_fact = cholesky_factorization(sys_mat)
    centered_y = targets - anp.reshape(mean(features), (-1, 1))
    pred_mat = aspl.solve_triangular(chol_fact, centered_y, lower=True)
    return chol_fact, pred_mat


def predict_posterior_marginals(
    features,
    mean: MeanFunction,
    kernel: KernelFunctionWithCovarianceScale,
    chol_fact,
    pred_mat,
    test_features,
):
    """
    Computes posterior means and variances for test_features.
    If pred_mat is a matrix, so will be posterior_means, but not
    posterior_variances. Reflects the fact that for GP regression and fixed
    hyperparameters, the posterior mean depends on the targets y, but the
    posterior covariance does not.

    :param features: Training inputs
    :param mean: Mean function
    :param kernel: Kernel function, or tuple
    :param chol_fact: Part L of posterior state
    :param pred_mat: Part P of posterior state
    :param test_features: Test inputs
    :return: posterior_means, posterior_variances
    """
    _kernel, covariance_scale = _extract_kernel_and_scale(kernel)
    k_tr_te = _kernel(features, test_features) * covariance_scale
    linv_k_tr_te = aspl.solve_triangular(chol_fact, k_tr_te, lower=True)
    posterior_means = anp.matmul(anp.transpose(linv_k_tr_te), pred_mat) + anp.reshape(
        mean(test_features), (-1, 1)
    )
    posterior_variances = _kernel.diagonal(test_features) * covariance_scale - anp.sum(
        anp.square(linv_k_tr_te), axis=0
    )
    return posterior_means, anp.reshape(
        anp.maximum(posterior_variances, MIN_POSTERIOR_VARIANCE), (-1,)
    )


def sample_posterior_marginals(
    features,
    mean: MeanFunction,
    kernel: KernelFunctionWithCovarianceScale,
    chol_fact,
    pred_mat,
    test_features,
    random_state: RandomState,
    num_samples: int = 1,
):
    """
    Draws num_sample samples from the product of marginals of the posterior
    over input points test_features. If pred_mat is a matrix with m columns,
    the samples returned have shape (n_test, m, num_samples).

    :param features: Training inputs
    :param mean: Mean function
    :param kernel: Kernel function, or tuple
    :param chol_fact: Part L of posterior state
    :param pred_mat: Part P of posterior state
    :param test_features: Test inputs
    :param num_samples: Number of samples to draw
    :return: Samples, shape (n_test, num_samples) or (n_test, m, num_samples)
    """
    post_means, post_vars = predict_posterior_marginals(
        features, mean, kernel, chol_fact, pred_mat, test_features
    )
    post_means = anp.expand_dims(post_means, axis=-1)  # (n_test, m, 1)
    post_stds = anp.sqrt(anp.reshape(post_vars, (-1, 1, 1)))  # (n_test, 1, 1)
    n01_vecs = [
        random_state.normal(size=getval(post_means.shape)) for _ in range(num_samples)
    ]
    n01_mat = anp.concatenate(n01_vecs, axis=-1)
    samples = anp.multiply(n01_mat, post_stds) + post_means

    if samples.shape[1] == 1:
        n_test = getval(samples.shape)[0]
        samples = anp.reshape(samples, (n_test, -1))  # (n_test, num_samples)

    return samples


def sample_posterior_joint(
    features,
    mean: MeanFunction,
    kernel: KernelFunctionWithCovarianceScale,
    chol_fact,
    pred_mat,
    test_features,
    random_state: RandomState,
    num_samples: int = 1,
):
    """
    Draws num_sample samples from joint posterior distribution over inputs
    test_features. This is done by computing mean and covariance matrix of
    this posterior, and using the Cholesky decomposition of the latter. If
    pred_mat is a matrix with m columns, the samples returned have shape
    (n_test, m, num_samples).

    :param features: Training inputs
    :param mean: Mean function
    :param kernel: Kernel function, or tuple
    :param chol_fact: Part L of posterior state
    :param pred_mat: Part P of posterior state
    :param test_features: Test inputs
    :param num_samples: Number of samples to draw
    :return: Samples, shape (n_test, num_samples) or (n_test, m, num_samples)
    """
    _kernel, covariance_scale = _extract_kernel_and_scale(kernel)
    k_tr_te = _kernel(features, test_features) * covariance_scale
    linv_k_tr_te = aspl.solve_triangular(chol_fact, k_tr_te, lower=True)
    posterior_mean = anp.matmul(anp.transpose(linv_k_tr_te), pred_mat) + anp.reshape(
        mean(test_features), (-1, 1)
    )
    posterior_cov = _kernel(test_features, test_features) * covariance_scale - anp.dot(
        anp.transpose(linv_k_tr_te), linv_k_tr_te
    )
    jitter_init = anp.ones((1,)) * (1e-5)
    sys_mat = AddJitterOp(
        flatten_and_concat(posterior_cov, jitter_init),
        initial_jitter_factor=NOISE_VARIANCE_LOWER_BOUND,
    )
    lfact = cholesky_factorization(sys_mat)
    # Draw samples
    # posterior_mean.shape = (n_test, m), where m is number of cols of pred_mat
    # Reshape to (n_test, m, 1)
    n_test = getval(posterior_mean.shape)[0]
    posterior_mean = anp.expand_dims(posterior_mean, axis=-1)
    n01_vecs = [
        random_state.normal(size=getval(posterior_mean.shape))
        for _ in range(num_samples)
    ]
    n01_mat = anp.reshape(anp.concatenate(n01_vecs, axis=-1), (n_test, -1))
    samples = anp.reshape(anp.dot(lfact, n01_mat), (n_test, -1, num_samples))
    samples = samples + posterior_mean

    if samples.shape[1] == 1:
        samples = anp.reshape(samples, (n_test, -1))  # (n_test, num_samples)

    return samples


def _compute_lvec(features, chol_fact, kernel, covariance_scale, feature):
    kvec = anp.reshape(kernel(features, feature), (-1, 1)) * covariance_scale
    return anp.reshape(aspl.solve_triangular(chol_fact, kvec, lower=True), (1, -1))


def cholesky_update(
    features,
    mean: MeanFunction,
    kernel: KernelFunctionWithCovarianceScale,
    chol_fact,
    pred_mat,
    noise_variance,
    feature,
    target,
    lvec=None,
):
    """
    Incremental update of posterior state (Cholesky factor, prediction
    matrix), given one datapoint (feature, target).

    Note: noise_variance is the initial value, before any jitter may have
    been added to compute chol_fact. Here, we add the minimum amount of
    jitter such that the new diagonal entry of the Cholesky factor is
    >= MIN_CHOLESKY_DIAGONAL_VALUE. This means that if cholesky_update is
    used several times, we in fact add a diagonal (but not spherical)
    jitter matrix.

    :param features: Shape (n, d)
    :param chol_fact: Shape (n, n)
    :param pred_mat: Shape (n, m)
    :param mean:
    :param kernel:
    :param noise_variance:
    :param feature: Shape (1, d)
    :param target: Shape (1, m)
    :param lvec: If given, this is the new column of the Cholesky factor
        except the diagonal entry. If not, this is computed here
    :return: chol_fact_new (n+1, n+1), pred_mat_new (n+1, m)
    """
    _kernel, covariance_scale = _extract_kernel_and_scale(kernel)
    if lvec is None:
        lvec = _compute_lvec(features, chol_fact, _kernel, covariance_scale, feature)
    kscal = anp.reshape(_kernel.diagonal(feature) * covariance_scale, (1,))
    noise_variance = anp.reshape(noise_variance, (1,))
    lsqscal = anp.maximum(
        kscal + noise_variance - anp.sum(anp.square(lvec)),
        MIN_CHOLESKY_DIAGONAL_VALUE**2,
    )
    lscal = anp.reshape(anp.sqrt(lsqscal), (1, 1))
    mscal = anp.reshape(mean(feature), (1, 1))
    pvec = target - mscal
    pvec = anp.divide(pvec - anp.matmul(lvec, pred_mat), lscal)
    pred_mat_new = anp.concatenate([pred_mat, pvec], axis=0)
    zerovec = anp.zeros((getval(lvec.size), 1))
    chol_fact_new = anp.concatenate(
        [
            anp.concatenate([chol_fact, lvec], axis=0),
            anp.concatenate([zerovec, lscal], axis=0),
        ],
        axis=1,
    )

    return chol_fact_new, pred_mat_new


# Specialized routine, used in IncrementalUpdateGPPosteriorState.
# The idea is to share the computation of lvec between sampling a new target
# value and incremental Cholesky update.
# If mean_impute_mask is given, it is a boolean vector of size m (number
# columns of pred_mat). Columns j of target, where mean_impute_ mask[j] is
# true, are set to the predictive mean (instead of being sampled).
def sample_and_cholesky_update(
    features,
    mean: MeanFunction,
    kernel: KernelFunctionWithCovarianceScale,
    chol_fact,
    pred_mat,
    noise_variance,
    feature,
    random_state: RandomState,
    mean_impute_mask=None,
):
    _kernel, covariance_scale = _extract_kernel_and_scale(kernel)
    # Draw sample target. Also, lvec is reused below
    lvec = _compute_lvec(features, chol_fact, _kernel, covariance_scale, feature)
    pred_mean = anp.dot(lvec, pred_mat) + anp.reshape(mean(feature), (1, 1))
    # Note: We do not add noise_variance to the predictive variance
    pred_std = anp.reshape(
        anp.sqrt(
            anp.maximum(
                _kernel.diagonal(feature) * covariance_scale
                - anp.sum(anp.square(lvec)),
                MIN_POSTERIOR_VARIANCE,
            )
        ),
        (1, 1),
    )
    n01mat = random_state.normal(size=getval(pred_mean.shape))
    if mean_impute_mask is not None:
        assert len(mean_impute_mask) == pred_mat.shape[1]
        n01mat[0, mean_impute_mask] = 0
    target = pred_mean + anp.multiply(n01mat, pred_std)
    chol_fact_new, pred_mat_new = cholesky_update(
        features=features,
        mean=mean,
        kernel=kernel,
        chol_fact=chol_fact,
        pred_mat=pred_mat,
        noise_variance=noise_variance,
        feature=feature,
        target=target,
        lvec=lvec,
    )
    features_new = anp.concatenate([features, feature], axis=0)

    return chol_fact_new, pred_mat_new, features_new, target


def negative_log_marginal_likelihood(chol_fact, pred_mat):
    """
    The marginal likelihood is only computed if pred_mat has a single column
    (not for fantasy sample case).
    """
    assert (
        pred_mat.ndim == 1 or pred_mat.shape[1] == 1
    ), "Multiple target vectors are not supported"
    sqnorm_predmat = anp.sum(anp.square(pred_mat))
    logdet_cholfact = 2.0 * anp.sum(anp.log(anp.abs(anp.diag(chol_fact))))
    n_samples = getval(pred_mat.size)
    part1 = 0.5 * (n_samples * anp.log(2 * anp.pi) + logdet_cholfact)
    part2 = 0.5 * sqnorm_predmat
    return part1 + part2

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/slice.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Callable, Tuple, List
import numpy as np
from numpy.random import RandomState


from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd import SliceException


MAX_STEP_OUT = 200
MAX_STEP_LOOP = 200


class SliceSampler:
    def __init__(
        self,
        log_density: Callable[[np.ndarray], float],
        scale: float,
        random_state: RandomState,
    ):
        self.log_density = log_density
        self.scale = scale  # default in scala core is 1.0
        self._random_state = random_state

    def _gen_next_sample(self, x0: np.ndarray) -> np.ndarray:
        random_direction = gen_random_direction(len(x0), self._random_state)

        def sliced_log_density(_movement: float) -> float:
            return self.log_density(x0 + random_direction * _movement)

        # a quantity used to determine the bounds and accept movement along random_direction
        log_pivot = sliced_log_density(0.0) + np.log(self._random_state.rand())

        lower_bound, upper_bound = slice_sampler_step_out(
            log_pivot, self.scale, sliced_log_density, self._random_state
        )
        movement = slice_sampler_step_in(
            lower_bound, upper_bound, log_pivot, sliced_log_density, self._random_state
        )
        return x0 + random_direction * movement

    def sample(
        self, init_sample: np.ndarray, num_samples: int, burn: int, thin: int
    ) -> List[np.ndarray]:
        samples = []
        next_sample = init_sample
        for _ in range(num_samples):
            next_sample = self._gen_next_sample(next_sample)
            samples.append(next_sample)
        return samples[burn::thin]


def gen_random_direction(dimension: int, random_state: RandomState) -> np.ndarray:
    random_direction = random_state.randn(dimension)
    random_direction *= 1.0 / np.linalg.norm(random_direction)
    return random_direction


def slice_sampler_step_out(
    log_pivot: float,
    scale: float,
    sliced_log_density: Callable[[float], float],
    random_state: RandomState,
) -> Tuple[float, float]:
    r = random_state.rand()
    lower_bound = -r * scale
    upper_bound = lower_bound + scale

    def bound_step_out(bound, direction):
        """direction -1 for lower bound, +1 for upper bound"""
        for _ in range(MAX_STEP_OUT):
            if sliced_log_density(bound) <= log_pivot:
                return bound
            else:
                bound += direction * scale
        raise SliceException(
            "Reach maximum iteration ({}) while stepping out for bound ({})".format(
                MAX_STEP_OUT, direction
            )
        )

    lower_bound = bound_step_out(lower_bound, -1.0)
    upper_bound = bound_step_out(upper_bound, 1.0)
    return lower_bound, upper_bound


def slice_sampler_step_in(
    lower_bound: float,
    upper_bound: float,
    log_pivot: float,
    sliced_log_density: Callable[[float], float],
    random_state: RandomState,
) -> float:
    """Find the right amount of movement along with a random_direction"""
    for _ in range(MAX_STEP_LOOP):
        movement = (upper_bound - lower_bound) * random_state.rand() + lower_bound
        if movement == 0.0:
            raise SliceException(
                "The interval for slice sampling has reduced to zero in step in"
            )
        if sliced_log_density(movement) > log_pivot:
            return movement
        else:
            lower_bound = movement if movement < 0.0 else lower_bound
            upper_bound = movement if movement > 0.0 else upper_bound
    raise SliceException(
        "Reach maximum iteration ({}) while stepping in".format(MAX_STEP_LOOP)
    )

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/gpautograd/warping.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import autograd.numpy as anp

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_ENCODING,
    INITIAL_WARPING,
    WARPING_LOWER_BOUND,
    WARPING_UPPER_BOUND,
    NUMERICAL_JITTER,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    LogNormal,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon import Block
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
    register_parameter,
    create_encoding,
)


class OneDimensionalWarping(Block):
    """
    Block that is responsible for the warping of a single, column
    feature x. Typically, the full data X = [x1, x2,..., xd] is in (n, d) and
    each xi is a column feature in (n, 1).

    Consider column feature x and assume that the entries of x are contained in
    the range input_range. Each entry of x is transformed by
        warping(u) = 1. - (1. - R(u)^a)^b,
    with a,b two non negative parameters learned by empirical Bayes, and R(.)
    is a linear transformation that, based on input_range, rescales the entry
    of x into [eps, 1-eps] for some small eps > 0.

    :param input_range: tuple that contains the lower and upper bounds of the
        entries of x.
    """

    def __init__(self, input_range, encoding_type=DEFAULT_ENCODING, **kwargs):
        super().__init__(**kwargs)
        self.input_range = input_range
        self.encoding = create_encoding(
            encoding_type,
            INITIAL_WARPING,
            WARPING_LOWER_BOUND,
            WARPING_UPPER_BOUND,
            2,
            LogNormal(0.0, 0.75),
        )
        with self.name_scope():
            self.warping_internal = register_parameter(
                self.params, "warping", self.encoding, shape=(2,)
            )

    def _rescale(self, x):
        """
        We linearly rescale the entries of x into [NUMERICAL_JITTER, 1-NUMERICAL_JITTER]
        In this way, we avoid the differentiability problems at 0
        :param x: np.ndarray to be rescaled
        """
        lower, upper = self.input_range
        P = (1.0 - 2 * NUMERICAL_JITTER) / (upper - lower)
        Q = (NUMERICAL_JITTER * (upper + lower) - lower) / (upper - lower)

        return P * x + Q

    def _warping(self):
        return encode_unwrap_parameter(self.warping_internal, self.encoding)

    def forward(self, x):
        """
        Actual computation of the warping transformation (see details above)

        :param x: input data of size (n,1)
        """
        warping = anp.reshape(self._warping(), (-1,))
        warping_a = warping[0]
        warping_b = warping[1]

        return 1.0 - anp.power(1.0 - anp.power(self._rescale(x), warping_a), warping_b)

    def param_encoding_pairs(self):
        """
        Return a list of tuples with the Gluon parameters of the 1-D warping
        and their respective encodings
        """
        return [(self.warping_internal, self.encoding)]

    def get_params(self):
        warping = anp.reshape(self._warping(), (-1,))
        return {"warping_a": warping[0], "warping_b": warping[1]}

    def set_params(self, param_dict):
        warping = [param_dict["warping_a"], param_dict["warping_b"]]
        self.encoding.set(self.warping_internal, warping)


class Warping(Block):
    """
    Block that computes warping over all the columns of some input data X.
    If X is of size (n,dimension), where dimension has to be specified, a 1-D warping
    transformation is applied to each column X[:,j] with j a key in index_to_range.
    More precisely, index_to_range is a dictionary of the form
        {
            j : (lower_bound_column_j, upper_bound_column_j),
            k : (lower_bound_column_k, upper_bound_column_k),
            ....
        }
    that maps column indexes to their corresponding ranges.
    """

    def __init__(
        self, dimension, index_to_range, encoding_type=DEFAULT_ENCODING, **kwargs
    ):
        super().__init__(**kwargs)

        assert isinstance(index_to_range, dict)
        assert all(isinstance(r, tuple) for r in index_to_range.values())
        assert all(r[0] < r[1] for r in index_to_range.values())  # for dictionary

        self.transformations = []
        self._params_encoding_pairs = []
        self.dimension = dimension
        self.index_to_range = index_to_range

        some_are_warped = False
        for col_index in range(dimension):
            if col_index in index_to_range:
                transformation = OneDimensionalWarping(
                    index_to_range[col_index], encoding_type=encoding_type
                )
                # To make sure that OneDimensionalWarping will get initialized
                # and managed by Warping, we register it as a child.
                self.register_child(transformation, name=transformation.name)
                self._params_encoding_pairs += transformation.param_encoding_pairs()
                some_are_warped = True
            else:
                # if a column is not warped, we do not apply any transformation
                transformation = lambda x: x
            self.transformations.append(transformation)
        assert some_are_warped, "At least one of the dimensions must be warped"

    def forward(self, X):
        """
        Actual computation of warping applied to each column of X

        :param X: input data of size (n,dimension)
        """
        warped_X = []
        for col_index, transformation in enumerate(self.transformations):
            x = X[:, col_index : (col_index + 1)]
            warped_X.append(transformation(x))

        return anp.concatenate(warped_X, axis=1)

    def param_encoding_pairs(self):
        """
        Return a list of tuples with the Gluon parameters of the warping and
        their respective encodings
        """
        return self._params_encoding_pairs

    def get_params(self):
        """
        Keys are warping_a, warping_b if there is one dimension, and
        warping_a<k>, warping_b<k> otherwise.
        """
        if len(self.transformations) == 1:
            result = self.transformations[0].get_params()
        else:
            result = dict()
            for i, warping in enumerate(self.transformations):
                if isinstance(warping, OneDimensionalWarping):
                    istr = str(i)
                    for k, v in warping.get_params().items():
                        result[k + istr] = v
        return result

    def set_params(self, param_dict):
        if len(self.transformations) == 1:
            self.transformations[0].set_params(param_dict)
        else:
            transf_keys = None
            for i, warping in enumerate(self.transformations):
                if isinstance(warping, OneDimensionalWarping):
                    if transf_keys is None:
                        transf_keys = warping.get_params().keys()
                    istr = str(i)
                    stripped_dict = dict()
                    for k in transf_keys:
                        stripped_dict[k] = param_dict[k + istr]
                    warping.set_params(stripped_dict)


class WarpedKernel(KernelFunction):
    """
    Block that composes warping with an arbitrary kernel
    """

    def __init__(self, kernel: KernelFunction, warping: Warping, **kwargs):
        super().__init__(kernel.dimension, **kwargs)
        self.kernel = kernel
        self.warping = warping

    def forward(self, X1, X2):
        """
        Actual computation of the composition of warping with an arbitrary
        kernel K. If we have input data X1 and X2, of respective dimensions
        (n1, d) and (n2, d), we compute the matrix

            K(warping(X1), warping(X2)) of size (n1,n2)
            whose (i,j) entry is given by K(warping(X1[i,:]), warping(X2[j,:]))

        :param X1: input data of size (n1, d)
        :param X2: input data of size (n2, d)
        """
        warped_X1 = self.warping(X1)
        if X1 is X2:
            warped_X2 = warped_X1
        else:
            warped_X2 = self.warping(X2)
        return self.kernel(warped_X1, warped_X2)

    def diagonal(self, X):
        # If kernel.diagonal does not depend on content of X (but just its
        # size), can pass X instead of self.warping(X)
        warped_X = self.warping(X) if self.kernel.diagonal_depends_on_X() else X
        return self.kernel.diagonal(warped_X)

    def diagonal_depends_on_X(self):
        return self.kernel.diagonal_depends_on_X()

    def param_encoding_pairs(self):
        return self.kernel.param_encoding_pairs() + self.warping.param_encoding_pairs()

    def get_params(self):
        # We use the union of get_params for kernel and warping, without
        # prefixes.
        result = self.kernel.get_params()
        result.update(self.warping.get_params())
        return result

    def set_params(self, param_dict):
        self.kernel.set_params(param_dict)
        self.warping.set_params(param_dict)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/cost/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/cost/cost_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List
from dataclasses import dataclass

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)

__all__ = ["CostValue", "CostModel"]


@dataclass
class CostValue:
    """
    Represents cost value (c0(x), c1(x)):
        c_0(x): Startup cost for evaluation at config x
        c_1(x): Cost per unit of resource r at config x
    Our assumption is that, under the model, an evaluation at x until resource
    level r = 1, 2, 3, ... costs
        c(x, r) = c_0(x) + r c_1(x)
    """

    c0: float
    c1: float


class CostModel:
    """
    Interface for (temporal) cost model in the context of multi-fidelity HPO.
    We assume there are configurations x and resource levels r (for example,
    r may be number of epochs). Here, r is a positive int.
    Can be seen as simplified version of surrogate model, which is mainly used
    in order to draw (jointly dependent) values from the posterior over
    cost values (c0(x), c1(x)).

    Note: The model may be random (in which case joint samples are drawn from
    the posterior) or deterministic (in which case the model is fitted to data,
    and then cost values returned are deterministic.

    A cost model has an inner state, which is set by calling `update` passing a
    dataset. This inner state is then used when `sample_joint` is called.

    """

    @property
    def cost_metric_name(self) -> str:
        """
        :return: Name of metric in TrialEvaluations of cases in
            TuningJobState
        """
        raise NotImplementedError

    def update(self, state: TuningJobState):
        """
        Update inner representation in order to be ready to return cost value
        samples.

        Note: The metric self.cost_metric_name must be dict-valued in `state`,
        wiht keys being resource values r. In order to support a proper
        estimation of c_0 and c_1, there should (ideally) be entries with the
        same x and different resource levels r. The likelihood function takes
        into account that
            c(x, r) = c_0(x) + r c_1(x)

        :param state: Current dataset (only trials_evaluations is used)
        """
        raise NotImplementedError

    def resample(self):
        """
        For a random cost model, the state is resampled, such that calls of
        joint_sample before and after are conditionally independent. Normally,
        successive calls of sample_joint are jointly dependent.
        For example, for a linear model, the state resampled here would be the
        weight vector, which is then used in 'sample_joint'.

        For a deterministic cost model, this method does nothing.
        """
        pass

    def sample_joint(self, candidates: List[Configuration]) -> List[CostValue]:
        """
        Draws cost values (c_0(x), c_1(x)) for candidates (non-extended).

        If the model is random, the sampling is done jointly. Also, if
        sample_joint is called multiple times, the posterior is to be updated
        after each call, such that the sample over the union of candidates over
        all calls is drawn jointly (but see 'resample'). Also, if measurement
        noise is allowed in update, this noise is *not* added here. A sample
        from c(x, r) is obtained as c_0(x) + r c_1(x).
        If the model is deterministic, the model determined in update is just
        evaluated.

        :param candidates: Non-extended configs
        :return: List of (c_0(x), c_1(x))
        """
        raise NotImplementedError

    @staticmethod
    def event_time(
        start_time: float, level: int, next_milestone: int, cost: CostValue
    ) -> float:
        """
        If a task reported its last recent value at start_time at level level,
        return time of reaching level next_milestone, given cost cost.

        :param start_time:
        :param level:
        :param next_milestone:
        :param cost:
        :return: Time of reaching next_milestone under cost model
        """
        result = start_time + cost.c1 * (next_milestone - level)
        if level == 0:
            # Add startup time
            result += cost.c0
        return result

    def predict_times(
        self,
        candidates: List[Configuration],
        resources: List[int],
        cost_values: List[CostValue],
        start_time: float = 0,
    ) -> List[float]:
        """
        Given configs x, resource values r and cost values from sample_joint,
        compute time predictions for when each config x reaches its resource
        level r if started at start_time.

        :param candidates: Configs
        :param resources: Resource levels
        :param cost_values: Cost values from sample_joint
        :param start_time:
        :return: Predicted times
        """
        num_cases = len(candidates)
        assert len(resources) == num_cases
        assert len(cost_values) == num_cases
        time_predictions = []
        for candidate, resource, cost in zip(candidates, resources, cost_values):
            time_predictions.append(
                self.event_time(
                    start_time=start_time, level=0, next_milestone=resource, cost=cost
                )
            )
        return time_predictions

    def _check_dataset_has_cost_metric(self, state: TuningJobState):
        assert all(
            self.cost_metric_name in x.metrics for x in state.trials_evaluations
        ), "All labeled cases in state must have metrics[{}]".format(
            self.cost_metric_name
        )

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/cost/linear_cost_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Callable, Optional, Dict, Tuple
import numpy as np
from sklearn.linear_model import RidgeCV
from enum import IntEnum

from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.cost_model import (
    CostModel,
    CostValue,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_COST_NAME,
)
from syne_tune.optimizer.schedulers.searchers.searcher import impute_points_to_evaluate

__all__ = [
    "LinearCostModel",
    "MLPLinearCostModel",
    "FixedLayersMLPCostModel",
    "NASBench201LinearCostModel",
    "BiasOnlyLinearCostModel",
]


class LinearCostModel(CostModel):
    """
    Deterministic cost model where both c0(x) and c1(x) are linear models of
    the form

        c0(x) = np.dot(features0(x), weights0),
        c1(x) = np.dot(features1(x), weights1)

    The feature maps features0, features1 are supplied by subclasses.
    The weights are fit by ridge regression, using scikit.learn RidgeCV, the
    regularization constant is set by LOO cross-validation.

    """

    def __init__(self):
        self.weights0 = None
        self.weights1 = None

    @property
    def cost_metric_name(self) -> str:
        return INTERNAL_COST_NAME

    def feature_matrices(
        self, candidates: List[Configuration]
    ) -> (np.ndarray, np.ndarray):
        """
        Has to be supplied by subclasses

        :param candidates: List of n candidate configs (non-extended)
        :return: Feature matrices features0 (n, dim0), features1 (n, dim1)
        """
        raise NotImplementedError

    def update(self, state: TuningJobState):
        # Compile feature matrix and targets for linear regression problem
        configs = [
            state.config_for_trial[ev.trial_id] for ev in state.trials_evaluations
        ]
        features0, features1 = self.feature_matrices(configs)
        dim0 = features0.shape[1]
        feature_parts = []
        cost_parts = []
        for feature0, feature1, ev in zip(
            features0, features1, state.trials_evaluations
        ):
            metric_vals = ev.metrics.get(self.cost_metric_name)
            if metric_vals is not None:
                assert isinstance(metric_vals, dict)
                resource_values, cost_values = zip(*metric_vals.items())
                resource_values = np.array(resource_values, dtype=np.float64).reshape(
                    (-1, 1)
                )
                feature0 = feature0.astype(np.float64, copy=False).reshape((1, -1))
                feature1 = feature1.astype(np.float64, copy=False).reshape((1, -1))
                feature_parts.append(
                    np.concatenate(
                        (
                            np.broadcast_to(
                                feature0, (resource_values.size, feature0.size)
                            ),
                            resource_values * feature1,
                        ),
                        axis=1,
                    )
                )
                cost_parts.append(
                    np.array(cost_values, dtype=np.float64).reshape((-1, 1))
                )
        features = np.vstack(feature_parts)
        targets = np.vstack(cost_parts).reshape((-1,))
        assert features.shape[0] == targets.size
        assert features.shape[1] == dim0 + features1.shape[1]
        # Fit with RidgeCV, where alpha is selected by LOO cross-validation
        predictor = RidgeCV(alphas=np.exp(np.arange(-4, 5)), fit_intercept=False).fit(
            features, targets
        )
        self.weights0 = predictor.coef_[:dim0].reshape((-1, 1))
        self.weights1 = predictor.coef_[dim0:].reshape((-1, 1))
        self.alpha = predictor.alpha_

    def sample_joint(self, candidates: List[Configuration]) -> List[CostValue]:
        assert self.weights0 is not None, "Must call 'update' before 'sample_joint'"
        features0, features1 = self.feature_matrices(candidates)
        c0_vals = np.matmul(features0, self.weights0).reshape((-1,))
        c1_vals = np.matmul(features1, self.weights1).reshape((-1,))
        return [CostValue(c0, c1) for c0, c1 in zip(c0_vals, c1_vals)]


class BiasOnlyLinearCostModel(LinearCostModel):
    """
    Simple baseline: features0(x) = [1], features1(x) = [1]

    """

    def __init__(self):
        super().__init__()

    def feature_matrices(
        self, candidates: List[Configuration]
    ) -> (np.ndarray, np.ndarray):
        one_feats = np.ones((len(candidates), 1))
        return one_feats, one_feats


class MLPLinearCostModel(LinearCostModel):
    """
    Deterministic linear cost model for multi-layer perceptron.

    """

    def __init__(
        self,
        num_inputs: int,
        num_outputs: int,
        num_hidden_layers: Callable[[dict], int],
        hidden_layer_width: Callable[[dict, int], int],
        batch_size: Callable[[dict], int],
        bs_exponent: Optional[float] = None,
        extra_mlp: bool = False,
        c0_mlp_feature: bool = False,
        expected_hidden_layer_width: Optional[Callable[[int], float]] = None,
    ):
        """
        If config is a HP configuration, num_hidden_layers(config) is the
        number of hidden layers, hidden_layer_width(config, layer) is the
        number of units in hidden layer layer (0-based), batch_size(config)
        is the batch size.

        If expected_hidden_layer_width is given, it maps layer (0-based) to
        expected layer width under random sampling. In this case, all MLP
        features are normalized to expected value 1 under random sampling
        (but ignoring bs_exponent if != 1).
        Note: If needed, we could incorporate bs_exponent in general. If
        batch_size was uniform between a and b:
            E[ power(bs, bs_exp - 1) ] =
            (power(b, bs_exp) - power(a, bs_exp)) / (bs_exp * (b - a))

        :param num_inputs: Number of input nodes
        :param num_outputs: Number of output nodes
        :param num_hidden_layers: See above
        :param hidden_layer_width: See above
        :param batch_size: See above
        :param bs_exponent: Main MLP feature is multiplied by
            power(batch_size, bs_exponent - 1)
        :param extra_mlp: Add additional "linear" MLP feature to c_1?
        :param c0_mlp_feature: Use main MLP feature in c_0 as well?
        :param expected_hidden_layer_width: See above
        """
        super().__init__()
        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.num_hidden_layers = num_hidden_layers
        self.hidden_layer_width = hidden_layer_width
        self.batch_size = batch_size
        if bs_exponent is not None:
            self.bs_exponent = bs_exponent
        else:
            self.bs_exponent = 1
        self.extra_mlp = extra_mlp
        self.c0_mlp_feature = c0_mlp_feature
        self.expected_hidden_layer_width = expected_hidden_layer_width

    def feature_matrices(
        self, candidates: List[Configuration]
    ) -> (np.ndarray, np.ndarray):
        features1_1 = []
        features1_2 = []
        for config in candidates:
            value = self._mlp_feature(config)
            if self.bs_exponent != 1:
                bs = self.batch_size(config)
                value *= np.power(bs, self.bs_exponent - 1)
            features1_1.append(value)
            if self.extra_mlp:
                features1_2.append(self._mlp_feature2(config))
        ones_vec = np.ones((len(features1_1), 1))
        features1_1 = np.array(features1_1).reshape((-1, 1))
        features1_tpl = (ones_vec, features1_1)
        if self.extra_mlp:
            features1_tpl += (np.array(features1_2).reshape((-1, 1)),)
        features1 = np.concatenate(features1_tpl, axis=1)
        if self.c0_mlp_feature:
            features0 = np.concatenate((ones_vec, features1_1), axis=1)
        else:
            features0 = ones_vec
        return features0, features1

    def _mlp_feature(self, config: Configuration) -> float:
        layers = range(self.num_hidden_layers(config))
        width_list = [self.hidden_layer_width(config, layer) for layer in layers]
        if self.expected_hidden_layer_width is None:
            norm_const = 1
        else:
            norm_const = self._sum_of_prod(
                [self.expected_hidden_layer_width(layer) for layer in layers]
            )
        return self._sum_of_prod(width_list) / norm_const

    def _sum_of_prod(self, lst):
        return sum(
            x * y for x, y in zip([self.num_inputs] + lst, lst + [self.num_outputs])
        )

    def _mlp_feature2(self, config: Configuration) -> float:
        layers = range(self.num_hidden_layers(config))
        width_list = [self.hidden_layer_width(config, layer) for layer in layers]
        if self.expected_hidden_layer_width is None:
            norm_const = 1
        else:
            norm_const = sum(
                self.expected_hidden_layer_width(layer) for layer in layers
            )
        return sum(width_list) / norm_const


class FixedLayersMLPCostModel(MLPLinearCostModel):
    """
    Linear cost model for MLP with num_hidden_layers hidden layers.

    """

    def __init__(
        self,
        num_inputs: int,
        num_outputs: int,
        num_units_keys: List[str] = None,
        bs_exponent: Optional[float] = None,
        extra_mlp: bool = False,
        c0_mlp_feature: bool = False,
        expected_hidden_layer_width: Optional[Callable[[int], float]] = None,
    ):
        if num_units_keys is None:
            num_units_keys = ["n_units_1", "n_units_2"]
        num_hidden_layers = len(num_units_keys)

        def hidden_layer_width(config, layer):
            return int(config[num_units_keys[layer]])

        super().__init__(
            num_inputs=num_inputs,
            num_outputs=num_outputs,
            num_hidden_layers=lambda config: num_hidden_layers,
            hidden_layer_width=hidden_layer_width,
            batch_size=lambda config: int(config["batch_size"]),
            bs_exponent=bs_exponent,
            extra_mlp=extra_mlp,
            c0_mlp_feature=c0_mlp_feature,
            expected_hidden_layer_width=expected_hidden_layer_width,
        )

    @staticmethod
    def get_expected_hidden_layer_width(config_space: Dict, num_units_keys: List[str]):
        """
        Constructs expected_hidden_layer_width function from the training
        evaluation function.
        Works because `impute_points_to_evaluate` imputes with the expected
        value under random sampling.

        :param config_space: Configuration space
        :param num_units_keys: Keys into `config_space` for number of
            units of different layers
        :return: expected_hidden_layer_width, exp_vals
        """
        default_config = impute_points_to_evaluate(None, config_space)[0]
        exp_vals = [default_config[k] for k in num_units_keys]

        def expected_hidden_layer_width(x):
            return exp_vals[x]

        return expected_hidden_layer_width, exp_vals


class NASBench201LinearCostModel(LinearCostModel):
    """
    Deterministic linear cost model for NASBench201.

    The cell graph is:
        node1 = x0(node0)
        node2 = x1(node0) + x2(node1)
        node3 = x3(node0) + x4(node1) + x5(node2)

    """

    class Op(IntEnum):
        SKIP_CONNECT = 0
        NONE = 1
        NOR_CONV_1x1 = 2
        NOR_CONV_3x3 = 3
        AVG_POOL_3x3 = 4

    def __init__(
        self,
        config_keys: Tuple[str, ...],
        map_config_values: Dict[str, int],
        conv_separate_features: bool,
        count_sum: bool,
    ):
        """
        `config_keys` contains attribute names of x0, ..., x5 in a config, in
        this ordering. `map_config_values` maps values in the config (for
        fields corresponding to x0, ..., x5) to entries of `Op`.

        :param config_keys: See above
        :param map_config_values: See above
        :param conv_separate_features: If True, we use separate features for
            nor_conv_1x1, nor_conv_3x3 (c1 has 4 features). Otherwise, these
            two are captured by a single features (c1 has 3 features)
        :param count_sum: If True, we use an additional feature for pointwise
            sum operators inside a cell (there are between 0 and 3)

        """
        super().__init__()
        self._config_keys = config_keys
        self._map_config_values = map_config_values
        self.conv_separate_features = conv_separate_features
        self.count_sum = count_sum

    def _translate(self, config: Configuration) -> List[int]:
        return [self._map_config_values[config[name]] for name in self._config_keys]

    def feature_matrices(
        self, candidates: List[Configuration]
    ) -> (np.ndarray, np.ndarray):
        features1_1 = []
        features1_2 = []
        features1_3 = []  # If conv_separate_features
        features1_4 = []  # If count_sum
        none_val = NASBench201LinearCostModel.Op.NONE
        for config in candidates:
            operators = self._translate(config)
            # Certain NONE (or "zeroize") values imply other NONE values:
            # x0 = > x2 and x4
            # x1 and x2 = > x5
            if operators[0] == none_val:
                operators[2] = none_val
                operators[4] = none_val
            if operators[1] == none_val and operators[2] == none_val:
                operators[5] = none_val
            n_conv1, n_conv3, n_apool = map(
                sum,
                zip(
                    *(
                        (
                            x == NASBench201LinearCostModel.Op.NOR_CONV_1x1,
                            x == NASBench201LinearCostModel.Op.NOR_CONV_3x3,
                            x == NASBench201LinearCostModel.Op.AVG_POOL_3x3,
                        )
                        for x in operators
                    )
                ),
            )
            features1_1.append((5 / 6) * n_apool)
            if self.conv_separate_features:
                features1_2.append((5 / 6) * n_conv1)
                features1_3.append((5 / 6) * n_conv3)
            else:
                features1_2.append((n_conv1 + 9 * n_conv3) / 12)
            if self.count_sum:
                features1_4.append(
                    (25 / 76)
                    * (
                        (operators[1] != none_val) * (operators[2] != none_val)
                        + (operators[3] != none_val)
                        + (operators[4] != none_val)
                        + (operators[5] != none_val)
                    )
                )

        ones_vec = np.ones((len(features1_1), 1))
        features1_1 = np.array(features1_1).reshape((-1, 1))
        features1_2 = np.array(features1_2).reshape((-1, 1))
        features1_tpl = (ones_vec, features1_1, features1_2)
        if self.conv_separate_features:
            features1_tpl += (np.array(features1_3).reshape((-1, 1)),)
        if self.count_sum:
            features1_tpl += (np.array(features1_4).reshape((-1, 1)),)
        features1 = np.concatenate(features1_tpl, axis=1)
        return ones_vec, features1

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/cost/sklearn_cost_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Tuple, Callable
import numpy as np
import logging
from syne_tune.try_import import try_import_blackbox_repository_message

try:
    from sklearn.ensemble import (
        RandomForestRegressor,
        GradientBoostingRegressor,
    )
except ImportError:
    logging.info(try_import_blackbox_repository_message())
    raise
from scipy.optimize import brentq
from scipy.interpolate import UnivariateSpline

from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.cost_model import (
    CostModel,
    CostValue,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_COST_NAME,
)

__all__ = ["ScikitLearnCostModel", "UnivariateSplineCostModel"]


class NonLinearCostModel(CostModel):
    """
    Deterministic cost model, where c0(x) = b0 (constant), and c1(x) is given
    by a nonlinear regression model specified in subclasses. Parameters are b0
    and those of the regression model. We use a simple algorithm to jointly fit
    b0 and c1(x).

    """

    def __init__(self):
        self.b0 = None
        self.regr_model = None
        self.hp_ranges = None

    @property
    def cost_metric_name(self) -> str:
        return INTERNAL_COST_NAME

    def transform_dataset(
        self, dataset: List[Tuple[Configuration, float]], num_data0: int, res_min: int
    ) -> dict:
        """
        Transforms dataset (see `_data_for_c1_regression`) into a dataset
        representation (dict), which is used as `kwargs` in `fit_regressor`.

        :param dataset:
        :param num_data0:
        :param res_min:
        :return: Used as kwargs in fit_regressor
        """
        raise NotImplementedError

    @staticmethod
    def fit_regressor(b0: float, **kwargs):
        """
        Given value for b0, fits regressor to dataset specified via kwargs
        (see `transform_dataset`). Returns the criterion function value for
        b0 as well as the fitted regression model.

        :param b0:
        :param kwargs:
        :return: fval, model
        """
        raise NotImplementedError

    def predict_c1_values(self, candidates: List[Configuration]):
        """
        :param candidates: Test configs
        :return: Corresponding c1 values
        """
        raise NotImplementedError

    def update(self, state: TuningJobState):
        self.hp_ranges = state.hp_ranges  # Needed in transform_dataset
        # Prepare data for fitting c1(x)
        dataset, num_data0, res_min, target_min = self._data_for_c1_regression(state)
        assert target_min > 0  # Sanity check
        data_kwargs = self.transform_dataset(dataset, num_data0, res_min)
        best = [None]  # Model corresponding to root
        # Since critfunc is not strictly well-defined, we need to
        # cache values for previous evals at the same b0. In
        # particular, this avoids "invalid bracket" errors when
        # brentq evaluates at the bracket ends.
        cf_cache = dict()

        def critfunc(b0):
            if b0 in cf_cache:
                fval = cf_cache[b0]
            else:
                fval, model = self.fit_regressor(b0, **data_kwargs)
                cf_cache[b0] = fval
                absfval = abs(fval)
                best_tup = best[0]
                if (best_tup is None) or (absfval < best_tup[0]):
                    best[0] = (absfval, b0, model)
            return fval

        # Root finding for b0
        atol = 1e-5
        ftol = 1e-8
        f_low = critfunc(0)
        if num_data0 < len(dataset) and f_low < -ftol:
            # f(0) < -ftol < 0
            f_high = critfunc(target_min)
            if f_high > ftol:
                # f(target_min) > ftol > 0: We have a bracket
                try:
                    brentq(critfunc, a=0, b=target_min, xtol=atol)
                except Exception:
                    # Use best evaluated until exception
                    pass
        _, self.b0, self.regr_model = best[0]

    def sample_joint(self, candidates: List[Configuration]) -> List[CostValue]:
        assert self.b0 is not None, "Must call 'update' before 'sample_joint'"
        c1_vals = self.predict_c1_values(candidates)
        c0_vals = np.full(len(c1_vals), self.b0)
        return [CostValue(c0, c1) for c0, c1 in zip(c0_vals, c1_vals)]

    def _data_for_c1_regression(self, state: TuningJobState):
        """
        Extracts `dataset` as list of (config, target) tuples. The first
        num_data0 records correspond to configs appearing only once in
        `state`, at the minimum resource level `res_min`.

        :param state: TuningJobState
        :return: dataset, num_data0, res_min, target_min
        """
        data_config = []
        for ev in state.trials_evaluations:
            metric_vals = ev.metrics[self.cost_metric_name]
            assert isinstance(metric_vals, dict)
            config = state.config_for_trial[ev.trial_id]
            data_config.append((config, list(metric_vals.items())))
        res_min = min(min(res for res, _ in tpls) for _, tpls in data_config)
        target_min = min(min(cost for _, cost in tpls) for _, tpls in data_config)
        # Split data into two parts (r = res_min, r > res_min),
        # compute transformed target values
        data_0, data_1 = [], []
        for config, targets in data_config:
            if len(targets) > 1:
                # config has >1 entry -> data_1
                targets = sorted(targets, key=lambda x: x[0])
                lst = [
                    (x1[1] - x2[1]) / (x1[0] - x2[0])
                    for x1, x2 in zip(targets[:-1], targets[1:])
                ]
                data_1.extend([(config, y) for y in lst])
            x = targets[0]
            assert x[0] == res_min, "config = {}, targets = {}".format(config, targets)
            data_0.append((config, x[1] / res_min))
        # Return dataset: data_0 comes before data_1
        num_data0 = len(data_0)
        return data_0 + data_1, num_data0, res_min, target_min


_supported_model_types = {"random_forest", "gradient_boosting"}


class ScikitLearnCostModel(NonLinearCostModel):
    """
    Deterministic cost model, where c0(x) = b0 (constant), and c1(x) is given
    by a scikit.learn (or scipy) regression model. Parameters are b0 and those
    of the regression model.

    """

    def __init__(self, model_type=None):
        """
        :param model_type: Regression model for c1(x)

        """
        if model_type is None:
            model_type = "random_forest"
        else:
            assert (
                model_type in _supported_model_types
            ), "model_type = '{}' not supported, must be in {}".format(
                model_type, _supported_model_types
            )
        super().__init__()
        self.model_type = model_type

    def transform_dataset(
        self, dataset: List[Tuple[Configuration, float]], num_data0: int, res_min: int
    ) -> dict:
        num_hps = len(self.hp_ranges)
        num_data = len(dataset)
        features = np.zeros((num_data, num_hps))
        targets = np.zeros(num_data)
        for i, (config, target) in enumerate(dataset):
            features[i, :] = self.hp_ranges.to_ndarray(config, categ_onehot=False)
            targets[i] = target
        return {
            "features": features,
            "targets": targets,
            "num_data0": num_data0,
            "res_min": res_min,
            "model_type": self.model_type,
        }

    @staticmethod
    def fit_regressor(b0: float, **kwargs):
        features = kwargs["features"]
        targets = kwargs["targets"]
        num_data0 = kwargs["num_data0"]
        res_min = kwargs["res_min"]
        _targets = targets.copy()
        _targets[:num_data0] -= b0 / res_min
        if kwargs["model_type"] == "random_forest":
            model = RandomForestRegressor(n_estimators=50)
        else:
            model = GradientBoostingRegressor()
        model.fit(features, _targets)
        # Compute root finding criterion for b0
        resvec = (
            model.predict(features[:num_data0]).reshape((-1,)) - targets[:num_data0]
        )
        crit_val = (np.sum(resvec) + b0 * num_data0 / res_min) / res_min
        return crit_val, model

    def predict_c1_values(self, candidates: List[Configuration]):
        features1 = self.hp_ranges.to_ndarray_matrix(candidates, categ_onehot=False)
        c1_vals = self.regr_model.predict(features1).reshape((-1,))
        return c1_vals


class UnivariateSplineCostModel(NonLinearCostModel):
    """
    Here, c1(x) is given by a univariate spline
    (scipy.optimize.UnivariateSpline), where a single scalar is extracted from
    x.

    In the second part of the dataset (pos >= num_data0), duplicate entries with
    the same config in dataset are grouped into one, using the mean as target
    value, and a weight equal to the number of duplicates. This still leaves
    duplicates in the overall dataset, one in data0, the other in data1, but
    spline smoothing can deal with this.

    """

    def __init__(
        self,
        scalar_attribute: Callable[[Configuration], float],
        input_range: Tuple[float, float],
        spline_degree: int = 3,
    ):
        """
        :param scalar_attribute: Maps config to scalar input attribute
        :param input_range: (lower, upper), range for input attribute
        :param spline_degree: Degree for smoothing spline, in 1, ..., 5
        """
        assert (
            spline_degree >= 1 and spline_degree <= 5
        ), "spline_degree = {} invalid, must be integer in [1, 5]".format(spline_degree)
        assert (
            len(input_range) == 2 and input_range[0] < input_range[1]
        ), "input_range = {} not valid range for input attribute"
        super().__init__()
        self.scalar_attribute = scalar_attribute
        self.input_range = input_range
        self.spline_degree = spline_degree

    def transform_dataset(
        self, dataset: List[Tuple[Configuration, float]], num_data0: int, res_min: int
    ) -> dict:
        # We combine duplicates in the second part of the dataset
        config_lst, target_lst = zip(*dataset[:num_data0])
        config_lst = list(config_lst)
        target_lst = list(target_lst)
        weight_lst = [1] * num_data0
        data_config = dict()
        for config, target in dataset[num_data0:]:
            config_key = self.hp_ranges.config_to_match_string(config)
            if config_key in data_config:
                data_config[config_key][1].append(target)
            else:
                data_config[config_key] = (config, [target])
        for config, targets in data_config.values():
            config_lst.append(config)
            target_lst.append(np.mean(targets))
            weight_lst.append(len(targets))
        # Create scalar features
        features = np.array([self.scalar_attribute(config) for config in config_lst])
        targets = np.array(target_lst)
        weights = np.array(weight_lst)
        return {
            "features": features,
            "targets": targets,
            "weights": weights,
            "num_data0": num_data0,
            "res_min": res_min,
            "input_range": self.input_range,
            "spline_degree": self.spline_degree,
        }

    @staticmethod
    def fit_regressor(b0: float, **kwargs):
        features = kwargs["features"]
        targets = kwargs["targets"]
        weights = kwargs["weights"]
        num_data0 = kwargs["num_data0"]
        res_min = kwargs["res_min"]
        input_range = kwargs["input_range"]
        spline_degree = min(kwargs["spline_degree"], targets.size - 1)
        _targets = targets.copy()
        _targets[:num_data0] -= b0 / res_min
        # Inputs must be in increasing order
        sort_ind = np.argsort(features)
        _features = features[sort_ind]
        _targets = _targets[sort_ind]
        _weights = weights[sort_ind]
        # Merge cases with equal inputs (UnivariateSpline does not work
        # with duplicate inputs)
        feature_lst = []
        target_lst = []
        weight_lst = []
        x = _features[0]
        wsum = _weights[0]
        y = wsum * _targets[0]
        sz = targets.size
        _features = np.insert(_features, sz, _features[-1] + 10)  # Guard
        for i in range(1, sz + 1):
            x_new = _features[i]
            if x_new == x:
                w_new = _weights[i]
                y += w_new * _targets[i]
                wsum += w_new
            else:
                feature_lst.append(x)
                target_lst.append(y / wsum)
                weight_lst.append(wsum)
                if i < sz:
                    x = x_new
                    wsum = _weights[i]
                    y = wsum * _targets[i]
        model = UnivariateSpline(
            x=feature_lst, y=target_lst, w=weight_lst, bbox=input_range, k=spline_degree
        )
        # Compute root finding criterion for b0
        resvec = model(features[:num_data0]).reshape((-1,)) - targets[:num_data0]
        crit_val = (np.sum(resvec) + b0 * num_data0 / res_min) / res_min
        return crit_val, model

    def predict_c1_values(self, candidates: List[Configuration]):
        features1 = np.array([self.scalar_attribute(config) for config in candidates])
        c1_vals = self.regr_model(features1).reshape((-1,))
        return c1_vals

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/cost_fifo_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, List, Set
import numpy as np
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    TransformerModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_base import (
    BaseSurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost.cost_model import (
    CostModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateModel,
)

logger = logging.getLogger(__name__)


class CostFixedResourceSurrogateModel(BaseSurrogateModel):
    """
    Wraps cost model c(x, r) of :class:`CostModel` to be used as
    surrogate model, where predictions are done at r = `fixed_resource`.

    Note: For random cost models, we approximate expectations in `predict`
    by resampling `num_samples` times (should be 1 for deterministic cost
    models).

    Note: Since this is a generic wrapper, we assume for `backward_gradient`
    that the gradient contribution through the cost model vanishes. For special
    cost models, the mapping from encoded input to predictive means may be
    differentiable, and prediction code in `autograd` may be available. For
    such cost models, this wrapper should not be used, and `backward_gradient`
    should be implemented properly.

    """

    def __init__(
        self,
        state: TuningJobState,
        model: CostModel,
        fixed_resource: int,
        num_samples: int = 1,
    ):
        """
        :param state: TuningJobSubState
        :param model: CostModel. Model parameters must have been fit
        :param fixed_resource: c(x, r) is predicted for this resource level r
        :param num_samples: Number of samples drawn in `predict`. Use this for
            random cost models only

        """
        super().__init__(state, active_metric=model.cost_metric_name)
        self._model = model
        self._fixed_resource = fixed_resource
        self._num_samples = num_samples

    @staticmethod
    def keys_predict() -> Set[str]:
        return {"mean"}

    def predict(self, inputs: np.ndarray) -> List[Dict[str, np.ndarray]]:
        # Inputs are encoded. For cost models, need to decode them back
        # to candidates (not necessarily differentiable)
        # Note: Both `inputs` and `hp_ranges` may correspond to extended
        # configs (where one attribute is the resource level). This is not
        # a problem, since the resource attribute is simply ignored by the
        # cost model.
        hp_ranges = self.hp_ranges_for_prediction()
        candidates = [hp_ranges.from_ndarray(enc_config) for enc_config in inputs]
        resources = [self._fixed_resource] * len(candidates)
        prediction_list = []
        for _ in range(self._num_samples):
            self._model.resample()
            cost_values = self._model.sample_joint(candidates)
            prediction_list.append(
                np.array(
                    self._model.predict_times(
                        candidates=candidates,
                        resources=resources,
                        cost_values=cost_values,
                    )
                )
            )
        return [{"mean": np.mean(prediction_list, axis=0)}]

    def backward_gradient(
        self, input: np.ndarray, head_gradients: List[Dict[str, np.ndarray]]
    ) -> List[np.ndarray]:
        """
        The gradient contribution through the cost model is blocked.

        """
        return [np.zeros_like(input)]

    def predict_mean_current_candidates(self) -> List[np.ndarray]:
        raise NotImplementedError()

    # We currently do not support cost models for the primary metric to be
    # optimized
    def current_best(self) -> List[np.ndarray]:
        raise NotImplementedError()


class CostSurrogateModelFactory(TransformerModelFactory):
    def __init__(self, model: CostModel, fixed_resource: int, num_samples: int = 1):
        """
        The name of the cost metric is `model.cost_metric_name`.

        :param model: CostModel to be wrapped
        :param fixed_resource: c(x, r) is predicted for this resource level r
        :param num_samples: Number of samples drawn in `predict`. Use this for
            random cost models only

        """
        self._model = model
        self._num_samples = num_samples
        self._fixed_resource = None
        self.set_fixed_resource(fixed_resource)

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass

    @property
    def fixed_resource(self) -> int:
        return self._fixed_resource

    def set_fixed_resource(self, resource: int):
        assert resource >= 1, "Must be positive integer"
        self._fixed_resource = resource

    def model(self, state: TuningJobState, fit_params: bool) -> SurrogateModel:
        """
        Models of type :class:`CostModel` do not have hyperparameters to be
        fit, so `fit_params` is ignored here (TODO?).

        """
        self._model.update(state)
        return CostFixedResourceSurrogateModel(
            state=state,
            model=self._model,
            fixed_resource=self._fixed_resource,
            num_samples=self._num_samples,
        )

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/gp_mcmc_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gpr_mcmc import (
    GPRegressionMCMC,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    GaussProcModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
    ConfigurationFilter,
)

logger = logging.getLogger(__name__)


class GaussProcMCMCModelFactory(GaussProcModelFactory):
    def __init__(
        self,
        gpmodel: GPRegressionMCMC,
        active_metric: str = INTERNAL_METRIC_NAME,
        normalize_targets: bool = True,
        profiler: Optional[SimpleProfiler] = None,
        debug_log: Optional[DebugLogPrinter] = None,
        filter_observed_data: Optional[ConfigurationFilter] = None,
        hp_ranges_for_prediction: Optional[HyperparameterRanges] = None,
    ):
        """
        We support pending evaluations via fantasizing. Note that state does
        not contain the fantasy values, but just the pending configs. Fantasy
        values are sampled here.

        We draw one fantasy sample per MCMC sample here. This could be extended
        by sampling >1 fantasy samples for each MCMC sample.

        :param gpmodel: GPRegressionMCMC model
        :param active_metric: Name of the metric to optimize.
        :param normalize_targets: Normalize target values in
            state.trials_evaluations?

        """
        super().__init__(
            gpmodel=gpmodel,
            active_metric=active_metric,
            normalize_targets=normalize_targets,
            profiler=profiler,
            debug_log=debug_log,
            filter_observed_data=filter_observed_data,
            hp_ranges_for_prediction=hp_ranges_for_prediction,
        )

    def get_params(self):
        return dict()  # Model has no parameters to be fit

    def set_params(self, param_dict):
        pass  # Model has no parameters to fit

    def _get_num_fantasy_samples(self) -> int:
        return self._gpmodel.number_samples

    def _num_samples_for_fantasies(self) -> int:
        assert not self._gpmodel.multiple_targets()  # Sanity check
        return 1

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/gp_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, List, Optional, Union
import numpy as np
import logging
from dataclasses import dataclass

from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    TransformerModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_base import (
    BaseSurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    FantasizedPendingEvaluation,
    INTERNAL_METRIC_NAME,
    ConfigurationFilter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_regression import (
    GaussianProcessRegression,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gpr_mcmc import (
    GPRegressionMCMC,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.independent.gpind_model import (
    IndependentGPPerResourceModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    PosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler

logger = logging.getLogger(__name__)


GPModel = Union[
    GaussianProcessRegression,
    GPRegressionMCMC,
    IndependentGPPerResourceModel,
]


class GaussProcSurrogateModel(BaseSurrogateModel):
    """
    Gaussian process surrogate model, where model parameters are either fit by
    marginal likelihood maximization (`GaussianProcessRegression`), or
    integrated out by MCMC sampling (`GPRegressionMCMC`).
    """

    def __init__(
        self,
        state: TuningJobState,
        gpmodel: GPModel,
        fantasy_samples: List[FantasizedPendingEvaluation],
        active_metric: str = None,
        normalize_mean: float = 0.0,
        normalize_std: float = 1.0,
        filter_observed_data: Optional[ConfigurationFilter] = None,
        hp_ranges_for_prediction: Optional[HyperparameterRanges] = None,
    ):
        """
        Both `state` and `gpmodel` are immutable. If parameters of the latter
        are to be fit, this has to be done before.

        `fantasy_samples` contains the sampled (normalized) target values for
        pending configs. Only `active_metric` target values are considered.
        The target values for a pending config are a flat vector. If MCMC is
        used, its length is a multiple of the number of MCMC samples,
        containing the fantasy values for MCMC sample 0, sample 1, ...

        :param state: TuningJobSubState
        :param gpmodel: GPModel. Model parameters must have been fit and/or
            posterior states been computed
        :param fantasy_samples: See above
        :param active_metric: Name of the metric to optimize.
        :param normalize_mean: Mean used to normalize targets
        :param normalize_std: Stddev used to normalize targets
        """
        super().__init__(state, active_metric, filter_observed_data)
        self._gpmodel = gpmodel
        self.mean = normalize_mean
        self.std = normalize_std
        self.fantasy_samples = fantasy_samples
        self._hp_ranges_for_prediction = hp_ranges_for_prediction

    def hp_ranges_for_prediction(self) -> HyperparameterRanges:
        if self._hp_ranges_for_prediction is not None:
            return self._hp_ranges_for_prediction
        else:
            return super().hp_ranges_for_prediction()

    def predict(self, inputs: np.ndarray) -> List[Dict[str, np.ndarray]]:
        predictions_list = []
        for post_mean, post_variance in self._gpmodel.predict(inputs):
            assert post_mean.shape[0] == inputs.shape[0], (
                post_mean.shape,
                inputs.shape,
            )
            assert post_variance.shape == (inputs.shape[0],), (
                post_variance.shape,
                inputs.shape,
            )
            # Undo normalization applied to targets
            mean_denorm = post_mean * self.std + self.mean
            std_denorm = np.sqrt(post_variance) * self.std
            predictions_list.append({"mean": mean_denorm, "std": std_denorm})
        return predictions_list

    def backward_gradient(
        self, input: np.ndarray, head_gradients: List[Dict[str, np.ndarray]]
    ) -> List[np.ndarray]:
        poster_states = self.posterior_states
        assert (
            poster_states is not None
        ), "Cannot run backward_gradient without a posterior state"
        assert len(poster_states) == len(
            head_gradients
        ), "len(posterior_states) = {} != {} = len(head_gradients)".format(
            len(poster_states), len(head_gradients)
        )
        return [
            poster_state.backward_gradient(input, head_gradient, self.mean, self.std)
            for poster_state, head_gradient in zip(poster_states, head_gradients)
        ]

    def does_mcmc(self):
        return isinstance(self._gpmodel, GPRegressionMCMC)

    @property
    def posterior_states(self) -> Optional[List[PosteriorState]]:
        return self._gpmodel.states

    def _current_best_filter_candidates(self, candidates):
        candidates = super()._current_best_filter_candidates(candidates)
        hp_ranges = self.state.hp_ranges
        candidates = hp_ranges.filter_for_last_pos_value(candidates)
        assert candidates, (
            "state.hp_ranges does not contain any candidates "
            + "(labeled or pending) with resource attribute "
            + "'{}' = {}".format(hp_ranges.name_last_pos, hp_ranges.value_for_last_pos)
        )
        return candidates


@dataclass
class InternalCandidateEvaluations:
    features: np.ndarray
    targets: np.ndarray
    mean: float
    std: float


# Note: If state.pending_evaluations is not empty, it must contain entries
# of type FantasizedPendingEvaluation, which contain the fantasy samples. This
# is the case only for internal states.
def get_internal_candidate_evaluations(
    state: TuningJobState,
    active_metric: str,
    normalize_targets: bool,
    num_fantasy_samples: int,
) -> InternalCandidateEvaluations:
    candidates, evaluation_values = state.observed_data_for_metric(
        metric_name=active_metric
    )
    hp_ranges = state.hp_ranges
    features = hp_ranges.to_ndarray_matrix(candidates)
    # Normalize
    # Note: The fantasy values in state.pending_evaluations are sampled
    # from the model fit to normalized targets, so they are already
    # normalized
    targets = np.vstack(evaluation_values).reshape((-1, 1))
    mean = 0.0
    std = 1.0
    if normalize_targets:
        std = max(np.std(targets).item(), 1e-9)
        mean = np.mean(targets).item()
        targets = (targets - mean) / std
    if state.pending_evaluations:
        # In this case, y becomes a matrix, where the observed values are
        # broadcast
        cand_lst = [
            hp_ranges.to_ndarray(config) for config in state.pending_configurations()
        ]
        fanta_lst = []
        for pending_eval in state.pending_evaluations:
            assert isinstance(
                pending_eval, FantasizedPendingEvaluation
            ), "state.pending_evaluations has to contain FantasizedPendingEvaluation"
            fantasies = pending_eval.fantasies[active_metric]
            assert (
                fantasies.size == num_fantasy_samples
            ), "All state.pending_evaluations entries must have length {}".format(
                num_fantasy_samples
            )
            fanta_lst.append(fantasies.reshape((1, -1)))
        targets = np.vstack([targets * np.ones((1, num_fantasy_samples))] + fanta_lst)
        features = np.vstack([features] + cand_lst)
    return InternalCandidateEvaluations(features, targets, mean, std)


class GaussProcModelFactory(TransformerModelFactory):
    def __init__(
        self,
        gpmodel: GPModel,
        active_metric: str,
        normalize_targets: bool = True,
        profiler: Optional[SimpleProfiler] = None,
        debug_log: Optional[DebugLogPrinter] = None,
        filter_observed_data: Optional[ConfigurationFilter] = None,
        no_fantasizing: bool = False,
        hp_ranges_for_prediction: Optional[HyperparameterRanges] = None,
    ):
        """
        We support pending evaluations via fantasizing. Note that state does
        not contain the fantasy values, but just the pending configs. Fantasy
        values are sampled here.

        :param gpmodel: GPModel model
        :param active_metric: Name of the metric to optimize.
        :param normalize_targets: Normalize observed target values?
        :param debug_log: DebugLogPrinter (optional)
        :param filter_observed_data: Filter for observed data before
            computing incumbent
        :param no_fantasizing: If True, pending evaluations in the state are
            simply ignored, fantasizing is not done (not recommended)
        :param hp_ranges_for_prediction: If given, `GaussProcSurrogateModel`
            should use this instead of `state.hp_ranges`

        """
        self._gpmodel = gpmodel
        self.active_metric = active_metric
        self.normalize_targets = normalize_targets
        self._debug_log = debug_log
        self._profiler = profiler
        self._filter_observed_data = filter_observed_data
        self._no_fantasizing = no_fantasizing
        self._hp_ranges_for_prediction = hp_ranges_for_prediction
        self._mean = None
        self._std = None

    @property
    def debug_log(self) -> Optional[DebugLogPrinter]:
        return self._debug_log

    @property
    def profiler(self) -> Optional[SimpleProfiler]:
        return self._profiler

    @property
    def gpmodel(self) -> GPModel:
        return self._gpmodel

    def model(self, state: TuningJobState, fit_params: bool) -> SurrogateModel:
        """
        Parameters of `self._gpmodel` are optimized iff `fit_params`. This
        requires `state` to contain labeled examples.

        If self.state.pending_evaluations is not empty, we proceed as follows:
        - Compute posterior for state without pending evals
        - Draw fantasy values for pending evals
        - Recompute posterior (without fitting)

        """
        if self._debug_log is not None:
            self._debug_log.set_state(state)
        # Compute posterior for state without pending evals
        no_pending_state = state
        if state.pending_evaluations:
            no_pending_state = TuningJobState(
                hp_ranges=state.hp_ranges,
                config_for_trial=state.config_for_trial,
                trials_evaluations=state.trials_evaluations,
                failed_trials=state.failed_trials,
            )
        self._posterior_for_state(
            no_pending_state, fit_params=fit_params, profiler=self._profiler
        )
        if state.pending_evaluations and not self._no_fantasizing:
            # Sample fantasy values for pending evaluations
            state_with_fantasies = self._draw_fantasy_values(state)
            # Compute posterior for state with pending evals
            # Note: profiler is not passed here, this would overwrite the
            # results from the first call
            self._posterior_for_state(
                state_with_fantasies, fit_params=False, profiler=None
            )
            fantasy_samples = state_with_fantasies.pending_evaluations
        else:
            fantasy_samples = []
        return GaussProcSurrogateModel(
            state=state,
            active_metric=self.active_metric,
            gpmodel=self._gpmodel,
            fantasy_samples=fantasy_samples,
            normalize_mean=self._mean,
            normalize_std=self._std,
            filter_observed_data=self._filter_observed_data,
            hp_ranges_for_prediction=self._hp_ranges_for_prediction,
        )

    def _get_num_fantasy_samples(self) -> int:
        raise NotImplementedError()

    def _posterior_for_state(
        self,
        state: TuningJobState,
        fit_params: bool,
        profiler: Optional[SimpleProfiler] = None,
    ):
        """
        Computes posterior for state.
        If fit_params and state.pending_evaluations is empty, we first
        optimize the model parameters.
        If state.pending_evaluations are given, these must be
        FantasizedPendingEvaluations, i.e. the fantasy values must have been
        sampled.
        """
        assert state.num_observed_cases(self.active_metric) > 0, (
            "Cannot compute posterior: state has no labeled datapoints "
            + f"for metric {self.active_metric}"
        )
        internal_candidate_evaluations = get_internal_candidate_evaluations(
            state,
            self.active_metric,
            self.normalize_targets,
            self._get_num_fantasy_samples(),
        )
        features = internal_candidate_evaluations.features
        targets = internal_candidate_evaluations.targets
        assert features.shape[0] == targets.shape[0]
        self._mean = internal_candidate_evaluations.mean
        self._std = internal_candidate_evaluations.std

        fit_params = fit_params and (not state.pending_evaluations)
        data = {"features": features, "targets": targets}
        if not fit_params:
            if self._debug_log is not None:
                logger.info("Recomputing posterior state")
            self._gpmodel.recompute_states(data)
        else:
            if self._debug_log is not None:
                logger.info(f"Fitting surrogate model for {self.active_metric}")
            self._gpmodel.fit(data, profiler=profiler)
        if self._debug_log is not None:
            self._debug_log.set_model_params(self.get_params())
            if not state.pending_evaluations:
                deb_msg = "[GaussProcModelFactory._posterior_for_state]\n"
                deb_msg += "- self.mean = {}\n".format(self._mean)
                deb_msg += "- self.std = {}".format(self._std)
                logger.info(deb_msg)
                self._debug_log.set_targets(targets)
            else:
                num_pending = len(state.pending_evaluations)
                fantasies = targets[-num_pending:, :]
                self._debug_log.set_fantasies(fantasies)

    def _num_samples_for_fantasies(self) -> int:
        raise NotImplementedError()

    def _draw_fantasy_values(self, state: TuningJobState) -> TuningJobState:
        """
        Note: The fantasy values need not be de-normalized, because they are
        only used internally here (e.g., get_internal_candidate_evaluations).

        Note: A complication is that if the sampling methods of _gpmodel
        are called when there are no pending candidates (with fantasies) yet,
        they do return a single sample (instead of num_fantasy_samples). This
        is because GaussianProcessRegression knows about num_fantasy_samples
        only due to the form of the posterior state (bad design!).
        In this case, we draw num_fantasy_samples i.i.d.

        """
        if state.pending_evaluations:
            configs = state.pending_configurations()
            features_new = state.hp_ranges.to_ndarray_matrix(configs)
            num_samples = self._num_samples_for_fantasies()
            # We need joint sampling for >1 new candidates
            num_candidates = len(configs)
            sample_func = (
                self._gpmodel.sample_joint
                if num_candidates > 1
                else self._gpmodel.sample_marginals
            )
            targets_new = sample_func(
                features_test=features_new, num_samples=num_samples
            ).reshape((num_candidates, -1))
            new_pending = [
                FantasizedPendingEvaluation(
                    trial_id=ev.trial_id,
                    resource=ev.resource,
                    fantasies={self.active_metric: y_new.reshape((1, -1))},
                )
                for ev, y_new in zip(state.pending_evaluations, targets_new)
            ]
        else:
            new_pending = []
        return TuningJobState(
            hp_ranges=state.hp_ranges,
            config_for_trial=state.config_for_trial,
            trials_evaluations=state.trials_evaluations,
            failed_trials=state.failed_trials,
            pending_evaluations=new_pending,
        )

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler

        if isinstance(self._gpmodel, IndependentGPPerResourceModel):
            assert isinstance(scheduler, HyperbandScheduler), (
                "gpmodel of type IndependentGPPerResourceModel requires "
                + "HyperbandScheduler scheduler"
            )
            # Likelihood of internal model still has to be created (depends on
            # rung levels of scheduler). Note that `max_t` must be included
            max_t = scheduler.max_t
            if scheduler.rung_levels[-1] == max_t:
                rung_levels = scheduler.rung_levels
            else:
                rung_levels = scheduler.rung_levels + [max_t]
            self._gpmodel.create_likelihood(rung_levels)


class GaussProcEmpiricalBayesModelFactory(GaussProcModelFactory):
    def __init__(
        self,
        gpmodel: GPModel,
        num_fantasy_samples: int,
        active_metric: str = INTERNAL_METRIC_NAME,
        normalize_targets: bool = True,
        profiler: Optional[SimpleProfiler] = None,
        debug_log: Optional[DebugLogPrinter] = None,
        filter_observed_data: Optional[ConfigurationFilter] = None,
        no_fantasizing: bool = False,
        hp_ranges_for_prediction: Optional[HyperparameterRanges] = None,
    ):
        """
        We support pending evaluations via fantasizing. Note that state does
        not contain the fantasy values, but just the pending configs. Fantasy
        values are sampled here.

        :param gpmodel: GaussianProcessRegression model
        :param num_fantasy_samples: See above
        :param active_metric: Name of the metric to optimize.
        :param normalize_targets: Normalize target values in
            state.candidate_evaluations?

        """
        assert num_fantasy_samples > 0
        super().__init__(
            gpmodel=gpmodel,
            active_metric=active_metric,
            normalize_targets=normalize_targets,
            profiler=profiler,
            debug_log=debug_log,
            filter_observed_data=filter_observed_data,
            no_fantasizing=no_fantasizing,
            hp_ranges_for_prediction=hp_ranges_for_prediction,
        )
        self.num_fantasy_samples = num_fantasy_samples

    def get_params(self):
        return self._gpmodel.get_params()

    def set_params(self, param_dict):
        self._gpmodel.set_params(param_dict)

    def _get_num_fantasy_samples(self) -> int:
        return self.num_fantasy_samples

    def _num_samples_for_fantasies(self) -> int:
        # Special case (see header comment): If the current posterior state
        # does not contain pending candidates (no fantasies), we sample
        # `num_fantasy_samples` times i.i.d.
        return 1 if self._gpmodel.multiple_targets() else self.num_fantasy_samples

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/gpiss_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, List, Optional
import numpy as np
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    TransformerModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_base import (
    BaseSurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.config_ext import (
    ExtendedConfiguration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.gpiss_model import (
    GaussianProcessLearningCurveModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.issm import (
    prepare_data,
    prepare_data_with_pending,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.posterior_state import (
    GaussProcAdditivePosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    ConfigurationFilter,
    FantasizedPendingEvaluation,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler

logger = logging.getLogger(__name__)


class GaussProcAdditiveSurrogateModel(BaseSurrogateModel):
    def __init__(
        self,
        state: TuningJobState,
        gpmodel: GaussianProcessLearningCurveModel,
        fantasy_samples: List[FantasizedPendingEvaluation],
        active_metric: str,
        filter_observed_data: Optional[ConfigurationFilter] = None,
        normalize_mean: float = 0.0,
        normalize_std: float = 1.0,
    ):
        """
        Gaussian Process additive surrogate model, where model parameters are
        fit by marginal likelihood maximization.

        Note: `predict_mean_current_candidates` calls `predict` for all
        observed and pending extended configs. This may not be exactly
        correct, because `predict` is not meant to be used for configs
        which have observations (it IS correct at r = r_max).

        `fantasy_samples` contains the sampled (normalized) target values for
        pending configs. Only `active_metric` target values are considered.
        The target values for a pending config are a flat vector.

        :param state: TuningJobSubState
        :param gpmodel: GaussianProcessLearningCurveModel
        :param fantasy_samples: See above
        :param active_metric: See parent class
        :param filter_observed_data: See parent class
        :param normalize_mean: Mean used to normalize targets
        :param normalize_std: Stddev used to normalize targets

        """
        super().__init__(state, active_metric, filter_observed_data)
        self._gpmodel = gpmodel
        self.mean = normalize_mean
        self.std = normalize_std
        self.fantasy_samples = fantasy_samples

    def predict(self, inputs: np.ndarray) -> List[Dict[str, np.ndarray]]:
        """
        Input features `inputs` are w.r.t. extended configs (x, r).

        :param inputs: Input features
        :return: Predictive means, stddevs
        """
        predictions_list = []
        for post_mean, post_variance in self._gpmodel.predict(inputs):
            assert post_mean.shape[0] == inputs.shape[0], (
                post_mean.shape,
                inputs.shape,
            )
            assert post_variance.shape == (inputs.shape[0],), (
                post_variance.shape,
                inputs.shape,
            )
            # Undo normalization applied to targets
            mean_denorm = post_mean * self.std + self.mean
            std_denorm = np.sqrt(post_variance) * self.std
            predictions_list.append({"mean": mean_denorm, "std": std_denorm})
        return predictions_list

    def backward_gradient(
        self, input: np.ndarray, head_gradients: List[Dict[str, np.ndarray]]
    ) -> List[np.ndarray]:
        poster_states = self.posterior_states
        assert (
            poster_states is not None
        ), "Cannot run backward_gradient without a posterior state"
        assert len(poster_states) == len(
            head_gradients
        ), "len(posterior_states) = {} != {} = len(head_gradients)".format(
            len(poster_states), len(head_gradients)
        )
        return [
            poster_state.backward_gradient(input, head_gradient, self.mean, self.std)
            for poster_state, head_gradient in zip(poster_states, head_gradients)
        ]

    def does_mcmc(self):
        return False

    @property
    def posterior_states(self) -> Optional[List[GaussProcAdditivePosteriorState]]:
        return self._gpmodel.states


class GaussProcAdditiveModelFactory(TransformerModelFactory):
    def __init__(
        self,
        gpmodel: GaussianProcessLearningCurveModel,
        num_fantasy_samples: int,
        active_metric: str,
        config_space_ext: ExtendedConfiguration,
        normalize_targets: bool = False,
        profiler: Optional[SimpleProfiler] = None,
        debug_log: Optional[DebugLogPrinter] = None,
        filter_observed_data: Optional[ConfigurationFilter] = None,
    ):
        """
        If `num_fantasy_samples > 0`, we draw this many fantasy targets
        independently, while each sample is dependent over all pending
        evaluations. If `num_fantasy_samples == 0`, pending evaluations
        in `state` are ignored.

        :param gpmodel: GaussianProcessLearningCurveModel
        :param num_fantasy_samples: See above
        :param active_metric: Name of the metric to optimize.
        :param config_space_ext: ExtendedConfiguration
        :param normalize_targets: Normalize observed target values?
        :param debug_log: DebugLogPrinter (optional)
        :param filter_observed_data: Filter for observed data before
            computing incumbent

        """
        self._gpmodel = gpmodel
        self.active_metric = active_metric
        r_min, r_max = config_space_ext.resource_attr_range
        assert (
            0 < r_min < r_max
        ), f"r_min = {r_min}, r_max = {r_max}: Need 0 < r_min < r_max"
        assert (
            num_fantasy_samples >= 0
        ), f"num_fantasy_samples = {num_fantasy_samples}, must be non-negative int"
        self.num_fantasy_samples = num_fantasy_samples
        self._config_space_ext = config_space_ext
        self._debug_log = debug_log
        self._profiler = profiler
        self._filter_observed_data = filter_observed_data
        self.normalize_targets = normalize_targets

    @property
    def debug_log(self) -> Optional[DebugLogPrinter]:
        return self._debug_log

    @property
    def profiler(self) -> Optional[SimpleProfiler]:
        return self._profiler

    def get_params(self):
        return self._gpmodel.get_params()

    def set_params(self, param_dict):
        self._gpmodel.set_params(param_dict)

    def model(self, state: TuningJobState, fit_params: bool) -> SurrogateModel:
        assert state.num_observed_cases(self.active_metric) > 0, (
            "Cannot compute posterior: state has no labeled datapoints "
            + f"for metric {self.active_metric}"
        )
        if self._debug_log is not None:
            self._debug_log.set_state(state)
        do_fantasizing = state.pending_evaluations and self.num_fantasy_samples > 0

        # [1] Fit model and compute posterior state, ignoring pending evals
        data = prepare_data(
            state,
            self._config_space_ext,
            self.active_metric,
            normalize_targets=self.normalize_targets,
            do_fantasizing=False,
        )
        if fit_params:
            logger.info(f"Fitting surrogate model for {self.active_metric}")
            self._gpmodel.fit(data, profiler=self._profiler)
        elif not do_fantasizing:
            # Only if part below is skipped
            logger.info("Recomputing posterior state")
            self._gpmodel.recompute_states(data)
        if self._debug_log is not None:
            self._debug_log.set_model_params(self.get_params())
        if self.normalize_targets:
            extra_kwargs = {
                "normalize_mean": data["mean_targets"],
                "normalize_std": data["std_targets"],
            }
        else:
            extra_kwargs = dict()

        # [2] Fantasizing for pending evaluations (optional)
        if do_fantasizing:
            # Sample fantasy values for pending evaluations
            logger.info("Sampling fantasy target values for pending evaluations")
            state_with_fantasies = self._draw_fantasy_values(state)
            fantasy_samples = state_with_fantasies.pending_evaluations
            # Recompute posterior state with fantasy samples
            logger.info("Recomputing posterior state with fantasy targets")
            data = prepare_data(
                state=state_with_fantasies,
                config_space_ext=self._config_space_ext,
                active_metric=self.active_metric,
                normalize_targets=self.normalize_targets,
                do_fantasizing=True,
            )
            self._gpmodel.recompute_states(data)
        else:
            fantasy_samples = []

        return GaussProcAdditiveSurrogateModel(
            state=state,
            gpmodel=self._gpmodel,
            fantasy_samples=fantasy_samples,
            active_metric=self.active_metric,
            filter_observed_data=self._filter_observed_data,
            **extra_kwargs,
        )

    def model_for_fantasy_samples(
        self, state: TuningJobState, fantasy_samples: List[FantasizedPendingEvaluation]
    ) -> SurrogateModel:
        """
        Same as `model` with `fit_params=False`, but `fantasy_samples` are
        passed in, rather than sampled here.

        :param state: See `model`
        :param fantasy_samples: See above
        :return: See `model`

        """
        assert state.num_observed_cases(self.active_metric) > 0, (
            "Cannot compute posterior: state has no labeled datapoints "
            + f"for metric {self.active_metric}"
        )
        assert state.pending_evaluations and self.num_fantasy_samples > 0

        # Recompute posterior state with fantasy samples
        state_with_fantasies = TuningJobState(
            hp_ranges=state.hp_ranges,
            config_for_trial=state.config_for_trial,
            trials_evaluations=state.trials_evaluations,
            failed_trials=state.failed_trials,
            pending_evaluations=fantasy_samples,
        )
        # Recompute posterior state with fantasy samples
        data = prepare_data(
            state=state_with_fantasies,
            config_space_ext=self._config_space_ext,
            active_metric=self.active_metric,
            normalize_targets=self.normalize_targets,
            do_fantasizing=True,
        )
        self._gpmodel.recompute_states(data)
        if self.normalize_targets:
            extra_kwargs = {
                "normalize_mean": data["mean_targets"],
                "normalize_std": data["std_targets"],
            }
        else:
            extra_kwargs = dict()

        return GaussProcAdditiveSurrogateModel(
            state=state,
            gpmodel=self._gpmodel,
            fantasy_samples=fantasy_samples,
            active_metric=self.active_metric,
            filter_observed_data=self._filter_observed_data,
            **extra_kwargs,
        )

    def _draw_fantasy_values(self, state: TuningJobState) -> TuningJobState:
        """
        Note: Fantasized target values are not de-normalized, because they
        are used internally only (see `prepare_data` with
        `do_fantasizing=True`).

        :param state: State with pending evaluations without fantasies
        :return: Copy of `state`, where `pending_evaluations` contains
            fantasized target values

        """
        assert self.num_fantasy_samples > 0
        # Fantasies are drawn in sequential chunks, one trial with pending
        # evaluations at a time.
        data_nopending, data_pending = prepare_data_with_pending(
            state=state,
            config_space_ext=self._config_space_ext,
            active_metric=self.active_metric,
            normalize_targets=self.normalize_targets,
        )
        if not data_nopending["configs"]:
            # It can happen that all trials with observed data also have
            # pending evaluations. This is possible only at the very start,
            # as long as no trial has been stopped or paused.
            # In this case, we find the trial with the largest number of
            # observed targets and remove its pending evaluations, so
            # `data_nopending` gets one entry. It is not possible to compute
            # a posterior state without any data, so handling this case
            # correctly would be very tedious).
            assert data_pending[
                "configs"
            ], "State is empty, cannot do posterior inference:\n" + str(state)
            names = ("configs", "targets", "trial_ids")
            elem = {k: data_pending[k].pop(0) for k in names}
            for k, v in elem.items():
                data_nopending[k] = [v]
            k = "features"
            all_features = data_pending[k]
            data_nopending[k] = all_features[0].reshape((1, -1))
            data_pending[k] = all_features[1:, :]
            logger.info(
                "All trials currently have pending evaluations. In order to "
                "sample fantasy targets, I'll remove pending evaluations "
                f"from trial_id {elem['trial_ids']} (which has "
                f"{elem['targets'].size} observations)"
            )
        # Start with posterior state, conditioned on data from trials without
        # pending evaluations
        self._gpmodel.recompute_states(data_nopending)
        poster_state_nopending = self._gpmodel.states[0]
        # Loop over trials with pending evaluations: For each trial, we sample
        # fantasy targets given observed ones, then update `poster_state` by
        # conditioning on both. This ensures we obtain a joint sample (the
        # ordering of trials does not matter). For the application here, we
        # do not need the final `poster_state`.
        all_fantasy_targets = []
        for sample_it in range(self.num_fantasy_samples):
            fantasy_targets, _ = poster_state_nopending.sample_and_update_for_pending(
                data_pending,
                sample_all_nonobserved=False,
                random_state=self._gpmodel.random_state,
            )
            for pos, fantasies in enumerate(fantasy_targets):
                if sample_it == 0:
                    all_fantasy_targets.append([fantasies])
                else:
                    all_fantasy_targets[pos].append(fantasies)
        # Convert into `FantasizedPendingEvaluation`
        r_min = self._config_space_ext.resource_attr_range[0]
        pending_evaluations_with_fantasies = []
        for trial_id, targets, fantasies in zip(
            data_pending["trial_ids"], data_pending["targets"], all_fantasy_targets
        ):
            n_observed = targets.size
            n_pending = fantasies[0].size
            start = r_min + n_observed
            resources = list(range(start, start + n_pending))
            fantasy_matrix = np.hstack(v.reshape((-1, 1)) for v in fantasies)
            assert fantasy_matrix.shape == (n_pending, self.num_fantasy_samples)
            for resource, fantasy in zip(resources, fantasy_matrix):
                pending_evaluations_with_fantasies.append(
                    FantasizedPendingEvaluation(
                        trial_id=trial_id,
                        fantasies={self.active_metric: fantasy},
                        resource=resource,
                    )
                )
        # Return new state, with `pending_evaluations` replaced
        return TuningJobState(
            hp_ranges=state.hp_ranges,
            config_for_trial=state.config_for_trial,
            trials_evaluations=state.trials_evaluations,
            failed_trials=state.failed_trials,
            pending_evaluations=pending_evaluations_with_fantasies,
        )

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/kernel_factory.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    KernelFunction,
    Matern52,
    ExponentialDecayResourcesKernelFunction,
    ExponentialDecayResourcesMeanFunction,
    FreezeThawKernelFunction,
    FreezeThawMeanFunction,
    CrossValidationMeanFunction,
    CrossValidationKernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.warping import (
    WarpedKernel,
    Warping,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
)


SUPPORTED_RESOURCE_MODELS = (
    "exp-decay-sum",
    "exp-decay-combined",
    "exp-decay-delta1",
    "freeze-thaw",
    "matern52",
    "matern52-res-warp",
    "cross-validation",
)


def resource_kernel_factory(
    name: str, kernel_x: KernelFunction, mean_x: MeanFunction, **kwargs
) -> (KernelFunction, MeanFunction):
    """
    Given kernel function kernel_x and mean function mean_x over config x,
    create kernel and mean functions over (x, r), where r is the resource
    attribute (nonnegative scalar, usually in [0, 1]).

    :param name: Selects resource kernel type
    :param kernel_x: Kernel function over configs x
    :param mean_x: Mean function over configs x
    :param kwargs: Extra arguments (optional)
    :return: res_kernel, res_mean, both over (x, r)

    """
    if name == "matern52":
        res_kernel = Matern52(dimension=kernel_x.dimension + 1, ARD=True)
        res_mean = mean_x
    elif name == "matern52-res-warp":
        # Warping on resource dimension (last one)
        dim_x = kernel_x.dimension
        res_warping = Warping(dimension=dim_x + 1, index_to_range={dim_x: (0.0, 1.0)})
        res_kernel = WarpedKernel(
            kernel=Matern52(dimension=dim_x + 1, ARD=True), warping=res_warping
        )
        res_mean = mean_x
    elif name == "freeze-thaw":
        res_kernel = FreezeThawKernelFunction(kernel_x, mean_x)
        res_mean = FreezeThawMeanFunction(kernel=res_kernel)
    elif name == "cross-validation":
        # `CrossValidationKernelFunction` needs two kernels, one over the main
        # effect f(x), the other over the residuals g_k(x). We use `kernel_x`,
        # `mean_x` for the former, and create a `Matern52` kernel (no ARD) here
        # for the latter
        num_folds = kwargs.get("num_folds")
        assert (
            num_folds is not None
        ), f"Resource kenel '{name}' needs num_folds argument"
        dim_x = kernel_x.dimension
        kernel_residual = Matern52(dimension=dim_x, ARD=False)
        res_kernel = CrossValidationKernelFunction(
            kernel_main=kernel_x,
            kernel_residual=kernel_residual,
            mean_main=mean_x,
            num_folds=num_folds,
        )
        res_mean = CrossValidationMeanFunction(kernel=res_kernel)
    else:
        if name == "exp-decay-sum":
            delta_fixed_value = 0.0
        elif name == "exp-decay-combined":
            delta_fixed_value = None
        elif name == "exp-decay-delta1":
            delta_fixed_value = 1.0
        else:
            raise AssertionError("name = '{}' not supported".format(name))
        res_kernel = ExponentialDecayResourcesKernelFunction(
            kernel_x, mean_x, delta_fixed_value=delta_fixed_value
        )
        res_mean = ExponentialDecayResourcesMeanFunction(kernel=res_kernel)

    return res_kernel, res_mean

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/meanstd_acqfunc.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
from typing import Dict, Tuple, Optional, Set, List
from dataclasses import dataclass
import itertools

from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateModel,
    AcquisitionFunction,
    SurrogateOutputModel,
    assign_active_metric,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    dictionarize_objective,
)


# Type for predictions from (potentially) multiple models
# `output_to_predictions[name]` is a list of dicts, one entry for each
# MCMC sample (list is size 1 if no MCMC), see also `predict` of
# :class:`SurrogateModel`.
# Note: List sizes of different entries can be different. MCMC averaging
# is done over the Cartesian product of these lists.
PredictionsPerOutput = Dict[str, List[Dict[str, np.ndarray]]]

SamplePredictionsPerOutput = Dict[str, Dict[str, np.ndarray]]


@dataclass
class HeadWithGradient:
    """
    `gradient` maps each output model to a dict of head gradients, whose keys
    are those used by `predict` (e.g., `mean`, `std`)
    """

    hval: np.ndarray
    gradient: SamplePredictionsPerOutput


class CurrentBestProvider:
    """
    Helper class for :class:`MeanStdAcquisitionFunction`.
    The `current_best` values required in `compute_acq` and
    `compute_acq_with_gradient` may depend on the MCMC sample index for each
    model (if none of the models use MCMC, this index is always
    (0, 0, ..., 0)).

    """

    def __call__(self, positions: Tuple[int, ...]) -> Optional[np.ndarray]:
        raise NotImplementedError


class NoneCurrentBestProvider(CurrentBestProvider):
    def __call__(self, positions: Tuple[int, ...]) -> Optional[np.ndarray]:
        return None


class ActiveMetricCurrentBestProvider(CurrentBestProvider):
    """
    Default implementation in which `current_best` depends on the
    active metric only.

    """

    def __init__(self, active_metric_current_best: List[np.ndarray]):
        self._active_metric_current_best = [
            v.reshape((1, -1)) for v in active_metric_current_best
        ]
        self._constant_list = len(active_metric_current_best) == 1

    def __call__(self, positions: Tuple[int, ...]) -> Optional[np.ndarray]:
        pos = positions[0] if not self._constant_list else 0
        return self._active_metric_current_best[pos]


class MeanStdAcquisitionFunction(AcquisitionFunction):
    """
    Base class for standard acquisition functions which depend on predictive
    mean and stddev. Subclasses have to implement the head and its derivatives
    w.r.t. mean and std:

        f(x, model) = h(mean, std, model.current_best())

    If model is a SurrogateModel, then active_metric is ignored. If model is a Dict mapping output names to models,
    then active_metric must be given.

    NOTE that acquisition functions will always be *minimized*!

    """

    def __init__(self, model: SurrogateOutputModel, active_metric: str = None):
        super().__init__(model, active_metric)
        if isinstance(model, SurrogateModel):
            # Ignore active_metric
            model = dictionarize_objective(model)
        assert isinstance(model, Dict)
        self.model = model
        self.model_output_names = sorted(model.keys())
        self.active_metric = assign_active_metric(model, active_metric)
        output_names = list(model.keys())
        active_pos = output_names.index(self.active_metric)
        # active_metric to come first
        self.model_output_names = (
            [self.active_metric]
            + output_names[:active_pos]
            + output_names[(active_pos + 1) :]
        )
        self._check_keys_predict_of_models()
        self._current_bests = None

    def _output_to_keys_predict(self) -> Dict[str, Set[str]]:
        """
        Required `keys_predict` for each output model. The default requires
        each output model to return 'mean' and 'std'.

        """
        mean_and_std = {"mean", "std"}
        return {k: mean_and_std for k in self.model_output_names}

    def _check_keys_predict_of_models(self):
        for output_name, required_keys in self._output_to_keys_predict().items():
            keys_predict = self.model[output_name].keys_predict()
            for k in required_keys:
                assert k in keys_predict, (
                    f"output_name {output_name}: Required key {k} not "
                    + "provided by predictions of surrogate model"
                )

    def _get_num_fantasies(self, output_to_predictions: PredictionsPerOutput) -> int:
        """
        If fantasizing is used, the number of fantasy samples must be
        the same over all models. Even if this number is >1, a model
        may always not use fantasizing, in which case its mean predictions
        are broadcasted.

        :param output_to_predictions:
        :return: Number of fantasies
        """
        num_fantasy_values = set()
        for predictions in output_to_predictions.values():
            for prediction in predictions:
                assert "mean" in prediction  # Sanity check
                means = prediction["mean"]
                num_fantasies = means.shape[1] if means.ndim == 2 else 1
                num_fantasy_values.add(num_fantasies)
        max_size = 2 if (1 in num_fantasy_values) else 1
        assert (
            len(num_fantasy_values) <= max_size
        ), "Predictive means have inconsistent numbers of fantasies: " + str(
            num_fantasy_values
        )
        return max(list(num_fantasy_values))

    def _get_current_bests(self, model: SurrogateOutputModel) -> CurrentBestProvider:
        current_bests = self._current_bests
        default_model = model is self.model
        if (not default_model) or current_bests is None:
            if self._head_needs_current_best():
                current_bests = self._get_current_bests_internal(model)
            else:
                current_bests = NoneCurrentBestProvider()
            if default_model:
                self._current_bests = current_bests
        return current_bests

    def _get_current_bests_internal(
        self, model: SurrogateOutputModel
    ) -> CurrentBestProvider:
        """
        Implements default where `current_best` only depends on the model for
        `active_metric`. To be overwritten by child classes where this does not
        hold.

        Note: The resulting current_bests is redetermined every time, since
        `model` may change.

        """
        active_metric_current_best = model[self.active_metric].current_best()
        return ActiveMetricCurrentBestProvider(active_metric_current_best)

    def compute_acq(
        self, inputs: np.ndarray, model: Optional[SurrogateOutputModel] = None
    ) -> np.ndarray:
        if model is None:
            model = self.model
        elif isinstance(model, SurrogateModel):
            model = dictionarize_objective(model)
        if inputs.ndim == 1:
            inputs = inputs.reshape((1, -1))
        output_to_predictions = self._map_outputs_to_predictions(model, inputs)
        current_bests = self._get_current_bests(model)

        # Reshaping of predictions to accomodate _compute_head.
        for preds_for_samples in output_to_predictions.values():
            for prediction in preds_for_samples:
                for k in prediction.keys():
                    v = prediction[k]
                    if (k == "mean" and v.ndim == 1) or k == "std":
                        prediction[k] = v.reshape((-1, 1))

        # MCMC average is product over lists coming from each model. The
        # resulting function values are stored in a flat list.
        fvals_list = []
        # We also need the position in each list in order to select
        # current_best
        list_values = [
            list(enumerate(output_to_predictions[name]))
            for name in self.model_output_names
        ]
        for preds_and_pos in itertools.product(*list_values):
            positions, predictions = zip(*preds_and_pos)
            output_to_preds = dict(zip(self.model_output_names, predictions))
            current_best = current_bests(positions)
            # Compute the acquisition function value
            fvals = self._compute_head(output_to_preds, current_best)
            fvals_list.append(fvals.reshape((-1,)))

        return np.mean(fvals_list, axis=0)

    @staticmethod
    def _add_head_gradients(
        grad1: Dict[str, np.ndarray], grad2: Optional[Dict[str, np.ndarray]]
    ) -> Dict[str, np.ndarray]:
        if grad2 is None:
            return grad1
        else:
            return {k: v + grad2[k] for k, v in grad1.items()}

    def compute_acq_with_gradient(
        self, input: np.ndarray, model: Optional[SurrogateOutputModel] = None
    ) -> (float, np.ndarray):
        if model is None:
            model = self.model
        if isinstance(model, SurrogateModel):
            model = dictionarize_objective(model)
        output_to_predictions = self._map_outputs_to_predictions(
            model, input.reshape(1, -1)
        )
        current_bests = self._get_current_bests(model)

        # Reshaping of predictions to accomodate _compute_head_and_gradient. We
        # also store the original shapes, which are needed below
        shapes = dict()
        for output_name, preds_for_samples in output_to_predictions.items():
            shapes[output_name] = {k: v.shape for k, v in preds_for_samples[0].items()}
            for prediction in preds_for_samples:
                for k in prediction.keys():
                    prediction[k] = prediction[k].reshape((-1,))

        # MCMC average is product over lists coming from each model. We need to
        # accumulate head gradients w.r.t. each model, each of which being a
        # list over MCMC samples from that model (size 1 if no MCMC)
        fvals_list = []
        # For accumulation of head gradients, we also need to have the
        # position in each list
        list_values = [
            list(enumerate(output_to_predictions[name]))
            for name in self.model_output_names
        ]
        head_gradient = {
            name: [None] * len(predictions)
            for name, predictions in output_to_predictions.items()
        }
        for preds_and_pos in itertools.product(*list_values):
            positions, predictions = zip(*preds_and_pos)
            output_to_preds = dict(zip(self.model_output_names, predictions))
            current_best = current_bests(positions)
            head_result = self._compute_head_and_gradient(output_to_preds, current_best)
            fvals_list.append(head_result.hval)
            for output_name, pos in zip(self.model_output_names, positions):
                head_gradient[output_name][pos] = self._add_head_gradients(
                    head_result.gradient[output_name], head_gradient[output_name][pos]
                )

        # Sum up the gradients coming from each output model
        fval = np.mean(fvals_list)
        num_total = len(fvals_list)
        gradient = 0.0
        for output_name, output_model in model.items():
            # Reshape head gradients so they have the same shape as corresponding
            # predictions. This is required for `backward_gradient` to work.
            shp = shapes[output_name]
            head_grad = [
                {k: v.reshape(shp[k]) for k, v in orig_grad.items()}
                for orig_grad in head_gradient[output_name]
            ]
            # Gradients are computed by the model
            gradient_list = output_model.backward_gradient(input, head_grad)
            # Average over MCMC samples
            output_gradient = np.sum(gradient_list, axis=0) / num_total
            gradient += output_gradient
        return fval, gradient

    def _map_outputs_to_predictions(
        self, model: SurrogateOutputModel, inputs: np.ndarray
    ) -> PredictionsPerOutput:
        return {
            output_name: output_model.predict(inputs)
            for output_name, output_model in model.items()
        }

    def _extract_mean_and_std(
        self, output_to_predictions: SamplePredictionsPerOutput, metric: str = None
    ) -> (np.ndarray, np.ndarray):
        if metric is None:
            metric = self.active_metric
        predictions = output_to_predictions[metric]
        return predictions["mean"], predictions["std"]

    def _head_needs_current_best(self) -> bool:
        """
        :return: Is the current_best argument in _compute_head needed?
        """
        raise NotImplementedError

    def _compute_head(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> np.ndarray:
        """
        If mean has nf > 1 columns, both std and current_best are supposed to
        be broadcasted, and the return value is averaged over this dimension.

        :param output_to_predictions: Dictionary mapping each output to a
            dict containing predictive moments, keys as in
            `_output_to_keys_predict`. 'mean' has shape (n, nf), 'std' has
            shape (n, 1)
        :param current_best: Incumbent, shape (1, nf)
        :return: h(predictions, current_best), shape (n,)

        """
        raise NotImplementedError

    def _compute_head_and_gradient(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> HeadWithGradient:
        """
        Computes both head value and head gradients, for a single input.

        :param: output_to_predictions: Dictionary mapping each output to a
            dict containing predictive moments, keys as in
            `_output_to_keys_predict`. 'mean' has shape (nf,), 'std' has shape
            (1,)
        :param current_best: Incumbent, shape (nf,)
        :return: HeadWithGradient containing hval and head gradients for
            each output model. All HeadWithGradient values have the same
            shape as the corresponding predictions

        """
        raise NotImplementedError

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/meanstd_acqfunc_impl.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
from typing import Dict, Optional, Set, List, Tuple
import logging
from scipy.stats import norm
import itertools

from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc import (
    MeanStdAcquisitionFunction,
    HeadWithGradient,
    SamplePredictionsPerOutput,
    CurrentBestProvider,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_base import (
    BaseSurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateOutputModel,
    SurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.density import (
    get_quantiles,
)

logger = logging.getLogger(__name__)


MIN_COST = 1e-12  # For numerical stability when dividing EI / cost
MIN_STD_CONSTRAINT = (
    1e-12  # For numerical stability when computing the constraint probability in CEI
)


def _extract_active_and_secondary_metric(model_output_names, active_metric):
    """
    Returns the active metric and the secondary metric (such as the cost or constraint metric) from model_output_names.
    """

    assert len(model_output_names) == 2, (
        f"The model should consist of exactly 2 outputs, "
        f"while the current outputs are {model_output_names}"
    )
    assert active_metric in model_output_names, (
        f"{active_metric} is not a valid metric. "
        f"The metric name must match one of the following metrics "
        f"in the model output: {model_output_names}"
    )
    if model_output_names[0] == active_metric:
        secondary_metric = model_output_names[1]
    else:
        secondary_metric = model_output_names[0]
    logger.debug(
        f"There are two metrics in the output: {model_output_names}. "
        f"The metric to optimize was set to '{active_metric}'. "
        f"The secondary metric is assumed to be '{secondary_metric}'"
    )
    return active_metric, secondary_metric


def _postprocess_gradient(grad: np.ndarray, nf: int) -> np.ndarray:
    if nf > 1:
        assert grad.size == nf  # Sanity check
        return grad / nf
    else:
        return np.mean(grad, keepdims=True)


class EIAcquisitionFunction(MeanStdAcquisitionFunction):
    """
    Minus expected improvement acquisition function
    (minus because the convention is to always minimize acquisition functions)

    """

    def __init__(
        self,
        model: SurrogateOutputModel,
        active_metric: str = None,
        jitter: float = 0.01,
    ):
        assert isinstance(model, SurrogateModel)
        super().__init__(model, active_metric)
        self.jitter = jitter

    def _head_needs_current_best(self) -> bool:
        return True

    def _compute_head(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> np.ndarray:
        assert current_best is not None
        means, stds = self._extract_mean_and_std(output_to_predictions)

        # phi, Phi is PDF and CDF of Gaussian
        phi, Phi, u = get_quantiles(self.jitter, current_best, means, stds)
        return np.mean((-stds) * (u * Phi + phi), axis=1)

    def _compute_head_and_gradient(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> HeadWithGradient:
        assert current_best is not None
        mean, std = self._extract_mean_and_std(output_to_predictions)
        nf_mean = mean.size
        assert current_best.size == nf_mean

        # phi, Phi is PDF and CDF of Gaussian
        phi, Phi, u = get_quantiles(self.jitter, current_best, mean, std)
        f_acqu = std * (u * Phi + phi)
        dh_dmean = _postprocess_gradient(Phi, nf=nf_mean)
        dh_dstd = _postprocess_gradient(-phi, nf=1)
        return HeadWithGradient(
            hval=-np.mean(f_acqu),
            gradient={self.active_metric: dict(mean=dh_dmean, std=dh_dstd)},
        )


class LCBAcquisitionFunction(MeanStdAcquisitionFunction):
    """
    Lower confidence bound (LCB) acquisition function:

        h(mean, std) = mean - kappa * std

    """

    def __init__(
        self, model: SurrogateOutputModel, kappa: float, active_metric: str = None
    ):
        super().__init__(model, active_metric)
        assert isinstance(model, SurrogateModel)
        assert kappa > 0, "kappa must be positive"
        self.kappa = kappa

    def _head_needs_current_best(self) -> bool:
        return False

    def _compute_head(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> np.ndarray:
        means, stds = self._extract_mean_and_std(output_to_predictions)
        return np.mean(means - stds * self.kappa, axis=1)

    def _compute_head_and_gradient(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> HeadWithGradient:
        mean, std = self._extract_mean_and_std(output_to_predictions)
        nf_mean = mean.size

        dh_dmean = np.ones_like(mean) / nf_mean
        dh_dstd = (-self.kappa) * np.ones_like(std)
        return HeadWithGradient(
            hval=np.mean(mean - std * self.kappa),
            gradient={self.active_metric: dict(mean=dh_dmean, std=dh_dstd)},
        )


class EIpuAcquisitionFunction(MeanStdAcquisitionFunction):
    """
    Minus cost-aware expected improvement acquisition function.

    This is defined as

        EIpu(x) = EI(x) / power(cost(x), exponent_cost),

    where EI(x) is expected improvement, cost(x) is the predictive mean of
    a cost model, and `exponent_cost` is an exponent in (0, 1].

    `exponent_cost` scales the influence of the cost term on the acquisition
    function. See also:

        Lee etal.
        Cost-aware Bayesian Optimization
        https://arxiv.org/abs/2003.10870

    Note: two metrics are expected in the model output: the main objective and the cost.
    The main objective needs to be indicated as active_metric when initializing EIpuAcquisitionFunction.
    The cost is automatically assumed to be the other metric.

    """

    def __init__(
        self,
        model: SurrogateOutputModel,
        active_metric: str = None,
        exponent_cost: float = 1.0,
        jitter: float = 0.01,
    ):
        super().__init__(model, active_metric)
        assert (
            0 < exponent_cost <= 1
        ), f"exponent_cost = {exponent_cost} must lie in (0, 1]"
        self.jitter = jitter
        self.exponent_cost = exponent_cost
        self.active_metric, self.cost_metric = _extract_active_and_secondary_metric(
            self.model_output_names, active_metric
        )

    def _head_needs_current_best(self) -> bool:
        return True

    def _output_to_keys_predict(self) -> Dict[str, Set[str]]:
        """
        The cost model may be deterministic, as the acquisition function
        only needs the mean.
        """
        return {
            self.model_output_names[0]: {"mean", "std"},
            self.model_output_names[1]: {"mean"},
        }

    def _compute_head(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> np.ndarray:
        """
        Returns minus the cost-aware expected improvement.
        """
        assert current_best is not None
        means, stds = self._extract_mean_and_std(output_to_predictions)
        pred_costs = self._extract_positive_cost(output_to_predictions)

        # phi, Phi is PDF and CDF of Gaussian
        phi, Phi, u = get_quantiles(self.jitter, current_best, means, stds)
        f_acqu = stds * (u * Phi + phi) * np.power(pred_costs, -self.exponent_cost)
        return -np.mean(f_acqu, axis=1)

    def _compute_head_and_gradient(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> HeadWithGradient:
        """
        Returns minus cost-aware expected improvement and, for each output model, the gradients
        with respect to the mean and standard deviation of that model.
        """
        assert current_best is not None
        mean, std = self._extract_mean_and_std(output_to_predictions)
        pred_cost = self._extract_positive_cost(output_to_predictions)
        nf_active = mean.size
        nf_cost = pred_cost.size

        # phi, Phi is PDF and CDF of Gaussian
        phi, Phi, u = get_quantiles(self.jitter, current_best, mean, std)
        inv_cost_power = np.power(pred_cost, -self.exponent_cost)
        f_acqu = std * (u * Phi + phi) * inv_cost_power

        dh_dmean_active = _postprocess_gradient(Phi * inv_cost_power, nf=nf_active)
        dh_dstd_active = _postprocess_gradient(-phi * inv_cost_power, nf=1)
        # Flip the sign twice: once because of the derivative of 1 / x, and
        # once because the head is actually - f_ei
        dh_dmean_cost = _postprocess_gradient(
            self.exponent_cost * f_acqu / pred_cost, nf=nf_cost
        )

        gradient = {
            self.active_metric: dict(mean=dh_dmean_active, std=dh_dstd_active),
            self.cost_metric: dict(mean=dh_dmean_cost),
        }
        return HeadWithGradient(hval=-np.mean(f_acqu), gradient=gradient)

    def _extract_positive_cost(self, output_to_predictions):
        pred_cost = output_to_predictions[self.cost_metric]["mean"]
        if np.any(pred_cost) < 0.0:
            logger.warning(
                f"The model for {self.cost_metric} predicted some negative cost. "
                f"Capping the minimum cost at {MIN_COST}."
            )
        pred_cost = np.maximum(
            pred_cost, MIN_COST
        )  # ensure that the predicted cost/run-time is positive
        return pred_cost


class ConstraintCurrentBestProvider(CurrentBestProvider):
    """
    Here, `current_best` depends on two models, for active and
    constraint metric.

    """

    def __init__(self, current_best_list: List[np.ndarray], num_samples_active: int):
        list_size = len(current_best_list)
        assert list_size > 0 and list_size % num_samples_active == 0
        self._active_and_constraint_current_best = [
            v.reshape((1, -1)) for v in current_best_list
        ]
        self._num_samples_active = num_samples_active

    def __call__(self, positions: Tuple[int, ...]) -> Optional[np.ndarray]:
        flat_pos = positions[1] * self._num_samples_active + positions[0]
        return self._active_and_constraint_current_best[flat_pos]


class CEIAcquisitionFunction(MeanStdAcquisitionFunction):
    """
    Minus constrained expected improvement acquisition function.
    (minus because the convention is to always minimize the acquisition function)

    This is defined as CEI(x) = EI(x) * P(c(x) <= 0), where EI is the standard expected improvement with respect
    to the current *feasible best*, and P(c(x) <= 0) is the probability that the hyperparameter
    configuration x satisfies the constraint modeled by c(x).

    If there are no feasible hyperparameters yet, the current feasible best is undefined. Thus, CEI is
    reduced to the P(c(x) <= 0) term until a feasible configuration is found.

    Two metrics are expected in the model output: the main objective and the constraint metric.
    The main objective needs to be indicated as active_metric when initializing CEIAcquisitionFunction.
    The constraint is automatically assumed to be the other metric.

    References on CEI:
    Gardner et al., Bayesian Optimization with Inequality Constraints. In ICML, 2014.
    Gelbart et al., Bayesian Optimization with Unknown Constraints. In UAI, 2014.

    """

    def __init__(
        self,
        model: SurrogateOutputModel,
        active_metric: str = None,
        jitter: float = 0.01,
    ):
        super().__init__(model, active_metric)
        self.jitter = jitter
        self._feasible_best_list = None
        (
            self.active_metric,
            self.constraint_metric,
        ) = _extract_active_and_secondary_metric(self.model_output_names, active_metric)

    def _head_needs_current_best(self) -> bool:
        return True

    def _compute_head(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> np.ndarray:
        """
        Returns minus the constrained expected improvement (- CEI).
        """
        assert current_best is not None
        means, stds = self._extract_mean_and_std(output_to_predictions)
        means_constr, stds_constr = self._extract_mean_and_std(
            output_to_predictions, metric=self.constraint_metric
        )

        # Compute the probability of satisfying the constraint P(c(x) <= 0)
        constr_probs = norm.cdf(-means_constr / (stds_constr + MIN_STD_CONSTRAINT))
        # If for some fantasies there are not feasible candidates, there is also no current_best (i.e., a nan).
        # The acquisition function is replaced by only the P(c(x) <= 0) term when no feasible best exist.
        feas_idx = ~np.isnan(current_best).reshape((1, -1))

        # phi, Phi is PDF and CDF of Gaussian
        phi, Phi, u = get_quantiles(self.jitter, current_best, means, stds)
        f_ei = stds * (u * Phi + phi)
        # CEI(x) = EI(x) * P(c(x) <= 0) if feasible best exists, CEI(x) = P(c(x) <= 0) otherwise
        f_acqu = np.where(feas_idx, f_ei * constr_probs, constr_probs)
        return -np.mean(f_acqu, axis=1)

    def _compute_head_and_gradient(
        self,
        output_to_predictions: SamplePredictionsPerOutput,
        current_best: Optional[np.ndarray],
    ) -> HeadWithGradient:
        """
        Returns minus cost-aware expected improvement (- CEI) and, for each output model, the gradients
        with respect to the mean and standard deviation of that model.
        """
        assert current_best is not None
        mean, std = self._extract_mean_and_std(output_to_predictions)
        mean_constr, std_constr = self._extract_mean_and_std(
            output_to_predictions, metric=self.constraint_metric
        )
        nf_mean = mean.size
        nf_constr = mean_constr.size

        # Compute the probability of satisfying the constraint P(c(x) <= 0)
        std_constr = std_constr + MIN_STD_CONSTRAINT
        z = -mean_constr / std_constr
        constr_prob = norm.cdf(z)
        # Useful variables for computing the head gradients
        mean_over_squared_std_constr = mean_constr / std_constr**2
        inverse_std_constr = 1.0 / std_constr
        phi_constr = norm.pdf(z)

        # If for some fantasies there are not feasible candidates, there is also no current_best (i.e., a nan).
        # The acquisition function is replaced by only the P(c(x) <= 0) term when no feasible best exist.
        feas_idx = ~np.isnan(current_best)
        phi, Phi, u = get_quantiles(
            self.jitter, current_best, mean, std
        )  # phi, Phi is PDF and CDF of Gaussian
        f_ei = std * (u * Phi + phi)
        f_acqu = np.where(
            feas_idx, f_ei * constr_prob, constr_prob
        )  # CEI(x) = EI(x) * P(c(x) <= 0) if feasible best
        # exists, CEI(x) = P(c(x) <= 0) otherwise
        dh_dmean_constraint_feas = f_ei * inverse_std_constr * phi_constr
        dh_dstd_constraint_feas = -f_ei * mean_over_squared_std_constr * phi_constr
        dh_dmean_active_feas = Phi * constr_prob
        dh_dstd_active_feas = -phi * constr_prob
        dh_dmean_constraint_infeas = inverse_std_constr * phi_constr
        dh_dstd_constraint_infeas = -mean_over_squared_std_constr * phi_constr
        dh_dmean_active_infeas = np.zeros_like(phi_constr)
        dh_dstd_active_infeas = np.zeros_like(phi_constr)
        dh_dmean_active = _postprocess_gradient(
            np.where(feas_idx, dh_dmean_active_feas, dh_dmean_active_infeas), nf=nf_mean
        )
        dh_dstd_active = _postprocess_gradient(
            np.where(feas_idx, dh_dstd_active_feas, dh_dstd_active_infeas), nf=1
        )
        dh_dmean_constraint = _postprocess_gradient(
            np.where(feas_idx, dh_dmean_constraint_feas, dh_dmean_constraint_infeas),
            nf=nf_constr,
        )
        dh_dstd_constraint = _postprocess_gradient(
            np.where(feas_idx, dh_dstd_constraint_feas, dh_dstd_constraint_infeas), nf=1
        )
        gradient = {
            self.active_metric: dict(mean=dh_dmean_active, std=dh_dstd_active),
            self.constraint_metric: dict(
                mean=dh_dmean_constraint, std=dh_dstd_constraint
            ),
        }
        return HeadWithGradient(hval=-np.mean(f_acqu), gradient=gradient)

    def _get_current_bests_internal(
        self, model: SurrogateOutputModel
    ) -> CurrentBestProvider:
        active_model = model[self.active_metric]
        assert isinstance(active_model, BaseSurrogateModel)
        all_means_active = active_model.predict_mean_current_candidates()
        num_samples_active = len(all_means_active)
        constraint_model = model[self.constraint_metric]
        assert isinstance(constraint_model, BaseSurrogateModel)
        all_means_constraint = constraint_model.predict_mean_current_candidates()
        common_shape = all_means_active[0].shape
        assert all(
            x.shape == common_shape for x in all_means_constraint
        ), "Shape mismatch between models for predict_mean_current_candidates"
        current_best_list = []
        for means_constraint, means_active in itertools.product(
            all_means_constraint, all_means_active
        ):
            # Remove all infeasible candidates (i.e., where means_constraint
            # is >= 0)
            means_active[means_constraint >= 0] = np.nan
            # Compute the current *feasible* best (separately for every fantasy)
            min_across_observations = np.nanmin(means_active, axis=0)
            current_best_list.append(min_across_observations)
        return ConstraintCurrentBestProvider(current_best_list, num_samples_active)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/model_base.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Optional
import numpy as np
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    ConfigurationFilter,
)

logger = logging.getLogger(__name__)


class BaseSurrogateModel(SurrogateModel):
    """
    Base class for (most) SurrogateModel implementations, provides common code

    """

    def __init__(
        self,
        state: TuningJobState,
        active_metric: str = None,
        filter_observed_data: Optional[ConfigurationFilter] = None,
    ):
        super().__init__(state, active_metric)
        self._current_best = None
        self._filter_observed_data = filter_observed_data

    def predict_mean_current_candidates(self) -> List[np.ndarray]:
        """
        Returns the predictive mean (signal with key 'mean') at all current candidates
        in the state (observed, pending).

        If the hyperparameters of the surrogate model are being optimized (e.g.,
        by empirical Bayes), the returned list has length 1. If its
        hyperparameters are averaged over by MCMC, the returned list has one
        entry per MCMC sample.

        :return: List of predictive means
        """
        candidates, _ = self.state.observed_data_for_metric(self.active_metric)
        candidates += self.state.pending_configurations()
        candidates = self._current_best_filter_candidates(candidates)
        assert (
            len(candidates) > 0
        ), "Cannot predict means at current candidates with no candidates at all"
        inputs = self.hp_ranges_for_prediction().to_ndarray_matrix(candidates)
        all_means = []
        # Loop over MCMC samples (if any)
        for prediction in self.predict(inputs):
            means = prediction["mean"]
            if means.ndim == 1:  # In case of no fantasizing
                means = means.reshape((-1, 1))
            all_means.append(means)
        return all_means

    def current_best(self) -> List[np.ndarray]:
        if self._current_best is None:
            all_means = self.predict_mean_current_candidates()
            result = [np.min(means, axis=0) for means in all_means]
            self._current_best = result
        return self._current_best

    def _current_best_filter_candidates(self, candidates):
        """
        In some subclasses, 'current_best' is not computed over all (observed
        and pending) candidates: they need to implement this filter.

        """
        if self._filter_observed_data is None:
            return candidates  # Default: No filtering
        else:
            return [
                config for config in candidates if self._filter_observed_data(config)
            ]

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/model_skipopt.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
)


class SkipOptimizationPredicate:
    """
    Interface for skip_optimization predicate in
    :class:`ModelStateTransformer`.

    """

    def reset(self):
        """
        If there is an internal state, reset it to its initial value
        """
        pass

    def __call__(self, state: TuningJobState) -> bool:
        """
        :param state: Current TuningJobState
        :return: Skip hyperparameter optimization?
        """
        raise NotImplementedError


class NeverSkipPredicate(SkipOptimizationPredicate):
    """
    Hyperparameter optimization is never skipped.

    """

    def __call__(self, state: TuningJobState) -> bool:
        return False


class AlwaysSkipPredicate(SkipOptimizationPredicate):
    """
    Hyperparameter optimization is always skipped.

    """

    def __call__(self, state: TuningJobState) -> bool:
        return True


class SkipPeriodicallyPredicate(SkipOptimizationPredicate):
    """
    Let N be the number of labeled points for metric `metric_name`.
    Optimizations are not skipped if N < init_length. Afterwards,
    we increase a counter whenever N is larger than in the previous
    call. With respect to this counter, optimizations are done every
    period times, in between they are skipped.

    """

    def __init__(
        self, init_length: int, period: int, metric_name: str = INTERNAL_METRIC_NAME
    ):
        assert init_length >= 0
        assert period > 1
        self.init_length = init_length
        self.period = period
        self.metric_name = metric_name
        self.reset()

    def reset(self):
        self._counter = 0
        # Need to make sure that if called several times with the same state,
        # we return the same value
        self._last_size = None
        self._last_retval = None

    def __call__(self, state: TuningJobState) -> bool:
        num_labeled = state.num_observed_cases(self.metric_name)
        if num_labeled == self._last_size:
            return self._last_retval
        if self._last_size is not None:
            assert (
                num_labeled > self._last_size
            ), "num_labeled = {} < {} = _last_size".format(num_labeled, self._last_size)
        if num_labeled < self.init_length:
            ret_value = False
        else:
            ret_value = self._counter % self.period != 0
            self._counter += 1
        self._last_size = num_labeled
        self._last_size = ret_value
        return ret_value


class SkipNoMaxResourcePredicate(SkipOptimizationPredicate):
    """
    This predicate works for multi-fidelity HPO, see for example
    :class:`GPMultiFidelitySearcher`.

    We track the number of labeled datapoints at resource level max_resource.
    HP optimization is skipped if the total number of labeled cases is >=
    init_length, and if the number of max_resource cases has not increased
    since the last recent optimization.

    This means that as long as the dataset only grows w.r.t. cases at lower
    resources than max_resource, this does not trigger HP optimization.

    """

    def __init__(
        self,
        init_length: int,
        max_resource: int,
        metric_name: str = INTERNAL_METRIC_NAME,
    ):
        assert init_length >= 0
        self.init_length = init_length
        self.metric_name = metric_name
        self.max_resource = str(max_resource)
        self.reset()

    def reset(self):
        self.lastrec_max_resource_cases = None

    def _num_max_resource_cases(self, state: TuningJobState):
        def is_max_resource(metrics: dict) -> int:
            return int(self.max_resource in metrics[self.metric_name])

        return sum(is_max_resource(ev.metrics) for ev in state.trials_evaluations)

    def __call__(self, state: TuningJobState) -> bool:
        if state.num_observed_cases(self.metric_name) < self.init_length:
            return False
        num_max_resource_cases = self._num_max_resource_cases(state)
        if (
            self.lastrec_max_resource_cases is None
            or num_max_resource_cases > self.lastrec_max_resource_cases
        ):
            self.lastrec_max_resource_cases = num_max_resource_cases
            return False
        else:
            return True

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/models/model_transformer.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, Callable, Union
import logging
import copy

from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateModel,
    SurrogateOutputModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_skipopt import (
    SkipOptimizationPredicate,
    NeverSkipPredicate,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
    PendingEvaluation,
    TrialEvaluations,
    dictionarize_objective,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler

logger = logging.getLogger(__name__)


def _assert_same_keys(dict1, dict2):
    assert set(dict1.keys()) == set(
        dict2.keys()
    ), f"{list(dict1.keys())} and {list(dict2.keys())} need to be the same keys. "


class TransformerModelFactory:
    """
    Interface for model factories used in :class:`ModelStateTransformer`. A model
    factory provides access to tunable model parameters, and `model` creates
    :class:`SurrogateModel` instances.
    """

    def get_params(self) -> Dict:
        """
        :return: Current tunable model parameters
        """
        raise NotImplementedError()

    def set_params(self, param_dict: Dict):
        """
        :param param_dict: New model parameters
        """
        raise NotImplementedError()

    def model(self, state: TuningJobState, fit_params: bool) -> SurrogateModel:
        """
        Creates a `SurrogateModel` based on data in `state`. This involves
        fitting model parameters if `fit_params` is True. Otherwise, the current
        model parameters are not changed (so may be stale, given that `state`
        has changed). The idea is that often, model fitting is much more
        expensive than just creating the final `SurrogateModel` (posterior
        state). It then makes sense to partly work with stale model parameters.

        :param state: Current data model parameters are to be fit on, and the
            posterior state is to be computed from
        :param fit_params: See above
        :return: SurrogateModel, wrapping the posterior state for predictions
        """
        raise NotImplementedError()

    @property
    def debug_log(self) -> Optional[DebugLogPrinter]:
        return None

    @property
    def profiler(self) -> Optional[SimpleProfiler]:
        return None

    def configure_scheduler(self, scheduler):
        """
        Called by `configure_scheduler` of searchers which make use of a
        `TransformerModelFactory`. Allows the factory to depend on parameters
        of the scheduler.

        :param scheduler: Scheduler object
        """
        pass


# Convenience types allowing for multi-output HPO. These are used for methods that work both in the standard case
# of a single output model and in the multi-output case
TransformerOutputModelFactory = Union[
    TransformerModelFactory, Dict[str, TransformerModelFactory]
]

SkipOptimizationOutputPredicate = Union[
    SkipOptimizationPredicate, Dict[str, SkipOptimizationPredicate]
]


class ModelStateTransformer:
    """
    This class maintains the :class:`TuningJobState` alongside an HPO
    experiment, and manages the reaction to changes of this state.
    In particular, it provides a :class:`SurrogateModel` on demand, which
    encapsulates the GP posterior.

    The state transformer is generic, it uses :class:`TransformerModelFactory`
    for anything specific to the model type.

    `skip_optimization` is a predicate depending on the state, determining
    what is done at the next recent call of `model`. If False, the model
    parameters are refit, otherwise the current ones are not changed (which
    is usually faster, but risks stale-ness).

    We also track the observed data `state.trials_evaluations`. If this
    did not change since the last recent `model` call, we do not refit the
    model parameters. This is based on the assumption that model parameter
    fitting only depends on `state.trials_evaluations` (observed data),
    not on other fields (e.g., pending evaluations).

    Note that model_factory and skip_optimization can also be a dictionary mapping
    output names to models. In that case, the state is shared but the models for each
    output metric are updated independently.

    """

    def __init__(
        self,
        model_factory: TransformerOutputModelFactory,
        init_state: TuningJobState,
        skip_optimization: Optional[SkipOptimizationOutputPredicate] = None,
    ):
        self._use_single_model = False
        if isinstance(model_factory, TransformerModelFactory):
            self._use_single_model = True
        if not self._use_single_model:
            assert isinstance(model_factory, Dict), (
                f"{model_factory} is not an instance of TransformerModelFactory. "
                f"It is assumed that we are in the multi-output case and that it "
                f"must be a Dict. No other types are supported. "
            )
            _assert_same_keys(model_factory, skip_optimization)
            # Default: Always refit model parameters for each output model
            if skip_optimization is None:
                skip_optimization = {
                    output_name: NeverSkipPredicate()
                    for output_name in model_factory.keys()
                }
            else:
                assert isinstance(skip_optimization, Dict), (
                    f"{skip_optimization} must be a Dict, consistently "
                    f"with {model_factory}."
                )
                skip_optimization = {
                    output_name: skip_optimization[output_name]
                    if skip_optimization.get(output_name) is not None
                    else NeverSkipPredicate()
                    for output_name in model_factory.keys()
                }
            # debug_log is shared by all output models
            self._debug_log = next(iter(model_factory.values())).debug_log
        else:
            if skip_optimization is None:
                # Default: Always refit model parameters
                skip_optimization = NeverSkipPredicate()
            assert isinstance(skip_optimization, SkipOptimizationPredicate)
            self._debug_log = model_factory.debug_log
            # Make model_factory and skip_optimization single-key dictionaries
            # for convenience, so that we can treat the single model and multi-model case in the same way
            model_factory = dictionarize_objective(model_factory)
            skip_optimization = dictionarize_objective(skip_optimization)
        self._model_factory = model_factory
        self._skip_optimization = skip_optimization
        self._state = copy.copy(init_state)
        # SurrogateOutputModel computed on demand
        self._model: Optional[SurrogateOutputModel] = None
        # Observed data for which model parameters were re-fit most
        # recently, separately for each model
        self._num_evaluations = {output_name: 0 for output_name in model_factory.keys()}

    @property
    def state(self) -> TuningJobState:
        return self._state

    def _unwrap_from_dict(self, x):
        if self._use_single_model:
            return next(iter(x.values()))
        else:
            return x

    @property
    def use_single_model(self) -> bool:
        return self._use_single_model

    @property
    def model_factory(self) -> TransformerOutputModelFactory:
        return self._unwrap_from_dict(self._model_factory)

    @property
    def skip_optimization(self) -> SkipOptimizationOutputPredicate:
        return self._unwrap_from_dict(self._skip_optimization)

    def model(self, **kwargs) -> SurrogateOutputModel:
        """
        If skip_optimization is given, it overrides the self._skip_optimization
        predicate.

        :return: SurrogateModel for current state in the standard single model case;
                 in the multi-model case, it returns a dictionary mapping output names
                 to SurrogateModel instances for current state (shared across models).
        """
        if self._model is None:
            skip_optimization = kwargs.get("skip_optimization")
            self._compute_model(skip_optimization=skip_optimization)
        return self._unwrap_from_dict(self._model)

    def get_params(self):
        params = {
            output_name: output_model.get_params()
            for output_name, output_model in self._model_factory.items()
        }
        return self._unwrap_from_dict(params)

    def set_params(self, param_dict):
        if self._use_single_model:
            param_dict = dictionarize_objective(param_dict)
        _assert_same_keys(self._model_factory, param_dict)
        for output_name in self._model_factory:
            self._model_factory[output_name].set_params(param_dict[output_name])

    def append_trial(
        self,
        trial_id: str,
        config: Optional[Configuration] = None,
        resource: Optional[int] = None,
    ):
        """
        Appends new pending evaluation to the state.

        :param trial_id:
        :param config: Must be given if this trial does not yet feature in the
            state
        :param resource: Must be given in the multi-fidelity case, to specify
            at which resource level the evaluation is pending

        """
        self._model = None  # Invalidate
        self._state.append_pending(trial_id, config=config, resource=resource)

    def drop_pending_evaluation(
        self, trial_id: str, resource: Optional[int] = None
    ) -> bool:
        """
        Drop pending evaluation from state. If it is not listed as pending,
        nothing is done

        :param trial_id:
        :param resource: Must be given in the multi-fidelity case, to specify
            at which resource level the evaluation is pending

        """
        return self._state.remove_pending(trial_id, resource)

    def remove_observed_case(
        self, trial_id: str, metric_name: str = INTERNAL_METRIC_NAME, key: str = None
    ):
        """
        Removes specific observation from the state.

        :param trial_id:
        :param metric_name:
        :param key: Must be given in the multi-fidelity case

        """
        pos = self._state._find_labeled(trial_id)
        assert pos != -1, f"Trial trial_id = {trial_id} has no observations"
        metrics = self._state.trials_evaluations[pos].metrics
        assert metric_name in metrics, (
            f"state.trials_evaluations entry for trial_id = {trial_id} "
            + f"does not contain metric {metric_name}"
        )
        if key is None:
            del metrics[metric_name]
        else:
            metric_vals = metrics[metric_name]
            assert isinstance(metric_vals, dict) and key in metric_vals, (
                f"state.trials_evaluations entry for trial_id = {trial_id} "
                + f"and metric {metric_name} does not contain case for "
                + f"key {key}"
            )
            del metric_vals[key]

    def label_trial(
        self, data: TrialEvaluations, config: Optional[Configuration] = None
    ):
        """
        Adds observed data for a trial. If it has observations in the state
        already, `data.metrics` are appended. Otherwise, a new entry is
        appended.
        If new observations replace pending evaluations, these are removed.

        `config` must be passed if the trial has not yet been registered in
        the state (this happens normally with the `append_trial` call). If
        already registered, `config` is ignored.

        """
        # Drop pending candidate if it exists
        trial_id = data.trial_id
        metric_vals = data.metrics.get(INTERNAL_METRIC_NAME)
        if metric_vals is not None:
            resource_attr_name = self._state.hp_ranges.name_last_pos
            if resource_attr_name is not None:
                assert isinstance(
                    metric_vals, dict
                ), f"Metric {INTERNAL_METRIC_NAME} must be dict-valued"
                for resource in metric_vals.keys():
                    self.drop_pending_evaluation(trial_id, resource=int(resource))
            else:
                self.drop_pending_evaluation(trial_id)
        # Assign / append new labels
        metrics = self._state.metrics_for_trial(trial_id, config=config)
        for name, new_labels in data.metrics.items():
            if name not in metrics or not isinstance(new_labels, dict):
                metrics[name] = new_labels
            else:
                metrics[name].update(new_labels)
        self._model = None  # Invalidate

    def filter_pending_evaluations(
        self, filter_pred: Callable[[PendingEvaluation], bool]
    ):
        """
        Filters state.pending_evaluations with filter_pred.

        :param filter_pred Filtering predicate

        """
        new_pending_evaluations = list(
            filter(filter_pred, self._state.pending_evaluations)
        )
        if len(new_pending_evaluations) != len(self._state.pending_evaluations):
            self._model = None  # Invalidate
            del self._state.pending_evaluations[:]
            self._state.pending_evaluations.extend(new_pending_evaluations)

    def mark_trial_failed(self, trial_id: str):
        failed_trials = self._state.failed_trials
        if trial_id not in failed_trials:
            failed_trials.append(trial_id)

    def _compute_model(self, skip_optimization=None):
        if skip_optimization is None:
            skip_optimization = dict()
            for (
                output_name,
                output_skip_optimization,
            ) in self._skip_optimization.items():
                skip_optimization[output_name] = output_skip_optimization(self._state)
        elif self._use_single_model:
            skip_optimization = dictionarize_objective(skip_optimization)
        if self._debug_log is not None:
            for output_name, skip_opt in skip_optimization.items():
                if skip_opt:
                    logger.info(
                        f"Skipping the refitting of model parameters for {output_name}"
                    )

        _assert_same_keys(skip_optimization, self._model_factory)
        output_models = dict()
        for output_name, output_skip_optimization in skip_optimization.items():
            fit_params = not output_skip_optimization
            if fit_params:
                # Did the labeled data really change since the last recent refit?
                # If not, skip the refitting
                num_evaluations = self._state.num_observed_cases(output_name)
                if num_evaluations == self._num_evaluations[output_name]:
                    fit_params = False
                    if self._debug_log is not None:
                        logger.info(
                            f"Skipping the refitting of model parameters for {output_name}, "
                            f"since the labeled data did not change since the last recent fit"
                        )
                else:
                    # Model will be refitted: Update
                    self._num_evaluations[output_name] = num_evaluations
            output_models[output_name] = self._model_factory[output_name].model(
                state=self._state, fit_params=fit_params
            )
        self._model = output_models

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/tuning_algorithms/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/tuning_algorithms/base_classes.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import (
    List,
    Iterator,
    Iterable,
    Tuple,
    Type,
    Optional,
    Set,
    Dict,
    Union,
    Any,
)
import numpy as np

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)


def assign_active_metric(model, active_metric):
    """Checks that active_metric is provided when model consists of multiple output models.
    Otherwise, just sets active_metric to the only model output name available.
    """
    model_output_names = sorted(model.keys())
    num_output_models = len(model_output_names)
    if num_output_models == 1:
        if active_metric is not None:
            assert active_metric == model_output_names[0], (
                "Only a single output model is given. "
                "Active metric must be set to that output model."
            )
        active_metric = model_output_names[0]
    else:
        assert active_metric is not None, (
            f"As model has {num_output_models}, active metric cannot be None. "
            f"Please set active_metric to one of the model output names: "
            f"{model_output_names}."
        )
        assert active_metric in model_output_names
    return active_metric


class NextCandidatesAlgorithm:
    def next_candidates(self) -> List[Configuration]:
        raise NotImplemented("Abstract method")


class CandidateGenerator:
    """
    Class to generate candidates from which to start the local minimization, typically random candidate
    or some form of more uniformly spaced variation, such as latin hypercube or sobol sequence
    """

    def generate_candidates(self) -> Iterator[Configuration]:
        raise NotImplementedError

    def generate_candidates_en_bulk(
        self, num_cands: int, exclusion_list=None
    ) -> List[Configuration]:
        """
        :param num_cands: Number of candidates to generate
        :param exclusion_list: If given, these candidates must not be returned
        :return: List of `num_cands` candidates. If `exclusion_list` is given,
            the number of candidates returned can be `< num_cands`
        """
        raise NotImplementedError()


class SurrogateModel:
    def __init__(self, state: TuningJobState, active_metric: str = None):
        self.state = state
        if active_metric is None:
            active_metric = INTERNAL_METRIC_NAME
        self.active_metric = active_metric

    @staticmethod
    def keys_predict() -> Set[str]:
        """
        Keys of signals returned by 'predict'.
        Note: In order to work with 'AcquisitionFunction' implementations,
        the following signals are required:

        - 'mean': Predictive mean
        - 'std': Predictive standard deviation

        :return:
        """
        return {"mean", "std"}

    def predict(self, inputs: np.ndarray) -> List[Dict[str, np.ndarray]]:
        """
        Given (n, d) matrix of encoded input points, returns signals which are
        statistics of the predictive distribution at these inputs. By default,
        signals are:

        - 'mean': Predictive means. If the model supports fantasizing with a
            number nf of fantasies, this has shape (n, nf), otherwise (n,)
        - 'std': Predictive stddevs, shape (n,)

        If the hyperparameters of the surrogate model are being optimized (e.g.,
        by empirical Bayes), the returned list has length 1. If its
        hyperparameters are averaged over by MCMC, the returned list has one
        entry per MCMC sample.
        """
        raise NotImplementedError

    def hp_ranges_for_prediction(self) -> HyperparameterRanges:
        return self.state.hp_ranges

    def predict_candidates(
        self, candidates: Iterable[Configuration]
    ) -> List[Dict[str, np.ndarray]]:
        """
        Convenience variant of 'predict', where the input is a list of n
        candidates, which are encoded to input points here.
        """
        return self.predict(
            self.hp_ranges_for_prediction().to_ndarray_matrix(candidates)
        )

    def current_best(self) -> List[np.ndarray]:
        """
        Returns the so-called incumbent, to be used in acquisition functions
        such as expected improvement. This is the minimum of predictive means
        (signal with key 'mean') at all current candidate locations (both
        state.trials_evaluations and state.pending_evaluations).
        Normally, a scalar is returned, but if the model supports fantasizing
        and the state contains pending evaluations, there is one incumbent
        per fantasy sample, so a vector is returned.

        If the hyperparameters of the surrogate model are being optimized (e.g.,
        by empirical Bayes), the returned list has length 1. If its
        hyperparameters are averaged over by MCMC, the returned list has one
        entry per MCMC sample.

        :return: Incumbent
        """
        raise NotImplementedError

    def backward_gradient(
        self, input: np.ndarray, head_gradients: List[Dict[str, np.ndarray]]
    ) -> List[np.ndarray]:
        """
        Computes the gradient nabla_x f of an acquisition function f(x),
        where x is a single input point. This is using reverse mode
        differentiation, the head gradients are passed by the acquisition
        function.

        If p = p(x) denotes the output of 'predict' for a single input point,
        'head_gradients' contains the head gradients nabla_p f. Its shape is
        that of p (where n=1).

        Lists have >1 entry if MCMC is used, otherwise they are all size 1.

        :param input: Single input point x, shape (d,)
        :param head_gradients: See above
        :return: Gradient nabla_x f (several if MCMC is used)
        """
        raise NotImplementedError


# Useful type that allows for a dictionary mapping each output name to a SurrogateModel.
# This is needed for multi-output BO methods such as constrained BO, where each output
# is associated to a model. This type includes the Union with the standard
# SurrogateModel type for backward compatibility.
SurrogateOutputModel = Union[SurrogateModel, Dict[str, SurrogateModel]]


class ScoringFunction:
    """
    Class to score candidates, typically combine an acquisition function with
    potentially Thompson sampling

    NOTE: it will be minimized, i.e. lower is better
    """

    def score(
        self,
        candidates: Iterable[Configuration],
        model: Optional[SurrogateOutputModel] = None,
    ) -> List[float]:
        """
        Requires multiple candidates, is this can be much quicker: we can use matrix operations

        lower is better
        """
        raise NotImplementedError


class AcquisitionFunction(ScoringFunction):
    def __init__(self, model: SurrogateOutputModel, active_metric: str = None):
        self.model = model
        if active_metric is None:
            active_metric = INTERNAL_METRIC_NAME
        self.active_metric = active_metric

    def compute_acq(
        self, inputs: np.ndarray, model: Optional[SurrogateOutputModel] = None
    ) -> np.ndarray:
        """
        Note: If inputs has shape (d,), it is taken to be (1, d)

        :param inputs: Encoded input points, shape (n, d)
        :param model: If given, overrides self.model
        :return: Acquisition function values, shape (n,)
        """
        raise NotImplementedError

    def compute_acq_with_gradient(
        self, input: np.ndarray, model: Optional[SurrogateOutputModel] = None
    ) -> Tuple[float, np.ndarray]:
        """
        For a single input point x, compute acquisition function value f(x)
        and gradient nabla_x f.

        :param input: Single input point x, shape (d,)
        :param model: If given, overrides self.model
        :return: f(x), nabla_x f
        """
        raise NotImplementedError

    def score(
        self,
        candidates: Iterable[Configuration],
        model: Optional[SurrogateOutputModel] = None,
    ) -> List[float]:
        if model is None:
            model = self.model
        if isinstance(model, dict):
            active_model = model[self.active_metric]
        else:
            active_model = model
        hp_ranges = active_model.hp_ranges_for_prediction()
        inputs = hp_ranges.to_ndarray_matrix(candidates)
        return list(self.compute_acq(inputs, model=model))


AcquisitionClassAndArgs = Union[
    Type[AcquisitionFunction], Tuple[Type[AcquisitionFunction], Dict[str, Any]]
]


def unwrap_acquisition_class_and_kwargs(
    acquisition_class: AcquisitionClassAndArgs,
) -> (Type[AcquisitionFunction], Dict[str, Any]):
    if isinstance(acquisition_class, tuple):
        return acquisition_class
    else:
        return acquisition_class, dict()


class LocalOptimizer:
    """
    Class that tries to find a local candidate with a better score, typically
    using a local optimization method such as L-BFGS. It would normally
    encapsulate an acquisition function and model.

    `acquisition_class` contains the type of the acquisition function
    (subclass of :class:`AcquisitionFunction`). It can also be a tuple of the
    form (type, kwargs), where kwargs are extra arguments to the class
    constructor.

    """

    def __init__(
        self,
        hp_ranges: HyperparameterRanges,
        model: SurrogateOutputModel,
        acquisition_class: AcquisitionClassAndArgs,
        active_metric: str = None,
    ):
        self.hp_ranges = hp_ranges
        self.model = model
        if active_metric is None:
            active_metric = INTERNAL_METRIC_NAME
        if isinstance(model, dict):
            self.active_metric = assign_active_metric(model, active_metric)
        else:
            self.active_metric = active_metric
        self.acquisition_class = acquisition_class

    def optimize(
        self, candidate: Configuration, model: Optional[SurrogateOutputModel] = None
    ) -> Configuration:
        """
        Run local optimization, starting from candidate.
        If model is given, it overrides self.model.

        :param candidate: Starting point
        :param model: See above
        :return: Configuration found by local optimization
        """
        raise NotImplementedError

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/tuning_algorithms/bo_algorithm.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Tuple, Iterator, Optional
import logging
from dataclasses import dataclass
import numpy as np
import itertools

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    ModelStateTransformer,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    NextCandidatesAlgorithm,
    CandidateGenerator,
    ScoringFunction,
    LocalOptimizer,
    SurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm_components import (
    LBFGSOptimizeAcquisition,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.common import (
    generate_unique_candidates,
    ExclusionList,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.duplicate_detector import (
    DuplicateDetector,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler

logger = logging.getLogger(__name__)


@dataclass
class BayesianOptimizationAlgorithm(NextCandidatesAlgorithm):
    """
    Core logic of the Bayesian optimization algorithm
    :param initial_candidates_generator: generator of candidates
    :param initial_scoring_function: scoring function used to rank the initial
        candidates.
        Note: If a batch is selected in one go (num_requested_candidates > 1,
        greedy_batch_selection = False), this function should encourage
        diversity among its top scorers. In general, greedy batch selection
        is recommended.
    :param num_initial_candidates: how many initial candidates to generate, if
        possible
    :param num_initial_candidates_for_batch: This is used only if
        num_requested_candidates > 1 and greedy_batch_selection is True. In
        this case, num_initial_candidates_for_batch overrides
        num_initial_candidates when selecting all but the first candidate for
        the batch. Typically, num_initial_candidates is larger than
        num_initial_candidates_for_batch in this case, which speeds up
        selecting large batches, but still select the first candidate very
        thoroughly
    :param local_optimizer: local optimizer which starts from score minimizer.
        If a batch is selected in one go (not greedily), then local
        optimizations are started from the top num_requested_candidates ranked
        candidates (after scoring)
    :param pending_candidate_state_transformer: Once a candidate is selected, it
        becomes pending, and the state is transformed by appending information.
        This is done by the transformer.
        This is object is needed only if next_candidates goes through > 1 outer
        iterations (i.e., if greedy_batch_selection is True and
        num_requested_candidates > 1. Otherwise, None can be passed here.
        Note: Model updates (by the state transformer) for batch candidates beyond
        the first do not involve fitting hyperparameters, so they are usually
        cheap.
    :param exclusion_candidates: set of tuples, candidates that should not be
        returned, because they are already labeled, currently pending, or have
        failed
    :param num_requested_candidates: number of candidates to return
    :param greedy_batch_selection: If True and num_requested_candidates > 1, we
        generate, order, and locally optimize for each single candidate to be
        selected. Otherwise (False), this is done just once, and
        num_requested_candidates are extracted in one go.
        Note: If this is True, pending_candidate_state_transformer is needed.
    :param duplicate_detector: used to make sure no candidates equal to already
        evaluated ones is returned
    :param profiler: If given, this is used for profiling parts in the code
    :param sample_unique_candidates: If True, we check that initial candidates
        sampled at random are unique and disjoint from the exclusion list.
        See below.
    :param debug_log: If a DebugLogPrinter is passed here, it is used to write
        log messages

    """

    initial_candidates_generator: CandidateGenerator
    initial_candidates_scorer: ScoringFunction
    num_initial_candidates: int
    local_optimizer: LocalOptimizer
    pending_candidate_state_transformer: Optional[ModelStateTransformer]
    exclusion_candidates: ExclusionList
    num_requested_candidates: int
    greedy_batch_selection: bool
    duplicate_detector: DuplicateDetector
    num_initial_candidates_for_batch: int = None
    profiler: SimpleProfiler = None
    sample_unique_candidates: bool = False
    debug_log: Optional[DebugLogPrinter] = None

    # Note: For greedy batch selection (num_outer_iterations > 1), the
    # underlying SurrrogateModel changes with each new pending candidate. The
    # model changes are managed by pending_candidate_state_transformer. The
    # model has to be passed to both initial_candidates_scorer and
    # local_optimizer.
    def next_candidates(self) -> List[Configuration]:
        if self.greedy_batch_selection:
            # Select batch greedily, one candidate at a time, updating the
            # model in between
            num_outer_iterations = self.num_requested_candidates
            num_inner_candidates = 1
        else:
            # Select batch in one go
            num_outer_iterations = 1
            num_inner_candidates = self.num_requested_candidates
        next_trial_id = None
        if num_outer_iterations > 1:
            assert (
                self.pending_candidate_state_transformer
            ), "Need pending_candidate_state_transformer for greedy batch selection"
            # For greedy batch selection, we need to assign new trial_id's to
            # configs included into the batch, in order to update the state
            # maintained in `pending_candidate_state_transformer`.
            # This is just to make batch suggestion work: neither the state
            # nor these trial_id's are used in the future.
            # Note: This code also works if trial_id's are arbitrary strings.
            # It guarantees that `str(next_trial_id + i)` is not equal to an
            # existing trial_id for all i >= 0.
            next_trial_id = 0
            for (
                trial_id
            ) in self.pending_candidate_state_transformer.state.config_for_trial.keys():
                try:
                    next_trial_id = max(next_trial_id, int(trial_id))
                except ValueError:
                    pass
            next_trial_id += 1
        candidates = []
        just_added = True
        model = None  # SurrogateModel, if num_outer_iterations > 1
        for outer_iter in range(num_outer_iterations):
            if just_added:
                if self.exclusion_candidates.config_space_exhausted():
                    logger.warning(
                        "All entries of finite config space (size "
                        + f"{self.exclusion_candidates.configspace_size}) have been selected. Returning "
                        + f"{len(candidates)} configs instead of {self.num_requested_candidates}"
                    )
                    break
                just_added = False
            if (
                self.num_initial_candidates_for_batch is not None
                and self.greedy_batch_selection
                and outer_iter > 0
            ):
                num_initial_candidates = self.num_initial_candidates_for_batch
            else:
                num_initial_candidates = self.num_initial_candidates
            inner_candidates = self._get_next_candidates(
                num_inner_candidates,
                model=model,
                num_initial_candidates=num_initial_candidates,
            )
            candidates.extend(inner_candidates)
            if outer_iter < num_outer_iterations - 1 and len(inner_candidates) > 0:
                just_added = True
                # This is not the last outer iteration
                for cand in inner_candidates:
                    self.exclusion_candidates.add(cand)
                # State transformer is used to produce new model
                # Note: We suppress fit_hyperpars for models obtained during
                # batch selection
                for candidate in inner_candidates:
                    self.pending_candidate_state_transformer.append_trial(
                        trial_id=str(next_trial_id), config=candidate
                    )
                    next_trial_id += 1
                model = self.pending_candidate_state_transformer.model(
                    skip_optimization=True
                )
            if (
                len(inner_candidates) < num_inner_candidates
                and len(candidates) < self.num_requested_candidates
            ):
                logger.warning(
                    "All entries of finite config space (size "
                    + f"{self.exclusion_candidates.configspace_size}) have been selected. Returning "
                    + f"{len(candidates)} configs instead of {self.num_requested_candidates}"
                )
                break

        return candidates

    def _get_next_candidates(
        self,
        num_candidates: int,
        model: Optional[SurrogateModel],
        num_initial_candidates: Optional[int] = None,
    ):
        if num_initial_candidates is None:
            num_initial_candidates = self.num_initial_candidates
        # generate a random candidates among which to pick the ones to be
        # locally optimized
        logger.info(
            f"BayesOpt Algorithm: Generating {num_initial_candidates} "
            "initial candidates."
        )
        if self.profiler is not None:
            self.profiler.push_prefix("nextcand")
            self.profiler.start("all")
            self.profiler.start("genrandom")
        if self.sample_unique_candidates:
            # This can be expensive, depending on what type Candidate is
            initial_candidates = generate_unique_candidates(
                self.initial_candidates_generator,
                num_initial_candidates,
                self.exclusion_candidates,
            )
        else:
            # Will not return candidates in `exclusion_candidates`, but there
            # can be duplicates
            initial_candidates = (
                self.initial_candidates_generator.generate_candidates_en_bulk(
                    num_initial_candidates, exclusion_list=self.exclusion_candidates
                )
            )
        if self.profiler is not None:
            self.profiler.stop("genrandom")
            self.profiler.start("scoring")
        logger.info("BayesOpt Algorithm: Scoring (and reordering) candidates.")
        if self.debug_log is not None:
            candidates_and_scores = _order_candidates(
                initial_candidates,
                self.initial_candidates_scorer,
                model=model,
                with_scores=True,
            )
            initial_candidates = [cand for score, cand in candidates_and_scores]
            config = initial_candidates[0]
            top_scores = np.array([x for x, _ in candidates_and_scores[:5]])
            self.debug_log.set_init_config(config, top_scores)
        else:
            initial_candidates = _order_candidates(
                initial_candidates, self.initial_candidates_scorer, model=model
            )
        if self.profiler is not None:
            self.profiler.stop("scoring")
            self.profiler.start("localsearch")
        candidates_with_optimization = _lazily_locally_optimize(
            initial_candidates,
            self.local_optimizer,
            hp_ranges=self.exclusion_candidates.hp_ranges,
            model=model,
        )
        logger.info("BayesOpt Algorithm: Selecting final set of candidates.")
        if self.debug_log is not None and isinstance(
            self.local_optimizer, LBFGSOptimizeAcquisition
        ):
            # We would like to get num_evaluations from the first run (usually
            # the only one). This requires peeking at the first entry of the
            # iterator
            peek = candidates_with_optimization.__next__()
            self.debug_log.set_num_evaluations(self.local_optimizer.num_evaluations)
            candidates_with_optimization = itertools.chain(
                [peek], candidates_with_optimization
            )
        candidates = _pick_from_locally_optimized(
            candidates_with_optimization,
            self.exclusion_candidates,
            num_candidates,
            self.duplicate_detector,
        )
        if self.profiler is not None:
            self.profiler.stop("localsearch")
            self.profiler.stop("all")
            self.profiler.pop_prefix()  # nextcand
        return candidates


def _order_candidates(
    candidates: List[Configuration],
    scoring_function: ScoringFunction,
    model: Optional[SurrogateModel],
    with_scores: bool = False,
):
    if len(candidates) == 0:
        return []
    # scored in batch as this can be more efficient
    scores = scoring_function.score(candidates, model=model)
    sorted_list = sorted(zip(scores, candidates), key=lambda x: x[0])
    if with_scores:
        return sorted_list
    else:
        return [cand for score, cand in sorted_list]


def _lazily_locally_optimize(
    candidates: List[Configuration],
    local_optimizer: LocalOptimizer,
    hp_ranges: HyperparameterRanges,
    model: Optional[SurrogateModel],
) -> Iterator[Tuple[Configuration, Configuration]]:
    """
    Due to local deduplication we do not know in advance how many candidates
    we have to locally optimize, hence this helper to create a lazy generator
    of locally optimized candidates.
    Note that `candidates` may contain duplicates, but such are skipped here.
    """
    considered_already = ExclusionList.empty_list(hp_ranges)
    for cand in candidates:
        if not considered_already.contains(cand):
            considered_already.add(cand)
            yield cand, local_optimizer.optimize(cand, model=model)


# Note: If duplicate_detector is at least DuplicateDetectorIdentical, it will
# filter out candidates in exclusion_candidates here. Such can in principle
# arise if sample_unique_candidates == False.
# This does not work if duplicate_detector is DuplicateDetectorNoDetection.
def _pick_from_locally_optimized(
    candidates_with_optimization: Iterator[Tuple[Configuration, Configuration]],
    exclusion_candidates: ExclusionList,
    num_candidates: int,
    duplicate_detector: DuplicateDetector,
) -> List[Configuration]:
    updated_excludelist = exclusion_candidates.copy()
    result = []
    for original_candidate, optimized_candidate in candidates_with_optimization:
        insert_candidate = None
        optimized_is_duplicate = duplicate_detector.contains(
            updated_excludelist, optimized_candidate
        )
        if optimized_is_duplicate:
            # in the unlikely case that the optimized candidate ended at a
            # place that caused a duplicate we try to return the original instead
            original_also_duplicate = duplicate_detector.contains(
                updated_excludelist, original_candidate
            )
            if not original_also_duplicate:
                insert_candidate = original_candidate
        else:
            insert_candidate = optimized_candidate
        if insert_candidate is not None:
            result.append(insert_candidate)
            updated_excludelist.add(insert_candidate)
        if len(result) == num_candidates:
            break

    return result

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/tuning_algorithms/bo_algorithm_components.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Iterable, List, Optional
import numpy as np
from scipy.optimize import fmin_l_bfgs_b
import logging
from numpy.random import RandomState

from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    SurrogateModel,
    ScoringFunction,
    LocalOptimizer,
    SurrogateOutputModel,
    AcquisitionClassAndArgs,
    unwrap_acquisition_class_and_kwargs,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)

logger = logging.getLogger(__name__)


class IndependentThompsonSampling(ScoringFunction):
    """
    Note: This is not Thompson sampling, but rather a variant called
    "independent Thompson sampling", where means and variances are drawn
    from the marginal rather than the joint distribution. This is cheap,
    but incorrect.

    """

    def __init__(
        self, model: SurrogateModel, random_state: Optional[RandomState] = None
    ):
        self.model = model
        if random_state is None:
            random_state = RandomState(31415629)
        self.random_state = random_state

    def score(
        self,
        candidates: Iterable[Configuration],
        model: Optional[SurrogateModel] = None,
    ) -> List[float]:
        if model is None:
            model = self.model
        predictions_list = model.predict_candidates(candidates)
        scores = []
        # If the model supports fantasizing, posterior_means is a matrix. In
        # that case, samples are drawn for every column, then averaged (why
        # we need np.mean)
        for predictions in predictions_list:
            posterior_means = predictions["mean"]
            posterior_stds = predictions["std"]
            new_score = [
                np.mean(self.random_state.normal(m, s))
                for m, s in zip(posterior_means, posterior_stds)
            ]
            scores.append(new_score)
        return list(np.mean(np.array(scores), axis=0))


class LBFGSOptimizeAcquisition(LocalOptimizer):
    def __init__(
        self,
        hp_ranges: HyperparameterRanges,
        model: SurrogateOutputModel,
        acquisition_class: AcquisitionClassAndArgs,
        active_metric: str = None,
    ):
        super().__init__(hp_ranges, model, acquisition_class, active_metric)
        # Number criterion evaluations in last recent optimize call
        self.num_evaluations = None

    def optimize(
        self, candidate: Configuration, model: Optional[SurrogateOutputModel] = None
    ) -> Configuration:
        # Before local minimization, the model for this state_id should have been fitted.
        if model is None:
            model = self.model
        acquisition_class, acquisition_kwargs = unwrap_acquisition_class_and_kwargs(
            self.acquisition_class
        )
        acquisition_function = acquisition_class(
            model, self.active_metric, **acquisition_kwargs
        )

        x0 = self.hp_ranges.to_ndarray(candidate)
        bounds = self.hp_ranges.get_ndarray_bounds()
        n_evaluations = [0]  # wrapped in list to allow access from function

        # unwrap 2d arrays
        def f_df(x):
            n_evaluations[0] += 1
            return acquisition_function.compute_acq_with_gradient(x)

        res = fmin_l_bfgs_b(f_df, x0=x0, bounds=bounds, maxiter=1000)
        self.num_evaluations = n_evaluations[0]
        if res[2]["task"] == b"ABNORMAL_TERMINATION_IN_LNSRCH":
            # this condition was copied from the old GPyOpt code
            logger.warning(
                f"ABNORMAL_TERMINATION_IN_LNSRCH in lbfgs after {n_evaluations[0]} evaluations, "
                "returning original candidate"
            )
            return candidate  # returning original candidate
        else:
            # Clip to avoid situation where result is small epsilon out of bounds
            a_min, a_max = zip(*bounds)
            optimized_x = np.clip(res[0], a_min, a_max)
            # Make sure the above clipping does really just fix numerical rounding issues in LBFGS
            # if any bigger change was made there is a bug and we want to throw an exception
            assert np.linalg.norm(res[0] - optimized_x) < 1e-6, (
                res[0],
                optimized_x,
                bounds,
            )
            result = self.hp_ranges.from_ndarray(optimized_x.flatten())
            return result


class NoOptimization(LocalOptimizer):
    def optimize(
        self, candidate: Configuration, model: Optional[SurrogateModel] = None
    ) -> Configuration:
        return candidate

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/tuning_algorithms/common.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Iterator, List, Union, Optional
import numpy as np
import logging

from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    CandidateGenerator,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
    ConfigurationFilter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.config_space import config_space_size

logger = logging.getLogger(__name__)


MAX_RETRIES_CANDIDATES_EN_BULK = 20


class RandomStatefulCandidateGenerator(CandidateGenerator):
    """
    This generator maintains a random state, so if generate_candidates is
    called several times, different sequences are returned.

    """

    def __init__(
        self, hp_ranges: HyperparameterRanges, random_state: np.random.RandomState
    ):
        self.hp_ranges = hp_ranges
        self.random_state = random_state

    def generate_candidates(self) -> Iterator[Configuration]:
        while True:
            yield self.hp_ranges.random_config(self.random_state)

    def generate_candidates_en_bulk(
        self, num_cands: int, exclusion_list=None
    ) -> List[Configuration]:
        if exclusion_list is None:
            return self.hp_ranges.random_configs(self.random_state, num_cands)
        else:
            assert isinstance(
                exclusion_list, ExclusionList
            ), "exclusion_list must be of type ExclusionList"
            configs = []
            num_done = 0
            for i in range(MAX_RETRIES_CANDIDATES_EN_BULK):
                # From iteration 1, we request more than what is still
                # needed, because the config space seems to not have
                # enough configs left
                num_requested = min(num_cands, (num_cands - num_done) * (i + 1))
                new_configs = [
                    config
                    for config in self.hp_ranges.random_configs(
                        self.random_state, num_requested
                    )
                    if not exclusion_list.contains(config)
                ]
                num_new = min(num_cands - num_done, len(new_configs))
                configs += new_configs[:num_new]
                num_done += num_new
                if num_done == num_cands:
                    break
            if num_done < num_cands:
                logger.warning(
                    f"Could only sample {num_done} candidates where "
                    f"{num_cands} were requested. len(exclusion_list) = "
                    f"{len(exclusion_list)}"
                )
            return configs


class ExclusionList:
    """
    Maintains exclusion list of configs, to avoid choosing configs several
    times.

    The exclusion list contains non-extended configs, but it can be fed with
    and queried with extended configs. In that case, the resource attribute
    is removed from the config.

    """

    def __init__(
        self,
        state: Union[TuningJobState, dict],
        filter_observed_data: Optional[ConfigurationFilter] = None,
    ):
        is_new = isinstance(state, TuningJobState)
        if is_new:
            self.hp_ranges = state.hp_ranges
            keys = self.hp_ranges.internal_keys
            # Remove resource attribute from `self.keys` if present
            resource_attr = self.hp_ranges.name_last_pos
            if resource_attr is None:
                self.keys = keys
            else:
                pos = keys.index(resource_attr)
                self.keys = keys[:pos] + keys[(pos + 1) :]
            _elist = [
                x.trial_id for x in state.pending_evaluations
            ] + state.failed_trials
            observed_trial_ids = [x.trial_id for x in state.trials_evaluations]
            if filter_observed_data is not None:
                observed_trial_ids = [
                    trial_id
                    for trial_id in observed_trial_ids
                    if filter_observed_data(state.config_for_trial[trial_id])
                ]
            _elist = set(_elist + observed_trial_ids)
            self.excl_set = set(
                self._to_matchstr(state.config_for_trial[trial_id])
                for trial_id in _elist
            )
        else:
            self.hp_ranges = state["hp_ranges"]
            self.excl_set = state["excl_set"]
            self.keys = state["keys"]
        self.configspace_size = config_space_size(self.hp_ranges.config_space)

    def _to_matchstr(self, config) -> str:
        return self.hp_ranges.config_to_match_string(config, keys=self.keys)

    def contains(self, config: Configuration) -> bool:
        return self._to_matchstr(config) in self.excl_set

    def add(self, config: Configuration):
        self.excl_set.add(self._to_matchstr(config))

    def copy(self) -> "ExclusionList":
        return ExclusionList(
            {
                "hp_ranges": self.hp_ranges,
                "excl_set": set(self.excl_set),
                "keys": self.keys,
            }
        )

    @staticmethod
    def empty_list(hp_ranges: HyperparameterRanges) -> "ExclusionList":
        return ExclusionList(TuningJobState.empty_state(hp_ranges))

    def __len__(self) -> int:
        return len(self.excl_set)

    def config_space_exhausted(self) -> bool:
        return (self.configspace_size is not None) and len(
            self.excl_set
        ) >= self.configspace_size


MAX_RETRIES_ON_DUPLICATES = 10000


def generate_unique_candidates(
    candidates_generator: CandidateGenerator,
    num_candidates: int,
    exclusion_candidates: ExclusionList,
) -> List[Configuration]:
    exclusion_candidates = exclusion_candidates.copy()  # Copy
    result = []
    num_results = 0
    retries = 0
    just_added = True
    for i, cand in enumerate(candidates_generator.generate_candidates()):
        if just_added:
            if exclusion_candidates.config_space_exhausted():
                logger.warning(
                    "All entries of finite config space (size "
                    + f"{exclusion_candidates.configspace_size}) have been selected. Returning "
                    + f"{len(result)} configs instead of {num_candidates}"
                )
                break
            just_added = False
        if not exclusion_candidates.contains(cand):
            result.append(cand)
            num_results += 1
            exclusion_candidates.add(cand)
            retries = 0
            just_added = True
        else:
            # found a duplicate; retry
            retries += 1
        # End loop if enough candidates where generated, or after too many retries
        # (this latter can happen when most of them are duplicates, and must be done
        # to avoid infinite loops in the purely discrete case)
        if num_results == num_candidates or retries > MAX_RETRIES_ON_DUPLICATES:
            if retries > MAX_RETRIES_ON_DUPLICATES:
                logger.warning(
                    f"Reached limit of {MAX_RETRIES_ON_DUPLICATES} retries "
                    f"with i={i}. Returning {len(result)} candidates instead "
                    f"of the requested {num_candidates}"
                )
            break
    return result

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/tuning_algorithms/defaults.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm_components import (
    LBFGSOptimizeAcquisition,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc_impl import (
    EIAcquisitionFunction,
)


DEFAULT_ACQUISITION_FUNCTION = EIAcquisitionFunction
DEFAULT_LOCAL_OPTIMIZER_CLASS = LBFGSOptimizeAcquisition
DEFAULT_NUM_INITIAL_CANDIDATES = 250
DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS = 3

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/utils/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/utils/comparison_gpy.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Dict
import numpy as np
import copy

from syne_tune.config_space import uniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    TrialEvaluations,
    Configuration,
    dictionarize_objective,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    get_internal_candidate_evaluations,
)


class ThreeHumpCamel:
    @property
    def search_space(self):
        return [{"min": -5.0, "max": 5.0}, {"min": -5.0, "max": 5.0}]

    def evaluate(self, x1, x2):
        return 2 * x1**2 - 1.05 * x1**4 + x1**6 / 6 + x1 * x2 + x2**2


def branin_function(x1, x2, r=6):
    return (
        (x2 - (5.1 / (4 * np.pi**2)) * x1**2 + (5 / np.pi) * x1 - r) ** 2
        + 10 * (1 - 1 / (8 * np.pi)) * np.cos(x1)
        + 10
    )


class Branin:
    @property
    def search_space(self):
        return [{"min": -5.0, "max": 10.0}, {"min": 0.0, "max": 15.0}]

    def evaluate(self, x1, x2):
        return branin_function(x1, x2)


class BraninWithR(Branin):
    def __init__(self, r):
        self.r = r

    def evaluate(self, x1, x2):
        return branin_function(x1, x2, r=self.r)


class Ackley:
    @property
    def search_space(self):
        const = 32.768
        return [{"min": -const, "max": const}, {"min": -const, "max": const}]

    def evaluate(self, x1, x2):
        a = 20
        b = 0.2
        c = 2 * np.pi
        ssq = (x1**2) + (x2**2)
        scos = np.cos(c * x1) + np.cos(c * x2)
        return (
            -a * np.exp(-b * np.sqrt(0.5 * ssq)) - np.exp(0.5 * scos) + (a + np.exp(1))
        )


class SimpleQuadratic:
    @property
    def search_space(self):
        return [{"min": 0.0, "max": 1.0}, {"min": 0.0, "max": 1.0}]

    def evaluate(self, x1, x2):
        return 2 * (x1 - 0.5) ** 2 + (x2 - 0.5) ** 2


def _decode_input(x, lim):
    mn, mx = lim["min"], lim["max"]
    return x * (mx - mn) + mn


def evaluate_blackbox(bb_func, inputs: np.ndarray) -> np.ndarray:
    num_dims = inputs.shape[1]
    input_list = []
    for x, lim in zip(np.split(inputs, num_dims, axis=1), bb_func.search_space):
        input_list.append(_decode_input(x, lim))
    return bb_func.evaluate(*input_list)


# NOTE: Inputs will always be in [0, 1] (so come in encoded form). They are
# only scaled to their native ranges (linearly) when evaluations of the
# blackbox are done. This avoids silly errors.
def sample_data(
    bb_cls, num_train: int, num_grid: int, expand_datadct: bool = True
) -> dict:
    bb_func = bb_cls()
    ss_limits = bb_func.search_space
    num_dims = len(ss_limits)
    # Sample training inputs
    train_inputs = np.random.uniform(low=0.0, high=1.0, size=(num_train, num_dims))
    # Training targets (currently, no noise is added)
    train_targets = evaluate_blackbox(bb_func, train_inputs).reshape((-1,))
    # Inputs for prediction (regular grid)
    grids = [np.linspace(0.0, 1.0, num_grid)] * num_dims
    grids2 = tuple(np.meshgrid(*grids))
    test_inputs = np.hstack([x.reshape(-1, 1) for x in grids2])
    # Also evaluate true function on grid
    true_targets = evaluate_blackbox(bb_func, test_inputs).reshape((-1,))
    data = {
        "ss_limits": ss_limits,
        "train_inputs": train_inputs,
        "train_targets": train_targets,
        "test_inputs": test_inputs,
        "grid_shape": grids2[0].shape,
        "true_targets": true_targets,
    }
    if expand_datadct:
        # Make sure that ours and GPy below receive exactly the same inputs
        data = expand_data(data)
    return data


def expand_data(data: dict) -> dict:
    """
    Appends derived entries to data dict, which have non-elementary types.
    """
    if "state" not in data:
        data = copy.copy(data)
        state = data_to_state(data)
        data_internal = get_internal_candidate_evaluations(
            state,
            active_metric=INTERNAL_METRIC_NAME,
            normalize_targets=True,
            num_fantasy_samples=20,
        )
        data["state"] = state
        data["train_inputs"] = data_internal.features
        data["train_targets_normalized"] = data_internal.targets
    return data


# Recall that inputs in data are encoded, so we have to decode them to their
# native ranges for `trials_evaluations`
def data_to_state(data: dict) -> TuningJobState:
    configs, cs = decode_inputs(data["train_inputs"], data["ss_limits"])
    config_for_trial = {
        str(trial_id): config for trial_id, config in enumerate(configs)
    }
    trials_evaluations = [
        TrialEvaluations(trial_id=str(trial_id), metrics=dictionarize_objective(y))
        for trial_id, y in enumerate(data["train_targets"])
    ]
    return TuningJobState(
        hp_ranges=make_hyperparameter_ranges(cs),
        config_for_trial=config_for_trial,
        trials_evaluations=trials_evaluations,
    )


def decode_inputs(inputs: np.ndarray, ss_limits) -> (List[Configuration], Dict):
    cs_names = [f"x{i}" for i in range(len(ss_limits))]
    cs = {
        name: uniform(lower=lims["min"], upper=lims["max"])
        for name, lims in zip(cs_names, ss_limits)
    }
    x_mult = []
    x_add = []
    for lim in ss_limits:
        mn, mx = lim["min"], lim["max"]
        x_mult.append(mx - mn)
        x_add.append(mn)
    x_mult = np.array(x_mult)
    x_add = np.array(x_add)
    configs = []
    for x in inputs:
        x_decoded = x * x_mult + x_add
        config_dct = dict(zip(cs_names, x_decoded))
        configs.append(config_dct)
    return configs, cs


def assert_equal_candidates(candidates1, candidates2, hp_ranges, decimal=5):
    inputs1 = hp_ranges.to_ndarray_matrix(candidates1)
    inputs2 = hp_ranges.to_ndarray_matrix(candidates2)
    np.testing.assert_almost_equal(inputs1, inputs2, decimal=decimal)


def assert_equal_randomstate(randomstate1, randomstate2):
    assert str(randomstate1.get_state()) == str(randomstate2.get_state())

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/utils/debug_log.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List
import logging
import numpy as np

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)

logger = logging.getLogger(__name__)

__all__ = ["DebugLogPrinter"]


def _param_dict_to_str(params: dict) -> str:
    parts = []
    for name, param in params.items():
        if isinstance(param, float):
            parts.append(f"{name}: {param:.4e}")
        else:
            parts.append(f"{name}: {param}")
    return "{" + ", ".join(parts) + "}"


class DebugLogPrinter:
    """
    Supports a concise debug log.
    In particular, information about `get_config` is displayed in a single
    block. For that, different parts are first collected until the end of
    `get_config`.

    """

    def __init__(self):
        self._reset()

    def _reset(self):
        self.get_config_trial_id = None
        self.get_config_type = None
        self.block_info = dict()

    def start_get_config(self, gc_type, trial_id: str):
        assert gc_type in {"random", "BO", "grid"}
        assert trial_id is not None
        assert (
            self.get_config_type is None
        ), "Block for get_config of type '{}' is currently open".format(
            self.get_config_type
        )
        self.get_config_trial_id = trial_id
        self.get_config_type = gc_type
        logger.debug(f"Starting get_config[{gc_type}] for trial_id {trial_id}")

    def set_final_config(self, config: Configuration):
        assert self.get_config_type is not None, "No block open right now"
        entries = ["{}: {}".format(k, v) for k, v in config.items()]
        msg = "\n".join(entries)
        self.block_info["final_config"] = msg

    def _observed_trial_ids(self, state: TuningJobState) -> List[str]:
        trial_ids = []
        for ev in state.trials_evaluations:
            trial_id = ev.trial_id
            metric_entry = ev.metrics.get(INTERNAL_METRIC_NAME)
            if metric_entry is not None:
                if isinstance(metric_entry, dict):
                    for resource in metric_entry.keys():
                        trial_ids.append(trial_id + ":" + resource)
                else:
                    trial_ids.append(trial_id)
        return trial_ids

    def _pending_trial_ids(self, state: TuningJobState) -> List[str]:
        trial_ids = []
        for ev in state.pending_evaluations:
            trial_id = ev.trial_id
            resource = ev.resource
            if resource is None:
                trial_ids.append(trial_id)
            else:
                trial_ids.append(trial_id + f":{resource}")
        return trial_ids

    def set_state(self, state: TuningJobState):
        assert self.get_config_type == "BO", "Need to be in 'BO' block"
        labeled_str = ", ".join(self._observed_trial_ids(state))
        msg = "Labeled: " + labeled_str
        if state.pending_evaluations:
            pending_str = ", ".join(self._pending_trial_ids(state))
            msg += ". Pending: " + pending_str
        self.block_info["state"] = msg

    def set_targets(self, targets: np.ndarray):
        assert self.get_config_type == "BO", "Need to be in 'BO' block"
        msg = "Targets: " + str(targets.reshape((-1,)))
        self.block_info["targets"] = msg

    def set_model_params(self, params: dict):
        assert self.get_config_type == "BO", "Need to be in 'BO' block"
        msg = "Model params: " + _param_dict_to_str(params)
        self.block_info["params"] = msg

    def set_fantasies(self, fantasies: np.ndarray):
        assert self.get_config_type == "BO", "Need to be in 'BO' block"
        msg = "Fantasized targets:\n" + str(fantasies)
        self.block_info["fantasies"] = msg

    def set_init_config(self, config: Configuration, top_scores: np.ndarray = None):
        assert self.get_config_type == "BO", "Need to be in 'BO' block"
        entries = ["{}: {}".format(k, v) for k, v in config.items()]
        msg = "Started BO from (top scorer):\n" + "\n".join(entries)
        if top_scores is not None:
            msg += "\nTop score values: " + str(top_scores.reshape((-1,)))
        self.block_info["start_config"] = msg

    def set_num_evaluations(self, num_evals: int):
        assert self.get_config_type == "BO", "Need to be in 'BO' block"
        self.block_info["num_evals"] = num_evals

    def append_extra(self, extra: str):
        if "extra" in self.block_info:
            self.block_info["extra"] = "\n".join([self.block_info["extra"], extra])
        else:
            self.block_info["extra"] = extra

    def write_block(self):
        assert self.get_config_type is not None, "No block open right now"
        info = self.block_info
        trial_id = self.get_config_trial_id
        if "num_evals" in info:
            parts = [
                "[{}: {}] ({} evaluations)".format(
                    trial_id, self.get_config_type, info["num_evals"]
                )
            ]
        else:
            parts = ["[{}: {}]".format(trial_id, self.get_config_type)]
        parts.append(info["final_config"])
        debug_parts = []  # Parts for logger.debug
        if self.get_config_type == "BO":
            if "start_config" in info:
                debug_parts.append(info["start_config"])
            # The following 3 should be present!
            for name in ("state", "targets", "params"):
                v = info.get(name)
                if v is not None:
                    if name == "targets":
                        debug_parts.append(v)
                    else:
                        parts.append(v)
                else:
                    logger.info(
                        "debug_log.write_block: '{}' part is missing!".format(name)
                    )
            if "fantasies" in info:
                debug_parts.append(info["fantasies"])
        if "extra" in info:
            debug_parts.append(info["extra"])
        msg = "\n".join(parts)
        logger.info(msg)
        msg = "\n".join(debug_parts)
        logger.debug(msg)
        self._reset()

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/utils/density.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from math import erfc
import numpy as np
from scipy.special import erfc


def get_quantiles(acquisition_par, fmin, m, s):
    """
    Quantiles of the Gaussian distribution useful to determine the acquisition function values
    :param acquisition_par: parameter of the acquisition function
    :param fmin: current minimum.
    :param m: vector of means.
    :param s: vector of standard deviations.
    """
    if isinstance(s, np.ndarray):
        s[s < 1e-10] = 1e-10
    elif s < 1e-10:
        s = 1e-10
    u = (fmin - m - acquisition_par) / s

    phi = np.exp(-0.5 * u**2) / np.sqrt(2 * np.pi)
    # vectorized version of erfc to not depend on scipy
    Phi = 0.5 * erfc(-u / np.sqrt(2))
    return (phi, Phi, u)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/utils/duplicate_detector.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.common import (
    ExclusionList,
)


class DuplicateDetector:
    def contains(
        self, existing_candidates: ExclusionList, new_candidate: Configuration
    ) -> bool:
        raise NotImplementedError


class DuplicateDetectorNoDetection(DuplicateDetector):
    def contains(
        self, existing_candidates: ExclusionList, new_candidate: Configuration
    ) -> bool:
        return False  # no duplicate detection at all


class DuplicateDetectorIdentical(DuplicateDetector):
    def contains(
        self, existing_candidates: ExclusionList, new_candidate: Configuration
    ) -> bool:
        return existing_candidates.contains(new_candidate)

File Path: syne_tune/optimizer/schedulers/searchers/bayesopt/utils/test_objects.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# Could eventually remove this code: Is this needed in unit tests?

"""
Object definitions that are used for testing.
"""

from typing import Iterator, Tuple, Dict, List, Optional, Union
import numpy as np

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Hyperparameter,
    Configuration,
    dictionarize_objective,
)
from syne_tune.config_space import Categorical, loguniform, randint, choice, uniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    TrialEvaluations,
    PendingEvaluation,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    MCMCConfig,
    OptimizationConfig,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_regression import (
    GaussianProcessRegression,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gpr_mcmc import (
    GPRegressionMCMC,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    Matern52,
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.warping import (
    WarpedKernel,
    Warping,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    CandidateGenerator,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.common import (
    ExclusionList,
)


def build_kernel(state: TuningJobState, do_warping: bool = False) -> KernelFunction:
    dims, warping_ranges = dimensionality_and_warping_ranges(state.hp_ranges)
    kernel = Matern52(dims, ARD=True)
    if do_warping:
        return WarpedKernel(kernel=kernel, warping=Warping(dims, warping_ranges))
    else:
        return kernel


def default_gpmodel(
    state: TuningJobState, random_seed: int, optimization_config: OptimizationConfig
) -> GaussianProcessRegression:
    return GaussianProcessRegression(
        kernel=build_kernel(state),
        optimization_config=optimization_config,
        random_seed=random_seed,
    )


def default_gpmodel_mcmc(
    state: TuningJobState, random_seed: int, mcmc_config: MCMCConfig
) -> GPRegressionMCMC:
    return GPRegressionMCMC(
        build_kernel=lambda: build_kernel(state),
        mcmc_config=mcmc_config,
        random_seed=random_seed,
    )


def dimensionality_and_warping_ranges(
    hp_ranges: HyperparameterRanges,
) -> Tuple[int, Dict[int, Tuple[float, float]]]:
    lower_config = dict()
    upper_config = dict()
    for name, hp_range in hp_ranges.config_space.items():
        if not isinstance(hp_range, Categorical):
            lower_config[name] = hp_range.lower
            upper_config[name] = hp_range.upper
        else:
            lower_config[name] = hp_range.categories[0]
            upper_config[name] = hp_range.categories[0]
    lower_internal = hp_ranges.to_ndarray(lower_config)
    upper_internal = hp_ranges.to_ndarray(upper_config)
    dims = 0
    warping_ranges = dict()
    for name in hp_ranges.internal_keys:
        hp_range = hp_ranges.config_space[name]
        if not isinstance(hp_range, Categorical):
            _lower = lower_internal[dims]
            _upper = upper_internal[dims]
            if _upper > _lower:  # exclude cases where max equal to min
                warping_ranges[dims] = (_lower, _upper)
            else:
                assert _lower == _upper
            dims += 1
        else:
            # For binary, we use a single dimension, not 2
            sz = len(hp_range.categories)
            if sz == 2:
                sz = 1
            dims += sz
    return dims, warping_ranges


class RepeatedCandidateGenerator(CandidateGenerator):
    """Generates candidates from a fixed set. Used to test the deduplication logic."""

    def __init__(self, n_unique_candidates: int):
        self.config_space = {
            "a": uniform(0, n_unique_candidates),
            "b": randint(0, n_unique_candidates),
            "c": choice([f"value_{i}" for i in range(n_unique_candidates)]),
        }
        self.hp_ranges = make_hyperparameter_ranges(self.config_space)
        self.all_unique_candidates = [
            {"a": 1.0 * j, "b": j, "c": f"value_{j}"}
            for j in range(n_unique_candidates)
        ]

    def generate_candidates(self) -> Iterator[Configuration]:
        i = 0
        while True:
            i += 1
            yield self.all_unique_candidates[i % len(self.all_unique_candidates)]


# Example black box function, with adjustable location of global minimum.
# Potentially could catch issues with optimizer, e.g. if the optimizer
# ignoring somehow candidates on the edge of search space.
# A simple quadratic function is used.
class Quadratic3d:
    def __init__(self, local_minima, active_metric, metric_names):
        # local_minima: point where local_minima is located
        self.local_minima = np.array(local_minima).astype("float")
        self.local_minima[0] = np.log10(self.local_minima[0])
        self.active_metric = active_metric
        self.metric_names = metric_names

    @property
    def search_space(self):
        config_space = {
            "x": loguniform(1.0, 100.0),
            "y": randint(0, 2),
            "z": choice(["0.0", "1.0", "2.0"]),
        }
        return make_hyperparameter_ranges(config_space)

    @property
    def f_min(self):
        return 0.0

    def __call__(self, candidate):
        p = np.array([float(hp) for hp in candidate])
        p[0] = np.log10(p[0])
        return dictionarize_objective(np.sum((self.local_minima - p) ** 2))


def tuples_to_configs(
    config_tpls: List[Tuple[Hyperparameter, ...]], hp_ranges: HyperparameterRanges
) -> List[Configuration]:
    """
    Many unit tests write configs as tuples.

    """
    return [hp_ranges.tuple_to_config(x) for x in config_tpls]


def create_exclusion_set(
    candidates_tpl, hp_ranges: HyperparameterRanges, is_dict: bool = False
) -> ExclusionList:
    """
    Creates exclusion list from set of tuples.

    """
    if not is_dict:
        candidates_tpl = tuples_to_configs(candidates_tpl, hp_ranges)
    config_for_trial = {
        str(trial_id): config for trial_id, config in enumerate(candidates_tpl)
    }
    state = TuningJobState(
        hp_ranges=hp_ranges,
        config_for_trial=config_for_trial,
        trials_evaluations=[],
        failed_trials=[str(x) for x in range(len(candidates_tpl))],
    )
    return ExclusionList(state)


TupleOrDict = Union[tuple, dict]


def create_tuning_job_state(
    hp_ranges: HyperparameterRanges,
    cand_tuples: List[TupleOrDict],
    metrics: List[Dict],
    pending_tuples: Optional[List[TupleOrDict]] = None,
    failed_tuples: Optional[List[TupleOrDict]] = None,
) -> TuningJobState:
    """
    Builds `TuningJobState` from basics, where configs are given as tuples or
    as dicts.

    NOTE: We assume that all configs in the different lists are different!

    """
    if cand_tuples and isinstance(cand_tuples[0], tuple):
        configs = tuples_to_configs(cand_tuples, hp_ranges)
    else:
        configs = cand_tuples
    trials_evaluations = [
        TrialEvaluations(trial_id=str(trial_id), metrics=y)
        for trial_id, y in enumerate(metrics)
    ]
    pending_evaluations = None
    if pending_tuples is not None:
        sz = len(configs)
        extra = len(pending_tuples)
        if pending_tuples and isinstance(pending_tuples[0], tuple):
            extra_configs = tuples_to_configs(pending_tuples, hp_ranges)
        else:
            extra_configs = pending_tuples
        configs.extend(extra_configs)
        pending_evaluations = [
            PendingEvaluation(trial_id=str(trial_id))
            for trial_id in range(sz, sz + extra)
        ]
    failed_trials = None
    if failed_tuples is not None:
        sz = len(configs)
        extra = len(failed_tuples)
        if failed_tuples and isinstance(failed_tuples[0], tuple):
            extra_configs = tuples_to_configs(failed_tuples, hp_ranges)
        else:
            extra_configs = failed_tuples
        configs.extend(extra_configs)
        failed_trials = [str(x) for x in range(sz, sz + extra)]

    config_for_trial = {
        str(trial_id): config for trial_id, config in enumerate(configs)
    }
    return TuningJobState(
        hp_ranges=hp_ranges,
        config_for_trial=config_for_trial,
        trials_evaluations=trials_evaluations,
        failed_trials=failed_trials,
        pending_evaluations=pending_evaluations,
    )

File Path: syne_tune/optimizer/schedulers/searchers/bore/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bore.bore import Bore  # noqa: F401

File Path: syne_tune/optimizer/schedulers/searchers/bore/bore.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import time
import xgboost
import logging
import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV

from typing import Dict

from syne_tune.optimizer.schedulers.searchers.searcher import SearcherWithRandomSeed
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bore.de import (
    DifferentialevolutionOptimizer,
)

logger = logging.getLogger(__name__)


class Bore(SearcherWithRandomSeed):
    def __init__(
        self,
        config_space: dict,
        metric: str,
        points_to_evaluate=None,
        mode: str = "max",
        gamma: float = 0.25,
        calibrate: bool = False,
        classifier: str = "xgboost",
        acq_optimizer: str = "rs",
        feval_acq: int = 500,
        random_prob: float = 0.0,
        init_random: int = 6,
        classifier_kwargs: dict = None,
        **kwargs,
    ):

        """
        Implements "Bayesian optimization by Density Ratio Estimation" as described in the following paper:

        BORE: Bayesian Optimization by Density-Ratio Estimation,
        Tiao, Louis C and Klein, Aaron and Seeger, Matthias W and Bonilla, Edwin V. and Archambeau, Cedric and Ramos, Fabio
        Proceedings of the 38th International Conference on Machine Learning


        Note: Bore only works in the non-parallel non-multi-fideltiy setting. Make sure that you use it with the
        FIFO scheduler and set num_workers to 1 in the backend.

        :param config_space: Configuration space. Constant parameters are filtered out
        :param metric: Name of metric reported by evaluation function.
        :param points_to_evaluate:
        :param gamma: Defines the percentile, i.e how many percent of configuration are used to model l(x).
        :param calibrate: If set to true, we calibrate the predictions of the classifier via CV
        :param classifier: The binary classifier to model the acquisition function.
            Choices: {'mlp', 'gp', 'xgboost', 'rf}
        :param random_seed: seed for the random number generator
        :param acq_optimizer: The optimization method to maximize the acquisition function. Choices: {'de', 'rs'}
        :param feval_acq: Maximum allowed function evaluations of the acquisition function.
        :param random_prob: probability for returning a random configurations (epsilon greedy)
        :param init_random: Number of initial random configurations before we start with the optimization.
        :param classifier_kwargs: Dict that contains all hyperparameters for the classifier
        """

        super().__init__(
            config_space=config_space,
            metric=metric,
            points_to_evaluate=points_to_evaluate,
            **kwargs,
        )

        self.calibrate = calibrate
        self.gamma = gamma
        self.classifier = classifier
        assert acq_optimizer in {"rs", "de", "rs_with_replacement"}
        self.acq_optimizer = acq_optimizer
        self.feval_acq = feval_acq
        self.init_random = init_random
        self.random_prob = random_prob
        self.mode = mode

        self._hp_ranges = make_hyperparameter_ranges(config_space)

        if classifier_kwargs is None:
            classifier_kwargs = dict()
        if self.classifier == "xgboost":
            self.model = xgboost.XGBClassifier(use_label_encoder=False)
        elif self.classifier == "logreg":
            self.model = LogisticRegression()
        elif self.classifier == "rf":
            self.model = RandomForestClassifier()
        elif self.classifier == "gp":
            from syne_tune.optimizer.schedulers.searchers.bore.gp_classififer import (
                GPModel,
            )

            self.model = GPModel(**classifier_kwargs)
        elif self.classifier == "mlp":
            from syne_tune.optimizer.schedulers.searchers.bore.mlp_classififer import (
                MLP,
            )

            self.model = MLP(n_inputs=self._hp_ranges.ndarray_size, **classifier_kwargs)

        self.inputs = []
        self.targets = []

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.fifo import FIFOScheduler

        assert isinstance(
            scheduler, FIFOScheduler
        ), "This searcher requires FIFOScheduler scheduler"

        super().configure_scheduler(scheduler)

    def loss(self, x):
        if len(x.shape) < 2:
            y = -self.model.predict_proba(x[None, :])
        else:
            y = -self.model.predict_proba(x)
        if self.classifier in ["gp", "mlp"]:
            return y[:, 0]
        else:
            return y[:, 1]  # return probability of class 1

    def get_config(self, **kwargs):
        """Function to sample a new configuration

        This function is called inside TaskScheduler to query a new
        configuration.

        Note: Query `_next_initial_config` for initial configs to return first.

        Args:
        kwargs:
            Extra information may be passed from scheduler to searcher
        returns: config
            must return a valid configuration
        """

        start_time = time.time()

        if len(self.inputs) < self.init_random or np.random.rand() < self.random_prob:
            config = self._hp_ranges.random_config(self.random_state)

        else:
            # train model
            self.train_model(self.inputs, self.targets)

            if self.model is None:
                config = self._hp_ranges.random_config(self.random_state)

            else:

                if self.acq_optimizer == "de":

                    def wrapper(x):
                        l = self.loss(x)
                        return l[:, None]

                    bounds = np.array(self._hp_ranges.get_ndarray_bounds())
                    lower = bounds[:, 0]
                    upper = bounds[:, 1]

                    de = DifferentialevolutionOptimizer(
                        wrapper, lower, upper, self.feval_acq
                    )
                    best, traj = de.run()
                    config = self._hp_ranges.from_ndarray(best)

                elif self.acq_optimizer == "rs_with_replacement":
                    values = []
                    X = []
                    for i in range(self.feval_acq):
                        xi = self._hp_ranges.random_config(self.random_state)
                        X.append(xi)
                        values.append(self.loss(self._hp_ranges.to_ndarray(xi))[0])

                    ind = np.array(values).argmin()
                    config = X[ind]
                else:

                    # sample random configurations without replacement
                    values = []
                    X = []
                    counter = 0
                    while len(values) < self.feval_acq and counter < 10:
                        xi = self._hp_ranges.random_config(self.random_state)
                        if xi not in X:
                            X.append(xi)
                            values.append(self.loss(self._hp_ranges.to_ndarray(xi))[0])
                            counter = 0
                        else:
                            logging.warning(
                                "Re-sampled the same configuration. Retry..."
                            )
                            counter += 1  # we stop sampling if after 10 retires we are not able to find a new config
                    if len(values) < self.feval_acq:
                        logging.warning(
                            f"Only {len(values)} instead of {self.feval_acq} configurations "
                            f"sampled to optimize the acquisition function"
                        )
                    ind = np.array(values).argmin()
                    config = X[ind]

        opt_time = time.time() - start_time
        logging.debug(
            f"[Select new candidate: "
            f"config={config}] "
            f"optimization time : {opt_time}"
        )

        return config

    def train_model(self, train_data, train_targets):

        start_time = time.time()

        X = np.array(self.inputs)

        if self.mode == "min":
            y = np.array(self.targets)
        else:
            y = -np.array(self.targets)

        tau = np.quantile(y, q=self.gamma)
        z = np.less(y, tau)

        if self.calibrate:
            self.model = CalibratedClassifierCV(
                self.model, cv=2, method=self.calibration
            )
            self.model.fit(X, np.array(z, dtype=np.int))
        else:
            self.model.fit(X, np.array(z, dtype=np.int))

        z_hat = self.model.predict(X)
        accuracy = np.mean(z_hat == z)

        train_time = time.time() - start_time
        logging.debug(
            f"[Model fit: "
            f"accuracy={accuracy:.3f}] "
            f"dataset size: {X.shape[0]}, "
            f"train time : {train_time}"
        )

    def _update(self, trial_id: str, config: Dict, result: Dict):
        """Update surrogate model with result

        :param config: new configuration
        :param result: observed results from the train function
        """

        self.inputs.append(self._hp_ranges.to_ndarray(config))
        self.targets.append(result[self._metric])

    def clone_from_state(self, state):
        pass

File Path: syne_tune/optimizer/schedulers/searchers/bore/de.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np


class DifferentialevolutionOptimizer:
    def __init__(self, f, lower, upper, fevals, strategy="best1", bin=1):

        self.f = f

        self.lower_bound = lower
        self.upper_bound = upper

        self.mut = 0.5
        self.crossp = 0.5

        self.popsize = 10
        self.its = fevals // self.popsize - 1
        self.dimensions = len(self.lower_bound)

        self.de_pop = []
        self.fitness = []
        self.fbest = np.float("inf")
        self.idxbest = 1
        self.strategy = strategy
        self.bin = bin

    def evolve(self, j):
        best_idv = self.de_pop[self.idxbest]
        current_idv = self.de_pop[j]

        # perform mutation operation
        if self.strategy == "rand1":
            idxs = [idx for idx in range(self.popsize) if idx != j]
            r1, r2, r3 = self.de_pop[np.random.choice(idxs, 3, replace=False)]

            # Step 3.1: Perform mutation and checking
            temp = r1 + self.mut * (r2 - r3)
            vi = np.clip(temp, self.lower_bound, self.upper_bound)

        if self.strategy == "best1":
            idxs = [idx for idx in range(self.popsize) if idx != j]
            r1, r2 = self.de_pop[np.random.choice(idxs, 2, replace=False)]
            temp = best_idv + self.mut * (r1 - r2)
            vi = np.clip(temp, self.lower_bound, self.upper_bound)

        if self.strategy == "rand2":
            idxs = [idx for idx in range(self.popsize) if idx != j]
            r1, r2, r3, r4, r5 = self.de_pop[np.random.choice(idxs, 5, replace=False)]
            temp = r1 + self.mut * (r1 - r2) + self.mut * (r3 - r4)
            vi = np.clip(temp, self.lower_bound, self.upper_bound)

        if self.strategy == "best2":
            idxs = [idx for idx in range(self.popsize) if idx != j]
            r1, r2, r3, r4 = self.de_pop[np.random.choice(idxs, 4, replace=False)]
            temp = best_idv + self.mut * (r1 - r2) + self.mut * (r3 - r4)
            vi = np.clip(temp, self.lower_bound, self.upper_bound)

        if self.strategy == "currenttobest1":
            idxs = [idx for idx in range(self.popsize) if idx != j]
            r1, r2 = self.de_pop[np.random.choice(idxs, 2, replace=False)]
            temp = (
                current_idv + self.mut * (best_idv - current_idv) + self.mut * (r1 - r2)
            )
            vi = np.clip(temp, self.lower_bound, self.upper_bound)

        if self.strategy == "randtobest1":
            idxs = [idx for idx in range(self.popsize) if idx != j]
            r1, r2, r3 = self.de_pop[np.random.choice(idxs, 3, replace=False)]
            temp = r1 + self.mut * (best_idv - r1) + self.mut * (r2 - r3)
            vi = np.clip(temp, self.lower_bound, self.upper_bound)

        # perform crossover operation
        if self.bin == 1:
            cross_points = np.random.rand(self.dimensions) < self.crossp

            if not np.any(cross_points):
                cross_points[np.random.randint(0, self.dimensions)] = True
            ui = np.where(cross_points, vi, current_idv)

        else:
            i = 0
            ui = []
            fill_point = np.random.randint(0, self.dimensions)
            while i < self.dimensions and np.random.rand(0, 1) < self.crossp:
                ui[fill_point] = vi[fill_point]
                fill_point = (fill_point + 1) % self.dimensions
                i += 1

        return ui

    def run(self):

        traj = []

        # Step1: initialization
        rand_temp = np.random.rand(self.popsize, self.dimensions)
        diff = np.fabs(self.lower_bound - self.upper_bound)
        self.de_pop = self.lower_bound + rand_temp * diff

        # Step 2: population evaluation
        best = None
        for j in range(self.popsize):
            ftemp = self.f(self.de_pop[j])
            self.fitness.append(ftemp)

            if ftemp < self.fbest:
                self.fbest = ftemp
                self.idxbest = j
                best = self.de_pop[j]
            traj.append(self.fbest)

        # Step 3: Start evolutionary search
        for i in range(self.its):
            for j in range(self.popsize):
                ui = self.evolve(j)

                fit = self.f(ui)

                # Step3.5: Perform Selection
                if fit < self.fitness[j]:
                    self.fitness[j] = fit
                    self.de_pop[j] = ui
                    if fit < self.fitness[self.idxbest]:
                        self.idxbest = j
                        best = ui

                traj.append(self.fitness[self.idxbest])

        return best, traj

File Path: syne_tune/optimizer/schedulers/searchers/bore/gp_classififer.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np

import GPy


class GPModel:
    def __init__(self, kernel_type: str = "matern52"):
        self.kernel_type = kernel_type

    def fit(self, X, y):
        noise_prior = GPy.priors.Gamma(0.1, 0.1)
        noise_kernel = GPy.kern.White(X.shape[1])
        noise_kernel.set_prior(noise_prior)

        if self.kernel_type == "matern52":
            kern = GPy.kern.Matern52(X.shape[1], ARD=True) + noise_kernel
        elif self.kernel_type == "rbf":
            kern = GPy.kern.RBF(X.shape[1], ARD=True) + noise_kernel

        self.m = GPy.models.GPClassification(X, y[:, None], kernel=kern)
        self.m.optimize()

    def predict_proba(self, X):
        m = self.m.predict(X)[0]
        return m

    def predict(self, X):
        l = np.round(self.m.predict(X)[0])
        return l

File Path: syne_tune/optimizer/schedulers/searchers/bore/mlp_classififer.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np

from sklearn.neural_network import MLPClassifier


class MLP:
    def __init__(
        self,
        n_inputs: int,
        n_hidden: int = 32,
        epochs: int = 100,
        learning_rate: float = 1e-3,
        activation: str = "relu",
    ):
        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.model = MLPClassifier(
            activation=activation, hidden_layer_sizes=(n_hidden,)
        )

    def fit(self, X, y):
        self.model.fit(X, y)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

    def predict(self, X):
        return np.round(self.predict_proba(X))

File Path: syne_tune/optimizer/schedulers/searchers/bore/multi_fidelity_bore.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import logging
import numpy as np

from syne_tune.optimizer.schedulers.searchers.bore import Bore

__all__ = ["MultiFidelityBore"]

logger = logging.getLogger(__name__)


class MultiFidelityBore(Bore):
    """
    Adapts BORE (Tiao et al.) for the multi-fidelity Hyperband setting following Falkner et al. Once we collected enough
    data points on the smallest resource level, we fit a probabilistic classifier and sample from it until we have
    a sufficient amount of data points for the next higher resource level. We then refit the classifer on the data of
    this resource level. These steps are iterated until we reach the highest resource level.


    BORE: Bayesian Optimization by Density-Ratio Estimation,
    Tiao, Louis C and Klein, Aaron and Seeger, Matthias W and Bonilla, Edwin V. and Archambeau, Cedric and Ramos, Fabio
    Proceedings of the 38th International Conference on Machine Learning

    BOHB: Robust and Efficient Hyperparameter Optimization at Scale
    S. Falkner and A. Klein and F. Hutter
    Proceedings of the 35th International Conference on Machine Learning

    :param config_space: Configuration space. Constant parameters are filtered out
    :param metric: Name of metric reported by evaluation function.
    :param points_to_evaluate:
    :param gamma: Defines the percentile, i.e how many percent of configuration are used to model l(x).
    :param calibrate: If set to true, we calibrate the predictions of the classifier via CV
    :param classifier: The binary classifier to model the acquisition function.
        Choices: {'mlp', 'gp', 'xgboost', 'rf}
    :param random_seed: seed for the random number generator
    :param acq_optimizer: The optimization method to maximize the acquisition function. Choices: {'de', 'rs'}
    :param feval_acq: Maximum allowed function evaluations of the acquisition function.
    :param random_prob: probability for returning a random configurations (epsilon greedy)
    :param init_random: Number of initial random configurations before we start with the optimization.
    :param classifier_kwargs: Dict that contains all hyperparameters for the classifier
    """

    def __init__(
        self,
        config_space: dict,
        metric: str,
        points_to_evaluate=None,
        random_seed=None,
        mode: str = "max",
        gamma: float = 0.25,
        calibrate: bool = False,
        classifier: str = "xgboost",
        acq_optimizer: str = "rs_with_replacement",
        feval_acq: int = 500,
        random_prob: float = 0.0,
        init_random: int = 6,
        classifier_kwargs: dict = None,
        resource_attr: str = "epoch",
        **kwargs
    ):
        super().__init__(
            config_space,
            metric=metric,
            points_to_evaluate=points_to_evaluate,
            mode=mode,
            random_seed=random_seed,
            gamma=gamma,
            calibrate=calibrate,
            classifier=classifier,
            acq_optimizer=acq_optimizer,
            feval_acq=feval_acq,
            random_prob=random_prob,
            init_random=init_random,
            classifier_kwargs=classifier_kwargs,
            **kwargs
        )

        self.resource_attr = resource_attr
        self.resource_levels = []

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
        from syne_tune.optimizer.schedulers.synchronous.hyperband import (
            SynchronousHyperbandScheduler,
        )

        super().configure_scheduler(scheduler)
        assert isinstance(scheduler, HyperbandScheduler) or isinstance(
            scheduler, SynchronousHyperbandScheduler
        ), (
            "This searcher requires HyperbandScheduler or "
            + "SynchronousHyperbandScheduler scheduler"
        )

    def train_model(self, train_data, train_targets):
        # find the highest resource level we have at least one data points of the positive class
        min_data_points = int(1 / self.gamma)
        unique_resource_levels, counts = np.unique(
            self.resource_levels, return_counts=True
        )
        idx = np.where(counts >= min_data_points)[0]

        if len(idx) == 0:
            return

        # collect data on the highest resource level
        highest_resource_level = unique_resource_levels[idx[-1]]
        indices = np.where(self.resource_levels == highest_resource_level)[0]

        train_data = np.array([self.inputs[i] for i in indices])
        train_targets = np.array([self.targets[i] for i in indices])

        super().train_model(train_data, train_targets)

    def _update(self, trial_id: str, config: Dict, result: Dict):
        super()._update(trial_id=trial_id, config=config, result=result)
        resource_level = int(result[self.resource_attr])
        self.resource_levels.append(resource_level)

File Path: syne_tune/optimizer/schedulers/searchers/bracket_distribution.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np

from syne_tune.optimizer.scheduler import TrialScheduler


class BracketDistribution:
    """
    Configures multi-fidelity schedulers such as :class:`HyperbandScheduler` with
    distribution over brackets. This distribution can be fixed up front, or
    change adaptively during the course of an experiment.

    TODO: Support for adaptive update (needed for Hyper-Tune)
    """

    def __call__(self) -> np.ndarray:
        """
        :return: Distribution over brackets
        """
        raise NotImplementedError

    def configure(self, scheduler: TrialScheduler):
        """
        This method is called in by the scheduler just after
        `self.searcher.configure_scheduler`. The searcher must be accessible
        via `self.searcher`.
        The `__call__` method cannot be used before this method has been
        called.
        """
        raise NotImplementedError


class DefaultHyperbandBracketDistribution(BracketDistribution):
    """
    Implements default bracket distribution, where probability for each bracket
    is proportional to the number of slots in each bracket in synchronous
    Hyperband.
    """

    def __init__(self):
        self.num_brackets = None
        self.rung_levels = None
        self._distribution = None

    def configure(self, scheduler: TrialScheduler):
        from syne_tune.optimizer.schedulers import HyperbandScheduler

        assert isinstance(
            scheduler, HyperbandScheduler
        ), "Scheduler must be HyperbandScheduler"
        self.num_brackets = scheduler.terminator.num_brackets
        self.rung_levels = scheduler.rung_levels
        self._set_distribution()

    def __call__(self) -> np.ndarray:
        assert self._distribution is not None, "Call 'configure' first"
        return self._distribution

    def _set_distribution(self):
        if self.num_brackets > 1:
            smax_plus1 = len(self.rung_levels)
            assert self.num_brackets <= smax_plus1
            self._distribution = np.array(
                [
                    smax_plus1 / ((smax_plus1 - s) * self.rung_levels[s])
                    for s in range(self.num_brackets)
                ]
            )
            self._distribution /= self._distribution.sum()
        else:
            self._distribution = np.ones(1)

File Path: syne_tune/optimizer/schedulers/searchers/constrained_gp_fifo_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import logging

from syne_tune.optimizer.schedulers.searchers.cost_aware_gp_fifo_searcher import (
    MultiModelGPFIFOSearcher,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    constrained_gp_fifo_searcher_defaults,
    constrained_gp_fifo_searcher_factory,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import decode_state
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    TrialEvaluations,
    INTERNAL_CONSTRAINT_NAME,
)

logger = logging.getLogger(__name__)

__all__ = ["ConstrainedGPFIFOSearcher"]


class ConstrainedGPFIFOSearcher(MultiModelGPFIFOSearcher):
    """
    Gaussian process-based constrained hyperparameter optimization (to be used with a FIFO scheduler).

    The searcher requires a constraint metric, which is given by `constraint_attr`.

    """

    def __init__(self, config_space, metric, **kwargs):
        assert kwargs.get("constraint_attr") is not None, (
            "This searcher needs a constraint attribute. Please specify its "
            + "name in search_options['constraint_attr']"
        )
        super().__init__(config_space, metric, **kwargs)

    def _create_kwargs_int(self, kwargs):
        _kwargs = check_and_merge_defaults(
            kwargs, *constrained_gp_fifo_searcher_defaults(), dict_name="search_options"
        )
        kwargs_int = constrained_gp_fifo_searcher_factory(**_kwargs)
        self._copy_kwargs_to_kwargs_int(kwargs_int, kwargs)
        return kwargs_int

    def _copy_kwargs_to_kwargs_int(self, kwargs_int: Dict, kwargs: Dict):
        super()._copy_kwargs_to_kwargs_int(kwargs_int, kwargs)
        k = "constraint_attr"
        kwargs_int[k] = kwargs[k]

    def _call_create_internal(self, kwargs_int):
        self._constraint_attr = kwargs_int.pop("constraint_attr")
        super()._call_create_internal(kwargs_int)

    def _update(self, trial_id: str, config: Dict, result: Dict):
        # We can call the superclass method, because
        # `state_transformer.label_trial` can be called two times
        # with parts of the metrics
        super()._update(trial_id, config, result)
        # Get constraint metric
        assert self._constraint_attr in result, (
            f"Constraint metric {self._constraint_attr} not included in "
            + "reported result. Make sure your evaluation function reports it."
        )
        constr_val = float(result[self._constraint_attr])
        metrics = {INTERNAL_CONSTRAINT_NAME: constr_val}
        self.state_transformer.label_trial(
            TrialEvaluations(trial_id=trial_id, metrics=metrics), config=config
        )
        if self.debug_log is not None:
            logger.info(f"constraint_val = {constr_val}")

    def clone_from_state(self, state):
        # Create clone with mutable state taken from 'state'
        init_state = decode_state(state["state"], self._hp_ranges_in_state())
        output_skip_optimization = state["skip_optimization"]
        output_model_factory = self.state_transformer.model_factory
        # Call internal constructor
        new_searcher = ConstrainedGPFIFOSearcher(
            **self._new_searcher_kwargs_for_clone(),
            output_model_factory=output_model_factory,
            init_state=init_state,
            output_skip_optimization=output_skip_optimization,
            constraint_attr=self._constraint_attr,
        )
        new_searcher._restore_from_state(state)
        # Invalidate self (must not be used afterwards)
        self.state_transformer = None
        return new_searcher

File Path: syne_tune/optimizer/schedulers/searchers/cost_aware_gp_fifo_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import GPFIFOSearcher
from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    cost_aware_gp_fifo_searcher_defaults,
    cost_aware_coarse_gp_fifo_searcher_factory,
    cost_aware_fine_gp_fifo_searcher_factory,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import decode_state
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    ModelStateTransformer,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
)

logger = logging.getLogger(__name__)

__all__ = ["CostAwareGPFIFOSearcher"]


class MultiModelGPFIFOSearcher(GPFIFOSearcher):
    """
    Superclass for multi-model extensions of :class:`GPFIFOSearcher`. We first
    call `ModelBasedSearcher._create_internal` passing factory and
    skip_optimization predicate for the `INTERNAL_METRIC_NAME` model, then
    replace the state transformer by a multi-model one.

    """

    def _call_create_internal(self, kwargs_int):
        output_model_factory = kwargs_int.pop("output_model_factory")
        output_skip_optimization = kwargs_int.pop("output_skip_optimization")
        kwargs_int["model_factory"] = output_model_factory[INTERNAL_METRIC_NAME]
        kwargs_int["skip_optimization"] = output_skip_optimization[INTERNAL_METRIC_NAME]
        self._create_internal(**kwargs_int)
        # Replace `state_transformer`
        init_state = self.state_transformer.state
        self.state_transformer = ModelStateTransformer(
            model_factory=output_model_factory,
            init_state=init_state,
            skip_optimization=output_skip_optimization,
        )


class CostAwareGPFIFOSearcher(MultiModelGPFIFOSearcher):
    """
    Gaussian process-based cost-aware hyperparameter optimization (to be used
    with a FIFO scheduler). The searcher requires a cost metric, which is
    given by `cost_attr`.

    Implements two different variants. If `resource_attr` is given, cost values
    are read from each report and cost is modeled as c(x, r), the cost model
    being given by `kwargs['cost_model']`.
    If `resource_attr` is not given, cost values are read only at the end (just
    like the primary metric) and cost is modeled as c(x), using a default GP
    surrogate model.

    """

    def __init__(self, config_space, metric, **kwargs):
        assert kwargs.get("cost_attr") is not None, (
            "This searcher needs a cost attribute. Please specify its "
            + "name in search_options['cost_attr']"
        )
        super().__init__(config_space, metric, **kwargs)

    def _create_kwargs_int(self, kwargs):
        _kwargs = check_and_merge_defaults(
            kwargs, *cost_aware_gp_fifo_searcher_defaults(), dict_name="search_options"
        )
        # If `resource_attr` is specified, we do fine-grained, otherwise
        # coarse-grained
        if kwargs.get("resource_attr") is not None:
            logger.info(
                "Fine-grained: Modelling cost values c(x, r) "
                + "obtained at every resource level r"
            )
            kwargs_int = cost_aware_fine_gp_fifo_searcher_factory(**_kwargs)
        else:
            logger.info(
                "Coarse-grained: Modelling cost values c(x) "
                + "obtained together with metric values"
            )
            kwargs_int = cost_aware_coarse_gp_fifo_searcher_factory(**_kwargs)
        self._copy_kwargs_to_kwargs_int(kwargs_int, kwargs)
        return kwargs_int

    def clone_from_state(self, state):
        # Create clone with mutable state taken from 'state'
        init_state = decode_state(state["state"], self._hp_ranges_in_state())
        output_skip_optimization = state["skip_optimization"]
        output_model_factory = self.state_transformer.model_factory
        # Call internal constructor
        new_searcher = CostAwareGPFIFOSearcher(
            **self._new_searcher_kwargs_for_clone(),
            output_model_factory=output_model_factory,
            init_state=init_state,
            output_skip_optimization=output_skip_optimization
        )
        new_searcher._restore_from_state(state)
        # Invalidate self (must not be used afterwards)
        self.state_transformer = None
        return new_searcher

File Path: syne_tune/optimizer/schedulers/searchers/cost_aware_gp_multifidelity_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.optimizer.schedulers.searchers.gp_multifidelity_searcher import (
    GPMultiFidelitySearcher,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    cost_aware_gp_multifidelity_searcher_defaults,
    cost_aware_gp_multifidelity_searcher_factory,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import decode_state
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    ModelStateTransformer,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost_fifo_model import (
    CostSurrogateModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
    INTERNAL_COST_NAME,
)

logger = logging.getLogger(__name__)

__all__ = ["CostAwareGPMultiFidelitySearcher"]


class MultiModelGPMultiFidelitySearcher(GPMultiFidelitySearcher):
    """
    Superclass for multi-model extensions of :class:`GPMultiFidelitySearcher`.
    We first call `ModelBasedSearcher._create_internal` passing factory and
    skip_optimization predicate for the `INTERNAL_METRIC_NAME` model, then
    replace the state transformer by a multi-model one.

    """

    def _call_create_internal(self, kwargs_int):
        output_model_factory = kwargs_int.pop("output_model_factory")
        output_skip_optimization = kwargs_int.pop("output_skip_optimization")
        kwargs_int["model_factory"] = output_model_factory[INTERNAL_METRIC_NAME]
        kwargs_int["skip_optimization"] = output_skip_optimization[INTERNAL_METRIC_NAME]
        super()._call_create_internal(kwargs_int)
        # Replace `state_transformer`
        init_state = self.state_transformer.state
        self.state_transformer = ModelStateTransformer(
            model_factory=output_model_factory,
            init_state=init_state,
            skip_optimization=output_skip_optimization,
        )


class CostAwareGPMultiFidelitySearcher(MultiModelGPMultiFidelitySearcher):
    """
    Gaussian process-based cost-aware multi-fidelity hyperparameter
    optimization (to be used with `HyperbandScheduler`). The searcher requires
    a cost metric, which is given by `cost_attr`.

    The acquisition function used here is the same as in
    :class:`GPMultiFidelitySearcher`, but expected improvement (EI) is replaced
    by EIpu (see :class:`EIpuAcquisitionFunction`).

    Cost values are read from each report and cost is modeled as c(x, r), the
    cost model being given by `kwargs['cost_model']`.

    """

    def __init__(self, config_space, metric, **kwargs):
        assert kwargs.get("cost_attr") is not None, (
            "This searcher needs a cost attribute. Please specify its "
            + "name in search_options['cost_attr']"
        )
        super().__init__(config_space, metric, **kwargs)

    def _create_kwargs_int(self, kwargs):
        _kwargs = check_and_merge_defaults(
            kwargs,
            *cost_aware_gp_multifidelity_searcher_defaults(),
            dict_name="search_options"
        )
        kwargs_int = cost_aware_gp_multifidelity_searcher_factory(**_kwargs)
        self._copy_kwargs_to_kwargs_int(kwargs_int, kwargs)
        return kwargs_int

    def _fix_resource_attribute(self, **kwargs):
        if self.resource_for_acquisition is not None:
            super()._fix_resource_attribute(**kwargs)
            fixed_resource = self.config_space_ext.hp_ranges_ext.value_for_last_pos
        else:
            # Cost at r_max
            fixed_resource = self.config_space_ext.resource_attr_range[1]
        cost_model_factory = self.state_transformer.model_factory[INTERNAL_COST_NAME]
        assert isinstance(cost_model_factory, CostSurrogateModelFactory)
        cost_model_factory.set_fixed_resource(fixed_resource)

    def clone_from_state(self, state):
        # Create clone with mutable state taken from 'state'
        init_state = decode_state(state["state"], self._hp_ranges_in_state())
        output_skip_optimization = state["skip_optimization"]
        output_model_factory = self.state_transformer.model_factory
        # Call internal constructor
        new_searcher = CostAwareGPMultiFidelitySearcher(
            **self._new_searcher_kwargs_for_clone(),
            output_model_factory=output_model_factory,
            init_state=init_state,
            output_skip_optimization=output_skip_optimization,
            config_space_ext=self.config_space_ext,
            resource_for_acquisition=self.resource_for_acquisition
        )
        new_searcher._restore_from_state(state)
        # Invalidate self (must not be used afterwards)
        self.state_transformer = None
        return new_searcher

File Path: syne_tune/optimizer/schedulers/searchers/gp_fifo_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
from typing import Type, Optional, Dict, List
import logging
import copy
import time

from syne_tune.optimizer.schedulers.searchers.searcher import (
    SearcherWithRandomSeed,
    RandomSearcher,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    gp_fifo_searcher_factory,
    gp_fifo_searcher_defaults,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import (
    DEFAULT_INITIAL_SCORING,
    SUPPORTED_INITIAL_SCORING,
    MapReward,
    encode_state,
    decode_state,
)
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    TrialEvaluations,
    Configuration,
    MetricValues,
    dictionarize_objective,
    INTERNAL_METRIC_NAME,
    INTERNAL_COST_NAME,
    ConfigurationFilter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    TransformerModelFactory,
    ModelStateTransformer,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_skipopt import (
    SkipOptimizationPredicate,
    AlwaysSkipPredicate,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.base_classes import (
    LocalOptimizer,
    ScoringFunction,
    SurrogateOutputModel,
    AcquisitionClassAndArgs,
    unwrap_acquisition_class_and_kwargs,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm import (
    BayesianOptimizationAlgorithm,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm_components import (
    IndependentThompsonSampling,
    NoOptimization,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.common import (
    RandomStatefulCandidateGenerator,
    ExclusionList,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.defaults import (
    DEFAULT_LOCAL_OPTIMIZER_CLASS,
    DEFAULT_NUM_INITIAL_CANDIDATES,
    DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.duplicate_detector import (
    DuplicateDetectorIdentical,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler

logger = logging.getLogger(__name__)

__all__ = ["ModelBasedSearcher", "GPFIFOSearcher"]


GET_CONFIG_RANDOM_RETRIES = 50


def create_initial_candidates_scorer(
    initial_scoring: str,
    model: SurrogateOutputModel,
    acquisition_class: AcquisitionClassAndArgs,
    random_state: np.random.RandomState,
    active_output: str = INTERNAL_METRIC_NAME,
) -> ScoringFunction:
    if initial_scoring == "thompson_indep":
        if isinstance(model, Dict):
            assert active_output in model
            model = model[active_output]
        return IndependentThompsonSampling(model, random_state=random_state)
    else:
        acquisition_class, acquisition_kwargs = unwrap_acquisition_class_and_kwargs(
            acquisition_class
        )
        return acquisition_class(
            model, active_metric=active_output, **acquisition_kwargs
        )


def check_initial_candidates_scorer(initial_scoring: str) -> str:
    if initial_scoring is None:
        return DEFAULT_INITIAL_SCORING
    else:
        assert (
            initial_scoring in SUPPORTED_INITIAL_SCORING
        ), "initial_scoring = '{}' is not supported".format(initial_scoring)
        return initial_scoring


class ModelBasedSearcher(SearcherWithRandomSeed):
    """Common code for surrogate model based searchers"""

    def _create_internal(
        self,
        hp_ranges: HyperparameterRanges,
        model_factory: TransformerModelFactory,
        acquisition_class: AcquisitionClassAndArgs,
        map_reward: Optional[MapReward] = None,
        init_state: TuningJobState = None,
        local_minimizer_class: Type[LocalOptimizer] = None,
        skip_optimization: SkipOptimizationPredicate = None,
        num_initial_candidates: int = DEFAULT_NUM_INITIAL_CANDIDATES,
        num_initial_random_choices: int = DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,
        initial_scoring: Optional[str] = None,
        skip_local_optimization: bool = False,
        cost_attr: Optional[str] = None,
        resource_attr: Optional[str] = None,
        filter_observed_data: Optional[ConfigurationFilter] = None,
    ):
        self.hp_ranges = hp_ranges
        self.num_initial_candidates = num_initial_candidates
        self.num_initial_random_choices = num_initial_random_choices
        self.map_reward = map_reward
        if skip_local_optimization:
            self.local_minimizer_class = NoOptimization
        elif local_minimizer_class is None:
            self.local_minimizer_class = DEFAULT_LOCAL_OPTIMIZER_CLASS
        else:
            self.local_minimizer_class = local_minimizer_class
        self.acquisition_class = acquisition_class
        self._debug_log = model_factory.debug_log
        self.initial_scoring = check_initial_candidates_scorer(initial_scoring)
        self.skip_local_optimization = skip_local_optimization
        # Create state transformer
        # Initial state is empty (note that the state is mutable)
        if init_state is None:
            init_state = TuningJobState.empty_state(self._hp_ranges_in_state())
        self.state_transformer = ModelStateTransformer(
            model_factory=model_factory,
            init_state=init_state,
            skip_optimization=skip_optimization,
        )
        self.random_generator = RandomStatefulCandidateGenerator(
            self._hp_ranges_for_prediction(), random_state=self.random_state
        )
        self.set_profiler(model_factory.profiler)
        self._cost_attr = cost_attr
        self._resource_attr = resource_attr
        self._filter_observed_data = filter_observed_data
        self._random_searcher = None
        # Tracks the cumulative time spent in `get_config` calls
        self.cumulative_get_config_time = 0
        if model_factory.debug_log is not None:
            deb_msg = "[ModelBasedSearcher.__init__]\n"
            deb_msg += "- acquisition_class = {}\n".format(acquisition_class)
            deb_msg += "- local_minimizer_class = {}\n".format(local_minimizer_class)
            deb_msg += "- num_initial_candidates = {}\n".format(num_initial_candidates)
            deb_msg += "- num_initial_random_choices = {}\n".format(
                num_initial_random_choices
            )
            deb_msg += "- initial_scoring = {}\n".format(self.initial_scoring)
            logger.info(deb_msg)

    def _copy_kwargs_to_kwargs_int(self, kwargs_int: Dict, kwargs: Dict):
        # Extra arguments not parsed in factory
        for k in ("init_state", "local_minimizer_class", "cost_attr", "resource_attr"):
            kwargs_int[k] = kwargs.get(k)

    def _hp_ranges_in_state(self):
        """
        :return: HyperparameterRanges to be used in self.state_transformer.state
        """
        return self.hp_ranges

    def _hp_ranges_for_prediction(self):
        """
        :return: HyperparameterRanges to be used in predictions and acquisition
            functions
        """
        return self._hp_ranges_in_state()

    def _metric_val_update(self, crit_val: float, result: Dict) -> MetricValues:
        return crit_val

    def on_trial_result(self, trial_id: str, config: Dict, result: Dict, update: bool):
        # If both `cost_attr` and `resource_attr` are given, cost data (if
        # given) is written out from every `result`, independent of `update`
        cattr = self._cost_attr
        rattr = self._resource_attr
        if (
            cattr is not None
            and rattr is not None
            and cattr in result
            and rattr in result
        ):
            cost_val = float(result[cattr])
            resource = str(result[rattr])
            metrics = {INTERNAL_COST_NAME: {resource: cost_val}}
            self.state_transformer.label_trial(
                TrialEvaluations(trial_id=trial_id, metrics=metrics), config=config
            )
        if update:
            self._update(trial_id, config, result)

    def _trial_id_string(self, trial_id: str, result: Dict):
        """
        For multi-fidelity, we also want to output the resource level
        """
        return trial_id

    def _update(self, trial_id: str, config: Dict, result: Dict):
        metric_val = result[self._metric]
        if self.map_reward is not None:
            crit_val = self.map_reward(metric_val)
        else:
            crit_val = metric_val
        metrics = dictionarize_objective(self._metric_val_update(crit_val, result))
        # Cost value only dealt with here if `resource_attr` not given
        attr = self._cost_attr
        cost_val = None
        if attr is not None and attr in result:
            cost_val = float(result[attr])
            if self._resource_attr is None:
                metrics[INTERNAL_COST_NAME] = cost_val
        self.state_transformer.label_trial(
            TrialEvaluations(trial_id=trial_id, metrics=metrics), config=config
        )
        if self.debug_log is not None:
            _trial_id = self._trial_id_string(trial_id, result)
            msg = f"Update for trial_id {_trial_id}: metric = {metric_val:.3f}"
            if self.map_reward is not None:
                msg += f", crit_val = {crit_val:.3f}"
            if cost_val is not None:
                msg += f", cost_val = {cost_val:.2f}"
            logger.info(msg)

    def _get_config_modelbased(
        self, exclusion_candidates: ExclusionList, **kwargs
    ) -> Optional[Configuration]:
        raise NotImplementedError()

    def _get_exclusion_candidates(self, **kwargs) -> ExclusionList:
        return ExclusionList(
            self.state_transformer.state,
            filter_observed_data=self._filter_observed_data,
        )

    def _should_pick_random_config(self, exclusion_candidates: ExclusionList) -> bool:
        if len(exclusion_candidates) < self.num_initial_random_choices:
            return True
        # Determine whether there is any observed data after filtering
        state = self.state_transformer.state
        if not state.trials_evaluations:
            return True
        if self._filter_observed_data is None:
            return False
        for ev in state.trials_evaluations:
            config = state.config_for_trial[ev.trial_id]
            if self._filter_observed_data(config):
                return False
        return True

    def _get_config_not_modelbased(
        self, exclusion_candidates: ExclusionList
    ) -> (Optional[Configuration], bool):
        """
        Does job of `get_config`, as long as the decision does not involve
        model-based search. If False is returned, model-based search must be
        called.

        """
        self._assign_random_searcher()
        config = self._next_initial_config()  # Ask for initial config
        if config is None:
            pick_random = self._should_pick_random_config(exclusion_candidates)
        else:
            pick_random = True  # Initial configs count as random here
        if pick_random and config is None:
            if self.do_profile:
                self.profiler.start("random")
            for _ in range(GET_CONFIG_RANDOM_RETRIES):
                _config = self._random_searcher.get_config()
                if _config is None:
                    # If `RandomSearcher` returns no config at all, the
                    # search space is exhausted
                    break
                if not exclusion_candidates.contains(_config):
                    config = _config
                    break
            if self.do_profile:
                self.profiler.stop("random")
        return config, pick_random

    def get_config(self, **kwargs) -> Configuration:
        """
        Runs Bayesian optimization in order to suggest the next config to evaluate.

        :return: Next config to evaluate at
        """
        start_time = time.time()
        state = self.state_transformer.state
        if self.do_profile:
            # Start new profiler block
            skip_optimization = self.state_transformer.skip_optimization
            if isinstance(skip_optimization, dict):
                skip_optimization = skip_optimization[INTERNAL_METRIC_NAME]
            meta = {
                "fit_hyperparams": not skip_optimization(state),
                "num_observed": state.num_observed_cases(),
                "num_pending": len(state.pending_evaluations),
            }
            self.profiler.begin_block(meta)
            self.profiler.start("all")
            # Initial configs come from `points_to_evaluate` or are drawn at random
        exclusion_candidates = self._get_exclusion_candidates(**kwargs)
        config, pick_random = self._get_config_not_modelbased(exclusion_candidates)
        if self.debug_log is not None:
            trial_id = kwargs.get("trial_id")
            self.debug_log.start_get_config(
                "random" if pick_random else "BO", trial_id=trial_id
            )
        if not pick_random:
            # Model-based decision
            if not exclusion_candidates.config_space_exhausted():
                config = self._get_config_modelbased(exclusion_candidates, **kwargs)

        if config is not None:
            if self.debug_log is not None:
                self.debug_log.set_final_config(config)
                # All get_config debug log info is only written here
                self.debug_log.write_block()
        else:
            msg = (
                "Failed to sample a configuration not already chosen "
                + f"before. Exclusion list has size {len(exclusion_candidates)}."
            )
            cs_size = exclusion_candidates.configspace_size
            if cs_size is not None:
                msg += f" Configuration space has size {cs_size}."
            logger.warning(msg)
        if self.do_profile:
            self.profiler.stop("all")
            self.profiler.clear()
        self.cumulative_get_config_time += time.time() - start_time

        return config

    def dataset_size(self):
        return self.state_transformer.state.num_observed_cases()

    def model_parameters(self):
        return self.state_transformer.get_params()

    def set_params(self, param_dict):
        self.state_transformer.set_params(param_dict)

    def get_state(self):
        """
        The mutable state consists of the GP model parameters, the
        TuningJobState, and the skip_optimization predicate (which can have a
        mutable state).
        We assume that skip_optimization can be pickled.

        """
        state = dict(
            super().get_state(),
            model_params=self.model_parameters(),
            state=encode_state(self.state_transformer.state),
            skip_optimization=self.state_transformer.skip_optimization,
        )
        if self._random_searcher is not None:
            state["random_searcher_state"] = self._random_searcher.get_state()
        return state

    def _restore_from_state(self, state: dict):
        super()._restore_from_state(state)
        self.state_transformer.set_params(state["model_params"])
        self.random_generator.random_state = self.random_state
        if "random_searcher_state" in state:
            # Restore self._random_searcher as well
            # Note: It is important to call `_assign_random_searcher` with a
            # random seed. Otherwise, one is drawn from `random_state`, which
            # modifies that state. The seed passed does not matter, since
            # `_random_searcher.random_state` will be restored anyway
            self._assign_random_searcher(random_seed=0)
            self._random_searcher._restore_from_state(state["random_searcher_state"])

    def set_profiler(self, profiler: Optional[SimpleProfiler]):
        self.profiler = profiler
        self.do_profile = profiler is not None

    @property
    def debug_log(self):
        return self._debug_log

    def _assign_random_searcher(self, random_seed=None):
        if self._random_searcher is None:
            # Used for initial random configs (if any)
            # We do not have to deal with points_to_evaluate
            if random_seed is None:
                random_seed = self.random_state.randint(0, 2**32)
            self._random_searcher = RandomSearcher(
                self.hp_ranges.config_space_for_sampling,
                metric=self._metric,
                points_to_evaluate=[],
                random_seed=random_seed,
                debug_log=False,
            )


class GPFIFOSearcher(ModelBasedSearcher):
    """Gaussian process Bayesian optimization for FIFO scheduler

    This searcher must be used with `FIFOScheduler`. It provides Bayesian
    optimization, based on a Gaussian process surrogate model.

    NOTE: The searcher uses `map_reward` to map metric values to internal
    criterion values, and *minimizes* the latter. If your metric is to be
    maximized, you need to pass a strictly decreasing `map_reward`.

    Pending configurations (for which evaluation tasks are currently running)
    are dealt with by fantasizing (i.e., target values are drawn from the
    current posterior, and acquisition functions are averaged over this
    sample, see `num_fantasy_samples`).
    The GP surrogate model uses a Matern 5/2 covariance function with automatic
    relevance determination (ARD) of input attributes, and a constant mean
    function. The acquisition function is expected improvement (EI). All
    hyperparameters of the surrogate model are estimated by empirical Bayes
    (maximizing the marginal likelihood). In general, this hyperparameter
    fitting is the most expensive part of a `get_config` call.

    The following happens in `get_config`. For the first `num_init_random` calls,
    a config is drawn at random (the very first call results in the default
    config of the space). Afterwards, Bayesian optimization is used, unless
    there are no finished evaluations yet.
    First, model hyperparameter are refit. This step can be skipped (see
    `opt_skip*` parameters). Next, `num_init_candidates` configs are sampled at
    random, and ranked by a scoring function (`initial_scoring`). BFGS local
    optimization is then run starting from the top scoring config, where EI
    is minimized.

    Parameters
    ----------
    config_space : Dict
        Configuration space. Constant parameters are filtered out
    metric : str
        Name of metric reported by evaluation function.
    points_to_evaluate: List[Dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified, or even be an
        empty dict. For each hyperparameter not specified, the default value
        is determined using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
    random_seed_generator : RandomSeedGenerator (optional)
        If given, the random_seed for `random_state` is obtained from there,
        otherwise `random_seed` is used
    random_seed : int (optional)
        This is used if `random_seed_generator` is not given.
    debug_log : bool (default: False)
        If True, both searcher and scheduler output an informative log, from
        which the configs chosen and decisions being made can be traced.
    resource_attr : str (optional)
        Name of resource attribute in reports. This is optional here, but
        required for multi-fidelity searchers.
        If `resource_attr` and `cost_attr` are given, cost values are read from
        each report and stored in the state. This allows cost models to be fit
        on more data.
    cost_attr : str (optional)
        Name of cost attribute in data obtained from reporter (e.g., elapsed
        training time). Needed only by cost-aware searchers. Depending on
        whether `resource_attr` is given, cost values are read from each
        report or only at the end.
    num_init_random : int
        Number of initial `get_config` calls for which randomly sampled configs
        are returned. Afterwards, Bayesian optimization is used
    num_init_candidates : int
        Number of initial candidates sampled at random in order to seed the
        search for `get_config`
    num_fantasy_samples : int
        Number of samples drawn for fantasizing (latent target values for
        pending evaluations)
    no_fantasizing : bool
        If True, fantasizing is not done and pending evaluations are ignored.
        This may lead to loss of diversity in decisions
    initial_scoring : str
        Scoring function to rank initial candidates (local optimization of EI
        is started from top scorer). Values are 'thompson_indep' (independent
        Thompson sampling; randomized score, which can increase exploration),
        'acq_func' (score is the same (EI) acquisition function which is afterwards
        locally optimized).
    skip_local_optimization : bool
        If True, the local gradient-based optimization of the acquisition
        function is skipped, and the top-tanked initial candidate is returned
        instead. In this case, `initial_scoring='acq_func'` makes most sense,
        otherwise the acquisition function will not be used.
    opt_nstarts : int
        Parameter for hyperparameter fitting. Number of random restarts
    opt_maxiter : int
        Parameter for hyperparameter fitting. Maximum number of iterations
        per restart
    opt_warmstart : bool
        Parameter for hyperparameter fitting. If True, each fitting is started
        from the previous optimum. Not recommended in general
    opt_verbose : bool
        Parameter for hyperparameter fitting. If True, lots of output
    opt_skip_init_length : int
        Parameter for hyperparameter fitting, skip predicate. Fitting is never
        skipped as long as number of observations below this threshold
    opt_skip_period : int
        Parameter for hyperparameter fitting, skip predicate. If >1, and number
        of observations above `opt_skip_init_length`, fitting is done only
        K-th call, and skipped otherwise
    map_reward : str or MapReward
        If `mode == 'max'`, the scheduler maximizes reward, while
        internally, Bayesian optimization is minimizing the criterion. States
        how reward is mapped to criterion. If the mode is 'min', this
        argument is ignored.
        Built-in are `minus_x` (criterion = -reward) and `<a>_minus_x`, where
        <a> is a constant (criterion = <a> - reward), for example `1_minus_x`.
        From a technical standpoint, it does not matter what is chosen here,
        because criterion is only used internally. Also note that criterion
        data is always normalized to mean 0, variance 1 before fitted with a
        GP.
    transfer_learning_task_attr : str (optional)
        Used to support transfer HPO, where the state contains observed data
        from several tasks, one of which is the active one. To this end,
        `config_space` must contain a categorical parameter of name
        `transfer_learning_task_attr`, whose range are all task IDs. Also,
        `transfer_learning_active_task` must denote the active task, and
        `transfer_learning_active_config_space` is used as
        `active_config_space` argument in :class:`HyperparameterRanges`. This
        allows us to use a narrower search space for the active task than for
        the union of all tasks (`config_space` must be that), which is needed
        if some configurations of non-active tasks lie outside of the ranges
        in `active_config_space`.
        One of the implications is that `filter_observed_data` is selecting
        configs of the active task, so that incumbents or exclusion lists are
        restricted to data from the active task.
    transfer_learning_active_task : str (optional)
        See `transfer_learning_task_attr`.
    transfer_learning_active_config_space : Dict (optional)
        See `transfer_learning_task_attr`. If not given, `config_space` is the
        search space for the active task as well. This active config space need
        not contain the `transfer_learning_task_attr` parameter. In fact, this
        parameter is set to a categorical with `transfer_learning_active_task`
        as single value, so that new configs are chosen for the active task
        only.
    transfer_learning_model : str (optional)
        See `transfer_learning_task_attr`. Specifies the surrogate model to be
        used for transfer lerning:
        - 'matern52_product': Kernel is product of Matern 5/2 (not ARD) on
            `transfer_learning_task_attr` and Matern 5/2 (ARD) on the rest.
            Assumes that data from same task are more closely related than
            data from different tasks
        - 'matern52_same': Kernel is Matern 5/2 (ARD) on the rest of the
            variables, `transfer_learning_task_attr` is ignored. Assumes
            that data from all tasks can be merged together

    """

    def __init__(self, config_space, metric, clone_from_state=False, **kwargs):
        if not clone_from_state:
            super().__init__(
                config_space,
                metric=metric,
                points_to_evaluate=kwargs.get("points_to_evaluate"),
                random_seed_generator=kwargs.get("random_seed_generator"),
                random_seed=kwargs.get("random_seed"),
            )
            kwargs["config_space"] = config_space
            kwargs["metric"] = metric
            kwargs_int = self._create_kwargs_int(kwargs)
        else:
            # Internal constructor, bypassing the factory
            # Note: Members which are part of the mutable state, will be
            # overwritten in `_restore_from_state`
            super().__init__(config_space, metric=metric)
            kwargs_int = kwargs.copy()
        self._call_create_internal(kwargs_int)

    def _create_kwargs_int(self, kwargs):
        _kwargs = check_and_merge_defaults(
            kwargs, *gp_fifo_searcher_defaults(), dict_name="search_options"
        )
        kwargs_int = gp_fifo_searcher_factory(**_kwargs)
        # Extra arguments not parsed in factory
        self._copy_kwargs_to_kwargs_int(kwargs_int, kwargs)
        return kwargs_int

    def _call_create_internal(self, kwargs_int):
        """
        Part of constructor which can be different in subclasses
        """
        self._create_internal(**kwargs_int)

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.fifo import FIFOScheduler

        assert isinstance(
            scheduler, FIFOScheduler
        ), "This searcher requires FIFOScheduler scheduler"
        super().configure_scheduler(scheduler)
        # Allow model factory to depend on `scheduler` as well
        model_factory = self.state_transformer.model_factory
        model_factory.configure_scheduler(scheduler)

    def register_pending(
        self, trial_id: str, config: Optional[Dict] = None, milestone=None
    ):
        """
        Registers trial as pending. This means the corresponding evaluation
        task is running. Once it finishes, update is called for this trial.

        """
        # It is OK for the candidate already to be registered as pending, in
        # which case we do nothing
        state = self.state_transformer.state
        if not state.is_pending(trial_id):
            assert not state.is_labeled(trial_id), (
                f"Trial trial_id = {trial_id} is already labeled, so cannot "
                + "be pending"
            )
            self.state_transformer.append_trial(trial_id, config=config)

    def _fix_resource_attribute(self, **kwargs):
        pass

    def _postprocess_config(self, config: dict) -> dict:
        return config

    def _get_config_modelbased(
        self, exclusion_candidates, **kwargs
    ) -> Optional[Configuration]:
        # Obtain current SurrogateModel from state transformer. Based on
        # this, the BO algorithm components can be constructed
        if self.do_profile:
            self.profiler.push_prefix("getconfig")
            self.profiler.start("all")
            self.profiler.start("gpmodel")
        # Note: Asking for the model triggers the posterior computation
        model = self.state_transformer.model()
        if self.do_profile:
            self.profiler.stop("gpmodel")
        # Select and fix target resource attribute (relevant in subclasses)
        self._fix_resource_attribute(**kwargs)
        # Create BO algorithm
        initial_candidates_scorer = create_initial_candidates_scorer(
            initial_scoring=self.initial_scoring,
            model=model,
            acquisition_class=self.acquisition_class,
            random_state=self.random_state,
        )
        local_optimizer = self.local_minimizer_class(
            hp_ranges=self._hp_ranges_for_prediction(),
            model=model,
            acquisition_class=self.acquisition_class,
            active_metric=INTERNAL_METRIC_NAME,
        )
        bo_algorithm = BayesianOptimizationAlgorithm(
            initial_candidates_generator=self.random_generator,
            initial_candidates_scorer=initial_candidates_scorer,
            num_initial_candidates=self.num_initial_candidates,
            local_optimizer=local_optimizer,
            pending_candidate_state_transformer=None,
            exclusion_candidates=exclusion_candidates,
            num_requested_candidates=1,
            greedy_batch_selection=False,
            duplicate_detector=DuplicateDetectorIdentical(),
            profiler=self.profiler,
            sample_unique_candidates=False,
            debug_log=self.debug_log,
        )
        # Next candidate decision
        _config = bo_algorithm.next_candidates()
        if len(_config) > 0:
            config = self._postprocess_config(_config[0])
        else:
            config = None
        if self.do_profile:
            self.profiler.stop("all")
            self.profiler.pop_prefix()  # getconfig
        return config

    def get_batch_configs(
        self,
        batch_size: int,
        num_init_candidates_for_batch: Optional[int] = None,
        **kwargs,
    ) -> List[Configuration]:
        """
        Asks for a batch of `batch_size` configurations to be suggested. This
        is roughly equivalent to calling `get_config` `batch_size` times,
        marking the suggested configs as pending in the state (but the state
        is not modified here).
        If `num_init_candidates_for_batch` is given, it is used instead
        of `num_init_candidates` for the selection of all but the first
        config in the batch. In order to speed up batch selection, choose
        `num_init_candidates_for_batch` smaller than
        `num_init_candidates`.

        If less than `batch_size` configs are returned, the search space
        has been exhausted.

        Note: Batch selection does not support `debug_log` right now: make sure
        to switch this off when creating scheduler and searcher.
        """
        assert round(batch_size) == batch_size and batch_size >= 1
        configs = []
        if batch_size == 1:
            config = self.get_config(**kwargs)
            if config is not None:
                configs.append(config)
        else:
            # `DebugLogWriter` does not support batch selection right now,
            # must be switched off
            assert self.debug_log is None, (
                "get_batch_configs does not support debug_log right now. "
                + "Please set debug_log=False in search_options argument "
                + "of scheduler, or create your searcher with debug_log=False"
            )
            exclusion_candidates = self._get_exclusion_candidates(**kwargs)
            pick_random = True
            while pick_random and len(configs) < batch_size:
                config, pick_random = self._get_config_not_modelbased(
                    exclusion_candidates
                )
                if pick_random:
                    if config is not None:
                        configs.append(config)
                        exclusion_candidates.add(config)
                    else:
                        break  # Space exhausted
            if not pick_random:
                # Model-based decision for remaining ones
                num_requested_candidates = batch_size - len(configs)
                model = self.state_transformer.model()
                # Select and fix target resource attribute (relevant in subclasses)
                self._fix_resource_attribute(**kwargs)
                # Create BO algorithm
                initial_candidates_scorer = create_initial_candidates_scorer(
                    initial_scoring=self.initial_scoring,
                    model=model,
                    acquisition_class=self.acquisition_class,
                    random_state=self.random_state,
                )
                local_optimizer = self.local_minimizer_class(
                    hp_ranges=self._hp_ranges_for_prediction(),
                    model=model,
                    acquisition_class=self.acquisition_class,
                    active_metric=INTERNAL_METRIC_NAME,
                )
                pending_candidate_state_transformer = None
                if num_requested_candidates > 1:
                    # Internally, if num_requested_candidates > 1, the candidates are
                    # selected greedily. This needs model updates after each greedy
                    # selection, because of one more pending evaluation.
                    model_factory = self.state_transformer._model_factory
                    if isinstance(model_factory, dict):
                        model_factory = model_factory[INTERNAL_METRIC_NAME]
                    # We need a copy of the state here, since
                    # `pending_candidate_state_transformer` modifies the state (it
                    # appends pending trials)
                    temporary_state = copy.deepcopy(self.state_transformer.state)
                    pending_candidate_state_transformer = ModelStateTransformer(
                        model_factory=model_factory,
                        init_state=temporary_state,
                        skip_optimization=AlwaysSkipPredicate(),
                    )
                bo_algorithm = BayesianOptimizationAlgorithm(
                    initial_candidates_generator=self.random_generator,
                    initial_candidates_scorer=initial_candidates_scorer,
                    num_initial_candidates=self.num_initial_candidates,
                    num_initial_candidates_for_batch=num_init_candidates_for_batch,
                    local_optimizer=local_optimizer,
                    pending_candidate_state_transformer=pending_candidate_state_transformer,
                    exclusion_candidates=exclusion_candidates,
                    num_requested_candidates=num_requested_candidates,
                    greedy_batch_selection=True,
                    duplicate_detector=DuplicateDetectorIdentical(),
                    sample_unique_candidates=False,
                    debug_log=self.debug_log,
                )
                # Next candidate decision
                _configs = bo_algorithm.next_candidates()
                configs.extend(self._postprocess_config(config) for config in _configs)
        return configs

    def evaluation_failed(self, trial_id: str):
        # Remove pending evaluation
        self.state_transformer.drop_pending_evaluation(trial_id)
        # Mark config as failed (which means it will be blacklisted in
        # future get_config calls)
        self.state_transformer.mark_trial_failed(trial_id)

    def _new_searcher_kwargs_for_clone(self) -> Dict:
        """
        Helper method for `clone_from_state`. Args need to be extended
        by `model_factory`, `init_state`, `skip_optimization`, and others
        args becoming relevant in subclasses only.

        :return: kwargs for creating new searcher object in `clone_from_state`
        """
        return dict(
            config_space=self.config_space,
            metric=self._metric,
            clone_from_state=True,
            hp_ranges=self.hp_ranges,
            acquisition_class=self.acquisition_class,
            map_reward=self.map_reward,
            local_minimizer_class=self.local_minimizer_class,
            num_initial_candidates=self.num_initial_candidates,
            num_initial_random_choices=self.num_initial_random_choices,
            initial_scoring=self.initial_scoring,
            skip_local_optimization=self.skip_local_optimization,
            cost_attr=self._cost_attr,
            resource_attr=self._resource_attr,
            filter_observed_data=self._filter_observed_data,
        )

    def clone_from_state(self, state):
        # Create clone with mutable state taken from 'state'
        init_state = decode_state(state["state"], self._hp_ranges_in_state())
        skip_optimization = state["skip_optimization"]
        model_factory = self.state_transformer.model_factory
        # Call internal constructor
        new_searcher = GPFIFOSearcher(
            **self._new_searcher_kwargs_for_clone(),
            model_factory=model_factory,
            init_state=init_state,
            skip_optimization=skip_optimization,
        )
        new_searcher._restore_from_state(state)
        # Invalidate self (must not be used afterwards)
        self.state_transformer = None
        return new_searcher

File Path: syne_tune/optimizer/schedulers/searchers/gp_multifidelity_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional
import logging

from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    gp_multifidelity_searcher_factory,
    gp_multifidelity_searcher_defaults,
)
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import (
    GPFIFOSearcher,
    decode_state,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import (
    ResourceForAcquisitionMap,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    PendingEvaluation,
    MetricValues,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gpiss_model import (
    GaussProcAdditiveModelFactory,
)

logger = logging.getLogger(__name__)

__all__ = ["GPMultiFidelitySearcher"]


class GPMultiFidelitySearcher(GPFIFOSearcher):
    """Gaussian process Bayesian optimization for Hyperband scheduler

    This searcher must be used with `HyperbandScheduler`. It provides a novel
    combination of Bayesian optimization, based on a Gaussian process surrogate
    model, with Hyperband scheduling. In particular, observations across
    resource levels are modelled jointly. It is created along with the
    scheduler, using `searcher='bayesopt'`:

    Most of `GPFIFOSearcher` comments apply here as well.
    In multi-fidelity HPO, we optimize a function f(x, r), x the configuration,
    r the resource (or time) attribute. The latter must be a positive integer.
    In most applications, `resource_attr` == 'epoch', and the resource is the
    number of epochs already trained.

    We model the function f(x, r) jointly over all resource levels r at which
    it is observed (but see `searcher_data` in `HyperbandScheduler`). The kernel
    and mean function of our surrogate model are over (x, r). The surrogate
    model is selected by `gp_resource_kernel`. More details about the supported
    kernels is in:

        Tiao, Klein, Lienart, Archambeau, Seeger (2020)
        Model-based Asynchronous Hyperparameter and Neural Architecture Search
        https://arxiv.org/abs/2003.10865

    The acquisition function (EI) which is optimized in `get_config`, is obtained
    by fixing the resource level r to a value which is determined depending on
    the current state. If `resource_acq` == 'bohb', r is the largest value
    <= max_t, where we have seen >= dimension(x) metric values. If
    `resource_acq` == 'first', r is the first milestone which config x would
    reach when started.

    Parameters
    ----------
    config_space : Dict
        Configuration space. Constant parameters are filtered out
    metric : str
        Name of reward attribute reported by evaluation function
    points_to_evaluate: List[Dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified, or even be an
        empty dict. For each hyperparameter not specified, the default value
        is determined using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
    random_seed_generator : RandomSeedGenerator (optional)
        If given, the random_seed for `random_state` is obtained from there,
        otherwise `random_seed` is used
    random_seed : int (optional)
        This is used if `random_seed_generator` is not given.
    resource_attr : str
        Name of resource attribute in reports, equal to `resource_attr` of
        scheduler
    debug_log : bool (default: False)
        If True, both searcher and scheduler output an informative log, from
        which the configs chosen and decisions being made can be traced.
    cost_attr : str (optional)
        Name of cost attribute in data obtained from reporter (e.g., elapsed
        training time). Needed only by cost-aware searchers.
    model : str
        Selects surrogate model (learning curve model) to be used. Choices
        are 'gp_multitask' (default), 'gp_independent', 'gp_issm',
        'gp_expdecay'
    num_init_random : int
        See :class:`GPFIFOSearcher`
    num_init_candidates : int
        See :class:`GPFIFOSearcher`
    num_fantasy_samples : int
        See :class:`GPFIFOSearcher`
    no_fantasizing : bool
        See :class:`GPFIFOSearcher`
    initial_scoring : str
        See :class:`GPFIFOSearcher`
    skip_local_optimization : str
        See :class:`GPFIFOSearcher`
    opt_nstarts : int
        See :class:`GPFIFOSearcher`
    opt_maxiter : int
        See :class:`GPFIFOSearcher`
    opt_warmstart : bool
        See :class:`GPFIFOSearcher`
    opt_verbose : bool
        See :class:`GPFIFOSearcher`
    opt_skip_init_length : int
        See :class:`GPFIFOSearcher`
    opt_skip_period : int
        See `:class:GPFIFOSearcher`
    map_reward : str or MapReward
        See :class:`GPFIFOSearcher`
    gp_resource_kernel : str
        Only relevant for `model == 'gp_multitask'`.
        Surrogate model over criterion function f(x, r), x the config, r the
        resource. Note that x is encoded to be a vector with entries in [0, 1],
        and r is linearly mapped to [0, 1], while the criterion data is
        normalized to mean 0, variance 1. The reference above provides details
        on the models supported here. For the exponential decay kernel, the
        base kernel over x is Matern 5/2 ARD.
        Values are 'matern52' (Matern 5/2 ARD kernel over [x, r]),
        'matern52-res-warp' (Matern 5/2 ARD kernel over [x, r], with additional
        warping on r),
        'exp-decay-sum' (exponential decay kernel, with delta=0. This is the
        additive kernel from Freeze-Thaw Bayesian Optimization),
        'exp-decay-delta1' (exponential decay kernel, with delta=1),
        'exp-decay-combined' (exponential decay kernel, with delta in [0, 1]
        a hyperparameter).
    resource_acq : str
        Only relevant for `model in {'gp_multitask', 'gp_independent'}`
        Determines how the EI acquisition function is used (see above).
        Values: 'bohb', 'first'
    opt_skip_num_max_resource : bool
        Parameter for hyperparameter fitting, skip predicate. If True, and
        number of observations above `opt_skip_init_length`, fitting is done
        only when there is a new datapoint at r = max_t, and skipped otherwise.
    issm_gamma_one : bool
        Only relevant for `model == 'gp_issm'`.
        If True, the gamma parameter of the ISSM is fixed to 1, otherwise it
        is optimized over.
    expdecay_normalize_inputs : bool
        Only relevant for `model == 'gp_expdecay'`.
        If True, resource values r are normalized to [0, 1] as input to the
        exponential decay surrogate model.

    See Also
    --------
    GPFIFOSearcher
    """

    def __init__(self, config_space, **kwargs):
        super().__init__(config_space, **kwargs)

    def _create_kwargs_int(self, kwargs):
        _kwargs = check_and_merge_defaults(
            kwargs, *gp_multifidelity_searcher_defaults(), dict_name="search_options"
        )
        kwargs_int = gp_multifidelity_searcher_factory(**_kwargs)
        self._copy_kwargs_to_kwargs_int(kwargs_int, kwargs)
        return kwargs_int

    def _call_create_internal(self, kwargs_int):
        """
        Part of constructor which can be different in subclasses
        """
        k = "resource_for_acquisition"
        self.resource_for_acquisition = kwargs_int.get(k)
        if self.resource_for_acquisition is not None:
            kwargs_int.pop(k)
            assert isinstance(self.resource_for_acquisition, ResourceForAcquisitionMap)
        self.config_space_ext = kwargs_int.pop("config_space_ext")
        self._create_internal(**kwargs_int)

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler

        super().configure_scheduler(scheduler)
        assert isinstance(
            scheduler, HyperbandScheduler
        ), "This searcher requires HyperbandScheduler scheduler"
        self._resource_attr = scheduler._resource_attr
        model_factory = self.state_transformer.model_factory
        if isinstance(model_factory, GaussProcAdditiveModelFactory):
            assert scheduler.searcher_data == "all", (
                "For an additive Gaussian learning curve model (model="
                + "'gp_issm' or model='gp_expdecay' in search_options), you "
                + "need to set searcher_data='all' when creating the "
                + "HyperbandScheduler"
            )

    def _hp_ranges_in_state(self):
        return self.config_space_ext.hp_ranges_ext

    def _config_ext_update(self, config, result):
        resource = int(result[self._resource_attr])
        return self.config_space_ext.get(config, resource)

    def _metric_val_update(self, crit_val: float, result: Dict) -> MetricValues:
        resource = result[self._resource_attr]
        return {str(resource): crit_val}

    def _trial_id_string(self, trial_id: str, result: Dict):
        """
        For multi-fidelity, we also want to output the resource level
        """
        return f"{trial_id}:{result[self._resource_attr]}"

    def register_pending(
        self, trial_id: str, config: Optional[Dict] = None, milestone=None
    ):
        """
        Registers trial as pending for resource level `milestone`. This means
        the corresponding evaluation task is running and should reach that
        level later, when update is called for it.

        """
        assert (
            milestone is not None
        ), "This searcher works with a multi-fidelity scheduler only"
        # It is OK for the candidate already to be registered as pending, in
        # which case we do nothing
        state = self.state_transformer.state
        if not state.is_pending(trial_id, resource=milestone):
            assert not state.is_labeled(trial_id, resource=milestone), (
                f"Trial trial_id = {trial_id} already has observation at "
                + f"resource = {milestone}, so cannot be pending there"
            )
            self.state_transformer.append_trial(
                trial_id, config=config, resource=milestone
            )

    def _fix_resource_attribute(self, **kwargs):
        """
        Determines target resource level r at which the current call of
        `get_config` operates. This is done based on
        `resource_for_acquisition`. This resource level is then set in
        `config_space_ext.hp_ranges_ext.value_for_last_pos`. This does the
        job for GP surrogate models. But if in subclasses, other surrogate
        models are involved, they need to get informed separately (see
        :class:`CostAwareGPMultiFidelitySearcher` for an example).

        :param kwargs:
        :return:
        """
        if self.resource_for_acquisition is not None:
            # Only have to do this for 'gp_multitask' or 'gp_independent' model
            state = self.state_transformer.state
            # BO should only search over configs at resource level
            # target_resource
            if state.trials_evaluations:
                target_resource = self.resource_for_acquisition(state, **kwargs)
            else:
                # Any valid value works here:
                target_resource = self.config_space_ext.resource_attr_range[0]
            self.config_space_ext.hp_ranges_ext.value_for_last_pos = target_resource
            if self.debug_log is not None:
                self.debug_log.append_extra(
                    f"Score values computed at target_resource = {target_resource}"
                )

    def _postprocess_config(self, config: dict) -> dict:
        # If `config` is normal (not extended), nothing is removed
        return self.config_space_ext.remove_resource(config)

    def evaluation_failed(self, trial_id: str):
        # Remove all pending evaluations for trial
        self.cleanup_pending(trial_id)
        # Mark config as failed (which means it will not be suggested again)
        self.state_transformer.mark_trial_failed(trial_id)

    def cleanup_pending(self, trial_id: str):
        """
        Removes all pending evaluations for a trial.
        This should be called after an evaluation terminates. For various
        reasons (e.g., termination due to convergence), pending candidates
        for this evaluation may still be present.
        It is also called for a failed evaluation.

        """

        def filter_pred(x: PendingEvaluation) -> bool:
            return x.trial_id == trial_id

        self.state_transformer.filter_pending_evaluations(filter_pred)

    def remove_case(self, trial_id: str, **kwargs):
        resource = kwargs[self._resource_attr]
        self.state_transformer.remove_observed_case(trial_id, key=str(resource))

    def clone_from_state(self, state):
        # Create clone with mutable state taken from 'state'
        init_state = decode_state(state["state"], self._hp_ranges_in_state())
        skip_optimization = state["skip_optimization"]
        model_factory = self.state_transformer.model_factory
        # Call internal constructor
        new_searcher = GPMultiFidelitySearcher(
            **self._new_searcher_kwargs_for_clone(),
            model_factory=model_factory,
            init_state=init_state,
            skip_optimization=skip_optimization,
            config_space_ext=self.config_space_ext,
            resource_for_acquisition=self.resource_for_acquisition,
        )
        new_searcher._restore_from_state(state)
        # Invalidate self (must not be used afterwards)
        self.state_transformer = None
        return new_searcher

File Path: syne_tune/optimizer/schedulers/searchers/gp_searcher_factory.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Set, Optional
import logging

from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import (
    map_reward_const_minus_x,
    MapReward,
    DEFAULT_INITIAL_SCORING,
    SUPPORTED_INITIAL_SCORING,
    resource_for_acquisition_factory,
    SUPPORTED_RESOURCE_FOR_ACQUISITION,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.kernel_factory import (
    resource_kernel_factory,
    SUPPORTED_RESOURCE_MODELS,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.config_ext import (
    ExtendedConfiguration,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    OptimizationConfig,
    DEFAULT_OPTIMIZATION_CONFIG,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_regression import (
    GaussianProcessRegression,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    Matern52,
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    MeanFunction,
    ScalarMeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.freeze_thaw import (
    ExponentialDecayBaseKernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.model_params import (
    IndependentISSModelParameters,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.learncurve.gpiss_model import (
    GaussianProcessLearningCurveModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.independent.gpind_model import (
    IndependentGPPerResourceModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_skipopt import (
    SkipNoMaxResourcePredicate,
    SkipPeriodicallyPredicate,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    GaussProcEmpiricalBayesModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gpiss_model import (
    GaussProcAdditiveModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.cost_fifo_model import (
    CostSurrogateModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc_impl import (
    EIAcquisitionFunction,
    CEIAcquisitionFunction,
    EIpuAcquisitionFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.defaults import (
    DEFAULT_NUM_INITIAL_CANDIDATES,
    DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
    INTERNAL_CONSTRAINT_NAME,
    INTERNAL_COST_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)
from syne_tune.optimizer.schedulers.utils.simple_profiler import SimpleProfiler
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    Integer,
    Categorical,
    Boolean,
    Float,
)
from syne_tune.optimizer.schedulers.searchers.utils.warmstarting import (
    create_hp_ranges_for_warmstarting,
    create_filter_observed_data_for_warmstarting,
    create_base_gp_kernel_for_warmstarting,
)
from syne_tune.optimizer.schedulers.searchers.searcher import extract_random_seed

__all__ = [
    "gp_fifo_searcher_factory",
    "gp_multifidelity_searcher_factory",
    "constrained_gp_fifo_searcher_factory",
    "cost_aware_coarse_gp_fifo_searcher_factory",
    "cost_aware_fine_gp_fifo_searcher_factory",
    "cost_aware_gp_multifidelity_searcher_factory",
    "gp_fifo_searcher_defaults",
    "gp_multifidelity_searcher_defaults",
    "constrained_gp_fifo_searcher_defaults",
    "cost_aware_gp_fifo_searcher_defaults",
    "cost_aware_gp_multifidelity_searcher_defaults",
]

logger = logging.getLogger(__name__)


def _create_base_gp_kernel(hp_ranges: HyperparameterRanges, **kwargs) -> KernelFunction:
    """
    The default base kernel is :class:`Matern52` with ARD parameters.
    But in the transfer learning case, the base kernel is a product of
    two `Matern52` kernels, the first non-ARD over the categorical
    parameter determining the task, the second ARD over the remaining
    parameters.

    """
    if kwargs.get("transfer_learning_task_attr") is not None:
        # Transfer learning: Specific base kernel
        kernel = create_base_gp_kernel_for_warmstarting(hp_ranges, **kwargs)
    else:
        has_covariance_scale = kwargs.get("has_covariance_scale", True)
        kernel = Matern52(
            dimension=hp_ranges.ndarray_size,
            ARD=True,
            has_covariance_scale=has_covariance_scale,
        )
    return kernel


def _create_gp_common(hp_ranges: HyperparameterRanges, **kwargs):
    opt_warmstart = kwargs.get("opt_warmstart", False)
    kernel = _create_base_gp_kernel(hp_ranges, **kwargs)
    mean = ScalarMeanFunction()
    optimization_config = OptimizationConfig(
        lbfgs_tol=DEFAULT_OPTIMIZATION_CONFIG.lbfgs_tol,
        lbfgs_maxiter=kwargs["opt_maxiter"],
        verbose=kwargs["opt_verbose"],
        n_starts=kwargs["opt_nstarts"],
    )
    if kwargs.get("profiler", False):
        profiler = SimpleProfiler()
    else:
        profiler = None
    if kwargs.get("debug_log", False):
        debug_log = DebugLogPrinter()
    else:
        debug_log = None
    filter_observed_data = create_filter_observed_data_for_warmstarting(**kwargs)
    return {
        "opt_warmstart": opt_warmstart,
        "kernel": kernel,
        "mean": mean,
        "optimization_config": optimization_config,
        "profiler": profiler,
        "debug_log": debug_log,
        "filter_observed_data": filter_observed_data,
    }


def _create_gp_standard_model(
    hp_ranges: HyperparameterRanges,
    active_metric: Optional[str],
    random_seed: int,
    is_hyperband: bool,
    **kwargs,
):
    result = _create_gp_common(hp_ranges, **kwargs)
    kernel = result["kernel"]
    mean = result["mean"]
    if is_hyperband:
        # The `cross-validation` kernel needs an additional argument
        kernel_kwargs = {"num_folds": kwargs["max_epochs"]}
        kernel, mean = resource_kernel_factory(
            kwargs["gp_resource_kernel"], kernel_x=kernel, mean_x=mean, **kernel_kwargs
        )
    gpmodel = GaussianProcessRegression(
        kernel=kernel,
        mean=mean,
        optimization_config=result["optimization_config"],
        random_seed=random_seed,
        fit_reset_params=not result["opt_warmstart"],
    )
    filter_observed_data = result["filter_observed_data"]
    model_factory = GaussProcEmpiricalBayesModelFactory(
        active_metric=active_metric,
        gpmodel=gpmodel,
        num_fantasy_samples=kwargs["num_fantasy_samples"],
        normalize_targets=kwargs.get("normalize_targets", True),
        profiler=result["profiler"],
        debug_log=result["debug_log"],
        filter_observed_data=filter_observed_data,
        no_fantasizing=kwargs.get("no_fantasizing", False),
    )
    return {
        "model_factory": model_factory,
        "filter_observed_data": filter_observed_data,
    }


def _create_gp_independent_model(
    hp_ranges: HyperparameterRanges,
    active_metric: Optional[str],
    random_seed: int,
    **kwargs,
):
    def mean_factory(resource: int) -> MeanFunction:
        return ScalarMeanFunction()

    result = _create_gp_common(hp_ranges, has_covariance_scale=False, **kwargs)
    kernel = result["kernel"]
    resource_attr_range = (1, kwargs["max_epochs"])
    common_kwargs = dict(
        kernel=kernel,
        mean_factory=mean_factory,
        resource_attr_range=resource_attr_range,
        optimization_config=result["optimization_config"],
        random_seed=random_seed,
        fit_reset_params=not result["opt_warmstart"],
        separate_noise_variances=kwargs["separate_noise_variances"],
    )
    gpmodel = IndependentGPPerResourceModel(**common_kwargs)
    filter_observed_data = result["filter_observed_data"]
    model_factory = GaussProcEmpiricalBayesModelFactory(
        active_metric=active_metric,
        gpmodel=gpmodel,
        num_fantasy_samples=kwargs["num_fantasy_samples"],
        normalize_targets=kwargs.get("normalize_targets", True),
        profiler=result["profiler"],
        debug_log=result["debug_log"],
        filter_observed_data=filter_observed_data,
        no_fantasizing=kwargs.get("no_fantasizing", False),
    )
    return {
        "model_factory": model_factory,
        "filter_observed_data": filter_observed_data,
    }


def _create_gp_additive_model(
    model: str,
    hp_ranges: HyperparameterRanges,
    active_metric: Optional[str],
    random_seed: int,
    config_space_ext,
    **kwargs,
):
    result = _create_gp_common(hp_ranges, **kwargs)
    if model == "gp_issm":
        res_model = IndependentISSModelParameters(
            gamma_is_one=kwargs.get("issm_gamma_one", False)
        )
    else:
        assert model == "gp_expdecay", model
        res_model = ExponentialDecayBaseKernelFunction(
            r_max=kwargs["max_epochs"],
            r_min=1,
            normalize_inputs=kwargs.get("expdecay_normalize_inputs", False),
        )
    gpmodel = GaussianProcessLearningCurveModel(
        kernel=result["kernel"],
        res_model=res_model,
        mean=result["mean"],
        optimization_config=result["optimization_config"],
        random_seed=random_seed,
        fit_reset_params=not result["opt_warmstart"],
    )
    filter_observed_data = result["filter_observed_data"]
    no_fantasizing = kwargs.get("no_fantasizing", False)
    num_fantasy_samples = 0 if no_fantasizing else kwargs["num_fantasy_samples"]
    model_factory = GaussProcAdditiveModelFactory(
        gpmodel=gpmodel,
        num_fantasy_samples=num_fantasy_samples,
        active_metric=active_metric,
        config_space_ext=config_space_ext,
        profiler=result["profiler"],
        debug_log=result["debug_log"],
        filter_observed_data=filter_observed_data,
        normalize_targets=kwargs.get("normalize_targets", True),
    )
    return {
        "model_factory": model_factory,
        "filter_observed_data": filter_observed_data,
    }


def _create_common_objects(model=None, **kwargs):
    scheduler = kwargs["scheduler"]
    is_hyperband = scheduler.startswith("hyperband")
    if model is None:
        model = "gp_multitask"
    assert (
        model == "gp_multitask" or is_hyperband
    ), f"model = {model} only together with hyperband_* scheduler"
    hp_ranges = create_hp_ranges_for_warmstarting(**kwargs)
    random_seed, _kwargs = extract_random_seed(kwargs)
    # Skip optimization predicate for GP surrogate model
    if kwargs.get("opt_skip_num_max_resource", False) and is_hyperband:
        skip_optimization = SkipNoMaxResourcePredicate(
            init_length=kwargs["opt_skip_init_length"],
            max_resource=kwargs["max_epochs"],
        )
    elif kwargs.get("opt_skip_period", 1) > 1:
        skip_optimization = SkipPeriodicallyPredicate(
            init_length=kwargs["opt_skip_init_length"], period=kwargs["opt_skip_period"]
        )
    else:
        skip_optimization = None
    # Conversion from reward to metric (strictly decreasing) and back.
    # This is done only if the scheduler mode is 'max'.
    scheduler_mode = kwargs.get("scheduler_mode", "min")
    if scheduler_mode == "max":
        _map_reward = kwargs.get("map_reward", "1_minus_x")
        if isinstance(_map_reward, str):
            _map_reward_name = _map_reward
            assert _map_reward_name.endswith("minus_x"), (
                f"map_reward = {_map_reward_name} is not supported (use "
                + "'minus_x' or '*_minus_x')"
            )
            if _map_reward_name == "minus_x":
                const = 0.0
            else:
                # Allow strings '*_minus_x', parse const for *
                # Example: '1_minus_x' => const = 1
                offset = len(_map_reward_name) - len("_minus_x")
                const = float(_map_reward_name[:offset])
            _map_reward: Optional[MapReward] = map_reward_const_minus_x(const=const)
        else:
            assert isinstance(
                _map_reward, MapReward
            ), "map_reward must either be string or of MapReward type"
    else:
        assert (
            scheduler_mode == "min"
        ), f"scheduler_mode = {scheduler_mode}, must be in ('max', 'min')"
        _map_reward = kwargs.get("map_reward")
        if _map_reward is not None:
            logger.warning(
                f"Since scheduler_mode == 'min', map_reward = {_map_reward} is ignored"
            )
            _map_reward = None
    result = {
        "hp_ranges": hp_ranges,
        "map_reward": _map_reward,
        "skip_optimization": skip_optimization,
    }
    if is_hyperband:
        epoch_range = (1, kwargs["max_epochs"])
        result["config_space_ext"] = ExtendedConfiguration(
            hp_ranges,
            resource_attr_key=kwargs["resource_attr"],
            resource_attr_range=epoch_range,
        )

    # Create model factory
    if model == "gp_multitask":
        result.update(
            _create_gp_standard_model(
                hp_ranges=hp_ranges,
                active_metric=INTERNAL_METRIC_NAME,
                random_seed=random_seed,
                is_hyperband=is_hyperband,
                **_kwargs,
            )
        )
    elif model == "gp_independent":
        result.update(
            _create_gp_independent_model(
                hp_ranges=hp_ranges,
                active_metric=INTERNAL_METRIC_NAME,
                random_seed=random_seed,
                **_kwargs,
            )
        )
    else:
        result.update(
            _create_gp_additive_model(
                model=model,
                hp_ranges=hp_ranges,
                active_metric=INTERNAL_METRIC_NAME,
                random_seed=random_seed,
                config_space_ext=result["config_space_ext"],
                **_kwargs,
            )
        )
    result["num_initial_candidates"] = kwargs["num_init_candidates"]
    result["num_initial_random_choices"] = kwargs["num_init_random"]
    for k in ("initial_scoring", "cost_attr", "skip_local_optimization"):
        result[k] = kwargs[k]

    return result


def gp_fifo_searcher_factory(**kwargs) -> dict:
    """
    Returns kwargs for `GPFIFOSearcher._create_internal`, based on kwargs
    equal to search_options passed to and extended by scheduler (see
    :class:`FIFOScheduler`).

    Extensions of kwargs by the scheduler:
    - scheduler: Name of scheduler ('fifo', 'hyperband_*')
    - config_space: Configuration space
    Only Hyperband schedulers:
    - resource_attr: Name of resource (or time) attribute
    - max_epochs: Maximum resource value

    :param kwargs: search_options coming from scheduler
    :return: kwargs for GPFIFOSearcher._create_internal

    """
    assert (
        kwargs["scheduler"] == "fifo"
    ), "This factory needs scheduler = 'fifo' (instead of '{}')".format(
        kwargs["scheduler"]
    )
    # Common objects
    result = _create_common_objects(**kwargs)

    return dict(**result, acquisition_class=EIAcquisitionFunction)


def gp_multifidelity_searcher_factory(**kwargs) -> dict:
    """
    Returns kwargs for `GPMultiFidelitySearcher._create_internal`, based on
    kwargs equal to search_options passed to and extended by scheduler (see
    :class:`HyperbandScheduler`).

    :param kwargs: search_options coming from scheduler
    :return: kwargs for GPMultiFidelitySearcher._create_internal

    """
    supp_schedulers = {
        "hyperband_stopping",
        "hyperband_promotion",
        "hyperband_synchronous",
        "hyperband_pasha",
    }
    assert (
        kwargs["scheduler"] in supp_schedulers
    ), "This factory needs scheduler in {} (instead of '{}')".format(
        supp_schedulers, kwargs["scheduler"]
    )
    if kwargs.get("model") is None:
        kwargs["model"] = "gp_multitask"
    # Common objects
    result = _create_common_objects(**kwargs)

    kwargs_int = dict(
        result,
        resource_attr=kwargs["resource_attr"],
        acquisition_class=EIAcquisitionFunction,
    )
    if kwargs["model"] in {"gp_multitask", "gp_independent"}:
        kwargs_int["resource_for_acquisition"] = resource_for_acquisition_factory(
            kwargs, result["hp_ranges"]
        )
    return kwargs_int


def constrained_gp_fifo_searcher_factory(**kwargs) -> dict:
    """
    Returns kwargs for `ConstrainedGPFIFOSearcher._create_internal`, based on kwargs
    equal to search_options passed to and extended by scheduler (see
    :class:`FIFOScheduler`).

    :param kwargs: search_options coming from scheduler
    :return: kwargs for ConstrainedGPFIFOSearcher._create_internal

    """
    assert (
        kwargs["scheduler"] == "fifo"
    ), "This factory needs scheduler = 'fifo' (instead of '{}')".format(
        kwargs["scheduler"]
    )
    # Common objects
    result = _create_common_objects(**kwargs)
    model_factory = result.pop("model_factory")
    skip_optimization = result.pop("skip_optimization")
    # We need two model factories: one for active metric (model_factory),
    # the other for constraint metric (model_factory_constraint)
    random_seed, _kwargs = extract_random_seed(kwargs)
    model_factory_constraint = _create_gp_standard_model(
        hp_ranges=result["hp_ranges"],
        active_metric=INTERNAL_CONSTRAINT_NAME,
        random_seed=random_seed,
        is_hyperband=False,
        **_kwargs,
    )["model_factory"]
    # Sharing debug_log attribute across models
    model_factory_constraint._debug_log = model_factory._debug_log
    # The same skip_optimization strategy applies to both models
    skip_optimization_constraint = skip_optimization

    output_model_factory = {
        INTERNAL_METRIC_NAME: model_factory,
        INTERNAL_CONSTRAINT_NAME: model_factory_constraint,
    }
    output_skip_optimization = {
        INTERNAL_METRIC_NAME: skip_optimization,
        INTERNAL_CONSTRAINT_NAME: skip_optimization_constraint,
    }

    return dict(
        result,
        output_model_factory=output_model_factory,
        output_skip_optimization=output_skip_optimization,
        acquisition_class=CEIAcquisitionFunction,
    )


def cost_aware_coarse_gp_fifo_searcher_factory(**kwargs) -> dict:
    """
    Returns kwargs for `CostAwareGPFIFOSearcher._create_internal`, based on
    kwargs equal to search_options passed to and extended by scheduler (see
    :class:`FIFOScheduler`).
    This is for the coarse-grained variant, where costs c(x) are obtained
    together with metric values and are given a GP surrogate model.

    :param kwargs: search_options coming from scheduler
    :return: kwargs for CostAwareGPFIFOSearcher._create_internal

    """
    assert (
        kwargs["scheduler"] == "fifo"
    ), "This factory needs scheduler = 'fifo' (instead of '{}')".format(
        kwargs["scheduler"]
    )
    # Common objects
    result = _create_common_objects(**kwargs)
    model_factory = result.pop("model_factory")
    skip_optimization = result.pop("skip_optimization")
    # We need two model factories: one for active metric (model_factory),
    # the other for cost metric (model_factory_cost)
    random_seed, _kwargs = extract_random_seed(kwargs)
    model_factory_cost = _create_gp_standard_model(
        hp_ranges=result["hp_ranges"],
        active_metric=INTERNAL_COST_NAME,
        random_seed=random_seed,
        is_hyperband=False,
        **_kwargs,
    )["model_factory"]
    # Sharing debug_log attribute across models
    model_factory_cost._debug_log = model_factory._debug_log
    exponent_cost = kwargs.get("exponent_cost", 1.0)
    acquisition_class = (EIpuAcquisitionFunction, dict(exponent_cost=exponent_cost))
    # The same skip_optimization strategy applies to both models
    skip_optimization_cost = skip_optimization

    output_model_factory = {
        INTERNAL_METRIC_NAME: model_factory,
        INTERNAL_COST_NAME: model_factory_cost,
    }
    output_skip_optimization = {
        INTERNAL_METRIC_NAME: skip_optimization,
        INTERNAL_COST_NAME: skip_optimization_cost,
    }

    return dict(
        result,
        output_model_factory=output_model_factory,
        output_skip_optimization=output_skip_optimization,
        acquisition_class=acquisition_class,
    )


def cost_aware_fine_gp_fifo_searcher_factory(**kwargs) -> dict:
    """
    Returns kwargs for `CostAwareGPFIFOSearcher._create_internal`, based on
    kwargs equal to search_options passed to and extended by scheduler (see
    :class:`FIFOScheduler`).
    This is for the fine-grained variant, where costs c(x, r) are obtained
    with each report and are represented by a :class:`CostModel`
    surrogate model.

    :param kwargs: search_options coming from scheduler
    :return: kwargs for CostAwareGPFIFOSearcher._create_internal

    """
    assert kwargs["scheduler"] in [
        "fifo"
    ], "This factory needs scheduler = 'fifo' (instead of '{}')".format(
        kwargs["scheduler"]
    )
    cost_model = kwargs.get("cost_model")
    assert cost_model is not None, (
        "If search_options['resource_attr'] is given, a CostModel has "
        + "to be specified in search_options['cost_model']"
    )
    fixed_resource = kwargs.get("max_epochs")
    assert fixed_resource is not None, (
        "If search_options['resource_attr'] is given, the maximum "
        + "resource level has to be specified in "
        + "search_options['max_epochs'], or (simpler) as max_t when "
        + "creating FIFOScheduler"
    )
    # Common objects
    result = _create_common_objects(**kwargs)
    model_factory = result.pop("model_factory")
    skip_optimization = result.pop("skip_optimization")
    # We need two model factories: one for active metric (model_factory),
    # the other for cost metric (model_factory_cost)
    model_factory_cost = CostSurrogateModelFactory(
        model=kwargs["cost_model"], fixed_resource=fixed_resource, num_samples=1
    )
    exponent_cost = kwargs.get("exponent_cost", 1.0)
    acquisition_class = (EIpuAcquisitionFunction, dict(exponent_cost=exponent_cost))
    # The same skip_optimization strategy applies to both models
    skip_optimization_cost = skip_optimization

    output_model_factory = {
        INTERNAL_METRIC_NAME: model_factory,
        INTERNAL_COST_NAME: model_factory_cost,
    }
    output_skip_optimization = {
        INTERNAL_METRIC_NAME: skip_optimization,
        INTERNAL_COST_NAME: skip_optimization_cost,
    }

    return dict(
        result,
        output_model_factory=output_model_factory,
        output_skip_optimization=output_skip_optimization,
        acquisition_class=acquisition_class,
        resource_attr=kwargs["resource_attr"],
    )


def cost_aware_gp_multifidelity_searcher_factory(**kwargs) -> dict:
    """
    Returns kwargs for `CostAwareGPMultiFidelitySearcher._create_internal`,
    based on kwargs equal to search_options passed to and extended by
    scheduler (see :class:`HyperbandScheduler`).

    :param kwargs: search_options coming from scheduler
    :return: kwargs for CostAwareGPMultiFidelitySearcher._create_internal

    """
    supp_schedulers = {
        "hyperband_stopping",
        "hyperband_promotion",
        "hyperband_synchronous",
        "hyperband_pasha",
    }
    assert (
        kwargs["scheduler"] in supp_schedulers
    ), "This factory needs scheduler in {} (instead of '{}')".format(
        supp_schedulers, kwargs["scheduler"]
    )
    cost_model = kwargs.get("cost_model")
    assert cost_model is not None, (
        "If search_options['resource_attr'] is given, a CostModel has "
        + "to be specified in search_options['cost_model']"
    )
    # Common objects
    result = _create_common_objects(**kwargs)
    model_factory = result.pop("model_factory")
    skip_optimization = result.pop("skip_optimization")
    # We need two model factories: one for active metric (model_factory),
    # the other for cost metric (model_factory_cost)
    model_factory_cost = CostSurrogateModelFactory(
        model=kwargs["cost_model"], fixed_resource=kwargs["max_epochs"], num_samples=1
    )
    exponent_cost = kwargs.get("exponent_cost", 1.0)
    acquisition_class = (EIpuAcquisitionFunction, dict(exponent_cost=exponent_cost))
    # The same skip_optimization strategy applies to both models
    skip_optimization_cost = skip_optimization

    output_model_factory = {
        INTERNAL_METRIC_NAME: model_factory,
        INTERNAL_COST_NAME: model_factory_cost,
    }
    output_skip_optimization = {
        INTERNAL_METRIC_NAME: skip_optimization,
        INTERNAL_COST_NAME: skip_optimization_cost,
    }

    resource_for_acquisition = resource_for_acquisition_factory(
        kwargs, result["hp_ranges"]
    )
    return dict(
        result,
        resource_attr=kwargs["resource_attr"],
        output_model_factory=output_model_factory,
        output_skip_optimization=output_skip_optimization,
        resource_for_acquisition=resource_for_acquisition,
        acquisition_class=acquisition_class,
    )


def _common_defaults(
    is_hyperband: bool, is_multi_output: bool
) -> (Set[str], dict, dict):
    mandatory = set()

    default_options = {
        "opt_skip_init_length": 150,
        "opt_skip_period": 1,
        "profiler": False,
        "opt_maxiter": 50,
        "opt_nstarts": 2,
        "opt_warmstart": False,
        "opt_verbose": False,
        "opt_debug_writer": False,
        "num_fantasy_samples": 20,
        "scheduler": "fifo",
        "num_init_random": DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,
        "num_init_candidates": DEFAULT_NUM_INITIAL_CANDIDATES,
        "initial_scoring": DEFAULT_INITIAL_SCORING,
        "skip_local_optimization": False,
        "debug_log": True,
        "cost_attr": "elapsed_time",
        "normalize_targets": True,
        "no_fantasizing": False,
    }
    if is_hyperband:
        default_options["model"] = "gp_multitask"
        default_options["opt_skip_num_max_resource"] = False
        default_options["gp_resource_kernel"] = "exp-decay-sum"
        default_options["resource_acq"] = "bohb"
        default_options["resource_acq_bohb_threshold"] = 3
        default_options["num_init_random"] = 6
        default_options["issm_gamma_one"] = False
        default_options["expdecay_normalize_inputs"] = False
        default_options["separate_noise_variances"] = False
    if is_multi_output:
        default_options["initial_scoring"] = "acq_func"
        default_options["exponent_cost"] = 1.0

    constraints = {
        "random_seed": Integer(0, 2**32 - 1),
        "opt_skip_init_length": Integer(0, None),
        "opt_skip_period": Integer(1, None),
        "profiler": Boolean(),
        "opt_maxiter": Integer(1, None),
        "opt_nstarts": Integer(1, None),
        "opt_warmstart": Boolean(),
        "opt_verbose": Boolean(),
        "opt_debug_writer": Boolean(),
        "num_fantasy_samples": Integer(1, None),
        "num_init_random": Integer(0, None),
        "num_init_candidates": Integer(5, None),
        "initial_scoring": Categorical(choices=tuple(SUPPORTED_INITIAL_SCORING)),
        "skip_local_optimization": Boolean(),
        "debug_log": Boolean(),
        "normalize_targets": Boolean(),
    }

    if is_hyperband:
        model_choices = ("gp_multitask", "gp_independent", "gp_issm", "gp_expdecay")
        constraints["model"] = Categorical(choices=model_choices)
        constraints["opt_skip_num_max_resource"] = Boolean()
        constraints["gp_resource_kernel"] = Categorical(
            choices=SUPPORTED_RESOURCE_MODELS
        )
        constraints["resource_acq"] = Categorical(
            choices=tuple(SUPPORTED_RESOURCE_FOR_ACQUISITION)
        )
        constraints["issm_gamma_one"] = Boolean()
        constraints["expdecay_normalize_inputs"] = Boolean()
        constraints["separate_noise_variances"] = Boolean()
    if is_multi_output:
        constraints["initial_scoring"] = Categorical(choices=tuple({"acq_func"}))
        constraints["exponent_cost"] = Float(0.0, 1.0)

    return mandatory, default_options, constraints


def gp_fifo_searcher_defaults() -> (Set[str], dict, dict):
    """
    Returns mandatory, default_options, config_space for
    check_and_merge_defaults to be applied to search_options for
    :class:`GPFIFOSearcher`.

    :return: (mandatory, default_options, config_space)

    """
    return _common_defaults(is_hyperband=False, is_multi_output=False)


def constrained_gp_fifo_searcher_defaults() -> (Set[str], dict, dict):
    """
    Returns mandatory, default_options, config_space for
    check_and_merge_defaults to be applied to search_options for
    :class:`ConstrainedGPFIFOSearcher`.

    :return: (mandatory, default_options, config_space)

    """
    return _common_defaults(is_hyperband=False, is_multi_output=True)


def cost_aware_gp_fifo_searcher_defaults() -> (Set[str], dict, dict):
    """
    Returns mandatory, default_options, config_space for
    check_and_merge_defaults to be applied to search_options for
    :class:`CostAwareGPFIFOSearcher`.

    :return: (mandatory, default_options, config_space)

    """
    return _common_defaults(is_hyperband=False, is_multi_output=True)


def gp_multifidelity_searcher_defaults() -> (Set[str], dict, dict):
    """
    Returns mandatory, default_options, config_space for
    check_and_merge_defaults to be applied to search_options for
    :class:`GPMultiFidelitySearcher`.

    :return: (mandatory, default_options, config_space)

    """
    return _common_defaults(is_hyperband=True, is_multi_output=False)


def cost_aware_gp_multifidelity_searcher_defaults() -> (Set[str], dict, dict):
    """
    Returns mandatory, default_options, config_space for
    check_and_merge_defaults to be applied to search_options for
    :class:`CostAwareGPMultiFidelitySearcher`.

    :return: (mandatory, default_options, config_space)

    """
    return _common_defaults(is_hyperband=True, is_multi_output=True)

File Path: syne_tune/optimizer/schedulers/searchers/gp_searcher_utils.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from collections import Counter
from typing import Callable
from dataclasses import dataclass

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    TrialEvaluations,
    PendingEvaluation,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)


@dataclass
class MapReward:
    forward: Callable[[float], float]
    reverse: Callable[[float], float]

    def __call__(self, x: float) -> float:
        return self.forward(x)


def map_reward_const_minus_x(const=1.0) -> MapReward:
    """
    Factory for map_reward argument in GPMultiFidelitySearcher.
    """

    def const_minus_x(x):
        return const - x

    return MapReward(forward=const_minus_x, reverse=const_minus_x)


SUPPORTED_INITIAL_SCORING = {"thompson_indep", "acq_func"}


DEFAULT_INITIAL_SCORING = "thompson_indep"


def encode_state(state: TuningJobState) -> dict:
    trials_evaluations = [
        {"trial_id": x.trial_id, "metrics": x.metrics} for x in state.trials_evaluations
    ]
    pending_evaluations = [
        {"trial_id": x.trial_id, "resource": x.resource}
        if x.resource is not None
        else {"trial_id": x.trial_id}
        for x in state.pending_evaluations
    ]
    enc_state = {
        "config_for_trial": state.config_for_trial,
        "trials_evaluations": trials_evaluations,
        "failed_trials": state.failed_trials,
        "pending_evaluations": pending_evaluations,
    }
    return enc_state


def decode_state(enc_state: dict, hp_ranges: HyperparameterRanges) -> TuningJobState:
    trials_evaluations = [
        TrialEvaluations(**x) for x in enc_state["trials_evaluations"]
    ]
    pending_evaluations = [
        PendingEvaluation(**x) for x in enc_state["pending_evaluations"]
    ]
    return TuningJobState(
        hp_ranges=hp_ranges,
        config_for_trial=enc_state["config_for_trial"],
        trials_evaluations=trials_evaluations,
        failed_trials=enc_state["failed_trials"],
        pending_evaluations=pending_evaluations,
    )


def _get_trial_id(
    hp_ranges: HyperparameterRanges,
    config: dict,
    config_for_trial: dict,
    trial_for_config: dict,
) -> str:
    match_str = hp_ranges.config_to_match_string(config, skip_last=True)
    trial_id = trial_for_config.get(match_str)
    if trial_id is None:
        trial_id = str(len(trial_for_config))
        trial_for_config[match_str] = trial_id
        config_for_trial[trial_id] = config
    return trial_id


def decode_state_from_old_encoding(
    enc_state: dict, hp_ranges: HyperparameterRanges
) -> TuningJobState:
    """
    Decodes `TuningJobState` from encoding done for the old definition of
    `TuningJobState`. Code maintained for backwards compatibility.

    Note: Since the old `TuningJobState` did not contain `trial_id`, we need
    to make them up here. We assign these IDs in the order
    `candidate_evaluations`, `failed_candidates`, `pending_candidates`,
    matching for duplicates.

    :param enc_state:
    :param hp_ranges:
    :return:
    """
    config_for_trial = dict()
    trial_for_config = dict()
    trials_evaluations = []
    for cand_eval in enc_state["candidate_evaluations"]:
        config = cand_eval["candidate"]
        trial_id = _get_trial_id(hp_ranges, config, config_for_trial, trial_for_config)
        trials_evaluations.append(TrialEvaluations(trial_id, cand_eval["metrics"]))
    failed_trials = []
    for failed_cand in enc_state["failed_candidates"]:
        failed_trials.append(
            _get_trial_id(hp_ranges, failed_cand, config_for_trial, trial_for_config)
        )
    pending_evaluations = []
    resource_attr_name = hp_ranges.name_last_pos
    for pending_cand in enc_state["pending_candidates"]:
        resource = None
        if resource_attr_name is not None and resource_attr_name in pending_cand:
            # Extended config (multi-fidelity)
            resource = int(pending_cand[resource_attr_name])
            pending_cand = pending_cand.copy()
            del pending_cand[resource_attr_name]
        trial_id = _get_trial_id(
            hp_ranges, pending_cand, config_for_trial, trial_for_config
        )
        pending_evaluations.append(PendingEvaluation(trial_id, resource))
    return TuningJobState(
        hp_ranges=hp_ranges,
        config_for_trial=config_for_trial,
        trials_evaluations=trials_evaluations,
        failed_trials=failed_trials,
        pending_evaluations=pending_evaluations,
    )


class ResourceForAcquisitionMap:
    """
    In order to use a standard acquisition function (like expected improvement)
    for multi-fidelity HPO, we need to decide at which `r_acq` we would like
    to evaluate the AF, w.r.t. the posterior distribution over `f(x, r=r_acq)`.
    This decision can depend on the current state.

    """

    def __call__(self, state: TuningJobState, **kwargs) -> int:
        raise NotImplementedError()


class ResourceForAcquisitionBOHB(ResourceForAcquisitionMap):
    """
    Implements a heuristic proposed in the BOHB paper: `r_acq` is the
    largest `r` such that we have at least `threshold` observations at
    `r`. If there are less than `threshold` observations at all levels,
    the smallest level is returned.

    """

    def __init__(self, threshold: int, active_metric: str = INTERNAL_METRIC_NAME):
        self.threshold = threshold
        self.active_metric = active_metric

    def __call__(self, state: TuningJobState, **kwargs) -> int:
        assert (
            state.num_observed_cases(self.active_metric) > 0
        ), f"state must have data for metric {self.active_metric}"
        all_resources = []
        for cand_eval in state.trials_evaluations:
            all_resources += [
                int(r) for r in cand_eval.metrics[self.active_metric].keys()
            ]
        histogram = Counter(all_resources)
        return self._max_at_least_threshold(histogram)

    def _max_at_least_threshold(self, counter: Counter) -> int:
        """
        Get largest key of `counter` whose value is at least `threshold`.

        :param counter: dict with keys that support comparison operators
        :return: largest key of `counter`
        """
        return max(
            filter(lambda r: counter[r] >= self.threshold, counter.keys()),
            default=min(counter.keys()),
        )


class ResourceForAcquisitionFirstMilestone(ResourceForAcquisitionMap):
    """
    Here, `r_acq` is the smallest rung level to be attained by a config
    started from scratch.

    """

    def __call__(self, state: TuningJobState, **kwargs) -> int:
        assert "milestone" in kwargs, (
            "Need the first milestone to be attained by the new config "
            + "passed as kwargs['milestone']. Use a scheduler which does "
            + "that (e.g., HyperbandScheduler)"
        )
        return kwargs["milestone"]


class ResourceForAcquisitionFinal(ResourceForAcquisitionMap):
    """
    Here, `r_acq = r_max` is the largest resource level.

    """

    def __init__(self, r_max: int):
        self._r_max = r_max

    def __call__(self, state: TuningJobState, **kwargs) -> int:
        return self._r_max


SUPPORTED_RESOURCE_FOR_ACQUISITION = {"bohb", "first", "final"}


def resource_for_acquisition_factory(
    kwargs: dict, hp_ranges: HyperparameterRanges
) -> ResourceForAcquisitionMap:
    resource_acq = kwargs.get("resource_acq", "bohb")
    assert (
        resource_acq in SUPPORTED_RESOURCE_FOR_ACQUISITION
    ), f"resource_acq = {resource_acq} not supported, must be in " + str(
        SUPPORTED_RESOURCE_FOR_ACQUISITION
    )
    if resource_acq == "bohb":
        threshold = kwargs.get("resource_acq_bohb_threshold", hp_ranges.ndarray_size)
        resource_for_acquisition = ResourceForAcquisitionBOHB(threshold=threshold)
    elif resource_acq == "first":
        assert resource_acq == "first", "resource_acq must be 'bohb' or 'first'"
        resource_for_acquisition = ResourceForAcquisitionFirstMilestone()
    else:
        r_max = kwargs["max_epochs"]
        resource_for_acquisition = ResourceForAcquisitionFinal(r_max=r_max)
    return resource_for_acquisition

File Path: syne_tune/optimizer/schedulers/searchers/kde_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, List
import logging
import numpy as np
import statsmodels.api as sm
import scipy.stats as sps

from syne_tune.optimizer.schedulers.searchers import SearcherWithRandomSeed
import syne_tune.config_space as sp
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)

__all__ = ["KernelDensityEstimator"]

logger = logging.getLogger(__name__)


class KernelDensityEstimator(SearcherWithRandomSeed):
    """
    Fits two kernel density estimators (KDE) to model the density of the top N configurations as well as the density
    of the configurations that are not among the top N, respectively. New configurations are sampled by optimizing
    the ratio of these two densities. KDE as model for Bayesian optimization has been originally proposed
    by Bergstra et al. Compared to their original implementation TPE, we use multi-variate instead of univariate KDE
    as proposed by Falkner et al.
    Code is based on the implementation by Falkner et al: https://github.com/automl/HpBandSter/tree/master/hpbandster

    Algorithms for Hyper-Parameter Optimization
    J. Bergstra and R. Bardenet and Y. Bengio and B. K{\'e}gl
    Proceedings of the 24th International Conference on Advances in Neural Information Processing Systems

    BOHB: Robust and Efficient Hyperparameter Optimization at Scale
    S. Falkner and A. Klein and F. Hutter
    Proceedings of the 35th International Conference on Machine Learning


    Parameters
    ----------
    config_space: dict
        Configuration space for trial evaluation function
    metric : str
        Name of metric to optimize, key in result's obtained via
        `on_trial_result`
    mode : str
        Mode to use for the metric given, can be 'min' or 'max', default to 'min'.
    random_seed_generator : RandomSeedGenerator (optional)
        If given, the random_seed for `random_state` is obtained from there,
        otherwise `random_seed` is used
    random_seed : int (optional)
        This is used if `random_seed_generator` is not given.
    num_min_data_points: int
        Minimum number of data points that we use to fit the KDEs. If set to None than we set this to the number of
        hyperparameters.
    top_n_percent: int
        Determines how many datapoints we use to fit the first KDE model for modeling the well
        performing configurations.
    min_bandwidth: float
        The minimum bandwidth for the KDE models
    num_candidates: int
        Number of candidates that are sampled to optimize the acquisition function
    bandwidth_factor: int
        We sample continuous hyperparameter from a truncated Normal. This factor is multiplied to the bandwidth to
        define the standard deviation of this trunacted Normal.
    random_fraction: float
        Defines the fraction of configurations that are drawn uniformly at random instead of sampling from the model
    points_to_evaluate: List[Dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified, or even be an
        empty dict. For each hyperparameter not specified, the default value
        is determined using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
    """

    def __init__(
        self,
        config_space: Dict,
        metric: str,
        points_to_evaluate: Optional[List[Dict]] = None,
        mode: str = "min",
        num_min_data_points: int = None,
        top_n_percent: int = 15,
        min_bandwidth: float = 1e-3,
        num_candidates: int = 64,
        bandwidth_factor: int = 3,
        random_fraction: float = 0.33,
        **kwargs,
    ):
        super().__init__(
            config_space=config_space,
            metric=metric,
            points_to_evaluate=points_to_evaluate,
            **kwargs,
        )
        self.mode = mode
        self.num_evaluations = 0
        self.min_bandwidth = min_bandwidth
        self.random_fraction = random_fraction
        self.num_candidates = num_candidates
        self.bandwidth_factor = bandwidth_factor
        self.top_n_percent = top_n_percent
        self.X = []
        self.y = []
        self.categorical_maps = {
            k: {cat: i for i, cat in enumerate(v.categories)}
            for k, v in config_space.items()
            if isinstance(v, sp.Categorical)
        }
        self.inv_categorical_maps = {
            hp: dict(zip(map.values(), map.keys()))
            for hp, map in self.categorical_maps.items()
        }

        self.good_kde = None
        self.bad_kde = None

        self.vartypes = []

        for name, hp in self.config_space.items():
            if isinstance(hp, sp.Categorical):
                self.vartypes.append(("u", len(hp.categories)))
            elif isinstance(hp, sp.Integer):
                self.vartypes.append(("o", (hp.lower, hp.upper)))
            elif isinstance(hp, sp.Float):
                self.vartypes.append(("c", 0))
            elif isinstance(hp, sp.FiniteRange):
                if hp.cast_int:
                    self.vartypes.append(("o", (hp.lower, hp.upper)))
                else:
                    self.vartypes.append(("c", 0))

        self.num_min_data_points = (
            len(self.vartypes) if num_min_data_points is None else num_min_data_points
        )
        assert self.num_min_data_points >= len(self.vartypes)
        self._resource_attr = kwargs.get("resource_attr")
        # Debug log printing (switched on by default)
        debug_log = kwargs.get("debug_log", True)
        if isinstance(debug_log, bool):
            if debug_log:
                self._debug_log = DebugLogPrinter()
            else:
                self._debug_log = None
        else:
            assert isinstance(debug_log, DebugLogPrinter)
            self._debug_log = debug_log

    def to_feature(self, config):
        def numerize(value, domain, categorical_map):
            if isinstance(domain, sp.Categorical):
                res = categorical_map[value] / len(domain)
                return res
            elif isinstance(domain, sp.Float):
                return [(value - domain.lower) / (domain.upper - domain.lower)]
            elif isinstance(domain, sp.FiniteRange):
                if domain.cast_int:
                    a = 1 / (2 * (domain.upper - domain.lower + 1))
                    b = domain.upper
                    return [(value - a) / (b - a)]
                else:
                    return [(value - domain.lower) / (domain.upper - domain.lower)]
            elif isinstance(domain, sp.Integer):
                a = 1 / (2 * (domain.upper - domain.lower + 1))
                b = domain.upper
                return [(value - a) / (b - a)]

        return np.hstack(
            [
                numerize(
                    value=config[k],
                    domain=v,
                    categorical_map=self.categorical_maps.get(k, {}),
                )
                for k, v in self.config_space.items()
                if isinstance(v, sp.Domain)
            ]
        )

    def from_feature(self, feature_vector):
        def inv_numerize(values, domain, categorical_map):
            if not isinstance(domain, sp.Domain):
                # constant value
                return domain
            else:
                if isinstance(domain, sp.Categorical):
                    index = int(values * len(domain))
                    return categorical_map[index]
                elif isinstance(domain, sp.Float):
                    return values * (domain.upper - domain.lower) + domain.lower
                elif isinstance(domain, sp.FiniteRange):
                    if domain.cast_int:
                        a = 1 / (2 * (domain.upper - domain.lower + 1))
                        b = domain.upper
                        return np.ceil(values * (b - a) + a)
                    else:
                        return values * (domain.upper - domain.lower) + domain.lower
                elif isinstance(domain, sp.Integer):
                    a = 1 / (2 * (domain.upper - domain.lower + 1))
                    b = domain.upper
                    return np.ceil(values * (b - a) + a)

        res = {}
        curr_pos = 0
        for k, domain in self.config_space.items():
            if isinstance(domain, sp.Domain):
                res[k] = domain.cast(
                    inv_numerize(
                        values=feature_vector[curr_pos],
                        domain=domain,
                        categorical_map=self.inv_categorical_maps.get(k, {}),
                    )
                )
                curr_pos += 1
            else:
                res[k] = domain
        return res

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.fifo import FIFOScheduler

        assert isinstance(
            scheduler, FIFOScheduler
        ), "This searcher requires FIFOScheduler scheduler"
        super().configure_scheduler(scheduler)

    def to_objective(self, result: Dict):
        if self.mode == "min":
            return result[self._metric]
        elif self.mode == "max":
            return -result[self._metric]

    def _update(self, trial_id: str, config: Dict, result: Dict):
        self.X.append(self.to_feature(config=config))
        self.y.append(self.to_objective(result))
        if self._debug_log is not None:
            metric_val = result[self._metric]
            if self._resource_attr is not None:
                # For HyperbandScheduler, also add the resource attribute
                resource = int(result[self._resource_attr])
                trial_id = trial_id + ":{}".format(resource)
            msg = f"Update for trial_id {trial_id}: metric = {metric_val:.3f}"
            logger.info(msg)

    def get_config(self, **kwargs):
        suggestion = self._next_initial_config()

        if suggestion is None:
            models = self.train_kde(np.array(self.X), np.array(self.y))

            if models is None or self.random_state.rand() < self.random_fraction:
                # return random candidate because a) we don't have enough data points or
                # b) we sample some fraction of all samples randomly
                suggestion = {
                    k: v.sample() if isinstance(v, sp.Domain) else v
                    for k, v in self.config_space.items()
                }
            else:
                self.bad_kde = models[0]
                self.good_kde = models[1]
                l = self.good_kde.pdf
                g = self.bad_kde.pdf

                acquisition_function = lambda x: max(1e-32, g(x)) / max(l(x), 1e-32)

                current_best = None
                val_current_best = None
                for i in range(self.num_candidates):
                    idx = self.random_state.randint(0, len(self.good_kde.data))
                    mean = self.good_kde.data[idx]
                    candidate = []

                    for m, bw, t in zip(mean, self.good_kde.bw, self.vartypes):
                        bw = max(bw, self.min_bandwidth)
                        vartype = t[0]
                        domain = t[1]
                        if vartype == "c":
                            # continuous parameter
                            bw = self.bandwidth_factor * bw
                            candidate.append(
                                sps.truncnorm.rvs(
                                    -m / bw,
                                    (1 - m) / bw,
                                    loc=m,
                                    scale=bw,
                                    random_state=self.random_state,
                                )
                            )
                        else:
                            # categorical or integer parameter
                            if self.random_state.rand() < (1 - bw):
                                candidate.append(m)
                            else:
                                if vartype == "o":
                                    # integer
                                    sample = self.random_state.randint(
                                        domain[0], domain[1]
                                    )
                                    sample = (sample - domain[0]) / (
                                        domain[1] - domain[0]
                                    )
                                    candidate.append(sample)
                                elif vartype == "u":
                                    # categorical
                                    candidate.append(
                                        self.random_state.randint(domain) / domain
                                    )
                    val = acquisition_function(candidate)

                    if not np.isfinite(val):
                        logging.warning(
                            "candidate has non finite acquisition function value"
                        )

                    if val_current_best is None or val_current_best > val:
                        current_best = candidate
                        val_current_best = val

                suggestion = self.from_feature(feature_vector=current_best)

        return suggestion

    def train_kde(self, train_data, train_targets):

        if train_data.shape[0] < self.num_min_data_points:
            return None

        n_good = max(
            self.num_min_data_points, (self.top_n_percent * train_data.shape[0]) // 100
        )
        n_bad = max(
            self.num_min_data_points,
            ((100 - self.top_n_percent) * train_data.shape[0]) // 100,
        )

        idx = np.argsort(train_targets)

        train_data_good = train_data[idx[:n_good]]
        train_data_bad = train_data[idx[n_good : n_good + n_bad]]

        if train_data_good.shape[0] <= train_data_good.shape[1]:
            return None
        if train_data_bad.shape[0] <= train_data_bad.shape[1]:
            return None

        types = [t[0] for t in self.vartypes]

        bad_kde = sm.nonparametric.KDEMultivariate(
            data=train_data_bad, var_type=types, bw="normal_reference"
        )
        good_kde = sm.nonparametric.KDEMultivariate(
            data=train_data_good, var_type=types, bw="normal_reference"
        )

        bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)
        good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)

        return bad_kde, good_kde

    def clone_from_state(self, state: dict):
        raise NotImplementedError

File Path: syne_tune/optimizer/schedulers/searchers/multi_fidelity_kde_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, Optional, List
import logging
import numpy as np

from syne_tune.optimizer.schedulers.searchers.kde_searcher import KernelDensityEstimator

__all__ = ["MultiFidelityKernelDensityEstimator"]

logger = logging.getLogger(__name__)


class MultiFidelityKernelDensityEstimator(KernelDensityEstimator):
    """
    Adapts the KernelDensityEstimator to the multi-fidelity setting as proposed by Falkner et al such that we can use
    it with Hyperband. Following Falkner et al, we fit the KDE only on the highest resource level where we
    have at least num_min_data_points.
    Code is based on the implementation by Falkner et al: https://github.com/automl/HpBandSter/tree/master/hpbandster

    BOHB: Robust and Efficient Hyperparameter Optimization at Scale
    S. Falkner and A. Klein and F. Hutter
    Proceedings of the 35th International Conference on Machine Learning


    Parameters
    ----------
    config_space: dict
        Configuration space for trial evaluation function
    metric : str
        Name of metric to optimize, key in result's obtained via
        `on_trial_result`
    random_seed_generator : RandomSeedGenerator (optional)
        If given, the random_seed for `random_state` is obtained from there,
        otherwise `random_seed` is used
    random_seed : int (optional)
        This is used if `random_seed_generator` is not given.
    mode : str
        Mode to use for the metric given, can be 'min' or 'max', default to 'min'.
    num_min_data_points: int
        Minimum number of data points that we use to fit the KDEs. If set to None than we set this to the number of
        hyperparameters.
    top_n_percent: int
        Determines how many datapoints we use use to fit the first KDE model for modeling the well
        performing configurations.
    min_bandwidth: float
        The minimum bandwidth for the KDE models
    num_candidates: int
        Number of candidates that are sampled to optimize the acquisition function
    bandwidth_factor: int
        We sample continuous hyperparameter from a truncated Normal. This factor is multiplied to the bandwidth to
        define the standard deviation of this trunacted Normal.
    random_fraction: float
        Defines the fraction of configurations that are drawn uniformly at random instead of sampling from the model
    points_to_evaluate: List[Dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified, or even be an
        empty dict. For each hyperparameter not specified, the default value
        is determined using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
    """

    def __init__(
        self,
        config_space: Dict,
        metric: str,
        points_to_evaluate: Optional[List[Dict]] = None,
        mode: str = "min",
        num_min_data_points: int = None,
        top_n_percent: int = 15,
        min_bandwidth: float = 0.1,
        num_candidates: int = 64,
        bandwidth_factor: int = 3,
        random_fraction: float = 0.33,
        resource_attr: str = "epoch",
        **kwargs
    ):
        super().__init__(
            config_space,
            metric=metric,
            points_to_evaluate=points_to_evaluate,
            mode=mode,
            num_min_data_points=num_min_data_points,
            top_n_percent=top_n_percent,
            min_bandwidth=min_bandwidth,
            num_candidates=num_candidates,
            bandwidth_factor=bandwidth_factor,
            random_fraction=random_fraction,
            **kwargs
        )
        self.resource_attr = resource_attr
        self.resource_levels = []

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
        from syne_tune.optimizer.schedulers.synchronous.hyperband import (
            SynchronousHyperbandScheduler,
        )

        assert isinstance(scheduler, HyperbandScheduler) or isinstance(
            scheduler, SynchronousHyperbandScheduler
        ), (
            "This searcher requires HyperbandScheduler or "
            + "SynchronousHyperbandScheduler scheduler"
        )

    def train_kde(self, train_data, train_targets):

        # find the highest resource level we have at least num_min_data_points data points
        unique_resource_levels, counts = np.unique(
            self.resource_levels, return_counts=True
        )
        idx = np.where(counts >= self.num_min_data_points)[0]
        if len(idx) == 0:
            return

        # collect data on the highest resource level
        highest_resource_level = unique_resource_levels[idx[-1]]
        indices = np.where(self.resource_levels == highest_resource_level)[0]

        train_data = np.array([self.X[i] for i in indices])
        train_targets = np.array([self.y[i] for i in indices])

        super().train_kde(train_data, train_targets)

    def _update(self, trial_id: str, config: Dict, result: Dict):
        super()._update(trial_id=trial_id, config=config, result=result)
        resource_level = int(result[self.resource_attr])
        self.resource_levels.append(resource_level)

File Path: syne_tune/optimizer/schedulers/searchers/regularized_evolution.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import copy
import logging

from collections import deque
from typing import Optional, Dict, List
from dataclasses import dataclass

from syne_tune.optimizer.schedulers.searchers import SearcherWithRandomSeed
from syne_tune.config_space import Domain


@dataclass
class PopulationElement:
    score: int = 0
    config: dict = None


class RegularizedEvolution(SearcherWithRandomSeed):
    def __init__(
        self,
        config_space,
        metric: str,
        mode: str = "min",
        population_size: int = 100,
        sample_size: int = 10,
        points_to_evaluate: Optional[List[Dict]] = None,
        **kwargs,
    ):
        """
        Implements the regularized evolution algorithm proposed by Real et al. The original implementation only
        considers categorical hyperparameters. For integer and float parameters we sample a new value uniformly
        at random.

        Real, E., Aggarwal, A., Huang, Y., and Le, Q. V.
        Regularized Evolution for Image Classifier Architecture Search.
        In Proceedings of the Conference on Artificial Intelligence (AAAI’19)

        The code is based one the original regularized evolution open-source implementation:
        https://colab.research.google.com/github/google-research/google-research/blob/master/evolution/regularized_evolution_algorithm/regularized_evolution.ipynb


        Parameters
        ----------
        config_space: dict
            Configuration space for trial evaluation function
        metric : str
            Name of metric to optimize, key in result's obtained via
            `on_trial_result`
        mode : str
            Mode to use for the metric given, can be 'min' or 'max', default to 'min'.
        population_size : int
            Size of the population.
        sample_size : int
            Size of the candidate set to obtain a parent for the mutation.
        random_seed : int
            Seed for the random number generation. If set to None, use random seed.
        """

        super(RegularizedEvolution, self).__init__(
            config_space, metric, points_to_evaluate=points_to_evaluate, **kwargs
        )
        self.mode = mode
        self.population_size = population_size
        self.sample_size = sample_size
        self.population = deque()
        self.num_sample_try = 1000  # number of times allowed to sample a mutation

    def mutate_config(self, config: Dict) -> Dict:

        child_config = copy.deepcopy(config)

        # sample mutation until a different configuration is found
        for sample_try in range(self.num_sample_try):
            if child_config == config:
                # sample a random hyperparameter to mutate
                hps = [
                    (k, v)
                    for k, v in self.config_space.items()
                    if isinstance(v, Domain) and len(v) > 1
                ]
                assert (
                    len(hps) >= 0
                ), "all hyperparameters only have a single value, cannot perform mutations."
                hp_name, hp = hps[self.random_state.randint(len(hps))]

                # mutate the value by sampling
                config[hp_name] = hp.sample(random_state=self.random_state)
            else:
                break
        if sample_try == self.num_sample_try:
            logging.INFO(
                f"Did not manage to sample a different configuration with {self.num_sample_try}, "
                f"sampling at random"
            )
            return self.sample_random_config()

        return child_config

    def sample_random_config(self) -> Dict:
        return {
            k: v.sample(random_state=self.random_state) if isinstance(v, Domain) else v
            for k, v in self.config_space.items()
        }

    def get_config(self, **kwargs):

        initial_config = self._next_initial_config()

        if initial_config is not None:
            return initial_config

        if len(self.population) < self.population_size:
            config = self.sample_random_config()
        else:
            candidates = self.random_state.choice(
                list(self.population), size=self.sample_size
            )
            parent = min(candidates, key=lambda i: i.score)

            config = self.mutate_config(parent.config)

        return config

    def _update(self, trial_id: str, config: Dict, result: Dict):

        score = result[self._metric]

        if self.mode == "max":
            score *= -1

        # Add element to the population
        element = PopulationElement(score=score, config=config)
        self.population.append(element)

        # Remove the oldest element of the population.
        if len(self.population) > self.population_size:
            self.population.popleft()

    def configure_scheduler(self, scheduler):
        from syne_tune.optimizer.schedulers.fifo import FIFOScheduler

        assert isinstance(
            scheduler, FIFOScheduler
        ), "This searcher requires FIFOScheduler scheduler"
        super().configure_scheduler(scheduler)

    def clone_from_state(self, state: dict):
        raise NotImplementedError

File Path: syne_tune/optimizer/schedulers/searchers/searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import numpy as np
from random import shuffle
from typing import Dict, Optional, List, Tuple

from syne_tune.config_space import Domain, config_space_size, is_log_space, Categorical
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.common import (
    ExclusionList,
)
from itertools import product

__all__ = [
    "BaseSearcher",
    "SearcherWithRandomSeed",
    "RandomSearcher",
    "GridSearcher",
    "impute_points_to_evaluate",
    "extract_random_seed",
]

logger = logging.getLogger(__name__)


def _impute_default_config(default_config, config_space):
    new_config = dict()
    for name, hp_range in config_space.items():
        if isinstance(hp_range, Domain):
            if name not in default_config:
                if isinstance(hp_range, Categorical):
                    # For categorical: Pick first entry
                    new_config[name] = hp_range.categories[0]
                else:
                    lower, upper = float(hp_range.lower), float(hp_range.upper)
                    if not is_log_space(hp_range):
                        midpoint = 0.5 * (upper + lower)
                    else:
                        midpoint = np.exp(0.5 * (np.log(upper) + np.log(lower)))
                    # Casting may involve rounding to nearest value in
                    # a finite range
                    midpoint = hp_range.cast(midpoint)
                    midpoint = np.clip(midpoint, hp_range.lower, hp_range.upper)
                    new_config[name] = midpoint
            else:
                # Check validity
                # Note: For `FiniteRange`, the value is mapped to
                # the closest one in the range
                val = hp_range.cast(default_config[name])
                if isinstance(hp_range, Categorical):
                    assert val in hp_range.categories, (
                        f"default_config[{name}] = {val} is not in "
                        + f"categories = {hp_range.categories}"
                    )
                else:
                    assert hp_range.lower <= val <= hp_range.upper, (
                        f"default_config[{name}] = {val} is not in "
                        + f"[{hp_range.lower}, {hp_range.upper}]"
                    )
                new_config[name] = val
    return new_config


def _to_tuple(config: dict, keys: List) -> Tuple:
    return tuple(config[k] for k in keys)


def _sorted_keys(config_space: dict) -> List[str]:
    return sorted(k for k, v in config_space.items() if isinstance(v, Domain))


def impute_points_to_evaluate(
    points_to_evaluate: Optional[List[dict]], config_space: dict
) -> List[dict]:
    """
    Transforms `points_to_evaluate` argument to `BaseSearcher`. Each config in
    the list can be partially specified, or even be an empty dict. For each
    hyperparameter not specified, the default value is determined using a
    midpoint heuristic. Also, duplicate entries are filtered out.
    If None (default), this is mapped to [dict()], a single default config
    determined by the midpoint heuristic. If [] (empty list), no initial
    configurations are specified.

    :param points_to_evaluate:
    :param config_space:
    :return: List of fully specified initial configs
    """
    if points_to_evaluate is None:
        points_to_evaluate = [dict()]
    # Impute and filter out duplicates
    result = []
    excl_set = set()
    keys = _sorted_keys(config_space)
    for point in points_to_evaluate:
        config = _impute_default_config(point, config_space)
        config_tpl = _to_tuple(config, keys)
        if config_tpl not in excl_set:
            result.append(config)
            excl_set.add(config_tpl)
    return result


class BaseSearcher:
    """Base Searcher (virtual class to inherit from if you are creating a custom Searcher).

    Parameters
    ----------
    config_space : dict
        The configuration space to sample from. It contains the full
        specification of the Hyperparameters with their priors
    metric : str
        Name of metric passed to update.
    points_to_evaluate : List[dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified, or even be an
        empty dict. For each hyperparameter not specified, the default value
        is determined using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
    """

    def __init__(self, config_space, metric, points_to_evaluate=None):
        self.config_space = config_space
        assert metric is not None, "Argument 'metric' is required"
        self._metric = metric
        self._points_to_evaluate = impute_points_to_evaluate(
            points_to_evaluate, config_space
        )

    def configure_scheduler(self, scheduler):
        """
        Some searchers need to obtain information from the scheduler they are
        used with, in order to configure themselves.
        This method has to be called before the searcher can be used.

        The implementation here sets _metric for schedulers which specify it.

        Args:
            scheduler: TaskScheduler
                Scheduler the searcher is used with.

        """
        from syne_tune.optimizer.schedulers.fifo import FIFOScheduler

        if isinstance(scheduler, FIFOScheduler):
            self._metric = scheduler.metric

    def _next_initial_config(self) -> Optional[dict]:
        if self._points_to_evaluate:
            return self._points_to_evaluate.pop(0)
        else:
            return None  # No more initial configs

    def get_config(self, **kwargs):
        """Function to sample a new configuration

        This function is called inside TaskScheduler to query a new
        configuration.

        Note: Query `_next_initial_config` for initial configs to return first.

        Args:
        kwargs:
            Extra information may be passed from scheduler to searcher
        returns: New configuration. The searcher may return None if a new
            configuration cannot be suggested. In this case, the tuning will
            stop. This happens if searchers never suggest the same config more
            than once, and all configs in the (finite) search space are
            exhausted.
        """
        raise NotImplementedError

    def on_trial_result(self, trial_id: str, config: dict, result: dict, update: bool):
        """Inform searcher about result

        The scheduler passes every result. If `update` is True, the searcher
        should update its surrogate model (if any), otherwise `result` is an
        intermediate result not modelled.

        The default implementation calls self._update if `update` is True. It
        can be overwritten by searchers which also react to intermediate
        results.

        :param trial_id:
        :param config:
        :param result:
        :param update: Should surrogate model be updated?
        """
        if update:
            self._update(trial_id, config, result)

    def _update(self, trial_id: str, config: dict, result: dict):
        """Update surrogate model with result

        :param trial_id:
        :param config:
        :param result:
        """
        raise NotImplementedError

    def register_pending(
        self, trial_id: str, config: Optional[dict] = None, milestone=None
    ):
        """
        Signals to searcher that evaluation for trial has started, but not
        yet finished, which allows model-based searchers to register this
        evaluation as pending.
        For multi-fidelity schedulers, milestone is the next milestone the
        evaluation will attend, so that model registers (config, milestone)
        as pending.
        The configuration for the trial has to be passed in `config` for a
        new trial, which the searcher has not seen before. If the trial is
        already registered with th searcher, `config` is ignored.
        """
        pass

    def remove_case(self, trial_id: str, **kwargs):
        """Remove data case previously appended by update

        For searchers which maintain the dataset of all cases (reports) passed
        to update, this method allows to remove one case from the dataset.
        """
        pass

    def evaluation_failed(self, trial_id: str):
        """
        Called by scheduler if an evaluation job for a trial failed. The
        searcher should react appropriately (e.g., remove pending evaluations
        for this trial, not suggest the configuration again).
        """
        pass

    def cleanup_pending(self, trial_id: str):
        """
        Removes all pending candidates whose configuration is equal to
        `config`.
        This should be called after an evaluation terminates. For various
        reasons (e.g., termination due to convergence), pending candidates
        for this evaluation may still be present.

        """
        pass

    def dataset_size(self):
        """
        :return: Size of dataset a model is fitted to, or 0 if no model is
            fitted to data
        """
        return 0

    def model_parameters(self):
        """
        :return: Dictionary with current model (hyper)parameter values if
            this is supported; otherwise empty
        """
        return dict()

    def get_state(self) -> dict:
        """
        Together with clone_from_state, this is needed in order to store and
        re-create the mutable state of the searcher.

        The state returned here must be pickle-able.

        :return: Pickle-able mutable state of searcher
        """
        return {"points_to_evaluate": self._points_to_evaluate}

    def clone_from_state(self, state: dict):
        """
        Together with get_state, this is needed in order to store and
        re-create the mutable state of the searcher.

        Given state as returned by get_state, this method combines the
        non-pickle-able part of the immutable state from self with state
        and returns the corresponding searcher clone. Afterwards, self is
        not used anymore.

        :param state: See above
        :return: New searcher object
        """
        raise NotImplementedError

    def _restore_from_state(self, state: dict):
        self._points_to_evaluate = state["points_to_evaluate"].copy()

    @property
    def debug_log(self):
        """
        Some BaseSearcher subclasses support writing a debug log, using
        DebugLogPrinter. See RandomSearcher for an example.

        :return: DebugLogPrinter; or None (not supported)
        """
        return None


def extract_random_seed(kwargs: dict) -> (int, dict):
    key = "random_seed_generator"
    if kwargs.get(key) is not None:
        random_seed = kwargs[key]()
    else:
        key = "random_seed"
        if kwargs.get(key) is not None:
            random_seed = kwargs[key]
        else:
            random_seed = 31415927
            key = None
    _kwargs = {k: v for k, v in kwargs.items() if k != key}
    return random_seed, _kwargs


class SearcherWithRandomSeed(BaseSearcher):
    """
    Base class of searchers which use random decisions. Creates the
    `random_state` member, which must be used for all random draws.

    Making proper use of this interface allows us to run experiments
    with control of random seeds, e.g. for paired comparisons or
    integration testing.

    Extra parameters
    ----------------
    random_seed_generator : RandomSeedGenerator (optional)
        If given, the random_seed for `random_state` is obtained from there,
        otherwise `random_seed` is used
    random_seed : int (optional)
        This is used if `random_seed_generator` is not given.

    """

    def __init__(self, config_space, metric, points_to_evaluate=None, **kwargs):
        super().__init__(
            config_space, metric=metric, points_to_evaluate=points_to_evaluate
        )
        random_seed, _ = extract_random_seed(kwargs)
        self.random_state = np.random.RandomState(random_seed)

    def get_state(self) -> dict:
        state = dict(super().get_state(), random_state=self.random_state.get_state())
        return state

    def _restore_from_state(self, state: dict):
        super()._restore_from_state(state)
        self.random_state.set_state(state["random_state"])


class RandomSearcher(SearcherWithRandomSeed):
    """Searcher which randomly samples configurations to try next.

    Extra parameters
    ----------------
    debug_log : bool
        If True (default), debug log printing is activated. Logs which
        configs are chosen when, and which metric values are obtained.

    """

    MAX_RETRIES = 100

    def __init__(self, config_space, metric, points_to_evaluate=None, **kwargs):
        super().__init__(config_space, metric, points_to_evaluate, **kwargs)
        self._hp_ranges = make_hyperparameter_ranges(config_space)
        self._resource_attr = kwargs.get("resource_attr")
        self._excl_list = ExclusionList.empty_list(self._hp_ranges)
        # Debug log printing (switched on by default)
        debug_log = kwargs.get("debug_log", False)
        if isinstance(debug_log, bool):
            if debug_log:
                self._debug_log = DebugLogPrinter()
            else:
                self._debug_log = None
        else:
            assert isinstance(debug_log, DebugLogPrinter)
            self._debug_log = debug_log

    def configure_scheduler(self, scheduler):
        """
        Some searchers need to obtain information from the scheduler they are
        used with, in order to configure themselves.
        This method has to be called before the searcher can be used.

        The implementation here sets _metric for schedulers which
        specify it.

        Args:
            scheduler: TaskScheduler
                Scheduler the searcher is used with.

        """
        from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler

        super().configure_scheduler(scheduler)
        # If the scheduler is Hyperband, we want to know the resource
        # attribute, this is used for debug_log
        if isinstance(scheduler, HyperbandScheduler):
            self._resource_attr = scheduler._resource_attr

    def get_config(self, **kwargs):
        """Sample a new configuration at random
        This is done without replacement, so previously returned configs are
        not suggested again.

        Returns
        -------
        A new configuration that is valid, or None if no new config can be
        suggested.
        """
        new_config = self._next_initial_config()
        if new_config is None:
            if not self._excl_list.config_space_exhausted():
                for _ in range(self.MAX_RETRIES):
                    _config = self._hp_ranges.random_config(self.random_state)
                    if not self._excl_list.contains(_config):
                        new_config = _config
                        break

        if new_config is not None:
            self._excl_list.add(new_config)  # Should not be suggested again
            if self._debug_log is not None:
                trial_id = kwargs.get("trial_id")
                self._debug_log.start_get_config("random", trial_id=trial_id)
                self._debug_log.set_final_config(new_config)
                # All get_config debug log info is only written here
                self._debug_log.write_block()
        else:
            msg = (
                "Failed to sample a configuration not already chosen "
                + f"before. Exclusion list has size {len(self._excl_list)}."
            )
            cs_size = self._excl_list.configspace_size
            if cs_size is not None:
                msg += f" Configuration space has size {cs_size}."
            logger.warning(msg)
        return new_config

    def _update(self, trial_id: str, config: dict, result: dict):
        if self._debug_log is not None:
            metric_val = result[self._metric]
            if self._resource_attr is not None:
                # For HyperbandScheduler, also add the resource attribute
                resource = int(result[self._resource_attr])
                trial_id = trial_id + ":{}".format(resource)
            msg = f"Update for trial_id {trial_id}: metric = {metric_val:.3f}"
            logger.info(msg)

    def clone_from_state(self, state: dict):
        new_searcher = RandomSearcher(
            self.config_space, metric=self._metric, debug_log=self._debug_log
        )
        new_searcher._resource_attr = self._resource_attr
        new_searcher._restore_from_state(state)
        return new_searcher

    @property
    def debug_log(self):
        return self._debug_log


class GridSearcher(BaseSearcher):
    """Searcher that samples configurations from a equally spaced grid over config_space.
    It first evaluates configurations defined in points_to_evaluate and then
    continues with the remaining points from the grid"

    Note: GridSearcher only support Categorical hyperparameters for now

    Parameters
    ----------
    config_space : Dict
        The configuration space that defines search space grid. It contains
        the full specification of the Hyperparameters, and the configurations
        generated is the combinations of these Hyperparameters.
        The specified hyperparameter must be Categorical for now.
    metric : str
        Name of metric passed to update.
    points_to_evaluate : List[Dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified,
        or even be an empty dict.
        Each specified hyperparameter must be within the config space. That is,
        all the configurations in this list should be on the search space grid.
        For each hyperparameter not specified, the default value is determined
        using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
    shuffle_config : bool
        If True (default), the order of configurations suggested after those
        specified in points_to_evalutate is shuffled. Otherwised, the order
        will follow the Cartesian product of the configurations, and the
        hyperparameter specified first in the config space will be explore
        first. For example (shuffle_config is False):
        config_space = {'hp1': [1,2,3], 'hp2': [4,5], 'hp3': [6,7]}
        order: (1, 4, 6), (2, 4, 6), (3, 4, 6), (1, 5, 6), (2, 5, 6), (3, 5, 6),
            (1, 4, 7), (2, 4, 7), (3, 4, 7), (1, 5, 7), (2, 5, 7), (3, 5, 7)
    """

    def __init__(
        self,
        config_space,
        metric,
        points_to_evaluate=None,
        shuffle_config=True,
        **kwargs,
    ):
        super().__init__(config_space, metric, points_to_evaluate)
        self._validate_config_space(config_space)
        self._hp_ranges = make_hyperparameter_ranges(config_space)
        if not isinstance(shuffle_config, bool):
            shuffle_config = True
        self._shuffle_config = shuffle_config
        self._remaining_candidates = self._generate_remaining_candidates()

    def get_config(self, **kwargs):
        """Select the next configuration from the grid.
        This is done without replacement, so previously returned configs are
        not suggested again.

        Returns
        -------
        A new configuration that is valid, or None if no new config can be
        suggested.
        """
        new_config = self._next_initial_config()
        if new_config is None:
            new_config = self._next_candidate_on_grid()

        if new_config is not None:
            # Write debug log for the the config
            entries = ["{}: {}".format(k, v) for k, v in new_config.items()]
            msg = "\n".join(entries)
            trial_id = kwargs.get("trial_id")
            if trial_id is not None:
                msg = "get_config[grid] for trial_id {}\n".format(trial_id) + msg
            else:
                msg = "get_config[grid]: \n".format(trial_id) + msg
            logger.debug(msg)
        else:
            msg = "All the configurations has already been evaluated."
            cs_size = config_space_size(self.config_space)
            if cs_size is not None:
                msg += " Configuration space has size".format(cs_size)
            logger.warning(msg)

        return new_config

    def get_batch_configs(self, batch_size: int, **kwargs):
        """
        Asks for a batch of `batch_size` configurations to be suggested. This
        is roughly equivalent to calling `get_config` `batch_size` times,

        If less than `batch_size` configs are returned, the search space
        has been exhausted.
        """
        assert round(batch_size) == batch_size and batch_size >= 1
        configs = []
        for _ in range(batch_size):
            config = self.get_config(**kwargs)
            if config is not None:
                configs.append(config)
        return configs

    def _validate_config_space(self, config_space: Dict):
        # GridSearcher only supports Categorical hyperparameters for now
        for hp_range in config_space.values():
            if isinstance(hp_range, Domain):
                assert isinstance(hp_range, Categorical)

    def _generate_remaining_candidates(self) -> List[Dict]:
        excl_list = ExclusionList.empty_list(self._hp_ranges)
        for candidate in self._points_to_evaluate:
            excl_list.add(candidate)
        remaining_candidates = []
        for candidate in self._generate_all_candidates_on_grid():
            if not excl_list.contains(candidate):
                remaining_candidates.append(candidate)
                excl_list.add(candidate)
        return remaining_candidates

    def _generate_all_candidates_on_grid(self) -> List[Dict]:
        # Get hp values that are specified as Domain
        hp_keys = []
        hp_values = []
        for key, hp_range in reversed(list(self.config_space.items())):
            if isinstance(hp_range, Domain):
                hp_keys.append(key)
                hp_values.append(hp_range.categories)

        hp_values_combinations = list(product(*hp_values))
        if self._shuffle_config:
            shuffle(hp_values_combinations)

        all_candidates_on_grid = []
        for values in hp_values_combinations:
            all_candidates_on_grid.append(dict(zip(hp_keys, values)))
        return all_candidates_on_grid

    def _next_candidate_on_grid(self) -> Optional[Dict]:
        if self._remaining_candidates:
            return self._remaining_candidates.pop(0)
        else:
            # No more candidates
            return None

    def get_state(self) -> dict:
        state = dict(
            super().get_state(),
            remaining_candidates=self._remaining_candidates,
        )
        return state

    def clone_from_state(self, state: dict):
        new_searcher = GridSearcher(
            config_space=self.config_space,
            metric=self._metric,
            shuffle_config=self._shuffle_config,
        )
        new_searcher._restore_from_state(state)
        return new_searcher

    def _restore_from_state(self, state: dict):
        super()._restore_from_state(state)
        self._remaining_candidates = state["remaining_candidates"].copy()

    def _update(self, trial_id: str, config: Dict, result: Dict):
        # GridSearcher does not contains a surrogate model, just return.
        return

File Path: syne_tune/optimizer/schedulers/searchers/searcher_callback.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import logging

from syne_tune.backend.trial_status import Trial
from syne_tune.tuner_callback import StoreResultsCallback
from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import ModelBasedSearcher
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.model_transformer import (
    ModelStateTransformer,
)

logger = logging.getLogger(__name__)


def _get_model_based_searcher(tuner):
    searcher = None
    scheduler = tuner.scheduler
    if isinstance(scheduler, FIFOScheduler):
        if isinstance(scheduler.searcher, ModelBasedSearcher):
            searcher = scheduler.searcher
            state_transformer = searcher.state_transformer
            if state_transformer is not None:
                if not isinstance(state_transformer, ModelStateTransformer):
                    searcher = None
                elif not state_transformer.use_single_model:
                    logger.warning(
                        "StoreResultsAndModelParamsCallback does not currently "
                        "support multi-model setups. Model parameters sre "
                        "not logged."
                    )
                    searcher = None
            else:
                searcher = None
    return searcher


def _extended_result(searcher, result):
    if searcher is not None:
        kwargs = dict()
        # Append surrogate model parameters to `result`
        params = searcher.state_transformer.get_params()
        if params:
            prefix = "model_"
            kwargs = {prefix + k: v for k, v in params.items()}
        kwargs["cumulative_get_config_time"] = searcher.cumulative_get_config_time
        result = dict(result, **kwargs)
    return result


class StoreResultsAndModelParamsCallback(StoreResultsCallback):
    """
    Extends :class:`StoreResultsCallback` by also storing the current
    surrogate model parameters in `on_trial_result`. This works for
    schedulers with model-based searchers. For other schedulers, this
    callback behaves the same as the superclass.

    """

    def __init__(
        self,
        add_wallclock_time: bool = True,
    ):
        super().__init__(add_wallclock_time)
        self._searcher = None

    def on_tuning_start(self, tuner):
        super().on_tuning_start(tuner)
        self._searcher = _get_model_based_searcher(tuner)

    def on_trial_result(self, trial: Trial, status: str, result: Dict, decision: str):
        result = _extended_result(self._searcher, result)
        super().on_trial_result(trial, status, result, decision)


class SimulatorAndModelParamsCallback(SimulatorCallback):
    """
    Extends :class:`SimulatorCallback` by also storing the current
    surrogate model parameters in `on_trial_result`. This works for
    schedulers with model-based searchers. For other schedulers, this
    callback behaves the same as the superclass.

    """

    def __init__(self):
        super().__init__()
        self._searcher = None

    def on_tuning_start(self, tuner):
        super().on_tuning_start(tuner)
        self._searcher = _get_model_based_searcher(tuner)

    def on_trial_result(self, trial: Trial, status: str, result: Dict, decision: str):
        result = _extended_result(self._searcher, result)
        super().on_trial_result(trial, status, result, decision)

File Path: syne_tune/optimizer/schedulers/searchers/searcher_factory.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune.try_import import (
    try_import_gpsearchers_message,
    try_import_kde_message,
    try_import_bore_message,
)
from syne_tune.optimizer.schedulers.searchers.searcher import (
    RandomSearcher,
    GridSearcher,
)

__all__ = ["searcher_factory"]

logger = logging.getLogger(__name__)


_OUR_MULTIFIDELITY_SCHEDULERS = {
    "hyperband_stopping",
    "hyperband_promotion",
    "hyperband_cost_promotion",
    "hyperband_pasha",
    "hyperband_synchronous",
}


def searcher_factory(searcher_name, **kwargs):
    """Factory for searcher objects

    This function creates searcher objects from string argument name and
    additional kwargs. It is typically called in the constructor of a
    scheduler (see :class:`FIFOScheduler`), which provides most of the required
    kwargs.

    """
    supported_schedulers = None
    scheduler = kwargs.get("scheduler")
    model = kwargs.get("model", "gp_multitask")
    if searcher_name == "random":
        searcher_cls = RandomSearcher
    elif searcher_name == "grid":
        searcher_cls = GridSearcher
    elif searcher_name == "kde":
        try:
            from syne_tune.optimizer.schedulers.searchers.kde_searcher import (
                KernelDensityEstimator,
            )
            from syne_tune.optimizer.schedulers.searchers.multi_fidelity_kde_searcher import (
                MultiFidelityKernelDensityEstimator,
            )
        except ImportError:
            logger.info(try_import_kde_message())
            raise

        if scheduler == "fifo":
            searcher_cls = KernelDensityEstimator
        else:
            supported_schedulers = _OUR_MULTIFIDELITY_SCHEDULERS
            searcher_cls = MultiFidelityKernelDensityEstimator
    elif searcher_name == "bore":
        try:
            from syne_tune.optimizer.schedulers.searchers.bore import Bore
            from syne_tune.optimizer.schedulers.searchers.bore.multi_fidelity_bore import (
                MultiFidelityBore,
            )
        except ImportError:
            logger.info(try_import_bore_message())
            raise

        if scheduler == "fifo":
            searcher_cls = Bore
        else:
            supported_schedulers = _OUR_MULTIFIDELITY_SCHEDULERS
            searcher_cls = MultiFidelityBore
    else:
        gp_searchers = {
            "bayesopt",
            "bayesopt_constrained",
            "bayesopt_cost",
        }
        assert (
            searcher_name in gp_searchers
        ), f"searcher '{searcher_name}' is not supported"
        try:
            from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import (
                GPFIFOSearcher,
            )
            from syne_tune.optimizer.schedulers.searchers.constrained_gp_fifo_searcher import (
                ConstrainedGPFIFOSearcher,
            )
            from syne_tune.optimizer.schedulers.searchers.cost_aware_gp_fifo_searcher import (
                CostAwareGPFIFOSearcher,
            )
            from syne_tune.optimizer.schedulers.searchers.gp_multifidelity_searcher import (
                GPMultiFidelitySearcher,
            )
            from syne_tune.optimizer.schedulers.searchers.cost_aware_gp_multifidelity_searcher import (
                CostAwareGPMultiFidelitySearcher,
            )
        except ImportError:
            logger.info(try_import_gpsearchers_message())
            raise

        if searcher_name == "bayesopt":
            if scheduler == "fifo":
                searcher_cls = GPFIFOSearcher
            else:
                supported_schedulers = _OUR_MULTIFIDELITY_SCHEDULERS
                if (
                    model == "gp_multitask"
                    and kwargs.get("gp_resource_kernel") == "freeze-thaw"
                ):
                    logger.warning(
                        "You are combining model = gp_multitask with "
                        "gp_resource_kernel = freeze-thaw. This is mainly "
                        "for debug purposes. The same surrogate model is "
                        "obtained with model = gp_expdecay, but computations "
                        "are faster then."
                    )
                searcher_cls = GPMultiFidelitySearcher
        elif searcher_name == "bayesopt_constrained":
            supported_schedulers = {"fifo"}
            searcher_cls = ConstrainedGPFIFOSearcher
        else:  # bayesopt_cost
            if scheduler == "fifo":
                searcher_cls = CostAwareGPFIFOSearcher
            else:
                supported_schedulers = _OUR_MULTIFIDELITY_SCHEDULERS
                searcher_cls = CostAwareGPMultiFidelitySearcher

    if supported_schedulers is not None:
        assert scheduler is not None, "Scheduler must set search_options['scheduler']"
        assert scheduler in supported_schedulers, (
            f"Searcher '{searcher_name}' only works with schedulers "
            + f"{supported_schedulers} (not with '{scheduler}')"
        )
    searcher = searcher_cls(**kwargs)
    return searcher

File Path: syne_tune/optimizer/schedulers/searchers/utils/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/searchers/utils/default_arguments.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Set, Tuple, Dict
import logging
import numbers

logger = logging.getLogger(__name__)


class CheckType:
    def assert_valid(self, key: str, value):
        pass


class Float(CheckType):
    def __init__(self, lower: float = None, upper: float = None):
        if lower and upper:
            assert lower < upper
        self.lower = lower
        self.upper = upper

    def assert_valid(self, key: str, value):
        assert isinstance(
            value, numbers.Real
        ), "{}: Value = {} must be of type float".format(key, value)
        assert (
            not self.lower
        ) or value >= self.lower, "{}: Value = {} must be >= {}".format(
            key, value, self.lower
        )
        assert (
            not self.upper
        ) or value <= self.upper, "{}: Value = {} must be <= {}".format(
            key, value, self.upper
        )


class Integer(CheckType):
    def __init__(self, lower: int = None, upper: int = None):
        if lower and upper:
            assert lower < upper
        self.lower = lower
        self.upper = upper

    def assert_valid(self, key: str, value):
        assert isinstance(
            value, numbers.Integral
        ), "{}: Value = {} must be of type int".format(key, value)
        assert (
            not self.lower
        ) or value >= self.lower, "{}: Value = {} must be >= {}".format(
            key, value, self.lower
        )
        assert (
            not self.upper
        ) or value <= self.upper, "{}: Value = {} must be <= {}".format(
            key, value, self.upper
        )


class Categorical(CheckType):
    def __init__(self, choices: Tuple[str, ...]):
        self.choices = set(choices)

    def assert_valid(self, key: str, value):
        assert (
            isinstance(value, str) and value in self.choices
        ), "{}: Value = {} must be in {}".format(key, value, self.choices)


class String(CheckType):
    def assert_valid(self, key: str, value):
        assert isinstance(value, str), "{}: Value = {} must be of type str"


class Boolean(CheckType):
    def assert_valid(self, key: str, value):
        assert isinstance(value, bool), "{}: Value = {} must be boolean".format(
            key, value
        )


class Dictionary(CheckType):
    def assert_valid(self, key: str, value):
        assert isinstance(value, dict), "{}: Value = {} must be a dictionary".format(
            key, value
        )


def check_and_merge_defaults(
    options: dict,
    mandatory: Set[str],
    default_options: dict,
    constraints: Dict[str, CheckType] = None,
    dict_name=None,
) -> dict:
    """
    First, check that all keys in mandatory appear in options. Second, create
    result_options by merging options and default_options, where entries in
    options have precedence. Finally, if constraints is given, this is used to
    check validity of values.

    :param options:
    :param mandatory:
    :param default_options:
    :param constraints:
    :param dict_name:
    :return: result_options
    """
    prefix = "" if dict_name is None else "{}: ".format(dict_name)
    for key in mandatory:
        assert key in options, prefix + "Key '{}' is missing (but is mandatory)".format(
            key
        )
    log_msg = ""
    result_options = {k: v for k, v in options.items() if v is not None}
    for key, value in default_options.items():
        if key not in result_options:
            log_msg += prefix + "Key '{}': Imputing default value {}\n".format(
                key, value
            )
            result_options[key] = value
        # If the argument is a dict, we impute only the missing entries
        if isinstance(value, dict):
            result_dict = result_options[key]
            assert isinstance(
                result_dict, dict
            ), f"Key '{key}': Value must be dictionary, but is {result_dict}"
            for kd, vd in value.items():
                if kd not in result_dict:
                    log_msg += (
                        prefix
                        + "Key '{}' in dict {}: Imputing default value {}\n".format(
                            kd, key, vd
                        )
                    )
                    result_dict[kd] = vd
    if log_msg:
        logger.debug(log_msg.rstrip("\n"))
    # Check constraints
    if constraints:
        for key, value in result_options.items():
            check = constraints.get(key)
            if check:
                check.assert_valid(prefix + "Key '{}'".format(key), value)

    return result_options


def filter_by_key(options: dict, remove_keys: Set[str]) -> dict:
    """
    Filter options by removing entries whose keys are in remove_keys.
    Used to filter kwargs passed to a constructor, before passing it to
    the superclass constructor.

    :param options:
    :param remove_keys:
    :return: Filtered options
    """
    return {k: v for k, v in options.items() if k not in remove_keys}


def assert_no_invalid_options(options: dict, all_keys: Set[str], name: str):
    for k in options:
        assert k in all_keys, "{}: Invalid argument '{}'".format(name, k)

File Path: syne_tune/optimizer/schedulers/searchers/utils/warmstarting.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
    ConfigurationFilter,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    Matern52,
    ProductKernelFunction,
    KernelFunction,
    RangeKernelFunction,
)


def create_hp_ranges_for_warmstarting(**kwargs) -> HyperparameterRanges:
    """
    See :class:`GPFIFOSearcher` for details on transfer_learning_task_attr',
    'transfer_learning_active_task', 'transfer_learning_active_config_space'
    as optional fields in `kwargs`. If given, they determine
    `active_config_space` and `prefix_keys` of `hp_ranges` created here,
    and they also places constraints on 'config_space'.

    This function is not only called in `gp_searcher_factory` to create
    `hp_ranges` for a new :class:`GPFIFOSearcher` object. It is also needed to
    create the `TuningJobState` containing the data to be used in warmstarting.

    """
    task_attr = kwargs.get("transfer_learning_task_attr")
    config_space = kwargs["config_space"]
    prefix_keys = None
    active_config_space = None
    if task_attr is not None:
        from syne_tune.config_space import Categorical

        active_task = kwargs.get("transfer_learning_active_task")
        assert (
            active_task is not None
        ), "transfer_learning_active_task is needed if transfer_learning_task_attr is given"
        hp_range = config_space.get(task_attr)
        assert isinstance(
            hp_range, Categorical
        ), f"config_space[{task_attr}] must be a categorical parameter"
        assert active_task in hp_range.categories, (
            f"'{active_task}' must be value in config_space[{task_attr}] "
            + f"(values: {hp_range.categories})"
        )
        prefix_keys = [task_attr]
        active_config_space = kwargs.get("transfer_learning_active_config_space")
        if active_config_space is None:
            active_config_space = config_space
        # The parameter `task_attr` in `active_config_space` must be restricted
        # to `active_task` as a single value
        task_param = Categorical(categories=[active_task])
        active_config_space = dict(active_config_space, **{task_attr: task_param})
    return make_hyperparameter_ranges(
        config_space, active_config_space=active_config_space, prefix_keys=prefix_keys
    )


def create_filter_observed_data_for_warmstarting(
    **kwargs,
) -> Optional[ConfigurationFilter]:
    """
    See :class:`GPFIFOSearcher` for details on transfer_learning_task_attr',
    'transfer_learning_active_task' as optional fields in `kwargs`.

    """
    task_attr = kwargs.get("transfer_learning_task_attr")
    if task_attr is not None:
        active_task = kwargs.get("transfer_learning_active_task")
        assert (
            active_task is not None
        ), "transfer_learning_active_task is needed if transfer_learning_task_attr is given"

        def filter_observed_data(config: Configuration) -> bool:
            return config[task_attr] == active_task

        return filter_observed_data
    else:
        return None


def create_base_gp_kernel_for_warmstarting(
    hp_ranges: HyperparameterRanges, **kwargs
) -> KernelFunction:
    """
    In the transfer learning case, the base kernel is a product of
    two `Matern52` kernels, the first non-ARD over the categorical
    parameter determining the task, the second ARD over the remaining
    parameters.

    """
    task_attr = kwargs.get("transfer_learning_task_attr")
    assert task_attr is not None
    # Note: This attribute is the first in `hp_ranges`, see
    # `create_hp_ranges_for_warmstarting`
    assert hp_ranges.internal_keys[0] == task_attr  # Sanity check
    _, categ_dim = hp_ranges.encoded_ranges[task_attr]
    full_dim = hp_ranges.ndarray_size
    model = kwargs.get("transfer_learning_model", "matern52_product")
    kernel2 = Matern52(full_dim - categ_dim, ARD=True)
    if model == "matern52_product":
        # Kernel is a product of Matern with single length scale on task_id
        # attribute, and Matern ARD kernel on the rest
        kernel1 = Matern52(categ_dim, ARD=False)
        return ProductKernelFunction(kernel1, kernel2)
    else:
        assert (
            model == "matern52_same"
        ), f"transfer_learning_model = {model} not supported"
        # Kernel is Matern ARD on rest, ignoring the task_id attribute
        return RangeKernelFunction(dimension=full_dim, kernel=kernel2, start=categ_dim)

File Path: syne_tune/optimizer/schedulers/synchronous/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

__all__ = [
    "SynchronousHyperbandScheduler",
    "SynchronousGeometricHyperbandScheduler",
    "DifferentialEvolutionHyperbandScheduler",
    "GeometricDifferentialEvolutionHyperbandScheduler",
]

from syne_tune.optimizer.schedulers.synchronous.hyperband import (
    SynchronousHyperbandScheduler,
)
from syne_tune.optimizer.schedulers.synchronous.dehb import (
    DifferentialEvolutionHyperbandScheduler,
)
from syne_tune.optimizer.schedulers.synchronous.hyperband_impl import (
    SynchronousGeometricHyperbandScheduler,
    GeometricDifferentialEvolutionHyperbandScheduler,
)

File Path: syne_tune/optimizer/schedulers/synchronous/dehb.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List, Tuple
import logging
import numpy as np
from dataclasses import dataclass

from syne_tune.optimizer.schedulers.synchronous.dehb_bracket_manager import (
    DifferentialEvolutionHyperbandBracketManager,
)
from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket import (
    SlotInRung,
)
from syne_tune.optimizer.scheduler import TrialSuggestion, SchedulerDecision
from syne_tune.optimizer.schedulers.fifo import ResourceLevelsScheduler
from syne_tune.backend.trial_status import Trial
from syne_tune.config_space import cast_config_values
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
    Categorical,
    String,
    assert_no_invalid_options,
    Integer,
    Float,
    Boolean,
)
from syne_tune.optimizer.schedulers.random_seeds import RandomSeedGenerator
from syne_tune.optimizer.schedulers.searchers.searcher import BaseSearcher
from syne_tune.optimizer.schedulers.searchers.searcher_factory import searcher_factory
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.common import (
    ExclusionList,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log import (
    DebugLogPrinter,
)

__all__ = ["DifferentialEvolutionHyperbandScheduler"]

logger = logging.getLogger(__name__)


_ARGUMENT_KEYS = {
    "searcher",
    "search_options",
    "metric",
    "mode",
    "points_to_evaluate",
    "random_seed",
    "max_resource_attr",
    "resource_attr",
    "mutation_factor",
    "crossover_probability",
    "support_pause_resume",
}

_DEFAULT_OPTIONS = {
    "searcher": "random_encoded",
    "mode": "min",
    "resource_attr": "epoch",
    "mutation_factor": 0.5,
    "crossover_probability": 0.5,
    "support_pause_resume": True,
}

_CONSTRAINTS = {
    "metric": String(),
    "mode": Categorical(choices=("min", "max")),
    "random_seed": Integer(0, 2**32 - 1),
    "max_resource_attr": String(),
    "resource_attr": String(),
    "mutation_factor": Float(lower=0, upper=1),
    "crossover_probability": Float(lower=0, upper=1),
    "support_pause_resume": Boolean(),
}


@dataclass
class TrialInformation:
    """
    Information the scheduler maintains per trial.
    """

    encoded_config: np.ndarray
    level: int
    metric_val: Optional[float] = None


class ExtendedSlotInRung:
    """
    Extends :class:`SlotInRung` mostly for convenience
    """

    def __init__(self, bracket_id: int, slot_in_rung: SlotInRung):
        self.bracket_id = bracket_id
        self.rung_index = slot_in_rung.rung_index
        self.level = slot_in_rung.level
        self.slot_index = slot_in_rung.slot_index
        self.trial_id = slot_in_rung.trial_id
        self.metric_val = slot_in_rung.metric_val
        self.do_selection = False

    def slot_in_rung(self) -> SlotInRung:
        return SlotInRung(
            rung_index=self.rung_index,
            level=self.level,
            slot_index=self.slot_index,
            trial_id=self.trial_id,
            metric_val=self.metric_val,
        )


class DifferentialEvolutionHyperbandScheduler(ResourceLevelsScheduler):
    """
    Differential Evolution Hyperband, as proposed in

        DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient
        Hyperparameter Optimization
        Noor Awad, Neeratyoy Mallik, Frank Hutter
        IJCAI 30 (2021)
        Pages 2147-2153
        https://arxiv.org/abs/2105.09821

    We implement DEHB as a variant of synchronous Hyperband, which may
    differ slightly from the implementation of the authors.

    Main differences to synchronous Hyperband:

    * In DEHB, trials are not paused and potentially promoted (except in the
        very first bracket). Therefore, checkpointing is not used (except in
        the very first bracket, if `support_pause_resume` is True)
    * Only the initial configurations are drawn at random (or drawn from the
        searcher). Whenever possible, new configurations (in their internal
        encoding) are derived from earlier ones by way of differential evolution

    Parameters
    ----------
    config_space : dict
        Configuration space for trial evaluation function
    rungs_first_bracket : List[Tuple[int, int]]
        Determines rung level systems for each bracket, see
        :class:`DifferentialEvolutionHyperbandBracketManager`
    num_brackets_per_iteration : Optional[int]
        Number of brackets per iteration. The algorithm cycles through
        these brackets in one iteration. If not given, the maximum
        number is used (i.e., `len(rungs_first_bracket)`)
    searcher : str
        Selects searcher. Passed to `searcher_factory`.
        If searcher == "random_encoded", the encoded configs are
        sampled directly, each entry independent from U([0, 1]).
        This distribution has higher entropy than for "random" if
        there are discrete hyperparameters in `config_space`.
    search_options : dict
        Passed to `searcher_factory`
    metric : str
        Name of metric to optimize, key in result's obtained via
        `on_trial_result`
    mode : str
        Mode to use for the metric given, can be 'min' or 'max'
    points_to_evaluate: list[dict] or None
        See :class:`SynchronousHyperbandScheduler`.
        Note that this list is only used for initial configurations. Once
        DEHB starts to combine new configs from earlier ones, the list here
        is ignored, even if it still contains entries.
    random_seed : int
        Master random seed. Generators used in the scheduler or searcher are
        seeded using `RandomSeedGenerator`. If not given, the master random
        seed is drawn at random here.
    max_resource_attr : str
        Key name in config for fixed attribute containing the maximum resource.
        If given, trials need not be stopped, which can run more efficiently.
    resource_attr : str
        Name of resource attribute in result's obtained via `on_trial_result`.
        Note: The type of resource must be int.
    mutation_factor : float, in (0, 1]
        Factor F used in the rand/1 mutation operation of DE
    crossover_probability : float, in (0, 1)
        Probability p used in crossover operation (child entries are chosen
        with probability p)
    support_pause_resume : bool
        If True, `_suggest` supports pause and resume in the first bracket. If
        the objective supports checkpointing, this is made use of.
        Note: The resumed trial still gets assigned a new trial_id, but it
        starts from the earlier checkpoint.
    """

    MAX_RETRIES = 50

    def __init__(
        self,
        config_space: dict,
        rungs_first_bracket: List[Tuple[int, int]],
        num_brackets_per_iteration: Optional[int] = None,
        **kwargs,
    ):
        super().__init__(config_space)
        self._create_internal(rungs_first_bracket, num_brackets_per_iteration, **kwargs)

    def _create_internal(
        self,
        rungs_first_bracket: List[Tuple[int, int]],
        num_brackets_per_iteration: Optional[int] = None,
        **kwargs,
    ):
        # Check values and impute default values
        assert_no_invalid_options(
            kwargs, _ARGUMENT_KEYS, name="DifferentialEvolutionHyperbandScheduler"
        )
        kwargs = check_and_merge_defaults(
            kwargs,
            set(),
            _DEFAULT_OPTIONS,
            _CONSTRAINTS,
            dict_name="scheduler_options",
        )
        self.metric = kwargs.get("metric")
        assert self.metric is not None, (
            "Argument 'metric' is mandatory. Pass the name of the metric "
            + "reported by your training script, which you'd like to "
            + "optimize, and use 'mode' to specify whether it should "
            + "be minimized or maximized"
        )
        self.mode = kwargs["mode"]
        self.max_resource_attr = kwargs.get("max_resource_attr")
        self._resource_attr = kwargs["resource_attr"]
        self.mutation_factor = kwargs["mutation_factor"]
        self.crossover_probability = kwargs["crossover_probability"]
        self._support_pause_resume = kwargs["support_pause_resume"]
        # Generator for random seeds
        random_seed = kwargs.get("random_seed")
        if random_seed is None:
            random_seed = np.random.randint(0, 2**32)
        logger.info(f"Master random_seed = {random_seed}")
        self.random_seed_generator = RandomSeedGenerator(random_seed)
        # Generate searcher
        searcher = kwargs["searcher"]
        assert isinstance(
            searcher, str
        ), f"searcher must be of type string, but has type {type(searcher)}"
        search_options = kwargs.get("search_options")
        if search_options is None:
            search_options = dict()
        else:
            search_options = search_options.copy()
        self._debug_log = None
        if searcher == "random_encoded":
            self.searcher = None
            if search_options.get("debug_log", True):
                self._debug_log = DebugLogPrinter()
        else:
            search_options.update(
                {
                    "config_space": self.config_space.copy(),
                    "metric": self.metric,
                    "points_to_evaluate": kwargs.get("points_to_evaluate"),
                    "scheduler_mode": kwargs["mode"],
                    "random_seed_generator": self.random_seed_generator,
                    "resource_attr": self._resource_attr,
                    "scheduler": "hyperband_synchronous",
                }
            )
            if searcher == "bayesopt":
                # We need `max_epochs` in this case
                max_epochs = self._infer_max_resource_level(
                    max_resource_level=None, max_resource_attr=self.max_resource_attr
                )
                assert max_epochs is not None, (
                    "If searcher='bayesopt', need to know the maximum resource "
                    + "level. Please provide max_resource_attr argument."
                )
                search_options["max_epochs"] = max_epochs
            self.searcher: BaseSearcher = searcher_factory(searcher, **search_options)
            self._debug_log = self.searcher.debug_log
        # Bracket manager
        self.bracket_manager = DifferentialEvolutionHyperbandBracketManager(
            rungs_first_bracket=rungs_first_bracket,
            mode=self.mode,
            num_brackets_per_iteration=num_brackets_per_iteration,
        )
        # Needed to convert encoded configs to configs
        self._hp_ranges = make_hyperparameter_ranges(self.config_space)
        self._excl_list = ExclusionList.empty_list(self._hp_ranges)
        # PRNG for mutation and crossover random draws
        self.random_state = np.random.RandomState(self.random_seed_generator())
        # How often is selection skipped because target still pending?
        self.num_selection_skipped = 0
        # Maps trial_id to ext_slot, as returned by `bracket_manager.next_job`,
        # and required by `bracket_manager.on_result`. Entries are removed once
        # passed to `on_result`. Here, ext_slot is of type `ExtendedSlotInRung`.
        self._trial_to_pending_slot = dict()
        # Maps trial_id to trial information (in particular, the encoded
        # config)
        self._trial_info = dict()
        # Maps level to list of trial_ids of all completed jobs (so that
        # metric values are available). This global "parent pool" is used
        # during mutations if the normal parent pool is too small
        self._global_parent_pool = {level: [] for _, level in rungs_first_bracket}

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        if self._excl_list.config_space_exhausted():
            logger.warning("All configurations in config space have been suggested")
            return None
        if self._debug_log is not None:
            if trial_id == 0:
                # This is printed at the start of the experiment. Cannot do this
                # at construction, because with `RemoteLauncher` this does not end
                # up in the right log
                parts = ["Rung systems for each bracket:"] + [
                    f"Bracket {bracket}: {rungs}"
                    for bracket, rungs in enumerate(self.bracket_manager.bracket_rungs)
                ]
                logger.info("\n".join(parts))
            if self.searcher is None:
                self._debug_log.start_get_config("random", trial_id=trial_id)
        # Ask bracket manager for job
        bracket_id, slot_in_rung = self.bracket_manager.next_job()
        ext_slot = ExtendedSlotInRung(bracket_id, slot_in_rung)
        if self._debug_log is not None:
            logger.info(
                f"trial_id {trial_id} for bracket {bracket_id}, level "
                f"{slot_in_rung.level}, rung index "
                + f"{slot_in_rung.rung_index}, slot {slot_in_rung.slot_index}"
            )
        is_base_rung = ext_slot.rung_index == 0  # Slot in base rung?
        encoded_config = None
        promoted_from_trial_id = None
        for next_config_iter in range(self.MAX_RETRIES):
            if next_config_iter < self.MAX_RETRIES / 2:
                draw_from_searcher = False
                if is_base_rung:
                    if bracket_id == 0:
                        draw_from_searcher = True
                    else:
                        parent_trial_id = (
                            self.bracket_manager.trial_id_from_parent_slot(
                                bracket_id=bracket_id,
                                level=ext_slot.level,
                                slot_index=ext_slot.slot_index,
                            )
                        )
                        draw_from_searcher = parent_trial_id is None
                if draw_from_searcher:
                    # At the very start, we draw configs from the searcher
                    encoded_config = self._encoded_config_from_searcher(trial_id)
                elif bracket_id == 0:
                    # First bracket, but not base rung. Promotion as in synchronous
                    # HB, but we assign new trial_id
                    (
                        encoded_config,
                        promoted_from_trial_id,
                    ) = self._encoded_config_by_promotion(ext_slot)
                else:
                    # Here, we can do DE (mutation, crossover)
                    encoded_config = self._extended_config_by_mutation_crossover(
                        ext_slot
                    )
            else:
                # Draw encoded config at random
                restore_searcher = self.searcher
                self.searcher = None
                encoded_config = self._encoded_config_from_searcher(trial_id)
                self.searcher = restore_searcher
            if encoded_config is None:
                break  # Searcher failed to return config
            if promoted_from_trial_id is not None:
                break  # Promotion is a config suggested before, that is OK
            _config = self._hp_ranges.from_ndarray(encoded_config)
            if not self._excl_list.contains(_config):
                break
            else:
                encoded_config = None
        if encoded_config is not None:
            if self._support_pause_resume and promoted_from_trial_id is not None:
                suggestion = self._promote_trial_and_make_suggestion(
                    trial_id=promoted_from_trial_id, ext_slot=ext_slot
                )
                if self._debug_log is not None:
                    logger.info(
                        f"trial_id {promoted_from_trial_id} resumes (milestone = "
                        f"{ext_slot.level})"
                    )
            else:
                suggestion = self._register_new_config_and_make_suggestion(
                    trial_id=trial_id, ext_slot=ext_slot, encoded_config=encoded_config
                )
                if self._debug_log is not None:
                    logger.info(
                        f"trial_id {trial_id} starts (milestone = {ext_slot.level})"
                    )
        else:
            # Searcher failed to return a config for a new trial_id. We report
            # the corresponding job as failed, so that in case the experiment
            # is continued, the bracket is not blocked with a slot which remains
            # pending forever
            logger.warning(
                "Searcher failed to suggest a configuration for new trial "
                f"{trial_id}. The corresponding slot is marked as failed."
            )
            self._report_as_failed(ext_slot)
            suggestion = None
        return suggestion

    def _encoded_config_from_searcher(self, trial_id: int) -> np.ndarray:
        if self.searcher is not None:
            if self._debug_log is not None:
                logger.info("Draw new config from searcher")
            config = self.searcher.get_config(trial_id=str(trial_id))
            if config is not None:
                encoded_config = self._hp_ranges.to_ndarray(config)
            else:
                encoded_config = None
        else:
            if self._debug_log is not None:
                logger.info("Draw new encoded config uniformly at random")
            encoded_config = self.random_state.uniform(
                low=0, high=1, size=self._hp_ranges.ndarray_size
            )
        return encoded_config

    def _encoded_config_by_promotion(
        self, ext_slot: ExtendedSlotInRung
    ) -> (np.ndarray, int):
        parent_trial_id = self.bracket_manager.top_of_previous_rung(
            bracket_id=ext_slot.bracket_id, pos=ext_slot.slot_index
        )
        trial_info = self._trial_info[parent_trial_id]
        assert trial_info.metric_val is not None  # Sanity check
        if self._debug_log is not None:
            logger.info(
                f"Promote config from trial_id {parent_trial_id}"
                f", level {trial_info.level}"
            )
        return trial_info.encoded_config, parent_trial_id

    def _extended_config_by_mutation_crossover(
        self, ext_slot: ExtendedSlotInRung
    ) -> np.ndarray:
        ext_slot.do_selection = True
        mutant = self._mutation(ext_slot)
        target_trial_id = self._get_target_trial_id(ext_slot)
        if self._debug_log is not None:
            logger.info(f"Target (cross-over): trial_id {target_trial_id}")
        return self._crossover(
            mutant=mutant,
            target=self._trial_info[target_trial_id].encoded_config,
        )

    def _draw_random_trial_id(self) -> int:
        return self.random_state.choice(list(self._trial_info.keys()))

    def _get_target_trial_id(self, ext_slot: ExtendedSlotInRung) -> Optional[int]:
        """
        The target trial_id is the trial_id in the parent slot. If this is None,
        a random existing trial_id is returned.
        """
        target_trial_id = self.bracket_manager.trial_id_from_parent_slot(
            bracket_id=ext_slot.bracket_id,
            level=ext_slot.level,
            slot_index=ext_slot.slot_index,
        )
        if target_trial_id is None:
            target_trial_id = self._draw_random_trial_id()
        return target_trial_id

    def _register_new_config_and_make_suggestion(
        self, trial_id: int, ext_slot: ExtendedSlotInRung, encoded_config: np.ndarray
    ) -> TrialSuggestion:
        # Register as pending
        self._trial_to_pending_slot[trial_id] = ext_slot
        # Register new trial_id
        self._trial_info[trial_id] = TrialInformation(
            encoded_config=encoded_config,
            level=ext_slot.level,
        )
        # Return new config
        config = self._hp_ranges.from_ndarray(encoded_config)
        self._excl_list.add(config)  # Should not be suggested again
        if self._debug_log is not None and self.searcher is None:
            self._debug_log.set_final_config(config)
            self._debug_log.write_block()
        config = cast_config_values(config, self.config_space)
        if self.max_resource_attr is not None:
            config = dict(config, **{self.max_resource_attr: ext_slot.level})
        return TrialSuggestion.start_suggestion(config=config)

    def _promote_trial_and_make_suggestion(
        self, trial_id: int, ext_slot: ExtendedSlotInRung
    ) -> TrialSuggestion:
        # Register as pending
        self._trial_to_pending_slot[trial_id] = ext_slot
        # Modify entry to new milestone level
        trial_info = self._trial_info.get(trial_id)
        assert (
            trial_info is not None
        ), f"Cannot promote trial_id {trial_id}, which is not registered"
        trial_info.level = ext_slot.level
        trial_info.metric_val = None
        if self.max_resource_attr is None:
            config = None
        else:
            config = cast_config_values(
                self._hp_ranges.from_ndarray(trial_info.encoded_config),
                self.config_space,
            )
            config = dict(config, **{self.max_resource_attr: ext_slot.level})
        return TrialSuggestion.resume_suggestion(trial_id=trial_id, config=config)

    def _report_as_failed(self, ext_slot: ExtendedSlotInRung):
        result_failed = ext_slot.slot_in_rung()
        result_failed.metric_val = np.NAN
        self.bracket_manager.on_result((ext_slot.bracket_id, result_failed))

    def on_trial_result(self, trial: Trial, result: dict) -> str:
        trial_id = trial.trial_id
        if trial_id in self._trial_to_pending_slot:
            ext_slot = self._trial_to_pending_slot[trial_id]
            milestone = ext_slot.level
            metric_val, resource = self._extract_from_result(trial_id, result)
            trial_decision = SchedulerDecision.CONTINUE
            if resource >= milestone:
                assert resource == milestone, (
                    f"Trial trial_id {trial_id}: Obtained result for "
                    + f"resource = {resource}, but not for {milestone}. "
                    + "Training script must not skip rung levels!"
                )
                if self._debug_log is not None:
                    logger.info(
                        f"Trial trial_id {trial_id}: Reached milestone "
                        f"{milestone} with metric {metric_val:.3f}"
                    )
                self._record_new_metric_value(trial_id, milestone, metric_val)
                # Selection
                winner_trial_id = self._selection(trial_id, ext_slot, metric_val)
                # Return updated slot information to bracket
                self._return_slot_result_to_bracket(winner_trial_id, ext_slot)
                if self._support_pause_resume and ext_slot.bracket_id == 0:
                    trial_decision = SchedulerDecision.PAUSE
                else:
                    trial_decision = SchedulerDecision.STOP
                if self.searcher is not None:
                    config = self._hp_ranges.from_ndarray(
                        self._trial_info[trial_id].encoded_config
                    )
                    self.searcher.on_trial_result(
                        trial_id=str(trial_id),
                        config=config,
                        result=result,
                        update=True,
                    )
        else:
            trial_decision = SchedulerDecision.STOP
            logger.warning(
                f"Received result for trial_id {trial_id}, which is not "
                f"pending. This result is not used:\n{result}"
            )

        return trial_decision

    def _extract_from_result(self, trial_id: int, result: dict) -> (float, int):
        metric_vals = []
        for name in (self.metric, self._resource_attr):
            assert name in result, (
                f"Result for trial_id {trial_id} does not contain " + f"'{name}' field"
            )
            metric_vals.append(result[name])
        return float(metric_vals[0]), int(metric_vals[1])

    def _record_new_metric_value(
        self, trial_id: int, milestone: int, metric_val: float
    ):
        # Record metric value
        trial_info = self._trial_info[trial_id]
        assert trial_info.level == milestone  # Sanity check
        trial_info.metric_val = metric_val
        # Update global parent pool
        self._global_parent_pool[milestone].append(trial_id)

    def _return_slot_result_to_bracket(
        self, winner_trial_id: int, ext_slot: ExtendedSlotInRung
    ):
        ext_slot.trial_id = winner_trial_id
        ext_slot.metric_val = self._trial_info[winner_trial_id].metric_val
        slot_in_rung = ext_slot.slot_in_rung()
        self.bracket_manager.on_result((ext_slot.bracket_id, slot_in_rung))

    def _mutation(self, ext_slot: ExtendedSlotInRung) -> np.ndarray:
        bracket_id = ext_slot.bracket_id
        level = ext_slot.level
        assert bracket_id > 0
        # Determine the parent pool, from which the 3 parents are sampled. If
        # this is too small (< 3), we also use the global parent pool for this
        # level (i.e., all completed trials), or even global parent pools at
        # all levels.
        orig_pool_size = self.bracket_manager.size_of_current_rung(bracket_id)
        pool_size = orig_pool_size
        global_pool = None
        if orig_pool_size < 3:
            global_pool = self._global_parent_pool[level]
            pool_size += len(global_pool)
            # If this is still too small, we add parent pools of other levels
            for _, other_level in reversed(self.bracket_manager.bracket_rungs[0]):
                if pool_size >= 3:
                    break
                if other_level != level:
                    extra_pool = self._global_parent_pool[other_level]
                    global_pool = global_pool + extra_pool
                    pool_size += len(extra_pool)
            # TODO: If this ever happens, have to do something else here. For
            # example, could pick trial_id's which are still pending
            assert pool_size >= 3, f"Cannot compose parent pool of size >= 3"
        # Sample 3 entries at random from parent pool
        positions = list(self.random_state.choice(pool_size, 3, replace=False))
        is_base_rung = ext_slot.rung_index == 0
        msg = None
        if self._debug_log is not None:
            from_str = "parent rung" if is_base_rung else "top of rung below"
            msg = f"Mutation: Sample parents from {from_str}: pool_size = {pool_size}"
            if orig_pool_size != pool_size:
                msg += f", orig_pool_size = {orig_pool_size}"
        parent_trial_ids = []
        for pos in positions:
            if pos >= orig_pool_size:
                trial_id = global_pool[pos - orig_pool_size]
            elif is_base_rung:
                trial_id = self.bracket_manager.trial_id_from_parent_slot(
                    bracket_id=bracket_id, level=level, slot_index=pos
                )
                if trial_id is None:
                    trial_id = self._draw_random_trial_id()
            else:
                trial_id = self.bracket_manager.top_of_previous_rung(
                    bracket_id=bracket_id, pos=pos
                )
            parent_trial_ids.append(trial_id)
        if self._debug_log is not None:
            msg += "\n" + str(parent_trial_ids)
            logger.info(msg)
        return self._de_mutation(parent_trial_ids)

    def _de_mutation(self, parent_trial_ids: List[int]) -> np.ndarray:
        """
        Corresponds to rand/1 mutation strategy of DE.
        """
        ec = [
            self._trial_info[trial_id].encoded_config for trial_id in parent_trial_ids
        ]
        mutant = (ec[1] - ec[2]) * self.mutation_factor + ec[0]
        # Entries which violate boundaries are resampled at random
        violations = np.where((mutant > 1) | (mutant < 0))[0]
        if len(violations) > 0:
            mutant[violations] = self.random_state.uniform(
                low=0.0, high=1.0, size=len(violations)
            )
        return mutant

    def _crossover(self, mutant: np.ndarray, target: np.ndarray) -> np.ndarray:
        # For any HP whose encoding has dimension > 1 (e.g., categorical), we
        # make sure not to cross-over inside the encoding
        num_hps = len(self._hp_ranges)
        hp_mask = self.random_state.rand(num_hps) < self.crossover_probability
        if not np.any(hp_mask):
            # Offspring must be different from target
            hp_mask[self.random_state.randint(0, num_hps)] = True
        cross_points = np.empty(self._hp_ranges.ndarray_size, dtype=bool)
        for (start, end), val in zip(self._hp_ranges.encoded_ranges.values(), hp_mask):
            cross_points[start:end] = val
        offspring = np.where(cross_points, mutant, target)
        return offspring

    def _selection(
        self, trial_id: int, ext_slot: ExtendedSlotInRung, metric_val: float
    ) -> int:
        winner_trial_id = trial_id
        if ext_slot.do_selection:
            # Note: This can have changed since crossover
            target_trial_id = self._get_target_trial_id(ext_slot)
            if self._debug_log is not None:
                logger.info(f"Target (selection): trial_id {target_trial_id}")
            target_metric_val = self._trial_info[target_trial_id].metric_val
            if target_metric_val is not None:
                # Selection
                metric_sign = -1 if self.mode == "max" else 1
                if metric_sign * (metric_val - target_metric_val) >= 0:
                    winner_trial_id = target_trial_id
                if self._debug_log is not None:
                    logger.info(
                        f"Target metric = {target_metric_val:.3f}: "
                        f"winner_trial_id = {winner_trial_id}"
                    )
            else:
                # target has no metric value yet. This should not happen often
                logger.warning(
                    "Could not do selection because target metric "
                    f"value still pending (bracket_id = {ext_slot.bracket_id}"
                    f", trial_id = {trial_id}, "
                    f"target_trial_id = {target_trial_id})"
                )
                self.num_selection_skipped += 1
        return winner_trial_id

    def on_trial_error(self, trial: Trial):
        """
        Given the `trial` is currently pending, we send a result at its
        milestone for metric value NaN. Such trials are ranked after all others
        and will most likely not be promoted.

        """
        trial_id = trial.trial_id
        if self.searcher is not None:
            self.searcher.evaluation_failed(str(trial_id))
        if trial_id in self._trial_to_pending_slot:
            ext_slot = self._trial_to_pending_slot[trial_id]
            self._report_as_failed(ext_slot)
        else:
            logger.warning(
                f"Trial trial_id {trial_id} not registered at pending: "
                "on_trial_error call is ignored"
            )

    def metric_names(self) -> List[str]:
        return [self.metric]

    def metric_mode(self) -> str:
        return self.mode

File Path: syne_tune/optimizer/schedulers/synchronous/dehb_bracket.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List, Tuple

from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket import (
    SynchronousBracket,
    get_top_list,
)


class DifferentialEvolutionHyperbandBracket(SynchronousBracket):
    """
    Represents a bracket in Differential Evolution Hyperband (DEHB).

    There are a number of differences to brackets in standard synchronous
    Hyperband (:class:`SynchronousHyperbandBracket`):

    * `on_result`: `result.trial_id` overwrites `trial_id` in rung even if
        latter is not None.
    * Promotions are not triggered automatically when a rung is complete
    * Some additional methods
    """

    def __init__(
        self,
        rungs: List[Tuple[int, int]],
        mode: str,
    ):
        self.assert_check_rungs(rungs)
        super().__init__(mode)
        # Represents rung levels by (rung, level), where rung is a list of
        # (trial_id, metric_val) tuples for all rungs
        self._rungs = [([(None, None)] * size, level) for size, level in rungs]

    @property
    def num_rungs(self) -> int:
        return len(self._rungs)

    def _current_rung_and_level(
        self,
    ) -> (List[Tuple[Optional[int], Optional[float]]], int):
        return self._rungs[self.current_rung]

    def size_of_current_rung(self) -> int:
        return len(self._current_rung_and_level()[0])

    def trial_id_for_slot(self, rung_index: int, slot_index: int) -> Optional[int]:
        rung, _ = self._rungs[rung_index]
        return rung[slot_index][0]

    def top_list_for_previous_rung(self) -> List[int]:
        """
        Returns list of trial_ids corresponding to best scoring entries
        in rung below the currently active one (which must not be the base
        rung). The list is of the size of the current rung.
        """
        assert self.current_rung > 0, "Current rung is base rung"
        previous_rung, _ = self._rungs[self.current_rung - 1]
        return get_top_list(
            rung=previous_rung, new_len=self.size_of_current_rung(), mode=self._mode
        )

    def _promote_trials_at_rung_complete(self):
        pass

File Path: syne_tune/optimizer/schedulers/synchronous/dehb_bracket_manager.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Tuple, Optional

from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket_manager import (
    SynchronousHyperbandBracketManager,
)
from syne_tune.optimizer.schedulers.synchronous.dehb_bracket import (
    DifferentialEvolutionHyperbandBracket,
)


class DifferentialEvolutionHyperbandBracketManager(SynchronousHyperbandBracketManager):
    """
    Special case of :class:`SynchronousHyperbandBracketManager` to manage DEHB
    brackets (type :class:`DifferentialEvolutionHyperbandBracket`).

    In DEHB, the list of brackets is determined by the first one and the number
    of brackets. Also, later brackets have less total budget, because the size
    of a rung is determined by its level, independent of the bracket. This is
    different to what is done in synchronous Hyperband, where the rungs of
    later brackets have larger sizes, so the total budget of each bracket is
    the same.

    We also need additional methods to access trial_id's in specific rungs, as
    well as entries of the top lists for completed rungs. This is because DEHB
    controls the creation of new configurations at higher rungs, while
    synchronous Hyperband relies on automatic promotion from lower rungs.
    """

    def __init__(
        self,
        rungs_first_bracket: List[Tuple[int, int]],
        mode: str,
        num_brackets_per_iteration: Optional[int] = None,
    ):
        max_num_offsets = len(rungs_first_bracket)
        assert max_num_offsets > 0
        if num_brackets_per_iteration is None:
            num_brackets_per_iteration = max_num_offsets
        else:
            assert 1 <= num_brackets_per_iteration <= max_num_offsets, (
                f"num_brackets_per_iteration = {num_brackets_per_iteration}"
                + f", must be in [1, {max_num_offsets}]"
            )
        # All brackets are determined by the first one in DEHB
        bracket_rungs = [
            rungs_first_bracket[offset:] for offset in range(num_brackets_per_iteration)
        ]
        super().__init__(bracket_rungs, mode)
        # Maps (bracket_id, rung_index) to top list of previous rung, as
        # returned by
        # `DifferentialEvolutionHyperbandBracket.top_list_for_previous_rung`
        # when the current rung is `rung_index` in that bracket. We cache
        # these, so we don't have to repeat sorting many times
        self._top_list_of_previous_rung_cache = dict()
        # Maps (offset, level) to (bracket_delta, rung_index) in order to
        # determine the parent rung of a rung in a bracket with offset and
        # level (the parent rung has the same level).
        self._parent_rung = self._set_parent_rung()

    def _set_parent_rung(self):
        parent_rung = dict()
        for offset, rungs in enumerate(self._bracket_rungs):
            if offset > 0:
                # For bracket with offset > 0, the parent rung is in the
                # bracket just to the left
                bracket_delta = 1
                for rung_index, (_, level) in enumerate(rungs):
                    parent_rung[(offset, level)] = (
                        bracket_delta,
                        rung_index + 1,
                    )
            else:
                # For bracket with offset 0, the parent rung is the base
                # rung in a bracket to the left
                for rung_index, (_, level) in enumerate(rungs):
                    parent_rung[(offset, level)] = (
                        self.num_bracket_offsets - rung_index,
                        0,
                    )
        return parent_rung

    def _create_new_bracket(self) -> int:
        # Sanity check:
        assert len(self._brackets) == len(self._bracket_id_to_offset)
        bracket_id = self._next_bracket_id
        offset = bracket_id % self.num_bracket_offsets
        self._bracket_id_to_offset.append(offset)
        rungs = self._bracket_rungs[offset]
        self._brackets.append(
            DifferentialEvolutionHyperbandBracket(rungs=rungs, mode=self.mode)
        )
        return bracket_id

    def size_of_current_rung(self, bracket_id: int) -> int:
        return self._brackets[bracket_id].size_of_current_rung()

    def trial_id_from_parent_slot(
        self, bracket_id: int, level: int, slot_index: int
    ) -> Optional[int]:
        """
        The parent slot has the same slot index and rung level in the
        largest bracket `< bracket_id` with a trial_id not None. If no
        such slot exists, None is returned.
        For a cross-over or selection operation, the target is chosen
        from the parent slot.
        """
        trial_id = None
        while trial_id is None and bracket_id > 0:
            bracket_delta, rung_index = self._parent_rung[
                (self._bracket_id_to_offset[bracket_id], level)
            ]
            bracket_id = bracket_id - bracket_delta
            trial_id = self._brackets[bracket_id].trial_id_for_slot(
                rung_index=rung_index, slot_index=slot_index
            )
        return trial_id

    def top_of_previous_rung(self, bracket_id: int, pos: int) -> int:
        """
        For the current rung in bracket `bracket_id`, consider the slots of
        the previous rung (below) in sorted order. We return the trial_id of
        position `pos` (so for `pos=0`, the best entry).
        """
        bracket = self._brackets[bracket_id]
        rung_index = bracket.current_rung
        key = (bracket_id, rung_index)
        top_list = self._top_list_of_previous_rung_cache.get(key)
        if top_list is None:
            top_list = bracket.top_list_for_previous_rung()
            self._top_list_of_previous_rung_cache[key] = top_list
        return top_list[pos]

File Path: syne_tune/optimizer/schedulers/synchronous/hyperband.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, Dict, List
import logging
import numpy as np

from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket_manager import (
    SynchronousHyperbandBracketManager,
)
from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket import SlotInRung
from syne_tune.optimizer.schedulers.synchronous.hyperband_rung_system import (
    RungSystemsPerBracket,
)
from syne_tune.optimizer.scheduler import TrialSuggestion, SchedulerDecision
from syne_tune.optimizer.schedulers.fifo import ResourceLevelsScheduler
from syne_tune.backend.trial_status import Trial
from syne_tune.config_space import cast_config_values
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
    Categorical,
    String,
    assert_no_invalid_options,
    Integer,
)
from syne_tune.optimizer.schedulers.random_seeds import RandomSeedGenerator
from syne_tune.optimizer.schedulers.searchers.searcher import BaseSearcher
from syne_tune.optimizer.schedulers.searchers.searcher_factory import searcher_factory

__all__ = ["SynchronousHyperbandScheduler"]

logger = logging.getLogger(__name__)


_ARGUMENT_KEYS = {
    "searcher",
    "search_options",
    "metric",
    "mode",
    "points_to_evaluate",
    "random_seed",
    "max_resource_attr",
    "resource_attr",
    "searcher_data",
}

_DEFAULT_OPTIONS = {
    "searcher": "random",
    "mode": "min",
    "resource_attr": "epoch",
    "searcher_data": "rungs",
}

_CONSTRAINTS = {
    "metric": String(),
    "mode": Categorical(choices=("min", "max")),
    "random_seed": Integer(0, 2**32 - 1),
    "max_resource_attr": String(),
    "resource_attr": String(),
    "searcher_data": Categorical(("rungs", "all")),
}


class SynchronousHyperbandScheduler(ResourceLevelsScheduler):
    """
    Synchronous Hyperband. This is still scheduling jobs asynchronously, but
    decision-making is synchronized, in that trials are only promoted to the
    next milestone once the rung they are currently paused at, is completely
    occupied.

    Our implementation never delays scheduling of a job. If the currently
    active bracket does not accept jobs, we assign the job to a later bracket.
    This means that at any point in time, several brackets can be active, but
    jobs are preferentially assigned to the first one (the "primary" active
    bracket).

    Parameters
    ----------
    config_space : dict
        Configuration space for trial evaluation function
    bracket_rungs : RungSystemsPerBracket
        Determines rung level systems for each bracket, see
        :class:`SynchronousHyperbandBracketManager`
    searcher : str
        Selects searcher. Passed to `searcher_factory`.
        NOTE: Different to :class:`FIFOScheduler`, we do not accept a
        `BaseSearcher` object here.
    search_options : dict
        Passed to `searcher_factory`
    metric : str
        Name of metric to optimize, key in result's obtained via
        `on_trial_result`
    mode : str
        Mode to use for the metric given, can be 'min' or 'max'
    points_to_evaluate: list[dict] or None
        List of configurations to be evaluated initially (in that order).
        Each config in the list can be partially specified, or even be an
        empty dict. For each hyperparameter not specified, the default value
        is determined using a midpoint heuristic.
        If None (default), this is mapped to [dict()], a single default config
        determined by the midpoint heuristic. If [] (empty list), no initial
        configurations are specified.
        Note: If `searcher` is BaseSearcher, points_to_evaluate must be set
        there.
    random_seed : int
        Master random seed. Generators used in the scheduler or searcher are
        seeded using `RandomSeedGenerator`. If not given, the master random
        seed is drawn at random here.
    max_resource_attr : str
        Key name in config for fixed attribute containing the maximum resource.
        If given, trials need not be stopped, which can run more efficiently.
    resource_attr : str
        Name of resource attribute in result's obtained via `on_trial_result`.
        Note: The type of resource must be int.
    searcher_data : str
        Relevant only if a model-based searcher is used.
        Example: For NN tuning and `resource_attr == epoch', we receive a
        result for each epoch, but not all epoch values are also rung levels.
        searcher_data determines which of these results are passed to the
        searcher. As a rule, the more data the searcher receives, the better
        its fit, but also the more expensive get_config may become. Choices:
        - 'rungs' (default): Only results at rung levels. Cheapest
        - 'all': All results. Most expensive
        Note: For a Gaussian additive learning curve surrogate model, this
        has to be set to 'all'.

    """

    def __init__(
        self, config_space: Dict, bracket_rungs: RungSystemsPerBracket, **kwargs
    ):
        super().__init__(config_space)
        self._create_internal(bracket_rungs, **kwargs)

    def _create_internal(self, bracket_rungs: RungSystemsPerBracket, **kwargs):
        # Check values and impute default values
        assert_no_invalid_options(
            kwargs, _ARGUMENT_KEYS, name="SynchronousHyperbandScheduler"
        )
        kwargs = check_and_merge_defaults(
            kwargs, set(), _DEFAULT_OPTIONS, _CONSTRAINTS, dict_name="scheduler_options"
        )
        self.metric = kwargs.get("metric")
        assert self.metric is not None, (
            "Argument 'metric' is mandatory. Pass the name of the metric "
            + "reported by your training script, which you'd like to "
            + "optimize, and use 'mode' to specify whether it should "
            + "be minimized or maximized"
        )
        self.mode = kwargs["mode"]
        self.max_resource_attr = kwargs.get("max_resource_attr")
        self._resource_attr = kwargs["resource_attr"]
        # Generator for random seeds
        random_seed = kwargs.get("random_seed")
        if random_seed is None:
            random_seed = np.random.randint(0, 2**32)
        logger.info(f"Master random_seed = {random_seed}")
        self.random_seed_generator = RandomSeedGenerator(random_seed)
        # Generate searcher
        searcher = kwargs["searcher"]
        assert isinstance(
            searcher, str
        ), f"searcher must be of type string, but has type {type(searcher)}"
        search_options = kwargs.get("search_options")
        if search_options is None:
            search_options = dict()
        else:
            search_options = search_options.copy()
        search_options.update(
            {
                "config_space": self.config_space.copy(),
                "metric": self.metric,
                "points_to_evaluate": kwargs.get("points_to_evaluate"),
                "scheduler_mode": kwargs["mode"],
                "random_seed_generator": self.random_seed_generator,
                "resource_attr": self._resource_attr,
                "scheduler": "hyperband_synchronous",
            }
        )
        if searcher == "bayesopt":
            # We need `max_epochs` in this case
            max_epochs = self._infer_max_resource_level(
                max_resource_level=None, max_resource_attr=self.max_resource_attr
            )
            assert max_epochs is not None, (
                "If searcher='bayesopt', need to know the maximum resource "
                + "level. Please provide max_resource_attr argument."
            )
            search_options["max_epochs"] = max_epochs
        self.searcher: BaseSearcher = searcher_factory(searcher, **search_options)
        # Bracket manager
        self.bracket_manager = SynchronousHyperbandBracketManager(
            bracket_rungs, mode=self.mode
        )
        self.searcher_data = kwargs["searcher_data"]
        # Maps trial_id to tuples (bracket_id, slot_in_rung), as returned
        # by `bracket_manager.next_job`, and required by
        # `bracket_manager.on_result`. Entries are removed once passed to
        # `on_result`. Note that a trial_id can be associated with different
        # job descriptions in its lifetime
        self._trial_to_pending_slot = dict()
        # Maps trial_id (active) to config
        self._trial_to_config = dict()

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        do_debug_log = self.searcher.debug_log is not None
        if do_debug_log and trial_id == 0:
            # This is printed at the start of the experiment. Cannot do this
            # at construction, because with `RemoteLauncher` this does not end
            # up in the right log
            parts = ["Rung systems for each bracket:"] + [
                f"Bracket {bracket}: {rungs}"
                for bracket, rungs in enumerate(self.bracket_manager.bracket_rungs)
            ]
            logger.info("\n".join(parts))
        # Ask bracket manager for job
        bracket_id, slot_in_rung = self.bracket_manager.next_job()
        suggestion = None
        if slot_in_rung.trial_id is not None:
            # Paused trial to be resumed (`trial_id` passed in is ignored)
            trial_id = slot_in_rung.trial_id
            _config = self._trial_to_config[trial_id]
            if self.max_resource_attr is not None:
                config = dict(_config, **{self.max_resource_attr: slot_in_rung.level})
            else:
                config = _config
            suggestion = TrialSuggestion.resume_suggestion(
                trial_id=trial_id, config=config
            )
            if do_debug_log:
                logger.info(f"trial_id {trial_id} promoted to {slot_in_rung.level}")
        else:
            # New trial to be started (id is `trial_id` passed in)
            config = self.searcher.get_config(trial_id=str(trial_id))
            if config is not None:
                config = cast_config_values(config, self.config_space)
                if self.max_resource_attr is not None:
                    config[self.max_resource_attr] = slot_in_rung.level
                self._trial_to_config[trial_id] = config
                suggestion = TrialSuggestion.start_suggestion(config=config)
                # Assign trial id to job descriptor
                slot_in_rung.trial_id = trial_id
                if do_debug_log:
                    logger.info(
                        f"trial_id {trial_id} starts (milestone = "
                        f"{slot_in_rung.level})"
                    )
        if suggestion is not None:
            assert trial_id not in self._trial_to_pending_slot, (
                f"Trial for trial_id = {trial_id} is already registered as "
                + "pending, cannot resume or start it"
            )
            self._trial_to_pending_slot[trial_id] = (bracket_id, slot_in_rung)
        else:
            # Searcher failed to return a config for a new trial_id. We report
            # the corresponding job as failed, so that in case the experiment
            # is continued, the bracket is not blocked with a slot which remains
            # pending forever
            logger.warning(
                "Searcher failed to suggest a configuration for new trial "
                f"{trial_id}. The corresponding rung slot is marked as failed."
            )
            self._report_as_failed(bracket_id, slot_in_rung)
        return suggestion

    def _report_as_failed(self, bracket_id: int, slot_in_rung: SlotInRung):
        result_failed = SlotInRung(
            rung_index=slot_in_rung.rung_index,
            level=slot_in_rung.level,
            slot_index=slot_in_rung.slot_index,
            trial_id=slot_in_rung.trial_id,
            metric_val=np.NAN,
        )
        self.bracket_manager.on_result((bracket_id, result_failed))

    def on_trial_result(self, trial: Trial, result: Dict) -> str:
        trial_id = trial.trial_id
        if trial_id in self._trial_to_pending_slot:
            bracket_id, slot_in_rung = self._trial_to_pending_slot[trial_id]
            assert slot_in_rung.trial_id == trial_id  # Sanity check
            assert self.metric in result, (
                f"Result for trial_id {trial_id} does not contain "
                + f"'{self.metric}' field"
            )
            metric_val = float(result[self.metric])
            assert self._resource_attr in result, (
                f"Result for trial_id {trial_id} does not contain "
                + f"'{self._resource_attr}' field"
            )
            resource = int(result[self._resource_attr])
            milestone = slot_in_rung.level
            prev_level = self.bracket_manager.level_to_prev_level(bracket_id, milestone)
            trial_decision = SchedulerDecision.CONTINUE
            if resource >= milestone:
                assert resource == milestone, (
                    f"Trial trial_id {trial_id}: Obtained result for "
                    + f"resource = {resource}, but not for {milestone}. "
                    + "Training script must not skip rung levels!"
                )
                # Reached rung level: Pass result to bracket manager
                slot_in_rung.metric_val = metric_val
                self.bracket_manager.on_result((bracket_id, slot_in_rung))
                # Remove it from pending slots
                del self._trial_to_pending_slot[trial_id]
                # Trial should be paused
                trial_decision = SchedulerDecision.PAUSE
            if resource > prev_level:
                # If the training script does not implement checkpointing, each
                # trial starts from scratch. In this case, the condition
                # `resource > prev_level` ensures that the searcher does not
                # receive multiple reports for the same resource
                update = self.searcher_data == "all" or resource == milestone
                self.searcher.on_trial_result(
                    trial_id=str(trial_id),
                    config=self._trial_to_config[trial_id],
                    result=result,
                    update=update,
                )
        else:
            trial_decision = SchedulerDecision.STOP
            logger.warning(
                f"Received result for trial_id {trial_id}, which is not "
                f"pending. This result is not used:\n{result}"
            )

        return trial_decision

    def on_trial_error(self, trial: Trial):
        """
        Given the `trial` is currently pending, we send a result at its
        milestone for metric value NaN. Such trials are ranked after all others
        and will most likely not be promoted.

        """
        trial_id = trial.trial_id
        self.searcher.evaluation_failed(str(trial_id))
        if trial_id in self._trial_to_pending_slot:
            bracket_id, slot_in_rung = self._trial_to_pending_slot[trial_id]
            self._report_as_failed(bracket_id, slot_in_rung)
        else:
            logger.warning(
                f"Trial trial_id {trial_id} not registered at pending: "
                "on_trial_error call is ignored"
            )

    def metric_names(self) -> List[str]:
        return [self.metric]

    def metric_mode(self) -> str:
        return self.mode

File Path: syne_tune/optimizer/schedulers/synchronous/hyperband_bracket.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Optional, List, Tuple
from operator import itemgetter
import numpy as np
from dataclasses import dataclass


@dataclass
class SlotInRung:
    """
    Used to communicate slot positions and content for them
    """

    rung_index: int  # 0 is lowest rung
    level: int  # Resource level of rung
    slot_index: int  # index of slot in rung
    trial_id: Optional[int]  # None as long as no trial_id assigned
    metric_val: Optional[float]  # Metric value (None if not yet occupied)


class SynchronousBracket:
    """
    Base class for a single bracket in synchronous Hyperband algorithms.

    A bracket consists of a list of rungs. Each rung consists of a number of
    slots and a resource level (called rung level). The larger the rung level,
    the smaller the number of slots.

    A slot is occupied (by a metric value), free, or pending. A pending slot
    has already been returned by `next_free_slot`. Slots
    in the lowest rung (smallest rung level, largest size) are filled first.
    At any point in time, only slots in the lowest not fully occupied rung
    can be filled. If there are no free slots in the current rung, but there
    are pending ones, the bracket is blocked, and another bracket needs to
    be worked on.
    """

    def __init__(self, mode: str):
        assert mode in {"min", "max"}
        self._mode = mode
        self._first_free_pos = 0
        self.current_rung = 0

    @staticmethod
    def _is_increasing(lst: List[int]) -> bool:
        return all(x < y for x, y in zip(lst, lst[1:]))

    @staticmethod
    def _is_positive_integer(lst: List[int]) -> bool:
        return all(x == int(x) and x >= 1 for x in lst)

    @staticmethod
    def assert_check_rungs(rungs: List[Tuple[int, int]]):
        assert len(rungs) > 0, "There must be at least one rung"
        sizes, levels = zip(*rungs)
        assert SynchronousHyperbandBracket._is_positive_integer(
            levels
        ), f"Rung levels {levels} are not positive integers"
        assert SynchronousHyperbandBracket._is_increasing(
            levels
        ), f"Rung levels {levels} are not increasing"
        assert SynchronousHyperbandBracket._is_positive_integer(
            sizes
        ), f"Rung sizes {sizes} are not positive integers"
        assert SynchronousHyperbandBracket._is_increasing(
            [-x for x in sizes]
        ), f"Rung sizes {sizes} are not decreasing"

    @property
    def num_rungs(self) -> int:
        raise NotImplementedError

    def is_bracket_complete(self) -> bool:
        return self.current_rung >= self.num_rungs

    def _current_rung_and_level(
        self,
    ) -> (List[Tuple[Optional[int], Optional[float]]], int):
        raise NotImplementedError

    def num_pending_slots(self) -> int:
        """
        :return: Number of pending slots (have been returned by
            `next_free_slot`, but not yet occupied
        """
        if self.is_bracket_complete():
            return 0
        rung, _ = self._current_rung_and_level()
        return sum(x[1] is None for x in rung[: self._first_free_pos])

    def next_free_slot(self) -> Optional[SlotInRung]:
        if self.is_bracket_complete():
            return None
        rung, milestone = self._current_rung_and_level()
        pos = self._first_free_pos
        if pos >= len(rung):
            return None
        trial_id = rung[pos][0]
        self._first_free_pos += 1
        return SlotInRung(
            rung_index=self.current_rung,
            level=milestone,
            slot_index=pos,
            trial_id=trial_id,
            metric_val=None,
        )

    def on_result(self, result: SlotInRung) -> bool:
        """
        Provides result for slot previously requested by `next_free_slot`.
        Here, `result.metric` is written to the slot in order to make it
        occupied. Also, `result.trial_id` is written there.

        :param result: See above
        :return: Has the rung been completely occupied with this result?
        """
        assert (
            result.rung_index == self.current_rung
        ), f"Only accept result for rung index {self.current_rung}:\n" + str(result)
        pos = result.slot_index
        assert (
            0 <= pos < self._first_free_pos
        ), f"slot_index must be in [0, {self._first_free_pos}):\n" + str(result)
        rung, milestone = self._current_rung_and_level()
        assert result.level == milestone, (result, milestone)
        trial_id, metric_val = rung[pos]
        self._assert_on_result_trial_id(result, trial_id)
        assert (
            metric_val is None
        ), f"Slot at {pos} already has metric_val = {metric_val}:\n" + str(result)
        assert result.metric_val is not None, "result.metric_val is missing:\n" + str(
            result
        )
        rung[pos] = (result.trial_id, result.metric_val)
        # Check whether rung is complete. If so, move to next one and trigger
        # promotions (optional)
        is_complete = (
            self._first_free_pos >= len(rung) and self.num_pending_slots() == 0
        )
        if is_complete:
            self.current_rung += 1
            self._first_free_pos = 0
            if not self.is_bracket_complete():
                self._promote_trials_at_rung_complete()
        return is_complete

    def _assert_on_result_trial_id(self, result: SlotInRung, trial_id: int):
        pass

    def _promote_trials_at_rung_complete(self):
        raise NotImplementedError


class SynchronousHyperbandBracket(SynchronousBracket):
    """
    Represents a bracket in standard synchronous Hyperband.

    When a rung is fully occupied, slots for the next rung are assigned with
    the trial_id's having the best metric values. At any point in time, only
    slots in the lowest not fully occupied rung can be filled.
    """

    def __init__(self, rungs: List[Tuple[int, int]], mode: str):
        """
        :param rungs: List of `(rung_size, level)`, where `level` is rung
            (resource) level, `rung_size` is rung size (number of slots).
            All entries must be positive int's. The list must be increasing
            in the first and decreasing in the second component
        :param mode: Criterion is minimized ('min') or maximized ('max')
        """
        self.assert_check_rungs(rungs)
        super().__init__(mode)
        # Represents rung levels by (rung, level), where rung is a list of
        # (trial_id, metric_val) tuples for rungs <= self._current_rung.
        # For rungs > self._current_rung, the tuple is (rung_size, level).
        size, level = rungs[0]
        self._rungs = [([(None, None)] * size, level)] + rungs[1:]

    @property
    def num_rungs(self) -> int:
        return len(self._rungs)

    def _current_rung_and_level(
        self,
    ) -> (List[Tuple[Optional[int], Optional[float]]], int):
        return self._rungs[self.current_rung]

    def _assert_on_result_trial_id(self, result: SlotInRung, trial_id: int):
        if trial_id is not None:
            assert result.trial_id == trial_id, (result, trial_id)

    def _promote_trials_at_rung_complete(self):
        pos = self.current_rung
        new_len, milestone = self._rungs[pos]
        previous_rung, _ = self._rungs[pos - 1]
        top_list = get_top_list(rung=previous_rung, new_len=new_len, mode=self._mode)
        # Set metric_val entries to None, since this distinguishes
        # between a pending and occupied slot
        top_list = [(trial_id, None) for trial_id in top_list]
        self._rungs[pos] = (top_list, milestone)


def get_top_list(rung: List[Tuple[int, float]], new_len: int, mode: str) -> List[int]:
    # Failed trials insert NaN's
    rung_valid = [x for x in rung if not np.isnan(x[1])]
    num_valid = len(rung_valid)
    if num_valid >= new_len:
        top_list = sorted(rung_valid, key=itemgetter(1), reverse=mode == "max")[
            :new_len
        ]
    else:
        # Not enough valid entries to fill the new rung (this is
        # very unlikely to happen). In this case, some failed trials
        # are still promoted to the next rung.
        rung_invalid = [x for x in rung if np.isnan(x[1])]
        top_list = rung_valid + rung_invalid[: (new_len - num_valid)]
    return [x[0] for x in top_list]

File Path: syne_tune/optimizer/schedulers/synchronous/hyperband_bracket_manager.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Tuple
import copy

from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket import (
    SynchronousHyperbandBracket,
    SlotInRung,
)
from syne_tune.optimizer.schedulers.synchronous.hyperband_rung_system import (
    RungSystemsPerBracket,
)


class SynchronousHyperbandBracketManager:
    """
    Maintains all brackets, relays requests for another job and report of
    result to one of the brackets.

    Each bracket contains a number of rungs, the largest one `max_num_rungs`.
    A bracket with k rungs has offset `max_num_rungs - k`. Hyperband cycles
    through brackets with offset 0, ..., `num_brackets - 1`, where
    `num_brackets <= max_num_rungs`.

    At any given time, one bracket is primary, all other active brackets are
    secondary. Jobs are preferentially assigned to the primary bracket, but
    if its current rung has no free slots (all are pending), secondary
    brackets are considered.

    Each bracket has a bracket_id (nonnegative int), which is used as key for
    the dicts in `next_jobs`, `on_results`. The primary bracket always has
    the lowest id of all active ones. For job assignment, we iterate over
    active brackets starting from the primary, and assign the job to the
    first bracket which has a free slot. If none of the active brackets have
    a free slot, a new bracket is created.

    """

    def __init__(self, bracket_rungs: RungSystemsPerBracket, mode: str):
        """
        :param bracket_rungs: Rungs for successive brackets, from largest to
            smallest
        :param mode: Criterion is minimized ('min') or maximized ('max')
        """
        self.num_bracket_offsets = len(bracket_rungs)
        assert self.num_bracket_offsets > 0
        assert mode in {"min", "max"}
        self.mode = mode
        self.max_num_rungs = len(bracket_rungs[0])
        for offset, rungs in enumerate(bracket_rungs):
            assert len(rungs) == self.max_num_rungs - offset, (
                f"bracket_rungs[{offset}] has size {len(rungs)}, should "
                + f"have size {self.max_num_rungs - offset}"
            )
            SynchronousHyperbandBracket.assert_check_rungs(rungs)
        self._bracket_rungs = copy.deepcopy(bracket_rungs)
        # List of all brackets. We do not delete brackets which are
        # complete, but just keep them for a record
        self._brackets = []
        # Maps bracket_id to offset
        self._bracket_id_to_offset = []
        # Maps (offset, level), level a rung level in the bracket, to
        # the previous rung level (or 0)
        self._level_to_prev_level = dict()
        for offset, rungs in enumerate(bracket_rungs):
            _, levels = zip(*rungs)
            levels = (0,) + levels
            self._level_to_prev_level.update(
                ((offset, lv), plv) for (lv, plv) in zip(levels[1:], levels[:-1])
            )
        # Create primary bracket
        self._primary_bracket_id = self._create_new_bracket()

    @property
    def bracket_rungs(self) -> RungSystemsPerBracket:
        return self._bracket_rungs

    @property
    def _next_bracket_id(self) -> int:
        return len(self._brackets)

    def level_to_prev_level(self, bracket_id: int, level: int) -> int:
        """
        :param bracket_id:
        :param level: Level in bracket
        :return: Previous level; or 0
        """
        offset = self._bracket_id_to_offset[bracket_id]
        return self._level_to_prev_level[(offset, level)]

    def _create_new_bracket(self) -> int:
        # Sanity check:
        assert len(self._brackets) == len(self._bracket_id_to_offset)
        bracket_id = self._next_bracket_id
        offset = bracket_id % self.num_bracket_offsets
        self._bracket_id_to_offset.append(offset)
        self._brackets.append(
            SynchronousHyperbandBracket(self._bracket_rungs[offset], self.mode)
        )
        return bracket_id

    def next_job(self) -> Tuple[int, SlotInRung]:
        """
        Called by scheduler to request a new job. Jobs are preferentially
        assigned to the primary bracket, which has the lowest id among all
        active brackets. If the primary bracket does not accept jobs (because
        all remaining slots are already pending), further active brackets are
        polled. If none of the active brackets accept jobs, a new bracket is
        created.

        The job description returned is (bracket_id, slot_in_rung), where
        `slot_in_rung` is :class:`SlotInRung`, containing the info of what
        is to be done (`trial_id`, `level` fields). It is this entry which
        has to be returned in 'on_result`, which the `metric_val` field set.
        If the job returned here has `trial_id == None`, it comes from the
        lowest rung of its bracket, and the `trial_id` has to be set as well
        when returning the record in `on_result`.

        :return: Tuple (bracket_id, slot_in_rung)
        """
        # Try to assign job to active bracket. There must be at least one,
        # the primary one
        bracket_ids = range(self._primary_bracket_id, self._next_bracket_id)
        for bracket_id in bracket_ids:
            slot_in_rung = self._brackets[bracket_id].next_free_slot()
            if slot_in_rung is not None:
                return bracket_id, slot_in_rung
        # None of the existing brackets accept jobs. Create a new one
        bracket_id = self._create_new_bracket()
        slot_in_rung = self._brackets[bracket_id].next_free_slot()
        assert slot_in_rung is not None, "Newly created bracket has to have a free slot"
        return bracket_id, slot_in_rung

    def on_result(self, result: Tuple[int, SlotInRung]):
        """
        Called by scheduler to provide result for previously requested job.
        See `next_job`.

        :param result: Tuple (bracket_id, slot_in_rung)
        """
        bracket_id, slot_in_rung = result
        assert self._primary_bracket_id <= bracket_id < self._next_bracket_id, (
            f"Invalid bracket_id = {bracket_id}, must be in "
            + f"[{self._primary_bracket_id}, {self._next_bracket_id})"
        )
        bracket = self._brackets[bracket_id]
        bracket.on_result(slot_in_rung)
        for_primary = bracket_id == self._primary_bracket_id
        if for_primary:
            # Primary bracket is complete: Move to next one. While very
            # unlikely, brackets after the primary one could be complete
            # as well
            last_bracket = self._next_bracket_id - 1
            while (
                bracket.is_bracket_complete()
                and self._primary_bracket_id < last_bracket
            ):
                self._primary_bracket_id += 1
                bracket = self._brackets[self._primary_bracket_id]
            # May have to create a new bracket
            if bracket.is_bracket_complete():
                self._primary_bracket_id = self._create_new_bracket()

File Path: syne_tune/optimizer/schedulers/synchronous/hyperband_impl.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict

from syne_tune.optimizer.scheduler import TrialScheduler
from syne_tune.optimizer.schedulers.synchronous.hyperband import (
    SynchronousHyperbandScheduler,
)
from syne_tune.optimizer.schedulers.synchronous.hyperband_rung_system import (
    SynchronousHyperbandRungSystem,
)
from syne_tune.optimizer.schedulers.synchronous.dehb import (
    DifferentialEvolutionHyperbandScheduler,
)
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
    Integer,
    Float,
    filter_by_key,
)


_ARGUMENT_KEYS = {"grace_period", "max_resource_level", "reduction_factor", "brackets"}

_DEFAULT_OPTIONS = {
    "grace_period": 1,
    "reduction_factor": 3,
}

_CONSTRAINTS = {
    "grace_period": Integer(1, None),
    "max_resource_level": Integer(1, None),
    "reduction_factor": Float(2, None),
    "brackets": Integer(1, None),
}


class SynchronousGeometricHyperbandScheduler(SynchronousHyperbandScheduler):
    """
    Special case of :class:`SynchronousHyperbandScheduler` with rung system
    defined by geometric sequences (see
    `SynchronousHyperbandRungSystem.geometric`). This is the most frequently
    used case.

    Additional parameters
    ---------------------
    grace_period : int
        Smallest (resource) rung level. Must be positive int.
    reduction_factor : float
        Approximate ratio of successive rung levels. Must be >= 2.
    max_resource_level : int (optional)
        Largest rung level, corresponds to `max_t` in
        :class:`FIFOScheduler`. Must be positive int larger than
        `grace_period`. If this is not given, it is inferred like in
        :class:`FIFOScheduler`.
    brackets : int (optional)
        Number of brackets to be used. The default is to use the maximum
        number of brackets per iteration. Pass 1 for successive halving.

    """

    def __init__(self, config_space: Dict, **kwargs):
        TrialScheduler.__init__(self, config_space)
        # Additional parameters to determine rung systems
        kwargs = check_and_merge_defaults(
            kwargs, set(), _DEFAULT_OPTIONS, _CONSTRAINTS, dict_name="scheduler_options"
        )
        self.grace_period = kwargs["grace_period"]
        self.reduction_factor = kwargs["reduction_factor"]
        num_brackets = kwargs.get("brackets")
        max_resource_level = self._infer_max_resource_level(
            kwargs.get("max_resource_level"), kwargs.get("max_resource_attr")
        )
        assert max_resource_level is not None, (
            "The maximum resource level must be specified, either as "
            + "explicit argument 'max_resource_level', or as entry in "
            + "'config_space', with name 'max_resource_attr'"
        )
        self.max_resource_level = max_resource_level
        bracket_rungs = SynchronousHyperbandRungSystem.geometric(
            min_resource=self.grace_period,
            max_resource=max_resource_level,
            reduction_factor=self.reduction_factor,
            num_brackets=num_brackets,
        )
        self._create_internal(bracket_rungs, **filter_by_key(kwargs, _ARGUMENT_KEYS))


class GeometricDifferentialEvolutionHyperbandScheduler(
    DifferentialEvolutionHyperbandScheduler
):
    """
    Special case of :class:`DifferentialEvolutionHyperbandScheduler` with
    rung system defined by geometric sequences. This is the most frequently
    used case.

    Additional parameters
    ---------------------
    grace_period : int
        Smallest (resource) rung level. Must be positive int.
    reduction_factor : float
        Approximate ratio of successive rung levels. Must be >= 2.
    max_resource_level : int (optional)
        Largest rung level, corresponds to `max_t` in
        :class:`FIFOScheduler`. Must be positive int larger than
        `grace_period`. If this is not given, it is inferred like in
        :class:`FIFOScheduler`.
    brackets : int (optional)
        Number of brackets to be used. The default is to use the maximum
        number of brackets per iteration, which is recommended for DEHB

    """

    def __init__(self, config_space: Dict, **kwargs):
        TrialScheduler.__init__(self, config_space)
        # Additional parameters to determine rung systems
        kwargs = check_and_merge_defaults(
            kwargs, set(), _DEFAULT_OPTIONS, _CONSTRAINTS, dict_name="scheduler_options"
        )
        self.grace_period = kwargs["grace_period"]
        self.reduction_factor = kwargs["reduction_factor"]
        num_brackets = kwargs.get("brackets")
        max_resource_level = self._infer_max_resource_level(
            kwargs.get("max_resource_level"), kwargs.get("max_resource_attr")
        )
        assert max_resource_level is not None, (
            "The maximum resource level must be specified, either as "
            + "explicit argument 'max_resource_level', or as entry in "
            + "'config_space', with name 'max_resource_attr':\n"
            + f"max_resource_attr = {kwargs.get('max_resource_attr')}\n"
            + f"config_space = {config_space}"
        )
        self.max_resource_level = max_resource_level
        bracket_rungs = SynchronousHyperbandRungSystem.geometric(
            min_resource=self.grace_period,
            max_resource=max_resource_level,
            reduction_factor=self.reduction_factor,
            num_brackets=num_brackets,
        )
        num_brackets = len(bracket_rungs)
        rungs_first_bracket = bracket_rungs[0]
        self._create_internal(
            rungs_first_bracket=rungs_first_bracket,
            num_brackets_per_iteration=num_brackets,
            **filter_by_key(kwargs, _ARGUMENT_KEYS),
        )

File Path: syne_tune/optimizer/schedulers/synchronous/hyperband_rung_system.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Tuple, Optional
import numpy as np
import logging

logger = logging.getLogger(__name__)


RungSystemsPerBracket = List[List[Tuple[int, int]]]


class SynchronousHyperbandRungSystem:
    """
    Collects factory methods for `RungSystemsPerBracket` rung systems to be
    used in :class:`SynchronousHyperbandBracketManager`.

    """

    @staticmethod
    def geometric(
        min_resource: int,
        max_resource: int,
        reduction_factor: float,
        num_brackets: Optional[int] = None,
    ) -> RungSystemsPerBracket:
        """
        This is the geometric progression setup from the original papers on
        successive halving and Hyperband.

        If `smax = ceil(log(max_resource / min_resource) /
        log(reduction_factor))`, there can be at most `s_max + 1` brackets.
        Here, bracket s has `r_num = s_max - s + 1` rungs, and the size of
        rung r in bracket s is
            `n(r,s) = ceil( (s_max + 1) / r_num) *
            power(reduction_factor, r_num - r - 1)`

        :param min_resource: Smallest resource level (positive int)
        :param max_resource: Largest resource level (positive int)
        :param reduction_factor: Approximate ratio between successive rung levels
        :param num_brackets: Number of brackets. If not given, the maximum number
            of brackets is used. Pass 1 for successive halving
        :return: Rung system
        """
        SynchronousHyperbandRungSystem._assert_positive_int(
            min_resource, "min_resource"
        )
        SynchronousHyperbandRungSystem._assert_positive_int(
            max_resource, "max_resource"
        )
        assert min_resource < max_resource
        assert (
            reduction_factor >= 2
        ), f"reduction_factor = {reduction_factor} must be >= 2"
        s_max = int(
            np.ceil(
                (np.log(max_resource) - np.log(min_resource)) / np.log(reduction_factor)
            )
        )
        if num_brackets is not None:
            SynchronousHyperbandRungSystem._assert_positive_int(
                num_brackets, "num_brackets"
            )
        else:
            num_brackets = s_max + 1  # Max number of brackets
        msg_prefix = (
            f"min_resource = {min_resource}, max_resource = "
            + f"{max_resource}, reduction_factor = {reduction_factor}"
        )
        if s_max <= 0:
            logger.warning(
                msg_prefix + ": supports only one bracket with a single rung level of "
                "size 1. Is that really what you want?"
            )
            return [[(1, max_resource)]]
        if num_brackets > s_max + 1:
            logger.warning(
                msg_prefix
                + f": does not support num_brackets = {num_brackets}, but at "
                f"most {s_max + 1}. I am switching to the latter one."
            )
            num_brackets = s_max + 1
        rung_systems = []
        for bracket in range(num_brackets):
            rungs = []
            r_num_m1 = s_max - bracket
            pre_fact = (s_max + 1) / (r_num_m1 + 1)
            for rung in range(r_num_m1):
                resource = int(
                    round(min_resource * np.power(reduction_factor, rung + bracket))
                )
                rsize = int(
                    np.ceil(pre_fact * np.power(reduction_factor, r_num_m1 - rung))
                )
                rungs.append((rsize, resource))
            rungs.append((int(np.ceil(pre_fact)), max_resource))
            rung_systems.append(rungs)
        return rung_systems

    @staticmethod
    def _assert_positive_int(x: int, name: str):
        assert round(x) == x and x >= 1, f"{name} = {x} must be a positive integer"

File Path: syne_tune/optimizer/schedulers/transfer_learning/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from typing import Any, Dict, List

import numpy as np
import pandas as pd

__all__ = [
    "TransferLearningTaskEvaluations",
    "TransferLearningMixin",
    "BoundingBox",
    "RUSHScheduler",
]


@dataclass
class TransferLearningTaskEvaluations:
    """Class that contains offline evaluations for a task that can be used for transfer learning.
    Args:
        configuration_space: Dict the configuration space that was used when sampling evaluations.
        hyperparameters: pd.DataFrame the hyperparameters values that were acquired, all keys of configuration-space
         should appear as columns.
        objectives_names: List[str] the name of the objectives that were acquired
        objectives_evaluations: np.array values of recorded objectives, must have shape
            (num_evals, num_seeds, num_fidelities, num_objectives)
    """

    configuration_space: Dict
    hyperparameters: pd.DataFrame
    objectives_names: List[str]
    objectives_evaluations: np.array

    def __post_init__(self):
        assert len(self.objectives_names) == self.objectives_evaluations.shape[-1]
        assert len(self.hyperparameters) == self.objectives_evaluations.shape[0]
        assert self.objectives_evaluations.ndim == 4, (
            "objective evaluations should be of shape "
            "(num_evals, num_seeds, num_fidelities, num_objectives)"
        )
        for col in self.hyperparameters.keys():
            assert col in self.configuration_space

    def objective_values(self, objective_name: str) -> np.array:
        return self.objectives_evaluations[
            ..., self.objective_index(objective_name=objective_name)
        ]

    def objective_index(self, objective_name: str) -> int:
        matches = [
            i for i, name in enumerate(self.objectives_names) if name == objective_name
        ]
        assert len(matches) >= 1, (
            f"could not find objective {objective_name} in recorded objectives "
            f"{self.objectives_names}"
        )
        return matches[0]

    def top_k_hyperparameter_configurations(
        self, k: int, mode: str, objective: str
    ) -> List[Dict[str, Any]]:
        """
        Returns the best k hyperparameter configurations.
        :param k: The number of top hyperparameters to return.
        :param mode: 'min' or 'max', indicating the type of optimization problem.
        :param objective: The objective to consider for ranking hyperparameters.
        :returns: List of hyperparameters in order.
        """
        assert k > 0 and isinstance(k, int), f"{k} is no positive integer."
        assert mode in ["min", "max"], f"Unknown mode {mode}, must be 'min' or 'max'."
        assert objective in self.objectives_names, f"Unknown objective {objective}."

        # average over seed and take best fidelity
        avg_objective = self.objective_values(objective_name=objective).mean(axis=1)
        if mode == "max":
            avg_objective = avg_objective.max(axis=1)
        else:
            avg_objective = avg_objective.min(axis=1)
        best_hp_task_indices = avg_objective.argsort()
        if mode == "max":
            best_hp_task_indices = best_hp_task_indices[::-1]
        return self.hyperparameters.loc[best_hp_task_indices[:k]].to_dict("records")


class TransferLearningMixin:
    def __init__(
        self,
        config_space: Dict,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        metric_names: List[str],
        **kwargs,
    ):
        """
        A mixin that adds basic functionality for using offline evaluations.
        :param config_space: configuration space to be sampled from
        :param transfer_learning_evaluations: dictionary from task name to offline evaluations.
        :param metric_names: name of the metric to be optimized.
        """
        super().__init__(config_space=config_space, **kwargs)
        self._metric_names = metric_names
        self._check_consistency(
            config_space=config_space,
            transfer_learning_evaluations=transfer_learning_evaluations,
            metric_names=metric_names,
        )

    def _check_consistency(
        self,
        config_space: Dict,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        metric_names: List[str],
    ):
        for task, evals in transfer_learning_evaluations.items():
            for key in config_space.keys():
                assert key in evals.hyperparameters.columns, (
                    f"the key {key} of the config space should appear in transfer learning evaluations "
                    f"hyperparameters {evals.hyperparameters.columns}"
                )
            assert all([m in evals.objectives_names for m in metric_names]), (
                f"all objectives used in the scheduler {self.metric_names()} should appear in transfer learning "
                f"evaluations objectives {evals.objectives_names}"
            )

    def metric_names(self) -> List[str]:
        return self._metric_names

    def top_k_hyperparameter_configurations_per_task(
        self,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        num_hyperparameters_per_task: int,
        mode: str,
        metric: str,
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Returns the best hyperparameter configurations for each task.
        :param transfer_learning_evaluations: Set of candidates to choose from.
        :param num_hyperparameters_per_task: The number of top hyperparameters per task to return.
        :param mode: 'min' or 'max', indicating the type of optimization problem.
        :param metric: The metric to consider for ranking hyperparameters.
        :returns: Dict which maps from task name to list of hyperparameters in order.
        """
        assert num_hyperparameters_per_task > 0 and isinstance(
            num_hyperparameters_per_task, int
        ), f"{num_hyperparameters_per_task} is no positive integer."
        assert mode in ["min", "max"], f"Unknown mode {mode}, must be 'min' or 'max'."
        assert metric in self.metric_names(), f"Unknown metric {metric}."
        best_hps = dict()
        for task, evaluation in transfer_learning_evaluations.items():
            best_hps[task] = evaluation.top_k_hyperparameter_configurations(
                num_hyperparameters_per_task, mode, metric
            )
        return best_hps


from syne_tune.optimizer.schedulers.transfer_learning.bounding_box import BoundingBox
from syne_tune.optimizer.schedulers.transfer_learning.rush import RUSHScheduler

File Path: syne_tune/optimizer/schedulers/transfer_learning/bounding_box.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Dict, Callable, Optional

import pandas as pd

from syne_tune.optimizer.scheduler import TrialScheduler
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningMixin,
    TransferLearningTaskEvaluations,
)
from syne_tune.config_space import (
    Categorical,
    restrict_domain,
    choice,
    config_space_size,
)


class BoundingBox(TransferLearningMixin, TrialScheduler):
    def __init__(
        self,
        scheduler_fun: Callable[[Dict, str, str], TrialScheduler],
        config_space: Dict,
        metric: str,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        mode: Optional[str] = "min",
        num_hyperparameters_per_task: int = 1,
    ):
        """
        Simple baseline that computes a bounding-box of the best candidate found in previous tasks to restrict the
         search space to only good candidates. The bounding-box is obtained by restricting to the min-max of best
         numerical hyperparameters and restricting to the set of best candidates on categorical parameters.

        Reference: Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning.
        Valerio Perrone, Huibin Shen, Matthias Seeger, Cédric Archambeau, Rodolphe Jenatton. Neurips 2019.

        :param scheduler_fun: function that takes a configuration space (Dict), a mode (str) and a metric (str)
        and returns a scheduler. This is required since the final configuration space is known only after computing
        a bounding-box. For instance,
        `scheduler_fun=lambda new_config_space, mode, metric: RandomSearch(new_config_space, metric, mode)`
        will consider a random-search on the config-space is restricted to the bounding of best evaluations of previous
        tasks.
        :param config_space: initial search-space to consider, will be updated to the bounding of best evaluations of
        previous tasks
        :param metric: objective name to optimize, must be present in transfer learning evaluations.
        :param transfer_learning_evaluations: dictionary from task name to offline evaluations.
        :param mode: mode to be considered, default to min.
        :param num_hyperparameters_per_task: number of best hyperparameter to take per task when computing the bounding
        box, default to 1.
        """
        super().__init__(
            config_space=config_space,
            transfer_learning_evaluations=transfer_learning_evaluations,
            metric_names=[metric],
        )
        assert mode in ["min", "max"], "mode must be either 'min' or 'max'."

        config_space = self.compute_box(
            config_space=config_space,
            transfer_learning_evaluations=transfer_learning_evaluations,
            mode=mode,
            num_hyperparameters_per_task=num_hyperparameters_per_task,
            metric=metric,
        )
        print(f"hyperparameter ranges of best previous configurations {config_space}")
        print(f"({config_space_size(config_space)} options)")
        self.scheduler = scheduler_fun(config_space, mode, metric)

    def compute_box(
        self,
        config_space: Dict,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        mode: str,
        num_hyperparameters_per_task: int,
        metric: str,
    ) -> Dict:
        top_k_per_task = self.top_k_hyperparameter_configurations_per_task(
            transfer_learning_evaluations=transfer_learning_evaluations,
            num_hyperparameters_per_task=num_hyperparameters_per_task,
            mode=mode,
            metric=metric,
        )
        hp_df = pd.DataFrame(
            [hp for _, top_k_hp in top_k_per_task.items() for hp in top_k_hp]
        )

        # compute bounding-box on all hyperparameters that are numerical or categorical
        new_config_space = {}
        for i, (name, domain) in enumerate(config_space.items()):
            if hasattr(domain, "sample"):
                if isinstance(domain, Categorical):
                    hp_values = list(sorted(hp_df.loc[:, name].unique()))
                    new_config_space[name] = choice(hp_values)
                elif hasattr(domain, "lower") and hasattr(domain, "upper"):
                    # domain is numerical, set new lower and upper ranges with bounding-box values
                    new_config_space[name] = restrict_domain(
                        numerical_domain=domain,
                        lower=hp_df.loc[:, name].min(),
                        upper=hp_df.loc[:, name].max(),
                    )
                else:
                    # no known way to compute bounding over non numerical domains such as functional
                    new_config_space[name] = domain
            else:
                new_config_space[name] = domain
        logging.info(
            f"new configuration space obtained after computing bounding-box: {new_config_space}"
        )

        return new_config_space

    def suggest(self, *args, **kwargs):
        return self.scheduler.suggest(*args, **kwargs)

    def on_trial_add(self, *args, **kwargs):
        self.scheduler.on_trial_add(*args, **kwargs)

    def on_trial_complete(self, *args, **kwargs):
        self.scheduler.on_trial_complete(*args, **kwargs)

    def on_trial_remove(self, *args, **kwargs):
        self.scheduler.on_trial_remove(*args, **kwargs)

    def on_trial_error(self, *args, **kwargs):
        self.scheduler.on_trial_error(*args, **kwargs)

    def on_trial_result(self, *args, **kwargs) -> str:
        return self.scheduler.on_trial_result(*args, **kwargs)

    def metric_mode(self) -> str:
        return self.scheduler.metric_mode()

File Path: syne_tune/optimizer/schedulers/transfer_learning/quantile_based/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/transfer_learning/quantile_based/normalization_transforms.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from functools import partial
from typing import Optional

from scipy import stats

import numpy as np


class GaussianTransform:
    """
    Transform data into Gaussian by applying psi = Phi^{-1} o F where F is the truncated empirical CDF.
    :param y: shape (n, dim)
    :param random_state: If specified, randomize the rank when consecutive values exists between extreme values.
     If none use lowest rank of duplicated values.
    """

    def __init__(
        self, y: np.array, random_state: Optional[np.random.RandomState] = None
    ):
        assert y.ndim == 2
        self.dim = y.shape[1]
        self.sorted = y.copy()
        self.sorted.sort(axis=0)
        self.random_state = random_state

    @staticmethod
    def z_transform(
        series, values_sorted, random_state: Optional[np.random.RandomState] = None
    ):
        """
        :param series: shape (n, dim)
        :param values_sorted: series sorted on the first axis
        :param random_state: if not None, ranks are drawn uniformly for values with consecutive ranges
        :return: data with same shape as input series where distribution is normalized on all dimensions
        """
        # Cutoff ranks since `Phi^{-1}` is infinite at `0` and `1` with winsorized constants.
        def winsorized_delta(n):
            return 1.0 / (4.0 * n**0.25 * np.sqrt(np.pi * np.log(n)))

        delta = winsorized_delta(len(series))

        def quantile(values_sorted, values_to_insert, delta):
            low = np.searchsorted(values_sorted, values_to_insert, side="left")
            if random_state is not None:
                # in case where multiple occurences of the same value exists in sorted array
                # we return a random index in the valid range
                high = np.searchsorted(values_sorted, values_to_insert, side="right")
                res = random_state.randint(low, np.maximum(high, low + 1))
            else:
                res = low
            return np.clip(res / len(values_sorted), a_min=delta, a_max=1 - delta)

        quantiles = quantile(values_sorted, series, delta)

        quantiles = np.clip(quantiles, a_min=delta, a_max=1 - delta)

        return stats.norm.ppf(quantiles)

    def transform(self, y: np.array):
        """
        :param y: shape (n, dim)
        :return: shape (n, dim), distributed along a normal
        """
        assert y.shape[1] == self.dim
        # compute truncated quantile, apply gaussian inv cdf
        return np.stack(
            [
                self.z_transform(y[:, i], self.sorted[:, i], self.random_state)
                for i in range(self.dim)
            ]
        ).T


class StandardTransform:
    def __init__(self, y: np.array):
        """
        Transformation that removes mean and divide by standard error.
        :param y:
        """
        assert y.ndim == 2
        self.dim = y.shape[1]
        self.mean = y.mean(axis=0, keepdims=True)
        self.std = y.std(axis=0, keepdims=True)

    def transform(self, y: np.array):
        z = (y - self.mean) / np.clip(self.std, a_min=0.001, a_max=None)
        return z


def from_string(name: str, random_state: Optional[np.random.RandomState] = None):
    assert name in ["standard", "gaussian"]
    mapping = {
        "standard": StandardTransform,
        "gaussian": partial(GaussianTransform, random_state=random_state),
    }
    return mapping[name]

File Path: syne_tune/optimizer/schedulers/transfer_learning/quantile_based/quantile_based_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Dict, Optional
import numpy as np
import xgboost
from sklearn.model_selection import train_test_split

from syne_tune.blackbox_repository.blackbox_surrogate import BlackboxSurrogate
from syne_tune.optimizer.schedulers.searchers import SearcherWithRandomSeed

import pandas as pd

from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
)
from syne_tune.optimizer.schedulers.transfer_learning.quantile_based.normalization_transforms import (
    from_string,
)
from syne_tune.config_space import Domain
from syne_tune.util import catchtime


def extract_input_output(
    transfer_learning_evaluations, normalization: str, random_state
):
    X = pd.concat(
        [evals.hyperparameters for evals in transfer_learning_evaluations.values()],
        ignore_index=True,
    )
    normalizer = from_string(normalization)
    ys = []
    for evals in transfer_learning_evaluations.values():
        # take average over seed and last fidelity and first objective
        y = evals.objectives_evaluations.mean(axis=1)[:, -1, 0:1]
        ys.append(normalizer(y, random_state=random_state).transform(y))
    y = np.concatenate(ys, axis=0)
    return X, y


def fit_model(
    config_space,
    transfer_learning_evaluations,
    normalization: str,
    max_fit_samples: int,
    random_state,
    model=xgboost.XGBRegressor(),
):
    model_pipeline = BlackboxSurrogate.make_model_pipeline(
        configuration_space=config_space,
        fidelity_space={},
        model=model,
    )
    X, y = extract_input_output(
        transfer_learning_evaluations, normalization, random_state=random_state
    )
    with catchtime("time to fit the model"):
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.1, random_state=random_state
        )
        X_train, y_train = subsample(
            X_train, y_train, max_samples=max_fit_samples, random_state=random_state
        )
        model_pipeline.fit(X_train, y_train)

        # compute residuals (num_metrics,)
        sigma_train = eval_model(model_pipeline, X_train, y_train)

        # compute residuals (num_metrics,)
        sigma_val = eval_model(model_pipeline, X_test, y_test)

    return model_pipeline, sigma_train, sigma_val


def eval_model(model_pipeline, X, y):
    # compute residuals (num_metrics,)
    mu_pred = model_pipeline.predict(X)
    if mu_pred.ndim == 1:
        mu_pred = mu_pred.reshape(-1, 1)
    res = np.std(y - mu_pred, axis=0)
    return res.mean()


def subsample(
    X_train,
    z_train,
    max_samples: int = 10000,
    random_state: np.random.RandomState = None,
):
    assert len(X_train) == len(z_train)
    X_train.reset_index(inplace=True)
    if max_samples is not None and max_samples < len(X_train):
        if random_state is None:
            random_indices = np.random.permutation(len(X_train))[:max_samples]
        else:
            random_indices = random_state.permutation(len(X_train))[:max_samples]
        X_train = X_train.loc[random_indices]
        z_train = z_train[random_indices]
    return X_train, z_train


class QuantileBasedSurrogateSearcher(SearcherWithRandomSeed):
    def __init__(
        self,
        config_space: Dict,
        metric: str,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        mode: Optional[str] = None,
        max_fit_samples: int = 100000,
        normalization: str = "gaussian",
        random_seed: Optional[int] = None,
    ):
        """
        Implement the transfer-learning method:
        A Quantile-based Approach for Hyperparameter Transfer Learning.
        David Salinas, Huibin Shen, Valerio Perrone. ICML 2020.
        This is the Copula Thompson Sampling approach described in the paper where a surrogate is fitted on the
        transfer learning data to predict mean/variance of configuration performance given a hyperparameter.
        The surrogate is then sampled from and the best configurations are returned as next candidate to evaluate.
        :param config_space:
        :param mode: whether to minimize or maximize, default to 'min'.
        :param metric: metric to optimize
        :param transfer_learning_evaluations: dictionary from task name to offline evaluations.
        :param max_fit_samples: maximum number to use when fitting the method.
        :param normalization: default to "gaussian" which first computes the rank and then applies Gaussian inverse CDF.
        "standard" applies just standard normalization (remove mean and divide by variance) but performs significanly
        worse.
        :param random_seed:
        """
        super(QuantileBasedSurrogateSearcher, self).__init__(
            config_space=config_space,
            metric=metric,
            random_seed=random_seed,
            points_to_evaluate=[],
        )
        self.mode = mode
        self.model_pipeline, sigma_train, sigma_val = fit_model(
            config_space=config_space,
            transfer_learning_evaluations=transfer_learning_evaluations,
            normalization=normalization,
            max_fit_samples=max_fit_samples,
            model=xgboost.XGBRegressor(),
            random_state=self.random_state,
        )
        logging.info(f"residual train: {sigma_train}")
        logging.info(f"residual val: {sigma_val}")

        with catchtime("time to predict"):
            # note the candidates could also be sampled every time, we cache them rather to save compute time.
            num_candidates = 100000
            self.X_candidates = pd.DataFrame(
                [self._sample_random_config() for _ in range(num_candidates)]
            )
            self.mu_pred = self.model_pipeline.predict(self.X_candidates)
            # simple homoskedastic variance estimate for now
            if self.mu_pred.ndim == 1:
                self.mu_pred = self.mu_pred.reshape(-1, 1)
            self.sigma_pred = np.ones_like(self.mu_pred) * sigma_val

    def _update(self, trial_id: str, config: Dict, result: Dict):
        pass

    def clone_from_state(self, state):
        pass

    def get_config(self, **kwargs):
        samples = self.random_state.normal(loc=self.mu_pred, scale=self.sigma_pred)
        if self.mode == "max":
            samples *= -1
        candidate = self.X_candidates.loc[np.argmin(samples)]
        return dict(candidate)

    def _sample_random_config(self):
        return {
            k: v.sample(random_state=self.random_state) if isinstance(v, Domain) else v
            for k, v in self.config_space.items()
        }

File Path: syne_tune/optimizer/schedulers/transfer_learning/rush.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict, List, Optional

from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
    TransferLearningMixin,
)


class RUSHScheduler(TransferLearningMixin, HyperbandScheduler):
    def __init__(
        self,
        config_space: Dict,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        metric: str,
        type: str = "stopping",
        points_to_evaluate: Optional[List[Dict]] = None,
        custom_rush_points: Optional[List[Dict]] = None,
        num_hyperparameters_per_task: int = 1,
        **kwargs,
    ) -> None:
        """
        A transfer learning variation of Hyperband which uses previously well-performing hyperparameter configurations
        as an initialization. The best hyperparameter configuration of each individual task provided is evaluated.
        The one among them which performs best on the current task will serve as a hurdle and is used to prune
        other candidates. This changes the standard successive halving promotion as follows. As usual, only the top-
        performing fraction is promoted to the next rung level. However, these candidates need to be at least as good
        as the hurdle configuration to be promoted. In practice this means that much fewer candidates can be promoted.

        Reference: A resource-efficient method for repeated HPO and NAS.
        Giovanni Zappella, David Salinas, Cédric Archambeau. AutoML workshop @ ICML 2021.

        :param config_space: configuration space for trial evaluation function.
        :param transfer_learning_evaluations: dictionary from task name to offline evaluations.
        :param metric: objective name to optimize, must be present in transfer learning evaluations.
        :param type: scheduler type ('stopping' or 'promotion'). See :class:`HyperbandScheduler`.
        :param points_to_evaluate: when points_to_evaluate is not None, these configurations are evaluated after
        custom_rush_points and hyperparameter configurations inferred from transfer_learning_evaluations. These points
        are not used to prune any configurations.
        :param custom_rush_points: when custom_rush_points is not None, the provided configurations are evaluated first
        in addition to top performing configurations from other tasks and also serve to preemptively prune
        underperforming configurations
        :param num_hyperparameters_per_task: the number of top hyperparameter configurations to consider per task.
        """
        self._metric_names = [metric]
        assert type in ["stopping", "promotion"], f"Unknown scheduler type {type}"
        top_k_per_task = self.top_k_hyperparameter_configurations_per_task(
            transfer_learning_evaluations=transfer_learning_evaluations,
            num_hyperparameters_per_task=num_hyperparameters_per_task,
            metric=metric,
            mode=kwargs.get("mode", "min"),
        )
        threshold_candidates = [
            hp for _, top_k_hp in top_k_per_task.items() for hp in top_k_hp
        ]
        if custom_rush_points is not None:
            threshold_candidates += custom_rush_points
            threshold_candidates = [
                dict(s) for s in set(frozenset(p.items()) for p in threshold_candidates)
            ]
        num_threshold_candidates = len(threshold_candidates)
        if points_to_evaluate is not None:
            points_to_evaluate = threshold_candidates + [
                hp for hp in points_to_evaluate if hp not in threshold_candidates
            ]
        else:
            points_to_evaluate = threshold_candidates
        super().__init__(
            config_space=config_space,
            transfer_learning_evaluations=transfer_learning_evaluations,
            metric=metric,
            type=f"rush_{type}",
            points_to_evaluate=points_to_evaluate,
            metric_names=[metric],
            rung_system_kwargs={"num_threshold_candidates": num_threshold_candidates},
            **kwargs,
        )

File Path: syne_tune/optimizer/schedulers/transfer_learning/zero_shot.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from typing import Dict, Optional

import numpy as np
import pandas as pd
import xgboost

from syne_tune.blackbox_repository.blackbox_surrogate import BlackboxSurrogate
from syne_tune.config_space import Domain
from syne_tune.optimizer.schedulers.searchers.searcher import BaseSearcher
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
    TransferLearningMixin,
)

logger = logging.getLogger(__name__)


class ZeroShotTransfer(TransferLearningMixin, BaseSearcher):
    def __init__(
        self,
        config_space: Dict,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        metric: str,
        mode: str = "min",
        sort_transfer_learning_evaluations: bool = True,
        use_surrogates: bool = False,
        random_seed: Optional[int] = None,
    ) -> None:
        """
        A zero-shot transfer hyperparameter optimization method which jointly selects configurations that minimize the
        average rank obtained on historic metadata (transfer_learning_evaluations).

        Reference: Sequential Model-Free Hyperparameter Tuning.
        Martin Wistuba, Nicolas Schilling, Lars Schmidt-Thieme.
        IEEE International Conference on Data Mining (ICDM) 2015.

        :param config_space: Configuration space for trial evaluation function.
        :param transfer_learning_evaluations: Dictionary from task name to offline evaluations.
        :param metric: Objective name to optimize, must be present in transfer learning evaluations.
        :param mode: Whether to minimize (min) or maximize (max)
        :param sort_transfer_learning_evaluations: Use False if the hyperparameters for each task in
        transfer_learning_evaluations Are already in the same order. If set to True, hyperparameters are sorted.
        :param use_surrogates: If the same configuration is not evaluated on all tasks, set this to true. This will
        generate a set of configurations and will impute their performance using surrogate models.
        :param random_seed: Used for randomly sampling candidates. Only used if use_surrogate is True.
        """
        super().__init__(
            config_space=config_space,
            transfer_learning_evaluations=transfer_learning_evaluations,
            metric=metric,
            metric_names=[metric],
        )
        self._mode = mode
        self._random_state = np.random.RandomState(random_seed)
        if use_surrogates and len(transfer_learning_evaluations) <= 1:
            use_surrogates = False
            sort_transfer_learning_evaluations = False
        if use_surrogates:
            sort_transfer_learning_evaluations = False
            transfer_learning_evaluations = (
                self._create_surrogate_transfer_learning_evaluations(
                    config_space, transfer_learning_evaluations, metric
                )
            )
        warning_message = "This searcher assumes that each hyperparameter configuration occurs in all tasks. "
        scores = list()
        hyperparameters = None
        for task_name, task_data in transfer_learning_evaluations.items():
            assert (
                hyperparameters is None
                or task_data.hyperparameters.shape == hyperparameters.shape
            ), warning_message
            hyperparameters = task_data.hyperparameters
            if sort_transfer_learning_evaluations:
                hyperparameters = task_data.hyperparameters.sort_values(
                    list(task_data.hyperparameters.columns)
                )
            idx = hyperparameters.index.values
            avg_scores = task_data.objective_values(metric).mean(axis=1)
            if self._mode == "max":
                avg_scores = avg_scores.max(axis=1)[idx]
            else:
                avg_scores = avg_scores.min(axis=1)[idx]
            scores.append(avg_scores)
        if not use_surrogates:
            if len(transfer_learning_evaluations) > 1:
                logger.warning(
                    warning_message
                    + "If this is not the case, this searcher fails without a warning."
                )
            if not sort_transfer_learning_evaluations:
                hyperparameters = hyperparameters.copy()
        hyperparameters.reset_index(drop=True, inplace=True)
        self._hyperparameters = hyperparameters
        sign = 1 if self._mode == "min" else -1
        self._scores = sign * pd.DataFrame(scores)
        self._ranks = self._update_ranks()

    def _create_surrogate_transfer_learning_evaluations(
        self,
        config_space: Dict,
        transfer_learning_evaluations: Dict[str, TransferLearningTaskEvaluations],
        metric: str,
    ) -> Dict[str, TransferLearningTaskEvaluations]:
        """
        Creates transfer_learning_evaluations where each configuration is evaluated on each task using surrogate models.
        """
        surrogate_transfer_learning_evaluations = dict()
        for task_name, task_data in transfer_learning_evaluations.items():
            estimator = BlackboxSurrogate.make_model_pipeline(
                configuration_space=config_space,
                fidelity_space={},
                model=xgboost.XGBRegressor(),
            )
            X_train = task_data.hyperparameters
            y_train = task_data.objective_values(metric).mean(axis=1)
            if self._mode == "max":
                y_train = y_train.max(axis=1)
            else:
                y_train = y_train.min(axis=1)
            estimator.fit(X_train, y_train)

            num_candidates = 10000 if len(config_space) >= 6 else 5 ** len(config_space)
            hyperparameters_new = pd.DataFrame(
                [
                    self._sample_random_config(config_space)
                    for _ in range(num_candidates)
                ]
            )
            objectives_evaluations_new = estimator.predict(hyperparameters_new).reshape(
                -1, 1, 1, 1
            )
            surrogate_transfer_learning_evaluations[
                task_name
            ] = TransferLearningTaskEvaluations(
                configuration_space=config_space,
                hyperparameters=hyperparameters_new,
                objectives_names=[metric],
                objectives_evaluations=objectives_evaluations_new,
            )
        return surrogate_transfer_learning_evaluations

    def get_config(self, **kwargs) -> Optional[Dict]:
        if self._ranks.shape[1] == 0:
            return None
        # Select greedy-best configuration considering all others
        best_idx = self._ranks.mean(axis=0).idxmin()
        # Update ranks for choosing each configuration considering the previously chosen ones
        self._ranks.clip(upper=self._ranks[best_idx], axis=0, inplace=True)
        # Drop the chosen configuration as a future candidate
        self._scores.drop(columns=best_idx, inplace=True)
        best_config = self._hyperparameters.loc[best_idx]
        self._hyperparameters.drop(index=best_idx, inplace=True)
        if self._ranks.std(axis=1).sum() == 0:
            self._ranks = self._update_ranks()
        return best_config.to_dict()

    def _sample_random_config(self, config_space: Dict) -> Dict:
        return {
            k: v.sample(random_state=self._random_state) if isinstance(v, Domain) else v
            for k, v in config_space.items()
        }

    def _update_ranks(self) -> pd.DataFrame:
        return self._scores.rank(axis=1)

    def _update(self, trial_id: str, config: Dict, result: Dict) -> None:
        pass

    def clone_from_state(self, state: Dict):
        raise NotImplementedError()

File Path: syne_tune/optimizer/schedulers/utils/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/optimizer/schedulers/utils/simple_profiler.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Dict
import time
import logging
from dataclasses import dataclass
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class ProfilingBlock:
    meta: dict
    time_stamp: float
    durations: Dict[str, List[float]]


class SimpleProfiler:
    """
    Useful to profile time of recurring computations, for example
    `get_config` calls in searchers.

    Measurements are divided into blocks. A block is started by `begin_block`.
    Each block stores meta data, a time stamp when `begin_block` was called
    (relative to the time stamp for the first block, which is 0), and a dict of
    lists of durations, whose keys are tags. A tag corresponds to a range of
    code to be profiled. It may be executed many times within a block,
    therefore lists of durations.

    Tags can have multiple levels of prefixes, corresponding to brackets.

    """

    def __init__(self):
        self.records = list()
        self.start_time = dict()
        self.time_stamp_first_block = None
        self.meta_keys = None
        self.prefix = ""

    def begin_block(self, meta: dict):
        assert not self.start_time, "Timers for these tags still running:\n{}".format(
            self.start_time.keys()
        )
        meta_keys = tuple(sorted(meta.keys()))
        if self.time_stamp_first_block is None:
            self.meta_keys = meta_keys
            self.time_stamp_first_block = time.time()
        else:
            assert (
                meta_keys == self.meta_keys
            ), "meta.keys() = {}, but must be the same as for all previous meta dicts ({})".format(
                meta_keys, self.meta_keys
            )
        time_stamp = time.time() - self.time_stamp_first_block
        new_block = ProfilingBlock(
            meta=meta.copy(), time_stamp=time_stamp, durations=dict()
        )
        self.records.append(new_block)
        self.prefix = ""

    def push_prefix(self, prefix: str):
        assert "_" not in prefix, "Prefix must not contain '_'"
        self.prefix += prefix + "_"

    def pop_prefix(self):
        lpref = len(self.prefix)
        assert lpref > 0, "Prefix is empty"
        pos = self.prefix.rfind("_", 0, lpref - 1)
        if pos != -1:
            self.prefix = self.prefix[: (pos + 1)]
        else:
            self.prefix = ""

    def start(self, tag: str):
        assert self.records, "No block has been started yet (use 'begin_block')"
        tag = self.prefix + tag
        assert tag not in self.start_time, "Timer for '{}' already running".format(tag)
        self.start_time[tag] = time.time()

    def stop(self, tag: str):
        assert self.records, "No block has been started yet (use 'begin_block')"
        tag = self.prefix + tag
        assert tag in self.start_time, "Timer for '{}' does not exist".format(tag)
        duration = time.time() - self.start_time[tag]
        block = self.records[-1]
        if tag in block.durations:
            block.durations[tag].append(duration)
        else:
            block.durations[tag] = [duration]
        del self.start_time[tag]

    def clear(self):
        remaining_tags = list(self.start_time.keys())
        if remaining_tags:
            logger.warning(
                "Timers for these tags not stopped (will be removed):\n{}".format(
                    remaining_tags
                )
            )
        self.start_time = dict()

    def records_as_dict(self) -> dict:
        """
        Return records as a dict of lists, can be converted into Pandas
        data-frame by:

            pandas.DataFrame.fromDict(...)

        Each entry corresponds to a column.
        """
        if len(self.records) == 0:
            return dict()
        # For each tag, we emit the following columns: tag_num, tag_mean,
        # tag_std
        data = {"time_stamp": []}
        union_tags = self._union_of_tags()
        suffices = ("_num", "_mean", "_std", "_sum")
        for tag in union_tags:
            for suffix in suffices:
                data[tag + suffix] = []
        for k in self.meta_keys:
            data[k] = []
        # fill the columns row by row
        for block in self.records:
            data["time_stamp"].append(block.time_stamp)
            for k, v in block.meta.items():
                data[k].append(v)
            for tag in union_tags:
                for suffix in suffices:
                    data[tag + suffix].append(0)
            for tag, durations in block.durations.items():
                data[tag + "_num"][-1] = len(durations)
                data[tag + "_mean"][-1] = np.mean(durations)
                data[tag + "_std"][-1] = np.std(durations)
                data[tag + "_sum"][-1] = sum(durations)
        return data

    def _union_of_tags(self):
        union_tags = set()
        for block in self.records:
            union_tags.update(block.durations.keys())
        return union_tags

File Path: syne_tune/remote/__init__.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

File Path: syne_tune/remote/remote_launcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import shutil
import subprocess
from pathlib import Path
from typing import Optional, List
import os

import boto3
from sagemaker.pytorch import PyTorch

from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    add_syne_tune_dependency,
    get_execution_role,
)
from syne_tune import Tuner
from syne_tune.util import s3_experiment_path
from syne_tune.constants import ST_REMOTE_UPLOAD_DIR_NAME

import syne_tune

logger = logging.getLogger(__name__)


class RemoteLauncher:
    def __init__(
        self,
        tuner: Tuner,
        role: Optional[str] = None,
        instance_type: str = "ml.m5.xlarge",
        dependencies: Optional[List[str]] = None,
        store_logs_localbackend: bool = False,
        log_level: Optional[int] = None,
        s3_path: Optional[str] = None,
        no_tuner_logging: bool = False,
        **estimator_kwargs,
    ):
        """
        This class allows to launch a tuning job remotely
        The remote tuning job may use either the local backend (in which case the remote
        instance will be used to evaluate trials) or the Sagemaker backend in which case the remote instance
        will spawn one Sagemaker job per trial.

        :param tuner: Tuner that should be run remotely on a `instance_type` instance. Note that StoppingCriterion
        should be used for the Tuner rather than a lambda function to ensure serialization.
        :param role: sagemaker role to be used to launch the remote tuning instance.
        :param instance_type: instance where the tuning is going to happen.
        :param dependencies: list of folders that should be included as dependencies for the backend script to run
        :param estimator_kwargs: Extra arguments for creating the SageMaker estimator for the tuning code.
        :param store_logs_localbackend: whether to store logs of trials when using the local backend.
            When using Sagemaker backend, logs are persisted by Sagemaker.
        :param log_level: Logging level. Default is logging.INFO, while
            logging.DEBUG gives more messages
        :param s3_path: S3 base path used for checkpointing, outputs of tuning will be stored under
        {s3_path}/{tuner_name}. The logs of the local backend are only stored if `store_logs_localbackend` is True.
        :param no_tuner_logging: If True, the logging level for
            syne_tune.tuner is set to ERROR
        """
        assert not self.is_lambda(tuner.stop_criterion), (
            "remote launcher does not support using lambda functions for stopping criterion. Use StoppingCriterion, "
            "with Tuner if you want to use the remote launcher. See launch_height_sagemaker_remotely.py for"
            " a full example."
        )
        self.tuner = tuner
        self.role = get_execution_role() if role is None else role
        self.instance_type = instance_type
        self.base_job_name = f"smtr-{tuner.name}"
        if dependencies is not None:
            for dep in dependencies:
                assert Path(dep).exists(), f"dependency {dep} was not found."
        self.dependencies = dependencies
        if estimator_kwargs is None:
            estimator_kwargs = dict()
        self.estimator_kwargs = estimator_kwargs

        self.store_logs_localbackend = store_logs_localbackend
        self.log_level = log_level
        if s3_path is None:
            s3_path = s3_experiment_path()
        self.s3_path = s3_path
        assert isinstance(no_tuner_logging, bool)
        self.no_tuner_logging = no_tuner_logging

    def is_lambda(self, f):
        """
        :param f:
        :return: True iff f is a lambda function
        """
        try:
            return callable(f) and f.__name__ == "<lambda>"
        except AttributeError:
            return False

    def run(
        self,
        wait: bool = True,
    ):
        """
        :param wait: Whether the call should wait until the job completes (default: True). If False the call returns
        once the tuning job is scheduled on Sagemaker.
        :return:
        """
        self.prepare_upload()

        if boto3.Session().region_name is None:
            # launching in this is needed to send a default configuration on the tuning loop running on Sagemaker
            # todo restore the env variable if present to avoid a side effect
            os.environ["AWS_DEFAULT_REGION"] = "us-west-2"
        self.launch_tuning_job_on_sagemaker(wait=wait)

    def prepare_upload(self):
        """
        Prepares the files that needs to be uploaded by Sagemaker so that the Tuning job can happen.
        This includes, 1) the entrypoint script of the backend and 2) the tuner that needs to run
        remotely.
        :return:
        """
        upload_dir = str(self.upload_dir())
        shutil.rmtree(upload_dir, ignore_errors=True)

        # Save entrypoint script and content in a folder to be send by sagemaker.
        # This is required so that the entrypoint is found on Sagemaker.
        source_dir = str(self.get_source_dir())
        logger.info(f"copy endpoint files from {source_dir} to {upload_dir}")
        shutil.copytree(source_dir, upload_dir)

        backup = str(self.tuner.trial_backend.entrypoint_path())

        # update the path of the endpoint script so that it can be found when launching remotely
        self.update_backend_with_remote_paths()

        # save tuner
        self.tuner.save(upload_dir)

        # avoid side effect
        self.tuner.trial_backend.set_entrypoint(backup)

        # todo clean copy of remote dir
        tgt_requirement = self.remote_script_dir() / "requirements.txt"
        try:
            os.remove(tgt_requirement)
        except OSError:
            pass
        endpoint_requirements = (
            self.tuner.trial_backend.entrypoint_path().parent / "requirements.txt"
        )
        if endpoint_requirements.exists():
            logger.info(
                f"copy endpoint script requirements to {self.remote_script_dir()}"
            )
            shutil.copy(endpoint_requirements, tgt_requirement)
            pass

    def get_source_dir(self) -> Path:
        # note: this logic would be better moved to the backend.
        if self.is_source_dir_specified():
            return Path(self.tuner.trial_backend.source_dir)
        else:
            return Path(self.tuner.trial_backend.entrypoint_path()).parent

    def is_source_dir_specified(self) -> bool:
        return (
            hasattr(self.tuner.trial_backend, "source_dir")
            and self.tuner.trial_backend.sm_estimator.source_dir is not None
        )

    def update_backend_with_remote_paths(self):
        """
        Update the paths of the backend of the endpoint script and source dir with their remote location.
        """
        if self.is_source_dir_specified():
            # the source_dir is deployed to `upload_dir`
            self.tuner.trial_backend.sm_estimator.source_dir = str(
                Path(self.upload_dir().name)
            )
        else:
            self.tuner.trial_backend.set_entrypoint(
                f"{self.upload_dir().name}/{self.tuner.trial_backend.entrypoint_path().name}"
            )

    def upload_dir(self) -> Path:
        return Path(syne_tune.__path__[0]).parent / ST_REMOTE_UPLOAD_DIR_NAME

    def remote_script_dir(self) -> Path:
        return Path(__file__).parent

    def launch_tuning_job_on_sagemaker(self, wait: bool):
        # todo add Sagemaker cloudwatch metrics to visualize live results of tuning best results found over time.
        if self.instance_type != "local":
            checkpoint_s3_root = f"{self.s3_path}/{self.tuner.name}"
            logger.info(f"Tuner will checkpoint results to {checkpoint_s3_root}")
        else:
            # checkpointing is not supported in local mode. When using local mode with remote tuner (for instance for
            # debugging), results are not stored.
            checkpoint_s3_root = None
        # Create SM estimator for tuning code
        hyperparameters = {
            "tuner_path": f"{self.upload_dir().name}/",
            "store_logs": self.store_logs_localbackend,
            "no_tuner_logging": self.no_tuner_logging,
        }
        if self.log_level is not None:
            hyperparameters["log_level"] = self.log_level

        # avoids error "Must setup local AWS configuration with a region supported by SageMaker."
        # in case no region is explicitely configured by providing a default region
        environment = self.estimator_kwargs.pop("environment", {})
        if "AWS_DEFAULT_REGION" not in environment:
            environment["AWS_DEFAULT_REGION"] = boto3.Session().region_name

        image_uri = self.estimator_kwargs.pop("image_uri", None)
        if image_uri is not None:
            logger.info(
                f"Using custom image {image_uri}, make sure that Syne Tune is installed in your custom container."
            )
        else:
            image_uri = self.syne_tune_image_uri()

        # the choice of the estimator is arbitrary here since we use a base image of Syne Tune.
        tuner_estimator = PyTorch(
            # path which calls the tuner
            entry_point="remote_main.py",
            source_dir=str(self.remote_script_dir()),
            instance_type=self.instance_type,
            instance_count=1,
            role=self.role,
            py_version="py38",
            framework_version="1.10.0",
            image_uri=image_uri,
            hyperparameters=hyperparameters,
            checkpoint_s3_uri=checkpoint_s3_root,
            environment=environment,
            **self.estimator_kwargs,
        )

        add_syne_tune_dependency(tuner_estimator)

        # ask Sagemaker to send the path containing entrypoint script and tuner.
        tuner_estimator.dependencies.append(str(self.upload_dir()))

        if self.dependencies is not None:
            tuner_estimator.dependencies += self.dependencies

        # launches job on Sagemaker
        return tuner_estimator.fit(wait=wait, job_name=self.tuner.name)

    def syne_tune_image_uri(self) -> str:
        """
        :return: syne tune docker uri, if not present try to build it and returns an error if this failed.
        """
        docker_image_name = "syne-tune-cpu-py38"
        account_id = boto3.client("sts").get_caller_identity()["Account"]
        region_name = boto3.Session().region_name
        image_uri = (
            f"{account_id}.dkr.ecr.{region_name}.amazonaws.com/{docker_image_name}"
        )
        try:
            logger.info(f"Fetching Syne Tune image {image_uri}")
            boto3.client("ecr").list_images(repositoryName=docker_image_name)
        except Exception:
            # todo RepositoryNotFoundException should be caught but I did not manage to import it
            logger.warning(
                f"Docker-image of syne-tune {docker_image_name} could not be found, run \n"
                f"`cd {Path(__file__).parent}/container; bash build_syne_tune_container.sh`\n"
                f"in a terminal to build it. Trying to do it now."
            )
            subprocess.run(
                "./build_syne_tune_container.sh",
                cwd=Path(syne_tune.__path__[0]).parent / "container",
            )
            logger.info(f"attempting to fetch {docker_image_name} again.")
            boto3.client("ecr").list_images(repositoryName=docker_image_name)

        return image_uri

File Path: syne_tune/remote/remote_main.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Entrypoint script that allows to launch a tuning job remotely.
It loads the tuner from a specified path then runs it.
"""
import logging
from argparse import ArgumentParser
from pathlib import Path

from syne_tune import Tuner


def decode_bool(hp: str):
    # Sagemaker encodes hyperparameters in estimators as literals which are compatible with Python,
    # except for true and false that are respectively encoded as 'True' and 'False'.
    assert hp in ["True", "False"]
    return hp == "True"


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("--tuner_path", type=str, default="tuner/")
    parser.add_argument("--store_logs", type=str, default="False")
    parser.add_argument("--log_level", type=int, default=logging.INFO)
    parser.add_argument("--no_tuner_logging", type=str, default="False")
    args, _ = parser.parse_known_args()

    root = logging.getLogger()
    root.setLevel(args.log_level)

    args.store_logs = decode_bool(args.store_logs)
    args.no_tuner_logging = decode_bool(args.no_tuner_logging)

    tuner_path = Path(args.tuner_path)
    logging.info(f"load tuner from path {args.tuner_path}")
    tuner = Tuner.load(tuner_path)

    # The output of the tuner (results, metadata, tuner state) is written into SageMaker checkpoint directory
    # which is synced regularly by SageMaker so that results are updated continuously
    tuner.tuner_path = Path("/opt/ml/checkpoints/")

    # The logs/checkpoints of trials are persisted to the checkpoint directory only where store_logs is True
    if args.store_logs:
        # inform the backend of the desired path so that logs are persisted
        tuner.trial_backend.set_path(results_root=tuner.tuner_path)
    else:
        # sets a path where logs will not be stored, we use "/opt/ml/input/data/training" as it is mounted
        # on a partition that is larger than "~/"
        path = Path("/opt/ml/input/data/")
        path.mkdir(parents=True, exist_ok=True)
        tuner.trial_backend.set_path(results_root=str(path), tuner_name=tuner.name)

    # Run the tuner on the sagemaker instance. If the simulation back-end is
    # used, this needs a specific callback
    if args.no_tuner_logging == "True":
        logging.getLogger("syne_tune.tuner").setLevel(logging.ERROR)
    logging.info("starting remote tuning")
    tuner.run()

File Path: syne_tune/report.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import os
import re
import sys
import numpy as np
import json
import logging
from ast import literal_eval
from typing import List, Dict
from time import time, perf_counter
from dataclasses import dataclass

from syne_tune.constants import (
    ST_INSTANCE_TYPE,
    ST_INSTANCE_COUNT,
    ST_WORKER_TIME,
    ST_WORKER_COST,
    ST_WORKER_TIMESTAMP,
    ST_WORKER_ITER,
)

# this is required so that metrics are written
from syne_tune.backend.sagemaker_backend.instance_info import InstanceInfos

logging.basicConfig()
logger = logging.getLogger(__name__)


@dataclass
class Reporter:
    # Whether to add automatically `st_worker_time` information on the metric reported by the worker which measures
    # the number of seconds spent since the creation of the Reporter.
    add_time: bool = True

    # Whether to add automatically `st_worker_cost` information on the metric reported by the worker which measures
    # the estimated dollar-cost (only available and activated by default on Sagemaker backend). This option requires
    # add_time to be activated.
    add_cost: bool = True

    def __post_init__(self):
        if self.add_time:
            self.start = perf_counter()
            self.iter = 0
            # TODO dollar-cost computation is not available for file-based backends, what would be
            #  needed to add support for those backends will be to add a way to access instance-type
            #  information.
            if self.add_cost:
                # add instance_type and instance count so that cost can be computed easily
                self.instance_type = os.getenv(
                    f"SM_HP_{ST_INSTANCE_TYPE.upper()}", None
                )
                self.instance_count = literal_eval(
                    os.getenv(f"SM_HP_{ST_INSTANCE_COUNT.upper()}", "1")
                )
                if self.instance_type is not None:
                    logger.info(
                        f"detected instance-type/instance-count to {self.instance_type}/{self.instance_count}"
                    )
                    instance_infos = InstanceInfos()
                    if self.instance_type in instance_infos.instances:
                        cost_per_hour = instance_infos(
                            instance_type=self.instance_type
                        ).cost_per_hour
                        self.dollar_cost = cost_per_hour * self.instance_count / 3600

    def __call__(self, **kw) -> None:
        """
        Report metrics obtained after evaluating a training function, see `train_height.py` for a full
         example on how to use it and to define a training function.
        A time stamp `st_timestamp` is added, if `add_time` is True then `st_worker_time` is added
        which measures seconds spent in the worker, if `add_cost` is True `st_worker_cost` is added that measures
        dollar cost spent in worker (only available on Sagemaker instances and requires `st_worker_time` to be
        activated).
        :param kwargs: key word arguments of the metrics to report, for instance `report(epoch=1, loss=1.2)` the only
        constrain is that values must be serializable with json and keys should not start with `st_` which is a
        reserved namespace for Syne Tune internals.
        """
        for key in kw.keys():
            assert not key.startswith("st_"), (
                "The metric prefix 'st_' is used by Syne Tune internals, "
                "please use a metric name that does not start with 'st_'."
            )

        kw[ST_WORKER_TIMESTAMP] = time()
        if self.add_time:
            seconds_spent = perf_counter() - self.start
            kw[ST_WORKER_TIME] = seconds_spent
            # second cost will only be there if we were able to properly detect the instance-type and instance-count
            # from the environment
            if hasattr(self, "dollar_cost"):
                kw[ST_WORKER_COST] = seconds_spent * self.dollar_cost
        kw[ST_WORKER_ITER] = self.iter
        self.iter += 1
        _report_logger(**kw)


def _report_logger(**kwargs):
    print(f"[tune-metric]: {_serialize_report_dict(kwargs)}")
    sys.stdout.flush()


def _serialize_report_dict(report_dict: Dict) -> str:
    """
    :param report_dict: a dictionary of metrics to be serialized
    :return: serialized string of the reported metrics, an exception is raised if the size is too large or
    if the dictionary values are not JSON-serializable
    """
    try:

        def np_encoder(object):
            if isinstance(object, np.generic):
                return object.item()

        report_str = json.dumps(report_dict, default=np_encoder)
        assert sys.getsizeof(report_str) < 50_000
        return report_str
    except TypeError as e:
        print("The dictionary set to be reported does not seem to be serializable.")
        raise e
    except AssertionError as e:
        print("The dictionary set to be reported is too large.")
        raise e
    except Exception as e:
        raise e


def retrieve(log_lines: List[str]) -> List[Dict[str, float]]:
    """
    Retrieves metrics reported with `_report_logger` given log lines.
    :param log_lines:
    :return: list of metrics retrieved from the log lines.
    """
    metrics = []
    regex = r"\[tune-metric\]: (\{.*\})"
    for metric_values in re.findall(regex, "\n".join(log_lines)):
        metrics.append(json.loads(metric_values))
    return metrics

File Path: syne_tune/stopping_criterion.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
import numpy as np

from dataclasses import dataclass
from typing import Optional, Dict

from syne_tune.tuning_status import TuningStatus

logger = logging.getLogger(__name__)


@dataclass
class StoppingCriterion:
    """
    Stopping criterion that can be used in a Tuner, for instance
    `Tuner(stop_criterion=StoppingCriterion(max_wallclock_time=3600), ...)`
    While a lambda can be used for the Tuner, e.g.
    `Tuner(stop_criterion=lambda status: status.wallclock_time > 3600, ...)`
    Using this class is needed when using the remote launcher to ensure serialization works correctly.
    """

    max_wallclock_time: float = None
    max_num_evaluations: int = None
    max_num_trials_started: int = None
    max_num_trials_completed: int = None
    max_cost: float = None
    max_num_trials_finished: int = None

    # minimum value for metrics, any value bellow this threshold will trigger a stop
    min_metric_value: Optional[Dict[str, float]] = None
    # maximum value for metrics, any value above this threshold will trigger a stop
    max_metric_value: Optional[Dict[str, float]] = None

    # todo we should have unit-test for all those cases.
    def __call__(self, status: TuningStatus) -> bool:
        if (
            self.max_wallclock_time is not None
            and status.wallclock_time > self.max_wallclock_time
        ):
            logger.info(
                f"reaching max wallclock time ({self.max_wallclock_time}), stopping there."
            )
            return True
        if (
            self.max_num_trials_started is not None
            and status.num_trials_started > self.max_num_trials_started
        ):
            logger.info(
                f"reaching max number of trials started ({self.max_num_trials_started}), stopping there."
            )
            return True
        if (
            self.max_num_trials_completed is not None
            and status.num_trials_completed > self.max_num_trials_completed
        ):
            logger.info(
                f"reaching max number of trials completed ({self.max_num_trials_completed}), stopping there."
            )
            return True
        if (
            self.max_num_trials_finished is not None
            and status.num_trials_finished > self.max_num_trials_finished
        ):
            logger.info(
                f"reaching max number of trials finished ({self.max_num_trials_finished}), stopping there."
            )
            return True
        if self.max_cost is not None and status.cost > self.max_cost:
            logger.info(f"reaching max cost ({self.max_cost}), stopping there.")
            return True
        if (
            self.max_num_evaluations is not None
            and status.overall_metric_statistics.count > self.max_num_evaluations
        ):
            logger.info(
                f"reaching {status.overall_metric_statistics.count} evaluations, stopping there. "
            )
            return True
        if (
            self.max_metric_value is not None
            and status.overall_metric_statistics.count > 0
        ):
            max_metrics_observed = status.overall_metric_statistics.max_metrics
            for metric, max_metric_accepted in self.max_metric_value.items():
                if (
                    metric in max_metrics_observed
                    and max_metrics_observed[metric] > max_metric_accepted
                ):
                    logger.info(
                        f"found {metric} with value ({max_metrics_observed[metric]}), "
                        f"above the provided threshold {max_metric_accepted} stopping there."
                    )
                    return True

        if (
            self.min_metric_value is not None
            and status.overall_metric_statistics.count > 0
        ):
            min_metrics_observed = status.overall_metric_statistics.min_metrics
            for metric, min_metric_accepted in self.min_metric_value.items():
                if (
                    metric in min_metrics_observed
                    and min_metrics_observed[metric] < min_metric_accepted
                ):
                    logger.info(
                        f"found {metric} with value ({min_metrics_observed[metric]}), "
                        f"bellow the provided threshold {min_metric_accepted} stopping there."
                    )
                    return True
        return False


class PlateauStopper(object):
    """
    Stops the experiment when a metric plateaued for N consecutive trials
    for more than the given amount of iterations specified in the patience parameter.
    This code is mostly copied from RayTune.

    :param metric: The metric to be monitored.
    :param std: The minimal standard deviation after which
             the tuning process has to stop.
    :param num_trials: The number of consecutive trials
    :param mode: The mode to select the top results.
             Can either be "min" or "max".
    :param patience: Number of iterations to wait for
             a change in the top models.
    """

    def __init__(
        self,
        metric: str,
        std: float = 0.001,
        num_trials: int = 10,
        mode: str = "min",
        patience: int = 0,
    ):
        if mode not in ("min", "max"):
            raise ValueError("The mode parameter can only be either min or max.")

        if not isinstance(num_trials, int) or num_trials <= 1:
            raise ValueError(
                "Top results to consider must be"
                " a positive integer greater than one."
            )
        if not isinstance(patience, int) or patience < 0:
            raise ValueError("Patience must be a strictly positive integer.")
        if not isinstance(std, float) or std <= 0:
            raise ValueError(
                "The standard deviation must be a strictly positive float number."
            )
        self._mode = mode
        self._metric = metric
        self._patience = patience
        self._iterations = 0
        self._std = std
        self._num_trials = num_trials

        if self._mode == "min":
            self.multiplier = 1
        else:
            self.multiplier = -1

    def __call__(self, status: TuningStatus) -> bool:

        """Return a boolean representing if the tuning has to stop."""

        if status.num_trials_finished == 0:
            return False

        trials = status.trial_rows
        trajectory = []
        curr_best = None

        for ti in trials.values():
            if self._metric in ti:
                y = self.multiplier * ti[self._metric]
                if curr_best is None or y < curr_best:
                    curr_best = y
                trajectory.append(curr_best)

        top_values = trajectory[-self._num_trials :]
        # If the current iteration has to stop
        has_plateaued = (
            len(top_values) == self._num_trials and np.std(top_values) <= self._std
        )
        if has_plateaued:
            # we increment the total counter of iterations
            self._iterations += 1
        else:
            # otherwise we reset the counter
            self._iterations = 0

        # and then call the method that re-executes
        # the checks, including the iterations.
        return has_plateaued and self._iterations >= self._patience

File Path: syne_tune/try_import.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.


def try_import_gpsearchers_message() -> str:
    return _try_import_message(
        "Gaussian process based searchers are not imported", tag="gpsearchers"
    )


def try_import_kde_message() -> str:
    return _try_import_message("KDE searchers are not imported", tag="kde")


def try_import_bore_message() -> str:
    return _try_import_message("BORE searchers are not imported", tag="bore")


def try_import_raytune_message() -> str:
    return _try_import_message(
        "Ray Tune schedulers and searchers are not imported", tag="raytune"
    )


def try_import_benchmarks_message() -> str:
    return _try_import_message(
        "Dependencies for benchmarks are not imported", tag="benchmarks"
    )


def try_import_aws_message() -> str:
    return _try_import_message("AWS dependencies are not imported", tag="aws")


def try_import_blackbox_repository_message() -> str:
    return _try_import_message(
        "Dependencies of blackbox repository are not imported",
        tag="blackbox-repository",
    )


def try_import_backend_message(backend_type: str) -> str:
    return (
        f"{backend_type} is not imported"
        + " since dependencies are missing. You can install them with\n"
        + "   pip install 'syne-tune[extra]'"
    )


def _try_import_message(message_text: str, tag: str) -> str:
    return (
        message_text
        + " since dependencies are missing. You can install them with\n"
        + f"   pip install 'syne-tune[{tag}]'\n"
        + "or (for everything)\n"
        + "   pip install 'syne-tune[extra]'"
    )

File Path: syne_tune/tuner.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import json
import logging
import time
from collections import OrderedDict
from pathlib import Path
from typing import List, Callable, Tuple, Optional, Dict, Set
import dill as dill

from syne_tune.backend.trial_backend import TrialBackend
from syne_tune.backend.trial_status import Status, Trial
from syne_tune.config_space import to_dict, Domain
from syne_tune.constants import ST_TUNER_CREATION_TIMESTAMP, ST_TUNER_START_TIMESTAMP
from syne_tune.optimizer.scheduler import SchedulerDecision, TrialScheduler
from syne_tune.tuner_callback import TunerCallback, StoreResultsCallback
from syne_tune.tuning_status import TuningStatus, print_best_metric_found
from syne_tune.util import (
    RegularCallback,
    experiment_path,
    name_from_base,
    check_valid_sagemaker_name,
)

logger = logging.getLogger(__name__)

DEFAULT_SLEEP_TIME = 5.0


class Tuner:
    def __init__(
        self,
        trial_backend: TrialBackend,
        scheduler: TrialScheduler,
        stop_criterion: Callable[[TuningStatus], bool],
        n_workers: int,
        sleep_time: float = DEFAULT_SLEEP_TIME,
        results_update_interval: float = 10.0,
        print_update_interval: float = 30.0,
        max_failures: int = 1,
        tuner_name: Optional[str] = None,
        asynchronous_scheduling: bool = True,
        wait_trial_completion_when_stopping: bool = False,
        callbacks: Optional[List[TunerCallback]] = None,
        metadata: Optional[Dict] = None,
        suffix_tuner_name: bool = True,
    ):
        """
        Allows to run an tuning job, call `run` after initializing.
        :param trial_backend:
        :param scheduler:
        :param stop_criterion: the tuning stops when this predicates returns True, called each iteration with the
        current tuning status, for instance pass `stop_criterion=lambda status: status.num_trials_completed > 200`
        to stop after 200 completed jobs.
        :param n_workers: Number of workers used here. Note that the backend
            needs to support (at least) this number of workers to be run
            in parallel
        :param sleep_time: time to sleep when all workers are busy
        :param results_update_interval: frequency at which results are updated and stored in seconds
        :param max_failures: max failures allowed,
        :param tuner_name: name associated with the tuning experiment, default to the name of the entrypoint.
        It can only consists in alpha-digits characters, possibly separated by '-'. A postfix with a date time-stamp
        is added to ensure unicity.
        :param asynchronous_scheduling: whether to use asynchronous scheduling when scheduling new trials. If `True`,
        trials are scheduled as soon as a worker is available, if `False`, the tuner waits that all trials are finished
         before scheduling a new batch.
        :param wait_trial_completion_when_stopping: how to deal with running trials when stopping criterion is
        met. If `True`, the tuner waits that all trials are finished, if `False`, all trials are terminated.
        :param callbacks: called when events happens in the tuning loop such as when a result is seen, by default
        a callback that stores results every `results_update_interval` is used.
        :param metadata: dictionary of user-metadata that will be persistend in {tuner_path}/metadata.json, in addition
        to the metadata provided by the user, `SMT_TUNER_CREATION_TIMESTAMP` is always included which measures
        the time-stamp when the tuner started to run.
        :param suffix_tuner_name: If True, a timestamp is appended to the provided `tuner_name` that ensures uniqueness
        otherwise the name is left unchanged and is expected to be unique.
        """
        self.trial_backend = trial_backend
        self.scheduler = scheduler
        self.n_workers = n_workers
        self.sleep_time = sleep_time
        self.results_update_interval = results_update_interval
        self.stop_criterion = stop_criterion
        self.asynchronous_scheduling = asynchronous_scheduling
        self.wait_trial_completion_when_stopping = wait_trial_completion_when_stopping
        self.metadata = self._enrich_metadata(metadata)

        self.max_failures = max_failures
        self.print_update_interval = print_update_interval

        if tuner_name is not None:
            check_valid_sagemaker_name(tuner_name)
        else:
            tuner_name = Path(self.trial_backend.entrypoint_path()).stem.replace(
                "_", "-"
            )
        if suffix_tuner_name or tuner_name is None:
            self.name = name_from_base(tuner_name, default="st-tuner")
        else:
            self.name = tuner_name

        # we keep track of the last result seen to send it to schedulers when trials complete.
        self.last_seen_result_per_trial = {}
        self.trials_scheduler_stopped = set()
        self.tuner_path = Path(experiment_path(tuner_name=self.name))

        # inform the backend to the folder of the Tuner. This allows the local backend
        # to store the logs and tuner results in the same folder.
        self.trial_backend.set_path(results_root=self.tuner_path, tuner_name=self.name)
        self.callbacks = (
            callbacks if callbacks is not None else [self._default_callback()]
        )

        self.tuning_status = None

    def run(self):
        """
        Launches the tuning.
        :return: the tuning status when finished
        """
        try:
            logger.info(f"results of trials will be saved on {self.tuner_path}")

            if self.tuning_status is None:
                self.tuning_status = TuningStatus(
                    metric_names=self.scheduler.metric_names()
                )
            # prints the status every print_update_interval seconds
            self.status_printer = RegularCallback(
                call_seconds_frequency=self.print_update_interval,
                callback=lambda tuning_status: logger.info(
                    "tuning status (last metric is reported)\n" + str(tuning_status)
                ),
            )
            # saves the tuner every results_update_interval seconds
            self.tuner_saver = RegularCallback(
                callback=lambda tuner: tuner.save(),
                call_seconds_frequency=self.results_update_interval,
            )

            self.metadata[ST_TUNER_START_TIMESTAMP] = time.time()

            for callback in self.callbacks:
                callback.on_tuning_start(self)

            self.tuner_path.mkdir(exist_ok=True, parents=True)

            self._save_metadata()

            done_trials_statuses = OrderedDict()
            # `running_trial_ids` contains the ids of all trials currently running,
            # whether they were started from scratch or were resumed from a pausing
            # state
            running_trials_ids = set()

            config_space_exhausted = False
            stop_condition_reached = self._stop_condition()

            while (
                # we stop when either the stop condition is reached
                not stop_condition_reached
                # or when all trials are done if the wait_trial_completion is activated
                or self.wait_trial_completion_when_stopping
                and len(running_trials_ids) > 0
            ):
                for callback in self.callbacks:
                    callback.on_loop_start()

                new_done_trial_statuses, new_results = self._process_new_results(
                    running_trials_ids=running_trials_ids,
                )

                if new_results:
                    # Save tuner state only if there have been new results
                    self.tuner_saver(tuner=self)

                # update the list of done trials and remove those from `running_trials_ids`
                # Note: It is important to update `running_trials_ids` before
                # calling `_schedule_new_tasks`.
                # Otherwise, a trial can be registered as paused in
                # `_process_new_results`, and immediately be resumed in
                # `_schedule_new_tasks`. If `new_done_trial_statuses` is subtracted from
                # `running_trials_ids` afterwards only, this trial is removed from
                # `running_trials_ids` even though it is running. Also, its status remains
                # paused, because the next call of `_process_new_results` only considers
                # trials in `running_trials_ids`.
                done_trials_statuses.update(new_done_trial_statuses)
                running_trials_ids.difference_update(new_done_trial_statuses.keys())

                if (
                    config_space_exhausted
                    or self.wait_trial_completion_when_stopping
                    and stop_condition_reached
                ):
                    # if the search space is exhausted, we loop until the running trials are done or until the
                    # stop condition is reached
                    if len(running_trials_ids) > 0:
                        if config_space_exhausted:
                            logger.debug(
                                f"Configuration space exhausted, waiting for completion of running trials "
                                f"{running_trials_ids}"
                            )
                        else:
                            logger.debug(
                                f"Stopping criterion reached, waiting for completion of running trials "
                                f"{running_trials_ids}"
                            )
                        self._sleep()
                    else:
                        break
                else:
                    try:
                        self._schedule_new_tasks(running_trials_ids=running_trials_ids)
                    except StopIteration:
                        logger.info(
                            "Tuning is finishing as the whole configuration space got exhausted."
                        )
                        config_space_exhausted = True
                        print(
                            "Tuning is finishing as the whole configuration space got exhausted."
                        )

                self.status_printer(self.tuning_status)

                for callback in self.callbacks:
                    callback.on_loop_end()

                stop_condition_reached = self._stop_condition()
        except Exception as e:
            logger.error(
                "An error happened during the tuning, cleaning up resources and logging final resources "
                "before throwing the exception."
            )
            raise e
        finally:
            # graceful termination block called when the tuner reached its stop condition, when an error happened or
            # when the job got interrupted (can happen in spot-instances or when sending a SIGINT signal with ctrl+C).
            # the block displays the best configuration found and stops trials that may still be running.
            print_best_metric_found(
                tuning_status=self.tuning_status,
                metric_names=self.scheduler.metric_names(),
                mode=self.scheduler.metric_mode(),
            )

            # Callbacks (typically includes writing final results)
            for callback in self.callbacks:
                callback.on_tuning_end()

            # Serialize Tuner object
            self.save()

            logger.info("Stopping trials that may still be running.")
            self.trial_backend.stop_all()

            # notify tuning status that jobs were stopped without having to query their status in the backend since
            # we know that all trials were stopped
            self.tuning_status.mark_running_job_as_stopped()

            # in case too many errors were triggered, show log of last failed job and terminates with an error
            if self.tuning_status.num_trials_failed > self.max_failures:
                self._handle_failure(done_trials_statuses=done_trials_statuses)

            logger.info(
                f"Tuning finished, results of trials can be found on {self.tuner_path}"
            )

    def _sleep(self):
        time.sleep(self.sleep_time)
        for callback in self.callbacks:
            callback.on_tuning_sleep(self.sleep_time)

    @staticmethod
    def _set_metadata(metadata: Dict, name: str, value):
        if name in metadata:
            logger.warning(
                f"Entry {name} in metadata is used, but will be overwritten:\n"
                f"Old value: {metadata[name]}\n"
                f"Overwrite: {value}\n"
            )
        metadata[name] = value

    def _enrich_metadata(self, metadata: Dict):
        """
        :return: adds creation time stamp, metric names and mode, entrypoint and backend to the metadata.
        """
        res = metadata if metadata is not None else dict()
        self._set_metadata(res, ST_TUNER_CREATION_TIMESTAMP, time.time())
        self._set_metadata(res, "metric_names", self.scheduler.metric_names())
        self._set_metadata(res, "metric_mode", self.scheduler.metric_mode())
        self._set_metadata(res, "entrypoint", self.trial_backend.entrypoint_path().stem)
        self._set_metadata(res, "backend", str(type(self.trial_backend).__name__))
        self._set_metadata(
            res, "scheduler_name", str(self.scheduler.__class__.__name__)
        )
        config_space_json = json.dumps(
            {
                k: to_dict(v) if isinstance(v, Domain) else v
                for k, v in self.scheduler.config_space.items()
            }
        )
        self._set_metadata(res, "config_space", config_space_json)
        return res

    def _save_metadata(self):
        with open(self.tuner_path / "metadata.json", "w") as f:
            json.dump(self.metadata, f)

    def _stop_condition(self) -> bool:
        return (
            self.stop_criterion(self.tuning_status)
            or self.tuning_status.num_trials_failed > self.max_failures
        )

    def _process_new_results(self, running_trials_ids: Set[int]):
        """
        Communicates new results from the backend to the scheduler
        :param running_trials_ids: list of trials currently running
        :return: dictionary from trial-id to status of trials that are not running and new results observed
        """

        # fetch new results
        trial_status_dict, new_results = self.trial_backend.fetch_status_results(
            trial_ids=list(running_trials_ids)
        )

        for callback in self.callbacks:
            callback.on_fetch_status_results(
                trial_status_dict=trial_status_dict, new_results=new_results
            )

        assert len(running_trials_ids) <= self.n_workers

        # Gets list of trials that are done with the new results.
        # The trials can be finished for different reasons:
        # - they completed,
        # - they were stopped independently of the scheduler, e.g. due to a timeout argument or a manual interruption
        # - scheduler decided to interrupt them.
        # Note: `done_trials` includes trials which are paused.
        done_trials_statuses = self._update_running_trials(
            trial_status_dict, new_results, callbacks=self.callbacks
        )
        trial_status_dict.update(done_trials_statuses)

        # update status with new results and all done trials
        self.tuning_status.update(
            trial_status_dict=trial_status_dict, new_results=new_results
        )

        return done_trials_statuses, new_results

    def _schedule_new_tasks(self, running_trials_ids: Set[int]):
        """
        Schedules new tasks if resources are available or sleep.
        :param running_trials_ids: set if trial-ids currently running, gets updated if new trials are scheduled.
        """
        running_trials_threshold = self.n_workers if self.asynchronous_scheduling else 1
        num_running_trials = len(running_trials_ids)
        if num_running_trials >= running_trials_threshold:
            # Note: For synchronous scheduling, we need to sleep here if at
            # least one worker is busy
            logger.debug(
                f"{num_running_trials} of {self.n_workers} workers are "
                f"busy, wait for {self.sleep_time} seconds"
            )
            self._sleep()

        else:
            # Schedule as many trials as we have free workers
            for i in range(self.n_workers - num_running_trials):
                trial_id = self._schedule_new_task()
                running_trials_ids.add(trial_id)

    def _schedule_new_task(self) -> Optional[int]:
        """
        Schedules a new task according to scheduler suggestion.
        :return: the trial-id of the task suggested, None if the scheduler was done.
        """
        trial_id = self.trial_backend.new_trial_id()
        suggestion = self.scheduler.suggest(trial_id=trial_id)
        if suggestion is None:
            logger.info("Searcher ran out of candidates, tuning job is stopping.")
            raise StopIteration
        elif suggestion.spawn_new_trial_id:
            # we schedule a new trial, possibly using the checkpoint of `checkpoint_trial_id`
            # if given.
            trial = self.trial_backend.start_trial(
                config=suggestion.config.copy(),
                checkpoint_trial_id=suggestion.checkpoint_trial_id,
            )
            self.scheduler.on_trial_add(trial=trial)
            logger.info(f"(trial {trial_id}) - scheduled {suggestion}")
            return trial_id
        else:
            # suggestion is a trial_id to resume, with possibly a new configuration
            log_msg = f"Resuming trial {suggestion.checkpoint_trial_id}"
            if suggestion.config is not None:
                log_msg += f" with new_config = {suggestion.config}"
            logger.info(log_msg)
            self.trial_backend.resume_trial(
                trial_id=suggestion.checkpoint_trial_id, new_config=suggestion.config
            )
            return suggestion.checkpoint_trial_id

    def _handle_failure(self, done_trials_statuses: Dict[int, Tuple[Trial, str]]):
        logger.error(f"Stopped as {self.max_failures} failures were reached")
        for trial_id, (_, status) in done_trials_statuses.items():
            if status == Status.failed:
                logger.error(f"showing log of first failure")
                stdout = "".join(self.trial_backend.stdout(trial_id))
                stderr = "".join(self.trial_backend.stderr(trial_id))
                logger.error(stdout)
                logger.error(stderr)
                raise ValueError(f"Trial - {trial_id} failed")

    def save(self, folder: Optional[str] = None):
        if folder is None:
            tuner_serialized_path = self.tuner_path / "tuner.dill"
        else:
            tuner_serialized_path = Path(folder) / "tuner.dill"
        with open(tuner_serialized_path, "wb") as f:
            logger.debug(f"saving tuner in {tuner_serialized_path}")
            dill.dump(self, f)
            self.trial_backend.on_tuner_save()  # callback

    @staticmethod
    def load(tuner_path: Optional[str]):
        with open(Path(tuner_path) / "tuner.dill", "rb") as f:
            tuner = dill.load(f)
            tuner.tuner_path = Path(experiment_path(tuner_name=tuner.name))
            return tuner

    def _update_running_trials(
        self,
        trial_status_dict: Dict[int, Tuple[Trial, str]],
        new_results: List[Tuple[int, Dict]],
        callbacks: List[TunerCallback],
    ) -> Dict[int, Tuple[Trial, str]]:
        """
        Updates schedulers with new results and sends decision to stop/pause trials to the backend.
        :return: dictionary mapping trial-ids that are finished to status.
        Trials can be finished because:
         1) the scheduler decided to stop or pause.
         2) the trial failed.
         3) the trial was stopped independently of the scheduler, e.g. due to a timeout argument or a manual interruption.
         4) the trial completed.
        """
        # gets the list of jobs from running_jobs that are done
        done_trials = {}

        for trial_id, result in new_results:
            if trial_id not in done_trials:
                trial, status = trial_status_dict[trial_id]

                # communicate new result to the searcher and the scheduler
                self.last_seen_result_per_trial[trial_id] = result
                decision = self.scheduler.on_trial_result(trial=trial, result=result)

                for callback in callbacks:
                    callback.on_trial_result(
                        trial=trial,
                        status=status,
                        result=result,
                        decision=decision,
                    )

                if decision == SchedulerDecision.STOP:
                    if status != Status.completed:
                        # we override the status immediately, this avoids calling the backend status another time to
                        # update after the change which may be expensive
                        status = Status.stopped
                        self.trial_backend.stop_trial(trial_id=trial_id, result=result)
                    self.scheduler.on_trial_remove(trial=trial)
                    done_trials[trial_id] = (trial, status)
                    self.trials_scheduler_stopped.add(trial_id)

                elif decision == SchedulerDecision.PAUSE:
                    status = Status.paused
                    self.trial_backend.pause_trial(trial_id=trial_id, result=result)
                    self.scheduler.on_trial_remove(trial=trial)
                    done_trials[trial_id] = (trial, status)

        for trial_id, (trial, status) in trial_status_dict.items():
            # Status "completed", "stopped" and "failed" are signaled to scheduler.
            # Status "in_progress" and "stopping" are not signaled, although the first one could be added
            # to notify the scheduler of pending runtimes (even in the absence of new results).

            if status == Status.completed:
                # since the code above updates `trial_status_dict[trial_id]` after a pause/stop scheduling decision
                # this callback is never called after a pause/stop scheduler decision.
                if (
                    trial_id not in done_trials
                    or done_trials[trial_id][1] != Status.paused
                ):
                    logger.info(f"Trial trial_id {trial_id} completed.")
                assert (
                    trial_id in self.last_seen_result_per_trial
                ), f"trial {trial_id} completed and no metrics got observed"
                last_result = self.last_seen_result_per_trial[trial_id]
                if not trial_id in done_trials:
                    self.scheduler.on_trial_complete(trial, last_result)
                for callback in callbacks:
                    callback.on_trial_complete(trial, last_result)
                done_trials[trial_id] = (trial, status)

            if status == Status.failed:
                logger.info(f"Trial trial_id {trial_id} failed.")
                self.scheduler.on_trial_error(trial)
                done_trials[trial_id] = (trial, status)

            # For the case when the trial is stopped independently of the scheduler, we choose to use
            # scheduler.on_trial_error(...) since it was not the scheduler's decision to stop the trial.
            if (
                status == Status.stopped
                and trial_id not in self.trials_scheduler_stopped
            ):
                logger.info(
                    f"Trial trial_id {trial_id} was stopped independently of the scheduler."
                )
                self.scheduler.on_trial_error(trial)
                done_trials[trial_id] = (trial, status)

        return done_trials

    def _default_callback(self):
        """
        :return: default callback to store results
        """
        return StoreResultsCallback()

File Path: syne_tune/tuner_callback.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numbers
import os
from time import perf_counter
from typing import Dict, List, Tuple, Optional
import copy
import logging
import pandas as pd

from syne_tune.backend.trial_status import Trial
from syne_tune.constants import ST_DECISION, ST_TRIAL_ID, ST_STATUS, ST_TUNER_TIME
from syne_tune.util import RegularCallback

logger = logging.getLogger(__name__)


class TunerCallback:
    def on_tuning_start(self, tuner):
        pass

    def on_tuning_end(self):
        pass

    def on_loop_start(self):
        pass

    def on_loop_end(self):
        pass

    def on_fetch_status_results(
        self,
        trial_status_dict: Tuple[Dict[int, Tuple[Trial, str]]],
        new_results: List[Tuple[int, Dict]],
    ):
        """
        Called with the results of `backend.fetch_status_results`.
        """
        pass

    def on_trial_complete(self, trial: Trial, result: Dict):
        """
        Called when a trial completes.
        :param trial: trial that just completed.
        :param result: last result obtained.
        :return:
        """
        pass

    def on_trial_result(self, trial: Trial, status: str, result: Dict, decision: str):
        """
        Called when a new result is observed.
        :param trial:
        :param status: a string representing the status that is one value of `trial_status.Status`, such as
        `trial_status.Status.completed`.
        :param result:
        :param decision: decision that was returned by the scheduler
        :return:
        """
        pass

    def on_tuning_sleep(self, sleep_time: float):
        """
        Called when the tuner is put to sleep when no worker is available.
        :param sleep_time:
        :return:
        """
        pass


class StoreResultsCallback(TunerCallback):
    def __init__(
        self,
        add_wallclock_time: bool = True,
    ):
        """
        Minimal callback that enables plotting results over time,
        additional callback functionalities will be added as well as example to plot results over time.

        :param add_wallclock_time: whether to add wallclock time to results.
        """
        self.results = []

        self.csv_file = None
        self.save_results_at_frequency = None
        self.add_wallclock_time = add_wallclock_time
        self._start_time_stamp = None

    def _set_time_fields(self, result: Dict):
        """
        Note that we only add wallclock time to the result if this has not
        already been done (by the back-end)
        """
        if self._start_time_stamp is not None and ST_TUNER_TIME not in result:
            result[ST_TUNER_TIME] = perf_counter() - self._start_time_stamp

    def on_trial_result(self, trial: Trial, status: str, result: Dict, decision: str):
        assert (
            self.save_results_at_frequency is not None
        ), "on_tuning_start must always be called before on_trial_result."
        result = copy.copy(result)
        result[ST_DECISION] = decision
        result[ST_STATUS] = status
        result[ST_TRIAL_ID] = trial.trial_id

        for key in trial.config:
            result[f"config_{key}"] = trial.config[key]

        self._set_time_fields(result)

        self.results.append(result)

        if self.csv_file is not None:
            self.save_results_at_frequency()

    def store_results(self):
        if self.csv_file is not None:
            self.dataframe().to_csv(self.csv_file, index=False)

    def dataframe(self) -> pd.DataFrame:
        return pd.DataFrame(self.results)

    def on_tuning_start(self, tuner):
        # we set the path of the csv file once the tuner is created since the path may change when the tuner is stop
        # and resumed again on a different machine.
        self.csv_file = str(tuner.tuner_path / "results.csv.zip")

        # we only save results every `results_update_frequency` seconds as this operation
        # may be expensive on remote storage.
        self.save_results_at_frequency = RegularCallback(
            lambda: self.store_results(),
            call_seconds_frequency=tuner.results_update_interval,
        )
        if self.add_wallclock_time:
            self._start_time_stamp = perf_counter()

    def on_tuning_end(self):
        # store the results in case some results were not committed yet (since they are saved every
        # `results_update_interval` seconds)
        self.store_results()


class TensorboardCallback(TunerCallback):
    def __init__(
        self,
        ignore_metrics: Optional[List[str]] = None,
        target_metric: Optional[str] = None,
        mode: Optional[str] = "min",
    ):
        """
        Simple callback that logs metric reported in the train function such that we can visualize with Tensorboard.

        :param ignore_metrics: Defines which metrics should be ignored. If None, all metrics are reported
         to Tensorboard.
        :param target_metric: Defines the metric we aim to optimize. If this argument is set, we report
        the cumulative optimum of this metric as well as the optimal hyperparameters we have found so far.
        :param mode: Determined whether we maximize ('max') or minimize ('min') the target metric.
        """
        self.results = []

        if ignore_metrics is None:
            self.ignore_metrics = []
        else:
            self.ignore_metrics = ignore_metrics

        self.curr_best_value = None
        self.curr_best_config = None

        self.start_time_stamp = None
        self.writer = None
        self.iter = None
        self.mode = mode
        self.target_metric = target_metric
        self.trial_ids = set()

        self.metric_sign = -1 if mode == "max" else 1

    def _set_time_fields(self, result: Dict):
        """
        Note that we only add wallclock time to the result if this has not
        already been done (by the back-end)
        """
        if self.start_time_stamp is not None and ST_TUNER_TIME not in result:
            result[ST_TUNER_TIME] = perf_counter() - self.start_time_stamp

    def on_trial_result(self, trial: Trial, status: str, result: Dict, decision: str):
        walltime = result[ST_TUNER_TIME]
        self._set_time_fields(result)

        if self.target_metric is not None:

            assert (
                self.target_metric in result
            ), f"{self.target_metric} was not reported back to Syne tune"
            new_result = self.metric_sign * result[self.target_metric]

            if self.curr_best_value is None or self.curr_best_value > new_result:
                self.curr_best_value = new_result
                self.curr_best_config = trial.config
                self.writer.add_scalar(
                    self.target_metric, result[self.target_metric], self.iter, walltime
                )

            else:
                opt = self.metric_sign * self.curr_best_value
                self.writer.add_scalar(self.target_metric, opt, self.iter, walltime)

            for key, value in self.curr_best_config.items():
                if isinstance(value, numbers.Number):
                    self.writer.add_scalar(f"optimal_{key}", value, self.iter, walltime)
                else:
                    self.writer.add_text(
                        f"optimal_{key}", str(value), self.iter, walltime
                    )

        for metric in result:
            if metric not in self.ignore_metrics:
                self.writer.add_scalar(metric, result[metric], self.iter, walltime)

        for key, value in trial.config.items():
            if isinstance(value, numbers.Number):
                self.writer.add_scalar(key, value, self.iter, walltime)
            else:
                self.writer.add_text(key, str(value), self.iter, walltime)

        self.writer.add_scalar("runtime", result[ST_TUNER_TIME], self.iter, walltime)

        self.trial_ids.add(trial.trial_id)
        self.writer.add_scalar(
            "number_of_trials",
            len(self.trial_ids),
            self.iter,
            walltime=walltime,
            display_name="total number of trials",
        )

        self.iter += 1

    def _create_summary_writer(self):
        try:
            from tensorboardX import SummaryWriter
        except ImportError:
            logger.error(
                "TensoboardX is not installed. You can install it via: pip install tensorboardX"
            )
            raise
        return SummaryWriter(self.output_path)

    def on_tuning_start(self, tuner):
        self.output_path = os.path.join(tuner.tuner_path, "tensorboard_output")
        self.writer = self._create_summary_writer()
        self.iter = 0
        self.start_time_stamp = perf_counter()
        logger.info(
            f"Logging tensorboard information at {self.output_path}, to visualize results, run\n"
            f"tensorboard --logdir {self.output_path}"
        )

    def on_tuning_end(self):
        self.writer.close()
        logger.info(
            f"Tensorboard information has been logged at {self.output_path}, to visualize results, run\n"
            f"tensorboard --logdir {self.output_path}"
        )

    def __getstate__(self):
        state = {
            "results": self.results,
            "ignore_metrics": self.ignore_metrics,
            "curr_best_value": self.curr_best_value,
            "curr_best_config": self.curr_best_config,
            "start_time_stamp": self.start_time_stamp,
            "iter": self.iter,
            "mode": self.mode,
            "target_metric": self.target_metric,
            "trial_ids": self.trial_ids,
            "metric_sign": self.metric_sign,
            "output_path": self.output_path,
        }
        return state

    def __setstate__(self, state):
        super().__init__(
            ignore_metrics=state["ignore_metrics"],
            target_metric=state["target_metric"],
            mode=state["mode"],
        )
        self.results = state["results"]
        self.curr_best_value = state["curr_best_value"]
        self.curr_best_config = state["curr_best_config"]
        self.start_time_stamp = state["start_time_stamp"]
        self.iter = state["iter"]

        self.trial_ids = state["trial_ids"]
        self.metric_sign = state["metric_sign"]
        self.output_path = state["output_path"]
        self.writer = self._create_summary_writer()

File Path: syne_tune/tuning_status.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numbers
import logging
import time
from collections import defaultdict, OrderedDict
from typing import List, Dict, Tuple
import pandas as pd
from numpy import inf as np_inf

from syne_tune.backend.trial_status import Status, Trial
from syne_tune.constants import ST_WORKER_TIME, ST_WORKER_COST


class MetricsStatistics:
    def __init__(self):
        """
        Allows to maintain simple running statistics (min/max/sum/count) of metrics provided.
        Statistics are tracked for numeric types only. Types of first added metrics define its types.
        """
        self.metric_names = []
        self.count = 0
        self.min_metrics = {}
        self.max_metrics = {}
        self.sum_metrics = {}
        self.last_metrics = {}
        self.is_numeric = {}

    def add(self, metrics: Dict):
        for metric_name, current_metric in metrics.items():
            if metric_name in self.is_numeric:
                if self.is_numeric[metric_name] != isinstance(
                    current_metric, numbers.Number
                ):
                    logging.warning(
                        f"Numeric and non-numeric values reported for metric {metric_name}."
                    )
            if self.is_numeric.get(metric_name, True):
                self.is_numeric[metric_name] = isinstance(
                    current_metric, numbers.Number
                )
                if self.is_numeric[metric_name]:
                    self.min_metrics[metric_name] = min(
                        self.min_metrics.get(metric_name, np_inf), current_metric
                    )
                    self.max_metrics[metric_name] = max(
                        self.max_metrics.get(metric_name, -np_inf), current_metric
                    )
                    self.sum_metrics[metric_name] = (
                        self.sum_metrics.get(metric_name, 0) + current_metric
                    )
        self.metric_names = list(self.min_metrics.keys())
        self.last_metrics = metrics
        self.count += 1


class TuningStatus:
    """
    Information of a tuning job to display as progress or to use to decide whether to stop the tuning job.
    """

    def __init__(self, metric_names: List[str]):
        self.metric_names = metric_names
        self.start_time = time.perf_counter()

        self.overall_metric_statistics = MetricsStatistics()
        self.trial_metric_statistics = defaultdict(lambda: MetricsStatistics())

        self.last_trial_status_seen = OrderedDict()
        self.trial_rows = OrderedDict({})

    def update(
        self,
        trial_status_dict: Dict[int, Tuple[Trial, str]],
        new_results: List[Tuple[int, Dict]],
    ):
        """
        Updates the tuning status given new statuses and results.
        """

        self.last_trial_status_seen.update(
            {k: v[1] for k, v in trial_status_dict.items()}
        )

        for trial_id, new_result in new_results:
            self.overall_metric_statistics.add(new_result)
            self.trial_metric_statistics[trial_id].add(new_result)

        for trial_id, (trial, status) in trial_status_dict.items():
            num_metrics = self.trial_metric_statistics[trial_id].count
            row = {
                "trial_id": trial_id,
                "status": status,
                "iter": num_metrics,
            }
            row.update(trial.config)
            row.update(self.trial_metric_statistics[trial_id].last_metrics)

            if ST_WORKER_TIME in self.trial_metric_statistics[trial_id].max_metrics:
                row["worker-time"] = self.trial_metric_statistics[trial_id].max_metrics[
                    ST_WORKER_TIME
                ]
            if ST_WORKER_COST in self.trial_metric_statistics[trial_id].max_metrics:
                row["worker-cost"] = self.trial_metric_statistics[trial_id].max_metrics[
                    ST_WORKER_COST
                ]

            self.trial_rows[trial_id] = row

    def mark_running_job_as_stopped(self):
        """
        Update the status of all trials still running to be marked as stop.
        """
        self.last_trial_status_seen = {
            k: v if v != Status.in_progress else Status.stopped
            for k, v in self.last_trial_status_seen.items()
        }
        for trial_id, row in self.trial_rows.items():
            if row["status"] == Status.in_progress:
                row["status"] = Status.stopped

    @property
    def num_trials_started(self):
        return len(self.last_trial_status_seen)

    def _num_trials(self, status: str):
        return sum(
            trial_status == status
            for trial_status in self.last_trial_status_seen.values()
        )

    @property
    def num_trials_completed(self):
        return self._num_trials(status=Status.completed)

    @property
    def num_trials_failed(self):
        return self._num_trials(status=Status.failed)

    @property
    def num_trials_finished(self):
        """
        :return: number of trials that finished, e.g. that completed, were stopped or are stopping, or failed
        """
        # note it may be inefficient to query several times the dataframe in case a very large number of jobs are
        #  present, we could query the dataframe only once
        return (
            self._num_trials(status=Status.completed)
            + self._num_trials(status=Status.stopped)
            + self._num_trials(status=Status.stopping)
            + self._num_trials(status=Status.failed)
        )

    @property
    def num_trials_running(self):
        return self._num_trials(status=Status.in_progress)

    @property
    def wallclock_time(self):
        """
        :return: the wallclock time spent in the tuner
        """
        return time.perf_counter() - self.start_time

    @property
    def user_time(self):
        """
        :return: the total user time spent in the workers
        """
        if ST_WORKER_TIME in self.overall_metric_statistics.metric_names:
            usertime_per_trial = [
                metric.max_metrics.get(ST_WORKER_TIME, 0)
                for trial, metric in self.trial_metric_statistics.items()
            ]
            return sum(usertime_per_trial)
        else:
            return 0

    @property
    def cost(self):
        """
        :return: the estimated dollar-cost spent while tuning
        """
        if ST_WORKER_COST in self.overall_metric_statistics.metric_names:
            cost_per_trial = [
                metric.max_metrics.get(ST_WORKER_COST, 0)
                for trial, metric in self.trial_metric_statistics.items()
            ]
            return sum(cost_per_trial)
        else:
            return 0.0

    def get_dataframe(self) -> pd.DataFrame:
        return pd.DataFrame(self.trial_rows.values())

    def __str__(self):
        num_running = self.num_trials_running
        num_finished = self.num_trials_started - num_running

        if len(self.trial_rows) > 0:
            df = self.get_dataframe()
            cols = [col for col in df.columns if not col.startswith("st_")]
            res_str = df.loc[:, cols].to_string(index=False, na_rep="-") + "\n"
        else:
            res_str = ""
        res_str += (
            f"{num_running} trials running, "
            f"{num_finished} finished ({self.num_trials_completed} until the end), "
            f"{self.wallclock_time:.2f}s wallclock-time"
        )
        # f"{self.user_time:.2f}s approximated user-time"
        cost = self.cost
        if cost is not None and cost > 0.0:
            res_str += f", ${cost:.2f} estimated cost"
        res_str += "\n"
        return res_str


def print_best_metric_found(
    tuning_status: TuningStatus, metric_names: List[str], mode: str
) -> Tuple[int, float]:
    """
    Prints trial status summary and the best metric found.
    :param tuning_status:
    :param metric_names:
    :param mode:
    :return: trial-id and value of the best metric found
    """
    if tuning_status.overall_metric_statistics.count == 0:
        return
    # only plot results of the best first metric for now in summary, plotting the optimal metrics for multiple
    # objectives would require to display the Pareto set.
    metric_name = metric_names[0]
    print("-" * 20)
    print(f"Resource summary (last result is reported):\n{str(tuning_status)}")
    if mode == "min":
        metric_per_trial = [
            (trial_id, stats.min_metrics.get(metric_name, np_inf))
            for trial_id, stats in tuning_status.trial_metric_statistics.items()
        ]
        metric_per_trial = sorted(metric_per_trial, key=lambda x: x[1])
    else:
        metric_per_trial = [
            (trial_id, stats.max_metrics.get(metric_name, -np_inf))
            for trial_id, stats in tuning_status.trial_metric_statistics.items()
        ]
        metric_per_trial = sorted(metric_per_trial, key=lambda x: -x[1])
    best_trialid, best_metric = metric_per_trial[0]
    print(f"{metric_name}: best {best_metric} for trial-id {best_trialid}")
    print("-" * 20)
    return best_trialid, best_metric

File Path: syne_tune/util.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import os
import re
import string
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Optional
from time import perf_counter
from contextlib import contextmanager

from syne_tune.constants import SYNE_TUNE_DEFAULT_FOLDER, SYNE_TUNE_ENV_FOLDER
from syne_tune.try_import import try_import_aws_message

try:
    import sagemaker
except ImportError:
    print(try_import_aws_message())


class RegularCallback:
    def __init__(self, callback, call_seconds_frequency: float):
        """
        Allows to call the callback function at most once every `call_seconds_frequency` seconds.
        :param callback:
        :param call_seconds_frequency:
        """
        self.start = datetime.now()
        self.frequency = call_seconds_frequency
        self.callback = callback

    def __call__(self, *args, **kwargs):
        seconds_since_last_call = (datetime.now() - self.start).seconds
        if seconds_since_last_call > self.frequency:
            self.start = datetime.now()
            self.callback(*args, **kwargs)


def experiment_path(
    tuner_name: Optional[str] = None, local_path: Optional[str] = None
) -> Path:
    f"""
    Return the path of an experiment which is used both by the Tuner and to collect results of experiments.
    :param tuner_name: name of a tuning experiment
    :param local_path: local path where results should be saved when running locally outside of Sagemaker.
    If not specified, then the environment variable `"SYNETUNE_FOLDER"` is used if defined otherwise
    `~/syne-tune/` is used. Defining the enviroment variable `"SYNETUNE_FOLDER"` allows to override the default path.
    :return: path where to write logs and results for Syne Tune tuner. On Sagemaker, results are written
    under "/opt/ml/checkpoints/" so that files are persisted continuously by Sagemaker.
    """
    is_sagemaker = "SM_MODEL_DIR" in os.environ
    if is_sagemaker:
        # if SM_MODEL_DIR is present in the environment variable, this means that we are running on Sagemaker
        # we use this path to store results as it is persisted by Sagemaker.
        sagemaker_path = Path("/opt/ml/checkpoints")
        if tuner_name is not None:
            sagemaker_path = sagemaker_path / tuner_name
        return sagemaker_path
    else:
        # means we are running on a local machine, we store results in a local path
        if local_path is None:
            if SYNE_TUNE_ENV_FOLDER in os.environ:
                local_path = Path(os.environ[SYNE_TUNE_ENV_FOLDER]).expanduser()
            else:
                local_path = Path(f"~/{SYNE_TUNE_DEFAULT_FOLDER}").expanduser()
        else:
            local_path = Path(local_path)
        if tuner_name is not None:
            local_path = local_path / tuner_name
        return local_path


def s3_experiment_path(
    s3_bucket: Optional[str] = None,
    experiment_name: Optional[str] = None,
    tuner_name: Optional[str] = None,
) -> str:
    """
    Returns S3 path for storing results and checkpoints.

    :param s3_bucket: If not given,, the default bucket for the SageMaker
        session is used
    :param experiment_name: If given, this is used as first directory
    :param tuner_name: If given, this is used as second directory
    :return: S3 path
    """
    if s3_bucket is None:
        s3_bucket = sagemaker.Session().default_bucket()
    s3_path = f"s3://{s3_bucket}/{SYNE_TUNE_DEFAULT_FOLDER}"
    for part in (experiment_name, tuner_name):
        if part is not None:
            s3_path += "/" + part
    return s3_path


def check_valid_sagemaker_name(name: str):
    assert re.compile("^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}$").match(
        name
    ), f"{name} should consists in alpha-digits possibly separated by character -"


def name_from_base(base: Optional[str], default: str, max_length: int = 63) -> str:
    """Append a timestamp to the provided string.

    This function assures that the total length of the resulting string is
    not longer than the specified max length, trimming the input parameter if
    necessary.

    Args:
        base (str): String used as prefix to generate the unique name.
        default (str): String used in case base is None.
        max_length (int): Maximum length for the resulting string (default: 63).

    Returns:
        str: Input parameter with appended timestamp.
    """
    if base is None:
        check_valid_sagemaker_name(default)
        base = default
    else:
        check_valid_sagemaker_name(base)

    moment = time.time()
    moment_ms = repr(moment).split(".")[1][:3]
    timestamp = time.strftime(
        "%Y-%m-%d-%H-%M-%S-{}".format(moment_ms), time.gmtime(moment)
    )
    trimmed_base = base[: max_length - len(timestamp) - 1]
    return "{}-{}".format(trimmed_base, timestamp)


def random_string(length: int) -> str:
    pool = string.ascii_letters + string.digits
    return "".join(random.choice(pool) for _ in range(length))


def repository_root_path() -> Path:
    """
    :return: Returns path including `syne_tune`, `examples`,
        `benchmarking`
    """
    return Path(__file__).parent.parent


def script_checkpoint_example_path() -> Path:
    """
    Util to get easily the name of an example file
    :return:
    """
    path = (
        repository_root_path()
        / "examples"
        / "training_scripts"
        / "checkpoint_example"
        / "checkpoint_example.py"
    )
    assert path.exists()
    return path


def script_height_example_path():
    """
    Util to get easily the name of an example file
    :return:
    """
    path = (
        repository_root_path()
        / "examples"
        / "training_scripts"
        / "height_example"
        / "train_height.py"
    )
    assert path.exists()
    return path


@contextmanager
def catchtime(name: str) -> float:
    start = perf_counter()
    try:
        print(f"start: {name}")
        yield lambda: perf_counter() - start
    finally:
        print(f"Time for {name}: {perf_counter() - start:.4f} secs")

File Path: tst/blackbox_repository/test_blackbox.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import tempfile

import numpy as np
import pandas as pd

import syne_tune.config_space as sp

from syne_tune.blackbox_repository import BlackboxOffline
from syne_tune.blackbox_repository.blackbox import from_function
from syne_tune.blackbox_repository.blackbox_tabular import BlackboxTabular
from syne_tune.blackbox_repository.blackbox_offline import (
    deserialize as deserialize_offline,
)
from syne_tune.blackbox_repository.blackbox_tabular import (
    deserialize as deserialize_tabular,
)
from syne_tune.blackbox_repository.blackbox_offline import (
    serialize as serialize_offline,
)
from syne_tune.blackbox_repository.blackbox_tabular import (
    serialize as serialize_tabular,
)


n = 10
x1 = np.arange(n)
x2 = np.arange(n)[::-1]

cs = {
    "hp_x1": sp.randint(0, n),
    "hp_x2": sp.randint(0, n),
}

n_epochs = 5
cs_fidelity = {
    "hp_epoch": sp.randint(0, n_epochs),
}


def test_blackbox_from_function():
    def eval_fun(config, fidelity, seed):
        return {"metric_rmse": config["hp_x1"] * config["hp_x2"]}

    blackbox = from_function(configuration_space=cs, eval_fun=eval_fun)
    for u, v in zip(x1, x2):
        res = blackbox.objective_function({"hp_x1": u, "hp_x2": v})
        assert res["metric_rmse"] == u * v


def test_blackbox_dataframe_call():

    y = x1 * x2
    data = np.stack([x1, x2, y]).T
    df = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2", "metric_rmse"])
    blackbox = BlackboxOffline(df_evaluations=df, configuration_space=cs)

    for u, v in zip(x1, x2):
        res = blackbox.objective_function({"hp_x1": u, "hp_x2": v})
        assert res["metric_rmse"] == u * v


def test_blackbox_fidelity():

    # build dummy values for fidelities
    fidelities = []
    for fidelity in range(n_epochs):
        dummy_y = x1 * x2 + fidelity
        fidelity_vec = np.ones_like(x1) * fidelity
        fidelities.append(np.stack([x1, x2, fidelity_vec, dummy_y]).T)
    data = np.vstack(fidelities)

    df = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2", "hp_epoch", "metric_rmse"])

    blackbox = BlackboxOffline(
        df_evaluations=df, configuration_space=cs, fidelity_space=cs_fidelity
    )

    for u, v in zip(x1, x2):
        for epoch in range(n_epochs):
            res = blackbox.objective_function(
                {"hp_x1": u, "hp_x2": v}, {"hp_epoch": epoch}
            )
            assert res["metric_rmse"] == u * v + epoch

    # check that blackbox can be called with configuration instead of dict
    config = {k: v.sample() for k, v in blackbox.configuration_space.items()}
    config["hp_x1"] = u
    config["hp_x2"] = v
    res = blackbox.objective_function(config, {"hp_epoch": epoch})
    assert res["metric_rmse"] == u * v + epoch

    # check that blackbox can be called with fidelity value instead of dict
    config = {k: v.sample() for k, v in blackbox.configuration_space.items()}
    config["hp_x1"] = u
    config["hp_x2"] = v
    res = blackbox.objective_function(config, epoch)
    assert res["metric_rmse"] == u * v + epoch

    # check that blackbox can be called with fidelity value instead of dict
    config = {k: v.sample() for k, v in blackbox.configuration_space.items()}
    config["hp_x1"] = u
    config["hp_x2"] = v
    res = blackbox(config, epoch)
    assert res["metric_rmse"] == u * v + epoch


def test_blackbox_seed():
    # build dummy values for seeds
    n_seeds = 4
    seeds = []
    for seed in range(n_seeds):
        dummy_y = x1 * x2 + seed
        seed_vec = np.ones_like(x1) * seed
        seeds.append(np.stack([x1, x2, seed_vec, dummy_y]).T)
    data = np.vstack(seeds)

    df = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2", "seed", "metric_rmse"])
    blackbox = BlackboxOffline(
        df_evaluations=df, configuration_space=cs, seed_col="seed"
    )

    for u, v in zip(x1, x2):
        for seed in range(n_seeds):
            res = blackbox.objective_function({"hp_x1": u, "hp_x2": v}, seed=seed)
            assert res["metric_rmse"] == u * v + seed


def test_blackbox_offline_serialization():
    y = x1 * x2
    data = np.stack([x1, x2, y]).T
    df = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2", "metric_rmse"])

    blackbox = BlackboxOffline(df_evaluations=df, configuration_space=cs)

    with tempfile.TemporaryDirectory() as tmpdirname:
        print(f"serializing and deserializing blackbox in folder {tmpdirname}")
        serialize_offline({"task": blackbox}, tmpdirname)
        blackbox_deserialized = deserialize_offline(tmpdirname)["task"]
        for u, v in zip(x1, x2):
            res = blackbox_deserialized.objective_function({"hp_x1": u, "hp_x2": v})
            assert res["metric_rmse"] == u * v


def test_blackbox_offline_fidelities():
    data = np.concatenate(
        [
            np.stack([x1, x2, x1 * x2, np.ones_like(x1, dtype=np.int)], axis=1),
            np.stack(
                [x1, x2, 0.5 * x1 * x2, 2 * np.ones_like(x1, dtype=np.int)], axis=1
            ),
        ],
        axis=0,
    )
    df = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2", "metric_rmse", "step"])

    blackbox = BlackboxOffline(
        df_evaluations=df,
        configuration_space=cs,
        fidelity_space=dict(step=sp.randint(1, 2)),
    )

    with tempfile.TemporaryDirectory() as tmpdirname:
        print(f"serializing and deserializing blackbox in folder {tmpdirname}")
        for u, v in zip(x1, x2):
            res = blackbox.objective_function({"hp_x1": u, "hp_x2": v}, fidelity=1)
            assert res["metric_rmse"] == u * v

            res = blackbox.objective_function({"hp_x1": u, "hp_x2": v}, fidelity=2)
            assert res["metric_rmse"] == 0.5 * u * v

            res = blackbox.objective_function({"hp_x1": u, "hp_x2": v}, fidelity=None)
            # Returns a tensor with shape (num_fidelities, num_objectives)
            assert res.shape == (2, 1)
            assert (res == np.array([u * v, 0.5 * u * v]).reshape(2, 1)).all()


def test_blackbox_tabular_serialization():
    hyperparameters = pd.DataFrame(
        data=np.stack([x1, x2]).T, columns=["hp_x1", "hp_x2"]
    )
    num_seeds = 1
    num_fidelities = 2
    num_objectives = 1

    def make_dummy_blackbox():
        objectives_evaluations = np.random.rand(
            len(hyperparameters), num_seeds, num_fidelities, num_objectives
        )
        return BlackboxTabular(
            hyperparameters=hyperparameters,
            configuration_space=cs,
            fidelity_space=cs_fidelity,
            objectives_evaluations=objectives_evaluations,
        )

    bb_dict = {
        "protein": make_dummy_blackbox(),
        "slice": make_dummy_blackbox(),
    }

    with tempfile.TemporaryDirectory() as tmpdirname:
        print(f"serializing and deserializing blackbox in folder {tmpdirname}")
        serialize_tabular(bb_dict, tmpdirname)
        bb_dict2 = deserialize_tabular(tmpdirname)

        print(
            bb_dict2["slice"].objective_function(
                {"hp_x1": x1[0], "hp_x2": x2[0]}, fidelity={"hp_epochs": 1}
            )
        )

        for key in bb_dict2.keys():
            bb1 = bb_dict[key]
            bb2 = bb_dict2[key]
            # assert sp.equal(bb1.configuration_space, bb2.configuration_space)
            # assert sp.equal(bb1.fidelity_space, bb2.fidelity_space)
            assert np.all(bb1.fidelity_values == bb2.fidelity_values)
            assert bb1.objectives_names == bb2.objectives_names
            np.testing.assert_allclose(
                bb1.objectives_evaluations.reshape(-1),
                bb2.objectives_evaluations.reshape(-1),
            )

        # blackbox.serialize(tmpdirname)
        # blackbox_deserialized = deserialize(tmpdirname)
        # for u, v in zip(x1, x2):
        #    res = blackbox_deserialized.objective_function({"hp_x1": u, "hp_x2": v})
        #    assert res['metric_rmse'] == u * v


def test_blackbox_tabular():
    data = np.stack([x1, x2]).T
    hyperparameters = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2"])
    num_seeds = 3
    num_fidelities = 5
    num_objectives = 2

    objectives_evaluations = np.random.rand(
        len(hyperparameters), num_seeds, num_fidelities, num_objectives
    )

    blackbox = BlackboxTabular(
        hyperparameters=hyperparameters,
        configuration_space=cs,
        fidelity_space=cs_fidelity,
        objectives_evaluations=objectives_evaluations,
        objectives_names=["a", "b"],
    )

    for i, (u, v) in enumerate(zip(x1, x2)):
        res = blackbox.objective_function(
            configuration={"hp_x1": u, "hp_x2": v},
            fidelity={"hp_epoch": num_fidelities},
            seed=num_seeds - 1,
        )
        assert np.allclose(
            list(res.values()),
            objectives_evaluations[i, num_seeds - 1, num_fidelities - 1, :],
        )

File Path: tst/blackbox_repository/test_data_consistency.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from dataclasses import dataclass
from typing import Optional, List
import pytest
import numpy as np

from syne_tune.blackbox_repository.simulated_tabular_backend import (
    make_surrogate,
)
from syne_tune.blackbox_repository import load_blackbox, add_surrogate
from syne_tune.blackbox_repository.utils import metrics_for_configuration
from syne_tune.blackbox_repository.blackbox_tabular import BlackboxTabular
from syne_tune.optimizer.schedulers.searchers.searcher import RandomSearcher


@dataclass
class BenchmarkDefinition:
    elapsed_time_attr: str
    metric: str
    mode: str
    blackbox_name: str
    dataset_name: str
    max_resource_attr: str
    surrogate: Optional[str] = None
    surrogate_kwargs: Optional[dict] = None


def fcnet_benchmark(dataset_name):
    return BenchmarkDefinition(
        elapsed_time_attr="metric_elapsed_time",
        metric="metric_valid_loss",
        mode="min",
        blackbox_name="fcnet",
        dataset_name=dataset_name,
        max_resource_attr="epochs",
    )


def nas201_benchmark(dataset_name):
    return BenchmarkDefinition(
        elapsed_time_attr="metric_elapsed_time",
        metric="metric_valid_error",
        mode="min",
        blackbox_name="nasbench201",
        dataset_name=dataset_name,
        max_resource_attr="epochs",
    )


def lcbench_benchmark(dataset_name):
    return BenchmarkDefinition(
        elapsed_time_attr="time",
        metric="val_accuracy",
        mode="max",
        blackbox_name="lcbench",
        dataset_name=dataset_name,
        surrogate="KNeighborsRegressor",
        surrogate_kwargs={"n_neighbors": 1},
        max_resource_attr="epochs",
    )


benchmark_definitions = {
    "fcnet-protein": fcnet_benchmark("protein_structure"),
    "fcnet-naval": fcnet_benchmark("naval_propulsion"),
    "fcnet-parkinsons": fcnet_benchmark("parkinsons_telemonitoring"),
    "fcnet-slice": fcnet_benchmark("slice_localization"),
    "nas201-cifar10": nas201_benchmark("cifar10"),
    "nas201-cifar100": nas201_benchmark("cifar100"),
    "nas201-ImageNet16-120": nas201_benchmark("ImageNet16-120"),
}

# 5 most expensive lcbench datasets
lc_bench_datasets = [
    "Fashion-MNIST",
    "airlines",
    "albert",
    "covertype",
    "christine",
]
for task in lc_bench_datasets:
    benchmark_definitions[
        "lcbench-" + task.replace("_", "-").replace(".", "")
    ] = lcbench_benchmark(task)


def create_blackbox(benchmark: BenchmarkDefinition):
    # See also :class:`BlackboxRepositoryBackend`
    blackbox = load_blackbox(benchmark.blackbox_name)[benchmark.dataset_name]
    if benchmark.surrogate is not None:
        surrogate = make_surrogate(benchmark.surrogate, benchmark.surrogate_kwargs)
        blackbox = add_surrogate(blackbox=blackbox, surrogate=surrogate)
    return blackbox


def _assert_strictly_increasing(elapsed_times: List[float], error_prefix: str):
    error_msg_parts = []
    for pos, (et1, et2) in enumerate(zip(elapsed_times[:-1], elapsed_times[1:])):
        if et1 >= et2:
            error_msg_parts.append(f"{pos + 1}:{et1} -> {pos + 2}:{et2}")
    if error_msg_parts:
        error_msg = error_prefix + "\n" + ", ".join(error_msg_parts)
    else:
        error_msg = ""
    assert not error_msg, error_msg


def _assert_no_extreme_deviations(elapsed_times: List[float], error_prefix: str):
    pairs = list(zip(elapsed_times[:-1], elapsed_times[1:]))
    epoch_times = [et2 - et1 for et1, et2 in pairs]
    median_val = np.median(epoch_times)
    error_msg_parts = []
    for pos, (val, (et1, et2)) in enumerate(zip(epoch_times, pairs)):
        if val < 0.01 * median_val or val > 100 * median_val:
            error_msg_parts.append(f"{pos + 1}:{et1} -> {pos + 2}:{et2}")
    if error_msg_parts:
        error_msg = (
            error_prefix
            + f"\nmedian_diff = {median_val}: "
            + ", ".join(error_msg_parts)
        )
    else:
        error_msg = ""
    assert not error_msg, error_msg


@pytest.mark.skip("Needs blackbox data files locally or on S3")
@pytest.mark.parametrize("benchmark", benchmark_definitions.values())
def test_elapsed_time_consistency(benchmark):
    num_configs = 20
    random_seed = 382378624

    blackbox = create_blackbox(benchmark)
    resource_attr = next(iter(blackbox.fidelity_space.keys()))
    elapsed_time_attr = benchmark.elapsed_time_attr
    num_fidelities = len(blackbox.fidelity_values)
    if isinstance(blackbox, BlackboxTabular):
        seeds = list(range(blackbox.num_seeds))
    else:
        seeds = [None]
    random_searcher = RandomSearcher(
        config_space=blackbox.configuration_space,
        metric=benchmark.metric,
        random_seed=random_seed,
    )
    configs = [random_searcher.get_config() for _ in range(num_configs)]
    for config in configs:
        for seed in seeds:
            error_prefix = (
                f"blackbox = {benchmark.blackbox_name}, "
                f"dataset = {benchmark.dataset_name}, "
                f"seed = {seed}, config = {config}"
            )
            all_results = metrics_for_configuration(
                blackbox=blackbox,
                config=config,
                resource_attr=resource_attr,
                seed=seed,
            )
            assert len(all_results) == num_fidelities, error_prefix
            elapsed_times = [np.nan] * num_fidelities
            for result in all_results:
                resource = int(result[resource_attr])
                elapsed_time = float(result[elapsed_time_attr])
                assert 1 <= resource <= num_fidelities, (
                    error_prefix + f", result = {result}"
                )
                elapsed_times[resource - 1] = elapsed_time
            assert not any([np.isnan(x) for x in elapsed_times]), (
                error_prefix + f", elapsed_times = {elapsed_times}"
            )
            # elapsed_times must be strictly increasing
            _assert_strictly_increasing(elapsed_times, error_prefix)
            # No extreme deviation from median in per epoch times
            _assert_no_extreme_deviations(elapsed_times, error_prefix)

File Path: tst/blackbox_repository/test_simulated_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
import numpy as np
import pandas as pd

from syne_tune.config_space import randint
from syne_tune.blackbox_repository.blackbox_tabular import BlackboxTabular
from syne_tune.blackbox_repository.simulated_tabular_backend import (
    UserBlackboxBackend,
)


n = 10
x1 = np.arange(n)
x2 = np.arange(n)[::-1]
hp_names = ["hp_x1", "hp_x2"]

cs = {name: randint(0, n - 1) for name in hp_names}

n_epochs = 5
resource_attr = "hp_epoch"
cs_fidelity = {
    resource_attr: randint(1, n_epochs),
}


def test_pause_and_resume():
    data = np.stack([x1, x2]).T
    hyperparameters = pd.DataFrame(data=data, columns=hp_names)
    num_seeds = 1
    metric = "error"
    elapsed_time_attr = "elapsed_time"
    objective_names = [metric, elapsed_time_attr]
    num_objectives = len(objective_names)
    objectives_evaluations = np.random.rand(
        len(hyperparameters), num_seeds, n_epochs, num_objectives
    )
    objectives_evaluations[:, :, :, 1] = np.cumsum(
        np.abs(objectives_evaluations[:, :, :, 1]), axis=2
    )
    blackbox = BlackboxTabular(
        hyperparameters=hyperparameters,
        configuration_space=cs,
        fidelity_space=cs_fidelity,
        objectives_evaluations=objectives_evaluations,
        objectives_names=objective_names,
    )
    backend = UserBlackboxBackend(
        blackbox=blackbox,
        elapsed_time_attr=elapsed_time_attr,
    )

    inds = [2, 4]
    configs = [dict(zip(hp_names, list(data[ind]))) for ind in inds]
    metrics = [objectives_evaluations[ind, 0, :, 0] for ind in inds]
    elapsed_times = [objectives_evaluations[ind, 0, :, 1] for ind in inds]
    # Start 2 trials
    backend.time_keeper.start_of_time()
    for config in configs:
        backend.start_trial(config)
    # Advance time and read out results
    pause_resources = [1, 2]
    step = max(elapsed_times[0][-1], elapsed_times[1][-1])
    print(f"elapsed_times = {elapsed_times}, step = {step}")
    backend.time_keeper.advance(step)
    _, results = backend.fetch_status_results(trial_ids=[0, 1])
    num_found = [0, 0]
    for trial_id, result in results:
        resource = result[resource_attr]
        fval = result[metric]
        elapsed_time = result[elapsed_time_attr]
        assert fval == metrics[trial_id][resource - 1]
        assert elapsed_time == elapsed_times[trial_id][resource - 1]
        if resource <= pause_resources[trial_id]:
            num_found[trial_id] += 1
    assert all(x == y for x, y in zip(pause_resources, num_found)), (
        pause_resources,
        num_found,
    )
    # Pause trials
    results = [
        {
            metric: fval,
            elapsed_time_attr: elapsed_time,
            resource_attr: resource,
        }
        for fval, elapsed_time, resource in zip(metrics, elapsed_times, pause_resources)
    ]
    for trial_id, result in enumerate(results):
        backend.pause_trial(trial_id=trial_id, result=result)
    # Resume paused trials and check that they do not start from scratch
    for trial_id in range(2):
        backend.resume_trial(trial_id)
    backend.time_keeper.advance(step)
    _, results = backend.fetch_status_results(trial_ids=[0, 1])
    got_it = [False, False]
    for trial_id, result in results:
        resource = result[resource_attr]
        fval = result[metric]
        assert fval == metrics[trial_id][resource - 1]
        pause_resource = pause_resources[trial_id]
        assert resource > pause_resource
        if resource == pause_resource + 1:
            got_it[trial_id] = True
    assert all(got_it)

File Path: tst/blackbox_repository/test_surrogate.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import pandas as pd
import pytest
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor

from syne_tune.blackbox_repository import BlackboxOffline
from syne_tune.blackbox_repository.blackbox_surrogate import add_surrogate
from syne_tune.blackbox_repository.blackbox_tabular import BlackboxTabular

import syne_tune.config_space as sp


np.random.seed(0)


def test_surrogate_continuous():
    surrogate = KNeighborsRegressor(n_neighbors=1)
    n = 10
    x1 = np.arange(n)
    x2 = np.arange(n)[::-1]
    cs = {
        "hp_x1": sp.randint(0, n),
        "hp_x2": sp.randint(0, n),
    }
    y = x1 * x2
    data = np.stack([x1, x2, y]).T
    df = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2", "metric_rmse"])
    blackbox = BlackboxOffline(df_evaluations=df, configuration_space=cs)
    blackbox = add_surrogate(blackbox, surrogate)

    for u, v in zip(x1, x2):
        res = blackbox.objective_function({"hp_x1": u, "hp_x2": v})
        assert res["metric_rmse"] == u * v


def test_surrogate_categorical():
    surrogate = KNeighborsRegressor(n_neighbors=1)
    n = 10
    x1 = np.arange(n)
    x2 = np.arange(n)[::-1]
    x3 = [str(i - n / 2) for i in range(n)]
    y = x1 * x2 + np.array(x3).astype(float)
    data = np.stack([x1, x2, x3, y]).T
    df = pd.DataFrame(
        data=data, columns=["hp_x1", "hp_x2", "hp_x3", "metric_rmse"], dtype=float
    )
    df["hp_x3"] = df["hp_x3"].astype(str)
    cs = {
        "hp_x1": sp.randint(0, n),
        "hp_x2": sp.randint(0, n),
        "hp_x3": sp.choice(x3),
    }
    blackbox = BlackboxOffline(df_evaluations=df, configuration_space=cs)
    blackbox = add_surrogate(blackbox, surrogate)
    blackbox.objective_function({"hp_x1": 2, "hp_x2": 3, "hp_x3": "-2"})
    for u, v, w in zip(x1, x2, x3):
        print(u, v, w)
        res = blackbox.objective_function({"hp_x1": u, "hp_x2": v, "hp_x3": w})
        assert res["metric_rmse"] == u * v + float(w)


@pytest.mark.parametrize(
    "surrogate", [MLPRegressor(), LinearRegression(), KNeighborsRegressor()]
)
def test_different_surrogates(surrogate):
    n = 10
    x1 = np.arange(n)
    x2 = np.arange(n)[::-1]
    cs = {
        "hp_x1": sp.randint(0, n),
        "hp_x2": sp.randint(0, n),
    }
    y = x1 * x2
    data = np.stack([x1, x2, y]).T
    df = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2", "metric_rmse"])
    blackbox = BlackboxOffline(df_evaluations=df, configuration_space=cs)
    blackbox = add_surrogate(blackbox, surrogate)

    for u, v in zip(x1, x2):
        blackbox.objective_function({"hp_x1": u, "hp_x2": v})


def test_blackbox_tabular_surrogate():
    n = 10
    x1 = np.arange(n)
    x2 = np.arange(n)[::-1]
    cs = {
        "hp_x1": sp.randint(0, n),
        "hp_x2": sp.randint(0, n),
    }
    n_epochs = 5
    cs_fidelity = {
        "hp_epoch": sp.randint(0, n_epochs),
    }
    data = np.stack([x1, x2]).T
    hyperparameters = pd.DataFrame(data=data, columns=["hp_x1", "hp_x2"])
    num_seeds = 1
    num_fidelities = 2
    num_objectives = 1

    objectives_evaluations = np.random.rand(
        len(hyperparameters), num_seeds, num_fidelities, num_objectives
    )
    # # matches the seed, easier to test
    # for s in range(1, num_seeds):
    #     objectives_evaluations[:, s, :, :] = objectives_evaluations[:, 0, :, :]

    blackbox = BlackboxTabular(
        hyperparameters=hyperparameters,
        configuration_space=cs,
        fidelity_space=cs_fidelity,
        objectives_evaluations=objectives_evaluations,
    )
    surrogate = KNeighborsRegressor(n_neighbors=1)
    blackbox = add_surrogate(blackbox, surrogate=surrogate)

    for i, (u, v) in enumerate(zip(x1, x2)):
        for fidelity in range(num_fidelities):
            res = blackbox.objective_function(
                configuration={"hp_x1": u, "hp_x2": v},
                fidelity={"hp_epoch": fidelity + 1},
            )
            print(list(res.values()), objectives_evaluations[i, 0, fidelity, :])
            assert np.allclose(
                list(res.values()), objectives_evaluations[i, 0, fidelity, :]
            )

File Path: tst/remote_launcher/folder1/folder2/main.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

if __name__ == "__main__":
    print("ok")

File Path: tst/remote_launcher/test_remote_launcher_path.py
Content:
# # Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# #
# # Licensed under the Apache License, Version 2.0 (the "License").
# # You may not use this file except in compliance with the License.
# # A copy of the License is located at
# #
# #     http://www.apache.org/licenses/LICENSE-2.0
# #
# # or in the "license" file accompanying this file. This file is distributed
# # on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# # express or implied. See the License for the specific language governing
# # permissions and limitations under the License.
# import logging
# from pathlib import Path
#
# import pytest
# from sagemaker.pytorch import PyTorch
#
# from syne_tune.backend import SageMakerBackend
# from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role
# from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
# from syne_tune.remote.remote_launcher import RemoteLauncher
# from syne_tune import StoppingCriterion
# from syne_tune import Tuner
#
# root = Path(__file__).parent
# sm_estimator = PyTorch(
#     entry_point="folder2/main.py",
#     source_dir=str(root / "folder1"),
#     instance_type="local",
#     instance_count=1,
#     py_version="py3",
#     framework_version="1.7.1",
#     role="dummy",
# )
#
# backend = SageMakerBackend(sm_estimator=sm_estimator)
# remote_launcher = RemoteLauncher(
#     tuner=Tuner(
#         backend=backend,
#         scheduler=FIFOScheduler({}, searcher='random', metric="dummy"),
#         stop_criterion=StoppingCriterion(max_wallclock_time=600),
#         n_workers=4,
#     )
# )
# remote_launcher.prepare_upload()
#
#
# def test_check_paths():
#     # for now, we only check that sm_estimator source_dir, endpoint script is correct
#     # todo check that dependencies are correct
#     remote_sm_estimator = remote_launcher.tuner.backend.sm_estimator
#
#     assert remote_sm_estimator.source_dir == "tuner"
#     assert (remote_launcher.upload_dir() / "folder2" / "main.py").exists()
#     assert (remote_launcher.upload_dir() / "requirements.txt").exists()
#     assert (remote_launcher.upload_dir() / "tuner.dill").exists()
#
#
# @pytest.mark.skip("this test is skipped currently as it takes ~15s and requires docker installed locally.")
# def test_estimator():
#     tuner = Tuner.load(remote_launcher.upload_dir())
#     remote_sm_estimator = tuner.backend.sm_estimator
#     remote_sm_estimator.source_dir = str(remote_launcher.upload_dir())
#     remote_sm_estimator.fit()

File Path: tst/schedulers/bayesopt/gpautograd/test_autograd_backprop.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy
import pprint
import pytest
import autograd.numpy as anp
from autograd import grad

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import Matern52
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    GaussianProcessMarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    encode_unwrap_parameter,
)


slack_constant = 1e-10


def _deep_copy_params(input_params):
    """
    Make a deep copy of the input arg_dict
    :param input_params:
    :return: deep copy of input_arg_dict
    """
    output_params = {}
    for name, param in input_params.items():
        output_params[name] = anp.array(param, copy=True)
    return output_params


def negative_log_posterior(
    likelihood: GaussianProcessMarginalLikelihood, X: anp.array, Y: anp.array
):
    objective_nd = likelihood(X, Y)
    # Add neg log hyperpriors, whenever some are defined
    for param_int, encoding in likelihood.param_encoding_pairs():
        if encoding.regularizer is not None:
            param = encode_unwrap_parameter(param_int, encoding, X)
            objective_nd = objective_nd + encoding.regularizer(param)
    return objective_nd


@pytest.fixture(scope="function")
def test_autograd_backprop(n, d, print_results):
    """
    Compare the gradients of the negative_log_posterior computed via
    the method of finite difference and AutoGrad. The gradients are
    with respect to the internal parameters.
    """
    X = anp.random.normal(size=(n, d))
    y = anp.random.normal(size=(n, 1))

    kernel = Matern52(dimension=d)
    mean = ScalarMeanFunction()
    initial_noise_variance = None
    likelihood = GaussianProcessMarginalLikelihood(
        kernel=kernel, mean=mean, initial_noise_variance=initial_noise_variance
    )
    likelihood.initialize(force_reinit=True)

    params = {}
    params_ordict = likelihood.collect_params().values()
    for param in params_ordict:
        params[param.name] = param

    def negative_log_posterior_forward(param_dict, likelihood, X, y):
        for k in params.keys():
            params[k].set_data(param_dict[k])
        return negative_log_posterior(likelihood, X, y)

    params_custom = {}
    for key in params.keys():
        params_custom[key] = anp.array([anp.random.uniform() + 0.3])
    params_custom_copy = _deep_copy_params(params_custom)

    likelihood_value = negative_log_posterior_forward(params_custom, likelihood, X, y)
    finite_diff_grad_vec = []
    for key in params.keys():
        N = negative_log_posterior_forward(params_custom, likelihood, X, y)
        params_custom_plus = params_custom.copy()
        params_custom_plus[key] *= 1 + slack_constant
        N_plus = negative_log_posterior_forward(params_custom_plus, likelihood, X, y)
        finite_diff_grad_vec.append(
            (N_plus - N) / (params_custom[key] * slack_constant)
        )

    negative_log_posterior_gradient = grad(negative_log_posterior_forward)
    grad_vec = negative_log_posterior_gradient(params_custom_copy, likelihood, X, y)
    autograd_grad_vec = list(grad_vec.values())
    if print_results:
        print("Parameter dictionary:")
        pprint.pprint(params)
        print("\nLikelihood value: {}".format(likelihood_value))
        print("\nGradients through finite difference:\n{}".format(finite_diff_grad_vec))
        print("\nGradients through AutoGrad:\n{}\n".format(autograd_grad_vec))
    numpy.testing.assert_almost_equal(
        finite_diff_grad_vec, autograd_grad_vec, decimal=3
    )


def test_autograd_multiple_trials():
    n, d = 20, 5
    num_of_exceptions = 0
    num_of_trials = 100
    print_results = False
    for _ in range(num_of_trials):
        try:
            test_autograd_backprop(n, d, print_results)
        except:
            num_of_exceptions += 1
    print("{} exceptions in {} trials.".format(num_of_exceptions, num_of_trials))

File Path: tst/schedulers/bayesopt/gpautograd/test_cholesky_factorization.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import autograd.numpy as anp
import autograd.scipy.linalg as aspl
from autograd import grad

# from autograd.test_util import check_grads
import time

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.custom_op import (
    cholesky_factorization,
)


def _testfunc_logdet(a, use_my):
    if use_my:
        l = cholesky_factorization(a)
    else:
        l = anp.linalg.cholesky(a)
    return 2.0 * anp.sum(anp.log(anp.diag(l)))


def _testfunc_mahal(a, b, use_my):
    if use_my:
        l = cholesky_factorization(a)
    else:
        l = anp.linalg.cholesky(a)
    x = aspl.solve_triangular(l, b)
    return anp.sum(anp.square(x))


def _a_from_x(x):
    y = anp.matmul(anp.transpose(x), x)
    onevec = anp.ones_like(x[0])
    return y + 0.01 * anp.diag(onevec)


def _testfunc_logdet_from_x(x, use_my):
    return _testfunc_logdet(_a_from_x(x), use_my)


def _testfunc_mahal_from_xb(xb, use_my):
    a = _a_from_x(xb[:-1])
    b = xb[-1]
    return _testfunc_mahal(a, b, use_my)


def test_cholesky_factorization():
    # num_rep = 10
    # min_n = 100
    # max_n = 2500
    # Not so useful for time comparison, but runs faster:
    num_rep = 8
    min_n = 10
    max_n = 250
    grad_logdet_my = grad(lambda x: _testfunc_logdet_from_x(x, use_my=True))
    grad_logdet_cmp = grad(lambda x: _testfunc_logdet_from_x(x, use_my=False))
    grad_mahal_my = grad(lambda xb: _testfunc_mahal_from_xb(xb, use_my=True))
    grad_mahal_cmp = grad(lambda xb: _testfunc_mahal_from_xb(xb, use_my=False))
    for rep in range(num_rep):
        n = np.random.randint(min_n, max_n)
        xmat = np.random.randn(n, n)
        # check_grads(
        #    lambda x: testfunc_logdet_from_x(x, use_my=True),
        #    modes=['rev'], order=1)(xmat)
        # logdet
        print("\nn = {}\nlogdet:".format(n))
        ts_start = time.time()
        gval_my = grad_logdet_my(xmat)
        time_my = time.time() - ts_start
        ts_start = time.time()
        gval_cmp = grad_logdet_cmp(xmat)
        time_cmp = time.time() - ts_start
        max_diff_grad_logdet = np.max(np.abs(gval_my - gval_cmp))
        print(
            "max_abs_diff_grad = {}, time_my = {}, time_cmp = {}".format(
                max_diff_grad_logdet, time_my, time_cmp
            )
        )
        assert max_diff_grad_logdet < 1e-12
        # mahal
        print("mahal:")
        ts_start = time.time()
        gval_my = grad_mahal_my(xmat)
        time_my = time.time() - ts_start
        ts_start = time.time()
        gval_cmp = grad_mahal_cmp(xmat)
        time_cmp = time.time() - ts_start
        max_diff_grad_mahal = np.max(np.abs(gval_my - gval_cmp))
        print(
            "max_abs_diff_grad = {}, time_my = {}, time_cmp = {}".format(
                max_diff_grad_mahal, time_my, time_cmp
            )
        )
        assert max_diff_grad_mahal < 1e-11


if __name__ == "__main__":
    test_cholesky_factorization()

File Path: tst/schedulers/bayesopt/gpautograd/test_comparison_gpy.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import os
from typing import List
import numpy as np
import pickle
import tempfile
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    GaussProcEmpiricalBayesModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_regression import (
    GaussianProcessRegression,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import Matern52
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ZeroMeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    INITIAL_COVARIANCE_SCALE,
    INITIAL_INVERSE_BANDWIDTHS,
    INVERSE_BANDWIDTHS_LOWER_BOUND,
    INVERSE_BANDWIDTHS_UPPER_BOUND,
    COVARIANCE_SCALE_LOWER_BOUND,
    COVARIANCE_SCALE_UPPER_BOUND,
    INITIAL_NOISE_VARIANCE,
    NOISE_VARIANCE_LOWER_BOUND,
    NOISE_VARIANCE_UPPER_BOUND,
    OptimizationConfig,
    DEFAULT_OPTIMIZATION_CONFIG,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.comparison_gpy import (
    expand_data,
)


# Copied from autogluon.core.utils.files
def download(url, path=None, overwrite=False):
    """Download files from a given URL.

    Parameters
    ----------
    url : str
        URL where file is located
    path : str, optional
        Destination path to store downloaded file. By default stores to the
        current directory with same name as in url.
    overwrite : bool, optional
        Whether to overwrite destination file if one already exists at this location.

    Returns
    -------
    str
        The file path of the downloaded file.
    """
    import requests

    if path is None:
        fname = url.split("/")[-1]
    else:
        path = os.path.expanduser(path)
        if os.path.isdir(path):
            fname = os.path.join(path, url.split("/")[-1])
        else:
            fname = path

    if overwrite or not os.path.exists(fname):
        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        print(f"Downloading {fname} from {url}...")
        r = requests.get(url, stream=True)
        if r.status_code != 200:
            raise RuntimeError("Failed downloading url %s" % url)
        # total_length = r.headers.get('content-length')
        with open(fname, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    f.write(chunk)
    return fname


# Note: At present, the surrogate model is fixed
# Note: Instead of a ScalarMeanFunction, we use a ZeroMeanFunction here. This
# is because it is too annoying to make GPy use such a mean function
def fit_predict_ours(
    data: dict,
    random_seed: int,
    optimization_config: OptimizationConfig,
) -> dict:
    # Create surrogate model
    num_dims = len(data["ss_limits"])
    _gpmodel = GaussianProcessRegression(
        kernel=Matern52(num_dims, ARD=True),
        mean=ZeroMeanFunction(),  # Instead of ScalarMeanFunction
        optimization_config=optimization_config,
        random_seed=random_seed,
    )
    model_factory = GaussProcEmpiricalBayesModelFactory(
        active_metric=INTERNAL_METRIC_NAME, gpmodel=_gpmodel, num_fantasy_samples=20
    )
    model = model_factory.model(data["state"], fit_params=True)
    model_params = model_factory.get_params()
    print("Hyperparameters: {}".format(model_params))
    # Prediction
    predictions = model.predict(data["test_inputs"])[0]
    return {"means": predictions["mean"], "stddevs": predictions["std"]}


# Note: this function is not ran by the tests so that we do not have to require
# GPy as a dependency of the tests. With GPy 1.9.9, it can be executed to generate
# the numerical values that are tested in the main testing function `test_comparison_gpy`
def fit_predict_gpy(
    data: dict, random_seed: int, optimization_config: OptimizationConfig
) -> dict:
    import GPy  # Needs GPy to be installed

    assert GPy.__version__ == "1.9.9"
    # Create surrogate model
    # Note: Matern52 in GPy uses lengthscales, while Matern52 here uses inverse
    # lengthscales (or "bandwidths"), so have to apply 1 / x.
    # We use a zero mean function as it isn't straightforward to make GPy use
    # something else.
    num_dims = len(data["ss_limits"])
    _kernel = GPy.kern.Matern52(
        num_dims,
        variance=INITIAL_COVARIANCE_SCALE,
        lengthscale=1.0 / INITIAL_INVERSE_BANDWIDTHS,
        ARD=True,
    )
    _kernel.lengthscale.constrain_bounded(
        1.0 / INVERSE_BANDWIDTHS_UPPER_BOUND,
        1.0 / INVERSE_BANDWIDTHS_LOWER_BOUND,
        warning=False,
    )
    _kernel.variance.constrain_bounded(
        COVARIANCE_SCALE_LOWER_BOUND, COVARIANCE_SCALE_UPPER_BOUND, warning=False
    )
    # Normalize targets to mean 0, variance 1 (this is done in our code
    # internally)
    targets_mean = np.mean(data["train_targets"])
    targets_std = max(np.std(data["train_targets"]), 1e-8)
    targets_normalized = data["train_targets_normalized"]
    model = GPy.models.GPRegression(
        data["train_inputs"],
        targets_normalized.reshape((-1, 1)),
        kernel=_kernel,
        noise_var=INITIAL_NOISE_VARIANCE,
    )
    model.likelihood.variance.constrain_bounded(
        NOISE_VARIANCE_LOWER_BOUND, NOISE_VARIANCE_UPPER_BOUND, warning=False
    )
    # Note: We could also set hyperpriors somehow, using model.priors. But
    # this is pretty undocumented!
    # Fit hyperparameters
    # Note: Should set optimization_config.lbfgs_tol here, but don't know how
    np.random.seed(random_seed)  # Seed plays very different role here. Whatever...
    verbose = optimization_config.verbose
    model.optimize_restarts(
        num_restarts=optimization_config.n_starts,
        optimizer="bfgs",
        max_iters=optimization_config.lbfgs_maxiter,
        verbose=verbose,
    )
    # Print hyperparameter values
    print(model)
    print("\n" + str(_kernel.lengthscale))
    # Prediction (have to be rescaled to undo normalization)
    means, vars = model.predict(data["test_inputs"], include_likelihood=False)
    means = means * targets_std + targets_mean
    stddevs = np.sqrt(vars) * targets_std
    return {"means": means, "stddevs": stddevs}


def _plot_comparison(y_list: List[np.ndarray]):
    import matplotlib.pyplot as plt

    yshape = y_list[0].shape
    assert all(y.shape == yshape for y in y_list)
    assert len(yshape) == 2, "Can only do 2D plots"
    min_val = min([np.min(y) for y in y_list])
    separator = np.ones((yshape[0], 5)) * min_val
    ys_and_seps = [x for l in zip(y_list, [separator] * len(y_list)) for x in l]
    ys_and_seps = ys_and_seps[:-1]
    plt.imshow(np.concatenate(ys_and_seps, axis=1))
    plt.colorbar()


# Note: this function is not ran by the tests but can be executed separately
# to get a visual interpretation, see `test_comparison_gpy`
def plot_predictions(data: dict, pred_ours: dict, pred_gpy: dict, title: str):
    import matplotlib.pyplot as plt

    grid_shape = data["grid_shape"]
    plt.title(title)
    for i, key in enumerate(("means", "stddevs")):
        plt.subplot(2, 1, i + 1)
        lst = [pred_ours[key].reshape(grid_shape), pred_gpy[key].reshape(grid_shape)]
        if i == 0:
            lst.insert(1, data["true_targets"].reshape(grid_shape))
        _plot_comparison(lst)
        plt.title(key)
    plt.show()


SRC_URL = "https://autogluon.s3.amazonaws.com"


def download_pickle_file(fname):
    trg_path = tempfile.mkdtemp()
    trg_fname = os.path.join(trg_path, "numcomp", fname)
    if not os.path.exists(trg_fname):
        download(os.path.join(SRC_URL, "numcomp", fname), path=trg_fname)
    with open(trg_fname, "rb") as handle:
        data = pickle.load(handle)
    if "train_inputs" in data:
        data = expand_data(data)
    return data


# Main testing function
@pytest.mark.skip("AutoGluon S3 bucket not available anymore! FIXME")
def test_comparison_gpy():
    optimization_config = DEFAULT_OPTIMIZATION_CONFIG
    fname_msk = "{}_{}_{}.pickle"
    num_train = 200
    num_grid = 200

    do_branin = True
    do_threehump = False
    do_ackley = True
    # Uncomment the lines between --- to re-generate data and GPy predictions
    # (assumes you have GPy available and verifying the version assertion in
    # `fit_predict_gpy`).
    trg_path = os.path.join(tempfile.mkdtemp(), "numcomp")

    if do_branin:
        random_seed = 894623209
        data_name = "branin"
        fname = fname_msk.format(data_name, num_train, "data")
        branin = download_pickle_file(fname)
        fname = fname_msk.format(data_name, num_train, "gpy")
        branin_gpy = download_pickle_file(fname)
        # --------------------------------------------------------------------
        # bb_cls = Branin
        # branin = sample_data(bb_cls, num_train, num_grid,
        #    expand_datadct=False)
        # print("Storing files to " + trg_path)
        # os.makedirs(trg_path, exist_ok=True)
        # fname = os.path.join(
        #    trg_path, fname_msk.format(data_name, num_train, 'data'))
        # with open(fname, 'wb') as handle:
        #    pickle.dump(branin, handle, protocol=pickle.HIGHEST_PROTOCOL)
        # branin = expand_data(branin)
        #
        # branin_gpy = fit_predict_gpy(branin, random_seed, optimization_config)
        # fname = os.path.join(
        #    trg_path, fname_msk.format(data_name, num_train, 'gpy'))
        # with open(fname, 'wb') as handle:
        #    pickle.dump(branin_gpy, handle, protocol=pickle.HIGHEST_PROTOCOL)
        # --------------------------------------------------------------------

        branin_ours = fit_predict_ours(
            branin,
            random_seed,
            optimization_config,
        )

        # If you want a visual result, uncomment the following lines
        # title = "Branin, num_train={}".format(num_train)
        # plot_predictions(branin, branin_ours, branin_gpy, title)

        # Branin - means
        sse = sum((branin_gpy["means"].reshape(-1) - branin_ours["means"]) ** 2)
        num = branin_ours["means"].shape[0]
        branin_means_rmse = np.sqrt(sse / num)
        # print('branin_means = {}'.format(branin_means_rmse))
        assert branin_means_rmse <= 5e-3
        # Branin - stds
        sse = sum((branin_gpy["stddevs"].reshape(-1) - branin_ours["stddevs"]) ** 2)
        num = branin_ours["stddevs"].shape[0]
        branin_stds_rmse = np.sqrt(sse / num)
        # print('branin_stds = {}'.format(branin_stds_rmse))
        assert branin_stds_rmse <= 2e-2

    # NOTE: Fails with differences in the predictive stddevs!
    if do_threehump:
        random_seed = 54654209
        data_name = "threehump"
        fname = fname_msk.format(data_name, num_train, "data")
        threehump = download_pickle_file(fname)
        fname = fname_msk.format(data_name, num_train, "gpy")
        threehump_gpy = download_pickle_file(fname)
        # --------------------------------------------------------------------
        # bb_cls = ThreeHumpCamel
        # threehump = sample_data(bb_cls, num_train, num_grid,
        #    expand_datadct=False)
        # print("Storing files to " + trg_path)
        # os.makedirs(trg_path, exist_ok=True)
        # fname = os.path.join(
        #    trg_path, fname_msk.format(data_name, num_train, 'data'))
        # with open(fname, 'wb') as handle:
        #    pickle.dump(threehump, handle, protocol=pickle.HIGHEST_PROTOCOL)
        # threehump = expand_data(threehump)
        #
        # threehump_gpy = fit_predict_gpy(
        #    threehump, random_seed, optimization_config)
        # fname = os.path.join(
        #    trg_path, fname_msk.format(data_name, num_train, 'gpy'))
        # with open(fname, 'wb') as handle:
        #    pickle.dump(threehump_gpy, handle, protocol=pickle.HIGHEST_PROTOCOL)
        # --------------------------------------------------------------------

        threehump_ours = fit_predict_ours(
            threehump,
            random_seed,
            optimization_config,
        )

        # If you want a visual result, uncomment the following lines
        # title = "ThreeHump, num_train={}".format(num_train)
        # plot_predictions(threehump, threehump_ours, threehump_gpy, title)

        # ThreeHump - means
        sse = sum((threehump_gpy["means"].reshape(-1) - threehump_ours["means"]) ** 2)
        N = threehump_ours["means"].shape[0]
        threehump_means_rmse = np.sqrt(sse / N)
        assert threehump_means_rmse <= 5e-2
        # ThreeHump - stds
        sse = sum(
            (threehump_gpy["stddevs"].reshape(-1) - threehump_ours["stddevs"]) ** 2
        )
        N = threehump_ours["stddevs"].shape[0]
        threehump_stds_rmse = np.sqrt(sse / N)
        assert threehump_stds_rmse <= 6e-2

    if do_ackley:
        random_seed = 232098764
        data_name = "ackley"
        fname = fname_msk.format(data_name, num_train, "data")
        ackley = download_pickle_file(fname)
        fname = fname_msk.format(data_name, num_train, "gpy")
        ackley_gpy = download_pickle_file(fname)
        # --------------------------------------------------------------------
        # bb_cls = Ackley
        # ackley = sample_data(bb_cls, num_train, num_grid,
        #    expand_datadct=False)
        # print("Storing files to " + trg_path)
        # os.makedirs(trg_path, exist_ok=True)
        # fname = os.path.join(
        #    trg_path, fname_msk.format(data_name, num_train, 'data'))
        # with open(fname, 'wb') as handle:
        #    pickle.dump(ackley, handle, protocol=pickle.HIGHEST_PROTOCOL)
        # ackley = expand_data(ackley)
        #
        # ackley_gpy = fit_predict_gpy(ackley, random_seed, optimization_config)
        # fname = os.path.join(
        #    trg_path, fname_msk.format(data_name, num_train, 'gpy'))
        # with open(fname, 'wb') as handle:
        #    pickle.dump(ackley_gpy, handle, protocol=pickle.HIGHEST_PROTOCOL)
        # --------------------------------------------------------------------

        ackley_ours = fit_predict_ours(
            ackley,
            random_seed,
            optimization_config,
        )

        # If you want a visual result, uncomment the following lines
        # title = "Ackley, num_train={}".format(num_train)
        # plot_predictions(ackley, ackley_ours, ackley_gpy, title)

        # Ackley - means
        sse = sum((ackley_gpy["means"].reshape(-1) - ackley_ours["means"]) ** 2)
        N = ackley_ours["means"].shape[0]
        ackley_means_rmse = np.sqrt(sse / N)
        # print('ackley_means = {}'.format(ackley_means_rmse))
        assert ackley_means_rmse <= 7e-3
        # Ackley - stds
        sse = sum((ackley_gpy["stddevs"].reshape(-1) - ackley_ours["stddevs"]) ** 2)
        N = ackley_ours["stddevs"].shape[0]
        ackley_stds_rmse = np.sqrt(sse / N)
        # print('ackley_stds = {}'.format(ackley_stds_rmse))
        assert ackley_stds_rmse <= 3e-3


if __name__ == "__main__":
    test_comparison_gpy()

File Path: tst/schedulers/bayesopt/gpautograd/test_custom_gluon.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from autograd import numpy as np

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon import (
    Parameter,
    ParameterDict,
    Block,
)


def test_parameter():
    p = Parameter(name="abc", shape=(1,))
    p.initialize()
    data = p.data
    grad = p.grad


def test_parameter_dict():
    pd = ParameterDict("pd")
    pd.initialize()
    p = pd.get("def")


def test_block():
    class TestBlock(Block):
        def __init__(self):
            super(TestBlock, self).__init__()
            with self.name_scope():
                self.a = self.params.get("a", shape=(10,))
                self.b = self.params.get("b", shape=(10,))

        def forward(self, x):
            return x + self.a.data() + self.b.data()

    t = TestBlock()
    t.initialize()
    print(t.a.grad_req)
    t.a.set_data(np.ones((10,)))
    assert "a" in t.a.name
    x = np.zeros((10,))
    y = t(x)
    print(y)

File Path: tst/schedulers/bayesopt/gpautograd/test_gp.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy
import autograd.numpy as anp

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import Matern52
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    GaussianProcessMarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_regression import (
    GaussianProcessRegression,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    NOISE_VARIANCE_LOWER_BOUND,
    INVERSE_BANDWIDTHS_LOWER_BOUND,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    LogarithmScalarEncoding,
    PositiveScalarEncoding,
)


def test_likelihood_encoding():
    mean = ScalarMeanFunction()
    kernel = Matern52(dimension=1)
    likelihood = GaussianProcessMarginalLikelihood(mean=mean, kernel=kernel)
    assert isinstance(likelihood.encoding_noise, LogarithmScalarEncoding)
    likelihood = GaussianProcessMarginalLikelihood(
        mean=mean, kernel=kernel, encoding_type="positive"
    )
    assert isinstance(likelihood.encoding_noise, PositiveScalarEncoding)


def test_gp_regression_no_noise():
    def f(x):
        return anp.sin(x) / x

    x_train = anp.arange(-5, 5, 0.2)  # [-5,-4.8,-4.6,...,4.8]
    x_test = anp.arange(
        -4.9, 5, 0.2
    )  # [-4.9, -4.7, -4.5,...,4.9], note that train and test points do not overlap
    y_train = f(x_train)
    y_test = f(x_test)

    # to np.ndarray
    y_train_np_ndarray = anp.array(y_train)
    x_train_np_ndarray = anp.array(x_train).reshape((-1, 1))
    x_test_np_ndarray = anp.array(x_test).reshape((-1, 1))

    model = GaussianProcessRegression(kernel=Matern52(dimension=1))
    data = {"features": x_train_np_ndarray, "targets": y_train_np_ndarray}
    model.fit(data)

    # Check that the value of the residual noise variance learned by empirical Bayes is in the same order
    # as the smallest allowed value (since there is no noise)
    noise_variance = model.likelihood.get_noise_variance()
    numpy.testing.assert_almost_equal(noise_variance, NOISE_VARIANCE_LOWER_BOUND)

    mu_train, var_train = model.predict(x_train_np_ndarray)[0]
    mu_test, var_test = model.predict(x_test_np_ndarray)[0]

    numpy.testing.assert_almost_equal(mu_train, y_train, decimal=4)
    numpy.testing.assert_almost_equal(var_train, [0.0] * len(var_train), decimal=4)
    # Fewer decimals imposed for the test points
    numpy.testing.assert_almost_equal(mu_test, y_test, decimal=3)

    # If we wish plot
    # import matplotlib.pyplot as plt
    # plt.plot(x_train, y_train, "r")
    # plt.errorbar(x=x_train,
    #              y=mu_train,
    #              yerr=var_train)
    # plt.plot(x_test, y_test, "b")
    # plt.errorbar(x=x_test,
    #              y=mu_test,
    #              yerr=var_test)
    # plt.show()


def test_gp_regression_with_noise():
    def f(x):
        return anp.sin(x) / x

    anp.random.seed(7)

    x_train = anp.arange(-5, 5, 0.2)  # [-5, -4.8, -4.6,..., 4.8]
    x_test = anp.arange(
        -4.9, 5, 0.2
    )  # [-4.9, -4.7, -4.5,..., 4.9], note that train and test points do not overlap
    y_train = f(x_train)
    y_test = f(x_test)

    std_noise = 0.01
    noise_train = anp.random.normal(0.0, std_noise, size=y_train.shape)

    # to anp.ndarray
    y_train_np_ndarray = anp.array(y_train)
    noise_train_np_ndarray = anp.array(noise_train)
    x_train_np_ndarray = anp.array(x_train).reshape((-1, 1))
    x_test_np_ndarray = anp.array(x_test).reshape((-1, 1))

    model = GaussianProcessRegression(kernel=Matern52(dimension=1))
    data = {
        "features": x_train_np_ndarray,
        "targets": y_train_np_ndarray + noise_train_np_ndarray,
    }
    model.fit(data)

    # Check that the value of the residual noise variance learned by empirical Bayes is in the same order as std_noise^2
    noise_variance = model.likelihood.get_noise_variance()
    numpy.testing.assert_almost_equal(noise_variance, std_noise**2, decimal=4)

    mu_train, _ = model.predict(x_train_np_ndarray)[0]
    mu_test, _ = model.predict(x_test_np_ndarray)[0]

    numpy.testing.assert_almost_equal(mu_train, y_train, decimal=2)
    numpy.testing.assert_almost_equal(mu_test, y_test, decimal=2)


def test_gp_regression_2d_with_ard():
    def f(x):
        # Only dependent on the first column of x
        return anp.sin(x[:, 0]) / x[:, 0]

    anp.random.seed(7)

    dimension = 3

    # 30 train and test points in R^3
    x_train = anp.random.uniform(-5, 5, size=(30, dimension))
    x_test = anp.random.uniform(-5, 5, size=(30, dimension))
    y_train = f(x_train)
    y_test = f(x_test)

    # to np.ndarray
    y_train_np_ndarray = anp.array(y_train)
    x_train_np_ndarray = anp.array(x_train)
    x_test_np_ndarray = anp.array(x_test)

    model = GaussianProcessRegression(kernel=Matern52(dimension=dimension, ARD=True))
    data = {"features": x_train_np_ndarray, "targets": y_train_np_ndarray}
    model.fit(data)

    # Check that the value of the residual noise variance learned by empirical Bayes is in the same order as the smallest allowed value (since there is no noise)
    noise_variance = model.likelihood.get_noise_variance()
    numpy.testing.assert_almost_equal(noise_variance, NOISE_VARIANCE_LOWER_BOUND)

    # Check that the bandwidths learned by empirical Bayes reflect the fact that only the first column is useful
    # In particular, for the useless dimensions indexed by {1,2}, the inverse bandwidths should be close to INVERSE_BANDWIDTHS_LOWER_BOUND
    # (or conversely, bandwidths should be close to their highest allowed values)
    sqd = model.likelihood.kernel.squared_distance
    inverse_bandwidths = sqd.encoding.get(sqd.inverse_bandwidths_internal.data())

    assert (
        inverse_bandwidths[0] > inverse_bandwidths[1]
        and inverse_bandwidths[0] > inverse_bandwidths[2]
    )
    numpy.testing.assert_almost_equal(
        inverse_bandwidths[1], INVERSE_BANDWIDTHS_LOWER_BOUND
    )
    numpy.testing.assert_almost_equal(
        inverse_bandwidths[2], INVERSE_BANDWIDTHS_LOWER_BOUND
    )

    mu_train, _ = model.predict(x_train_np_ndarray)[0]
    mu_test, _ = model.predict(x_test_np_ndarray)[0]

    numpy.testing.assert_almost_equal(mu_train, y_train, decimal=2)
    # Fewer decimals imposed for the test points
    numpy.testing.assert_almost_equal(mu_test, y_test, decimal=1)

File Path: tst/schedulers/bayesopt/gpautograd/test_kernel.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import pytest

import numpy as np
import autograd.numpy as anp
from autograd.tracer import getval

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    Matern52,
    FabolasKernelFunction,
    ProductKernelFunction,
    CrossValidationKernelFunction,
    KernelFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ZeroMeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel.base import (
    SquaredDistance,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DATA_TYPE,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    LogarithmScalarEncoding,
    PositiveScalarEncoding,
)
from syne_tune.config_space import uniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.config_ext import (
    ExtendedConfiguration,
)


def test_square_distance_no_ard_unit_bandwidth():
    X = anp.array([[1, 0], [0, 1]], dtype=DATA_TYPE)
    # test default ard = False
    sqd = SquaredDistance(dimension=2)
    assert sqd.ARD == False
    sqd.collect_params().initialize()
    D = sqd(X, X)
    expected_D = anp.array([[0.0, 2.0], [2.0, 0.0]])
    np.testing.assert_almost_equal(expected_D, D)


def test_square_distance_no_ard_non_unit_bandwidth():
    X = anp.array([[1, 0], [0, 1]], dtype=DATA_TYPE)
    sqd = SquaredDistance(dimension=2)
    assert sqd.ARD == False
    sqd.collect_params().initialize()
    sqd.encoding.set(sqd.inverse_bandwidths_internal, 1.0 / anp.sqrt(2.0))
    D = sqd(X, X)
    expected_D = anp.array([[0.0, 1.0], [1.0, 0.0]])
    np.testing.assert_almost_equal(expected_D, D)


def test_square_distance_with_ard():
    X = anp.array([[2.0, 1.0], [1.0, 2.0], [0.0, 1.0]], dtype=DATA_TYPE)
    sqd = SquaredDistance(dimension=2, ARD=True)
    assert sqd.ARD == True
    sqd.collect_params().initialize()
    sqd.encoding.set(sqd.inverse_bandwidths_internal, [1.0 / anp.sqrt(2.0), 1.0])
    D = sqd(X, X)
    expected_D = anp.array(
        [[0.0, 3.0 / 2.0, 2.0], [3.0 / 2.0, 0.0, 3.0 / 2.0], [2.0, 3.0 / 2.0, 0.0]]
    )
    np.testing.assert_almost_equal(expected_D, D)


mater52 = lambda squared_dist: (
    1.0 + anp.sqrt(5.0 * squared_dist) + 5.0 / 3.0 * squared_dist
) * anp.exp(-anp.sqrt(5.0 * squared_dist))
freeze_thaw = lambda u, alpha, beta: beta**alpha / (u + beta) ** alpha


def test_matern52_unit_scale():
    X = anp.array([[1, 0], [0, 1]], dtype=DATA_TYPE)
    kernel = Matern52(dimension=2)
    assert kernel.ARD == False
    kernel.collect_params().initialize()
    K = kernel(X, X)
    expected_K = anp.array([[mater52(0.0), mater52(2.0)], [mater52(2.0), mater52(0.0)]])
    np.testing.assert_almost_equal(expected_K, K)


def test_matern52_non_unit_scale():
    X = anp.array([[1, 0], [0, 1]], dtype=DATA_TYPE)
    kernel = Matern52(dimension=2)
    assert kernel.ARD == False
    kernel.collect_params().initialize()
    kernel.encoding.set(kernel.covariance_scale_internal, 0.5)
    K = kernel(X, X)
    expected_K = 0.5 * anp.array(
        [[mater52(0.0), mater52(2.0)], [mater52(2.0), mater52(0.0)]]
    )
    np.testing.assert_almost_equal(expected_K, K)


def test_matern52_ard():
    X = anp.array([[2.0, 1.0], [1.0, 2.0], [0.0, 1.0]], dtype=DATA_TYPE)
    kernel = Matern52(dimension=2, ARD=True)
    kernel.collect_params().initialize()
    sqd = kernel.squared_distance
    assert kernel.ARD == True
    assert sqd.ARD == True
    sqd.encoding.set(sqd.inverse_bandwidths_internal, [1.0 / anp.sqrt(2.0), 1.0])
    K = kernel(X, X)
    # expected_D is taken from previous test about squared distances
    expected_D = anp.array(
        [[0.0, 3.0 / 2.0, 2.0], [3.0 / 2.0, 0.0, 3.0 / 2.0], [2.0, 3.0 / 2.0, 0.0]]
    )
    expected_K = mater52(expected_D)
    np.testing.assert_almost_equal(expected_K, K)


def test_matern52_encoding():
    kernel = Matern52(dimension=2, ARD=True)
    assert isinstance(kernel.encoding, LogarithmScalarEncoding)
    assert isinstance(kernel.squared_distance.encoding, LogarithmScalarEncoding)
    assert kernel.encoding.dimension == 1
    assert kernel.squared_distance.encoding.dimension == 2
    kernel = Matern52(dimension=2, ARD=True, encoding_type="positive")
    assert isinstance(kernel.encoding, PositiveScalarEncoding)
    assert isinstance(kernel.squared_distance.encoding, PositiveScalarEncoding)
    assert kernel.encoding.dimension == 1
    assert kernel.squared_distance.encoding.dimension == 2


def test_fabolas_encoding():
    kernel = FabolasKernelFunction()
    assert isinstance(kernel.encoding_u12, LogarithmScalarEncoding)
    assert kernel.encoding_u12.dimension == 1

    kernel = FabolasKernelFunction(encoding_type="positive")
    assert isinstance(kernel.encoding_u12, PositiveScalarEncoding)
    assert kernel.encoding_u12.dimension == 1


def test_matern52_wrongshape():
    kernel = Matern52(dimension=3)
    kernel.collect_params().initialize()
    X1 = anp.random.normal(0.0, 1.0, (5, 2))

    with pytest.raises(Exception):
        kernel(X1, X1)

    with pytest.raises(Exception):
        kernel.diagonal(X1)

    X2 = anp.random.normal(0.0, 1.0, (3, 3))
    with pytest.raises(Exception):
        kernel(X2, X1)


def _create_product_kernel(kernel1, kernel2):
    kernel1.collect_params().initialize()
    kernel2.collect_params().initialize()
    return ProductKernelFunction(kernel1, kernel2)


@pytest.mark.parametrize(
    "kernel1, kernel2, input_data_dimension",
    [
        (Matern52(dimension=1), Matern52(dimension=1), 2),
        (Matern52(dimension=2), Matern52(dimension=2), 4),
        (Matern52(dimension=3), Matern52(dimension=1), 4),
        (FabolasKernelFunction(), Matern52(dimension=2), 3),
    ],
)
@pytest.mark.parametrize("X0_samples, X1_samples", [(1, 1), (1, 2), (5, 7)])
def test_product_kernel_happy_path(
    kernel1, kernel2, input_data_dimension, X0_samples, X1_samples
):
    product_kernel = _create_product_kernel(kernel1, kernel2)
    X0 = anp.random.randn(X0_samples, input_data_dimension)
    X1 = anp.random.randn(X1_samples, input_data_dimension)
    kernel_output = product_kernel(X0, X1)
    assert kernel_output.shape == (X0_samples, X1_samples)


@pytest.mark.parametrize(
    "kernel1, kernel2",
    [
        (Matern52(dimension=2), Matern52(dimension=1)),
        (FabolasKernelFunction(), Matern52(dimension=2)),
    ],
)
def test_product_kernel_wrong_shape(kernel1, kernel2):
    product_kernel = _create_product_kernel(kernel1, kernel2)

    X1 = anp.random.randn(5, 4)
    with pytest.raises(Exception):
        product_kernel(X1, X1)

    with pytest.raises(Exception):
        product_kernel.diagonal(X1)

    X2 = anp.random.randn(3, 3)
    with pytest.raises(Exception):
        product_kernel(X2, X1)

    X3 = anp.random.randn(5, 2)
    with pytest.raises(Exception):
        product_kernel(X3, X3)


class ConstantKernelFunction(KernelFunction):
    """
    Kernel which is constant 1
    """

    def __init__(self, dimension=1, **kwargs):
        super().__init__(dimension=dimension, **kwargs)

    def diagonal(self, X):
        X = self._check_input_shape(X)
        return anp.ones((getval(X.shape[0]),))

    def diagonal_depends_on_X(self):
        return False

    def forward(self, X1, X2):
        X1 = self._check_input_shape(X1)
        if X2 is not X1:
            X2 = self._check_input_shape(X2)
        return anp.ones((getval(X1.shape[0]), getval(X2.shape[0])))

    def param_encoding_pairs(self):
        return []

    def get_params(self):
        return dict()

    def set_params(self, param_dict):
        pass


def test_crossvalidation_kernel():
    num_folds = 6
    config_space = {"x": uniform(0.0, 1.0)}
    hp_ranges = make_hyperparameter_ranges(config_space)
    config_ext = ExtendedConfiguration(
        hp_ranges, resource_attr_key="epoch", resource_attr_range=(1, num_folds)
    )
    num_configs = 50
    xvals = np.random.rand(num_configs)
    rvals = np.random.randint(low=1, high=num_folds + 1, size=num_configs)
    configs = []
    for x, r in zip(xvals, rvals):
        configs.append(config_ext.get({"x": x}, resource=r))
    kernel = CrossValidationKernelFunction(
        kernel_main=ConstantKernelFunction(),
        kernel_residual=ConstantKernelFunction(),
        mean_main=ZeroMeanFunction(),
        num_folds=num_folds,
    )
    inputs = config_ext.hp_ranges_ext.to_ndarray_matrix(configs)
    kern_mat = kernel(inputs, inputs)
    max_resources = np.maximum(rvals.reshape((-1, 1)), rvals.reshape((1, -1)))
    kern_mat_compare = (1.0 / max_resources) + 1.0
    np.testing.assert_almost_equal(kern_mat, kern_mat_compare)

File Path: tst/schedulers/bayesopt/gpautograd/test_mcmc.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy
import autograd.numpy as anp
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd import SliceException
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.slice import (
    SliceSampler,
    slice_sampler_step_out,
    slice_sampler_step_in,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.distribution import (
    Normal,
    LogNormal,
    Horseshoe,
    Uniform,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.warping import (
    Warping,
    WarpedKernel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import Matern52
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.mean import (
    ScalarMeanFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.likelihood import (
    GaussianProcessMarginalLikelihood,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gpr_mcmc import (
    GPRegressionMCMC,
    _get_gp_hps,
    _set_gp_hps,
    _create_likelihood,
)


# This is just to make the tests work. In GPRegressionMCMC, lower and upper
# bounds are dealt with through the encoding
def old_log_likelihood(x, distribution, lower=-float("inf"), upper=float("inf")):
    if any(x < lower) or any(x > upper):
        return -float("inf")
    return -distribution(x)


def test_uniform():
    uniform = Uniform(0.0, 1.0)
    lower, upper = 0.0, 1.0
    assert old_log_likelihood(
        anp.array([0.2]), uniform, lower, upper
    ) == old_log_likelihood(anp.array([0.3]), uniform, lower, upper)
    assert old_log_likelihood(anp.array([2.0]), uniform, lower, upper) == -float("inf")
    assert old_log_likelihood(anp.array([-1.0]), uniform, lower, upper) == -float("inf")


def test_normal():
    normal = Normal(0, 1)
    lower, upper = -1e3, 1e3
    assert old_log_likelihood(
        anp.array([0.0]), normal, lower, upper
    ) > old_log_likelihood(anp.array([0.1]), normal, lower, upper)
    assert old_log_likelihood(
        anp.array([0.0]), normal, lower, upper
    ) > old_log_likelihood(anp.array([-0.1]), normal, lower, upper)
    assert old_log_likelihood(anp.array([1e4]), normal, lower, upper) == -float("inf")
    assert old_log_likelihood(anp.array([-1e4]), normal, lower, upper) == -float("inf")


def test_log_normal():
    log_normal = LogNormal(0.0, 1.0)
    lower, upper = 1e-6, 1e9
    assert old_log_likelihood(
        anp.array([1.0]), log_normal, lower, upper
    ) > old_log_likelihood(anp.array([1.1]), log_normal, lower, upper)
    assert old_log_likelihood(
        anp.array([1.0]), log_normal, lower, upper
    ) < old_log_likelihood(anp.array([0.9]), log_normal, lower, upper)
    assert old_log_likelihood(anp.array([1e10]), log_normal, lower, upper) == -float(
        "inf"
    )
    assert old_log_likelihood(anp.array([1e-8]), log_normal, lower, upper) == -float(
        "inf"
    )


def test_horse_shoe():
    horse_shoe = Horseshoe(0.1)
    lower, upper = 1e-6, 1e6
    assert old_log_likelihood(
        anp.array([0.01]), horse_shoe, lower, upper
    ) > old_log_likelihood(anp.array([0.1]), horse_shoe, lower, upper)
    assert old_log_likelihood(anp.array([1e-7]), horse_shoe, lower, upper) == -float(
        "inf"
    )
    assert old_log_likelihood(anp.array([1e7]), horse_shoe, lower, upper) == -float(
        "inf"
    )


def test_slice_normal():
    normal = Normal(0, 1)
    random_state = anp.random.RandomState(0)
    slice = SliceSampler(
        log_density=lambda x: old_log_likelihood(x, normal),
        scale=1.0,
        random_state=random_state,
    )
    samples = slice.sample(anp.array([0.0]), 5000, 1, 1)
    numpy.testing.assert_almost_equal(anp.mean(samples), 0.0, decimal=2)
    numpy.testing.assert_almost_equal(anp.std(samples), 1.0, decimal=2)


def test_slice_step_out():
    normal = Normal(0, 1)

    def sliced_log_density(x):
        return old_log_likelihood(anp.array([x]), normal)

    # the lower and upper bound should has log density smaller than this log_pivot
    log_pivot = sliced_log_density(1.0)
    random_state = anp.random.RandomState(0)
    lower, upper = slice_sampler_step_out(
        log_pivot, 0.1, sliced_log_density, random_state
    )
    assert lower < -1.0 and upper > 1.0

    log_pivot = sliced_log_density(100)
    with pytest.raises(
        SliceException
    ):  # the log_pivot is too small so need > 200 steps
        slice_sampler_step_out(log_pivot, 0.1, sliced_log_density, random_state)


def test_slice_step_in():
    normal = Normal(0.0, 1.0)

    def sliced_log_density(x):
        return old_log_likelihood(anp.array([x]), normal)

    log_pivot = sliced_log_density(
        1.0
    )  # the movement should between [-1., 1.] after step in
    random_state = anp.random.RandomState(0)
    movement = slice_sampler_step_in(
        -20.0, 20.0, log_pivot, sliced_log_density, random_state
    )
    assert -1.0 < movement < 1.0

    with pytest.raises(SliceException):  # when bound is off, should get SliceException
        slice_sampler_step_in(2.0, 10.0, log_pivot, sliced_log_density, random_state)


def test_get_gp_hps():
    mean = ScalarMeanFunction()
    kernel = Matern52(dimension=1)
    warping = Warping(dimension=1, index_to_range={0: (-4.0, 4.0)})
    warped_kernel = WarpedKernel(kernel=kernel, warping=warping)
    likelihood = GaussianProcessMarginalLikelihood(
        kernel=warped_kernel, mean=mean, initial_noise_variance=1e-6
    )
    likelihood.initialize(force_reinit=True)
    likelihood.hybridize()
    hp_values = _get_gp_hps(likelihood)
    # the oder of hps are noise, mean, covariance scale, bandwidth, warping a, warping b
    numpy.testing.assert_array_almost_equal(
        hp_values, anp.array([1e-6, 0.0, 1.0, 1.0, 1.0, 1.0])
    )


def test_set_gp_hps():
    mean = ScalarMeanFunction()
    kernel = Matern52(dimension=1)
    warping = Warping(dimension=1, index_to_range={0: (-4.0, 4.0)})
    warped_kernel = WarpedKernel(kernel=kernel, warping=warping)
    likelihood = GaussianProcessMarginalLikelihood(
        kernel=warped_kernel, mean=mean, initial_noise_variance=1e-6
    )
    likelihood.initialize(force_reinit=True)
    likelihood.hybridize()
    hp_values = anp.array([1e-2, 1.0, 0.5, 0.3, 0.2, 1.1])
    _set_gp_hps(hp_values, likelihood)
    numpy.testing.assert_array_almost_equal(hp_values, _get_gp_hps(likelihood))


def test_create_likelihood():
    def build_kernel():
        kernel = Matern52(dimension=1)
        warping = Warping(dimension=1, index_to_range={0: (-4.0, 4.0)})
        return WarpedKernel(kernel=kernel, warping=warping)

    random_state = anp.random.RandomState(0)
    likelihood1 = _create_likelihood(build_kernel, random_state=random_state)
    likelihood2 = _create_likelihood(build_kernel, random_state=random_state)
    numpy.testing.assert_array_almost_equal(
        _get_gp_hps(likelihood1), _get_gp_hps(likelihood2)
    )


@pytest.mark.skip(reason="Need manual inspection on the plots")
def test_mcmc():
    import matplotlib.pyplot as plt

    anp.random.seed(7)

    def f_n(x):
        noise = anp.random.normal(0.0, 0.25, x.shape[0])
        return 0.1 * anp.power(x, 3) + noise

    def f(x):
        return 0.1 * anp.power(x, 3)

    x_train = anp.concatenate(
        (anp.random.uniform(-4.0, -1.0, 40), anp.random.uniform(1.0, 4.0, 40))
    )
    y_train = f_n(x_train)
    x_test = anp.sort(anp.random.uniform(-4.0, 4.0, 200))

    y_train_np_nd = anp.array(y_train, dtype=anp.float64)
    x_train_np_nd = anp.array(x_train, dtype=anp.float64)
    x_test_np_nd = anp.array(x_test, dtype=anp.float64)

    def build_kernel():
        return WarpedKernel(
            kernel=Matern52(dimension=1),
            warping=Warping(dimension=1, index_to_range={0: (-4.0, 4.0)}),
        )

    model_mcmc = GPRegressionMCMC(build_kernel=build_kernel, random_seed=1)
    data = {"features": x_train_np_nd, "targets": y_train_np_nd}
    model_mcmc.fit(data)
    mcmc_predictions = model_mcmc.predict(x_test_np_nd)

    for mcmc_mean, mcmc_var in mcmc_predictions:
        mcmc_mean, mcmc_std = mcmc_mean, anp.sqrt(mcmc_var)
        plt.figure()
        plt.scatter(x_train, y_train, color="red", label="observations")
        plt.plot(x_test, f(x_test), color="black", label="ground truth")
        plt.plot(x_test, mcmc_mean, color="blue", label="mcmc prediction")
        plt.fill_between(
            x_test, mcmc_mean - 1.96 * mcmc_std, mcmc_mean + 1.96 * mcmc_std, alpha=0.5
        )
        plt.legend()
    plt.show()

File Path: tst/schedulers/bayesopt/gpautograd/test_posterior_state.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy
import autograd.numpy as anp

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.posterior_state import (
    IncrementalUpdateGPPosteriorState,
    GaussProcPosteriorState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gp_regression import (
    GaussianProcessRegression,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import Matern52


def test_incremental_update():
    def f(x):
        return anp.sin(x) / x

    numpy.random.seed(298424)
    std_noise = 0.01

    # Sample data
    features_list = []
    targets_list = []
    num_incr_list = []
    for rep in range(10):
        num_train = anp.random.randint(low=5, high=15)
        num_incr = anp.random.randint(low=1, high=7)
        num_incr_list.append(num_incr)
        sizes = [num_train, num_incr]
        features = []
        targets = []
        for sz in sizes:
            feats = anp.random.uniform(low=-1.0, high=1.0, size=sz).reshape((-1, 1))
            features.append(feats)
            targs = f(feats)
            targs += anp.random.normal(0.0, std_noise, size=targs.shape)
            targets.append(targs)

        features_list.append(features)
        targets_list.append(targets)

    for rep in range(10):
        model = GaussianProcessRegression(kernel=Matern52(dimension=1))
        features = features_list[rep]
        targets = targets_list[rep]
        # Posterior state by incremental updating
        data = {"features": features[0], "targets": targets[0]}
        model.fit(data)
        noise_variance_1 = model.likelihood.get_noise_variance()
        state_incr = IncrementalUpdateGPPosteriorState(
            **data,
            mean=model.likelihood.mean,
            kernel=model.likelihood.kernel,
            noise_variance=model.likelihood.get_noise_variance(as_ndarray=True),
        )
        num_incr = num_incr_list[rep]
        for i in range(num_incr):
            state_incr = state_incr.update(
                features[1][i].reshape((1, -1)), targets[1][i].reshape((1, -1))
            )
        noise_variance_2 = state_incr.noise_variance[0]
        # Posterior state by direct computation
        state_comp = GaussProcPosteriorState(
            features=anp.concatenate(features, axis=0),
            targets=anp.concatenate(targets, axis=0),
            mean=model.likelihood.mean,
            kernel=model.likelihood.kernel,
            noise_variance=state_incr.noise_variance,
        )
        # Compare them
        assert (
            noise_variance_1 == noise_variance_2
        ), "noise_variance_1 = {} != {} = noise_variance_2".format(
            noise_variance_1, noise_variance_2
        )
        chol_fact_incr = state_incr.chol_fact
        chol_fact_comp = state_comp.chol_fact
        numpy.testing.assert_almost_equal(chol_fact_incr, chol_fact_comp, decimal=2)
        pred_mat_incr = state_incr.pred_mat
        pred_mat_comp = state_comp.pred_mat
        numpy.testing.assert_almost_equal(pred_mat_incr, pred_mat_comp, decimal=2)


if __name__ == "__main__":
    test_incremental_update()

File Path: tst/schedulers/bayesopt/gpautograd/test_warping.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy
import autograd.numpy as anp

from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.warping import (
    OneDimensionalWarping,
    Warping,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DATA_TYPE,
    NUMERICAL_JITTER,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.gluon_blocks_helpers import (
    LogarithmScalarEncoding,
    PositiveScalarEncoding,
)


def test_warping_encoding():
    input_range = (0.0, 2.0)
    warping = OneDimensionalWarping(input_range)
    assert isinstance(warping.encoding, LogarithmScalarEncoding)
    assert warping.encoding.dimension == 2
    warping = OneDimensionalWarping(input_range, encoding_type="positive")
    assert isinstance(warping.encoding, PositiveScalarEncoding)


def test_warping_default_parameters():
    x = anp.array([0.0, 1.0, 2.0], dtype=DATA_TYPE)
    input_range = (0.0, 2.0)
    warping = OneDimensionalWarping(input_range)
    warping.collect_params().initialize()

    warping_parameters = warping.encoding.get(warping.warping_internal.data())

    numpy.testing.assert_almost_equal(warping_parameters, anp.ones(2))
    numpy.testing.assert_almost_equal(
        warping(x), anp.array([NUMERICAL_JITTER, 0.5, 1.0 - NUMERICAL_JITTER])
    )


def test_warping_with_arbitrary_parameters():
    x = anp.array([0.0, 1.0, 2.0], dtype=DATA_TYPE)
    input_range = (0.0, 2.0)
    warping = OneDimensionalWarping(input_range)
    warping.collect_params().initialize()
    warping.encoding.set(warping.warping_internal, [2.0, 0.5])
    warping_parameters = warping.encoding.get(warping.warping_internal.data())
    numpy.testing.assert_almost_equal(warping_parameters, [2.0, 0.5])
    # In that case (with parameters [2., 0.5]), the warping is given by x => 1. - sqrt(1. - x^2)
    def expected_warping(x):
        return 1.0 - anp.sqrt(1.0 - x * x)

    numpy.testing.assert_almost_equal(
        warping(x),
        expected_warping(anp.array([NUMERICAL_JITTER, 0.5, 1.0 - NUMERICAL_JITTER])),
    )


def test_warping_with_multidimension_and_arbitrary_parameters():
    X = anp.array([[0.0, 1.0, 0.0], [1.0, 2.0, 1.0], [2.0, 0.0, 2.0]], dtype=DATA_TYPE)

    dimension = 3

    # We transform only the columns {0,2} of the 3-dimensional data X
    input_range = (0.0, 2.0)
    warping = Warping(
        index_to_range={0: input_range, 2: input_range}, dimension=dimension
    )

    assert len(warping.transformations) == dimension

    warping.collect_params().initialize()

    # We change the warping parameters of the first dimension only
    w0 = warping.transformations[0]
    w0.encoding.set(w0.warping_internal, [2.0, 0.5])

    w2 = warping.transformations[2]
    w2_parameters = w2.encoding.get(w2.warping_internal.data())

    # The parameters of w2 should be the default ones (as there was no set operations)
    numpy.testing.assert_almost_equal(w2_parameters, anp.ones(2))

    # With parameters [2., 0.5], the warping is given by x => 1. - sqrt(1. - x^2)
    def expected_warping(x):
        return 1.0 - anp.sqrt(1.0 - x * x)

    expected_column0 = expected_warping(
        anp.array([NUMERICAL_JITTER, 0.5, 1.0 - NUMERICAL_JITTER])
    ).reshape((-1, 1))
    expected_column1 = anp.array([1.0, 2.0, 0.0]).reshape((-1, 1))
    expected_column2 = anp.array(
        [NUMERICAL_JITTER, 0.5, 1.0 - NUMERICAL_JITTER]
    ).reshape((-1, 1))

    numpy.testing.assert_almost_equal(
        warping(X), anp.hstack([expected_column0, expected_column1, expected_column2])
    )

File Path: tst/schedulers/bayesopt/test_bayesopt_cei.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List
import numpy as np
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
    INTERNAL_CONSTRAINT_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.config_space import uniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_MCMC_CONFIG,
    DEFAULT_OPTIMIZATION_CONFIG,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc_impl import (
    CEIAcquisitionFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    GaussProcSurrogateModel,
    GaussProcEmpiricalBayesModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_mcmc_model import (
    GaussProcMCMCModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm_components import (
    LBFGSOptimizeAcquisition,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    default_gpmodel,
    default_gpmodel_mcmc,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_tuning_job_state,
)


def _construct_models(X, Y, metric, hp_ranges, do_mcmc, with_pending):
    pending_tuples = [(0.5, 0.5), (0.2, 0.2)] if with_pending else None
    state = create_tuning_job_state(
        hp_ranges=hp_ranges, cand_tuples=X, metrics=Y, pending_tuples=pending_tuples
    )
    random_seed = 0

    gpmodel = default_gpmodel(
        state, random_seed=random_seed, optimization_config=DEFAULT_OPTIMIZATION_CONFIG
    )
    model_factory = GaussProcEmpiricalBayesModelFactory(
        active_metric=metric, gpmodel=gpmodel, num_fantasy_samples=2
    )
    result = [model_factory.model(state, fit_params=True)]
    if do_mcmc:
        gpmodel_mcmc = default_gpmodel_mcmc(
            state, random_seed=random_seed, mcmc_config=DEFAULT_MCMC_CONFIG
        )
        model_factory = GaussProcMCMCModelFactory(
            active_metric=metric, gpmodel=gpmodel_mcmc
        )
        result.append(model_factory.model(state, fit_params=True))
    return result


def default_models(
    metric, do_mcmc=True, with_pending=False
) -> List[GaussProcSurrogateModel]:
    config_space = {"x": uniform(0.0, 1.0), "y": uniform(0.0, 1.0)}
    hp_ranges = make_hyperparameter_ranges(config_space)
    X = [(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)]
    # Continuous constraint, such as memory requirement: the larger x[0] the larger the memory footprint, and
    # the feasible region (i.e., Y <= 0) is for x[0] <= 0.5
    Y = [
        {
            INTERNAL_METRIC_NAME: np.sum(x) * 10.0,
            INTERNAL_CONSTRAINT_NAME: x[0] * 2.0 - 1.0,
        }
        for x in X
    ]
    result = _construct_models(X, Y, metric, hp_ranges, do_mcmc, with_pending)
    return result


def _build_models_with_and_without_feasible_candidates(do_mcmc, with_pending):
    active_models = default_models(
        INTERNAL_METRIC_NAME, do_mcmc=do_mcmc, with_pending=with_pending
    )
    constraint_models = default_models(
        INTERNAL_CONSTRAINT_NAME, do_mcmc=do_mcmc, with_pending=with_pending
    )
    active_models_infeasible = default_models_all_infeasible(
        INTERNAL_METRIC_NAME, do_mcmc=do_mcmc, with_pending=with_pending
    )
    constraint_models_infeasible = default_models_all_infeasible(
        INTERNAL_CONSTRAINT_NAME, do_mcmc=do_mcmc, with_pending=with_pending
    )

    all_active_models = active_models + active_models_infeasible
    all_constraint_models = constraint_models + constraint_models_infeasible
    return all_active_models, all_constraint_models


def default_models_all_infeasible(
    metric, do_mcmc=True, with_pending=False
) -> List[GaussProcSurrogateModel]:
    config_space = {"x": uniform(0.0, 1.0), "y": uniform(0.0, 1.0)}
    hp_ranges = make_hyperparameter_ranges(config_space)
    X = [
        (0.5, 0.0),
        (0.6, 0.0),
        (0.7, 0.0),
        (0.8, 0.0),
        (0.9, 0.0),
        (1.0, 0.0),
        (0.5, 1.0),
        (1.0, 1.0),
    ]

    # Continuous constraint, such as memory requirement: the larger x[0] the larger the memory footprint, and
    # There are no feasible points.
    Y = [
        {
            INTERNAL_METRIC_NAME: np.sum(x) * 10.0,
            INTERNAL_CONSTRAINT_NAME: x[0] * 2.0 + 0.01,
        }
        for x in X
    ]
    result = _construct_models(X, Y, metric, hp_ranges, do_mcmc, with_pending)
    return result


def plot_ei_mean_std(model, cei, max_grid=1.0):
    import matplotlib.pyplot as plt

    grid = np.linspace(0, max_grid, 400)
    Xgrid, Ygrid = np.meshgrid(grid, grid)
    inputs = np.hstack([Xgrid.reshape(-1, 1), Ygrid.reshape(-1, 1)])
    Z_ei = cei.compute_acq(inputs)[0]
    predictions = model.predict(inputs)[0]
    Z_means = predictions["mean"]
    Z_std = predictions["std"]
    titles = ["CEI", "mean", "std"]
    for i, (Z, title) in enumerate(zip([Z_ei, Z_means, Z_std], titles)):
        plt.subplot(1, 3, i + 1)
        plt.imshow(
            Z.reshape(Xgrid.shape), extent=[0, max_grid, 0, max_grid], origin="lower"
        )
        plt.colorbar()
        plt.title(title)
    plt.show()


# Note: This test fails when run with GP MCMC model. There, acq[5] > acq[7], and acq[8] > acq[5]
# ==> Need to look into GP MCMC model
def test_sanity_check():
    # - test that values are negative as we should be returning *minus* expected improvement
    # - test that values that are further from evaluated candidates have higher expected improvement
    #   given similar mean
    # - test that points closer to better points have higher expected improvement
    # - test that this holds both with and without pending candidates/fantasizing

    for with_pending in [False, True]:
        active_models = default_models(
            INTERNAL_METRIC_NAME, do_mcmc=False, with_pending=with_pending
        )
        constraint_models = default_models(
            INTERNAL_CONSTRAINT_NAME, do_mcmc=False, with_pending=with_pending
        )
        for active_model, constraint_model in zip(active_models, constraint_models):
            models = {
                INTERNAL_METRIC_NAME: active_model,
                INTERNAL_CONSTRAINT_NAME: constraint_model,
            }
            cei = CEIAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)
            X = np.array(
                [
                    (0.0, 0.0),  # 0
                    (1.0, 0.0),  # 1
                    (0.0, 1.0),  # 2
                    (1.0, 1.0),  # 3
                    (0.4, 0.0),  # 4
                    (0.0, 0.4),  # 5
                    (0.1, 0.0),  # 6
                    (0.0, 0.1),  # 7
                    (0.1, 0.1),  # 8
                    (0.9, 0.9),  # 9
                ]
            )
            acq = list(cei.compute_acq(X).flatten())
            assert all(a <= 0 for a in acq), acq

            # lower evaluations with lower memory footprint should correspond to better acquisition
            # second inequality is less equal because last two values are likely zero

            assert acq[0] < acq[1] <= acq[3], acq
            assert acq[8] < acq[9], acq

            # evaluations with same EI but lower memory footprint give a better CEI
            assert acq[5] < acq[4]
            assert acq[7] < acq[6]

            # further from an evaluated point should correspond to better acquisition
            assert acq[6] < acq[4] < acq[1], acq
            assert acq[7] < acq[5] < acq[2], acq


def test_no_feasible_candidates():
    # - test that values are negative as we should be returning *minus* expected improvement
    # - test that values that are further from evaluated candidates have higher expected improvement
    #   given similar mean
    # - test that points closer to better points have higher expected improvement
    # - test that this holds both with and without pending candidates/fantasizing

    for with_pending in [False, True]:
        active_models = default_models_all_infeasible(
            INTERNAL_METRIC_NAME, do_mcmc=False, with_pending=with_pending
        )
        constraint_models = default_models_all_infeasible(
            INTERNAL_CONSTRAINT_NAME, do_mcmc=False, with_pending=with_pending
        )
        for active_model, constraint_model in zip(active_models, constraint_models):
            models = {
                INTERNAL_METRIC_NAME: active_model,
                INTERNAL_CONSTRAINT_NAME: constraint_model,
            }
            cei = CEIAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)
            X = np.array(
                [
                    (1.0, 1.0),  # 0
                    (1.0, 0.0),  # 1
                    (0.5, 0.0),  # 2
                    (0.4, 0.0),  # 3
                    (0.3, 0.0),  # 4
                    (0.2, 0.0),  # 5
                    (0.1, 0.0),  # 6
                    (0.05, 0.0),  # 7
                ]
            )
            acq = list(cei.compute_acq(X).flatten())

            # the acquisition function should return only non-positive values
            assert all(a <= 0 for a in acq), acq

            # at the evaluated unfeasible candidates, the probability of satisfying the constraint should be zero
            np.testing.assert_almost_equal(acq[0], acq[1])
            np.testing.assert_almost_equal(acq[0], 0.0)
            # evaluations that are further from the infeasible candidates
            # should have a larger probability of satisfying the constraint
            assert acq[2] >= acq[3] >= acq[4] >= acq[5] >= acq[6] >= acq[7], acq


@pytest.mark.skip("this unit test is skipped to save time")
def test_best_value():
    # test that the best value affects the constrained expected improvement
    active_models = default_models(INTERNAL_METRIC_NAME)
    constraint_models = default_models(INTERNAL_CONSTRAINT_NAME)
    for active_model, constraint_model in zip(active_models, constraint_models):
        models = {
            INTERNAL_METRIC_NAME: active_model,
            INTERNAL_CONSTRAINT_NAME: constraint_model,
        }
        cei = CEIAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)

        random = np.random.RandomState(42)
        test_X = random.uniform(low=0.0, high=1.0, size=(10, 2))

        acq_best0 = list(cei.compute_acq(test_X).flatten())
        zero_row = np.zeros((1, 2))
        acq0_best0 = cei.compute_acq(zero_row)

        # override current best
        cei._feasible_best_list = np.array([30])

        acq_best2 = list(cei.compute_acq(test_X).flatten())
        acq0_best2 = cei.compute_acq(zero_row)

        # if the best is only 30 the acquisition function should be better (lower value)
        assert all(a2 < a0 for a2, a0 in zip(acq_best2, acq_best0))

        # there should be a considerable gap at the point of the best evaluation
        assert acq0_best2 < acq0_best0 - 1.0


@pytest.mark.skip("this unit test is skipped to save time")
def test_optimization_improves():
    debug_output = False
    # Pick a random point, optimize and the expected improvement should be better:
    # But only if the starting point is not too far from the origin
    random = np.random.RandomState(42)
    active_models = default_models(INTERNAL_METRIC_NAME)
    constraint_models = default_models(INTERNAL_CONSTRAINT_NAME)
    for active_model, constraint_model in zip(active_models, constraint_models):
        models = {
            INTERNAL_METRIC_NAME: active_model,
            INTERNAL_CONSTRAINT_NAME: constraint_model,
        }
        cei = CEIAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)
        hp_ranges = active_model.hp_ranges_for_prediction()
        opt = LBFGSOptimizeAcquisition(
            hp_ranges, models, CEIAcquisitionFunction, INTERNAL_METRIC_NAME
        )
        non_zero_acq_at_least_once = False
        initial_point = random.uniform(low=0.0, high=0.1, size=(2,))
        acq0, df0 = cei.compute_acq_with_gradient(initial_point)
        if debug_output:
            print("\nInitial point: f(x0) = {}, x0 = {}".format(acq0, initial_point))
            print("grad0 = {}".format(df0))
        if acq0 != 0:
            non_zero_acq_at_least_once = True
            init_cand = hp_ranges.from_ndarray(initial_point)
            optimized = hp_ranges.to_ndarray(opt.optimize(init_cand))
            acq_opt = cei.compute_acq(optimized)[0]
            if debug_output:
                print("Final point: f(x1) = {}, x1 = {}".format(acq_opt, optimized))
            assert acq_opt < 0
            assert acq_opt < acq0

        assert non_zero_acq_at_least_once


def test_numerical_gradient():
    # test that the analytical gradient computation is correct by comparing to the numerical gradient
    # both when the feasible best exists and when it does not
    debug_output = False
    do_mcmc = False
    random = np.random.RandomState(42)
    eps = 1e-6

    for with_pending in [True, False]:
        (
            all_active_models,
            all_constraint_models,
        ) = _build_models_with_and_without_feasible_candidates(do_mcmc, with_pending)

        for active_model, constraint_model in zip(
            all_active_models, all_constraint_models
        ):
            models = {
                INTERNAL_METRIC_NAME: active_model,
                INTERNAL_CONSTRAINT_NAME: constraint_model,
            }
            cei = CEIAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)

            high = 1.0
            x = random.uniform(low=0.0, high=high, size=(2,))
            f0, analytical_gradient = cei.compute_acq_with_gradient(x)
            analytical_gradient = analytical_gradient.flatten()
            if debug_output:
                print(
                    "x0 = {}, f(x_0) = {}, grad(x_0) = {}".format(
                        x, f0, analytical_gradient
                    )
                )

            for i in range(2):
                h = np.zeros_like(x)
                h[i] = eps
                fpeps = cei.compute_acq(x + h)[0]
                fmeps = cei.compute_acq(x - h)[0]
                numerical_derivative = (fpeps - fmeps) / (2 * eps)
                if debug_output:
                    print(
                        "f(x0+eps) = {}, f(x0-eps) = {}, findiff = {}, deriv = {}".format(
                            fpeps[0],
                            fmeps[0],
                            numerical_derivative[0],
                            analytical_gradient[i],
                        )
                    )
                np.testing.assert_almost_equal(
                    numerical_derivative.item(), analytical_gradient[i], decimal=2
                )


def test_value_same_as_with_gradient():
    # test that compute_acq and compute_acq_with_gradients return the same acquisition values
    # both when the feasible best exists and when it does not
    do_mcmc = False
    for with_pending in [True, False]:
        (
            all_active_models,
            all_constraint_models,
        ) = _build_models_with_and_without_feasible_candidates(do_mcmc, with_pending)
        for active_model, constraint_model in zip(
            all_active_models, all_constraint_models
        ):
            models = {
                INTERNAL_METRIC_NAME: active_model,
                INTERNAL_CONSTRAINT_NAME: constraint_model,
            }
            cei = CEIAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)

            random = np.random.RandomState(42)
            X = random.uniform(low=0.0, high=1.0, size=(10, 2))

            # assert same as computation with gradients
            vec1 = cei.compute_acq(X).flatten()
            vec2 = np.array([cei.compute_acq_with_gradient(x)[0] for x in X])
            np.testing.assert_almost_equal(vec1, vec2)


if __name__ == "__main__":
    test_optimization_improves()
    test_numerical_gradient()

File Path: tst/schedulers/bayesopt/test_bayesopt_ei.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List
import numpy as np

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    dictionarize_objective,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.config_space import uniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_MCMC_CONFIG,
    DEFAULT_OPTIMIZATION_CONFIG,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc_impl import (
    EIAcquisitionFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc import (
    ActiveMetricCurrentBestProvider,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    GaussProcSurrogateModel,
    GaussProcEmpiricalBayesModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_mcmc_model import (
    GaussProcMCMCModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm_components import (
    LBFGSOptimizeAcquisition,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    default_gpmodel,
    default_gpmodel_mcmc,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_tuning_job_state,
)


# This setup makes little sense for good testing.
#
# When default model for no MCMC is plotted:
# - Plot on [0, 1]^2:
#   - Mean essentially constant at 10, stddev essentially constant at 2.5
#   - EI essentially constant at -0.12717145
# - Plot on [0, 0.1]^2:
#   - Mean = 10, except dropping in corner, stddev = 2.5
#   - EI essentially constant, dropping in corner
# - Plot on [0, 0.01]^2:
#   - Mean growing 0 -> 8, sttdev = 2.5, but drops to 0 in corner
#   - EI from -0.66 to -0.12, -> 0 only very close to origin
# - Plot on [0, 0.001]^2:
#   - Mean growing 0 -> 1.5, stddev growing 0 -> 1.8
#   - EI about -0.6, but -> 0 close to origin
# EI is minimized (value -0.66817) very close to origin (order 0.001). Grows to
# 0 at origin, increases to constant -0.12717145 very rapidly away from origin.
#
# In fact, if EI is optimized starting at a point outside [0, 0.1]^2, the optimizer
# returns with the starting point, and test_optimization_improves fails.
def default_models(do_mcmc=True) -> List[GaussProcSurrogateModel]:
    config_space = {"x": uniform(0.0, 1.0), "y": uniform(0.0, 1.0)}
    hp_ranges = make_hyperparameter_ranges(config_space)
    X = [(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)]
    Y = [dictionarize_objective(np.sum(x) * 10.0) for x in X]
    state = create_tuning_job_state(hp_ranges=hp_ranges, cand_tuples=X, metrics=Y)
    random_seed = 0

    gpmodel = default_gpmodel(
        state, random_seed=random_seed, optimization_config=DEFAULT_OPTIMIZATION_CONFIG
    )
    model_factory = GaussProcEmpiricalBayesModelFactory(
        active_metric=INTERNAL_METRIC_NAME, gpmodel=gpmodel, num_fantasy_samples=20
    )
    result = [model_factory.model(state, fit_params=True)]
    if do_mcmc:
        gpmodel_mcmc = default_gpmodel_mcmc(
            state, random_seed=random_seed, mcmc_config=DEFAULT_MCMC_CONFIG
        )
        model_factory = GaussProcMCMCModelFactory(
            active_metric=INTERNAL_METRIC_NAME, gpmodel=gpmodel_mcmc
        )
        result.append(model_factory.model(state, fit_params=True))
    return result


def plot_ei_mean_std(model, ei, max_grid=1.0):
    import matplotlib.pyplot as plt

    grid = np.linspace(0, max_grid, 400)
    Xgrid, Ygrid = np.meshgrid(grid, grid)
    inputs = np.hstack([Xgrid.reshape(-1, 1), Ygrid.reshape(-1, 1)])
    Z_ei = ei.compute_acq(inputs)[0]
    predictions = model.predict(inputs)[0]
    Z_means = predictions["mean"]
    Z_std = predictions["std"]
    titles = ["EI", "mean", "std"]
    for i, (Z, title) in enumerate(zip([Z_ei, Z_means, Z_std], titles)):
        plt.subplot(1, 3, i + 1)
        plt.imshow(
            Z.reshape(Xgrid.shape), extent=[0, max_grid, 0, max_grid], origin="lower"
        )
        plt.colorbar()
        plt.title(title)
    plt.show()


# Note: This test fails when run with GP MCMC model. There, acq[5] > acq[7], and acq[8] > acq[5]
# ==> Need to look into GP MCMC model
def test_sanity_check():
    # - test that values are negative as we should be returning *minus* expected improvement
    # - test that values that are further from evaluated candidates have higher expected improvement
    #   given similar mean
    # - test that points closer to better points have higher expected improvement
    for model in default_models(do_mcmc=False):
        ei = EIAcquisitionFunction(model)
        X = np.array(
            [
                (0.0, 0.0),  # 0
                (1.0, 0.0),  # 1
                (0.0, 1.0),  # 2
                (1.0, 1.0),  # 3
                (0.2, 0.0),  # 4
                (0.0, 0.2),  # 5
                (0.1, 0.0),  # 6
                (0.0, 0.1),  # 7
                (0.1, 0.1),  # 8
                (0.9, 0.9),  # 9
            ]
        )
        _acq = ei.compute_acq(X).flatten()
        # print('Negative EI values:')
        # print(_acq)
        acq = list(_acq)

        assert all(a <= 0 for a in acq), acq

        # lower evaluations should correspond to better acquisition
        # second inequality is less equal because last two values are likely zero
        assert acq[0] < acq[1] <= acq[3], acq
        # Note: The improvement here is tiny, just 0.01%:
        assert acq[8] < acq[9], acq

        # further from an evaluated point should correspond to better acquisition
        assert acq[6] < acq[4] < acq[1], acq
        assert acq[7] < acq[5] < acq[2], acq


def test_best_value():
    # test that the best value affects expected improvement
    for model in default_models():
        ei = EIAcquisitionFunction(model)

        random = np.random.RandomState(42)
        test_X = random.uniform(low=0.0, high=0.01, size=(10, 2))

        acq_best0 = list(ei.compute_acq(test_X).flatten())
        zero_row = np.zeros((1, 2))
        acq0_best0 = ei.compute_acq(zero_row)

        # override current best
        ei._current_bests = ActiveMetricCurrentBestProvider([np.array([10.0])])

        acq_best10 = list(ei.compute_acq(test_X).flatten())
        acq0_best10 = ei.compute_acq(zero_row)

        # if the best is 10, the acquisition function should be better (lower value)
        assert all(
            a10 < a0 for a10, a0 in zip(acq_best10, acq_best0)
        ), f"\nbest=0:  {acq_best0}\nbest=10: {acq_best10}"

        # there should be a considerable gap at the point of the best evaluation
        assert acq0_best10 < acq0_best0 - 1.0


# The original version of this test is failing. See comments above.
# In fact, if EI is optimized from a starting point outside [0, 0.1]^2,
# the gradient is tiny there, so the optimizer returns with the starting
# point, and no improvement is made.
#
# If the starting point is sampled in [0, 0.1]^2, the test works. The optimum
# of EI is very close to the origin.
def test_optimization_improves():
    debug_output = False
    # Pick a random point, optimize and the expected improvement should be better:
    # But only if the starting point is not too far from the origin
    random = np.random.RandomState(42)
    for model in default_models():
        ei = EIAcquisitionFunction(model)
        hp_ranges = model.hp_ranges_for_prediction()
        opt = LBFGSOptimizeAcquisition(hp_ranges, model, EIAcquisitionFunction)
        if debug_output:
            print("\n\nGP MCMC" if model.does_mcmc() else "GP Opt")
            fzero = ei.compute_acq(np.zeros((1, 2)))[0]
            print("f(0) = {}".format(fzero))
        if debug_output and not model.does_mcmc():
            # Plot the thing!
            plot_ei_mean_std(model, ei, max_grid=0.001)
            plot_ei_mean_std(model, ei, max_grid=0.01)
            plot_ei_mean_std(model, ei, max_grid=0.1)
            plot_ei_mean_std(model, ei, max_grid=1.0)

        non_zero_acq_at_least_once = False
        for iter in range(10):
            # initial_point = random.uniform(low=0.0, high=1.0, size=(2,))
            initial_point = random.uniform(low=0.0, high=0.1, size=(2,))
            acq0, df0 = ei.compute_acq_with_gradient(initial_point)
            if debug_output:
                print(
                    "\nInitial point: f(x0) = {}, x0 = {}".format(acq0, initial_point)
                )
                print("grad0 = {}".format(df0))
            if acq0 != 0:
                non_zero_acq_at_least_once = True
                init_cand = hp_ranges.from_ndarray(initial_point)
                optimized = hp_ranges.to_ndarray(opt.optimize(init_cand))
                acq_opt = ei.compute_acq(optimized)[0]
                if debug_output:
                    print("Final point: f(x1) = {}, x1 = {}".format(acq_opt, optimized))
                assert acq_opt < 0
                assert acq_opt < acq0

        assert non_zero_acq_at_least_once


# Changes from original version: Half of the time, we sample x in [0, 0.02]^2, where
# the shape of EI is more interesting
def test_numerical_gradient():
    debug_output = False
    random = np.random.RandomState(42)
    eps = 1e-6

    for model in default_models():
        ei = EIAcquisitionFunction(model)

        for iter in range(10):
            high = 1.0 if iter < 5 else 0.02
            x = random.uniform(low=0.0, high=high, size=(2,))
            f0, analytical_gradient = ei.compute_acq_with_gradient(x)
            analytical_gradient = analytical_gradient.flatten()
            if debug_output:
                print(
                    "x0 = {}, f(x_0) = {}, grad(x_0) = {}".format(
                        x, f0, analytical_gradient
                    )
                )

            for i in range(2):
                h = np.zeros_like(x)
                h[i] = eps
                fpeps = ei.compute_acq(x + h)[0]
                fmeps = ei.compute_acq(x - h)[0]
                numerical_derivative = (fpeps - fmeps) / (2 * eps)
                if debug_output:
                    print(
                        "f(x0+eps) = {}, f(x0-eps) = {}, findiff = {}, deriv = {}".format(
                            fpeps[0],
                            fmeps[0],
                            numerical_derivative[0],
                            analytical_gradient[i],
                        )
                    )
                np.testing.assert_almost_equal(
                    numerical_derivative.item(), analytical_gradient[i], decimal=4
                )


def test_value_same_as_with_gradient():
    # test that compute_acq and compute_acq_with_gradients return the same acquisition values
    for model in default_models():
        ei = EIAcquisitionFunction(model)

        random = np.random.RandomState(42)
        X = random.uniform(low=0.0, high=1.0, size=(10, 2))

        # assert same as computation with gradients
        vec1 = ei.compute_acq(X).flatten()
        vec2 = np.array([ei.compute_acq_with_gradient(x)[0] for x in X])
        np.testing.assert_almost_equal(vec1, vec2)


if __name__ == "__main__":
    test_optimization_improves()
    test_numerical_gradient()

File Path: tst/schedulers/bayesopt/test_bayesopt_eipu.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List
import numpy as np
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    dictionarize_objective,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.config_space import uniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_MCMC_CONFIG,
    DEFAULT_OPTIMIZATION_CONFIG,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc_impl import (
    EIpuAcquisitionFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc import (
    ActiveMetricCurrentBestProvider,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    GaussProcSurrogateModel,
    GaussProcEmpiricalBayesModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_mcmc_model import (
    GaussProcMCMCModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm_components import (
    LBFGSOptimizeAcquisition,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    default_gpmodel,
    default_gpmodel_mcmc,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_tuning_job_state,
)


COST_METRIC_NAME = "cost_metric"


def default_models(metric, do_mcmc=True) -> List[GaussProcSurrogateModel]:
    config_space = {"x": uniform(0.0, 1.0), "y": uniform(0.0, 1.0)}
    hp_ranges = make_hyperparameter_ranges(config_space)
    X = [(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)]
    if metric == INTERNAL_METRIC_NAME:
        Y = [dictionarize_objective(np.sum(x) * 10.0) for x in X]
    elif metric == COST_METRIC_NAME:
        # Increasing the first hp increases cost
        Y = [{metric: 1.0 + x[0] * 2.0} for x in X]
    else:
        raise ValueError(f"{metric} is not a valid metric")
    state = create_tuning_job_state(hp_ranges=hp_ranges, cand_tuples=X, metrics=Y)
    random_seed = 0

    gpmodel = default_gpmodel(
        state, random_seed=random_seed, optimization_config=DEFAULT_OPTIMIZATION_CONFIG
    )
    model_factory = GaussProcEmpiricalBayesModelFactory(
        active_metric=metric, gpmodel=gpmodel, num_fantasy_samples=2
    )
    result = [model_factory.model(state, fit_params=True)]
    if do_mcmc:
        gpmodel_mcmc = default_gpmodel_mcmc(
            state, random_seed=random_seed, mcmc_config=DEFAULT_MCMC_CONFIG
        )
        model_factory = GaussProcMCMCModelFactory(
            active_metric=metric, gpmodel=gpmodel_mcmc
        )
        result.append(model_factory.model(state, fit_params=True))
    return result


def plot_ei_mean_std(model, eipu, max_grid=1.0):
    import matplotlib.pyplot as plt

    grid = np.linspace(0, max_grid, 400)
    Xgrid, Ygrid = np.meshgrid(grid, grid)
    inputs = np.hstack([Xgrid.reshape(-1, 1), Ygrid.reshape(-1, 1)])
    Z_ei = eipu.compute_acq(inputs)[0]
    predictions = model.predict(inputs)[0]
    Z_means = predictions["mean"]
    Z_std = predictions["std"]
    titles = ["EIpu", "mean", "std"]
    for i, (Z, title) in enumerate(zip([Z_ei, Z_means, Z_std], titles)):
        plt.subplot(1, 3, i + 1)
        plt.imshow(
            Z.reshape(Xgrid.shape), extent=[0, max_grid, 0, max_grid], origin="lower"
        )
        plt.colorbar()
        plt.title(title)
    plt.show()


# Note: This test fails when run with GP MCMC model. There, acq[5] > acq[7], and acq[8] > acq[5]
# ==> Need to look into GP MCMC model
def test_sanity_check():
    # - test that values are negative as we should be returning *minus* expected improvement
    # - test that values that are further from evaluated candidates have higher expected improvement
    #   given similar mean
    # - test that points closer to better points have higher expected improvement
    active_models = default_models(INTERNAL_METRIC_NAME, do_mcmc=False)
    cost_models = default_models(COST_METRIC_NAME, do_mcmc=False)
    for active_model, cost_model in zip(active_models, cost_models):
        models = {INTERNAL_METRIC_NAME: active_model, COST_METRIC_NAME: cost_model}
        eipu = EIpuAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)
        X = np.array(
            [
                (0.0, 0.0),  # 0
                (1.0, 0.0),  # 1
                (0.0, 1.0),  # 2
                (1.0, 1.0),  # 3
                (0.2, 0.0),  # 4
                (0.0, 0.2),  # 5
                (0.1, 0.0),  # 6
                (0.0, 0.1),  # 7
                (0.1, 0.1),  # 8
                (0.9, 0.9),  # 9
            ]
        )
        acq = list(eipu.compute_acq(X).flatten())
        assert all(a <= 0 for a in acq), acq

        # lower evaluations with lower cost should correspond to better acquisition
        # second inequality is less equal because last two values are likely zero
        assert acq[0] < acq[1] <= acq[3], acq
        assert acq[8] < acq[9], acq

        # evaluations with same EI but lower cost give a better EIpu
        assert acq[5] < acq[4]
        assert acq[7] < acq[6]

        # further from an evaluated point should correspond to better acquisition
        assert acq[6] < acq[4] < acq[1], acq
        assert acq[7] < acq[5] < acq[2], acq


def test_best_value():
    # test that the best value affects the cost-aware expected improvement
    active_models = default_models(INTERNAL_METRIC_NAME)
    cost_models = default_models(COST_METRIC_NAME)
    for active_model, cost_model in zip(active_models, cost_models):
        models = {INTERNAL_METRIC_NAME: active_model, COST_METRIC_NAME: cost_model}
        eipu = EIpuAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)

        random = np.random.RandomState(42)
        test_X = random.uniform(low=0.0, high=1.0, size=(10, 2))

        acq_best0 = list(eipu.compute_acq(test_X).flatten())
        zero_row = np.zeros((1, 2))
        acq0_best0 = eipu.compute_acq(zero_row)

        # override current best
        eipu._current_bests = ActiveMetricCurrentBestProvider([np.array([30.0])])

        acq_best2 = list(eipu.compute_acq(test_X).flatten())
        acq0_best2 = eipu.compute_acq(zero_row)

        # if the best is only 2 the acquisition function should be better (lower value)
        assert all(a2 < a0 for a2, a0 in zip(acq_best2, acq_best0))

        # there should be a considerable gap at the point of the best evaluation
        assert acq0_best2 < acq0_best0 - 1.0


@pytest.mark.skip("this unit test is skipped to save time")
def test_optimization_improves():
    debug_output = False
    # Pick a random point, optimize and the expected improvement should be better:
    # But only if the starting point is not too far from the origin
    random = np.random.RandomState(42)
    active_models = default_models(INTERNAL_METRIC_NAME)
    cost_models = default_models(COST_METRIC_NAME)
    for active_model, cost_model in zip(active_models, cost_models):
        models = {INTERNAL_METRIC_NAME: active_model, COST_METRIC_NAME: cost_model}
        eipu = EIpuAcquisitionFunction(models, active_metric=INTERNAL_METRIC_NAME)
        hp_ranges = active_model.hp_ranges_for_prediction()
        opt = LBFGSOptimizeAcquisition(
            hp_ranges, models, EIpuAcquisitionFunction, INTERNAL_METRIC_NAME
        )
        non_zero_acq_at_least_once = False
        initial_point = random.uniform(low=0.0, high=0.1, size=(2,))
        acq0, df0 = eipu.compute_acq_with_gradient(initial_point)
        if debug_output:
            print("\nInitial point: f(x0) = {}, x0 = {}".format(acq0, initial_point))
            print("grad0 = {}".format(df0))
        if acq0 != 0:
            non_zero_acq_at_least_once = True
            init_cand = hp_ranges.from_ndarray(initial_point)
            optimized = hp_ranges.to_ndarray(opt.optimize(init_cand))
            acq_opt = eipu.compute_acq(optimized)[0]
            if debug_output:
                print("Final point: f(x1) = {}, x1 = {}".format(acq_opt, optimized))
            assert acq_opt < 0
            assert acq_opt < acq0

        assert non_zero_acq_at_least_once


def test_numerical_gradient():
    debug_output = False
    random = np.random.RandomState(42)
    eps = 1e-6

    active_models = default_models(INTERNAL_METRIC_NAME)
    cost_models = default_models(COST_METRIC_NAME)
    for active_model, cost_model in zip(active_models, cost_models):
        models = {INTERNAL_METRIC_NAME: active_model, COST_METRIC_NAME: cost_model}
        for exponent_cost in [1.0, 0.5, 0.2]:
            eipu = EIpuAcquisitionFunction(
                models, active_metric=INTERNAL_METRIC_NAME, exponent_cost=exponent_cost
            )
            high = 0.02
            x = random.uniform(low=0.0, high=high, size=(2,))
            f0, analytical_gradient = eipu.compute_acq_with_gradient(x)
            analytical_gradient = analytical_gradient.flatten()
            if debug_output:
                print(
                    "x0 = {}, f(x_0) = {}, grad(x_0) = {}".format(
                        x, f0, analytical_gradient
                    )
                )
            for i in range(2):
                h = np.zeros_like(x)
                h[i] = eps
                fpeps = eipu.compute_acq(x + h)[0]
                fmeps = eipu.compute_acq(x - h)[0]
                numerical_derivative = (fpeps - fmeps) / (2 * eps)
                if debug_output:
                    print(
                        "f(x0+eps) = {}, f(x0-eps) = {}, findiff = {}, deriv = {}".format(
                            fpeps, fmeps, numerical_derivative, analytical_gradient[i]
                        )
                    )
                np.testing.assert_almost_equal(
                    numerical_derivative, analytical_gradient[i], decimal=4
                )


def test_value_same_as_with_gradient():
    # test that compute_acq and compute_acq_with_gradients return the same acquisition values
    active_models = default_models(INTERNAL_METRIC_NAME)
    cost_models = default_models(COST_METRIC_NAME)
    for active_model, cost_model in zip(active_models, cost_models):
        models = {INTERNAL_METRIC_NAME: active_model, COST_METRIC_NAME: cost_model}
        for exponent_cost in [1.0, 0.5, 0.2]:
            eipu = EIpuAcquisitionFunction(
                models, active_metric=INTERNAL_METRIC_NAME, exponent_cost=exponent_cost
            )
            random = np.random.RandomState(42)
            X = random.uniform(low=0.0, high=1.0, size=(10, 2))
            # assert same as computation with gradients
            vec1 = eipu.compute_acq(X).flatten()
            vec2 = np.array([eipu.compute_acq_with_gradient(x)[0] for x in X])
            np.testing.assert_almost_equal(vec1, vec2)

File Path: tst/schedulers/bayesopt/test_bo_algorithm_components.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    dictionarize_objective,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.config_space import uniform, choice, randint
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_tuning_job_state,
)


@pytest.fixture(scope="function")
def tuning_job_state():
    hp_ranges1 = make_hyperparameter_ranges(
        {"a1_hp_1": uniform(-5.0, 5.0), "a1_hp_2": choice(["a", "b", "c"])}
    )
    X1 = [(-3.0, "a"), (-1.9, "c"), (-3.5, "a")]
    Y1 = [dictionarize_objective(y) for y in (1.0, 2.0, 0.3)]
    hp_ranges2 = make_hyperparameter_ranges(
        {"a1_hp_1": uniform(-5.0, 5.0), "a1_hp_2": randint(-5, 5)}
    )
    X2 = [(-1.9, -1), (-3.5, 3)]
    Y2 = [dictionarize_objective(y) for y in (0.0, 2.0)]
    return {
        "algo-1": create_tuning_job_state(
            hp_ranges=hp_ranges1, cand_tuples=X1, metrics=Y1
        ),
        "algo-2": create_tuning_job_state(
            hp_ranges=hp_ranges2, cand_tuples=X2, metrics=Y2
        ),
    }


@pytest.fixture(scope="function")
def tuning_job_sub_state():
    hp_ranges = make_hyperparameter_ranges(dict())
    return TuningJobState.empty_state(hp_ranges)

File Path: tst/schedulers/bayesopt/test_bo_algorithm_functions.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm import (
    _pick_from_locally_optimized,
    _lazily_locally_optimize,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.bo_algorithm_components import (
    NoOptimization,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.duplicate_detector import (
    DuplicateDetectorIdentical,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.config_space import uniform, randint, choice
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    tuples_to_configs,
    create_exclusion_set,
)


def test_pick_from_locally_optimized():
    duplicate_detector = DuplicateDetectorIdentical()
    hp_ranges = make_hyperparameter_ranges(
        {"hp1": uniform(-10.0, 10.0), "hp2": uniform(-10.0, 10.0)}
    )
    _original = tuples_to_configs(
        [
            (0.1, 1.0),
            (0.1, 1.0),  # not a duplicate
            (0.2, 1.0),  # duplicate optimized; Resolved by the original
            (0.1, 1.0),  # complete duplicate
            (0.3, 1.0),  # blacklisted original
            (0.4, 3.0),  # blacklisted all
            (1.0, 2.0),  # final candidate to be selected into a batch
            (0.0, 2.0),  # skipped
            (0.0, 2.0),  # skipped
        ],
        hp_ranges=hp_ranges,
    )
    _optimized = tuples_to_configs(
        [
            (0.1, 1.0),
            (0.6, 1.0),
            (0.1, 1.0),
            (0.1, 1.0),
            (0.1, 1.0),
            (0.3, 1.0),
            (1.0, 1.0),
            (1.0, 0.0),
            (1.0, 0.0),
        ],
        hp_ranges=hp_ranges,
    )
    exclusion_candidates = create_exclusion_set(
        [(0.3, 1.0), (0.4, 3.0), (0.0, 0.0)], hp_ranges
    )
    got = _pick_from_locally_optimized(
        candidates_with_optimization=list(zip(_original, _optimized)),
        exclusion_candidates=exclusion_candidates,
        num_candidates=4,
        duplicate_detector=duplicate_detector,
    )

    expected = tuples_to_configs(
        [(0.1, 1.0), (0.6, 1.0), (0.2, 1.0), (1.0, 1.0)], hp_ranges=hp_ranges
    )

    # order of the candidates should be preserved
    assert len(expected) == len(got)
    assert all(a == b for a, b in zip(got, expected))


def test_lazily_locally_optimize():
    hp_ranges = make_hyperparameter_ranges(
        {
            "a": uniform(0.0, 2.0),
            "b": choice(["a", "c", "d"]),
            "c": randint(0, 3),
            "d": choice(["a", "b", "d"]),
        }
    )
    original_candidates = tuples_to_configs(
        [(1.0, "a", 3, "b"), (2.0, "c", 2, "a"), (0.0, "d", 0, "d")], hp_ranges
    )

    # NoOptimization class is used to check the interfaces only in here
    i = 0
    for candidate in _lazily_locally_optimize(
        candidates=original_candidates,
        local_optimizer=NoOptimization(None, None, None),
        hp_ranges=hp_ranges,
        model=None,
    ):
        # no optimization is applied ot the candidates
        assert candidate[0] == original_candidates[i]
        assert candidate[1] == original_candidates[i]
        i += 1

    assert i == len(original_candidates)
    assert (
        len(
            list(
                _lazily_locally_optimize(
                    candidates=[],
                    local_optimizer=NoOptimization(None, None, None),
                    hp_ranges=hp_ranges,
                    model=None,
                )
            )
        )
        == 0
    )

File Path: tst/schedulers/bayesopt/test_checkpointing.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import pickle

from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    gp_fifo_searcher_defaults,
    constrained_gp_fifo_searcher_defaults,
    cost_aware_gp_fifo_searcher_defaults,
)
from syne_tune.optimizer.schedulers.searchers.gp_fifo_searcher import GPFIFOSearcher
from syne_tune.optimizer.schedulers.searchers.constrained_gp_fifo_searcher import (
    ConstrainedGPFIFOSearcher,
)
from syne_tune.optimizer.schedulers.searchers.cost_aware_gp_fifo_searcher import (
    CostAwareGPFIFOSearcher,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    INTERNAL_METRIC_NAME,
    INTERNAL_CONSTRAINT_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.comparison_gpy import (
    Ackley,
    sample_data,
    assert_equal_candidates,
    assert_equal_randomstate,
)


def test_pickle_gp_fifo_searcher():
    random_seed = 894623209
    # This data is used below
    _, searcher_options, _ = gp_fifo_searcher_defaults()
    num_data = searcher_options["num_init_random"] + 2
    num_pending = 2
    data = sample_data(Ackley, num_train=num_data + num_pending, num_grid=5)
    # Create searcher1 using default arguments
    searcher_options["config_space"] = data["state"].hp_ranges.config_space
    searcher_options["scheduler"] = "fifo"
    searcher_options["random_seed"] = random_seed
    reward_attr = "accuracy"
    searcher_options["metric"] = reward_attr
    searcher_options["debug_log"] = False
    searcher1 = GPFIFOSearcher(**searcher_options)
    # Feed searcher1 with some data
    config_for_trial = data["state"].config_for_trial
    for ev in data["state"].trials_evaluations[:num_data]:
        reward = ev.metrics[INTERNAL_METRIC_NAME]
        map_reward = searcher1.map_reward
        if map_reward is not None:
            reward = map_reward.reverse(reward)
        config = config_for_trial[ev.trial_id]
        searcher1._update(ev.trial_id, config, {reward_attr: reward})
    # Calling next_config is forcing a GP hyperparameter update
    next_config = searcher1.get_config()
    # Register some pending evaluations
    for ev in data["state"].trials_evaluations[-num_pending:]:
        config = config_for_trial[ev.trial_id]
        searcher1.register_pending(ev.trial_id, config=config)
    # Pickle mutable state of searcher1
    pkl_state = pickle.dumps(searcher1.get_state())
    # Clone searcher2 from mutable state
    searcher2 = GPFIFOSearcher(**searcher_options)
    searcher2 = searcher2.clone_from_state(pickle.loads(pkl_state))
    # At this point, searcher1 and searcher2 should be essentially the same
    # Compare model parameters
    params1 = searcher1.model_parameters()
    params2 = searcher2.model_parameters()
    for k, v1 in params1.items():
        v2 = params2[k]
        np.testing.assert_almost_equal(np.array([v1]), np.array([v2]), decimal=4)
    # Compare states
    state1 = searcher1.state_transformer.state
    state2 = searcher2.state_transformer.state
    hp_ranges = state1.hp_ranges
    configs1, _ = state1.observed_data_for_metric()
    configs2, _ = state2.observed_data_for_metric()
    assert_equal_candidates(configs1, configs2, hp_ranges, decimal=5)
    eval_targets1 = np.array(
        [x.metrics[INTERNAL_METRIC_NAME] for x in state1.trials_evaluations]
    )
    eval_targets2 = np.array(
        [x.metrics[INTERNAL_METRIC_NAME] for x in state1.trials_evaluations]
    )
    np.testing.assert_almost_equal(eval_targets1, eval_targets2, decimal=5)
    assert_equal_candidates(
        state1.pending_configurations(),
        state2.pending_configurations(),
        hp_ranges,
        decimal=5,
    )
    # Compare random_state, random_generator state
    assert_equal_randomstate(searcher1.random_state, searcher2.random_state)


def test_pickle_constrained_gp_fifo_searcher():
    random_seed = 894623209
    # This data is used below
    _, searcher_options, _ = constrained_gp_fifo_searcher_defaults()
    num_data = searcher_options["num_init_random"] + 2
    num_pending = 2
    data = sample_data(Ackley, num_train=num_data + num_pending, num_grid=5)
    # Create searcher1 using default arguments
    searcher_options["config_space"] = data["state"].hp_ranges.config_space
    searcher_options["scheduler"] = "fifo"
    searcher_options["random_seed"] = random_seed
    reward_attr = "accuracy"
    searcher_options["metric"] = reward_attr
    searcher_options["debug_log"] = False
    constraint_attr = INTERNAL_CONSTRAINT_NAME
    searcher_options["constraint_attr"] = constraint_attr
    searcher1 = ConstrainedGPFIFOSearcher(**searcher_options)
    # Feed searcher1 with some data
    config_for_trial = data["state"].config_for_trial
    for ev in data["state"].trials_evaluations[:num_data]:
        reward = ev.metrics[INTERNAL_METRIC_NAME]
        map_reward = searcher1.map_reward
        if map_reward is not None:
            reward = map_reward.reverse(reward)
        config = config_for_trial[ev.trial_id]
        searcher1._update(
            ev.trial_id, config, {reward_attr: reward, constraint_attr: 1.0}
        )

    # Calling next_config is forcing a GP hyperparameter update
    next_config = searcher1.get_config()
    # Register some pending evaluations
    for ev in data["state"].trials_evaluations[-num_pending:]:
        config = config_for_trial[ev.trial_id]
        searcher1.register_pending(ev.trial_id, config=config)
    # Pickle mutable state of searcher1
    pkl_state = pickle.dumps(searcher1.get_state())
    # Clone searcher2 from mutable state
    searcher2 = ConstrainedGPFIFOSearcher(**searcher_options)
    searcher2 = searcher2.clone_from_state(pickle.loads(pkl_state))
    # At this point, searcher1 and searcher2 should be essentially the same
    # Compare model parameters
    params1 = searcher1.model_parameters()
    params2 = searcher2.model_parameters()
    for output_metric in params1.keys():
        for k, v1 in params1[output_metric].items():
            v2 = params2[output_metric][k]
            np.testing.assert_almost_equal(np.array([v1]), np.array([v2]), decimal=4)
    # Compare states
    state1 = searcher1.state_transformer.state
    state2 = searcher2.state_transformer.state
    hp_ranges = state1.hp_ranges
    configs1, _ = state1.observed_data_for_metric()
    configs2, _ = state2.observed_data_for_metric()
    assert_equal_candidates(configs1, configs2, hp_ranges, decimal=5)
    eval_targets1 = np.array(
        [x.metrics[INTERNAL_METRIC_NAME] for x in state1.trials_evaluations]
    )
    eval_targets2 = np.array(
        [x.metrics[INTERNAL_METRIC_NAME] for x in state1.trials_evaluations]
    )
    np.testing.assert_almost_equal(eval_targets1, eval_targets2, decimal=5)
    assert_equal_candidates(
        state1.pending_configurations(),
        state2.pending_configurations(),
        hp_ranges,
        decimal=5,
    )
    # Compare random_state, random_generator state
    assert_equal_randomstate(searcher1.random_state, searcher2.random_state)


def test_pickle_cost_aware_gp_fifo_searcher():
    random_seed = 894623209
    # This data is used below
    _, searcher_options, _ = cost_aware_gp_fifo_searcher_defaults()
    num_data = searcher_options["num_init_random"] + 2
    num_pending = 2
    data = sample_data(Ackley, num_train=num_data + num_pending, num_grid=5)
    # Create searcher1 using default arguments
    searcher_options["config_space"] = data["state"].hp_ranges.config_space
    searcher_options["scheduler"] = "fifo"
    searcher_options["random_seed"] = random_seed
    reward_attr = "accuracy"
    searcher_options["metric"] = reward_attr
    cost_attr = "elapsed_time"
    searcher_options["cost_attr"] = cost_attr
    searcher_options["debug_log"] = False
    searcher1 = CostAwareGPFIFOSearcher(**searcher_options)
    # Feed searcher1 with some data
    config_for_trial = data["state"].config_for_trial
    for ev in data["state"].trials_evaluations[:num_data]:
        reward = ev.metrics[INTERNAL_METRIC_NAME]
        map_reward = searcher1.map_reward
        if map_reward is not None:
            reward = map_reward.reverse(reward)
        config = config_for_trial[ev.trial_id]
        searcher1._update(ev.trial_id, config, {reward_attr: reward, cost_attr: 1.0})

    # Calling next_config is forcing a GP hyperparameter update
    next_config = searcher1.get_config()
    # Register some pending evaluations
    for ev in data["state"].trials_evaluations[-num_pending:]:
        config = config_for_trial[ev.trial_id]
        searcher1.register_pending(ev.trial_id, config=config)
    # Pickle mutable state of searcher1
    pkl_state = pickle.dumps(searcher1.get_state())
    # Clone searcher2 from mutable state
    searcher2 = CostAwareGPFIFOSearcher(**searcher_options)
    searcher2 = searcher2.clone_from_state(pickle.loads(pkl_state))
    # At this point, searcher1 and searcher2 should be essentially the same
    # Compare model parameters
    params1 = searcher1.model_parameters()
    params2 = searcher2.model_parameters()
    for output_metric in params1.keys():
        for k, v1 in params1[output_metric].items():
            v2 = params2[output_metric][k]
            np.testing.assert_almost_equal(np.array([v1]), np.array([v2]), decimal=4)
    # Compare states
    state1 = searcher1.state_transformer.state
    state2 = searcher2.state_transformer.state
    hp_ranges = state1.hp_ranges
    configs1, _ = state1.observed_data_for_metric()
    configs2, _ = state2.observed_data_for_metric()
    assert_equal_candidates(configs1, configs2, hp_ranges, decimal=5)
    eval_targets1 = np.array(
        [x.metrics[INTERNAL_METRIC_NAME] for x in state1.trials_evaluations]
    )
    eval_targets2 = np.array(
        [x.metrics[INTERNAL_METRIC_NAME] for x in state1.trials_evaluations]
    )
    np.testing.assert_almost_equal(eval_targets1, eval_targets2, decimal=5)
    assert_equal_candidates(
        state1.pending_configurations(),
        state2.pending_configurations(),
        hp_ranges,
        decimal=5,
    )
    # Compare random_state, random_generator state
    assert_equal_randomstate(searcher1.random_state, searcher2.random_state)


if __name__ == "__main__":
    test_pickle_gp_fifo_searcher()

File Path: tst/schedulers/bayesopt/test_common.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Set, Tuple
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    Configuration,
    dictionarize_objective,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.tuning_algorithms.common import (
    ExclusionList,
    generate_unique_candidates,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    RepeatedCandidateGenerator,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.config_space import randint, choice
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_tuning_job_state,
    create_exclusion_set,
)


@pytest.fixture(scope="function")
def hp_ranges():
    return make_hyperparameter_ranges(
        {"hp1": randint(0, 200), "hp2": choice(["a", "b", "c"])}
    )


@pytest.mark.parametrize(
    "observed_data,failed_tuples,pending_tuples,expected",
    [
        ([], [], [], set()),
        ([((123, "a"), 9.87)], [], [], {"hp1:123,hp2:0"}),
        ([], [(123, "a")], [], {"hp1:123,hp2:0"}),
        ([], [], [(123, "a")], {"hp1:123,hp2:0"}),
        (
            [((1, "a"), 9.87)],
            [(2, "b")],
            [(3, "c")],
            {"hp1:1,hp2:0", "hp1:2,hp2:1", "hp1:3,hp2:2"},
        ),
    ],
)
def test_compute_blacklisted_candidates(
    hp_ranges: HyperparameterRanges,
    observed_data: List[Tuple],
    failed_tuples: List[Tuple],
    pending_tuples: List[Tuple],
    expected: Set[str],
):
    if observed_data:
        cand_tuples, metrics = zip(*observed_data)
    else:
        cand_tuples = []
        metrics = []
    if metrics:
        metrics = [dictionarize_objective(y) for y in metrics]
    state = create_tuning_job_state(
        hp_ranges,
        cand_tuples=cand_tuples,
        metrics=metrics,
        pending_tuples=pending_tuples,
        failed_tuples=failed_tuples,
    )
    actual = ExclusionList(state)
    assert set(expected) == actual.excl_set


def _assert_no_duplicates(
    candidates: List[Configuration], hp_ranges: HyperparameterRanges
):
    cands_tpl = [hp_ranges.config_to_match_string(x) for x in candidates]
    assert len(candidates) == len(set(cands_tpl))


@pytest.mark.parametrize(
    "num_unique_candidates,num_requested_candidates", [(5, 10), (15, 10)]
)
def test_generate_unique_candidates(num_unique_candidates, num_requested_candidates):
    generator = RepeatedCandidateGenerator(num_unique_candidates)
    hp_ranges = generator.hp_ranges
    exclusion_candidates = create_exclusion_set([], hp_ranges)
    candidates = generate_unique_candidates(
        candidates_generator=generator,
        num_candidates=num_requested_candidates,
        exclusion_candidates=exclusion_candidates,
    )
    assert len(candidates) == min(num_unique_candidates, num_requested_candidates)
    _assert_no_duplicates(candidates, hp_ranges)

    # introduce excluded candidates, simply take a few already unique
    size_excluded = len(candidates) // 2
    excluded = list(candidates)[:size_excluded]
    exclusion_candidates = create_exclusion_set(
        excluded, generator.hp_ranges, is_dict=True
    )
    candidates = generate_unique_candidates(
        candidates_generator=generator,
        num_candidates=num_requested_candidates,
        exclusion_candidates=exclusion_candidates,
    )

    # total unique candidates are adjusted by the number of excluded candidates which are unique too due to set()
    assert len(candidates) == min(
        num_unique_candidates - len(excluded), num_requested_candidates
    )
    _assert_no_duplicates(candidates, hp_ranges)

File Path: tst/schedulers/bayesopt/test_duplicate_detector.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.duplicate_detector import (
    DuplicateDetectorIdentical,
    DuplicateDetectorNoDetection,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.config_space import uniform, randint, choice
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_exclusion_set,
)


hp_ranges = make_hyperparameter_ranges(
    {
        "hp1": randint(0, 1000000000),
        "hp2": uniform(-10.0, 10.0),
        "hp3": choice(["a", "b", "c"]),
    }
)


@pytest.mark.parametrize(
    "existing, new, contained",
    [
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10000, 3.0, "c"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.000001, "a"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (20, 2.000001, "b"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (25, 1.0, "a"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0, "a"), True),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (20, 2.0, "b"), True),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (19, 1.0, "a"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0000001, "a"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0, "c"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0, "b"), False),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (20, 1.0, "b"), False),
    ],
)
def test_contains_identical(existing, new, contained):
    existing = create_exclusion_set(existing, hp_ranges)
    new = hp_ranges.tuple_to_config(new)
    assert DuplicateDetectorIdentical().contains(existing, new) == contained


@pytest.mark.parametrize(
    "existing, new",
    [
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10000, 3.0, "c")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.000001, "a")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (20, 2.000001, "b")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (25, 1.0, "a")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0, "a")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (20, 2.0, "b")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (19, 1.0, "a")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0000001, "a")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0, "c")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (10, 1.0, "b")),
        ([(10, 1.0, "a"), (20, 2.0, "b")], (20, 1.0, "b")),
    ],
)
def test_contains_no_detection(existing, new):
    existing = create_exclusion_set(existing, hp_ranges)
    new = hp_ranges.tuple_to_config(new)
    assert not DuplicateDetectorNoDetection().contains(existing, new)

File Path: tst/schedulers/bayesopt/test_expdecay_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import json
import numpy as np
import pytest

from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    gp_multifidelity_searcher_defaults,
    gp_multifidelity_searcher_factory,
)
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import (
    decode_state_from_old_encoding,
)
from syne_tune.config_space import randint, uniform, loguniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)


def _common_kwargs(config_space: Dict) -> Dict:
    return {
        "config_space": config_space,
        "max_epochs": config_space["epochs"],
        "metric": "accuracy",
        "resource_attr": "epoch",
        "scheduler": "hyperband_stopping",
        "scheduler_mode": "max",
        "debug_log": False,
        "normalize_targets": True,
    }


def build_gp_model_factory(config_space: Dict, model_params: Dict) -> Dict:
    kwargs = dict(
        _common_kwargs(config_space),
        model="gp_multitask",
        gp_resource_kernel="freeze-thaw",
    )
    _kwargs = check_and_merge_defaults(
        kwargs, *gp_multifidelity_searcher_defaults(), dict_name="search_options"
    )
    kwargs_int = gp_multifidelity_searcher_factory(**_kwargs)
    kwargs_int["model_factory"].set_params(model_params)
    return kwargs_int


def _convert_keys(dict_old, dict_new, pref_old, pref_new):
    for k, v in dict_old.items():
        if k.startswith(pref_old):
            k_new = pref_new + k[len(pref_old) :]
            dict_new[k_new] = v


def _convert_model_params(model_params: Dict) -> Dict:
    """
    Convert from `GaussianProcessRegression` to
    `GaussianProcessLearningCurveModel` parameters.

    :param model_params:
    :return:
    """
    new_model_params = {"noise_variance": model_params["noise_variance"]}
    _convert_keys(model_params, new_model_params, "kernel_kernelx", "kernel")
    _convert_keys(model_params, new_model_params, "kernel_meanx", "mean")
    for tag in ["alpha", "mean_lam", "gamma"]:
        new_model_params["expdecay_" + tag] = model_params["kernel_" + tag]
    return new_model_params


def build_gped_model_factory(config_space: Dict, model_params: Dict, **kwargs):
    kwargs = dict(
        _common_kwargs(config_space),
        model="gp_expdecay",
        expdecay_normalize_inputs=True,
        **kwargs
    )
    _kwargs = check_and_merge_defaults(
        kwargs, *gp_multifidelity_searcher_defaults(), dict_name="search_options"
    )
    kwargs_int = gp_multifidelity_searcher_factory(**_kwargs)
    # Need to convert `model_params`
    kwargs_int["model_factory"].set_params(_convert_model_params(model_params))
    return kwargs_int


# We ran launch_sample_searcher_states.py to sample the searcher states
# used here, which runs MOBSTER (hyperband_stopping, bayesopt) with the
# mlp_fashionmnist_benchmark

_model_params = []
_state = []

# elapsed_time = 510.4632730484009
# num_observations = 80
# num_configs = 14
_model_params.append(
    '{"noise_variance": 0.0003465269369921202, "kernel_alpha": 0.32006841518063855, "kernel_mean_lam": 23.993482465964025, "kernel_gamma": 1.0, "kernel_kernelx_inv_bw0": 0.00010000000000000009, "kernel_kernelx_inv_bw1": 1.83640681099597, "kernel_kernelx_inv_bw2": 0.0022959259096544463, "kernel_kernelx_inv_bw3": 3.583941260907946, "kernel_kernelx_inv_bw4": 1.1390978939096217, "kernel_kernelx_inv_bw5": 0.0005059210120066444, "kernel_kernelx_inv_bw6": 0.00010000000000000009, "kernel_kernelx_covariance_scale": 2.208213263615384, "kernel_meanx_mean_value": 1.2132901253498967}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 22.84876799583435}, "active_metric": {"1": 0.7708333333333334}}}, {"candidate": {"n_units_1": 29, "n_units_2": 584, "batch_size": 41, "dropout_1": 0.6843134351325143, "dropout_2": 0.20866634010808638, "learning_rate": 0.0007531586746050282, "wd": 8.542702035112525e-08}, "metrics": {"cost_metric": {"1": 16.058557987213135}, "active_metric": {"1": 0.4832881662149955}}}, {"candidate": {"n_units_1": 529, "n_units_2": 660, "batch_size": 30, "dropout_1": 0.5490868842845377, "dropout_2": 0.7920401266415242, "learning_rate": 2.101654064946448e-06, "wd": 9.568417280766271e-08}, "metrics": {"cost_metric": {"1": 33.76100707054138}, "active_metric": {"1": 0.47597597597597596}}}, {"candidate": {"n_units_1": 828, "n_units_2": 634, "batch_size": 123, "dropout_1": 0.4257740273137951, "dropout_2": 0.23113551626017848, "learning_rate": 6.860930897860399e-06, "wd": 8.101814548564974e-08}, "metrics": {"cost_metric": {"1": 27.756996154785156}, "active_metric": {"1": 0.33835190203753884}}}, {"candidate": {"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06}, "metrics": {"cost_metric": {"1": 26.10046887397766, "2": 53.824394941329956, "3": 80.07230305671692, "4": 106.25571393966675, "5": 133.94080090522766, "6": 170.4485969543457, "7": 200.93623304367065, "8": 229.56330180168152, "9": 257.11693382263184, "10": 286.6725299358368, "11": 319.0151889324188, "12": 354.14807295799255, "13": 386.9317948818207, "14": 419.8007171154022}, "active_metric": {"1": 0.14744363929146542, "2": 0.13586956521739135, "3": 0.12983091787439616, "4": 0.12661030595813205, "5": 0.1279186795491143, "6": 0.12328904991948475, "7": 0.11674718196457323, "8": 0.1126207729468599, "9": 0.11292270531400961, "10": 0.11513687600644118, "11": 0.11584138486312401, "12": 0.11101046698872785, "13": 0.1090982286634461, "14": 0.10738727858293073}}}, {"candidate": {"n_units_1": 931, "n_units_2": 527, "batch_size": 20, "dropout_1": 0.41104972550281815, "dropout_2": 0.7568250476630992, "learning_rate": 0.8320700517813453, "wd": 1.9853072351534312e-08}, "metrics": {"cost_metric": {"1": 117.45865964889526}, "active_metric": {"1": 0.9045}}}, {"candidate": {"n_units_1": 576, "n_units_2": 265, "batch_size": 68, "dropout_1": 0.3230142899473453, "dropout_2": 0.3493653937208789, "learning_rate": 0.0004443989879229176, "wd": 0.012741750252890758}, "metrics": {"cost_metric": {"1": 23.032225131988525, "2": 53.855369091033936, "3": 78.89936113357544}, "active_metric": {"1": 0.18497398959583833, "2": 0.1910764305722289, "3": 0.17987194877951185}}}, {"candidate": {"n_units_1": 606, "n_units_2": 635, "batch_size": 36, "dropout_1": 0.43630659905530433, "dropout_2": 0.5387515572964027, "learning_rate": 0.0002641791837664535, "wd": 3.148533916384597e-05}, "metrics": {"cost_metric": {"1": 33.83635210990906, "2": 66.65014815330505, "3": 99.47553634643555}, "active_metric": {"1": 0.1605495387083835, "2": 0.1470116325711993, "3": 0.1400922583233053}}}, {"candidate": {"n_units_1": 534, "n_units_2": 1014, "batch_size": 35, "dropout_1": 0.40293710200390886, "dropout_2": 0.2615463742465658, "learning_rate": 0.0004610005312393892, "wd": 5.324412228404534e-07}, "metrics": {"cost_metric": {"1": 40.18646717071533, "2": 77.50762224197388, "3": 113.17754912376404}, "active_metric": {"1": 0.1589974937343358, "2": 0.1334335839598998, "3": 0.127218045112782}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06, "RESOURCE_ATTR_epoch": 12}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 32}, {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 22}, {"n_units_1": 534, "n_units_2": 1014, "batch_size": 35, "dropout_1": 0.40293710200390886, "dropout_2": 0.2615463742465658, "learning_rate": 0.0004610005312393892, "wd": 5.324412228404534e-07, "RESOURCE_ATTR_epoch": 2}]}'
)

# elapsed_time = 266.8967089653015
# num_observations = 40
# num_configs = 11
_model_params.append(
    '{"noise_variance": 0.0013634444543287733, "kernel_alpha": 0.33215948811021134, "kernel_mean_lam": 15.077221388452632, "kernel_gamma": 1.0, "kernel_kernelx_inv_bw0": 0.0025264221960287574, "kernel_kernelx_inv_bw1": 4.586734084695334, "kernel_kernelx_inv_bw2": 0.003154109460037928, "kernel_kernelx_inv_bw3": 0.000894766660821803, "kernel_kernelx_inv_bw4": 0.005214180628382712, "kernel_kernelx_inv_bw5": 10.093642962950414, "kernel_kernelx_inv_bw6": 0.016153252870882064, "kernel_kernelx_covariance_scale": 1.2311611100489817, "kernel_meanx_mean_value": 0.09552280164740308}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 22.84876799583435}, "active_metric": {"1": 0.7708333333333334}}}, {"candidate": {"n_units_1": 29, "n_units_2": 584, "batch_size": 41, "dropout_1": 0.6843134351325143, "dropout_2": 0.20866634010808638, "learning_rate": 0.0007531586746050282, "wd": 8.542702035112525e-08}, "metrics": {"cost_metric": {"1": 16.058557987213135}, "active_metric": {"1": 0.4832881662149955}}}, {"candidate": {"n_units_1": 529, "n_units_2": 660, "batch_size": 30, "dropout_1": 0.5490868842845377, "dropout_2": 0.7920401266415242, "learning_rate": 2.101654064946448e-06, "wd": 9.568417280766271e-08}, "metrics": {"cost_metric": {"1": 33.76100707054138}, "active_metric": {"1": 0.47597597597597596}}}, {"candidate": {"n_units_1": 828, "n_units_2": 634, "batch_size": 123, "dropout_1": 0.4257740273137951, "dropout_2": 0.23113551626017848, "learning_rate": 6.860930897860399e-06, "wd": 8.101814548564974e-08}, "metrics": {"cost_metric": {"1": 27.756996154785156}, "active_metric": {"1": 0.33835190203753884}}}, {"candidate": {"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06}, "metrics": {"cost_metric": {"1": 26.10046887397766, "2": 53.824394941329956, "3": 80.07230305671692, "4": 106.25571393966675, "5": 133.94080090522766, "6": 170.4485969543457, "7": 200.93623304367065, "8": 229.56330180168152, "9": 257.11693382263184, "10": 286.6725299358368, "11": 319.0151889324188, "12": 354.14807295799255, "13": 386.9317948818207, "14": 419.8007171154022}, "active_metric": {"1": 0.14744363929146542, "2": 0.13586956521739135, "3": 0.12983091787439616, "4": 0.12661030595813205, "5": 0.1279186795491143, "6": 0.12328904991948475, "7": 0.11674718196457323, "8": 0.1126207729468599, "9": 0.11292270531400961, "10": 0.11513687600644118, "11": 0.11584138486312401, "12": 0.11101046698872785, "13": 0.1090982286634461, "14": 0.10738727858293073}}}, {"candidate": {"n_units_1": 931, "n_units_2": 527, "batch_size": 20, "dropout_1": 0.41104972550281815, "dropout_2": 0.7568250476630992, "learning_rate": 0.8320700517813453, "wd": 1.9853072351534312e-08}, "metrics": {"cost_metric": {"1": 117.45865964889526}, "active_metric": {"1": 0.9045}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06, "RESOURCE_ATTR_epoch": 4}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 17}, {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 12}]}'
)

# elapsed_time = 164.4495289325714
# num_observations = 25
# num_configs = 9
_model_params.append(
    '{"noise_variance": 0.000499818813466635, "kernel_alpha": 0.23713455205952677, "kernel_mean_lam": 45.09309080404886, "kernel_gamma": 0.9458802138724483, "kernel_kernelx_inv_bw0": 0.047224584489824036, "kernel_kernelx_inv_bw1": 5.4925599066759005, "kernel_kernelx_inv_bw2": 0.0013332553417099433, "kernel_kernelx_inv_bw3": 0.0065960016396222725, "kernel_kernelx_inv_bw4": 0.020133444037227638, "kernel_kernelx_inv_bw5": 10.716957151297681, "kernel_kernelx_inv_bw6": 0.003199027133464099, "kernel_kernelx_covariance_scale": 1.0755903136336264, "kernel_meanx_mean_value": 0.13675161355207244}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 22.84876799583435}, "active_metric": {"1": 0.7708333333333334}}}, {"candidate": {"n_units_1": 29, "n_units_2": 584, "batch_size": 41, "dropout_1": 0.6843134351325143, "dropout_2": 0.20866634010808638, "learning_rate": 0.0007531586746050282, "wd": 8.542702035112525e-08}, "metrics": {"cost_metric": {"1": 16.058557987213135}, "active_metric": {"1": 0.4832881662149955}}}, {"candidate": {"n_units_1": 529, "n_units_2": 660, "batch_size": 30, "dropout_1": 0.5490868842845377, "dropout_2": 0.7920401266415242, "learning_rate": 2.101654064946448e-06, "wd": 9.568417280766271e-08}, "metrics": {"cost_metric": {"1": 33.76100707054138}, "active_metric": {"1": 0.47597597597597596}}}, {"candidate": {"n_units_1": 828, "n_units_2": 634, "batch_size": 123, "dropout_1": 0.4257740273137951, "dropout_2": 0.23113551626017848, "learning_rate": 6.860930897860399e-06, "wd": 8.101814548564974e-08}, "metrics": {"cost_metric": {"1": 27.756996154785156}, "active_metric": {"1": 0.33835190203753884}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 7}, {"batch_size": 20, "dropout_1": 0.41104972550281815, "dropout_2": 0.7568250476630992, "learning_rate": 0.8320700517813453, "n_units_1": 931, "n_units_2": 527, "wd": 1.9853072351534312e-08, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 11}]}'
)

# elapsed_time = 130.01123476028442
# num_observations = 19
# num_configs = 7
_model_params.append(
    '{"noise_variance": 0.00024460591274526826, "kernel_alpha": 0.48004176199483545, "kernel_mean_lam": 49.99999999999999, "kernel_gamma": 0.734348408988702, "kernel_kernelx_inv_bw0": 0.024364546709466313, "kernel_kernelx_inv_bw1": 2.4581694176756876, "kernel_kernelx_inv_bw2": 0.00025864436451015127, "kernel_kernelx_inv_bw3": 0.00010000000000000009, "kernel_kernelx_inv_bw4": 0.0001075879781512719, "kernel_kernelx_inv_bw5": 7.628312984876062, "kernel_kernelx_inv_bw6": 0.0009132144114327569, "kernel_kernelx_covariance_scale": 1.1661945422479474, "kernel_meanx_mean_value": 0.06308935787869324}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 22.84876799583435}, "active_metric": {"1": 0.7708333333333334}}}, {"candidate": {"n_units_1": 29, "n_units_2": 584, "batch_size": 41, "dropout_1": 0.6843134351325143, "dropout_2": 0.20866634010808638, "learning_rate": 0.0007531586746050282, "wd": 8.542702035112525e-08}, "metrics": {"cost_metric": {"1": 16.058557987213135}, "active_metric": {"1": 0.4832881662149955}}}], "failed_candidates": [], "pending_candidates": [{"batch_size": 30, "dropout_1": 0.5490868842845377, "dropout_2": 0.7920401266415242, "learning_rate": 2.101654064946448e-06, "n_units_1": 529, "n_units_2": 660, "wd": 9.568417280766271e-08, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 8}, {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 6}]}'
)

# elapsed_time = 60.27482986450195
# num_observations = 8
# num_configs = 4
_model_params.append(
    '{"noise_variance": 0.0010000000000000002, "kernel_alpha": 1.0, "kernel_mean_lam": 0.5, "kernel_gamma": 0.5, "kernel_kernelx_inv_bw0": 1.0, "kernel_kernelx_inv_bw1": 1.0, "kernel_kernelx_inv_bw2": 1.0, "kernel_kernelx_inv_bw3": 1.0, "kernel_kernelx_inv_bw4": 1.0, "kernel_kernelx_inv_bw5": 1.0, "kernel_kernelx_inv_bw6": 1.0, "kernel_kernelx_covariance_scale": 1.0, "kernel_meanx_mean_value": 0.0}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 3}, {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 3}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 4}]}'
)

# elapsed_time = 30.158570051193237
# num_observations = 2
# num_configs = 2
_model_params.append(
    '{"noise_variance": 0.0010000000000000002, "kernel_alpha": 1.0, "kernel_mean_lam": 0.5, "kernel_gamma": 0.5, "kernel_kernelx_inv_bw0": 1.0, "kernel_kernelx_inv_bw1": 1.0, "kernel_kernelx_inv_bw2": 1.0, "kernel_kernelx_inv_bw3": 1.0, "kernel_kernelx_inv_bw4": 1.0, "kernel_kernelx_inv_bw5": 1.0, "kernel_kernelx_inv_bw6": 1.0, "kernel_kernelx_covariance_scale": 1.0, "kernel_meanx_mean_value": 0.0}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 2}, {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 2}]}'
)

# elapsed_time = 80.36403489112854
# num_observations = 11
# num_configs = 5
_model_params.append(
    '{"noise_variance": 0.0010000000000000002, "kernel_alpha": 1.0, "kernel_mean_lam": 0.5, "kernel_gamma": 0.5, "kernel_kernelx_inv_bw0": 1.0, "kernel_kernelx_inv_bw1": 1.0, "kernel_kernelx_inv_bw2": 1.0, "kernel_kernelx_inv_bw3": 1.0, "kernel_kernelx_inv_bw4": 1.0, "kernel_kernelx_inv_bw5": 1.0, "kernel_kernelx_inv_bw6": 1.0, "kernel_kernelx_covariance_scale": 1.0, "kernel_meanx_mean_value": 0.0}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 3}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 5}, {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 4}]}'
)

# elapsed_time = 299.2465078830719
# num_observations = 45
# num_configs = 12
_model_params.append(
    '{"noise_variance": 0.0003423634954194395, "kernel_alpha": 0.27529357970772367, "kernel_mean_lam": 20.70369146606114, "kernel_gamma": 1.0, "kernel_kernelx_inv_bw0": 0.00021426786615588259, "kernel_kernelx_inv_bw1": 1.949848307645357, "kernel_kernelx_inv_bw2": 0.0007825688851177687, "kernel_kernelx_inv_bw3": 3.4671378069531356, "kernel_kernelx_inv_bw4": 1.090384354169918, "kernel_kernelx_inv_bw5": 0.0028908649203899387, "kernel_kernelx_inv_bw6": 0.00011473931202983017, "kernel_kernelx_covariance_scale": 1.5931693693573752, "kernel_meanx_mean_value": 0.8391456738845063}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 22.84876799583435}, "active_metric": {"1": 0.7708333333333334}}}, {"candidate": {"n_units_1": 29, "n_units_2": 584, "batch_size": 41, "dropout_1": 0.6843134351325143, "dropout_2": 0.20866634010808638, "learning_rate": 0.0007531586746050282, "wd": 8.542702035112525e-08}, "metrics": {"cost_metric": {"1": 16.058557987213135}, "active_metric": {"1": 0.4832881662149955}}}, {"candidate": {"n_units_1": 529, "n_units_2": 660, "batch_size": 30, "dropout_1": 0.5490868842845377, "dropout_2": 0.7920401266415242, "learning_rate": 2.101654064946448e-06, "wd": 9.568417280766271e-08}, "metrics": {"cost_metric": {"1": 33.76100707054138}, "active_metric": {"1": 0.47597597597597596}}}, {"candidate": {"n_units_1": 828, "n_units_2": 634, "batch_size": 123, "dropout_1": 0.4257740273137951, "dropout_2": 0.23113551626017848, "learning_rate": 6.860930897860399e-06, "wd": 8.101814548564974e-08}, "metrics": {"cost_metric": {"1": 27.756996154785156}, "active_metric": {"1": 0.33835190203753884}}}, {"candidate": {"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06}, "metrics": {"cost_metric": {"1": 26.10046887397766, "2": 53.824394941329956, "3": 80.07230305671692, "4": 106.25571393966675, "5": 133.94080090522766, "6": 170.4485969543457, "7": 200.93623304367065, "8": 229.56330180168152, "9": 257.11693382263184, "10": 286.6725299358368, "11": 319.0151889324188, "12": 354.14807295799255, "13": 386.9317948818207, "14": 419.8007171154022}, "active_metric": {"1": 0.14744363929146542, "2": 0.13586956521739135, "3": 0.12983091787439616, "4": 0.12661030595813205, "5": 0.1279186795491143, "6": 0.12328904991948475, "7": 0.11674718196457323, "8": 0.1126207729468599, "9": 0.11292270531400961, "10": 0.11513687600644118, "11": 0.11584138486312401, "12": 0.11101046698872785, "13": 0.1090982286634461, "14": 0.10738727858293073}}}, {"candidate": {"n_units_1": 931, "n_units_2": 527, "batch_size": 20, "dropout_1": 0.41104972550281815, "dropout_2": 0.7568250476630992, "learning_rate": 0.8320700517813453, "wd": 1.9853072351534312e-08}, "metrics": {"cost_metric": {"1": 117.45865964889526}, "active_metric": {"1": 0.9045}}}, {"candidate": {"n_units_1": 576, "n_units_2": 265, "batch_size": 68, "dropout_1": 0.3230142899473453, "dropout_2": 0.3493653937208789, "learning_rate": 0.0004443989879229176, "wd": 0.012741750252890758}, "metrics": {"cost_metric": {"1": 23.032225131988525, "2": 53.855369091033936, "3": 78.89936113357544}, "active_metric": {"1": 0.18497398959583833, "2": 0.1910764305722289, "3": 0.17987194877951185}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06, "RESOURCE_ATTR_epoch": 5}, {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 13}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 19}, {"n_units_1": 576, "n_units_2": 265, "batch_size": 68, "dropout_1": 0.3230142899473453, "dropout_2": 0.3493653937208789, "learning_rate": 0.0004443989879229176, "wd": 0.012741750252890758, "RESOURCE_ATTR_epoch": 2}]}'
)

# elapsed_time = 107.72016072273254
# num_observations = 16
# num_configs = 6
_model_params.append(
    '{"noise_variance": 0.0007127447108752395, "kernel_alpha": 0.6611830364595244, "kernel_mean_lam": 49.99999999999999, "kernel_gamma": 1.0, "kernel_kernelx_inv_bw0": 0.00404913469299303, "kernel_kernelx_inv_bw1": 0.016052742254326176, "kernel_kernelx_inv_bw2": 0.0006873813980510872, "kernel_kernelx_inv_bw3": 0.0018486468658638656, "kernel_kernelx_inv_bw4": 2.2411401206695025, "kernel_kernelx_inv_bw5": 3.6329202178079703, "kernel_kernelx_inv_bw6": 0.0006680897104882892, "kernel_kernelx_covariance_scale": 0.7202707931819882, "kernel_meanx_mean_value": -0.25886685846400814}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 22.84876799583435}, "active_metric": {"1": 0.7708333333333334}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 5}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 7}, {"batch_size": 30, "dropout_1": 0.5490868842845377, "dropout_2": 0.7920401266415242, "learning_rate": 2.101654064946448e-06, "n_units_1": 529, "n_units_2": 660, "wd": 9.568417280766271e-08, "RESOURCE_ATTR_epoch": 1}]}'
)

# elapsed_time = 196.5048840045929
# num_observations = 30
# num_configs = 10
_model_params.append(
    '{"noise_variance": 0.0013634444543287733, "kernel_alpha": 0.33215948811021134, "kernel_mean_lam": 15.077221388452632, "kernel_gamma": 1.0, "kernel_kernelx_inv_bw0": 0.0025264221960287574, "kernel_kernelx_inv_bw1": 4.586734084695334, "kernel_kernelx_inv_bw2": 0.003154109460037928, "kernel_kernelx_inv_bw3": 0.000894766660821803, "kernel_kernelx_inv_bw4": 0.005214180628382712, "kernel_kernelx_inv_bw5": 10.093642962950414, "kernel_kernelx_inv_bw6": 0.016153252870882064, "kernel_kernelx_covariance_scale": 1.2311611100489817, "kernel_meanx_mean_value": 0.09552280164740308}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 15.282694101333618, "2": 32.215107917785645, "3": 48.62396502494812, "4": 62.46452808380127, "5": 77.36029410362244, "6": 91.85972809791565, "7": 107.89106607437134, "8": 122.8071219921112, "9": 138.6427869796753, "10": 155.3775441646576, "11": 171.86729407310486, "12": 186.69335007667542, "13": 204.0226011276245, "14": 219.01899409294128, "15": 234.25396490097046, "16": 249.26809406280518, "17": 264.3408589363098, "18": 280.09520292282104, "19": 296.0671851634979, "20": 319.6674599647522, "21": 337.2237899303436, "22": 354.650151014328, "23": 370.17603182792664, "24": 386.20747780799866, "25": 401.40604305267334, "26": 415.9903359413147, "27": 431.2174470424652, "28": 445.8420739173889, "29": 460.1389670372009, "30": 475.33422088623047, "31": 493.9087038040161, "32": 509.9310200214386, "33": 525.421621799469, "34": 540.1411230564117, "35": 554.5012362003326, "36": 569.0483500957489, "37": 583.1704621315002}, "active_metric": {"1": 0.41328783621035725, "2": 0.3706342834203131, "3": 0.351264552388599, "4": 0.33500602167804094, "5": 0.32707747892412686, "6": 0.3204536330790847, "7": 0.3140305098354075, "8": 0.2992773986350863, "9": 0.2865315134484143, "10": 0.27509032517061416, "11": 0.26525491770373344, "12": 0.2552187876354878, "13": 0.24989963869931753, "14": 0.24197109594540345, "15": 0.2352468887996788, "16": 0.22902448815736653, "17": 0.2233038940184665, "18": 0.2191890806904857, "19": 0.2133681252509032, "20": 0.21045764753111196, "21": 0.20503813729425935, "22": 0.20373344038538743, "23": 0.2004215174628663, "24": 0.1956041750301084, "25": 0.19299478121236446, "26": 0.19199116820553996, "27": 0.18908069048574871, "28": 0.18667201926936972, "29": 0.18526696105981533, "30": 0.18426334805299072, "31": 0.18406262545162588, "32": 0.18255720594138902, "33": 0.18145323163388194, "34": 0.18105178643115216, "35": 0.1785427539140907, "36": 0.1785427539140907, "37": 0.17723805700521877}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 22.334229230880737, "2": 47.57227420806885, "3": 68.38872504234314, "4": 91.17699599266052, "5": 114.11172008514404, "6": 137.2291030883789, "7": 161.9407799243927, "8": 186.01491808891296, "9": 210.73575019836426, "10": 233.72683000564575, "11": 256.1681180000305, "12": 279.3303301334381, "13": 311.3050580024719, "14": 336.1765010356903, "15": 362.16334199905396, "16": 385.8361783027649, "17": 408.6689350605011, "18": 430.98025918006897, "19": 453.24402713775635, "20": 474.99853110313416, "21": 501.653440952301, "22": 525.0653901100159, "23": 547.2747151851654, "24": 569.0442740917206, "25": 590.4288313388824}, "active_metric": {"1": 0.1725761217948718, "2": 0.15024038461538458, "3": 0.1353165064102564, "4": 0.13842147435897434, "5": 0.1303084935897436, "6": 0.12239583333333337, "7": 0.12089342948717952, "8": 0.11738782051282048, "9": 0.11909054487179482, "10": 0.11498397435897434, "11": 0.11618589743589747, "12": 0.11388221153846156, "13": 0.11087740384615385, "14": 0.11247996794871795, "15": 0.11227964743589747, "16": 0.11037660256410253, "17": 0.11237980769230771, "18": 0.10917467948717952, "19": 0.10506810897435892, "20": 0.10947516025641024, "21": 0.10627003205128205, "22": 0.10366586538461542, "23": 0.10356570512820518, "24": 0.10837339743589747, "25": 0.10406650641025639}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 23.648607969284058, "2": 50.783905029296875, "3": 94.63147592544556}, "active_metric": {"1": 0.16546618647458988, "2": 0.1404561824729892, "3": 0.1386554621848739}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 50.06452512741089}, "active_metric": {"1": 0.22955092221331197}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 13.569249868392944}, "active_metric": {"1": 0.3023558897243108}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 22.84876799583435}, "active_metric": {"1": 0.7708333333333334}}}, {"candidate": {"n_units_1": 29, "n_units_2": 584, "batch_size": 41, "dropout_1": 0.6843134351325143, "dropout_2": 0.20866634010808638, "learning_rate": 0.0007531586746050282, "wd": 8.542702035112525e-08}, "metrics": {"cost_metric": {"1": 16.058557987213135}, "active_metric": {"1": 0.4832881662149955}}}, {"candidate": {"n_units_1": 529, "n_units_2": 660, "batch_size": 30, "dropout_1": 0.5490868842845377, "dropout_2": 0.7920401266415242, "learning_rate": 2.101654064946448e-06, "wd": 9.568417280766271e-08}, "metrics": {"cost_metric": {"1": 33.76100707054138}, "active_metric": {"1": 0.47597597597597596}}}, {"candidate": {"n_units_1": 828, "n_units_2": 634, "batch_size": 123, "dropout_1": 0.4257740273137951, "dropout_2": 0.23113551626017848, "learning_rate": 6.860930897860399e-06, "wd": 8.101814548564974e-08}, "metrics": {"cost_metric": {"1": 27.756996154785156}, "active_metric": {"1": 0.33835190203753884}}}, {"candidate": {"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06}, "metrics": {"cost_metric": {"1": 26.10046887397766, "2": 53.824394941329956, "3": 80.07230305671692, "4": 106.25571393966675, "5": 133.94080090522766, "6": 170.4485969543457, "7": 200.93623304367065, "8": 229.56330180168152, "9": 257.11693382263184, "10": 286.6725299358368, "11": 319.0151889324188, "12": 354.14807295799255, "13": 386.9317948818207, "14": 419.8007171154022}, "active_metric": {"1": 0.14744363929146542, "2": 0.13586956521739135, "3": 0.12983091787439616, "4": 0.12661030595813205, "5": 0.1279186795491143, "6": 0.12328904991948475, "7": 0.11674718196457323, "8": 0.1126207729468599, "9": 0.11292270531400961, "10": 0.11513687600644118, "11": 0.11584138486312401, "12": 0.11101046698872785, "13": 0.1090982286634461, "14": 0.10738727858293073}}}], "failed_candidates": [], "pending_candidates": [{"batch_size": 20, "dropout_1": 0.41104972550281815, "dropout_2": 0.7568250476630992, "learning_rate": 0.8320700517813453, "n_units_1": 931, "n_units_2": 527, "wd": 1.9853072351534312e-08, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07, "RESOURCE_ATTR_epoch": 9}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 13}, {"n_units_1": 635, "n_units_2": 538, "batch_size": 69, "dropout_1": 0.45999796301051993, "dropout_2": 0.1659629767457713, "learning_rate": 0.00045244429498637726, "wd": 7.074070723442468e-06, "RESOURCE_ATTR_epoch": 2}]}'
)


@pytest.mark.parametrize("_model_params, _state", zip(_model_params, _state))
def test_compare_gp_model_gped_model(_model_params, _state):
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "wd": loguniform(1e-8, 1),
        "epochs": 81,
    }

    model_params = json.loads(_model_params)
    gp_objs = build_gp_model_factory(config_space, model_params)
    config_space_ext = gp_objs["config_space_ext"]
    gp_model_factory = gp_objs["model_factory"]
    gped_model_factory = build_gped_model_factory(config_space, model_params)[
        "model_factory"
    ]

    state = decode_state_from_old_encoding(
        enc_state=json.loads(_state), hp_ranges=config_space_ext.hp_ranges_ext
    )
    if state.pending_evaluations:
        # Remove pending evaluations
        state = TuningJobState(
            hp_ranges=state.hp_ranges,
            config_for_trial=state.config_for_trial,
            trials_evaluations=state.trials_evaluations,
            failed_trials=state.failed_trials,
        )

    # We compare the two by computing the learning criterion for either
    gp_model = gp_model_factory.model(state, fit_params=False)
    gp_critval = gp_model.posterior_states[0].neg_log_likelihood().item()
    gped_model = gped_model_factory.model(state, fit_params=False)
    gped_critval = gped_model.posterior_states[0].neg_log_likelihood()
    np.testing.assert_allclose(gp_critval, gped_critval, rtol=1e-5)

File Path: tst/schedulers/bayesopt/test_gaussproc.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List
import numpy as np
import pytest

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    dictionarize_objective,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.tuning_job_state import (
    TuningJobState,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges import (
    HyperparameterRanges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.constants import (
    DEFAULT_MCMC_CONFIG,
    DEFAULT_OPTIMIZATION_CONFIG,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    GaussProcEmpiricalBayesModelFactory,
    GaussProcSurrogateModel,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_mcmc_model import (
    GaussProcMCMCModelFactory,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.models.meanstd_acqfunc_impl import (
    EIAcquisitionFunction,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    default_gpmodel,
    default_gpmodel_mcmc,
)
from syne_tune.config_space import uniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    create_tuning_job_state,
    tuples_to_configs,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)


def _simple_hp_ranges() -> HyperparameterRanges:
    return make_hyperparameter_ranges({"x": uniform(0.0, 1.0), "y": uniform(0.0, 1.0)})


@pytest.fixture(scope="function")
def tuning_job_state() -> TuningJobState:
    hp_ranges = _simple_hp_ranges()
    X = [(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)]
    Y = [dictionarize_objective(np.sum(x) * 10.0) for x in X]
    return create_tuning_job_state(hp_ranges=hp_ranges, cand_tuples=X, metrics=Y)


def _set_seeds(seed=0):
    np.random.seed(seed)


def _make_model_gp_optimize(
    state: TuningJobState,
    random_seed,
    opt_config=DEFAULT_OPTIMIZATION_CONFIG,
    num_fantasy_samples=20,
    normalize_targets=True,
    fit_params=True,
):
    gpmodel = default_gpmodel(state, random_seed, optimization_config=opt_config)
    model_factory = GaussProcEmpiricalBayesModelFactory(
        active_metric=INTERNAL_METRIC_NAME,
        gpmodel=gpmodel,
        num_fantasy_samples=num_fantasy_samples,
        normalize_targets=normalize_targets,
    )
    model = model_factory.model(state, fit_params=fit_params)
    return model, gpmodel


def _make_model_mcmc(
    state: TuningJobState, random_seed, mcmc_config=DEFAULT_MCMC_CONFIG, fit_params=True
):
    gpmodel = default_gpmodel_mcmc(state, random_seed, mcmc_config=mcmc_config)
    model_factory = GaussProcMCMCModelFactory(
        active_metric=INTERNAL_METRIC_NAME, gpmodel=gpmodel
    )
    model = model_factory.model(state, fit_params=fit_params)
    return model, gpmodel


def test_gp_fit(tuning_job_state):
    _set_seeds(0)
    hp_ranges = tuning_job_state.hp_ranges
    X = [(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)]
    Y = [np.sum(x) * 10.0 for x in X]
    X = tuples_to_configs(X, hp_ranges)

    # checks if fitting is running
    random_seed = 0
    model, _ = _make_model_gp_optimize(tuning_job_state, random_seed)

    X_enc = [hp_ranges.to_ndarray(x) for x in X]
    pred_train = model.predict(np.array(X_enc))[0]

    assert np.all(
        np.abs(pred_train["mean"] - Y) < 1e-1
    ), "in a noiseless setting, mean of GP should coincide closely with outputs at training points"

    X_test = tuples_to_configs(
        [(0.2, 0.2), (0.4, 0.2), (0.1, 0.9), (0.5, 0.5)], hp_ranges
    )
    X_test_enc = [hp_ranges.to_ndarray(x) for x in X_test]

    pred_test = model.predict(np.array(X_test_enc))[0]
    assert np.min(pred_train["std"]) < np.min(
        pred_test["std"]
    ), "Standard deviation on un-observed points should be greater than at observed ones"


def test_gp_mcmc_fit(tuning_job_state):
    hp_ranges = make_hyperparameter_ranges({"x": uniform(-4.0, 4.0)})

    def tuning_job_state_mcmc(X, Y) -> TuningJobState:
        Y = [dictionarize_objective(y) for y in Y]
        return create_tuning_job_state(hp_ranges=hp_ranges, cand_tuples=X, metrics=Y)

    _set_seeds(0)

    def f(x):
        return 0.1 * np.power(x, 3)

    X = np.concatenate(
        (np.random.uniform(-4.0, -1.0, 10), np.random.uniform(1.0, 4.0, 10))
    )
    Y = f(X)
    X_test = np.sort(np.random.uniform(-1.0, 1.0, 10))

    X = tuples_to_configs([(x,) for x in X], hp_ranges)
    X_test = tuples_to_configs([(x,) for x in X_test], hp_ranges)

    tuning_job_state = tuning_job_state_mcmc(X, Y)
    # checks if fitting is running
    random_seed = 0
    model, _ = _make_model_mcmc(tuning_job_state, random_seed)

    X = [hp_ranges.to_ndarray(x) for x in X]
    predictions = model.predict(np.array(X))

    Y_std_list = [p["std"] for p in predictions]
    Y_mean_list = [p["mean"] for p in predictions]
    Y_mean = np.mean(Y_mean_list, axis=0)
    Y_std = np.mean(Y_std_list, axis=0)

    assert np.all(
        np.abs(Y_mean - Y) < 1e-1
    ), "in a noiseless setting, mean of GP should coincide closely with outputs at training points"

    X_test = [hp_ranges.to_ndarray(x) for x in X_test]

    predictions_test = model.predict(np.array(X_test))
    Y_std_test_list = [p["std"] for p in predictions_test]
    Y_std_test = np.mean(Y_std_test_list, axis=0)
    assert np.max(Y_std) < np.min(
        Y_std_test
    ), "Standard deviation on un-observed points should be greater than at observed ones"


def _compute_acq_with_gradient_many(acq_func, X_test):
    fvals, grads = zip(
        *[acq_func.compute_acq_with_gradient(x_test) for x_test in X_test]
    )
    return np.array(fvals), np.stack(grads, axis=0)


def test_gp_fantasizing():
    """
    Compare whether acquisition function evaluations (values, gradients) with
    fantasizing are the same as averaging them by hand.
    """
    random_seed = 4567
    _set_seeds(random_seed)
    num_fantasy_samples = 10
    num_pending = 5

    hp_ranges = _simple_hp_ranges()
    X = tuples_to_configs([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)], hp_ranges)
    num_data = len(X)
    Y = [dictionarize_objective(np.random.randn(1, 1)) for _ in range(num_data)]
    # Draw fantasies. This is done for a number of fixed pending candidates
    # The model parameters are fit in the first iteration, when there are
    # no pending candidates

    # Note: It is important to not normalize targets, because this would be
    # done on the observed targets only, not the fantasized ones, so it
    # would be hard to compare below.
    pending_tuples = [
        tuple(
            np.random.rand(
                2,
            )
        )
        for _ in range(num_pending)
    ]
    state = create_tuning_job_state(
        hp_ranges=hp_ranges, cand_tuples=X, metrics=Y, pending_tuples=pending_tuples
    )
    model, gpmodel = _make_model_gp_optimize(
        state,
        random_seed,
        num_fantasy_samples=num_fantasy_samples,
        normalize_targets=False,
    )
    fantasy_samples = model.fantasy_samples
    # Evaluate acquisition function and gradients with fantasizing
    num_test = 50
    X_test_enc = [
        hp_ranges.to_ndarray(
            hp_ranges.tuple_to_config(
                tuple(
                    np.random.rand(
                        2,
                    )
                )
            )
        )
        for _ in range(num_test)
    ]
    acq_func = EIAcquisitionFunction(model)
    fvals, grads = _compute_acq_with_gradient_many(acq_func, X_test_enc)
    # Do the same computation by averaging by hand
    fvals_cmp = np.empty((num_fantasy_samples,) + fvals.shape)
    grads_cmp = np.empty((num_fantasy_samples,) + grads.shape)
    X_full = X + state.pending_configurations()
    for it in range(num_fantasy_samples):
        Y_full = Y + [
            dictionarize_objective(eval.fantasies[INTERNAL_METRIC_NAME][:, it])
            for eval in fantasy_samples
        ]
        state2 = create_tuning_job_state(
            hp_ranges=hp_ranges, cand_tuples=X_full, metrics=Y_full
        )
        # We have to skip parameter optimization here
        model_factory = GaussProcEmpiricalBayesModelFactory(
            active_metric=INTERNAL_METRIC_NAME,
            gpmodel=gpmodel,
            num_fantasy_samples=num_fantasy_samples,
            normalize_targets=False,
        )
        model2 = model_factory.model(state2, fit_params=False)
        acq_func2 = EIAcquisitionFunction(model2)
        fvals_, grads_ = _compute_acq_with_gradient_many(acq_func2, X_test_enc)
        fvals_cmp[it, :] = fvals_
        grads_cmp[it, :] = grads_
    # Comparison
    fvals2 = np.mean(fvals_cmp, axis=0)
    grads2 = np.mean(grads_cmp, axis=0)
    assert np.allclose(fvals, fvals2)
    assert np.allclose(grads, grads2)


def default_models() -> List[GaussProcSurrogateModel]:
    hp_ranges = _simple_hp_ranges()
    X = [
        (0.0, 0.0),
        (1.0, 0.0),
        (0.0, 1.0),
        (1.0, 1.0),
        (0.0, 0.0),  # same evals are added multiple times to force GP to unlearn prior
        (1.0, 0.0),
        (0.0, 1.0),
        (1.0, 1.0),
        (0.0, 0.0),
        (1.0, 0.0),
        (0.0, 1.0),
        (1.0, 1.0),
    ]
    Y = [dictionarize_objective(np.sum(x) * 10.0) for x in X]

    state = create_tuning_job_state(hp_ranges=hp_ranges, cand_tuples=X, metrics=Y)
    random_seed = 0

    return [
        _make_model_gp_optimize(state, random_seed)[0],
        _make_model_mcmc(state, random_seed)[0],
    ]


def test_current_best():
    for model in default_models():
        current_best = model.current_best()[0].item()
        print(current_best)
        assert -0.1 < current_best < 0.1

File Path: tst/schedulers/bayesopt/test_get_set_params.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np

from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    gp_multifidelity_searcher_defaults,
)
from syne_tune.optimizer.schedulers.searchers.gp_multifidelity_searcher import (
    GPMultiFidelitySearcher,
)

from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.comparison_gpy import (
    Ackley,
    sample_data,
)


def test_params_gp_multifidelity():
    # Create GP multifidelity searcher, including a GP surrogate model
    _, searcher_options, _ = gp_multifidelity_searcher_defaults()
    searcher_options["gp_resource_kernel"] = "exp-decay-combined"
    # Note: We are lazy here, we just need the config_space
    data = sample_data(Ackley, num_train=5, num_grid=5)
    searcher_options["config_space"] = data["state"].hp_ranges.config_space
    searcher_options["scheduler"] = "hyperband_stopping"
    searcher_options["min_epochs"] = 1
    searcher_options["max_epochs"] = 27
    searcher_options["metric"] = "accuracy"
    searcher_options["resource_attr"] = "epoch"
    searcher_options["debug_log"] = False
    searcher = GPMultiFidelitySearcher(**searcher_options)
    # Set parameters
    params = {
        "noise_variance": 0.01,
        "kernel_alpha": 9.0,
        "kernel_mean_lam": 0.25,
        "kernel_gamma": 0.75,
        "kernel_delta": 0.125,
        "kernel_kernelx_inv_bw0": 0.11,
        "kernel_kernelx_inv_bw1": 11.0,
        "kernel_kernelx_covariance_scale": 5.5,
        "kernel_meanx_mean_value": 1e-5,
    }
    searcher.set_params(params)
    # Get parameters: Must be the same
    params2 = searcher.model_parameters()
    assert len(params) == len(params2), (params, params2)
    for k, v in params.items():
        assert k in params2, (k, params, params2)
        v2 = params2[k]
        np.testing.assert_almost_equal([v], [v2], decimal=6, err_msg="key={}".format(k))


if __name__ == "__main__":
    test_params_gp_multifidelity()

File Path: tst/schedulers/bayesopt/test_gp_components.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np

from syne_tune.optimizer.schedulers.searchers.bayesopt.models.gp_model import (
    get_internal_candidate_evaluations,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.common import (
    dictionarize_objective,
    INTERNAL_METRIC_NAME,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.utils.test_objects import (
    dimensionality_and_warping_ranges,
    create_tuning_job_state,
)
from syne_tune.config_space import uniform, randint, choice, loguniform
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)


def test_get_internal_candidate_evaluations():
    """we do not test the case with no evaluations, as it is assumed
    that there will be always some evaluations generated in the beginning
    of the BO loop."""

    hp_ranges = make_hyperparameter_ranges(
        {"a": randint(0, 10), "b": uniform(0.0, 10.0), "c": choice(["X", "Y"])}
    )
    cand_tuples = [(2, 3.3, "X"), (1, 9.9, "Y"), (7, 6.1, "X")]
    metrics = [dictionarize_objective(y) for y in (5.3, 10.9, 13.1)]

    state = create_tuning_job_state(
        hp_ranges=hp_ranges, cand_tuples=cand_tuples, metrics=metrics
    )
    state.failed_trials.append("0")  # First trial with observation also failed

    result = get_internal_candidate_evaluations(
        state, INTERNAL_METRIC_NAME, normalize_targets=True, num_fantasy_samples=20
    )

    assert len(result.features.shape) == 2, "Input should be a matrix"
    assert len(result.targets.shape) == 2, "Output should be a matrix"

    assert result.features.shape[0] == len(cand_tuples)
    assert result.targets.shape[-1] == 1, "Only single output value per row is suppored"

    assert (
        np.abs(np.mean(result.targets)) < 1e-8
    ), "Mean of the normalized outputs is not 0.0"
    assert (
        np.abs(np.std(result.targets) - 1.0) < 1e-8
    ), "Std. of the normalized outputs is not 1.0"

    np.testing.assert_almost_equal(result.mean, 9.766666666666666)
    np.testing.assert_almost_equal(result.std, 3.283629428273267)


def test_dimensionality_and_warping_ranges():
    # Note: `choice` with binary value range is encoded as 1, not 2 dims
    hp_ranges = make_hyperparameter_ranges(
        {
            "a": choice(["X", "Y"]),  # pos 0
            "b": loguniform(0.1, 10.0),  # pos 1
            "c": choice(["a", "b", "c"]),  # pos 2
            "d": uniform(0.0, 10.0),  # pos 5
            "e": choice(["X", "Y"]),
        }
    )  # pos 6

    dim, warping_ranges = dimensionality_and_warping_ranges(hp_ranges)
    assert dim == 7
    assert warping_ranges == {1: (0.0, 1.0), 5: (0.0, 1.0)}

File Path: tst/schedulers/bayesopt/test_hp_ranges.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

from collections import Counter
import numpy as np
import pytest
from numpy.testing import assert_allclose, assert_almost_equal
from pytest import approx

from syne_tune.config_space import (
    uniform,
    randint,
    choice,
    loguniform,
    lograndint,
    finrange,
    logfinrange,
    reverseloguniform,
    ordinal,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_factory import (
    make_hyperparameter_ranges,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.hp_ranges_impl import (
    HyperparameterRangesImpl,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.config_ext import (
    ExtendedConfiguration,
)


def _assert_allclose_config(c1, c2, hp_ranges):
    c1_tpl = hp_ranges.config_to_tuple(c1)
    c2_tpl = hp_ranges.config_to_tuple(c2)
    assert_allclose(c1_tpl, c2_tpl)


@pytest.mark.parametrize(
    "lower,upper,external_hp,internal_ndarray,domain,size",
    [
        (0.0, 8.0, 0.0, 0.0, uniform, None),
        (0.0, 8.0, 8.0, 1.0, uniform, None),
        (0.0, 8.0, 2.0, 0.25, uniform, None),
        (100.2, 100.6, 100.4, 0.5, uniform, None),
        (-2.0, 8.0, 0.0, 0.2, uniform, None),
        (-11.0, -1.0, -10.0, 0.1, uniform, None),
        (1.0, 8.0, 1.0, 0.0, loguniform, None),
        (1.0, 8.0, 8.0, 1.0, loguniform, None),
        (1.0, 10000.0, 10.0, 0.25, loguniform, None),
        (1.0, 10000.0, 100.0, 0.5, loguniform, None),
        (1.0, 10000.0, 1000.0, 0.75, loguniform, None),
        (0.001, 0.1, 0.01, 0.5, loguniform, None),
        (0.1, 100, 1.0, 1.0 / 3, loguniform, None),
        (0.5, 0.99, 0.5, 0.0, reverseloguniform, None),
        (0.5, 0.99, 0.99, 1.0, reverseloguniform, None),
        (0.9, 0.99999, 0.99, 0.25, reverseloguniform, None),
        (0.9, 0.99999, 0.999, 0.5, reverseloguniform, None),
        (0.9, 0.99999, 0.9999, 0.75, reverseloguniform, None),
        (0.5, 15.0 / 16.0, 0.75, 1.0 / 3, reverseloguniform, None),
        (0.5, 15.0 / 16.0, 7.0 / 8.0, 2.0 / 3, reverseloguniform, None),
        (1, 10001, 5001, 0.5, randint, None),
        (-10, 10, 0, 0.5, randint, None),
        (0.1, 1.0, 0.1, 0.5, finrange, 1),
        (0.1, 1.0, 0.1, 0.5 / 10, finrange, 10),
        (0.1, 1.0, 1.0, 9.5 / 10, finrange, 10),
        (0.1, 1.0, 0.5, 4.5 / 10, finrange, 10),
        (np.exp(0.1), np.exp(1.0), np.exp(0.1), 0.5 / 10, logfinrange, 10),
        (np.exp(0.1), np.exp(1.0), np.exp(1.0), 9.5 / 10, logfinrange, 10),
        (np.exp(0.1), np.exp(1.0), np.exp(0.5), 4.5 / 10, logfinrange, 10),
    ],
)
def test_continuous_to_and_from_ndarray(
    lower, upper, external_hp, internal_ndarray, domain, size
):
    if size is None:
        hp_range = domain(lower, upper)
    else:
        hp_range = domain(lower, upper, size=size)
    hp_ranges = make_hyperparameter_ranges({"a": hp_range})
    config = hp_ranges.tuple_to_config((external_hp,))
    config_enc = np.array([internal_ndarray])
    assert_allclose(hp_ranges.to_ndarray(config), config_enc)
    _assert_allclose_config(hp_ranges.from_ndarray(config_enc), config, hp_ranges)


@pytest.mark.parametrize(
    "choices,external_hp,internal_ndarray",
    [
        (["a", "b"], "a", [0.25]),
        (["a", "b"], "b", [0.75]),
        (["a", "b", "c"], "b", [0.0, 1.0, 0.0]),
        (["a", "b", "c", "d"], "c", [0.0, 0.0, 1.0, 0.0]),
    ],
)
def test_categorical_to_and_from_ndarray(choices, external_hp, internal_ndarray):
    hp_ranges = make_hyperparameter_ranges({"a": choice(choices)})
    config = hp_ranges.tuple_to_config((external_hp,))
    config_enc = np.array(internal_ndarray)
    assert_allclose(hp_ranges.to_ndarray(config), config_enc)
    assert hp_ranges.from_ndarray(config_enc) == config


@pytest.mark.parametrize(
    "choices,external_hp,internal_ndarray",
    [
        (["a", "b"], "a", [0.25]),
        (["a", "b"], "b", [0.75]),
        (["a", "b", "c"], "b", [0.5]),
        (["a", "b", "c"], "c", [2.5 / 3]),
        (["a", "b", "c", "d"], "c", [2.5 / 4]),
    ],
)
def test_ordinal_to_and_from_ndarray(choices, external_hp, internal_ndarray):
    hp_ranges = make_hyperparameter_ranges({"a": ordinal(choices)})
    config = hp_ranges.tuple_to_config((external_hp,))
    config_enc = np.array(internal_ndarray)
    assert_allclose(hp_ranges.to_ndarray(config), config_enc)
    assert hp_ranges.from_ndarray(config_enc) == config


# Going to internal representation and back should give back the original value
@pytest.mark.parametrize(
    "lower,upper,domain",
    [
        (0.0, 8.0, uniform),
        (0.01, 0.1, uniform),
        (-10.0, -5.1, uniform),
        (-1000000000000000.0, 100000000000000000.0, uniform),
        (10.0, 10000000000.0, loguniform),
        (-1000.0, 100.0, uniform),
        (1.0, 1000.0, loguniform),
        (10.0, 15.0, loguniform),
        (0.1, 20.0, loguniform),
        (0.0, 0.99, reverseloguniform),
        (0.999999999, 0.99999999999, reverseloguniform),
    ],
)
def test_continuous_to_ndarray_and_back(lower, upper, domain):
    # checks the lower bound upper bound and 10 random values
    _test_to_ndarray_and_back(lower, upper, lower, domain)
    _test_to_ndarray_and_back(lower, upper, upper, domain)
    rnd = np.random.RandomState(0)
    for random_hp in rnd.uniform(lower, upper, size=10):
        _test_to_ndarray_and_back(lower, upper, random_hp, domain)


# helper for the previous test
def _test_to_ndarray_and_back(lower, upper, external_hp, domain):
    hp_ranges = make_hyperparameter_ranges({"a": domain(lower, upper)})
    config = hp_ranges.tuple_to_config((external_hp,))
    assert hp_ranges.from_ndarray(hp_ranges.to_ndarray(config))["a"] == approx(
        external_hp
    )


@pytest.mark.parametrize(
    "lower,upper,domain",
    [
        (0, 8, randint),
        (1, 20, randint),
        (-10, -5, randint),
        (-1000000000000000, 100000000000000000, randint),
        (10, 10000000000, lograndint),
        (-1000, 100, randint),
        (1, 1000, lograndint),
        (10, 15, lograndint),
    ],
)
def test_integer_to_ndarray_and_back(lower, upper, domain):
    # checks the lower bound upper bound and 15 random values
    _test_to_ndarray_and_back(lower, upper, lower, domain)
    _test_to_ndarray_and_back(lower, upper, upper, domain)
    rnd = np.random.RandomState(0)
    for random_hp in rnd.randint(lower + 1, upper, size=15):
        _test_to_ndarray_and_back(lower, upper, int(random_hp), domain)


# this is more of a functional test testing of HP conversion and scaling
# it generates random candidates and checks the distribution is correct
# and also that they can be transformed to internal representation and back while still obtaining
# the same value
def test_distribution_of_random_candidates():
    random_state = np.random.RandomState(0)
    hp_ranges = make_hyperparameter_ranges(
        {
            "0": uniform(1.0, 1000.0),
            "1": loguniform(1.0, 1000.0),
            "2": reverseloguniform(0.9, 0.9999),
            "3": randint(1, 1000),
            "4": lograndint(1, 1000),
            "5": choice(["a", "b", "c"]),
            "6": ordinal(["a", "b", "c"]),
        }
    )
    num_random_candidates = 600
    random_candidates = hp_ranges.random_configs(random_state, num_random_candidates)

    # check converting back gets to the same candidate
    for cand in random_candidates[2:]:
        cand_tpl = hp_ranges.config_to_tuple(cand)
        ndarray_candidate = hp_ranges.to_ndarray(cand)
        converted_back = hp_ranges.from_ndarray(ndarray_candidate)
        back_tpl = hp_ranges.config_to_tuple(converted_back)
        for hp, hp_converted_back in zip(cand_tpl, back_tpl):
            if isinstance(hp, str):
                assert hp == hp_converted_back
            else:
                assert_almost_equal(hp, hp_converted_back)

    hps0, hps1, hps2, hps3, hps4, hps5, hps6 = zip(
        *[hp_ranges.config_to_tuple(x) for x in random_candidates]
    )
    assert 200 < np.percentile(hps0, 25) < 300
    assert 450 < np.percentile(hps0, 50) < 550
    assert 700 < np.percentile(hps0, 75) < 800

    # same bounds as the previous but log scaling
    assert 3 < np.percentile(hps1, 25) < 10
    assert 20 < np.percentile(hps1, 50) < 50
    assert 100 < np.percentile(hps1, 75) < 250

    # reverse log
    assert 0.9 < np.percentile(hps2, 25) < 0.99
    assert 0.99 < np.percentile(hps2, 50) < 0.999
    assert 0.999 < np.percentile(hps2, 75) < 0.9999

    # integer
    assert 200 < np.percentile(hps3, 25) < 300
    assert 450 < np.percentile(hps3, 50) < 550
    assert 700 < np.percentile(hps3, 75) < 800

    # same bounds as the previous but log scaling
    assert 3 < np.percentile(hps4, 25) < 10
    assert 20 < np.percentile(hps4, 50) < 40
    assert 100 < np.percentile(hps4, 75) < 250

    for hps in [hps5, hps6]:
        counter = Counter(hps)
        assert len(counter) == 3
        assert 150 < counter["a"] < 250  # should be about 200
        assert 150 < counter["b"] < 250  # should be about 200
        assert 150 < counter["c"] < 250  # should be about 200


def _int_encode(val, lower, upper):
    denom = upper - lower + 1
    return (val - lower + 0.5) / denom


def test_get_ndarray_bounds():
    config_space = {
        "0": uniform(1.0, 1000.0),
        "1": loguniform(1.0, 1000.0),
        "2": reverseloguniform(0.9, 0.9999),
        "3": randint(1, 1000),
        "4": lograndint(1, 1000),
        "5": choice(["a", "b", "c"]),
    }
    hp_ranges = make_hyperparameter_ranges(config_space)
    for epochs, val_last_pos in ((3, 1), (9, 3), (81, 81), (27, 1), (27, 9)):
        config_space_ext = ExtendedConfiguration(
            hp_ranges=hp_ranges,
            resource_attr_key="epoch",
            resource_attr_range=(1, epochs),
        )
        hp_ranges_ext = config_space_ext.hp_ranges_ext
        hp_ranges_ext.value_for_last_pos = val_last_pos
        bounds = hp_ranges_ext.get_ndarray_bounds()
        val_enc = _int_encode(val_last_pos, lower=1, upper=epochs)
        assert all(x == (0.0, 1.0) for x in bounds[:-1])
        val_enc_cmp = bounds[-1][0]
        assert val_enc_cmp == bounds[-1][1]
        np.testing.assert_almost_equal(val_enc, val_enc_cmp, decimal=5)


def test_active_ranges_valid():
    config_space = {
        "0": uniform(1.0, 1000.0),
        "1": loguniform(1.0, 1000.0),
        "2": reverseloguniform(0.9, 0.9999),
        "3": randint(1, 1000),
        "4": lograndint(1, 1000),
        "5": choice(["a", "b", "c"]),
    }
    invalid_active_spaces = [
        {
            "6": randint(0, 1),
        },
        {
            "0": uniform(2.0, 500.0),
            "5": choice(["a", "b", "d"]),
        },
        {
            "0": uniform(2.0, 1000.0),
            "1": uniform(2.0, 500.0),
        },
        {
            "3": randint(1, 100),
            "4": lograndint(2, 1005),
        },
        {
            "2": reverseloguniform(0.99, 0.99999),
            "3": randint(5, 500),
        },
        {
            "2": reverseloguniform(0.9, 0.999),
            "4": lograndint(10, 1005),
        },
    ]
    for active_config_space in invalid_active_spaces:
        with pytest.raises(AssertionError):
            hp_ranges = HyperparameterRangesImpl(
                config_space=config_space, active_config_space=active_config_space
            )


@pytest.mark.parametrize(
    "config_space,active_config_space",
    [
        (
            {
                "0": uniform(1.0, 2.0),
                "1": choice(["a", "b", "c"]),
            },
            {
                "0": uniform(1.1, 1.9),
                "1": choice(["a", "c"]),
            },
        ),
        (
            {
                "0": randint(1, 3),
                "1": choice(["a", "c", "b"]),
            },
            {
                "0": randint(2, 3),
                "1": choice(["b", "c"]),
            },
        ),
        (
            {
                "0": lograndint(3, 5),
                "1": randint(2, 3),
            },
            {
                "0": lograndint(3, 4),
            },
        ),
    ],
)
def test_active_ranges_samples(config_space, active_config_space):
    seed = 31415927
    random_state = np.random.RandomState(seed)
    hp_ranges = HyperparameterRangesImpl(
        config_space=config_space, active_config_space=active_config_space
    )
    configs = hp_ranges.random_configs(random_state, num_configs=100)
    _active_config_space = dict(config_space, **active_config_space)
    hp_ranges2 = HyperparameterRangesImpl(config_space=_active_config_space)
    # This fails with high probability if the sampled configs fall outside of
    # the narrower active ranges
    features = hp_ranges2.to_ndarray_matrix(configs)


def _cast_config(config, config_space):
    return {name: domain.cast(config[name]) for name, domain in config_space.items()}


@pytest.mark.parametrize(
    "config1,config2,match",
    [
        ({"1": 1}, {"1": 1}, True),
        ({"0": 0.546003}, {}, False),
        ({"1": 3}, {}, False),
        ({"2": "b"}, {}, False),
        ({"3": 0.3}, {}, False),
        ({"4": 1}, {}, False),
        ({"5": 0.0001}, {}, False),
        ({"5": 0.0010005}, {}, True),
        ({"0": 0.546000000000001}, {}, True),
        ({"5": 0.01}, {"5": 0.01000001}, True),
    ],
)
def test_config_to_match_string(config1, config2, match):
    config_space = {
        "0": uniform(0.0, 1.0),
        "1": randint(1, 10),
        "2": choice(["a", "b", "c"]),
        "3": finrange(0.1, 1.0, 10),
        "4": choice([3, 2, 1]),
        "5": choice([0.01, 0.001, 0.0001, 0.00001]),
    }
    hp_ranges = make_hyperparameter_ranges(config_space)

    config_base = {"0": 0.546, "1": 4, "2": "a", "3": 0.4, "4": 3, "5": 0.001}
    _config1 = _cast_config(dict(config_base, **config1), config_space)
    _config2 = _cast_config(dict(config_base, **config2), config_space)
    match_str1 = hp_ranges.config_to_match_string(_config1)
    match_str2 = hp_ranges.config_to_match_string(_config2)
    assert (
        match_str1 == match_str2
    ) == match, f"match = {match}\nmatch_str1 = {match_str1}\nmatch_str2 = {match_str2}"


def test_config_space_for_sampling():
    config_space = {"0": uniform(0.0, 2.0)}
    active_space = {"0": uniform(0.5, 1.5)}
    hp_ranges = make_hyperparameter_ranges(
        config_space=config_space, active_config_space=active_space
    )
    assert (
        hp_ranges.config_space_for_sampling == active_space
    ), "Active space should be used for sampling when specified."

    hp_ranges = make_hyperparameter_ranges(config_space=config_space)
    assert (
        hp_ranges.config_space_for_sampling == config_space
    ), "Incorrect config space is used for sampling."


def test_encoded_ranges():
    config_space = {
        "0": uniform(0.0, 1.0),
        "1": randint(1, 10),
        "2": choice(["a", "b", "c"]),
        "3": finrange(0.1, 1.0, 10),
        "4": choice([3, 2, 1]),
        "5": choice([0.01, 0.001, 0.0001, 0.00001]),
        "6": choice(["a", "b"]),
    }
    hp_ranges = make_hyperparameter_ranges(config_space)
    encoded_ranges = hp_ranges.encoded_ranges
    assert encoded_ranges["0"] == (0, 1)
    assert encoded_ranges["1"] == (1, 2)
    assert encoded_ranges["2"] == (2, 5)
    assert encoded_ranges["3"] == (5, 6)
    assert encoded_ranges["4"] == (6, 9)
    assert encoded_ranges["5"] == (9, 13)
    assert encoded_ranges["6"] == (13, 14)

File Path: tst/schedulers/bayesopt/test_iss_model.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import json
import numpy as np
import pytest

from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    gp_multifidelity_searcher_defaults,
    gp_multifidelity_searcher_factory,
)
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.gp_searcher_utils import (
    decode_state_from_old_encoding,
)
from syne_tune.config_space import randint, uniform, loguniform


def _common_kwargs(config_space: Dict) -> Dict:
    return {
        "config_space": config_space,
        "max_epochs": config_space["epochs"],
        "metric": "accuracy",
        "resource_attr": "epoch",
        "scheduler": "hyperband_stopping",
        "scheduler_mode": "max",
        "debug_log": False,
        "normalize_targets": True,
    }


def build_gpiss_model_factory(config_space: Dict, model_params: Dict, **kwargs):
    kwargs = dict(
        _common_kwargs(config_space), model="gp_issm", issm_gamma_one=False, **kwargs
    )
    _kwargs = check_and_merge_defaults(
        kwargs, *gp_multifidelity_searcher_defaults(), dict_name="search_options"
    )
    kwargs_int = gp_multifidelity_searcher_factory(**_kwargs)
    # Need to convert `model_params`
    kwargs_int["model_factory"].set_params(model_params)
    return kwargs_int


# We ran launch_sample_searcher_states.py to sample the searcher states
# used here, which runs MOBSTER (hyperband_stopping, bayesopt) with the
# mlp_fashionmnist_benchmark

_model_params = []
_state = []


_model_params.append(
    '{"noise_variance": 0.008381548138906916, "kernel_inv_bw0": 0.004177002691678498, "kernel_inv_bw1": 0.000402494802013946, "kernel_inv_bw2": 0.00036005844016162423, "kernel_inv_bw3": 4.278552430496177, "kernel_inv_bw4": 0.38190450370225937, "kernel_inv_bw5": 0.0001674608736118065, "kernel_inv_bw6": 0.5371572608999335, "kernel_covariance_scale": 1.0487725555603677, "mean_mean_value": -0.37162308332346305, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.18364130320022903, "issm_beta": 1.1069304811899965}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}, {"candidate": {"n_units_1": 672, "n_units_2": 820, "batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "wd": 0.002536625472111785}, "metrics": {"cost_metric": {"1": 32.458306074142456}, "active_metric": {"1": 0.24486714975845414}}}, {"candidate": {"n_units_1": 688, "n_units_2": 597, "batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "wd": 4.489784182359429e-08}, "metrics": {"cost_metric": {"1": 30.61675500869751}, "active_metric": {"1": 0.8976211984342066}}}, {"candidate": {"n_units_1": 501, "n_units_2": 601, "batch_size": 34, "dropout_1": 0.7410256603874262, "dropout_2": 0.046625361151571336, "learning_rate": 0.07937041160202492, "wd": 8.340962845965557e-07}, "metrics": {"cost_metric": {"1": 38.63721990585327}, "active_metric": {"1": 0.904561824729892}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08}, "metrics": {"cost_metric": {"1": 44.914865016937256, "2": 95.70968389511108, "3": 134.2296760082245, "4": 167.57774996757507, "5": 205.92636585235596, "6": 242.58663892745972, "7": 283.2623338699341, "8": 326.5033459663391, "9": 360.76255893707275, "10": 398.4191679954529, "11": 434.4982190132141}, "active_metric": {"1": 0.15477145148356053, "2": 0.13502405773857262, "3": 0.1245990376904571, "4": 0.12840817963111473, "5": 0.12219326383319973, "6": 0.11778267842822776, "7": 0.1133720930232558, "8": 0.11166800320769843, "9": 0.10976343223736973, "10": 0.1067562149157979, "11": 0.10555332798716921}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08}, "metrics": {"cost_metric": {"1": 50.98606991767883, "2": 92.16159892082214, "3": 123.69115900993347, "4": 154.21311402320862, "5": 192.06259202957153, "6": 227.72296023368835, "7": 263.6407790184021, "8": 304.7051441669464, "9": 336.6670799255371, "10": 372.1759181022644, "11": 405.45882201194763}, "active_metric": {"1": 0.1700441412520064, "2": 0.1407504012841091, "3": 0.1359349919743178, "4": 0.131922150882825, "5": 0.1220906902086677, "6": 0.1213884430176565, "7": 0.1182784911717496, "8": 0.120284911717496, "9": 0.1221910112359551, "10": 0.1134630818619583, "11": 0.1151685393258427}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 10}, {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08, "RESOURCE_ATTR_epoch": 12}, {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08, "RESOURCE_ATTR_epoch": 12}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 36}]}'
)
# elapsed_time = 595.700856924057
# num_observations = 73
# num_configs = 11

_model_params.append(
    '{"noise_variance": 0.008381548138906916, "kernel_inv_bw0": 0.004177002691678498, "kernel_inv_bw1": 0.000402494802013946, "kernel_inv_bw2": 0.00036005844016162423, "kernel_inv_bw3": 4.278552430496177, "kernel_inv_bw4": 0.38190450370225937, "kernel_inv_bw5": 0.0001674608736118065, "kernel_inv_bw6": 0.5371572608999335, "kernel_covariance_scale": 1.0487725555603677, "mean_mean_value": -0.37162308332346305, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.18364130320022903, "issm_beta": 1.1069304811899965}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}, {"candidate": {"n_units_1": 672, "n_units_2": 820, "batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "wd": 0.002536625472111785}, "metrics": {"cost_metric": {"1": 32.458306074142456}, "active_metric": {"1": 0.24486714975845414}}}, {"candidate": {"n_units_1": 688, "n_units_2": 597, "batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "wd": 4.489784182359429e-08}, "metrics": {"cost_metric": {"1": 30.61675500869751}, "active_metric": {"1": 0.8976211984342066}}}, {"candidate": {"n_units_1": 501, "n_units_2": 601, "batch_size": 34, "dropout_1": 0.7410256603874262, "dropout_2": 0.046625361151571336, "learning_rate": 0.07937041160202492, "wd": 8.340962845965557e-07}, "metrics": {"cost_metric": {"1": 38.63721990585327}, "active_metric": {"1": 0.904561824729892}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08}, "metrics": {"cost_metric": {"1": 44.914865016937256, "2": 95.70968389511108, "3": 134.2296760082245, "4": 167.57774996757507, "5": 205.92636585235596, "6": 242.58663892745972, "7": 283.2623338699341, "8": 326.5033459663391, "9": 360.76255893707275, "10": 398.4191679954529, "11": 434.4982190132141}, "active_metric": {"1": 0.15477145148356053, "2": 0.13502405773857262, "3": 0.1245990376904571, "4": 0.12840817963111473, "5": 0.12219326383319973, "6": 0.11778267842822776, "7": 0.1133720930232558, "8": 0.11166800320769843, "9": 0.10976343223736973, "10": 0.1067562149157979, "11": 0.10555332798716921}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08}, "metrics": {"cost_metric": {"1": 50.98606991767883, "2": 92.16159892082214, "3": 123.69115900993347, "4": 154.21311402320862, "5": 192.06259202957153, "6": 227.72296023368835, "7": 263.6407790184021, "8": 304.7051441669464, "9": 336.6670799255371, "10": 372.1759181022644, "11": 405.45882201194763}, "active_metric": {"1": 0.1700441412520064, "2": 0.1407504012841091, "3": 0.1359349919743178, "4": 0.131922150882825, "5": 0.1220906902086677, "6": 0.1213884430176565, "7": 0.1182784911717496, "8": 0.120284911717496, "9": 0.1221910112359551, "10": 0.1134630818619583, "11": 0.1151685393258427}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 9}, {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08, "RESOURCE_ATTR_epoch": 10}, {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08, "RESOURCE_ATTR_epoch": 10}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 31}]}'
)
# elapsed_time = 520.2518529891968
# num_observations = 63
# num_configs = 11

_model_params.append(
    '{"noise_variance": 0.008381548138906916, "kernel_inv_bw0": 0.004177002691678498, "kernel_inv_bw1": 0.000402494802013946, "kernel_inv_bw2": 0.00036005844016162423, "kernel_inv_bw3": 4.278552430496177, "kernel_inv_bw4": 0.38190450370225937, "kernel_inv_bw5": 0.0001674608736118065, "kernel_inv_bw6": 0.5371572608999335, "kernel_covariance_scale": 1.0487725555603677, "mean_mean_value": -0.37162308332346305, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.18364130320022903, "issm_beta": 1.1069304811899965}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}, {"candidate": {"n_units_1": 672, "n_units_2": 820, "batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "wd": 0.002536625472111785}, "metrics": {"cost_metric": {"1": 32.458306074142456}, "active_metric": {"1": 0.24486714975845414}}}, {"candidate": {"n_units_1": 688, "n_units_2": 597, "batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "wd": 4.489784182359429e-08}, "metrics": {"cost_metric": {"1": 30.61675500869751}, "active_metric": {"1": 0.8976211984342066}}}, {"candidate": {"n_units_1": 501, "n_units_2": 601, "batch_size": 34, "dropout_1": 0.7410256603874262, "dropout_2": 0.046625361151571336, "learning_rate": 0.07937041160202492, "wd": 8.340962845965557e-07}, "metrics": {"cost_metric": {"1": 38.63721990585327}, "active_metric": {"1": 0.904561824729892}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08}, "metrics": {"cost_metric": {"1": 44.914865016937256, "2": 95.70968389511108, "3": 134.2296760082245, "4": 167.57774996757507, "5": 205.92636585235596, "6": 242.58663892745972, "7": 283.2623338699341, "8": 326.5033459663391, "9": 360.76255893707275, "10": 398.4191679954529, "11": 434.4982190132141}, "active_metric": {"1": 0.15477145148356053, "2": 0.13502405773857262, "3": 0.1245990376904571, "4": 0.12840817963111473, "5": 0.12219326383319973, "6": 0.11778267842822776, "7": 0.1133720930232558, "8": 0.11166800320769843, "9": 0.10976343223736973, "10": 0.1067562149157979, "11": 0.10555332798716921}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08}, "metrics": {"cost_metric": {"1": 50.98606991767883, "2": 92.16159892082214, "3": 123.69115900993347, "4": 154.21311402320862, "5": 192.06259202957153, "6": 227.72296023368835, "7": 263.6407790184021, "8": 304.7051441669464, "9": 336.6670799255371, "10": 372.1759181022644, "11": 405.45882201194763}, "active_metric": {"1": 0.1700441412520064, "2": 0.1407504012841091, "3": 0.1359349919743178, "4": 0.131922150882825, "5": 0.1220906902086677, "6": 0.1213884430176565, "7": 0.1182784911717496, "8": 0.120284911717496, "9": 0.1221910112359551, "10": 0.1134630818619583, "11": 0.1151685393258427}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 8}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 27}, {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08, "RESOURCE_ATTR_epoch": 9}, {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08, "RESOURCE_ATTR_epoch": 9}]}'
)
# elapsed_time = 469.9041178226471
# num_observations = 56
# num_configs = 11

_model_params.append(
    '{"noise_variance": 0.008381548138906916, "kernel_inv_bw0": 0.004177002691678498, "kernel_inv_bw1": 0.000402494802013946, "kernel_inv_bw2": 0.00036005844016162423, "kernel_inv_bw3": 4.278552430496177, "kernel_inv_bw4": 0.38190450370225937, "kernel_inv_bw5": 0.0001674608736118065, "kernel_inv_bw6": 0.5371572608999335, "kernel_covariance_scale": 1.0487725555603677, "mean_mean_value": -0.37162308332346305, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.18364130320022903, "issm_beta": 1.1069304811899965}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}, {"candidate": {"n_units_1": 672, "n_units_2": 820, "batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "wd": 0.002536625472111785}, "metrics": {"cost_metric": {"1": 32.458306074142456}, "active_metric": {"1": 0.24486714975845414}}}, {"candidate": {"n_units_1": 688, "n_units_2": 597, "batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "wd": 4.489784182359429e-08}, "metrics": {"cost_metric": {"1": 30.61675500869751}, "active_metric": {"1": 0.8976211984342066}}}, {"candidate": {"n_units_1": 501, "n_units_2": 601, "batch_size": 34, "dropout_1": 0.7410256603874262, "dropout_2": 0.046625361151571336, "learning_rate": 0.07937041160202492, "wd": 8.340962845965557e-07}, "metrics": {"cost_metric": {"1": 38.63721990585327}, "active_metric": {"1": 0.904561824729892}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08}, "metrics": {"cost_metric": {"1": 44.914865016937256, "2": 95.70968389511108, "3": 134.2296760082245, "4": 167.57774996757507, "5": 205.92636585235596, "6": 242.58663892745972, "7": 283.2623338699341, "8": 326.5033459663391, "9": 360.76255893707275, "10": 398.4191679954529, "11": 434.4982190132141}, "active_metric": {"1": 0.15477145148356053, "2": 0.13502405773857262, "3": 0.1245990376904571, "4": 0.12840817963111473, "5": 0.12219326383319973, "6": 0.11778267842822776, "7": 0.1133720930232558, "8": 0.11166800320769843, "9": 0.10976343223736973, "10": 0.1067562149157979, "11": 0.10555332798716921}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08}, "metrics": {"cost_metric": {"1": 50.98606991767883, "2": 92.16159892082214, "3": 123.69115900993347, "4": 154.21311402320862, "5": 192.06259202957153, "6": 227.72296023368835, "7": 263.6407790184021, "8": 304.7051441669464, "9": 336.6670799255371, "10": 372.1759181022644, "11": 405.45882201194763}, "active_metric": {"1": 0.1700441412520064, "2": 0.1407504012841091, "3": 0.1359349919743178, "4": 0.131922150882825, "5": 0.1220906902086677, "6": 0.1213884430176565, "7": 0.1182784911717496, "8": 0.120284911717496, "9": 0.1221910112359551, "10": 0.1134630818619583, "11": 0.1151685393258427}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 7}, {"n_units_1": 1024, "n_units_2": 707, "batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "wd": 1e-08, "RESOURCE_ATTR_epoch": 5}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 20}, {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08, "RESOURCE_ATTR_epoch": 6}]}'
)
# elapsed_time = 349.2686309814453
# num_observations = 41
# num_configs = 11

_model_params.append(
    '{"noise_variance": 0.008381548138906916, "kernel_inv_bw0": 0.004177002691678498, "kernel_inv_bw1": 0.000402494802013946, "kernel_inv_bw2": 0.00036005844016162423, "kernel_inv_bw3": 4.278552430496177, "kernel_inv_bw4": 0.38190450370225937, "kernel_inv_bw5": 0.0001674608736118065, "kernel_inv_bw6": 0.5371572608999335, "kernel_covariance_scale": 1.0487725555603677, "mean_mean_value": -0.37162308332346305, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.18364130320022903, "issm_beta": 1.1069304811899965}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}, {"candidate": {"n_units_1": 672, "n_units_2": 820, "batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "wd": 0.002536625472111785}, "metrics": {"cost_metric": {"1": 32.458306074142456}, "active_metric": {"1": 0.24486714975845414}}}, {"candidate": {"n_units_1": 688, "n_units_2": 597, "batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "wd": 4.489784182359429e-08}, "metrics": {"cost_metric": {"1": 30.61675500869751}, "active_metric": {"1": 0.8976211984342066}}}, {"candidate": {"n_units_1": 501, "n_units_2": 601, "batch_size": 34, "dropout_1": 0.7410256603874262, "dropout_2": 0.046625361151571336, "learning_rate": 0.07937041160202492, "wd": 8.340962845965557e-07}, "metrics": {"cost_metric": {"1": 38.63721990585327}, "active_metric": {"1": 0.904561824729892}}}, {"candidate": {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08}, "metrics": {"cost_metric": {"1": 44.914865016937256, "2": 95.70968389511108, "3": 134.2296760082245, "4": 167.57774996757507, "5": 205.92636585235596, "6": 242.58663892745972, "7": 283.2623338699341, "8": 326.5033459663391, "9": 360.76255893707275, "10": 398.4191679954529, "11": 434.4982190132141}, "active_metric": {"1": 0.15477145148356053, "2": 0.13502405773857262, "3": 0.1245990376904571, "4": 0.12840817963111473, "5": 0.12219326383319973, "6": 0.11778267842822776, "7": 0.1133720930232558, "8": 0.11166800320769843, "9": 0.10976343223736973, "10": 0.1067562149157979, "11": 0.10555332798716921}}}], "failed_candidates": [], "pending_candidates": [{"batch_size": 89, "dropout_1": 0.19654676887125966, "dropout_2": 0.8682666451901773, "learning_rate": 0.00031134631996358774, "n_units_1": 1024, "n_units_2": 707, "wd": 1e-08, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 5}, {"n_units_1": 1024, "n_units_2": 1002, "batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "wd": 1.1611672813117278e-08, "RESOURCE_ATTR_epoch": 2}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 12}]}'
)
# elapsed_time = 203.53759908676147
# num_observations = 23
# num_configs = 10

_model_params.append(
    '{"noise_variance": 0.012624704488939506, "kernel_inv_bw0": 0.0026714958295617746, "kernel_inv_bw1": 0.002294225496133934, "kernel_inv_bw2": 0.0005810444910329019, "kernel_inv_bw3": 4.756569311119674, "kernel_inv_bw4": 0.41912704911412996, "kernel_inv_bw5": 0.007082508117597436, "kernel_inv_bw6": 0.6008226671164758, "kernel_covariance_scale": 1.2790537663629489, "mean_mean_value": -0.29754767463440174, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.20709875141786813, "issm_beta": 1.1564145320327957}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}, {"candidate": {"n_units_1": 672, "n_units_2": 820, "batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "wd": 0.002536625472111785}, "metrics": {"cost_metric": {"1": 32.458306074142456}, "active_metric": {"1": 0.24486714975845414}}}, {"candidate": {"n_units_1": 688, "n_units_2": 597, "batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "wd": 4.489784182359429e-08}, "metrics": {"cost_metric": {"1": 30.61675500869751}, "active_metric": {"1": 0.8976211984342066}}}], "failed_candidates": [], "pending_candidates": [{"batch_size": 34, "dropout_1": 0.7410256603874262, "dropout_2": 0.046625361151571336, "learning_rate": 0.07937041160202492, "n_units_1": 501, "n_units_2": 601, "wd": 8.340962845965557e-07, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 4}, {"batch_size": 116, "dropout_1": 0.0366350257842321, "dropout_2": 0.6883751950302733, "learning_rate": 0.0003133897834907133, "n_units_1": 1024, "n_units_2": 1002, "wd": 1.1611672813117278e-08, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 9}]}'
)
# elapsed_time = 141.0116560459137
# num_observations = 17
# num_configs = 8

_model_params.append(
    '{"noise_variance": 0.02443305886195063, "kernel_inv_bw0": 0.01410539584512635, "kernel_inv_bw1": 1.4106734173901074, "kernel_inv_bw2": 0.002912772873874073, "kernel_inv_bw3": 0.00010000000000000009, "kernel_inv_bw4": 0.001289783525647755, "kernel_inv_bw5": 6.274402643366595, "kernel_inv_bw6": 0.014263119266637505, "kernel_covariance_scale": 1.0004606474604771, "mean_mean_value": -1.0965610760358047, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.6394260638653898, "issm_beta": 0.8896093870386187}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}, {"candidate": {"n_units_1": 672, "n_units_2": 820, "batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "wd": 0.002536625472111785}, "metrics": {"cost_metric": {"1": 32.458306074142456}, "active_metric": {"1": 0.24486714975845414}}}], "failed_candidates": [], "pending_candidates": [{"batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "n_units_1": 688, "n_units_2": 597, "wd": 4.489784182359429e-08, "RESOURCE_ATTR_epoch": 1}, {"batch_size": 34, "dropout_1": 0.7410256603874262, "dropout_2": 0.046625361151571336, "learning_rate": 0.07937041160202492, "n_units_1": 501, "n_units_2": 601, "wd": 8.340962845965557e-07, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 4}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 7}]}'
)
# elapsed_time = 108.29521012306213
# num_observations = 14
# num_configs = 7

_model_params.append(
    '{"noise_variance": 0.02433360380308369, "kernel_inv_bw0": 0.033230128902756034, "kernel_inv_bw1": 1.3832161502574647, "kernel_inv_bw2": 0.0010926642716173997, "kernel_inv_bw3": 0.0009913284444091315, "kernel_inv_bw4": 0.00037318250862594773, "kernel_inv_bw5": 7.150355629993121, "kernel_inv_bw6": 0.005367219098449991, "kernel_covariance_scale": 0.9234243919759128, "mean_mean_value": -1.0950448515295788, "issm_gamma": 0.0010000000000000002, "issm_alpha": -0.6000883418698378, "issm_beta": 0.814092090699343}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}, {"candidate": {"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338}, "metrics": {"cost_metric": {"1": 29.966220140457153}, "active_metric": {"1": 0.6366185897435898}}}], "failed_candidates": [], "pending_candidates": [{"batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "n_units_1": 672, "n_units_2": 820, "wd": 0.002536625472111785, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 3}, {"batch_size": 123, "dropout_1": 0.7829512576762913, "dropout_2": 0.2834197685256876, "learning_rate": 0.1784738929251937, "n_units_1": 688, "n_units_2": 597, "wd": 4.489784182359429e-08, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 6}]}'
)
# elapsed_time = 91.06228709220886
# num_observations = 11
# num_configs = 6

_model_params.append(
    '{"noise_variance": 4.023868196955162e-08, "kernel_inv_bw0": 0.0499484523829736, "kernel_inv_bw1": 0.5041477744353572, "kernel_inv_bw2": 0.045440426051123285, "kernel_inv_bw3": 3.509634264819305, "kernel_inv_bw4": 1.8117976798318889, "kernel_inv_bw5": 16.29050792588867, "kernel_inv_bw6": 0.011845890028904541, "kernel_covariance_scale": 2.72277711886595, "mean_mean_value": -1.412204593314323, "issm_gamma": 0.0010000000000000002, "issm_alpha": -1.7778733115941194, "issm_beta": 1.226405864173305}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}, {"candidate": {"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949}, "metrics": {"cost_metric": {"1": 44.07283306121826}, "active_metric": {"1": 0.23085404971932644}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 673, "n_units_2": 262, "batch_size": 78, "dropout_1": 0.9510740133913004, "dropout_2": 0.3263851441475057, "learning_rate": 0.009715536539110267, "wd": 0.0002984576239921338, "RESOURCE_ATTR_epoch": 1}, {"batch_size": 108, "dropout_1": 0.6443283647430158, "dropout_2": 0.8194904484310889, "learning_rate": 9.196365243521935e-05, "n_units_1": 672, "n_units_2": 820, "wd": 0.002536625472111785, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 3}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 4}]}'
)
# elapsed_time = 55.957242012023926
# num_observations = 8
# num_configs = 5

_model_params.append(
    '{"noise_variance": 0.0010000000000000002, "kernel_inv_bw0": 1.0, "kernel_inv_bw1": 1.0, "kernel_inv_bw2": 1.0, "kernel_inv_bw3": 1.0, "kernel_inv_bw4": 1.0, "kernel_inv_bw5": 1.0, "kernel_inv_bw6": 1.0, "kernel_covariance_scale": 1.0, "mean_mean_value": 0.0, "issm_gamma": 1.0, "issm_alpha": -0.5, "issm_beta": 0.0}'
)
_state.append(
    '{"candidate_evaluations": [{"candidate": {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607}, "metrics": {"cost_metric": {"1": 12.25258493423462, "2": 24.305160999298096, "3": 44.05741477012634, "4": 62.029183864593506, "5": 81.38737893104553, "6": 99.16185593605042, "7": 118.72888779640198, "8": 133.45333671569824, "9": 148.23734402656555, "10": 166.52369689941406, "11": 194.99460196495056, "12": 215.73117184638977, "13": 235.3977439403534, "14": 253.71279788017273, "15": 267.6743288040161, "16": 281.8612160682678, "17": 296.0602250099182, "18": 310.0040330886841, "19": 324.75612902641296, "20": 344.674284696579, "21": 360.0983910560608, "22": 375.9487638473511, "23": 395.81145191192627, "24": 411.6494069099426, "25": 426.79202795028687, "26": 448.74489879608154, "27": 464.90988278388977, "28": 480.28413486480713, "29": 494.5631868839264, "30": 510.31515073776245, "31": 527.6290948390961, "32": 542.7905468940735, "33": 558.1524910926819, "34": 572.6776859760284, "35": 588.3533399105072}, "active_metric": {"1": 0.4978924126856684, "2": 0.3896025692492975, "3": 0.3546768366118025, "4": 0.33289843436370936, "5": 0.3259735046166198, "6": 0.30971497390606184, "7": 0.29626655961461257, "8": 0.2863307908470494, "9": 0.2753914090726616, "10": 0.26455238859895625, "11": 0.25491770373344036, "12": 0.2485949417904456, "13": 0.24678843837816133, "14": 0.23996386993175434, "15": 0.2332396627860297, "16": 0.23143315937374553, "17": 0.22390606182256123, "18": 0.22350461661983134, "19": 0.2195905258932156, "20": 0.217482938578884, "21": 0.21176234443998398, "22": 0.2106583701324769, "23": 0.20764753111200318, "24": 0.2052388598956243, "25": 0.20102368526696102, "26": 0.1963067041348856, "27": 0.1949016459253312, "28": 0.19409875551987155, "29": 0.1904857486953031, "30": 0.18747490967482938, "31": 0.18516659975913285, "32": 0.18446407065435566, "33": 0.18115214773183463, "34": 0.18004817342432755, "35": 0.17844239261340822}}}, {"candidate": {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05}, "metrics": {"cost_metric": {"1": 18.50484609603882, "2": 45.980664014816284, "3": 96.58328175544739, "4": 148.91678476333618, "5": 229.4046459197998, "6": 300.6477417945862, "7": 383.9124698638916, "8": 473.07399010658264, "9": 555.2527649402618}, "active_metric": {"1": 0.15496198479391754, "2": 0.1519607843137255, "3": 0.13645458183273307, "4": 0.14435774309723892, "5": 0.13225290116046418, "6": 0.1253501400560224, "7": 0.12735094037615047, "8": 0.13265306122448983, "9": 0.12104841936774713}}}, {"candidate": {"n_units_1": 347, "n_units_2": 566, "batch_size": 48, "dropout_1": 0.40991313560097764, "dropout_2": 0.1486640484580416, "learning_rate": 0.0001521657976426163, "wd": 2.46706548222209e-07}, "metrics": {"cost_metric": {"1": 18.432047128677368}, "active_metric": {"1": 0.16786858974358976}}}, {"candidate": {"n_units_1": 91, "n_units_2": 459, "batch_size": 105, "dropout_1": 0.48639033141890325, "dropout_2": 0.21324913218446714, "learning_rate": 0.00013769715715418189, "wd": 0.02017249366944585}, "metrics": {"cost_metric": {"1": 17.439072132110596}, "active_metric": {"1": 0.3006516290726817}}}], "failed_candidates": [], "pending_candidates": [{"n_units_1": 774, "n_units_2": 917, "batch_size": 29, "dropout_1": 0.7778923725289609, "dropout_2": 0.7413003050986398, "learning_rate": 6.472832341968678e-05, "wd": 0.0007744951242384949, "RESOURCE_ATTR_epoch": 1}, {"n_units_1": 514, "n_units_2": 514, "batch_size": 68, "dropout_1": 0.495, "dropout_2": 0.495, "learning_rate": 0.0010000000000000002, "wd": 9.999999999999991e-05, "RESOURCE_ATTR_epoch": 2}, {"n_units_1": 38, "n_units_2": 187, "batch_size": 53, "dropout_1": 0.36209963448394383, "dropout_2": 0.09749003575393035, "learning_rate": 1.180123718822517e-05, "wd": 0.00011948182727147607, "RESOURCE_ATTR_epoch": 3}]}'
)
# elapsed_time = 50.29185605049133
# num_observations = 5
# num_configs = 4


@pytest.mark.parametrize("_model_params, _state", zip(_model_params, _state))
def test_compare_gpiss_likelihood_oldnew(_model_params, _state):
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "wd": loguniform(1e-8, 1),
        "epochs": 81,
    }

    gpiss_model_factory = []  # new, old
    model_params = json.loads(_model_params)
    kwargs = dict(no_fantasizing=True)
    gpiss_objs = build_gpiss_model_factory(config_space, model_params, **kwargs)
    config_space_ext = gpiss_objs["config_space_ext"]
    gpiss_model_factory.append(gpiss_objs["model_factory"])
    gpiss_model_factory.append(
        build_gpiss_model_factory(
            config_space, model_params, use_new_code=False, **kwargs
        )["model_factory"]
    )
    state = decode_state_from_old_encoding(
        enc_state=json.loads(_state), hp_ranges=config_space_ext.hp_ranges_ext
    )

    # Compare likelihoods
    likelihood = [
        factory.model(state, fit_params=False)
        .posterior_states[0]
        .poster_state["likelihood"]
        for factory in gpiss_model_factory
    ]
    for name, value in likelihood[0].items():
        if name != "num_data":
            np.testing.assert_almost_equal(value, likelihood[1][name])


@pytest.mark.parametrize("_model_params, _state", zip(_model_params, _state))
def test_compare_gpiss_likelihood_fantasizing_oldnew(_model_params, _state):
    config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "wd": loguniform(1e-8, 1),
        "epochs": 81,
    }
    num_fantasy_samples = 10

    gpiss_model_factory = []  # new, old
    model_params = json.loads(_model_params)
    kwargs = dict(num_fantasy_samples=num_fantasy_samples, no_fantasizing=False)
    gpiss_objs = build_gpiss_model_factory(config_space, model_params, **kwargs)
    config_space_ext = gpiss_objs["config_space_ext"]
    gpiss_model_factory.append(gpiss_objs["model_factory"])
    gpiss_model_factory.append(
        build_gpiss_model_factory(
            config_space, model_params, use_new_code=False, **kwargs
        )["model_factory"]
    )
    state = decode_state_from_old_encoding(
        enc_state=json.loads(_state), hp_ranges=config_space_ext.hp_ranges_ext
    )

    # Compare likelihoods
    # We need to force them to use the same fantasy samples
    gpiss_model1 = gpiss_model_factory[0].model(state, fit_params=False)
    likelihood = [gpiss_model1.posterior_states[0].poster_state["likelihood"]]
    gpiss_model2 = gpiss_model_factory[1].model_for_fantasy_samples(
        state, fantasy_samples=gpiss_model1.fantasy_samples
    )
    likelihood.append(gpiss_model2.posterior_states[0].poster_state["likelihood"])
    for name, value in likelihood[0].items():
        if name != "num_data":
            np.testing.assert_almost_equal(value, likelihood[1][name])

File Path: tst/schedulers/bayesopt/test_scaling.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# TODO: This code tests XYZScaling, which is only needed for HyperparameterRanges.
# If the latter code is removed, this test can go as well.

import pytest
from numpy.testing import assert_almost_equal

from syne_tune.optimizer.schedulers.searchers.bayesopt.datatypes.scaling import (
    LinearScaling,
    LogScaling,
    ReverseLogScaling,
)


@pytest.mark.parametrize(
    "value, expected, scaling",
    [
        (0.0, 0.0, LinearScaling()),
        (0.5, 0.5, LinearScaling()),
        (5.0, 5.0, LinearScaling()),
        (-5.0, -5.0, LinearScaling()),
        (0.5, -0.69314718055994529, LogScaling()),
        (5.0, 1.6094379124341003, LogScaling()),
        (0.0, 0.0, ReverseLogScaling()),
        (0.5, 0.69314718055994529, ReverseLogScaling()),
    ],
)
def test_to_internal(value, expected, scaling):
    assert_almost_equal(expected, scaling.to_internal(value))


@pytest.mark.parametrize(
    "value, expected, scaling",
    [
        (0.0001, -9.210340371976182, LogScaling()),
        (0.000001, -13.815510557964274, LogScaling()),
        (0.0001, 0.00010000500033334732, ReverseLogScaling()),
        (0.000001, 1.000000500029089e-06, ReverseLogScaling()),
        (0.9999, 9.210340371976294, ReverseLogScaling()),
        (0.999999, 13.815510557935518, ReverseLogScaling()),
    ],
)
def test_close_to_bounds_values(value, expected, scaling):
    assert_almost_equal(expected, scaling.to_internal(value))


@pytest.mark.parametrize(
    "value, scaling",
    [
        (-5.0, LogScaling()),
        (0.0, LogScaling()),
        (5.0, ReverseLogScaling()),
        (-5.0, ReverseLogScaling()),
    ],
)
def test_invalid_values(value, scaling):
    with pytest.raises(AssertionError):
        scaling.to_internal(value)


@pytest.mark.parametrize(
    "value, expected, scaling",
    [
        (0.0, 0.0, LinearScaling()),
        (0.5, 0.5, LinearScaling()),
        (5.0, 5.0, LinearScaling()),
        (-5.0, -5.0, LinearScaling()),
        (0.0, 1.0, LogScaling()),
        (0.5, 1.6487212707001282, LogScaling()),
        (5.0, 148.4131591025766, LogScaling()),
        (-5.0, 0.006737946999085467, LogScaling()),
        (0.0, 0.0, ReverseLogScaling()),
        (0.5, 0.39346934028736658, ReverseLogScaling()),
        (5.0, 0.99326205300091452, ReverseLogScaling()),
        (-5.0, -147.4131591025766, ReverseLogScaling()),
    ],
)
def test_from_internal(value, expected, scaling):
    assert_almost_equal(expected, scaling.from_internal(value))

File Path: tst/schedulers/bayesopt/test_transferlearning.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np

from syne_tune.config_space import uniform, randint, choice
from syne_tune.optimizer.schedulers.searchers.gp_searcher_factory import (
    gp_fifo_searcher_defaults,
    gp_fifo_searcher_factory,
)
from syne_tune.optimizer.schedulers.searchers.utils.default_arguments import (
    check_and_merge_defaults,
)
from syne_tune.optimizer.schedulers.searchers.bayesopt.gpautograd.kernel import (
    Matern52,
    ProductKernelFunction,
)


def test_create_transfer_learning():
    config_space = {
        "task_id": choice(["0", "1", "2", "3"]),
        "a": uniform(lower=0.0, upper=2.0),
        "b": randint(lower=2, upper=5),
        "c": choice(["a", "b", "c"]),
    }
    active_config_space = {
        "a": uniform(lower=0.2, upper=0.8),
        "b": randint(lower=2, upper=4),
        "c": choice(["a", "c"]),
    }
    search_options = {
        "scheduler": "fifo",
        "config_space": config_space,
        "transfer_learning_task_attr": "task_id",
        "transfer_learning_active_task": "2",
        "transfer_learning_active_config_space": active_config_space,
    }
    kwargs = check_and_merge_defaults(
        search_options, *gp_fifo_searcher_defaults(), dict_name="search_options"
    )
    kwargs_int = gp_fifo_searcher_factory(**kwargs)

    filter_observed_data = kwargs_int.get("filter_observed_data")
    assert filter_observed_data is not None
    config = dict(task_id="2", a=0.0, b=2, c="a")
    assert filter_observed_data(config)
    config = dict(task_id="0", a=0.0, b=2, c="a")
    assert not filter_observed_data(config)

    hp_ranges = kwargs_int["hp_ranges"]
    assert hp_ranges.internal_keys == ["task_id", "a", "b", "c"]
    assert hp_ranges.ndarray_size == 9
    expected_ndarray_bounds = [
        (0.0, 0.0),
        (0.0, 0.0),
        (1.0, 1.0),
        (0.0, 0.0),
        (0.1, 0.4),
        (0.0, 0.75),
        (0.0, 1.0),
        (0.0, 0.0),
        (0.0, 1.0),
    ]
    ndarray_bounds = hp_ranges.get_ndarray_bounds()
    mat = np.array([[x[i] for x in ndarray_bounds] for i in range(2)])
    expected_mat = np.array([[x[i] for x in expected_ndarray_bounds] for i in range(2)])
    np.testing.assert_almost_equal(mat, expected_mat)

    kernel = kwargs_int["model_factory"]._gpmodel.likelihood.kernel
    assert isinstance(kernel, ProductKernelFunction)
    kernel1 = kernel.kernel1
    assert isinstance(kernel1, Matern52)
    assert kernel1.dimension == 4
    assert not kernel1.ARD
    kernel2 = kernel.kernel2
    assert isinstance(kernel2, Matern52)
    assert kernel2.dimension == 5
    assert kernel2.ARD

File Path: tst/schedulers/test_dehb.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import pytest
import numpy as np
from typing import Optional

from syne_tune.config_space import randint, choice
from syne_tune.optimizer.schedulers.synchronous import (
    GeometricDifferentialEvolutionHyperbandScheduler,
    DifferentialEvolutionHyperbandScheduler,
)


def _create_scheduler(
    mutation_factor: Optional[float] = None,
    crossover_probability: Optional[float] = None,
) -> DifferentialEvolutionHyperbandScheduler:
    config_space = {
        "a": randint(0, 5),
        "b": choice(["a", "b", "c"]),
    }
    return GeometricDifferentialEvolutionHyperbandScheduler(
        config_space=config_space,
        searcher="random_encoded",
        search_options={"debug_log": False},
        mode="min",
        metric="criterion",
        max_resource_level=9,
        grace_period=1,
        reduction_factor=3,
        resource_attr="epoch",
        random_seed=31415927,
        mutation_factor=mutation_factor,
        crossover_probability=crossover_probability,
    )


class FixedUniformRandomState:
    def __init__(self, fill_value: float):
        self._fill_value = fill_value

    def uniform(self, low: float, high: float, size: int = 1):
        if size == 1:
            return self._fill_value
        else:
            return np.full(shape=(size,), fill_value=self._fill_value)


_de_mutation_parameterizations = [
    [0.5, 0.2],
    [0.75, 0.3],
    [0.1, 0.4],
]


@pytest.mark.parametrize("mutation_factor, fill_value", _de_mutation_parameterizations)
def test_de_mutation(mutation_factor, fill_value):
    scheduler = _create_scheduler(mutation_factor=mutation_factor)
    # Ask for 3 suggestions
    for trial_id in range(3):
        suggestion = scheduler.suggest(trial_id)
        assert suggestion.spawn_new_trial_id
    assert len(scheduler._trial_info) == 3
    assert all(x.level == 1 for x in scheduler._trial_info.values())
    # Control behavior for boundary violations
    scheduler.random_state = FixedUniformRandomState(fill_value)
    mutant = scheduler._de_mutation(list(range(3)))
    encoded_configs = [
        scheduler._trial_info[trial_id].encoded_config for trial_id in range(3)
    ]
    mutant_ours = (
        encoded_configs[1] - encoded_configs[2]
    ) * mutation_factor + encoded_configs[0]
    for i, v in enumerate(mutant_ours):
        if v > 1 or v < 0:
            mutant_ours[i] = fill_value
    assert np.all(
        mutant == mutant_ours
    ), f"mutant = {mutant}\nmutant_ours = {mutant_ours}"


class FixedCrossoverRandomState:
    def __init__(self, rand_vals: np.ndarray, randint_val: int):
        self._rand_vals = rand_vals
        self._randint_val = randint_val

    def rand(self, size: int):
        assert self._rand_vals.size == size
        return self._rand_vals

    def randint(self, start: int, end: int):
        assert start <= self._randint_val < end
        return self._randint_val


_crossover_parameterizations = [
    [0.5, [0.45, 0.7], 1, [True, False]],
    [0.2, [0.45, 0.1], 0, [False, True]],
    [0.1, [0.15, 0.55], 0, [True, False]],
]


@pytest.mark.parametrize(
    "crossover_probability, rand_vals, randint_val, _hp_mask",
    _crossover_parameterizations,
)
def test_crossover(crossover_probability, rand_vals, randint_val, _hp_mask):
    scheduler = _create_scheduler(crossover_probability=crossover_probability)
    # Ask for 2 suggestions
    for trial_id in range(2):
        suggestion = scheduler.suggest(trial_id)
        assert suggestion.spawn_new_trial_id
    assert len(scheduler._trial_info) == 2
    assert all(x.level == 1 for x in scheduler._trial_info.values())
    # Control random draws in _crossover
    scheduler.random_state = FixedCrossoverRandomState(
        rand_vals=np.array(rand_vals), randint_val=randint_val
    )
    target = scheduler._trial_info[0].encoded_config
    mutant = scheduler._trial_info[1].encoded_config
    offspring = scheduler._crossover(mutant, target)
    hp_mask = np.array(rand_vals) < crossover_probability
    if not np.any(hp_mask):
        hp_mask[randint_val] = True
    assert np.all(hp_mask == np.array(_hp_mask))
    offspring_ours = target.copy()
    if hp_mask[0]:
        offspring_ours[0] = mutant[0]
    if hp_mask[1]:
        offspring_ours[1:] = mutant[1:]
    assert np.all(
        offspring == offspring_ours
    ), f"offspring = {offspring}\noffspring_ours = {offspring_ours}"

File Path: tst/schedulers/test_grid_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

from syne_tune.optimizer.schedulers.searchers.searcher import GridSearcher
from syne_tune.config_space import choice

config_space = {
    "char_attr": choice(["a", "b"]),
    "int_attr": choice([1, 2]),
}
all_candidates_on_grid = [
    {"char_attr": "a", "int_attr": 1},
    {"char_attr": "b", "int_attr": 1},
    {"char_attr": "a", "int_attr": 2},
    {"char_attr": "b", "int_attr": 2},
]


def test_get_config():
    config_spaces = [
        {"single_attr": choice(["a", "b", "c", "d"])},
        {
            "attr_with_duplicates": choice([1, 1, 2, 2]),
            "other_attr": choice(["a", "b"]),
        },
    ]
    num_valid_config = 4

    for config_space in config_spaces:
        searcher = GridSearcher(config_space, metric="accuracy")
        for trial_id in range(num_valid_config):
            # These should get new config
            config = searcher.get_config(trial_id=trial_id)
            assert config is not None

        config = searcher.get_config(trial_id=trial_id)
        assert config is None


def test_generate_all_candidates_on_grid():
    searcher = GridSearcher(config_space, metric="accuracy", points_to_evaluate=[])
    generate_result = searcher._remaining_candidates
    for i in range(len(all_candidates_on_grid)):
        assert generate_result[i] in all_candidates_on_grid


def test_non_shuffle():
    searcher = GridSearcher(config_space, metric="accuracy", shuffle_config=False)
    for i in range(len(all_candidates_on_grid)):
        config = searcher.get_config(trial_id=i)
        assert config == all_candidates_on_grid[i]


def test_get_batch_configs():
    for batch_size in range(1, len(all_candidates_on_grid) + 1):
        searcher = GridSearcher(config_space, metric="accuracy", shuffle_config=False)
        assert (
            searcher.get_batch_configs(batch_size)
            == all_candidates_on_grid[:batch_size]
        )

    # If `batch_size` is larger than search space, only returns the remaining candidate
    TOO_LARGE_BATCH_SIZE = 100
    searcher = GridSearcher(config_space, metric="accuracy", shuffle_config=False)
    assert searcher.get_batch_configs(TOO_LARGE_BATCH_SIZE) == all_candidates_on_grid


def test_store_and_restore_state_without_initial_config():
    searcher = GridSearcher(
        config_space, metric="accuracy", point_to_evaluate=[], shuffle_config=False
    )
    previous_config = searcher.get_config(trial_id=0)
    state = searcher.get_state()
    new_searcher = searcher.clone_from_state(state)
    assert previous_config == all_candidates_on_grid[0]
    for i in range(1, len(all_candidates_on_grid)):
        new_config = new_searcher.get_config(trail_id=i)
        assert new_config == all_candidates_on_grid[i]


def test_store_and_restore_state_with_initial_config():
    inital_config = [
        {"char_attr": "a", "int_attr": 1},
        {"char_attr": "b", "int_attr": 2},
    ]
    searcher = GridSearcher(
        config_space,
        metric="accuracy",
        points_to_evaluate=inital_config,
        shuffle_config=False,
    )
    previous_config = searcher.get_config(trial_id=0)
    state = searcher.get_state()
    new_searcher = searcher.clone_from_state(state)
    assert previous_config == all_candidates_on_grid[0]
    for idx in [3, 1, 2]:
        new_config = new_searcher.get_config(trail_id=idx)
        assert new_config == all_candidates_on_grid[idx]

File Path: tst/schedulers/test_hyperband.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from datetime import datetime
from typing import Optional, Dict, Tuple
import pytest

from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.config_space import randint, uniform
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import SchedulerDecision
from syne_tune.optimizer.schedulers.searchers import RandomSearcher


def _make_result(epoch, metric):
    return dict(epoch=epoch, metric=metric)


def _new_trial(trial_id: int, config: dict):
    return Trial(trial_id=trial_id, config=config, creation_time=datetime.now())


class MyRandomSearcher(RandomSearcher):
    def __init__(self, config_space, metric, points_to_evaluate=None, **kwargs):
        super().__init__(config_space, metric, points_to_evaluate, **kwargs)
        self._pending_records = []

    def register_pending(
        self, trial_id: str, config: Optional[Dict] = None, milestone=None
    ):
        self._pending_records.append((trial_id, config, milestone))

    def get_pending_records(self):
        result = self._pending_records
        self._pending_records = []
        return result


def _should_be(record: Tuple, trial_id: int, milestone: int, config_none: bool):
    assert record[0] == str(trial_id) and record[2] == milestone, (
        record,
        trial_id,
        milestone,
    )
    assert config_none == (record[1] is None), (record, config_none)


def test_register_pending():
    config_space = {"int": randint(1, 2), "float": uniform(5.5, 6.5), "epochs": 27}
    grace_period = 3
    reduction_factor = 3

    for searcher_data in ("rungs", "all"):
        # We need to plug in a searcher which logs calls to `register_pending`
        scheduler = HyperbandScheduler(
            config_space,
            searcher="random",
            metric="metric",
            mode="min",
            resource_attr="epoch",
            max_resource_attr="epochs",
            grace_period=grace_period,
            reduction_factor=reduction_factor,
            searcher_data=searcher_data,
        )
        old_searcher = scheduler.searcher
        new_searcher = MyRandomSearcher(
            old_searcher.config_space, metric=old_searcher._metric
        )
        scheduler.searcher = new_searcher

        # Start 4 trials (0, 1, 2, 3)
        trials = dict()
        for trial_id in range(4):
            trials[trial_id] = _new_trial(
                trial_id, scheduler.suggest(trial_id=trial_id).config
            )
        records = new_searcher.get_pending_records()
        if searcher_data == "rungs":
            assert len(records) == 4, records
            for trial_id, record in enumerate(records):
                _should_be(record, trial_id, grace_period, False)
        else:
            assert len(records) == 4 * grace_period, records
            for i, record in enumerate(records):
                trial_id = i // grace_period
                milestone = (i % grace_period) + 1
                _should_be(record, trial_id, milestone, False)

        # 0, 1, 2 continue, but 3 is stopped
        decision = scheduler.on_trial_result(trials[0], _make_result(grace_period, 1.0))
        assert decision == SchedulerDecision.CONTINUE
        decision = scheduler.on_trial_result(trials[1], _make_result(grace_period, 0.9))
        assert decision == SchedulerDecision.CONTINUE
        decision = scheduler.on_trial_result(trials[2], _make_result(grace_period, 0.8))
        assert decision == SchedulerDecision.CONTINUE
        decision = scheduler.on_trial_result(trials[3], _make_result(grace_period, 1.2))
        assert decision == SchedulerDecision.STOP
        records = new_searcher.get_pending_records()
        if searcher_data == "rungs":
            assert len(records) == 3, records
            milestone = grace_period * reduction_factor
            for trial_id, record in enumerate(records):
                _should_be(record, trial_id, milestone, False)
        else:
            num_per_trial = grace_period * (reduction_factor - 1)
            assert len(records) == 3 * num_per_trial, records
            for i, record in enumerate(records):
                trial_id = i // num_per_trial
                milestone = (i % num_per_trial) + grace_period + 1
                _should_be(record, trial_id, milestone, False)


def test_hyperband_max_t_inference():
    config_space1 = {
        "epochs": 15,
        "max_t": 14,
        "max_epochs": 13,
        "blurb": randint(0, 20),
    }
    config_space2 = {"max_t": 14, "max_epochs": 13, "blurb": randint(0, 20)}
    config_space3 = {"max_epochs": 13, "blurb": randint(0, 20)}
    config_space4 = {
        "epochs": randint(15, 20),
        "max_t": 14,
        "max_epochs": 13,
        "blurb": randint(0, 20),
    }
    config_space5 = {
        "epochs": randint(15, 20),
        "max_t": randint(14, 21),
        "max_epochs": 13,
        "blurb": randint(0, 20),
    }
    config_space6 = {"blurb": randint(0, 20)}
    config_space7 = {
        "epochs": randint(15, 20),
        "max_t": randint(14, 21),
        "max_epochs": randint(13, 22),
        "blurb": randint(0, 20),
    }
    # Fields: (max_t, config_space, final_max_t)
    # If final_max_t is None, an assertion should be raised
    cases = [
        (None, config_space1, 15),
        (None, config_space2, 14),
        (None, config_space3, 13),
        (None, config_space4, 14),
        (None, config_space5, 13),
        (None, config_space6, None),
        (None, config_space7, None),
        (10, config_space1, 10),
        (10, config_space2, 10),
        (10, config_space3, 10),
        (10, config_space4, 10),
        (10, config_space5, 10),
        (10, config_space6, 10),
        (10, config_space7, 10),
    ]

    for max_t, config_space, final_max_t in cases:
        if final_max_t is not None:
            myscheduler = HyperbandScheduler(
                config_space,
                searcher="random",
                max_t=max_t,
                resource_attr="epoch",
                mode="max",
                metric="accuracy",
            )
            assert final_max_t == myscheduler.max_t
        else:
            with pytest.raises(AssertionError):
                myscheduler = HyperbandScheduler(
                    config_space,
                    searcher="random",
                    max_t=max_t,
                    resource_attr="epoch",
                    mode="max",
                    metric="accuracy",
                )

File Path: tst/schedulers/test_hyperband_cost_promotion.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from datetime import datetime

from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.config_space import randint, uniform
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import SchedulerDecision


def _make_result(epoch, metric, cost):
    return dict(epoch=epoch, metric=metric, cost=cost)


def _new_trial(trial_id: int, config: dict):
    return Trial(trial_id=trial_id, config=config, creation_time=datetime.now())


def test_cost_offset():
    config_space = {"int": randint(1, 2), "float": uniform(5.5, 6.5), "epochs": 27}
    scheduler = HyperbandScheduler(
        config_space,
        searcher="random",
        metric="metric",
        mode="max",
        resource_attr="epoch",
        type="cost_promotion",
        max_resource_attr="epochs",
        cost_attr="cost",
    )
    # Start 4 trials
    trials = dict()
    for trial_id in range(4):
        trials[trial_id] = _new_trial(
            trial_id, scheduler.suggest(trial_id=trial_id).config
        )
    # Make sure that 0, 1 are promoted eventually
    decision = scheduler.on_trial_result(trials[0], _make_result(1, 0.9, 1.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(0)] == 1.0
    sugg = scheduler.suggest(trial_id=len(trials))
    assert sugg.spawn_new_trial_id
    trials[4] = _new_trial(4, sugg.config)
    decision = scheduler.on_trial_result(trials[1], _make_result(1, 0.8, 3.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(1)] == 3.0
    # 1 < 4/3 -> Promote 0
    sugg = scheduler.suggest(trial_id=len(trials))
    assert not sugg.spawn_new_trial_id
    assert sugg.checkpoint_trial_id == 0
    assert sugg.config is not None
    trials[0].config = sugg.config
    decision = scheduler.on_trial_result(trials[2], _make_result(1, 0.7, 10.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(2)] == 10.0
    decision = scheduler.on_trial_result(trials[0], _make_result(3, 0.95, 3.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(0)] == 4.0
    # 4 < 14/3 -> Promote 1
    sugg = scheduler.suggest(trial_id=len(trials))
    assert not sugg.spawn_new_trial_id
    assert sugg.checkpoint_trial_id == 1
    assert sugg.config is not None
    trials[1].config = sugg.config
    decision = scheduler.on_trial_result(trials[1], _make_result(3, 0.85, 4.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(1)] == 7.0
    new_trial_id = len(trials)
    # Nothing can be promoted here (4 > 11/3)
    sugg = scheduler.suggest(trial_id=new_trial_id)
    assert sugg.spawn_new_trial_id
    trials[new_trial_id] = _new_trial(new_trial_id, sugg.config)


# Same scenario as above, but resumed trials start from
# scratch, which should lead to cost offsets being reset
def test_reset_cost_offset():
    config_space = {"int": randint(1, 2), "float": uniform(5.5, 6.5), "epochs": 27}
    scheduler = HyperbandScheduler(
        config_space,
        searcher="random",
        metric="metric",
        mode="max",
        resource_attr="epoch",
        type="cost_promotion",
        max_resource_attr="epochs",
        cost_attr="cost",
    )
    # Start 4 trials
    trials = dict()
    for trial_id in range(4):
        trials[trial_id] = _new_trial(
            trial_id, scheduler.suggest(trial_id=trial_id).config
        )
    # Make sure that 0, 1 are promoted eventually
    decision = scheduler.on_trial_result(trials[0], _make_result(1, 0.9, 1.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(0)] == 1.0
    sugg = scheduler.suggest(trial_id=len(trials))
    assert sugg.spawn_new_trial_id
    trials[4] = _new_trial(4, sugg.config)
    decision = scheduler.on_trial_result(trials[1], _make_result(1, 0.8, 3.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(1)] == 3.0
    # 1 < 4/3 -> Promote 0
    sugg = scheduler.suggest(trial_id=len(trials))
    assert not sugg.spawn_new_trial_id
    assert sugg.checkpoint_trial_id == 0
    assert sugg.config is not None
    trials[0].config = sugg.config
    decision = scheduler.on_trial_result(trials[2], _make_result(1, 0.7, 10.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(2)] == 10.0
    # trial_id 0 reports for epoch=1, which signals restart
    # This should trigger reset of cost offset
    decision = scheduler.on_trial_result(trials[0], _make_result(1, 0.9, 1.5))
    assert decision == SchedulerDecision.CONTINUE
    assert scheduler._cost_offset[str(0)] == 0.0
    decision = scheduler.on_trial_result(trials[0], _make_result(2, 0.91, 2.5))
    assert decision == SchedulerDecision.CONTINUE
    assert scheduler._cost_offset[str(0)] == 0.0
    decision = scheduler.on_trial_result(trials[0], _make_result(3, 0.95, 3.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(0)] == 3.0
    # 4 < 14/3 -> Promote 1
    sugg = scheduler.suggest(trial_id=len(trials))
    assert not sugg.spawn_new_trial_id
    assert sugg.checkpoint_trial_id == 1
    assert sugg.config is not None
    trials[1].config = sugg.config
    # trial_id 1 reports for epoch=1, which signals restart
    # This should trigger reset of cost offset
    decision = scheduler.on_trial_result(trials[1], _make_result(1, 0.8, 2.5))
    assert decision == SchedulerDecision.CONTINUE
    assert scheduler._cost_offset[str(1)] == 0.0
    decision = scheduler.on_trial_result(trials[1], _make_result(2, 0.81, 3.5))
    assert decision == SchedulerDecision.CONTINUE
    assert scheduler._cost_offset[str(1)] == 0.0
    decision = scheduler.on_trial_result(trials[1], _make_result(3, 0.85, 4.0))
    assert decision == SchedulerDecision.PAUSE
    assert scheduler._cost_offset[str(1)] == 4.0

File Path: tst/schedulers/test_hyperband_sychronous.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Tuple
import numpy as np
from collections import Counter

from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket import (
    SynchronousHyperbandBracket,
    SlotInRung,
)
from syne_tune.optimizer.schedulers.synchronous.hyperband_bracket_manager import (
    SynchronousHyperbandBracketManager,
)
from syne_tune.optimizer.schedulers.synchronous.hyperband_rung_system import (
    SynchronousHyperbandRungSystem,
)


def _trial_ids(lst):
    return [x[0] for x in lst]


def _ask_for_slots(
    bracket: SynchronousHyperbandBracket,
    rung_index: int,
    level: int,
    slot_index: int,
    trial_ids: list,
) -> (List[SlotInRung], int):
    slots = []
    for trial_id in trial_ids:
        slot_in_rung = bracket.next_free_slot()
        assert slot_in_rung is not None
        should_be = SlotInRung(
            rung_index=rung_index,
            level=level,
            slot_index=slot_index,
            trial_id=trial_id,
            metric_val=None,
        )
        assert slot_in_rung == should_be, (slot_in_rung, should_be)
        slots.append(slot_in_rung)
        slot_index += 1
    return slots, slot_index


def _send_results(
    bracket: SynchronousHyperbandBracket,
    slots: List[SlotInRung],
    all_results: List[Tuple[int, float]],
):
    for slot_in_rung in slots:
        trial_id, metric_val = all_results[slot_in_rung.slot_index]
        result = SlotInRung(
            rung_index=slot_in_rung.rung_index,
            level=slot_in_rung.level,
            slot_index=slot_in_rung.slot_index,
            trial_id=trial_id,
            metric_val=metric_val,
        )
        bracket.on_result(result)


def test_hyperband_bracket():
    rungs = [(9, 1), (4, 3), (1, 9)]
    results = [
        [
            (0, 3.0),
            (1, 5.0),
            (2, 1.0),
            (3, 4.0),
            (4, 9.0),
            (5, 6.0),
            (6, 2.0),
            (7, 7.0),
            (8, 8.0),
        ],
        [(2, 3.1), (6, 3.0), (0, 2.9), (3, 3.0)],
        [(0, 1.0)],
    ]
    bracket = SynchronousHyperbandBracket(rungs, mode="min")

    # Rung index 0
    rung_index = 0
    level = rungs[rung_index][1]
    slot_index = 0
    # Ask for some and return before asking for more
    num_jobs = 3
    slots, slot_index = _ask_for_slots(
        bracket, rung_index, level, slot_index, trial_ids=[None] * num_jobs
    )
    assert bracket.num_pending_slots() == num_jobs
    _send_results(bracket, slots, results[rung_index])
    assert bracket.num_pending_slots() == 0
    # Ask for some, but do not return all for now
    num_jobs = 3
    slots_remaining = []
    for i in range(2):
        slots, slot_index = _ask_for_slots(
            bracket, rung_index, level, slot_index, trial_ids=[None] * num_jobs
        )
        assert bracket.num_pending_slots() == num_jobs + i
        slots_remaining.append(slots[0])
        slots = slots[1:]
        _send_results(bracket, slots, results[rung_index])
        assert bracket.num_pending_slots() == i + 1
    # At this point, there are no free slots, but some are pending
    for slot in slots_remaining:
        assert bracket.next_free_slot() is None
        _send_results(bracket, [slot], results[rung_index])
    # The first rung must be fully occupied now
    assert bracket.num_pending_slots() == 0

    # Other rungs
    for rung_index, all_results in enumerate(results[1:], start=1):
        num_jobs, level = rungs[rung_index]
        slot_index = 0
        trial_ids = [x[0] for x in all_results]
        slots, slot_index = _ask_for_slots(
            bracket, rung_index, level, slot_index, trial_ids=trial_ids
        )
        assert bracket.num_pending_slots() == num_jobs
        assert bracket.next_free_slot() is None
        _send_results(bracket, slots, all_results)
        assert bracket.num_pending_slots() == 0
    # Now, the bracket must be complete
    assert bracket.is_bracket_complete()


def _send_result(
    bracket_manager: SynchronousHyperbandBracketManager,
    slots: List[Tuple[int, SlotInRung]],
    next_trial_id: int,
    random_state: np.random.RandomState,
) -> int:
    bracket_id, slot_in_rung = slots.pop(0)
    if slot_in_rung.trial_id is None:
        slot_in_rung.trial_id = next_trial_id
        next_trial_id += 1
    slot_in_rung.metric_val = random_state.random()
    bracket_manager.on_result((bracket_id, slot_in_rung))
    return next_trial_id


# Runs Hyperband for some number of iterations, checking that no assertions
# are raised
def test_hyperband_bracket_manager_running():
    random_seed = 31415927
    random_state = np.random.RandomState(random_seed)

    bracket_rungs = SynchronousHyperbandRungSystem.geometric(
        min_resource=2, max_resource=200, reduction_factor=3, num_brackets=6
    )
    bracket_manager = SynchronousHyperbandBracketManager(bracket_rungs, mode="min")
    num_jobs = 4
    num_return = 3
    num_steps = 5000
    next_trial_id = 0
    pending_slots = []
    for step in range(num_steps):
        for _ in range(num_jobs):
            pending_slots.append(bracket_manager.next_job())
        # Report results for some, but not all
        for _ in range(num_return):
            next_trial_id = _send_result(
                bracket_manager, pending_slots, next_trial_id, random_state
            )
        # Test whether number of pending are correct
        histogram = Counter([x[0] for x in pending_slots])
        for bracket_id, num_pending in histogram.items():
            assert (
                bracket_manager._brackets[bracket_id].num_pending_slots() == num_pending
            )
        if len(pending_slots) >= 200:
            # Clear all pending slots in random ordering
            for pos in random_state.permutation(len(pending_slots)):
                next_trial_id = _send_result(
                    bracket_manager, [pending_slots[pos]], next_trial_id, random_state
                )
            pending_slots = []
            # Nothing should be pending anymore
            for bracket_id in range(
                bracket_manager._primary_bracket_id, bracket_manager._next_bracket_id
            ):
                assert bracket_manager._brackets[bracket_id].num_pending_slots() == 0

File Path: tst/schedulers/test_pasha.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.hyperband_pasha import PASHARungSystem


def create_pasha_rung_system(
    mode="max", epsilon=1.0, epsilon_scaling=1.0, ranking_criterion="soft_ranking"
):
    """
    Function to set-up the rung system for testing. It is possible to pass the relevant arguments for testing.
    """
    rung_levels = [1, 3, 9, 27, 81]
    promote_quantiles = [
        0.3333333333333333,
        0.3333333333333333,
        0.3333333333333333,
        0.3333333333333333,
        0.405,
    ]
    metric = "objective"
    resource_attr = "epoch"
    max_t = 200

    pasha_rung_rystem = PASHARungSystem(
        rung_levels,
        promote_quantiles,
        metric,
        mode,
        resource_attr,
        max_t,
        ranking_criterion,
        epsilon,
        epsilon_scaling,
    )
    return pasha_rung_rystem


def test_resources_increase():
    # a simple test for a simple method
    prs = create_pasha_rung_system(epsilon=0.1)

    oneranking = [[("0", 0, 10.0), ("1", 3, 19.6), ("2", 2, 14.3), ("3", 1, 11.6)]]
    tworankings = [
        [("0", 0, 10.0), ("1", 3, 19.6), ("2", 2, 14.3), ("3", 1, 11.6)],
        [("0", 0, 10.1), ("1", 3, 19.7), ("2", 2, 14.2), ("3", 1, 11.5)],
    ]

    assert prs._decide_resource_increase(oneranking) == False

    assert prs._decide_resource_increase(tworankings) == False

    tworankings = [
        [("0", 0, 10.0), ("1", 1, 11.55), ("2", 3, 14.3), ("3", 2, 11.6)],
        [("0", 0, 10.1), ("1", 3, 19.7), ("2", 2, 14.2), ("3", 1, 11.5)],
    ]
    assert prs._decide_resource_increase(tworankings) == True


def test_soft_ranking_criterion():
    """
    Test for verifying the correctness of the soft ranking criterion.

    There are several variations of the soft ranking, but they are all
    fundamentally the same and the variations only use simple methods
    for estimating the value of epsilon - so it is enough to have one set of tests.
    """
    # test if the ranking is the same
    pasha_rung_system = create_pasha_rung_system(mode="min", epsilon=0.3)
    rankings = [
        [("0", 0, 10.0), ("1", 3, 19.6), ("2", 2, 14.3), ("3", 1, 11.6)],
        [("0", 0, 10.1), ("1", 3, 19.7), ("2", 2, 14.2), ("3", 1, 11.5)],
    ]
    sorted_top_rung, sorted_previous_rung = pasha_rung_system._get_sorted_top_rungs(
        rankings
    )
    keep_current_budget = pasha_rung_system._evaluate_soft_ranking(
        sorted_top_rung, sorted_previous_rung
    )

    assert keep_current_budget == True

    # test if the change is only within the group
    rankings = [
        [("0", 0, 10.0), ("1", 3, 14.6), ("2", 2, 14.3), ("3", 1, 11.6)],
        [("0", 0, 10.0), ("1", 2, 14.2), ("2", 3, 14.3), ("3", 1, 11.6)],
    ]
    sorted_top_rung, sorted_previous_rung = pasha_rung_system._get_sorted_top_rungs(
        rankings
    )
    keep_current_budget = pasha_rung_system._evaluate_soft_ranking(
        sorted_top_rung, sorted_previous_rung
    )

    assert keep_current_budget == True

    # test if the change is outside the group
    pasha_rung_system = create_pasha_rung_system(mode="min", epsilon=0.03)
    rankings = [
        [("0", 0, 10.0), ("1", 3, 14.6), ("2", 2, 14.3), ("3", 1, 11.6)],
        [("0", 0, 10.0), ("1", 2, 14.2), ("2", 3, 14.3), ("3", 1, 11.6)],
    ]
    sorted_top_rung, sorted_previous_rung = pasha_rung_system._get_sorted_top_rungs(
        rankings
    )
    keep_current_budget = pasha_rung_system._evaluate_soft_ranking(
        sorted_top_rung, sorted_previous_rung
    )

    assert keep_current_budget == False

    # test if the change is within the group while maximizing the objective
    pasha_rung_system = create_pasha_rung_system(mode="max", epsilon=0.3)
    rankings = [
        [("0", 0, 10.0), ("1", 3, 14.6), ("2", 2, 14.3), ("3", 1, 11.6)],
        [("0", 0, 10.0), ("1", 2, 14.2), ("2", 3, 14.3), ("3", 1, 11.6)],
    ]
    sorted_top_rung, sorted_previous_rung = pasha_rung_system._get_sorted_top_rungs(
        rankings
    )
    keep_current_budget = pasha_rung_system._evaluate_soft_ranking(
        sorted_top_rung, sorted_previous_rung
    )

    assert keep_current_budget == True

File Path: tst/schedulers/test_random_searcher.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.schedulers.searchers.searcher import RandomSearcher
from syne_tune.config_space import choice, randint


def test_no_duplicates():
    config_spaces = [
        {"cat_attr": choice(["a", "b"])},
        {"int_attr": randint(lower=0, upper=1)},
    ]
    num_suggest_to_fail = 3

    for config_space in config_spaces:
        searcher = RandomSearcher(config_space, metric="accuracy")
        for trial_id in range(num_suggest_to_fail):
            # These should not fail
            config = searcher.get_config(trial_id=trial_id)
            if trial_id < num_suggest_to_fail - 1:
                assert config is not None
            else:
                assert config is None

File Path: tst/schedulers/test_schedulers_api.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import tempfile
from pathlib import Path

import dill
import pytest

# FIXME: Needs Ray to be installed
from ray.tune.schedulers import AsyncHyperBandScheduler
import pandas as pd
import numpy as np

from examples.launch_height_standalone_scheduler import SimpleScheduler
from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.baselines import (
    GridSearch,
    RandomSearch,
    BayesianOptimization,
    ASHA,
    MOBSTER,
    REA,
    SyncHyperband,
    SyncBOHB,
    SyncMOBSTER,
    ZeroShotTransfer,
    # ASHACTS,
)
from syne_tune.optimizer.scheduler import SchedulerDecision
from syne_tune.optimizer.schedulers import (
    FIFOScheduler,
    MedianStoppingRule,
    HyperbandScheduler,
    PopulationBasedTraining,
    RayTuneScheduler,
)
from syne_tune.optimizer.schedulers.botorch.botorch_searcher import BotorchSearcher

# FIXME: Resolve #324 and bring back in:
# from syne_tune.optimizer.schedulers.botorch.botorch_searcher import BotorchSearcher
from syne_tune.optimizer.schedulers.multiobjective import MOASHA
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
    BoundingBox,
    RUSHScheduler,
)
from syne_tune.optimizer.schedulers.transfer_learning.quantile_based.quantile_based_searcher import (
    QuantileBasedSurrogateSearcher,
)
from syne_tune.config_space import randint, uniform, choice


config_space = {
    "steps": 100,
    "x": randint(0, 20),
    "y": uniform(0, 1),
    "z": choice(["a", "b", "c"]),
}

categorical_config_space = {
    "steps": 100,
    "x": choice(["0", "1", "2"]),
    "y": choice([0, 1, 2]),
    "z": choice(["a", "b", "c"]),
}

metric1 = "objective1"
metric2 = "objective2"
resource_attr = "step"
max_t = 10
mode = "max"


def make_ray_skopt():
    from ray.tune.suggest.skopt import SkOptSearch

    ray_searcher = SkOptSearch()
    ray_searcher.set_search_properties(
        mode=mode,
        metric=metric1,
        config=RayTuneScheduler.convert_config_space(config_space),
    )
    return ray_searcher


def make_transfer_learning_evaluations(num_evals: int = 10):
    num_seeds = 3
    num_fidelity = 5
    return {
        "dummy-task-1": TransferLearningTaskEvaluations(
            config_space,
            hyperparameters=pd.DataFrame(
                [
                    {
                        k: v.sample() if hasattr(v, "sample") else v
                        for k, v in config_space.items()
                    }
                    for _ in range(10)
                ]
            ),
            objectives_evaluations=np.arange(
                num_evals * num_seeds * num_fidelity * 2
            ).reshape(num_evals, num_seeds, num_fidelity, 2),
            objectives_names=[metric1, metric2],
        ),
        "dummy-task-2": TransferLearningTaskEvaluations(
            config_space,
            hyperparameters=pd.DataFrame(
                [
                    {
                        k: v.sample() if hasattr(v, "sample") else v
                        for k, v in config_space.items()
                    }
                    for _ in range(10)
                ]
            ),
            objectives_evaluations=-np.arange(
                num_evals * num_seeds * num_fidelity * 2
            ).reshape(num_evals, num_seeds, num_fidelity, 2),
            objectives_names=[metric1, metric2],
        ),
    }


transfer_learning_evaluations = make_transfer_learning_evaluations()


@pytest.mark.parametrize(
    "scheduler",
    [
        FIFOScheduler(config_space, searcher="random", metric=metric1, mode=mode),
        FIFOScheduler(config_space, searcher="bayesopt", metric=metric1, mode=mode),
        FIFOScheduler(config_space, searcher="kde", metric=metric1, mode=mode),
        FIFOScheduler(config_space, searcher="bore", metric=metric1, mode=mode),
        FIFOScheduler(
            categorical_config_space, searcher="grid", metric=metric1, mode=mode
        ),
        HyperbandScheduler(
            config_space,
            searcher="random",
            resource_attr=resource_attr,
            max_t=max_t,
            metric=metric1,
            mode=mode,
        ),
        HyperbandScheduler(
            config_space,
            searcher="bayesopt",
            resource_attr=resource_attr,
            max_t=max_t,
            metric=metric1,
            mode=mode,
        ),
        HyperbandScheduler(
            config_space,
            searcher="bore",
            resource_attr=resource_attr,
            max_t=max_t,
            metric=metric1,
            mode=mode,
        ),
        HyperbandScheduler(
            config_space,
            searcher="random",
            type="pasha",
            max_t=max_t,
            resource_attr=resource_attr,
            metric=metric1,
            mode=mode,
        ),
        PopulationBasedTraining(
            config_space=config_space,
            metric=metric1,
            resource_attr=resource_attr,
            max_t=max_t,
            mode=mode,
        ),
        RayTuneScheduler(
            config_space=config_space,
            ray_scheduler=AsyncHyperBandScheduler(
                max_t=max_t, time_attr=resource_attr, mode=mode, metric=metric1
            ),
        ),
        RayTuneScheduler(
            config_space=config_space,
            ray_scheduler=AsyncHyperBandScheduler(
                max_t=max_t, time_attr=resource_attr, mode=mode, metric=metric1
            ),
            ray_searcher=make_ray_skopt(),
        ),
        SimpleScheduler(config_space=config_space, metric=metric1, mode=mode),
        RandomSearch(config_space=config_space, metric=metric1, mode=mode),
        GridSearch(config_space=categorical_config_space, metric=metric1, mode=mode),
        BayesianOptimization(config_space=config_space, metric=metric1, mode=mode),
        REA(
            config_space=config_space,
            metric=metric1,
            population_size=1,
            sample_size=2,
            mode=mode,
        ),
        ASHA(
            config_space=config_space,
            metric=metric1,
            resource_attr=resource_attr,
            max_t=max_t,
            mode=mode,
        ),
        MOBSTER(
            config_space=config_space,
            metric=metric1,
            resource_attr=resource_attr,
            max_t=max_t,
            mode=mode,
        ),
        MOBSTER(
            config_space=config_space,
            search_options={"model": "gp_independent"},
            metric=metric1,
            resource_attr=resource_attr,
            max_t=max_t,
            mode=mode,
        ),
        # TODO fix me, assert is thrown refusing to take PASHA arguments as valid
        # PASHA(config_space=config_space, metric=metric1, resource_attr=resource_attr, max_t=max_t),
        MOASHA(
            config_space=config_space,
            time_attr=resource_attr,
            metrics=[metric1, metric2],
            mode=mode,
        ),
        MedianStoppingRule(
            scheduler=FIFOScheduler(
                config_space, searcher="random", metric=metric1, mode=mode
            ),
            resource_attr=resource_attr,
            metric=metric1,
        ),
        BoundingBox(
            scheduler_fun=lambda new_config_space, mode, metric: RandomSearch(
                new_config_space,
                points_to_evaluate=[],
                metric=metric,
                mode=mode,
            ),
            mode=mode,
            config_space=config_space,
            metric=metric1,
            transfer_learning_evaluations=transfer_learning_evaluations,
        ),
        FIFOScheduler(
            searcher=QuantileBasedSurrogateSearcher(
                mode=mode,
                config_space=config_space,
                metric=metric1,
                transfer_learning_evaluations=transfer_learning_evaluations,
            ),
            mode=mode,
            config_space=config_space,
            metric=metric1,
        ),
        RUSHScheduler(
            resource_attr=resource_attr,
            max_t=max_t,
            mode=mode,
            config_space=config_space,
            metric=metric1,
            transfer_learning_evaluations=make_transfer_learning_evaluations(),
        ),
        SyncHyperband(
            config_space=config_space,
            metric=metric1,
            resource_attr=resource_attr,
            max_resource_level=max_t,
            max_resource_attr="steps",
            brackets=3,
            mode=mode,
        ),
        SyncMOBSTER(
            config_space=config_space,
            metric=metric1,
            resource_attr=resource_attr,
            max_resource_level=max_t,
            max_resource_attr="steps",
            brackets=3,
            mode=mode,
        ),
        SyncBOHB(
            config_space=config_space,
            metric=metric1,
            resource_attr=resource_attr,
            max_resource_level=max_t,
            max_resource_attr="steps",
            brackets=3,
            mode=mode,
        ),
        ZeroShotTransfer(
            config_space=config_space,
            metric=metric1,
            transfer_learning_evaluations=transfer_learning_evaluations,
            use_surrogates=True,
            mode=mode,
        ),
        # Commented out for now as takes ~4s to run
        # ASHACTS(
        #     config_space=config_space,
        #     metric=metric1,
        #     transfer_learning_evaluations=transfer_learning_evaluations,
        #     max_t=max_t,
        #     resource_attr=resource_attr,
        # ),
        # FIXME: Resolve #324 and bring back in:
        FIFOScheduler(
            config_space,
            searcher=BotorchSearcher(
                config_space=config_space, metric=metric1, mode=mode
            ),
            metric=metric1,
            mode=mode,
        ),
    ],
)
def test_async_schedulers_api(scheduler):
    trial_ids = range(4)

    if isinstance(scheduler, MOASHA):
        assert scheduler.metric_names() == [metric1, metric2]
    else:
        assert scheduler.metric_names() == [metric1]
    assert scheduler.metric_mode() == mode

    # checks suggestions are properly formatted
    trials = []
    for i in trial_ids:
        suggestion = scheduler.suggest(i)
        assert all(
            x in suggestion.config.keys() for x in config_space.keys()
        ), "suggestion configuration should contain all keys of config_space."
        trials.append(Trial(trial_id=i, config=suggestion.config, creation_time=None))

    for trial in trials:
        scheduler.on_trial_add(trial=trial)

    # checks results can be transmitted with appropriate scheduling decisions
    make_metric = lambda t, x: {resource_attr: t, metric1: x, metric2: -x}
    for i, trial in enumerate(trials):
        for t in range(1, max_t + 1):
            decision = scheduler.on_trial_result(trial, make_metric(t, i))
            assert decision in [
                SchedulerDecision.CONTINUE,
                SchedulerDecision.PAUSE,
                SchedulerDecision.STOP,
            ]

    scheduler.on_trial_error(trials[0])
    for i, trial in enumerate(trials):
        scheduler.on_trial_complete(trial, make_metric(max_t, i))

    # checks serialization
    with tempfile.TemporaryDirectory() as local_path:
        with open(Path(local_path) / "scheduler.dill", "wb") as f:
            dill.dump(scheduler, f)
        with open(Path(local_path) / "scheduler.dill", "rb") as f:
            dill.load(f)

File Path: tst/schedulers/transfer_learning/test_rush.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import numpy as np
import pandas as pd
import pytest

from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.schedulers.hyperband_rush import (
    RUSHStoppingRungSystem,
    RUSHDecider,
)
from syne_tune.optimizer.schedulers.transfer_learning import (
    TransferLearningTaskEvaluations,
)
from syne_tune.optimizer.schedulers.transfer_learning.rush import RUSHScheduler
from syne_tune.config_space import randint


@pytest.fixture()
def config_space():
    return {"steps": 10, "m": randint(0, 20), "b": randint(-100, 100)}


@pytest.fixture()
def scheduler(metadata, config_space, request):
    return RUSHScheduler(
        config_space=config_space,
        metric="loss",
        max_t=10,
        type=request.param,
        transfer_learning_evaluations=metadata,
    )


@pytest.fixture()
def num_threshold_candidates():
    return 2


@pytest.fixture()
def rung_levels():
    return [1, 3, 9, 27, 81]


@pytest.fixture()
def promote_quantiles(rung_levels):
    return [1.0 / 3 for _ in range(len(rung_levels) - 1)] + [1]


@pytest.fixture()
def rung_system(num_threshold_candidates, rung_levels, promote_quantiles):
    rung_system = RUSHStoppingRungSystem(
        num_threshold_candidates=num_threshold_candidates,
        rung_levels=rung_levels,
        promote_quantiles=promote_quantiles,
        metric="loss",
        mode="min",
        resource_attr="steps",
    )
    return rung_system


@pytest.fixture()
def decider(num_threshold_candidates, rung_levels):
    decider = RUSHDecider(num_threshold_candidates=num_threshold_candidates, mode="min")
    decider._thresholds = {level: 0 for level in rung_levels if level < 10}
    return decider


@pytest.fixture()
def custom_rush_points():
    return [{"m": 1, "b": -2}]


@pytest.fixture()
def best_config():
    return {"m": 3, "b": -89, "steps": 10}


@pytest.fixture()
def metadata(config_space, best_config):
    max_steps = 10
    hp_data = [
        best_config,
        {"m": 10, "b": -10, "steps": 10},
        {"m": 2, "b": -92, "steps": 10},
    ]
    hp = pd.DataFrame(hp_data)
    metric = list()
    for i in range(len(hp_data)):
        metric.append(
            [
                [
                    [s, -s]
                    for s in get_learning_curve(
                        hp_data[i]["m"],
                        hp_data[i]["b"],
                        max_steps,
                        n,
                    )
                ]
                for n in [-1.2, 1.1]
            ]
        )

    metrics_names = ["loss", "gain"]
    return {
        "task": TransferLearningTaskEvaluations(
            configuration_space=config_space,
            hyperparameters=hp,
            objectives_evaluations=np.array(metric),
            objectives_names=metrics_names,
        )
    }


@pytest.fixture()
def trial():
    return Trial(trial_id=0, config=dict(), creation_time=0)


def get_learning_curve(m, b, steps, n):
    result = [(m * n) * step + b for step in range(steps)]
    return result


def get_result(loss=0, epoch=1):
    return {
        "loss": loss,
        "epoch": epoch,
    }


def num_estimated_threshold_candidates(scheduler):
    return scheduler.terminator._rung_systems[0]._decider._num_threshold_candidates


@pytest.mark.parametrize("scheduler_type", ["stopping", "promotion"])
def test_given_only_metadata_num_threshold_candidates_equals_number_of_tasks(
    metadata, config_space, scheduler_type
):
    scheduler = RUSHScheduler(
        config_space=config_space,
        metric="loss",
        max_t=10,
        type=scheduler_type,
        transfer_learning_evaluations=metadata,
    )
    assert num_estimated_threshold_candidates(scheduler) == len(metadata)


@pytest.mark.parametrize("scheduler_type", ["stopping", "promotion"])
def test_points_of_evaluations_has_no_impact_on__num_threshold_candidates(
    metadata, config_space, custom_rush_points, scheduler_type
):
    scheduler = RUSHScheduler(
        config_space=config_space,
        metric="loss",
        max_t=10,
        type=scheduler_type,
        transfer_learning_evaluations=metadata,
        points_to_evaluate=custom_rush_points,
    )
    assert num_estimated_threshold_candidates(scheduler) == len(metadata)


@pytest.mark.parametrize("scheduler_type", ["stopping", "promotion"])
def test_given_metadata_and_custom_rush_points_num_threshold_candidates_equals_sum_of_unique_configs(
    metadata, config_space, custom_rush_points, scheduler_type
):
    scheduler = RUSHScheduler(
        config_space=config_space,
        metric="loss",
        max_t=10,
        type=scheduler_type,
        transfer_learning_evaluations=metadata,
        custom_rush_points=custom_rush_points,
    )
    assert num_estimated_threshold_candidates(scheduler) == len(metadata) + len(
        custom_rush_points
    )


@pytest.mark.parametrize("scheduler_type", ["stopping", "promotion"])
def test_given_metadata_and_custom_rush_points_with_overlap_keep_only_unique_configurations(
    metadata, config_space, scheduler_type
):
    custom_rush_points = metadata["task"].hyperparameters.to_dict("records")
    scheduler = RUSHScheduler(
        config_space=config_space,
        metric="loss",
        max_t=10,
        type=scheduler_type,
        transfer_learning_evaluations=metadata,
        custom_rush_points=custom_rush_points,
    )
    assert num_estimated_threshold_candidates(scheduler) == len(custom_rush_points)


def test_given_hyperband_indicates_to_discontinue_return_discontinue(
    decider, num_threshold_candidates
):
    assert not decider.task_continues(
        task_continues=False,
        trial_id=num_threshold_candidates - 1,
        metric_value=-1,
        resource=1,
    )


def test_given_metric_better_than_threshold_update_threshold_if_threshold_configuration(
    decider, num_threshold_candidates, rung_levels
):
    loss = -1
    for rung_level in rung_levels:
        for trial_id in [num_threshold_candidates, num_threshold_candidates - 1]:
            old_val = decider._thresholds.get(rung_level)
            decider.task_continues(
                task_continues=True,
                trial_id=trial_id,
                metric_value=loss,
                resource=rung_level,
            )
            if trial_id == num_threshold_candidates:
                if old_val is None:
                    assert rung_level not in decider._thresholds
                else:
                    assert decider._thresholds[rung_level] == old_val
            else:
                assert decider._thresholds[rung_level] == loss


def test_given_metric_worse_than_threshold_return_discontinue_if_standard_trial(
    decider, num_threshold_candidates, rung_levels
):
    for rung_level in rung_levels[:3]:
        assert not decider.task_continues(
            task_continues=True,
            trial_id=num_threshold_candidates,
            metric_value=0.1,
            resource=rung_level,
        )


@pytest.mark.parametrize("hyperband_decision", [True, False])
def test_given_metric_worse_than_threshold_return_hyperband_decision_if_init_trial(
    decider, num_threshold_candidates, hyperband_decision
):
    assert (
        decider.task_continues(
            task_continues=hyperband_decision,
            trial_id=num_threshold_candidates - 1,
            metric_value=1,
            resource=1,
        )
        is hyperband_decision
    )

File Path: tst/test_benchmark.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import argparse

from syne_tune.config_space import add_to_argparse, randint, uniform, loguniform


def test_add_to_argparse():
    _config_space = {
        "n_units_1": randint(4, 1024),
        "n_units_2": randint(4, 1024),
        "batch_size": randint(8, 128),
        "dropout_1": uniform(0, 0.99),
        "dropout_2": uniform(0, 0.99),
        "learning_rate": loguniform(1e-6, 1),
        "wd": loguniform(1e-8, 1),
    }

    parser = argparse.ArgumentParser()
    parser.add_argument("--debug_log", action="store_true")
    add_to_argparse(parser, _config_space)

    _config = {
        "n_units_1": 6,
        "n_units_2": 100,
        "batch_size": 32,
        "dropout_1": 0.5,
        "dropout_2": 0.9,
        "learning_rate": 0.001,
        "wd": 0.25,
    }

    args, _ = parser.parse_known_args([f"--{k}={v}" for k, v in _config.items()])
    config = vars(args)
    for k, v in _config.items():
        assert k in config, f"{k} not in config"
        assert config[k] == v, f"{config[k]} = config[{k}] != _config[{k}] = {v}"
    assert "debug_log" in config
    assert not config["debug_log"]

File Path: tst/test_bore.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
# this test is commented as an import is causing the whole test suite to fail (without running)
# depending on the numpy/GPy versions installed.
"""
import numpy as np

from datetime import datetime

from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.schedulers.searchers.bore.bore import Bore
from syne_tune.config_space import randint

max_steps = 10

config_space = {
    "steps": max_steps,
    "width": randint(0, 20),
}
time_attr = "step"
metric1 = "mean_loss"
metric2 = "cost"


def make_trial(trial_id: int):
    return Trial(
        trial_id=trial_id,
        config={"steps": 0, "width": trial_id},
        creation_time=datetime.now(),
    )


def test_bore_xgboost():
    searcher = Bore(config_space, metric='accuracy', acq_optimizer='de')

    for i in range(10):
        config = searcher.get_config(trial_id=i)
        result = {'accuracy': np.random.rand(), 'time': 1.0, 'step': 2}

        searcher.on_trial_result('%d' % i, config, result, update=True)

    config = searcher.get_config(trial_id=10)


def test_bore_gp():
    searcher = Bore(config_space, metric='accuracy', classifier='gp')

    for i in range(10):
        config = searcher.get_config(trial_id=i)
        result = {'accuracy': np.random.rand(), 'time': 1.0, 'step': 2}

        searcher.on_trial_result('%d' % i, config, result, update=True)

    config = searcher.get_config(trial_id=10)


def test_bore_mlp():
    searcher = Bore(config_space, metric='accuracy', classifier='mlp')

    for i in range(10):
        config = searcher.get_config(trial_id=i)
        result = {'accuracy': np.random.rand(), 'time': 1.0, 'step': 2}

        searcher.on_trial_result('%d' % i, config, result, update=True)

    config = searcher.get_config(trial_id=10)


def test_bore_rf():
    searcher = Bore(config_space, metric='accuracy', classifier='rf')

    for i in range(10):
        config = searcher.get_config(trial_id=i)
        result = {'accuracy': np.random.rand(), 'time': 1.0, 'step': 2}

        searcher.on_trial_result('%d' % i, config, result, update=True)

    config = searcher.get_config(trial_id=10)
"""

File Path: tst/test_config_space.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import pytest
import numpy as np

from syne_tune.config_space import (
    randint,
    lograndint,
    uniform,
    loguniform,
    choice,
    config_space_size,
    randn,
    to_dict,
    from_dict,
    finrange,
    logfinrange,
    restrict_domain,
)


def test_convert_config_space():
    from ray.tune.sample import Float, Integer, Categorical
    from syne_tune.optimizer.schedulers.ray_scheduler import RayTuneScheduler

    config_space = {
        "int": randint(1, 2),
        "logint": lograndint(3, 4),
        "float": uniform(5.5, 6.5),
        "logfloat": loguniform(7.5, 8.5),
        "categorical": choice(["a", "b", "c"]),
        "normal": randn(2.0, 1.0),
        "const_str": "constant",
    }

    ray_config_space = RayTuneScheduler.convert_config_space(config_space)

    assert set(config_space.keys()) == set(ray_config_space.keys())
    v = ray_config_space["int"]
    # NOTE: In Ray Tune randint(lower, upper), upper is exclusive!
    assert (
        isinstance(v, Integer)
        and isinstance(v.get_sampler(), Integer._Uniform)
        and v.lower == 1
        and v.upper == 3
    )
    v = ray_config_space["logint"]
    assert (
        isinstance(v, Integer)
        and isinstance(v.get_sampler(), Integer._LogUniform)
        and v.lower == 3
        and v.upper == 5
    )
    v = ray_config_space["float"]
    assert (
        isinstance(v, Float)
        and isinstance(v.get_sampler(), Float._Uniform)
        and v.lower == 5.5
        and v.upper == 6.5
    )
    v = ray_config_space["logfloat"]
    assert (
        isinstance(v, Float)
        and isinstance(v.get_sampler(), Float._LogUniform)
        and v.lower == 7.5
        and v.upper == 8.5
    )
    v = ray_config_space["categorical"]
    assert isinstance(v, Categorical) and set(v.categories) == set(
        config_space["categorical"].categories
    )
    v = ray_config_space["normal"]
    assert (
        isinstance(v, Float)
        and isinstance(v.get_sampler(), Float._Normal)
        and v.sampler.mean == 2.0
        and v.sampler.sd == 1.0
    )
    assert ray_config_space["const_str"] == config_space["const_str"]

    for v in config_space.values():
        if hasattr(v, "sample"):
            v.sample()


def test_serialization():
    config_space = [
        randint(1, 2),
        lograndint(3, 4),
        uniform(5.5, 6.5),
        loguniform(7.5, 8.5),
        choice(["a", "b", "c"]),
        randn(2.0, 1.0),
        finrange(0.0, 1.0, 4),
        finrange(0, 6, 4, cast_int=True),
        logfinrange(0.001, 1.0, 4),
        logfinrange(2, 64, 7, cast_int=True),
    ]

    for x in config_space:
        x2 = from_dict(to_dict(x))
        assert type(x) == type(x2)
        if x.sampler is not None:
            assert x.sampler.__dict__ == x2.sampler.__dict__
            assert type(x.sampler) == type(x2.sampler)
        assert {k: v for k, v in x.__dict__.items() if k != "sampler"} == {
            k: v for k, v in x2.__dict__.items() if k != "sampler"
        }


def test_config_space_size():
    upper_limit = 2**20
    config_space = {
        "a": randint(1, 6),
        "b": lograndint(1, 6),
        "c": choice(["a", "b", "c"]),
        "d": "constant",
        "e": 3.1415927,
    }
    cs_size = 6 * 6 * 3
    cases = [
        (config_space, cs_size),
        (dict(config_space, f=uniform(0, 1)), None),
        (dict(config_space, f=loguniform(1, 1)), cs_size),
        (dict(config_space, f=randint(3, 3)), cs_size),
        (dict(config_space, f=choice(["d"])), cs_size),
        (dict(config_space, f=randint(0, upper_limit)), None),
        (dict(config_space, f=lograndint(1, upper_limit / 10)), None),
    ]
    for cs, size in cases:
        _size = config_space_size(cs)
        assert _size == size, f"config_space_size(cs) = {_size} != {size}\n{cs}"


@pytest.mark.parametrize(
    "domain,value_set",
    [
        (finrange(0.1, 1.0, 10), np.arange(0.1, 1.1, 0.1)),
        (finrange(0.5, 1.5, 2), np.array([0.5, 1.5])),
        (logfinrange(np.exp(0.1), np.exp(1.0), 10), np.exp(np.arange(0.1, 1.1, 0.1))),
        (logfinrange(0.0001, 1.0, 5), np.array([0.0001, 0.001, 0.01, 0.1, 1.0])),
        (finrange(0, 8, 5, cast_int=True), np.array([0, 2, 4, 6, 8])),
        (
            logfinrange(8, 512, 7, cast_int=True),
            np.array([8, 16, 32, 64, 128, 256, 512]),
        ),
        (finrange(0.1, 1.0, 1), np.array([0.1])),
    ],
)
def test_finrange_domain(domain, value_set):
    seed = 31415927
    random_state = np.random.RandomState(seed)
    num_samples = 500
    sampled_values = np.array(
        domain.sample(size=num_samples, random_state=random_state)
    ).reshape((-1, 1))
    min_distances = np.min(np.abs(sampled_values - value_set.reshape((1, -1))), axis=1)
    assert np.max(min_distances) < 1e-8


@pytest.mark.parametrize(
    "domain,tp",
    [
        (uniform(0.0, 1.0), float),
        (loguniform(1.0, 10.0), float),
        (randint(0, 10), int),
        (lograndint(1, 10), int),
        (finrange(0.1, 1.0, 10), float),
        (finrange(0.1, 1.0, 1), float),
        (logfinrange(np.exp(0.1), np.exp(1.0), 10), float),
        (finrange(0, 8, 5, cast_int=True), int),
        (logfinrange(8, 512, 7, cast_int=True), int),
    ],
)
def test_type_of_sample(domain, tp):
    num_samples = 5
    seed = 31415927
    random_state = np.random.RandomState(seed)
    value = domain.sample(random_state=random_state)
    assert isinstance(value, tp), domain
    values = domain.sample(random_state=random_state, size=num_samples)
    assert isinstance(values, list) and len(values) == num_samples, domain
    assert all(isinstance(x, tp) for x in values), domain


def test_restrict_domain():
    domain = logfinrange(16, 512, 6, cast_int=True)
    assert domain._values == [16, 32, 64, 128, 256, 512]

    new_domain = restrict_domain(domain, 32, 512)
    print(new_domain)
    assert new_domain._values == [32, 64, 128, 256, 512]

    new_domain = restrict_domain(domain, 32, 128)
    print(new_domain)
    assert new_domain._values == [32, 64, 128]

File Path: tst/test_constrained_bo.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
import pytest

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import uniform


@pytest.mark.skip("this unit test takes about two minutes and is skipped for now")
@pytest.mark.parametrize(
    "scheduler, searcher, constraint_offset",
    [
        ("fifo", "bayesopt", 1.0),  # ignored constraint
        ("fifo", "bayesopt_constrained", 20.0),  # loose constraint
        ("fifo", "bayesopt_constrained", 1.0),  # strict constraint
        ("fifo", "bayesopt_constrained", -10.0),  # infeasible constraint
    ],
)
def test_constrained_bayesopt(scheduler, searcher, constraint_offset):
    num_workers = 2

    config_space = {
        "x1": uniform(-5, 10),
        "x2": uniform(0, 15),
        "constraint_offset": constraint_offset,  # the lower, the stricter
    }

    trial_backend = LocalBackend(
        entry_point=Path(__file__).parent.parent
        / "examples"
        / "training_scripts"
        / "constrained_hpo"
        / "train_constrained_example.py"
    )

    search_options = {
        "num_init_random": num_workers,
        "constraint_attr": "my_constraint_metric",  # Name of the constraint metric captured
        # by the reporter. If not specified, it is assumed that this is named 'constraint_metric' in the reporter
    }
    stop_criterion = StoppingCriterion(max_wallclock_time=28)

    myscheduler = FIFOScheduler(
        config_space,
        searcher=searcher,
        search_options=search_options,
        mode="max",
        metric="objective",
    )

    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=myscheduler,
        stop_criterion=stop_criterion,
        n_workers=num_workers,
    )

    tuner.run()

File Path: tst/test_cost_aware_bo.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
import pytest

from syne_tune.backend import LocalBackend
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune import Tuner, StoppingCriterion
from syne_tune.config_space import uniform


@pytest.mark.skip("this unit test takes about a minute and is skipped for now")
@pytest.mark.parametrize(
    "scheduler, searcher, cost",
    [
        ("fifo", "bayesopt", 1),  # ignored cost
        ("fifo", "bayesopt_cost", 1),  # linear cost
        ("fifo", "bayesopt_cost", 2),  # quadratic cost
    ],
)
def test_cost_aware_bayesopt(scheduler, searcher, cost):
    num_workers = 2

    config_space = {
        "x1": uniform(-5, 10),
        "x2": uniform(0, 15),
        "cost": cost,  # cost_value = x2 ** cost
    }

    trial_backend = LocalBackend(
        entry_point=Path(__file__).parent.parent
        / "examples"
        / "training_scripts"
        / "cost_aware_hpo"
        / "train_cost_aware_example.py"
    )

    search_options = {
        "num_init_random": num_workers,
        "cost_attr": "elapsed_time",  # Name of the cost metric captured by the reporter (mandatory)
    }
    stop_criterion = StoppingCriterion(max_wallclock_time=18)

    myscheduler = FIFOScheduler(
        config_space,
        searcher=searcher,
        search_options=search_options,
        mode="max",
        metric="objective",
    )

    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=myscheduler,
        stop_criterion=stop_criterion,
        n_workers=num_workers,
    )

    tuner.run()

File Path: tst/test_estimators.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from benchmarking.cli.estimator_factory import sagemaker_estimator_factory
from benchmarking.cli.benchmark_factory import benchmark_factory, supported_benchmarks
from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role
from syne_tune.util import repository_root_path


def test_create_estimators():
    try:
        role = get_execution_role()
        for benchmark_name in supported_benchmarks():
            benchmark = benchmark_factory({"benchmark_name": benchmark_name})
            def_params = benchmark["default_params"]
            framework = def_params.get("framework")
            if framework is not None:
                sm_estimator = sagemaker_estimator_factory(
                    entry_point=benchmark["script"],
                    instance_type=def_params["instance_type"],
                    framework=framework,
                    role=role,
                    dependencies=[str(repository_root_path() / "benchmarking")],
                    framework_version=def_params.get("framework_version"),
                    pytorch_version=def_params.get("pytorch_version"),
                )
    except Exception:
        print(
            "Cannot run this test, because SageMaker role is not specified, "
            "and it cannot be inferred"
        )

File Path: tst/test_experiment_path.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import os
from pathlib import Path
from typing import Dict

import pytest

from syne_tune.constants import SYNE_TUNE_DEFAULT_FOLDER, SYNE_TUNE_ENV_FOLDER
from syne_tune.util import experiment_path


@pytest.mark.parametrize(
    "tuner_name, local_path, env, expected_path",
    [
        (
            "my-tuner",
            "/tmp/",
            {"SM_MODEL_DIR": "dummy"},
            "/opt/ml/checkpoints/my-tuner",
        ),
        (None, "/tmp/", {"SM_MODEL_DIR": "dummy"}, "/opt/ml/checkpoints/"),
        ("my-tuner", "/tmp/", {}, "/tmp/my-tuner"),
        (None, "/tmp/", {}, "/tmp/"),
        (
            "my-tuner",
            None,
            {},
            str(Path(f"~/{SYNE_TUNE_DEFAULT_FOLDER}").expanduser() / "my-tuner"),
        ),
        (
            "my-tuner",
            None,
            {},
            str(Path(f"~/{SYNE_TUNE_DEFAULT_FOLDER}/my-tuner").expanduser()),
        ),
        (
            "my-tuner",
            None,
            {SYNE_TUNE_ENV_FOLDER: "/home/foo/bar"},
            "/home/foo/bar/my-tuner",
        ),
    ],
)
def test_experiment_path(
    tuner_name: str, local_path: str, env: Dict, expected_path: str
):
    try:
        env_prev = os.environ.copy()
        os.environ.update(env)
        assert experiment_path(tuner_name=tuner_name, local_path=local_path) == Path(
            expected_path
        )
    finally:
        os.environ = env_prev

File Path: tst/test_instance_info.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.backend.sagemaker_backend.instance_info import (
    select_instance_type,
    InstanceInfos,
)


def test_instance_info():
    instance_infos = InstanceInfos()
    for instance in select_instance_type(max_gpu=0):
        assert instance_infos(instance).num_gpu == 0

    for instance in select_instance_type(min_gpu=1):
        assert instance_infos(instance).num_gpu >= 1

    for instance in select_instance_type(min_cost_per_hour=0.5, max_cost_per_hour=4.0):
        cost = instance_infos(instance).cost_per_hour
        assert 0.5 <= cost <= 4.0

File Path: tst/test_moasha.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from datetime import datetime
from functools import partial

import pytest
import numpy as np

from syne_tune.backend.trial_status import Trial
from syne_tune.optimizer.scheduler import SchedulerDecision
from syne_tune.optimizer.schedulers.multiobjective.moasha import MOASHA, _Bracket
from syne_tune.optimizer.schedulers.multiobjective.multiobjective_priority import (
    FixedObjectivePriority,
    LinearScalarizationPriority,
    NonDominatedPriority,
)
from syne_tune.config_space import randint


def test_bucket():
    b = _Bracket(1, 10, 2, 0)
    assert b.on_result(0, 1, {metric1: 2}) == "CONTINUE"
    assert b.on_result(1, 1, {metric1: 0}) == "CONTINUE"
    assert b.on_result(2, 1, {metric1: 3}) == "STOP"


max_steps = 10

config_space = {
    "steps": max_steps,
    "width": randint(0, 20),
}
time_attr = "step"
metric1 = "mean_loss"
metric2 = "cost"


def make_trial(trial_id: int):
    return Trial(
        trial_id=trial_id,
        config={"steps": 0, "width": trial_id},
        creation_time=datetime.now(),
    )


scheduler_fun = partial(
    MOASHA,
    max_t=max_steps,
    brackets=1,
    reduction_factor=2.0,
    time_attr="step",
    metrics=[metric1, metric2],
    config_space=config_space,
)


@pytest.mark.parametrize(
    "scheduler",
    [
        scheduler_fun(mode="max", multiobjective_priority=FixedObjectivePriority()),
        scheduler_fun(
            mode="max", multiobjective_priority=LinearScalarizationPriority()
        ),
        scheduler_fun(
            mode=["max", "max"], multiobjective_priority=LinearScalarizationPriority()
        ),
        scheduler_fun(mode="max", multiobjective_priority=NonDominatedPriority()),
    ],
)
def test_moasha_mode_max(scheduler):
    np.random.seed(0)
    trial1 = make_trial(trial_id=0)
    trial2 = make_trial(trial_id=1)
    trial3 = make_trial(trial_id=2)

    scheduler.on_trial_add(trial=trial1)
    scheduler.on_trial_add(trial=trial2)
    scheduler.on_trial_add(trial=trial3)

    make_metric = lambda x: {time_attr: 1, metric1: x, metric2: x}
    decision1 = scheduler.on_trial_result(trial1, make_metric(2.0))
    decision2 = scheduler.on_trial_result(trial2, make_metric(4.0))
    decision3 = scheduler.on_trial_result(trial3, make_metric(1.0))
    assert decision1 == SchedulerDecision.CONTINUE
    assert decision2 == SchedulerDecision.CONTINUE
    assert decision3 == SchedulerDecision.STOP


@pytest.mark.parametrize(
    "scheduler",
    [
        scheduler_fun(mode="min", multiobjective_priority=FixedObjectivePriority()),
        scheduler_fun(
            mode="min", multiobjective_priority=LinearScalarizationPriority()
        ),
        scheduler_fun(
            mode=["min", "min"], multiobjective_priority=LinearScalarizationPriority()
        ),
        scheduler_fun(mode="min", multiobjective_priority=NonDominatedPriority()),
    ],
)
def test_moasha_mode_min(scheduler):
    np.random.seed(0)
    trial1 = make_trial(trial_id=0)
    trial2 = make_trial(trial_id=1)
    trial3 = make_trial(trial_id=2)

    scheduler.on_trial_add(trial=trial1)
    scheduler.on_trial_add(trial=trial2)
    scheduler.on_trial_add(trial=trial3)

    make_metric = lambda x: {time_attr: 1, metric1: x, metric2: x}
    decision1 = scheduler.on_trial_result(trial1, make_metric(4.0))
    decision2 = scheduler.on_trial_result(trial2, make_metric(2.0))
    decision3 = scheduler.on_trial_result(trial3, make_metric(10.0))
    assert decision1 == SchedulerDecision.CONTINUE
    assert decision2 == SchedulerDecision.CONTINUE
    assert decision3 == SchedulerDecision.STOP


@pytest.mark.parametrize(
    "mo_priority,expected_priority",
    [
        (LinearScalarizationPriority(), [1.0, 4.0, 7.0, 10.0, 13.0]),
        (
            LinearScalarizationPriority(weights=[0.2, 0.4, 0.8]),
            [
                0.6666666666666666,
                2.066666666666667,
                3.466666666666667,
                4.866666666666667,
                6.266666666666667,
            ],
        ),
        (NonDominatedPriority(), [0, 1, 2, 3, 4]),
        (NonDominatedPriority(dim=1), [0, 1, 2, 3, 4]),
        (NonDominatedPriority(max_num_samples=10), [0, 1, 2, 3, 4]),
    ],
)
def test_multiobjective_priorities(mo_priority, expected_priority):
    num_samples = 5
    num_objectives = 3
    objectives = np.arange(num_samples * num_objectives).reshape(
        (num_samples, num_objectives)
    )

    priorities = mo_priority.__call__(objectives=objectives)
    assert np.allclose(priorities, expected_priority)
    assert priorities.shape == (num_samples,)

File Path: tst/test_pbt.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from datetime import datetime

from syne_tune.backend.trial_status import Trial
from syne_tune.config_space import loguniform
from syne_tune.optimizer.schedulers.pbt import PopulationBasedTraining

max_steps = 10

config_space = {
    "learning_rate": loguniform(1e-3, 1),
}
resource_attr = "step"
metric = "mean_loss"

total_steps = 10
population_size = 2

random_seed = 31415927

pbt = PopulationBasedTraining(
    config_space=config_space,
    metric=metric,
    resource_attr=resource_attr,
    population_size=population_size,
    mode="min",
    max_t=total_steps,
    perturbation_interval=1,
    random_seed=random_seed,
)


def update_state(suggest, state):
    """
    Mock-up of a backend to simulate the backend state after suggesting a new trial
    """

    if suggest.spawn_new_trial_id:
        i = len(state.keys())
        t = Trial(config=suggest.config, trial_id=i, creation_time=datetime.now())
        state[t.trial_id] = {}
        state[t.trial_id]["trial"] = t
        state[t.trial_id]["step"] = 1
        trial_id = t.trial_id
    else:
        trial_id, config = suggest.config
        t = state[trial_id]["trial"]
        t.config = config
        state[trial_id]["trial"] = t
        state[trial_id]["step"] += 1

    return trial_id


def test_ptb():

    state = {}
    for i in range(total_steps):
        suggest = pbt.suggest(i)

        if i < population_size:
            # first configs should be random
            assert suggest.spawn_new_trial_id
            assert suggest.checkpoint_trial_id is None

        else:
            # make sure that we keep config 0 in the population since it's the best
            assert suggest.checkpoint_trial_id == 0

            # do we have a new config
            assert suggest.config != state[0]["trial"].config

        trial_id = update_state(suggest, state)
        t = state[trial_id]["trial"]
        pbt.on_trial_add(t)
        results = {metric: i, resource_attr: state[trial_id]["step"]}

        s = pbt.on_trial_result(t, results)

        if i > 1:
            # all trials after the first one should be stopped and resampled
            assert s == "PAUSE"

    # add better config
    trial_id = update_state(suggest, state)
    t = state[trial_id]["trial"]
    pbt.on_trial_add(t)
    results = {metric: -1, resource_attr: state[trial_id]["step"]}

    s = pbt.on_trial_result(t, results)
    assert s == "CONTINUE"

    # config 0's performance dropped
    t = state[0]["trial"]
    results = {metric: 100, resource_attr: state[trial_id]["step"] + 1}
    s = pbt.on_trial_result(t, results)
    assert s == "PAUSE"

    # we should now continue with config 10
    suggest = pbt.suggest(total_steps)
    assert suggest.checkpoint_trial_id == 10

File Path: tst/test_plateau_stopper.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune import Tuner
from syne_tune.stopping_criterion import PlateauStopper
from syne_tune.config_space import randint
from syne_tune.util import script_height_example_path
from syne_tune.backend.trial_status import Trial, Status
from syne_tune.tuning_status import TuningStatus

from tst.util_test import temporary_local_backend


def test_plateau_scheduler_integration():
    max_steps = 5
    num_workers = 1
    random_seed = 382378624

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
        "sleep_time": 0.001,
    }

    entry_point = str(script_height_example_path())
    metric = "mean_loss"
    mode = "min"

    trial_backend = temporary_local_backend(entry_point=entry_point)

    search_options = {"debug_log": False, "num_init_random": num_workers}

    myscheduler = RandomSearch(
        config_space,
        search_options=search_options,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
        points_to_evaluate=[
            {"width": 10, "height": 0},
            {"width": 7, "height": 0},
            {"width": 6, "height": 0},
        ],
    )

    stop_criterion = PlateauStopper(
        metric=metric, mode=mode, std=0.1, num_trials=3, patience=1
    )
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=myscheduler,
        sleep_time=0.1,
        n_workers=num_workers,
        stop_criterion=stop_criterion,
    )

    tuner.run()

    assert tuner.tuning_status.num_trials_finished == 3


def test_plateau_stopper():
    metric = "loss"

    stop_criterion = PlateauStopper(
        metric=metric, mode="min", std=0.1, num_trials=2, patience=1
    )

    status = TuningStatus(metric_names=[metric])

    trial0 = Trial(trial_id=0, config={"x": 1.0}, creation_time=None)
    trial1 = Trial(trial_id=1, config={"x": 5.0}, creation_time=None)
    status.update(
        trial_status_dict={
            0: (trial0, Status.completed),
            1: (trial1, Status.completed),
        },
        new_results=[
            (0, {metric: 4.0}),
            (1, {metric: 3.0}),
        ],
    )
    assert not stop_criterion(status)

    trial2 = Trial(trial_id=2, config={"x": 4.0}, creation_time=None)
    trial3 = Trial(trial_id=3, config={"x": 6.0}, creation_time=None)
    trial4 = Trial(trial_id=4, config={"x": 7.0}, creation_time=None)

    status.update(
        trial_status_dict={
            2: (trial2, Status.completed),
            3: (trial3, Status.completed),
            4: (trial4, Status.completed),
        },
        new_results=[
            (2, {metric: 3.00000}),
            (3, {metric: 3.000}),
            (4, {metric: 3.0000}),
        ],
    )

    assert stop_criterion(status)

    trial5 = Trial(trial_id=5, config={"x": 10.0}, creation_time=None)

    status.update(
        trial_status_dict={
            5: (trial5, Status.completed),
        },
        new_results=[
            (5, {metric: 1.00000}),
        ],
    )

    assert not stop_criterion(status)

File Path: tst/test_pointstoevaluate.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import numpy as np

from syne_tune.config_space import (
    randint,
    lograndint,
    uniform,
    loguniform,
    choice,
    finrange,
    logfinrange,
)
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.schedulers.searchers import RandomSearcher


def _to_int(a, lower, upper):
    return int(np.clip(round(a), lower, upper))


def _to_float(a, lower, upper):
    return float(np.clip(a, lower, upper))


def _lin_avg(a, b):
    return 0.5 * (b + a)


def _log_avg(a, b):
    return np.exp(0.5 * (np.log(b) + np.log(a)))


def _impute_config(config: Dict) -> Dict:
    new_config = config.copy()
    k, lower, upper = "int", 1, 5
    if k not in config:
        new_config[k] = _to_int(_lin_avg(lower, upper), lower, upper)
    k, lower, upper = "logint", 3, 15
    if k not in config:
        new_config[k] = _to_int(_log_avg(lower, upper), lower, upper)
    k, lower, upper = "float", 5.5, 6.5
    if k not in config:
        new_config[k] = _to_float(_lin_avg(lower, upper), lower, upper)
    k, lower, upper = "logfloat", 7.5, 8.5
    if k not in config:
        new_config[k] = _to_float(_log_avg(lower, upper), lower, upper)
    k = "categorical"
    if k not in config:
        new_config[k] = "a"
    k = "finrange"
    if k not in config:
        new_config[k] = 0.5
    k = "logfinrange"
    if k not in config:
        new_config[k] = np.exp(3.0)
    return new_config


def _gen_testcase(inds, config_pairs):
    a, b = zip(*[config_pairs[i] for i in inds])
    return list(a), list(b)


def _remove_duplicates(inds):
    excl_set = set()
    result = []
    for i in inds:
        if i not in excl_set:
            result.append(i)
            excl_set.add(i)
    return result


def _prepare_for_compare(configs1, configs2, hp_ranges):
    res1, res2 = [], []
    for c1, c2 in zip(configs1, configs2):
        _c1 = hp_ranges.config_to_tuple(c1)
        _c2 = hp_ranges.config_to_tuple(c2)

        def remap(a, b):
            if isinstance(a, str):
                return (1, int(a == b))
            else:
                return (a, b)

        t1, t2 = zip(*[remap(a, b) for a, b in zip(_c1, _c2)])
        res1.extend(t1)
        res2.extend(t2)
    return res1, res2


def _prepare_test(is_ray_tune=False):
    np.random.seed(2838748673)
    num_extra_cases = 30

    config_space = {
        "int": randint(1, 5),
        "logint": lograndint(3, 15),
        "float": uniform(5.5, 6.5),
        "logfloat": loguniform(7.5, 8.5),
        "categorical": choice(["a", "b", "c"]),
    }
    configs = [
        dict(),
        {"float": 5.75, "categorical": "b"},
        {"int": 1, "logint": 3, "float": 5.5, "logfloat": 7.5, "categorical": "a"},
        {"int": 5, "logint": 15, "float": 6.5, "logfloat": 8.5, "categorical": "c"},
        {"logfloat": 8.125, "logint": 5, "int": 4},
        {"float": 6.125},
        {"categorical": "c"},
    ]
    if not is_ray_tune:
        config_space.update(
            {
                "finrange": finrange(0.1, 0.9, 9),
                "logfinrange": logfinrange(1.0, np.exp(6.0), 7),
            }
        )
        configs.extend(
            [
                {"finrange": 0.3},
                {"logfinrange": np.exp(2.0)},
            ]
        )

    num_configs = len(configs)
    config_pairs = [(c, _impute_config(c)) for c in configs]
    # for a, b in config_pairs:
    #    print(f"{a} --- {b}")
    testcases = [(None, [config_pairs[0][1]]), ([], [])] + [
        _gen_testcase([i], config_pairs) for i in range(num_configs)
    ]
    testcases.append(
        (
            _gen_testcase([0, 2, 4, 1, 4, 2], config_pairs)[0],
            _gen_testcase([0, 2, 4, 1], config_pairs)[1],
        )
    )
    for i in range(num_extra_cases):
        size = np.random.randint(1, 10)
        inds = np.random.randint(0, num_configs, size=size)
        tc_src, _ = _gen_testcase(inds, config_pairs)
        _, tc_trg = _gen_testcase(_remove_duplicates(inds), config_pairs)
        testcases.append((tc_src, tc_trg))

    return config_space, configs, testcases


def test_points_to_evaluate():
    config_space, configs, testcases = _prepare_test()
    search_options = {"debug_log": False}
    for tc_src, tc_trg in testcases:
        err_msg = f"tc_src = {tc_src}\ntc_trg = {tc_trg}"
        scheduler = FIFOScheduler(
            config_space,
            searcher="random",
            search_options=search_options,
            mode="min",
            metric="bogus",
            points_to_evaluate=tc_src,
        )
        searcher: RandomSearcher = scheduler.searcher
        hp_ranges = searcher._hp_ranges
        assert len(tc_trg) == len(searcher._points_to_evaluate), err_msg
        assert np.allclose(
            *_prepare_for_compare(tc_trg, searcher._points_to_evaluate, hp_ranges)
        ), err_msg
        tc_cmp = [scheduler.suggest(trial_id=i).config for i in range(len(tc_trg))]
        assert len(tc_trg) == len(tc_cmp), err_msg
        assert np.allclose(*_prepare_for_compare(tc_trg, tc_cmp, hp_ranges)), err_msg


def test_points_to_evaluate_raytune():
    from ray.tune.schedulers import FIFOScheduler as RT_FIFOScheduler
    from syne_tune.optimizer.schedulers.ray_scheduler import RayTuneScheduler
    from syne_tune.optimizer.schedulers.searchers import impute_points_to_evaluate

    config_space, configs, testcases = _prepare_test(is_ray_tune=True)
    # This is just to get hp_ranges, which is needed for comparisons below
    _myscheduler = FIFOScheduler(
        config_space, searcher="random", mode="min", metric="bogus"
    )
    _mysearcher: RandomSearcher = _myscheduler.searcher
    hp_ranges = _mysearcher._hp_ranges
    for tc_src, tc_trg in testcases:
        err_msg = f"tc_src = {tc_src}\ntc_trg = {tc_trg}"
        ray_scheduler = RT_FIFOScheduler()
        ray_scheduler.set_search_properties(mode="min", metric="bogus")
        scheduler = RayTuneScheduler(
            config_space=config_space,
            ray_scheduler=ray_scheduler,
            points_to_evaluate=impute_points_to_evaluate(tc_src, config_space),
        )
        tc_cmp = [scheduler.suggest(trial_id=i).config for i in range(len(tc_trg))]
        assert len(tc_trg) == len(tc_cmp), err_msg
        assert np.allclose(*_prepare_for_compare(tc_trg, tc_cmp, hp_ranges)), err_msg

File Path: tst/test_random_seed.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import Dict
import pytest
import itertools
import logging

from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune import Tuner
from syne_tune.tuner_callback import TunerCallback
from syne_tune.backend.trial_status import Trial
from syne_tune.config_space import randint
from syne_tune.util import script_height_example_path
from tst.util_test import temporary_local_backend


class StoreConfigCallback(TunerCallback):
    def __init__(self):
        super().__init__()
        self._keys = ["height", "width"]  # Only these change
        self.configs = []
        self._configs_set = set()

    def _reduce(self, config):
        return {k: config[k] for k in self._keys}

    def _to_tuple(self, config):
        return tuple(config[k] for k in self._keys)

    def on_trial_result(self, trial: Trial, status: str, result: Dict, decision: str):
        config = self._reduce(trial.config)
        key = self._to_tuple(config)
        if key not in self._configs_set:
            self.configs.append(config)
            self._configs_set.add(key)


_parameterizations = list(
    itertools.product(
        ["fifo", "hyperband_stopping", "hyperband_promotion"], [382378624]
    )
)


@pytest.mark.parametrize("scheduler, random_seed", _parameterizations)
def test_scheduler(scheduler, random_seed):
    max_steps = 5
    # Note: For num_workers > 1, the ordering of configs may change even if the
    # random seed stays the same
    num_workers = 1

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
        "sleep_time": 0.01,
    }

    entry_point = script_height_example_path()
    metric = "mean_loss"
    mode = "min"

    trial_backend = temporary_local_backend(entry_point=entry_point)

    search_options = {"debug_log": False}
    kwargs = dict(
        searcher="random",
        search_options=search_options,
        mode=mode,
        metric=metric,
        random_seed=random_seed,
    )
    if scheduler == "fifo":
        myscheduler1 = FIFOScheduler(config_space, **kwargs)
        myscheduler2 = FIFOScheduler(config_space, **kwargs)
    else:
        prefix = "hyperband_"
        assert scheduler.startswith(prefix)
        sch_type = scheduler[len(prefix) :]
        kwargs = dict(kwargs, max_t=max_steps, type=sch_type, resource_attr="epoch")
        myscheduler1 = HyperbandScheduler(config_space, **kwargs)
        myscheduler2 = HyperbandScheduler(config_space, **kwargs)

    logging.getLogger("syne_tune.tuner").setLevel(logging.ERROR)

    stop_criterion = lambda status: status.wallclock_time > 0.5
    callback1 = StoreConfigCallback()
    tuner1 = Tuner(
        trial_backend=trial_backend,
        scheduler=myscheduler1,
        sleep_time=0.02,
        n_workers=num_workers,
        stop_criterion=stop_criterion,
        callbacks=[callback1],
    )
    tuner1.run()

    trial_backend = temporary_local_backend(entry_point=entry_point)
    callback2 = StoreConfigCallback()
    tuner2 = Tuner(
        trial_backend=trial_backend,
        scheduler=myscheduler2,
        sleep_time=0.02,
        n_workers=num_workers,
        stop_criterion=stop_criterion,
        callbacks=[callback2],
    )
    tuner2.run()
    configs1 = callback1.configs
    configs2 = callback2.configs
    len1 = len(configs1)
    len2 = len(configs2)
    if len1 != len2:
        # Different lengths can happen even for the same random seed
        print(
            f"scheduler = {scheduler}, random_seed = {random_seed}: "
            f"len1 = {len1}, len2 = {len2}"
        )
        clen = min(len1, len2)
        configs1 = configs1[:clen]
        configs2 = configs2[:clen]
    if configs1 != configs2:
        parts = [
            f"scheduler = {scheduler}, random_seed = {random_seed}",
            f"configs1[{len1}] --- configs2[{len2}]",
        ]
        for i, (c1, c2) in enumerate(zip(configs1, configs2)):
            parts.append(f"{i}: {c1 == c2}: {c1} --- {c2}")
        raise AssertionError("\n".join(parts))

File Path: tst/test_report.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging

from syne_tune import Reporter
from syne_tune.report import retrieve


def test_report_logger():
    report = Reporter()

    logging.getLogger().setLevel(logging.INFO)

    report(train_nll=1.45, time=1.0, step=2)
    report(train_nll=1.2, time=2.0, step=3)

    lines = [
        '[tune-metric]: {"train_nll": 1.45, "time": 1.0, "step": 2}\n',
        '[tune-metric]: {"train_nll": 1.2, "time": 2.0, "step": 3}\n',
    ]
    metrics = retrieve(log_lines=lines)
    print(metrics)
    assert metrics == [
        {"train_nll": 1.45, "time": 1.0, "step": 2},
        {"train_nll": 1.2, "time": 2.0, "step": 3},
    ]

File Path: tst/test_schedulers.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import pytest
import itertools

from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.schedulers.synchronous.hyperband_impl import (
    SynchronousGeometricHyperbandScheduler,
    GeometricDifferentialEvolutionHyperbandScheduler,
)
from syne_tune import Tuner
from syne_tune import StoppingCriterion
from syne_tune.config_space import randint, choice
from syne_tune.util import script_checkpoint_example_path
from tst.util_test import temporary_local_backend


_async_parameterizations = list(
    itertools.product(
        ["fifo", "hyperband_stopping", "hyperband_promotion"],
        ["random", "bayesopt", "grid"],
        ["min", "max"],
    )
)


@pytest.mark.parametrize("scheduler, searcher, mode", _async_parameterizations)
def test_async_scheduler(scheduler, searcher, mode):
    max_steps = 5
    num_workers = 2
    random_seed = 382378624

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
        "sleep_time": 0.001,
    }
    # GridSearcher only support Categorical parameters
    if searcher == "grid":
        config_space["width"] = choice([1, 2, 3, 4, 5])
        config_space["height"] = choice([-3, -2, -1, 0, 1, 2, 3])

    entry_point = str(script_checkpoint_example_path())
    metric = "mean_loss"

    trial_backend = temporary_local_backend(entry_point=entry_point)

    search_options = {"debug_log": False, "num_init_random": num_workers}

    if scheduler == "fifo":
        myscheduler = FIFOScheduler(
            config_space,
            searcher=searcher,
            search_options=search_options,
            mode=mode,
            metric=metric,
            random_seed=random_seed,
        )
    else:
        prefix = "hyperband_"
        assert scheduler.startswith(prefix)
        sch_type = scheduler[len(prefix) :]
        myscheduler = HyperbandScheduler(
            config_space,
            searcher=searcher,
            search_options=search_options,
            max_t=max_steps,
            type=sch_type,
            resource_attr="epoch",
            random_seed=random_seed,
            mode=mode,
            metric=metric,
        )

    stop_criterion = StoppingCriterion(max_wallclock_time=0.2)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=myscheduler,
        sleep_time=0.1,
        n_workers=num_workers,
        stop_criterion=stop_criterion,
    )

    tuner.run()


_sync_parameterizations = [
    [SynchronousGeometricHyperbandScheduler, "random", "min"],
    [SynchronousGeometricHyperbandScheduler, "random", "max"],
    [SynchronousGeometricHyperbandScheduler, "bayesopt", "min"],
    [SynchronousGeometricHyperbandScheduler, "bayesopt", "max"],
    [SynchronousGeometricHyperbandScheduler, "grid", "min"],
    [SynchronousGeometricHyperbandScheduler, "grid", "max"],
    [GeometricDifferentialEvolutionHyperbandScheduler, "random", "min"],
    [GeometricDifferentialEvolutionHyperbandScheduler, "random_encoded", "max"],
    [GeometricDifferentialEvolutionHyperbandScheduler, "random", "min"],
    [GeometricDifferentialEvolutionHyperbandScheduler, "random_encoded", "max"],
]


@pytest.mark.parametrize("scheduler_cls, searcher, mode", _sync_parameterizations)
def test_sync_scheduler(scheduler_cls, searcher, mode):
    max_steps = 5
    num_workers = 2
    random_seed = 382378624

    config_space = {
        "steps": max_steps,
        "width": randint(0, 20),
        "height": randint(-100, 100),
        "sleep_time": 0.001,
    }
    # GridSearcher only support Categorical parameters
    if searcher == "grid":
        config_space["width"] = choice([1, 2, 3, 4, 5])
        config_space["height"] = choice([-3, -2, -1, 0, 1, 2, 3])

    entry_point = str(script_checkpoint_example_path())
    metric = "mean_loss"

    trial_backend = temporary_local_backend(entry_point=entry_point)

    search_options = {"debug_log": False, "num_init_random": num_workers}

    scheduler_kwargs = dict(
        searcher=searcher,
        search_options=search_options,
        mode=mode,
        metric=metric,
        resource_attr="epoch",
        max_resource_attr="steps",
        random_seed=random_seed,
    )
    myscheduler = scheduler_cls(config_space, **scheduler_kwargs)

    stop_criterion = StoppingCriterion(max_wallclock_time=0.2)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=myscheduler,
        sleep_time=0.1,
        n_workers=num_workers,
        stop_criterion=stop_criterion,
    )

    tuner.run()

File Path: tst/test_status.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from syne_tune.backend.trial_status import Trial, Status
from syne_tune.tuning_status import TuningStatus, print_best_metric_found


def test_status():
    metric_names = ["NLL", "time"]
    status = TuningStatus(metric_names=metric_names)

    trial0 = Trial(trial_id=0, config={"x": 1.0}, creation_time=None)
    trial1 = Trial(trial_id=1, config={"x": 5.0}, creation_time=None)
    status.update(
        trial_status_dict={
            0: (trial0, Status.in_progress),
            1: (trial1, Status.in_progress),
        },
        new_results=[
            (0, {"NLL": 2.0, "time": 10.0, "debug": "str"}),
            (0, {"NLL": 1.0, "time": 12.0, "debug": "str"}),
            (1, {"NLL": 3.0, "time": 5.0, "debug": "str"}),
        ],
    )
    assert status.overall_metric_statistics.max_metrics
    assert status.num_trials_started == 2
    assert status.overall_metric_statistics.max_metrics == {"NLL": 3.0, "time": 12.0}
    assert status.overall_metric_statistics.min_metrics == {"NLL": 1.0, "time": 5.0}
    assert status.overall_metric_statistics.sum_metrics == {"NLL": 6.0, "time": 27.0}

    assert status.trial_metric_statistics[0].max_metrics == {"NLL": 2.0, "time": 12.0}
    assert status.trial_metric_statistics[0].min_metrics == {"NLL": 1.0, "time": 10.0}
    assert status.trial_metric_statistics[0].sum_metrics == {"NLL": 3.0, "time": 22.0}

    status.update(
        trial_status_dict={
            0: (trial0, Status.in_progress),
        },
        new_results=[
            (0, {"NLL": 0.0, "time": 20.0}),
        ],
    )
    assert status.trial_metric_statistics[0].max_metrics == {"NLL": 2.0, "time": 20.0}
    assert status.trial_metric_statistics[0].min_metrics == {"NLL": 0.0, "time": 10.0}
    assert status.trial_metric_statistics[0].sum_metrics == {"NLL": 3.0, "time": 42.0}
    assert status.trial_metric_statistics[0].last_metrics == {"NLL": 0.0, "time": 20.0}

    print(str(status))

    best_trialid, best_metric = print_best_metric_found(
        tuning_status=status,
        metric_names=metric_names,
        mode="min",
    )
    assert best_trialid == 0
    assert best_metric == 0.0

    best_trialid, best_metric = print_best_metric_found(
        tuning_status=status,
        metric_names=metric_names,
        mode="max",
    )
    assert best_trialid == 1
    assert best_metric == 3.0


def test_stats_are_not_tracked_for_non_numeric_metrics():
    metric_names = ["metric1", "metric2"]
    status = TuningStatus(metric_names=metric_names)

    trial0 = Trial(trial_id=0, config={"x": 1.0}, creation_time=None)
    status.update(
        trial_status_dict={
            0: (trial0, Status.in_progress),
        },
        new_results=[
            (0, {metric_names[0]: 2.0, metric_names[1]: "str"}),
        ],
    )
    assert status.trial_metric_statistics[0].max_metrics == {metric_names[0]: 2.0}
    assert status.trial_metric_statistics[0].min_metrics == {metric_names[0]: 2.0}
    assert status.trial_metric_statistics[0].sum_metrics == {metric_names[0]: 2.0}
    assert status.trial_metric_statistics[0].last_metrics == {
        metric_names[0]: 2.0,
        metric_names[1]: "str",
    }

    status.update(
        trial_status_dict={
            0: (trial0, Status.in_progress),
        },
        new_results=[
            (0, {metric_names[0]: "str", metric_names[1]: 20}),
        ],
    )

    assert status.trial_metric_statistics[0].max_metrics == {metric_names[0]: 2.0}
    assert status.trial_metric_statistics[0].min_metrics == {metric_names[0]: 2.0}
    assert status.trial_metric_statistics[0].sum_metrics == {metric_names[0]: 2.0}
    assert status.trial_metric_statistics[0].last_metrics == {
        metric_names[0]: "str",
        metric_names[1]: 20,
    }

File Path: tst/trial_backend/checkpoint_sagemaker/checkpoint_script.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Script for testing copying of checkpoints in SageMaker backend.
"""

import argparse
import logging
import os
import json
from pathlib import Path
from typing import Optional

from syne_tune.constants import ST_CHECKPOINT_DIR
from syne_tune import Reporter


def load_checkpoint(checkpoint_path: Path) -> Optional[dict]:
    result = dict()
    if checkpoint_path.exists():
        with open(checkpoint_path, "r") as f:
            result = json.load(f)
    return result


def save_checkpoint(checkpoint_path: Path, content: dict):
    os.makedirs(checkpoint_path.parent, exist_ok=True)
    with open(checkpoint_path, "w") as f:
        json.dump(content, f)


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--trial_id", type=int, required=True)
    parser.add_argument("--value1", type=int, required=True)
    parser.add_argument("--value2", type=int, required=True)

    # convention the path where to serialize and deserialize is given as checkpoint-dir
    parser.add_argument(f"--{ST_CHECKPOINT_DIR}", type=str, default="./")

    args, _ = parser.parse_known_args()

    report = Reporter()
    error_msg = None

    # Try to load checkpoint (may not be present)
    checkpoint1_path = Path(getattr(args, ST_CHECKPOINT_DIR)) / "checkpoint1.json"
    checkpoint2_path = (
        Path(getattr(args, ST_CHECKPOINT_DIR)) / "subdir" / "checkpoint2.json"
    )
    old_checkpoint1 = load_checkpoint(checkpoint1_path)
    old_checkpoint2 = dict()
    if old_checkpoint1:
        root.info(f"Loaded checkpoint {checkpoint1_path}:\n{old_checkpoint1}")
        old_checkpoint2 = load_checkpoint(checkpoint2_path)
        if not old_checkpoint2:
            error_msg = (
                f"Found checkpoint at {checkpoint1_path}, but not at {checkpoint2_path}"
            )
        else:
            root.info(f"Loaded checkpoint {checkpoint2_path}:\n{old_checkpoint2}")

    if error_msg is None:
        # Write new checkpoints
        checkpoint1 = {
            "trial_id": args.trial_id,
            "value1": args.value1,
        }
        for k, v in old_checkpoint1.items():
            checkpoint1["parent_" + k] = v
        save_checkpoint(checkpoint1_path, checkpoint1)
        root.info(f"Wrote checkpoint {checkpoint1_path}:\n{checkpoint1}")
        checkpoint2 = {
            "trial_id": args.trial_id,
            "value2": args.value2,
        }
        for k, v in old_checkpoint2.items():
            checkpoint2["parent_" + k] = v
        save_checkpoint(checkpoint2_path, checkpoint2)
        root.info(f"Wrote checkpoint {checkpoint2_path}:\n{checkpoint2}")
        result = dict(trial_id=args.trial_id, value1=args.value1, value2=args.value2)
        for old_cp, prefix in [
            (old_checkpoint1, "parent1_"),
            (old_checkpoint2, "parent2_"),
        ]:
            for k, v in old_cp.items():
                result[prefix + k] = v
        root.info(f"Report back:\n{result}")
        report(**result)
    else:
        report(error_msg=error_msg)

File Path: tst/trial_backend/checkpoint_sagemaker/test_checkpoint_sagemaker.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from pathlib import Path
import logging
from typing import Dict, Optional, List
import copy
import pytest

from sagemaker.pytorch import PyTorch

from syne_tune.backend import SageMakerBackend
from syne_tune.backend.sagemaker_backend.sagemaker_utils import (
    get_execution_role,
    default_sagemaker_session,
)
from syne_tune.optimizer.scheduler import TrialScheduler, TrialSuggestion, Trial
from syne_tune.config_space import randint
from syne_tune import StoppingCriterion, Tuner

logger = logging.getLogger(__name__)


class TestCopyCheckpointScheduler(TrialScheduler):
    """
    Scheduler for `test_copy_checkpoint_sagemaker_backend`.

    """

    def __init__(self, config_space: Dict):
        super().__init__(config_space)
        self.result_for_trial = dict()

    def _suggest(self, trial_id: int) -> Optional[TrialSuggestion]:
        config = {
            "trial_id": trial_id,
            "value1": trial_id + 1,
            "value2": 2 * (trial_id + 1),
        }
        if trial_id == 1:
            # Start new trial from checkpoint of trial_id 0 (this requires
            # copying the checkpoint)
            return TrialSuggestion.start_suggestion(
                config=config, checkpoint_trial_id=0
            )
        else:
            # Start new trial from scratch
            return TrialSuggestion.start_suggestion(config=config)

    def on_trial_complete(self, trial: Trial, result: Dict):
        trial_id = trial.trial_id
        logger.info(f"on_trial_complete (trial_id = {trial_id}). Received:\n{result}")
        self.result_for_trial[trial_id] = copy.copy(result)

    def metric_names(self) -> List[str]:
        return ["value1"]


@pytest.mark.skip("this test needs sagemaker and runs for >10 minutes")
def test_copy_checkpoint_sagemaker_backend():
    logging.getLogger().setLevel(logging.INFO)
    # Create SageMaker backend
    entry_point = Path(__file__).parent / "checkpoint_script.py"
    trial_backend = SageMakerBackend(
        sm_estimator=PyTorch(
            entry_point=str(entry_point),
            instance_type="ml.m5.large",
            instance_count=1,
            role=get_execution_role(),
            max_run=10 * 60,
            framework_version="1.7.1",
            py_version="py3",
            sagemaker_session=default_sagemaker_session(),
        )
    )

    config_space = {
        "trial_id": randint(0, 9),
        "value1": randint(0, 20),
        "value2": randint(0, 20),
    }
    test_scheduler = TestCopyCheckpointScheduler(config_space)

    stop_criterion = StoppingCriterion(max_num_trials_completed=1)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=test_scheduler,
        stop_criterion=stop_criterion,
        n_workers=1,
        tuner_name="test-copy-checkpoint",
        callbacks=[],
    )

    logger.info(f"Starting tuning: {tuner.name}")
    tuner.run()
    logger.info("Done tuning. Checking results")

    result_for_trial = test_scheduler.result_for_trial
    logger.info(f"result_for_trial:\n{result_for_trial}")
    # Normal, no checkpoint copied from parent
    assert 0 in result_for_trial
    result0 = result_for_trial[0]
    assert "error_msg" not in result0, result0["error_msg"]
    assert result0["trial_id"] == 0
    assert result0["value1"] == 1
    assert result0["value2"] == 2
    assert "parent1_trial_id" not in result0
    assert "parent2_trial_id" not in result0
    # Checkpoint copied from parent trial_id=0
    assert 1 in result_for_trial
    result1 = result_for_trial[1]
    assert "error_msg" not in result1, result1["error_msg"]
    assert result1["trial_id"] == 1
    assert result1["value1"] == 2
    assert result1["value2"] == 4
    assert result1["parent1_trial_id"] == 0
    assert result1["parent1_value1"] == 1
    assert result1["parent2_trial_id"] == 0
    assert result1["parent2_value2"] == 2

File Path: tst/trial_backend/main_checkpoint.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
Script used for testing checkpointing.
The main reports "nothing" if the checkpoint folder is empty and writes the name given in argument to the checkpoint.
If a checkpoint is present, it reports the content of the checkpointing folder.
"""

import argparse
import logging
import os
from pathlib import Path

from syne_tune.constants import ST_CHECKPOINT_DIR
from syne_tune import Reporter


def load_checkpoint(checkpoint_path: Path):
    with open(checkpoint_path, "r") as f:
        return f.readline()


def save_checkpoint(checkpoint_path: Path, content: str):
    with open(checkpoint_path, "w") as f:
        f.write(content)


if __name__ == "__main__":
    root = logging.getLogger()
    root.setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--name", type=str, required=True)

    # convention the path where to serialize and deserialize is given as checkpoint-dir
    parser.add_argument(f"--{ST_CHECKPOINT_DIR}", type=str, default="./")

    args, _ = parser.parse_known_args()

    checkpoint_path = Path(getattr(args, ST_CHECKPOINT_DIR)) / "checkpoint.txt"
    os.makedirs(checkpoint_path.parent, exist_ok=True)

    if checkpoint_path.exists():
        checkpoint_content = load_checkpoint(checkpoint_path)
    else:
        checkpoint_content = "nothing"

    report = Reporter()
    report(checkpoint_content=checkpoint_content)

    save_checkpoint(checkpoint_path, args.name)

File Path: tst/trial_backend/test_fetch_results.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
from typing import List, Dict, Optional

from syne_tune.backend.trial_backend import TrialBackend
from syne_tune.backend.trial_status import TrialResult, Status
from syne_tune.constants import ST_WORKER_TIMESTAMP


class DeterministicBackend(TrialBackend):
    def copy_checkpoint(self, src_trial_id: int, tgt_trial_id: int):
        pass

    # a backend which returns deterministic status and metrics to test the logic of the tuner
    def __init__(self):
        super(DeterministicBackend, self).__init__()
        self.iteration = 0
        self.trialid_to_results = {}
        self.timestamp = 0

    def generate_event(
        self, trial_id: int, metrics: List[Dict], status: Optional[str] = None
    ):
        for m in metrics:
            m[ST_WORKER_TIMESTAMP] = self.timestamp
            self.timestamp += 1

        if trial_id in self.trialid_to_results:
            old_trial_result = self.trialid_to_results[trial_id]
            old_trial_result.status = status
            old_trial_result.metrics += metrics
        else:
            self.trialid_to_results[trial_id] = TrialResult(
                trial_id=trial_id,
                status=status,
                metrics=metrics,
                config=None,
                creation_time=None,
                training_end_time=None,
            )

    def _all_trial_results(self, trial_ids: List[int]) -> List[TrialResult]:
        return [self.trialid_to_results[trial_id] for trial_id in trial_ids]

    def _resume_trial(self, trial_id: int):
        pass

    def _pause_trial(self, trial_id: int, result: dict):
        pass

    def _stop_trial(self, trial_id: int, result: dict):
        pass

    def _schedule(self, trial_id: int, config: Dict):
        pass

    def stdout(self, trial_id: int) -> List[str]:
        return []

    def stderr(self, trial_id: int) -> List[str]:
        return []


def test_dummybackend():
    trial_ids = [3, 7]
    # check that the dummy backend behaves as expected, when we call status 2 observations are created
    backend = DeterministicBackend()
    backend.generate_event(trial_id=3, metrics=[{"metric": 6}])
    backend.generate_event(trial_id=7, metrics=[{"metric": 14}])
    metrics = [trial.metrics for trial in backend._all_trial_results(trial_ids)]
    assert metrics == [
        [{"metric": 6, ST_WORKER_TIMESTAMP: 0}],
        [{"metric": 14, ST_WORKER_TIMESTAMP: 1}],
    ]

    backend.generate_event(trial_id=3, metrics=[{"metric": 6}])
    backend.generate_event(trial_id=7, metrics=[{"metric": 14}])
    metrics = [trial.metrics for trial in backend._all_trial_results(trial_ids)]
    assert metrics == [
        [{"metric": 6, ST_WORKER_TIMESTAMP: 0}, {"metric": 6, ST_WORKER_TIMESTAMP: 2}],
        [
            {"metric": 14, ST_WORKER_TIMESTAMP: 1},
            {"metric": 14, ST_WORKER_TIMESTAMP: 3},
        ],
    ]


def test_fetch_results_metrics():
    # now check that we only get new metrics when we call fetch_results
    trial_ids = [3, 7]
    backend = DeterministicBackend()
    backend.generate_event(trial_id=3, metrics=[])
    backend.generate_event(trial_id=7, metrics=[])
    _, metrics = backend.fetch_status_results(trial_ids)
    assert metrics == []

    backend.generate_event(trial_id=3, metrics=[{"metric": 6, ST_WORKER_TIMESTAMP: 0}])
    backend.generate_event(trial_id=7, metrics=[{"metric": 14}])
    _, metrics = backend.fetch_status_results(trial_ids)
    assert metrics == [
        (3, {"metric": 6, ST_WORKER_TIMESTAMP: 0}),
        (7, {"metric": 14, ST_WORKER_TIMESTAMP: 1}),
    ]

    _, metrics = backend.fetch_status_results(trial_ids)
    assert metrics == []

    backend.generate_event(trial_id=3, metrics=[{"metric": 6}])
    backend.generate_event(trial_id=7, metrics=[{"metric": 14}])
    _, metrics = backend.fetch_status_results(trial_ids)
    assert metrics == [
        (3, {"metric": 6, ST_WORKER_TIMESTAMP: 2}),
        (7, {"metric": 14, ST_WORKER_TIMESTAMP: 3}),
    ]

    _, metrics = backend.fetch_status_results(trial_ids)
    assert metrics == []

    for i in range(2):
        backend.generate_event(trial_id=3, metrics=[{"metric": 6}])
        backend.generate_event(trial_id=7, metrics=[{"metric": 14}])

    _, metrics = backend.fetch_status_results(trial_ids)
    assert metrics == [
        (3, {"metric": 6, ST_WORKER_TIMESTAMP: 4}),
        (7, {"metric": 14, ST_WORKER_TIMESTAMP: 5}),
        (3, {"metric": 6, ST_WORKER_TIMESTAMP: 6}),
        (7, {"metric": 14, ST_WORKER_TIMESTAMP: 7}),
    ]

    _, metrics = backend.fetch_status_results(trial_ids)
    assert metrics == []


def get_status_metrics(backend, trial_ids):
    trial_status_dict, new_metrics = backend.fetch_status_results(trial_ids)
    trial_statuses = {
        trial_id: status for (trial_id, (_, status)) in trial_status_dict.items()
    }
    return trial_statuses, new_metrics


def test_fetch_results_status():
    # now check that we get the expected status when fetching_results
    trial_ids = [3, 7]
    backend = DeterministicBackend()

    backend.generate_event(
        trial_id=3, metrics=[{"metric": 6}], status=Status.in_progress
    )
    backend.generate_event(
        trial_id=7, metrics=[{"metric": 14}], status=Status.in_progress
    )

    trial_statuses, metrics = get_status_metrics(backend, trial_ids)
    assert metrics == [
        (3, {"metric": 6, "st_worker_timestamp": 0}),
        (7, {"metric": 14, "st_worker_timestamp": 1}),
    ]
    assert trial_statuses == {3: Status.in_progress, 7: Status.in_progress}

    # check that status gets updated to failed
    backend.generate_event(trial_id=7, metrics=[], status=Status.failed)
    trial_statuses, metrics = get_status_metrics(backend, trial_ids)

    assert metrics == []
    assert trial_statuses == {3: Status.in_progress, 7: Status.failed}

    # check that in case the trial completed but metrics are still to be seen, the status is in_progresss
    backend.generate_event(
        trial_id=3, metrics=[{"metric": 6}, {"metric": 6}], status=Status.completed
    )
    #     v
    # 6 6 6
    trial_statuses, metrics = get_status_metrics(backend, trial_ids)

    assert metrics == [
        (3, {"metric": 6, "st_worker_timestamp": 2}),
        (3, {"metric": 6, "st_worker_timestamp": 3}),
    ]
    assert trial_statuses == {3: Status.completed, 7: Status.failed}

    #       v
    # 6 6 6
    trial_statuses, metrics = get_status_metrics(backend, trial_ids)

    assert metrics == []
    assert trial_statuses == {3: Status.completed, 7: Status.failed}

    #       v
    # 6 6 6
    trial_statuses, metrics = get_status_metrics(backend, trial_ids)

    assert metrics == []
    assert trial_statuses == {3: Status.completed, 7: Status.failed}

File Path: tst/trial_backend/test_local_trial_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import logging
from pathlib import Path

from syne_tune.backend.trial_status import Status
from syne_tune.util import script_checkpoint_example_path
from tst.util_test import temporary_local_backend, wait_until_all_trials_completed


def check_metrics(metrics_observed, metrics_expected):
    assert len(metrics_observed) == len(metrics_expected)
    for (trial_id1, result1), (trial_id2, result2) in zip(
        metrics_observed, metrics_expected
    ):
        assert trial_id1 == trial_id2
        for key in ["step", "train_acc"]:
            assert result1[key] == result2[key]


def status(backend, trial_ids):
    return [trial.status for trial in backend._all_trial_results(trial_ids)]


def get_status_metrics(backend, trial_id):
    trial_status_dict, new_metrics = backend.fetch_status_results([trial_id])
    trial_statuses = {
        trial_id: status for (trial_id, (_, status)) in trial_status_dict.items()
    }
    return trial_statuses, new_metrics


def test_local_backend_checkpoint(caplog):
    caplog.set_level(logging.INFO)
    path_script = script_checkpoint_example_path()
    backend = temporary_local_backend(entry_point=path_script)
    trial_id = backend.start_trial(config={"num-epochs": 2}).trial_id
    wait_until_all_trials_completed(backend)

    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert trial_statuses == {trial_id: Status.completed}
    check_metrics(
        new_metrics,
        [
            (trial_id, {"step": 0, "train_acc": 1}),
            (trial_id, {"step": 1, "train_acc": 2}),
        ],
    )

    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    check_metrics(new_metrics, [])

    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert new_metrics == []

    backend.pause_trial(trial_id=trial_id)
    backend.resume_trial(trial_id=trial_id)

    wait_until_all_trials_completed(backend)
    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert trial_statuses == {trial_id: Status.completed}
    check_metrics(new_metrics, [])

    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert new_metrics == []

    trial_id = backend.start_trial(config={"num-epochs": 200}).trial_id
    backend.stop_trial(trial_id=trial_id)

    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert trial_statuses == {trial_id: Status.stopped}

    trial_id = backend.start_trial(config={"num-epochs": 200}).trial_id
    backend.pause_trial(trial_id=trial_id)
    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert trial_statuses == {trial_id: Status.paused}


def test_resume_config_local_backend(caplog):
    caplog.set_level(logging.INFO)
    path_script = script_checkpoint_example_path()
    backend = temporary_local_backend(entry_point=path_script)
    trial_id = backend.start_trial(config={"num-epochs": 2}).trial_id

    wait_until_all_trials_completed(backend)

    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert trial_statuses == {trial_id: Status.completed}
    check_metrics(
        new_metrics,
        [
            (trial_id, {"step": 0, "train_acc": 1}),
            (trial_id, {"step": 1, "train_acc": 2}),
        ],
    )

    backend.pause_trial(trial_id=trial_id)
    backend.resume_trial(trial_id, new_config={"num-epochs": 4})

    wait_until_all_trials_completed(backend)

    trial_statuses, new_metrics = get_status_metrics(backend, trial_id)
    assert trial_statuses == {trial_id: Status.completed}
    check_metrics(
        new_metrics,
        [
            (trial_id, {"step": 2, "train_acc": 3}),
            (trial_id, {"step": 3, "train_acc": 4}),
        ],
    )


def test_start_config_previous_checkpoint(caplog):
    caplog.set_level(logging.INFO)
    path_script = Path(__file__).parent / "main_checkpoint.py"
    backend = temporary_local_backend(entry_point=path_script)

    # we start two trials, the checkpoint content should be resp. state-0, state-1 and then reported.
    backend.start_trial(config={"name": "state-0"})
    backend.start_trial(config={"name": "state-1"})

    wait_until_all_trials_completed(backend)
    # we check whether a trial can be started from a previous checkpoint
    backend.start_trial(checkpoint_trial_id=0, config={"name": "state-2"})
    backend.start_trial(checkpoint_trial_id=1, config={"name": "state-3"})

    wait_until_all_trials_completed(backend)

    # the two trials that were started should have reported the content of their checkpoint, e.g. state-0 and state-1.
    _, new_metrics = backend.fetch_status_results([0, 1, 2, 3])
    results = list(sorted(new_metrics, key=lambda x: x[0]))
    results = [res["checkpoint_content"] for _, res in results]
    assert results == ["nothing", "nothing", "state-0", "state-1"]


def test_gpu_allocation(caplog):
    caplog.set_level(logging.INFO)
    path_script = Path(__file__).parent / "main_checkpoint.py"
    backend = temporary_local_backend(entry_point=path_script)

    backend._prepare_for_schedule(num_gpus=4)
    env = dict()
    for trial_id in range(4):
        backend._allocate_gpu(trial_id=trial_id, env=env)
    for trial_id in range(4):
        gpu = trial_id
        assert backend.trial_gpu[trial_id] == gpu
        assert backend.gpu_times_assigned[gpu] == 1
    backend._deallocate_gpu(trial_id=2)
    backend._allocate_gpu(trial_id=4, env=env)
    # GPU 2 is the only free one, so must be used:
    assert backend.trial_gpu[4] == 2
    assert backend.gpu_times_assigned[2] == 2
    backend._deallocate_gpu(trial_id=0)
    backend._deallocate_gpu(trial_id=2)
    backend._allocate_gpu(trial_id=5, env=env)
    # Both GPUs 0, 2 are free, but 0 has less prior assignments:
    assert backend.trial_gpu[5] == 0
    assert backend.gpu_times_assigned[0] == 2
    backend._allocate_gpu(trial_id=6, env=env)
    # All GPUs are allocated. 1, 3 have less prior assignments:
    gpu = backend.trial_gpu[6]
    assert gpu in {1, 3}
    assert backend.gpu_times_assigned[gpu] == 2

File Path: tst/trial_backend/test_python_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import tempfile
from syne_tune.backend import PythonBackend
from syne_tune.backend.trial_status import Status
from syne_tune.config_space import randint
from tst.util_test import wait_until_all_trials_completed


def f(x):
    import logging
    from syne_tune import Reporter

    root = logging.getLogger()
    root.setLevel(logging.DEBUG)
    reporter = Reporter()
    for i in range(5):
        reporter(step=i + 1, y=x + i)


def test_python_backend():
    with tempfile.TemporaryDirectory() as local_path:
        import logging

        root = logging.getLogger()
        root.setLevel(logging.INFO)
        backend = PythonBackend(f, config_space={"x": randint(0, 10)})
        backend.set_path(str(local_path))
        backend.start_trial({"x": 2})
        backend.start_trial({"x": 3})

        wait_until_all_trials_completed(backend)

        trials, metrics = backend.fetch_status_results([0, 1])

        for trial, status in trials.values():
            assert status == Status.completed, "\n".join(
                backend.stdout(trial.trial_id)
            ) + "\n".join(backend.stderr(trial.trial_id))

        metrics_first_trial = [metric["y"] for x, metric in metrics if x == 0]
        metrics_second_trial = [metric["y"] for x, metric in metrics if x == 1]
        assert metrics_first_trial == [2, 3, 4, 5, 6]
        assert metrics_second_trial == [3, 4, 5, 6, 7]

File Path: tst/trial_backend/test_simulator_trial_backend.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import math
import pytest
import numpy as np

from syne_tune.backend import LocalBackend
from syne_tune.backend.simulator_backend.simulator_backend import (
    SimulatorBackend,
    SimulatorConfig,
)
from syne_tune.backend.simulator_backend.events import (
    SimulatorState,
    StartEvent,
    CompleteEvent,
    OnTrialResultEvent,
)
from syne_tune.backend.simulator_backend.simulator_callback import SimulatorCallback
from syne_tune.backend.trial_status import Status
from syne_tune.tuner_callback import StoreResultsCallback
from syne_tune import StoppingCriterion
from syne_tune import Tuner
from syne_tune.constants import ST_DECISION, ST_TRIAL_ID
from syne_tune.optimizer.schedulers.hyperband import HyperbandScheduler
from syne_tune.optimizer.schedulers.fifo import FIFOScheduler
from syne_tune.optimizer.scheduler import SchedulerDecision


def _compare_results(res_local: dict, res_simul: dict, num: int):
    for key in (ST_TRIAL_ID, ST_DECISION, "epoch", "mean_loss"):
        rloc = res_local[key]
        rsim = res_simul[key]
        if key != "mean_loss":
            assert_cond = rloc == rsim
        else:
            assert_cond = math.isclose(rloc, rsim, rel_tol=1e-6)
        assert assert_cond, (
            f"{num}, {key}: local = {rloc} != {rsim} = simul\n"
            + f"res_local = {res_local}\n"
            + f"res_simul = {res_simul}"
        )


# Note: This test is very tricky to get right. When training times are very
# short, trivial differences between local and simulated back-end get
# amplified. These do not play a role with realistic training times of
# more than 1 sec per epoch.
@pytest.mark.skip(
    "skipping for now since it depends on examples which is not included in path"
)
@pytest.mark.parametrize("scheduler_name", ["fifo"])
def test_compare_local_simulator_backends(scheduler_name):
    from examples.training_scripts.height_with_cost.train_height_with_cost import (
        height_with_cost_default_params,
        height_with_cost_benchmark,
    )

    random_seed = 382378624
    n_workers = 4
    tuner_sleep_time = 0.1
    # For 'bayesopt', fixing the seed does not render an experiment entirely
    # deterministic
    searcher_name = "random"

    default_params = height_with_cost_default_params()
    default_params["max_resource_level"] = 9  # To make it run faster
    benchmark = height_with_cost_benchmark(default_params)
    # Benchmark must be tabulated to support simulation:
    assert benchmark.get("supports_simulated", False)

    if scheduler_name == "fifo":
        stop_criterion = StoppingCriterion(max_num_trials_completed=6)
    else:
        stop_criterion = StoppingCriterion(max_num_trials_started=15)
    # Run experiment with two different back-ends
    results = dict()
    for backend_name in ("local", "simulated"):
        benchmark["config_space"]["dont_sleep"] = backend_name == "simulated"
        # Create scheduler
        # search_options = {'debug_log': False}
        search_options = {"debug_log": True}
        scheduler_options = {
            "searcher": searcher_name,
            "search_options": search_options,
            "metric": benchmark["metric"],
            "mode": benchmark["mode"],
            "random_seed": random_seed,
        }
        if scheduler_name != "fifo":
            sch_type = scheduler_name[len("hyperband_") :]
            scheduler_options.update(
                {
                    "resource_attr": benchmark["resource_attr"],
                    "type": sch_type,
                    "grace_period": 1,
                    "reduction_factor": 3,
                }
            )
        scheduler_cls = (
            FIFOScheduler if scheduler_name == "fifo" else HyperbandScheduler
        )
        scheduler = scheduler_cls(benchmark["config_space"], **scheduler_options)
        # Create back-end
        if backend_name == "local":
            trial_backend = LocalBackend(entry_point=benchmark["script"])
        else:
            simulator_config = SimulatorConfig(
                delay_on_trial_result=0,
                delay_complete_after_final_report=0,
                delay_complete_after_stop=0,
                delay_start=0,
                delay_stop=0,
            )
            trial_backend = SimulatorBackend(
                entry_point=benchmark["script"],
                elapsed_time_attr=benchmark["elapsed_time_attr"],
                simulator_config=simulator_config,
                tuner_sleep_time=tuner_sleep_time,
            )
            scheduler.set_time_keeper(trial_backend.time_keeper)

        _tuner_sleep_time = 0 if backend_name == "simulated" else tuner_sleep_time
        # Run experiment
        if backend_name == "local":
            # Duplicates callback used in `tuner.run`, but we have access
            result_callback = StoreResultsCallback()
        else:
            result_callback = SimulatorCallback()
        local_tuner = Tuner(
            trial_backend=trial_backend,
            scheduler=scheduler,
            stop_criterion=stop_criterion,
            n_workers=n_workers,
            sleep_time=_tuner_sleep_time,
            results_update_interval=100,
            print_update_interval=100,
            callbacks=[result_callback],
        )
        local_tuner.run()
        results[backend_name] = result_callback.results

    # Compare results. Note that times are not comparable. We may not see
    # exactly the same number of results, but the prefix of results should
    # be the same.
    # Note: Differences between the two are mainly due to very short training
    # times, which amplify the importance of processing in `LocalBackend` we
    # do not simulate (e.g., starting subprocesses).
    # Filter results for stopped trials which are received after the stop
    # rung. These are filtered out by the simulator back-end, but not by
    # the local back-end
    rung_levels = (9, 3, 1)
    max_resource_when_stopped = dict()
    stop_decisions = {SchedulerDecision.STOP, SchedulerDecision.PAUSE}
    for result in results["local"]:
        decision = result[ST_DECISION]
        if decision in stop_decisions:
            trial_id = result[ST_TRIAL_ID]
            resource = int(result["epoch"])
            max_resource_when_stopped[trial_id] = resource
    for trial_id, max_resource in max_resource_when_stopped.items():
        for rung_level in rung_levels:
            if max_resource >= rung_level:
                max_resource_when_stopped[trial_id] = rung_level
                break
    new_local = []
    for result in results["local"]:
        trial_id = result[ST_TRIAL_ID]
        resource = int(result["epoch"])
        decision = result[ST_DECISION]
        if (
            decision not in stop_decisions
            or resource <= max_resource_when_stopped[trial_id]
        ):
            new_local.append(result)
    results["local"] = new_local
    num_local = len(results["local"])
    num_simul = len(results["simulated"])
    if num_local != num_simul:
        print(
            f"{scheduler_name}: num_results_local = {num_local}, num_results_simul = {num_simul}"
        )

    def sort_key(result):
        return (result[ST_TRIAL_ID], result["epoch"])

    if num_local <= num_simul:
        k_short = "local"
        k_long = "simulated"
    else:
        k_long = "local"
        k_short = "simulated"
    dict_long = {sort_key(v): v for v in results[k_long]}
    new_short, new_long = [], []
    for v in results[k_short]:
        k = sort_key(v)
        if k in dict_long:
            new_long.append(dict_long[k])
            new_short.append(v)
    results[k_short] = new_short
    results[k_long] = new_long
    assert len(new_short) >= 0.7 * max(
        num_local, num_simul
    ), f"num_matched = {len(new_short)}, num_results_local = {num_local}, num_results_simul = {num_simul}"
    for i, (res_local, res_simul) in enumerate(
        zip(results["local"], results["simulated"])
    ):
        _compare_results(res_local, res_simul, i)


def test_simulator_state():
    state = SimulatorState()
    result1 = dict(epoch=1, accuracy=0.5)
    result2 = dict(epoch=2, accuracy=0.75)
    result3 = dict(epoch=3, accuracy=0.8)
    state.push(StartEvent(trial_id=0), event_time=1)
    state.push(OnTrialResultEvent(trial_id=0, result=result1), event_time=2)
    state.push(OnTrialResultEvent(trial_id=0, result=result2), event_time=2)
    state.push(OnTrialResultEvent(trial_id=0, result=result3), event_time=3)
    state.push(CompleteEvent(trial_id=0, status="completed"), event_time=3.2)
    state.push(StartEvent(trial_id=1), event_time=1)
    state.push(OnTrialResultEvent(trial_id=1, result=result1), event_time=2.5)
    state.push(OnTrialResultEvent(trial_id=1, result=result2), event_time=2.6)
    state.push(OnTrialResultEvent(trial_id=1, result=result3), event_time=3.5)
    state.push(CompleteEvent(trial_id=1, status="completed"), event_time=4)
    # Everything until 2.2
    required_results = [
        (StartEvent, 0, 1.0),
        (StartEvent, 1, 1.0),
        (OnTrialResultEvent, 0, 1, 2.0),
        (OnTrialResultEvent, 0, 2, 2.0),
    ]
    for i in range(2):
        obtained_results = []
        time_until = 2.2 if i == 0 else 4
        while True:
            entry = state.next_until(time_until)
            if entry is None:
                break
            else:
                obtained_results.append(entry)
        assert len(required_results) == len(
            obtained_results
        ), f"i={i}: {required_results}\n{obtained_results}"
        for j, (res_req, res_obt) in enumerate(zip(required_results, obtained_results)):
            assert res_req[-1] == res_obt[0], (i, j, res_req[-1], res_obt[0])
            event = res_obt[1]
            assert isinstance(event, res_req[0])
            assert event.trial_id == res_req[1]
            if isinstance(event, OnTrialResultEvent):
                assert event.result["epoch"] == res_req[2]
            elif isinstance(event, CompleteEvent):
                assert event.status == res_req[2]
        if i == 0:
            state.remove_events(trial_id=0)
        required_results = [
            (OnTrialResultEvent, 1, 1, 2.5),
            (OnTrialResultEvent, 1, 2, 2.6),
            (OnTrialResultEvent, 1, 3, 3.5),
            (CompleteEvent, 1, "completed", 4),
        ]


@pytest.mark.skip("TODO: Needs solution for nasbench201 dataset_s3_bucket")
def test_simulator_backend_with_table_class():
    from benchmarking.definitions.definition_nasbench201 import (
        nasbench201_default_params,
        nasbench201_benchmark,
    )

    default_params = nasbench201_default_params({"backend": "simulated"})
    benchmark = nasbench201_benchmark(default_params)
    backend = SimulatorBackend(
        entry_point=benchmark["script"],
        elapsed_time_attr=benchmark["elapsed_time_attr"],
        table_class_name=benchmark["benchmark_table_class"],
    )
    time_keeper = backend.time_keeper
    time_keeper.start_of_time()
    config = benchmark["config_space"]
    config.update(
        {
            "x0": "skip_connect",
            "x1": "avg_pool_3x3",
            "x2": "nor_conv_3x3",
            "x3": "none",
            "x4": "nor_conv_1x1",
            "x5": "nor_conv_3x3",
            "epochs": 10,
        }
    )
    backend.start_trial(config)
    time_keeper.advance(4 * 60)
    trial_ids = [0]
    trial_status_dict, results = backend.fetch_status_results(trial_ids)

    assert len(trial_status_dict) == 1
    assert 0 in trial_status_dict
    trial, status = trial_status_dict[0]
    assert status == Status.completed
    assert trial.config == config
    required_results = [
        {
            "epoch": 1,
            "objective": 0.20663999996948243,
            "elapsed_time": 13.849495785576954,
        },
        {
            "epoch": 2,
            "objective": 0.34536000004882816,
            "elapsed_time": 27.698991571153908,
        },
        {
            "epoch": 3,
            "objective": 0.4388400000244141,
            "elapsed_time": 41.54848735673086,
        },
        {
            "epoch": 4,
            "objective": 0.4395599999389648,
            "elapsed_time": 55.397983142307815,
        },
        {
            "epoch": 5,
            "objective": 0.5435599999755859,
            "elapsed_time": 69.24747892788477,
        },
        {
            "epoch": 6,
            "objective": 0.5663600000610353,
            "elapsed_time": 83.09697471346172,
        },
        {
            "epoch": 7,
            "objective": 0.5497999999023438,
            "elapsed_time": 96.94647049903868,
        },
        {
            "epoch": 8,
            "objective": 0.5019599998657228,
            "elapsed_time": 110.79596628461563,
        },
        {
            "epoch": 9,
            "objective": 0.6233999998046875,
            "elapsed_time": 124.64546207019258,
        },
        {
            "epoch": 10,
            "objective": 0.6034399998901369,
            "elapsed_time": 138.49495785576954,
        },
    ]
    assert len(results) == 10
    assert all(x[0] == 0 for x in results)
    assert [x[1]["epoch"] for x in results] == list(range(1, 11))
    actual = dict()
    required = dict()
    for k in ("objective", "elapsed_time"):
        actual[k] = np.array([x[1][k] for x in results])
        required[k] = np.array([x[k] for x in required_results])
        np.testing.assert_almost_equal(actual[k], required[k])

File Path: tst/util_test.py
Content:
# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
import tempfile
import time

from syne_tune.backend import LocalBackend
from syne_tune.backend.trial_status import Status


def temporary_local_backend(entry_point: str):
    """
    :param entry_point:
    :return: a backend whose files are deleted after finishing to avoid side-effects. This is used in unit-tests.
    """
    with tempfile.TemporaryDirectory() as local_path:
        backend = LocalBackend(entry_point=entry_point)
        backend.set_path(results_root=local_path)
        return backend


def wait_until_all_trials_completed(backend):
    def status(backend, trial_ids):
        return [trial.status for trial in backend._all_trial_results(trial_ids)]

    i = 0
    while not all(
        [status == Status.completed for status in status(backend, backend.trial_ids)]
    ):
        time.sleep(0.1)
        assert i < 100, "backend trials did not finish after 10s"

Output:
{
    "experimental_code": "class PASHA(HyperbandScheduler):\n    def __init__(self, config_space: Dict, metric: str, resource_attr: str, **kwargs):\n        _assert_max_resource_args(kwargs)\n        super(PASHA, self).__init__(\n            config_space=config_space,\n            metric=metric,\n            searcher=\"random\",\n            resource_attr=resource_attr,\n            type=\"pasha\",\n            **kwargs,\n        )\n\nimport numpy as np\nfrom syne_tune.optimizer.schedulers.hyperband_promotion import PromotionRungSystem\nimport itertools\n\nclass PASHARungSystem(PromotionRungSystem):\n    def __init__(\n        self,\n        rung_levels,\n        promote_quantiles,\n        metric,\n        mode,\n        resource_attr,\n        max_t,\n        ranking_criterion,\n        epsilon,\n        epsilon_scaling,\n    ):\n        super().__init__(\n            rung_levels, promote_quantiles, metric, mode, resource_attr, max_t\n        )\n        self.ranking_criterion = ranking_criterion\n        self.current_rung_idx = 2\n        self.rung_levels = rung_levels\n        self.current_max_t = rung_levels[self.current_rung_idx - 1]\n        self.epsilon = epsilon\n        self.epsilon_scaling = epsilon_scaling\n        if ranking_criterion == 'soft_ranking_auto':\n            self.per_epoch_results = {}\n            self.epoch_to_trials = {}\n            self.current_max_epoch = -1\n\n    def _effective_max_t(self):\n        return self.current_max_t\n\n    def _get_top_rungs_rankings(self, num_rungs=2):\n        rankings = []\n        rungs = [self._rungs[-self.current_rung_idx + e] for e in range(num_rungs)]\n        for rung in rungs:\n            if rung.data != {}:\n                trial_ids = rung.data.keys()\n                values = []\n                for trial_id in trial_ids:\n                    values.append(rung.data[trial_id][0])\n                values_order = np.array(values).argsort()\n                values_ranking = values_order.argsort()\n                ranking = list(zip(trial_ids, values_ranking, values))\n\n                rankings.append(ranking)\n\n        return rankings\n\n    def _get_sorted_top_rungs(self, rankings):\n        top_rung_keys = set([e[0] for e in rankings[0]])\n        corresponding_previous_rung_trials = filter(\n            lambda e: e[0] in top_rung_keys, rankings[1]\n        )\n        if self._mode == \"max\":\n            reverse = True\n        else:\n            reverse = False\n\n        sorted_top_rung = sorted(rankings[0], key=lambda e: e[1], reverse=reverse)\n        sorted_previous_rung = sorted(\n            corresponding_previous_rung_trials, key=lambda e: e[1], reverse=reverse\n        )\n        return sorted_top_rung, sorted_previous_rung\n\n    def _evaluate_soft_ranking(self, sorted_top_rung, sorted_previous_rung) -> bool:\n        keep_current_budget = True\n        if len(sorted_previous_rung) < 2:\n            epsilon = 0.0\n        elif self.ranking_criterion == \"soft_ranking_std\":\n            epsilon = (\n                np.std([e[2] for e in sorted_previous_rung]) * self.epsilon_scaling\n            )\n        elif (\n            self.ranking_criterion == \"soft_ranking_median_dst\"\n            or self.ranking_criterion == \"soft_ranking_mean_dst\"\n        ):\n            scores = [e[2] for e in sorted_previous_rung]\n            distances = [\n                abs(e1 - e2)\n                for idx1, e1 in enumerate(scores)\n                for idx2, e2 in enumerate(scores)\n                if idx1 != idx2\n            ]\n            if self.ranking_criterion == \"soft_ranking_mean_dst\":\n                epsilon = np.mean(distances) * self.epsilon_scaling\n            elif self.ranking_criterion == \"soft_ranking_median_dst\":\n                epsilon = np.median(distances) * self.epsilon_scaling\n            else:\n                raise ValueError(\n                    \"Ranking criterion {} is not supported\".format(\n                        self.ranking_criterion\n                    )\n                )\n        else:\n            epsilon = self.epsilon\n\n        previous_rung_groups = []\n        for idx, item in enumerate(sorted_previous_rung):\n            current_rung_group = [item[0]]\n            for idx_after in range(idx + 1, len(sorted_previous_rung)):\n                new_item = sorted_previous_rung[idx_after]\n\n                if self._mode == \"max\":\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                else:\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            for idx_before in range(idx - 1, -1, -1):\n                new_item = sorted_previous_rung[idx_before]\n                if self._mode == \"max\":\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                else:\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            previous_rung_groups.append(set(current_rung_group))\n\n        for idx, item in enumerate(sorted_top_rung):\n            if item[0] not in previous_rung_groups[idx]:\n                keep_current_budget = False\n                break\n\n        return keep_current_budget\n\n    def _update_epsilon(self):\n        seen_pairs = set()\n        noisy_cfg_distances = []\n        top_epoch = min(self.current_max_epoch, self._rungs[-self.current_rung_idx].level)\n        bottom_epoch = min(self._rungs[-self.current_rung_idx+1].level, self.current_max_epoch)\n        for epoch in range(top_epoch, bottom_epoch, -1):\n            if len(self.epoch_to_trials[epoch]) > 1:\n                for pair in itertools.combinations(self.epoch_to_trials[epoch], 2):\n                    c1, c2 = pair[0], pair[1]\n                    if (c1, c2) not in seen_pairs:\n                        seen_pairs.add((c1, c2))\n                        p1, p2 = self.per_epoch_results[c1][epoch], self.per_epoch_results[c2][epoch]\n                        cond = p1 > p2\n\n                        opposite_order = False\n                        same_order_after_opposite = False\n                        for prev_epoch in range(epoch - 1, 0, -1):\n                            pp1, pp2 = self.per_epoch_results[c1][prev_epoch], self.per_epoch_results[c2][prev_epoch]\n                            p_cond = pp1 > pp2\n                            if p_cond == (not cond):\n                                opposite_order = True\n                            if opposite_order and p_cond == cond:\n                                same_order_after_opposite = True\n                                break\n\n                        if opposite_order and same_order_after_opposite:\n                            noisy_cfg_distances.append(abs(p1 - p2))\n\n        if len(noisy_cfg_distances) > 0:\n            self.epsilon = np.percentile(noisy_cfg_distances, 90)\n            if str(self.epsilon) == 'nan':\n                raise ValueError('Epsilon became nan') \n\n    def _update_per_epoch_results(self, trial_id, result):\n        if trial_id not in self.per_epoch_results:\n            self.per_epoch_results[trial_id] = {}\n        self.per_epoch_results[trial_id][result[self._resource_attr]] = result[self._metric]\n\n        if result[self._resource_attr] not in self.epoch_to_trials:\n            self.epoch_to_trials[result[self._resource_attr]] = set() \n        self.epoch_to_trials[result[self._resource_attr]].add(trial_id)\n\n        if result[self._resource_attr] > self.current_max_epoch:\n            self.current_max_epoch = result[self._resource_attr]\n\n    def _decide_resource_increase(self, rankings) -> bool:\n        if len(rankings) == 2:\n            sorted_top_rung, sorted_previous_rung = self._get_sorted_top_rungs(rankings)\n        else:\n            return False\n\n        keep_current_budget = self._evaluate_soft_ranking(\n            sorted_top_rung, sorted_previous_rung\n        )\n\n        return not keep_current_budget\n\n    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:\n        ret_dict = super().on_task_report(trial_id, result, skip_rungs)\n\n        if self.ranking_criterion == \"soft_ranking_auto\":\n            self._update_per_epoch_results(trial_id, result)\n            self._update_epsilon()\n\n        rankings = self._get_top_rungs_rankings(num_rungs=2)\n        increase_resources = self._decide_resource_increase(rankings)\n\n        if increase_resources:\n            if self.current_rung_idx < len(self._rungs):\n                self.current_rung_idx += 1\n                self.current_max_t = self.rung_levels[self.current_rung_idx - 1]\n            else:\n                self.current_max_t = self.max_t\n\n        return ret_dict\n\n# From benchmarking/cli/scheduler_factory.py\n        if scheduler == \"hyperband_pasha\":\n            rung_system_kwargs = scheduler_options.get(\"rung_system_kwargs\", dict())\n            for name, tp in (\n                (\"ranking_criterion\", str),\n                (\"epsilon\", float),\n                (\"epsilon_scaling\", float),\n            ):\n                name_cl = \"pasha_\" + name\n                v = params.get(name_cl)\n                if v is not None:\n                    rung_system_kwargs[name] = tp(v)\n            if rung_system_kwargs:\n                scheduler_options[\"rung_system_kwargs\"] = rung_system_kwargs\n\n# From notebooks/run_bo_experiments.py\n    if hpo_approach == 'pasha':\n        scheduler = baselines_dict['PASHA'](\n            config_space,\n            max_t=max_t,\n            grace_period=default_params['grace_period'],\n            reduction_factor=reduction_factor,\n            resource_attr=resource_attr,\n            mode=mode,\n            metric=metric,\n            random_seed=random_seed,\n            rung_system_kwargs=rung_system_kwargs)\n    elif hpo_approach == 'pasha-bo':\n        scheduler = HyperbandScheduler(\n            config_space,\n            max_t=max_t,\n            grace_period=default_params['grace_period'],\n            reduction_factor=reduction_factor,\n            resource_attr=resource_attr,\n            mode=mode,\n            searcher='bayesopt',\n            type='pasha',\n            metric=metric,\n            random_seed=random_seed,\n            rung_system_kwargs=rung_system_kwargs)",
    "experimental_info": "PASHA (Progressive ASynchronous HAlving) extends ASHA by dynamically increasing resource allocation only if the ranking of configurations within the top two rungs has not stabilized. It incorporates a 'soft ranking' approach, considering configurations equivalent if their performance difference is below a threshold \\u03b5. This \\u03b5 value can either be manually specified (`--pasha_epsilon`) or automatically estimated. Automatic estimation calculates \\u03b5 as the N-th percentile (default 90th) of performance differences among 'criss-crossing' configurations (those that repeatedly swap ranks across resource levels). The estimation method can be specified via `--pasha_ranking_criterion` (options include 'soft_ranking_std', 'soft_ranking_median_dst', 'soft_ranking_mean_dst', 'soft_ranking_auto'), and an optional `--pasha_epsilon_scaling` factor can be applied to the automatically estimated \\u03b5.\n\nKey configurable parameters include:\n- `pasha_ranking_criterion`: (string) Specifies the strategy for deciding ranking stability and resource increase. Options: 'soft_ranking', 'soft_ranking_std', 'soft_ranking_median_dst', 'soft_ranking_mean_dst', 'soft_ranking_auto'.\n- `pasha_epsilon`: (float) Threshold for soft ranking when `ranking_criterion` is 'soft_ranking'.\n- `pasha_epsilon_scaling`: (float) Scaling factor for automatically estimated epsilon.\n\nExample setting observed: `rung_system_kwargs = {'ranking_criterion': 'soft_ranking_auto', 'epsilon': 0.0}`, where `epsilon=0.0` suggests a placeholder when `soft_ranking_auto` is used to determine epsilon dynamically."
}
